Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Duplicate),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Required),Inward issue link (dependent),Outward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Authors),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Reviewer),Custom field (Reviewers),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Streaming retry is no longer performed,CASSANDRA-3686,12536702,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,30/Dec/11 22:34,16/Apr/19 09:32,14/Jul/23 05:51,31/Dec/11 05:05,1.0.7,,,,,,0,stream,,,"CASSANDRA-3532 changed exception handling when processing incoming stream, but since it wraps all exception into RuntimeException, streaming retry which had been occurred when IOException is thrown no longer works.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/11 22:36;yukim;0001-Fix-error-handling-for-streaming-retry.patch;https://issues.apache.org/jira/secure/attachment/12508958/0001-Fix-error-handling-for-streaming-retry.patch",,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,222385,,,Sat Dec 31 05:05:34 UTC 2011,,,,,,,,,,"0|i0gmzz:",95156,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"30/Dec/11 22:36;yukim;Patch attached to just throw IOException when it happens, otherwise wrap in RuntimeException.;;;","31/Dec/11 05:05;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Composite Column Support for PIG,CASSANDRA-3684,12536603,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jalkanen,bcoverston,bcoverston,29/Dec/11 18:26,16/Apr/19 09:32,14/Jul/23 05:52,29/Feb/12 20:36,1.0.9,1.1.0,,,,,1,,,,"It appears that some changes need to be made to support CompositeColumns. Right now if you try to load and use a column family that utilizes composite columns you get the following exception[1].

It appears to me that we need to modify the storage handler for Pig to support this scenario.


[1]

================================================================================
Backend error message
---------------------
java.lang.RuntimeException: Unexpected data type -1 found in stream.
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:478)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeBag(BinInterSedes.java:522)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:361)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:357)
	at org.apache.pig.data.BinSedesTuple.write(BinSedesTuple.java:57)
	at org.apache.pig.impl.io.PigNullableWritable.write(PigNullableWritable.java:123)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:90)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:77)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1061)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:691)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:116)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:239)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:232)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:272)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.mapred.Child.main(Child.java:266)

Backend error message
---------------------
java.lang.Throwable: Child Error
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)
Caused by: java.io.IOException: Task process exit with nonzero status of 65.
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:258)",,cherro,elubow,jalkanen,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Feb/12 20:21;jalkanen;3684-jalkanen-test-v2.txt;https://issues.apache.org/jira/secure/attachment/12516602/3684-jalkanen-test-v2.txt","29/Feb/12 19:37;jalkanen;3684-jalkanen-test.txt;https://issues.apache.org/jira/secure/attachment/12516595/3684-jalkanen-test.txt","28/Feb/12 20:16;jalkanen;3684-jalkanen.txt;https://issues.apache.org/jira/secure/attachment/12516385/3684-jalkanen.txt",,,,,,,,,,,,3.0,jalkanen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,222286,,,Wed Feb 29 20:36:36 UTC 2012,,,,,,,,,,"0|i0gmz3:",95152,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"28/Feb/12 20:16;jalkanen;This patch (against cassandra-1.0) brings in basic support for Composite Columns.

I needed a simple way to deconstruct an AbstractCompositeType, so I had to enhance that. Dunno if that's desireable, but it's certainly easy :-P

Also the column name is an untyped tuple; not sure what would be the best way to extract the schema for it.;;;","28/Feb/12 20:18;jalkanen;Also, apologies for extra crap; my OCD demands that my git is configured to remove extra space at the end of the lines :). If this approach looks feasible, I'll make a cleaner patch.;;;","29/Feb/12 19:37;jalkanen;Patch to provide tests against the Composite Columns.;;;","29/Feb/12 20:21;jalkanen;Improved test patch against the tests; this time with also Long:Long composite type.;;;","29/Feb/12 20:36;brandon.williams;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple threads can attempt hint handoff to the same target,CASSANDRA-3681,12536553,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,29/Dec/11 03:03,16/Apr/19 09:32,14/Jul/23 05:52,05/Jan/12 23:06,1.0.7,,,,,,0,hintedhandoff,,,"HintedHandOffManager attempts to prevent multiple threads sending hints to the same target with the queuedDeliveries set, but the code is buggy.  If two handoffs *do* occur concurrently, the second thread can use an arbitrarily large amount of memory skipping tombstones when it starts paging from the beginning of the hint row, looking for the first live hint.  (This is not a problem with a single thread, since it always pages starting with the last-seen hint column name, effectively skipping the tombstones.  Then it compacts when it's done.)

Technically this bug is present in all older Cassandra releases, but it only causes problems in 1.0.x since the hint rows tend to be much larger (since there is one hint per write containing the entire mutation, instead of just one per row consisting of just the key).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/12 22:41;jbellis;3681-v3.txt;https://issues.apache.org/jira/secure/attachment/12509620/3681-v3.txt","29/Dec/11 03:05;jbellis;3681.txt;https://issues.apache.org/jira/secure/attachment/12508816/3681.txt","05/Jan/12 19:26;brandon.williams;3681v2.txt;https://issues.apache.org/jira/secure/attachment/12509596/3681v2.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,222236,,,Thu Jan 05 23:06:49 UTC 2012,,,,,,,,,,"0|i0gmxj:",95145,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"29/Dec/11 03:05;jbellis;patch applies on top of the one for CASSANDRA-3624.;;;","04/Jan/12 15:48;brandon.williams;The problem here is that now the schema check comes before the FD check:

{noformat}
ERROR 15:45:37,154 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: Didin't receive gossiped schema from /10.179.64.227 in 60000ms
        at org.apache.cassandra.db.HintedHandOffManager.waitForSchemaAgreement(HintedHandOffManager.java:210)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:239)
        at org.apache.cassandra.db.HintedHandOffManager.access$200(HintedHandOffManager.java:84)
        at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:385)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

I added another FD check for this in the patch on CASSANDRA-3677 (which isn't the problem there) but something similar should work.;;;","05/Jan/12 19:26;brandon.williams;v2 does the FD check before and after the schema check.;;;","05/Jan/12 22:41;jbellis;v3 changes waitForSchemaAgreement to throw TimeoutException, which deliver can then catch and abort.  Also moves the queuedDeliveries removal back to a try/finally block, but covering the entire method.;;;","05/Jan/12 23:01;brandon.williams;+1;;;","05/Jan/12 23:06;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE during HH delivery when gossip turned off on target,CASSANDRA-3677,12536442,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,hsn,hsn,27/Dec/11 20:49,16/Apr/19 09:32,14/Jul/23 05:52,07/Feb/12 15:33,1.0.8,,,,,,0,,,,"probably not important bug

ERROR [OptionalTasks:1] 2011-12-27 21:44:25,342 AbstractCassandraDaemon.java (line 138) Fatal exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException
        at org.cliffc.high_scale_lib.NonBlockingHashMap.hash(NonBlockingHashMap.java:113)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:553)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:348)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfAbsent(NonBlockingHashMap.java:319)
        at org.cliffc.high_scale_lib.NonBlockingHashSet.add(NonBlockingHashSet.java:32)
        at org.apache.cassandra.db.HintedHandOffManager.scheduleHintDelivery(HintedHandOffManager.java:371)
        at org.apache.cassandra.db.HintedHandOffManager.scheduleAllDeliveries(HintedHandOffManager.java:356)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:84)
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:119)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
",,appodictic,soverton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/12 15:09;soverton;3677-v1.patch;https://issues.apache.org/jira/secure/attachment/12513620/3677-v1.patch","28/Dec/11 20:33;brandon.williams;3677.txt;https://issues.apache.org/jira/secure/attachment/12508789/3677.txt",,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,222125,,,Thu Feb 23 20:59:34 UTC 2012,,,,,,,,,,"0|i0gmvz:",95138,,,,,Low,,,,,,,,,,,,,,,,,"28/Dec/11 18:25;brandon.williams;Can you confirm the version is 1.0.6?  This only seems possible with CASSANDRA-3554, because otherwise hint delivery only happens when a node comes up.;;;","28/Dec/11 19:37;hsn;its 1.0.6 + patches from 1.0 branch;;;","28/Dec/11 20:33;brandon.williams;The simplest thing to do is double up on the FD check.;;;","29/Dec/11 10:14;hsn;This error is not related to target node down, i got it today and no nodes down are reported.


INFO [HintedHandoff:1] 2011-12-29 10:49:38,993 HintedHandOffManager.java (line 
334) Finished hinted handoff of 0 rows to endpoint /216.17.99.40

 INFO [CompactionExecutor:5] 2011-12-29 10:54:52,465 CompactionTask.java (line 113) Compacting [SSTableReader(path='/usr/local/cassandra/data/system/HintsColumnFamily-hc-963-Data.db'), SSTableReader(path='/usr/local/cassandra/data/system/HintsColumnFamily-hc-962-Data.db')]
 INFO [CompactionExecutor:5] 2011-12-29 10:55:08,744 CompactionTask.java (line 221) Compacted to [/usr/local/cassandra/data/system/HintsColumnFamily-hc-964-Data.db,].  709,504,640 to 165,449,718 (~23% of original) bytes for 2 keys at 9.692558MB/s.  Time: 16,279ms.

ERROR [OptionalTasks:1] 2011-12-29 10:59:06,482 AbstractCassandraDaemon.java (line 138) Fatal exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException
        at org.cliffc.high_scale_lib.NonBlockingHashMap.hash(NonBlockingHashMap.java:113)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:553)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:348)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfAbsent(NonBlockingHashMap.java:319)
        at org.cliffc.high_scale_lib.NonBlockingHashSet.add(NonBlockingHashSet.java:32)
        at org.apache.cassandra.db.HintedHandOffManager.scheduleHintDelivery(HintedHandOffManager.java:371)
        at org.apache.cassandra.db.HintedHandOffManager.scheduleAllDeliveries(HintedHandOffManager.java:356)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:84)
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:119)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679);;;","02/Jan/12 15:10;hsn;I suspect that this is happening when node is trying to deliver hints created for himself (to is null after all)

because i have some stuck hints, cant list them via CLI to look at keys because node will OOM.

INFO [CompactionExecutor:7] 2012-01-02 15:48:13,084 CompactionTask.java (line 221) Compacted to [/usr/local/cassandra/data/system/HintsColumnFamily-hc-1011-Data.db,].  168,568,056 to 163,770,679 (~97% of original) bytes for 1 keys at 29.619551MB/s.  Time: 5,273ms.

Its 2 node cluster and no hints were delivered to other node:

INFO [HintedHandoff:1] 2012-01-02 15:41:29,035 HintedHandOffManager.java (line 267) Started hinted handoff for token: 99070591730234615865843651857942052864
INFO [HintedHandoff:1] 2012-01-02 15:41:29,217 HintedHandOffManager.java (line 334) Finished hinted handoff of 0 rows to;;;","06/Jan/12 19:17;brandon.williams;Sounds like this is just fallout from CASSANDRA-3440 then, you can delete the hints and see if it continues.;;;","07/Feb/12 15:08;soverton;I've been able to reproduce this in 1.07 and in trunk as follows:
* create a cluster of 2 nodes
* stop one of the nodes
* insert some data at RF=1, CL=ANY - this causes hints to be stored 
* nodetool removetoken on the token of the dead node
* some time up to 10 minutes later the exception is logged:

ERROR [OptionalTasks:1] 2012-02-07 14:41:57,710 AbstractCassandraDaemon.java (line 134) Fatal exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException
	at org.cliffc.high_scale_lib.NonBlockingHashMap.hash(NonBlockingHashMap.java:113)
	at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:553)
	at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:348)
	at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfAbsent(NonBlockingHashMap.java:319)
	at org.cliffc.high_scale_lib.NonBlockingHashSet.add(NonBlockingHashSet.java:32)
	at org.apache.cassandra.db.HintedHandOffManager.scheduleHintDelivery(HintedHandOffManager.java:410)
	at org.apache.cassandra.db.HintedHandOffManager.scheduleAllDeliveries(HintedHandOffManager.java:395)
	at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:84)
	at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:119)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)

Adverse Effects:
* The 10min repeating scheduleAllDeliveries() introduced in CASSANDRA-3554 will no longer fire (since the exception was uncaught), so the node has now regressed to pre-CASSANDRA-3554 behaviour
* Even after the node is restarted, the exception will be thrown again next time the schedule is run. 
* This will continue until the row tombstone for the dead node's hints is garbage-collected (10 days)

Fix:
* don't try to schedule delivery of hints for tokens which are no longer valid endpoints (see attached).
;;;","07/Feb/12 15:09;soverton;attached patch against trunk;;;","07/Feb/12 15:33;brandon.williams;Good catch, Sam.  Committed.;;;","15/Feb/12 16:26;appodictic;Q: don't try to schedule delivery of hints for tokens which are no longer valid endpoints (see attached).

Does this mean the hints are just lost for good? Does anyone thing an attempt should be made to determine where the hints should go and possibly re-write them?;;;","15/Feb/12 17:00;brandon.williams;It doesn't make a lot of sense.  a) we have no way to quickly find such hints, and b) if you removetoken the node, the data from existing replicas will be copied to restore the RF, so the hint isn't necessary (unless you wrote at ANY, in which case you've already lived dangerously.);;;","15/Feb/12 17:03;soverton;When a token is removed, hints intended for that endpoint are removed (see StorageService.excise(Token token, InetAddress endpoint)) so yes, they are lost for good. 

The removetoken process involves streaming from replicas to the new endpoint, so it should be up to date assuming writes were at CL > ANY. I can't think of a case where we would gain anything by delivering the hints for the removed endpoint (except where writes were at CL.ANY).
;;;","15/Feb/12 17:31;appodictic;It seems wrong to abandon hints. Arguments like such as ""(unless you wrote at ANY, in which case you've already lived dangerously.)"" are  a slippery slope, and it says something about the contract of ANY.

According to Cassandra.thrift.
{quote}
 *   ANY          Ensure that the write has been written once somewhere, including possibly being hinted in a non-target node.
{quote}

It does not say :

{quote}
 *   ANY          Ensure that the write has been written once somewhere, including possibly being hinted in a non-target node, which probably wont get lost, unless .....
{quote}


Maybe there is some other harder to contrive scenario out there RF3, write ONE, two hints and one node failure then a move also causes an issue with lost hints.

It is an edge case, but I think it is important. Since writes are idempotent I would rather a hint gets delivered causing an extra write then it gets lost. 

1.0's made HH way more reliable, I would like to see Cassandra push that high standard and not have caveats associated around how ANY works.

 



;;;","23/Feb/12 20:59;jbellis;If a node is removed while it's up (decommission), then removing hints should be a no-op, since they should be handed off before the stream-to-new-owners.  I'd be okay with a minor tweak to perform a check before decommission to refuse to continue if this is not the case.

But if a node is removed while dead (removeToken) then the hints for the old node are worthless, since we're streaming from a different copy, that by definition didn't need the hints for the dead one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing compaction strategy from Leveled to SizeTiered logs millions of messages about nothing to compact,CASSANDRA-3666,12536283,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,vjevdokimov,vjevdokimov,23/Dec/11 13:09,16/Apr/19 09:32,14/Jul/23 05:52,03/Jan/12 15:32,1.0.7,,,,,,0,compaction,,,"When column family compaction strategy is changed from Leveled to SizeTiered and there're Leveled compaction tasks pending, Cassandra starting to flood in logs with thousands per sec messages:

Nothing to compact in ColumnFamily1.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)

As a result, log disk is full and system is down.",Windows Server 2008 R2 64bit,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Dec/11 20:21;jbellis;3666.txt;https://issues.apache.org/jira/secure/attachment/12508564/3666.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,221966,,,Tue Jan 03 15:32:42 UTC 2012,,,,,,,,,,"0|i0gmrr:",95119,,vjevdokimov,,vjevdokimov,Normal,,,,,,,,,,,,,,,,,"23/Dec/11 20:11;jbellis;Were there any error messages logged?;;;","23/Dec/11 20:21;jbellis;I bet the culprit is how LCS sets the min/max compaction threshold but STCS does not.  Can you try the attached patch?;;;","03/Jan/12 14:45;slebresne;+1 on the patch (but we can wait on Viktor to confirm it does fix it for him).

(I actually think that it's the settings of the threshold in LCS that is the ugly part, but let's not bother with that);;;","03/Jan/12 15:32;slebresne;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flush non-cfs backed secondary indexes along with CF,CASSANDRA-3659,12536158,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,22/Dec/11 14:04,16/Apr/19 09:32,14/Jul/23 05:52,22/Dec/11 16:14,1.0.7,,,Feature/2i Index,,,0,secondary_index,,,Non CFS backed secondary indexes currently don't get flushed alongside CF.  Only CFS backed ones do (i.e. KEYS),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/11 14:06;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3659-flush-non-cfs-backed-indexes.txt;https://issues.apache.org/jira/secure/attachment/12508390/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3659-flush-non-cfs-backed-indexes.txt",,,,,,,,,,,,,,1.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,221841,,,Thu Dec 22 16:14:40 UTC 2011,,,,,,,,,,"0|i0gmof:",95104,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"22/Dec/11 15:12;xedin;+1;;;","22/Dec/11 16:14;tjake;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix smallish problems find by FindBugs,CASSANDRA-3658,12536150,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,22/Dec/11 11:28,16/Apr/19 09:32,14/Jul/23 05:52,22/Dec/11 21:56,1.1.0,,,,,,0,fingbugs,,,"I've just run (the newly released) FindBugs 2 out of curiosity. Attaching a number of patches related to issue raised by it. There is nothing major at all so all patches are against trunk.

I've tried keep each issue to it's own patch with a self describing title. It far from covers all FindBugs alerts, but it's a picky tool so I've tried to address only what felt at least vaguely useful. Those are still mostly nits (only patch 2 is probably an actual bug).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/11 11:29;slebresne;0001-Respect-Future-semantic.patch;https://issues.apache.org/jira/secure/attachment/12508370/0001-Respect-Future-semantic.patch","22/Dec/11 11:29;slebresne;0002-Avoid-race-when-reloading-snitch-file.patch;https://issues.apache.org/jira/secure/attachment/12508371/0002-Avoid-race-when-reloading-snitch-file.patch","22/Dec/11 11:29;slebresne;0003-use-static-inner-class-when-possible.patch;https://issues.apache.org/jira/secure/attachment/12508372/0003-use-static-inner-class-when-possible.patch","22/Dec/11 11:29;slebresne;0004-Remove-dead-code.patch;https://issues.apache.org/jira/secure/attachment/12508373/0004-Remove-dead-code.patch","22/Dec/11 11:29;slebresne;0005-Protect-against-signed-byte-extension.patch;https://issues.apache.org/jira/secure/attachment/12508374/0005-Protect-against-signed-byte-extension.patch","22/Dec/11 11:29;slebresne;0006-Add-hashCode-method-when-equals-is-overriden.patch;https://issues.apache.org/jira/secure/attachment/12508375/0006-Add-hashCode-method-when-equals-is-overriden.patch","22/Dec/11 11:29;slebresne;0007-Inverse-argument-of-compare-instead-of-negating-to-a.patch;https://issues.apache.org/jira/secure/attachment/12508376/0007-Inverse-argument-of-compare-instead-of-negating-to-a.patch","22/Dec/11 11:29;slebresne;0008-stop-pretending-Token-is-Serializable-LocalToken-is-.patch;https://issues.apache.org/jira/secure/attachment/12508377/0008-stop-pretending-Token-is-Serializable-LocalToken-is-.patch","22/Dec/11 11:29;slebresne;0009-remove-useless-assert-that-is-always-true.patch;https://issues.apache.org/jira/secure/attachment/12508378/0009-remove-useless-assert-that-is-always-true.patch","22/Dec/11 11:29;slebresne;0010-Add-equals-and-hashCode-to-Expiring-column.patch;https://issues.apache.org/jira/secure/attachment/12508379/0010-Add-equals-and-hashCode-to-Expiring-column.patch",,,,,10.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,221833,,,Tue Dec 27 21:01:48 UTC 2011,,,,,,,,,,"0|i0gmnz:",95102,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"22/Dec/11 17:08;jbellis;+1 on all but 09; I'd rather leave that assert alone since its point is to catch a bug if we change the signature of scanner;;;","22/Dec/11 21:56;slebresne;I'm good with keeping the assert. Committed all except 09. Thanks;;;","23/Dec/11 00:47;hudson;Integrated in Cassandra #1266 (See [https://builds.apache.org/job/Cassandra/1266/])
    Fix minor issues reported by FindBugs
patch by slebresne; reviewed by jbellis for CASSANDRA-3658

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1222476
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/trunk/src/java/org/apache/cassandra/config/ReplicationStrategy.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ArrayBackedSortedColumns.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamily.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ExpiringColumn.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/RowIteratorFactory.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/SizeTieredCompactionStrategy.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/filter/QueryFilter.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/marshal/AbstractType.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/marshal/DynamicCompositeType.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/marshal/ReversedType.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/LocalToken.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/Token.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/compress/CompressionMetadata.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableLoader.java
* /cassandra/trunk/src/java/org/apache/cassandra/locator/PropertyFileSnitch.java
* /cassandra/trunk/src/java/org/apache/cassandra/net/ProtocolHeader.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/AntiEntropyService.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/FileStreamTask.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/EstimatedHistogram.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/NodeId.java
;;;","27/Dec/11 20:52;nickmbailey;This breaks a bunch of jmx stuff. A fair amount of jmx methods return Token objects so they need to be serializable. I plan on doing CASSANDRA-2805 for 1.1, but jmx will be broken in trunk until I get that done unless that specific patch is reverted.;;;","27/Dec/11 21:01;jbellis;reverted 0008 for now;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GC can take 0 ms,CASSANDRA-3656,12535960,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,21/Dec/11 04:41,16/Apr/19 09:32,14/Jul/23 05:52,22/Dec/11 20:43,0.8.10,1.0.7,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/11 20:11;jbellis;3656.txt;https://issues.apache.org/jira/secure/attachment/12508431/3656.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,221643,,,Fri Dec 23 01:04:50 UTC 2011,,,,,,,,,,"0|i0gmn3:",95098,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"21/Dec/11 04:42;jbellis;As reported by Michael Vaknine on the mailing list,

{noformat}
STG-Cass4 ERROR [ScheduledTasks:1] 2011-12-12 22:55:26,554 java.lang.AssertionError
STG-Cass4 ERROR [ScheduledTasks:1] 2011-12-12 22:55:26,554 at org.apache.cassandra.service.GCInspector.logGCResults(GCInspector.java:103)
STG-Cass4 ERROR [ScheduledTasks:1] 2011-12-12 22:55:26,554 at org.apache.cassandra.service.GCInspector.access$000(GCInspector.java:41)
STG-Cass4 ERROR [ScheduledTasks:1] 2011-12-12 22:55:26,554 at org.apache.cassandra.service.GCInspector$1.run(GCInspector.java:85)
{noformat};;;","21/Dec/11 04:42;jbellis;(Michael's stacktrace is against 1.0.3.);;;","22/Dec/11 20:20;brandon.williams;+1;;;","22/Dec/11 20:43;jbellis;committed;;;","23/Dec/11 01:04;hudson;Integrated in Cassandra-0.8 #423 (See [https://builds.apache.org/job/Cassandra-0.8/423/])
    avoid logging (harmless) exception when GC takes < 1ms
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3656

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1222440
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/GCInspector.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when running upgradesstables,CASSANDRA-3655,12535918,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,tupshin,tupshin,20/Dec/11 22:33,16/Apr/19 09:32,14/Jul/23 05:52,03/Jan/12 15:27,1.0.7,,,,,,0,compaction,,,"Running a test upgrade from 0.7(version f sstables) to 1.0.
upgradesstables runs for about 40 minutes and then NPE's when trying to retrieve a key.

No files have been succesfully upgraded. Likely related is that scrub (without having run upgrade) consumes all RAM and OOMs.

Possible theory is that a lot of paths call IPartitioner's decorateKey, and, at least in the randompartitioner's implementation, if any of those callers pass a null ByteBuffer, they key will be null in the stack trace below.


java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:203)
	at org.apache.cassandra.db.compaction.CompactionManager.performSSTableRewrite(CompactionManager.java:219)
	at org.apache.cassandra.db.ColumnFamilyStore.sstablesRewrite(ColumnFamilyStore.java:970)
	at org.apache.cassandra.service.StorageService.upgradeSSTables(StorageService.java:1540)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.compaction.PrecompactedRow.removeDeletedAndOldShards(PrecompactedRow.java:65)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:92)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:137)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:102)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:87)
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:200)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:172)
	at org.apache.cassandra.db.compaction.CompactionManager$4.perform(CompactionManager.java:229)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
",1.0.5 + patch for https://issues.apache.org/jira/browse/CASSANDRA-3618,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/11 19:37;jbellis;3655.txt;https://issues.apache.org/jira/secure/attachment/12508422/3655.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,221601,,,Wed Jan 04 20:03:58 UTC 2012,,,,,,,,,,"0|i0gmmn:",95096,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"22/Dec/11 19:37;jbellis;The stacktrace indicates that either the controller or the cf parameters have to be null.  (key being null would NOT cause this exception.)

Controller is always initialized to non-null except in one place in the Streaming code which should be irrelevant here.  And cf comes from merge(rows) which should also return non-null unless there's a deserialization exception.  I bet that's what's happening -- there should be errors in the log about ""Skipping row X"" in that case.

Still, we shouldn't be ignoring those exceptions -- we should kill the compaction (or upgrade) and let the operator decide how to deal with them (e.g. with scrub).

Patch to fix that behavior, clean up the Streaming controller use, and add extra assertions.;;;","22/Dec/11 21:49;slebresne;+1 (It would help to know if the log from the error had ""Skipping row X"" to know if the hypothesis is correct but in any case I agree we shouldn't ignore those exceptions);;;","03/Jan/12 15:27;slebresne;Committed as even if didn't fixed the issue (which it probably did) it's an improvement.;;;","04/Jan/12 20:03;tupshin;Original problem confirmed fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
correct and improve stream protocol mismatch error,CASSANDRA-3652,12535802,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,20/Dec/11 08:38,16/Apr/19 09:32,14/Jul/23 05:52,22/Dec/11 20:32,1.0.7,,,,,,0,,,,"The message (and code comment) claims it got a ""newer"" version despite the fact that the check only determines that it is non-equal.

Fix that, and also print the actual version gotten and expected.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/11 08:41;scode;CASSANDRA-3652-1.0.txt;https://issues.apache.org/jira/secure/attachment/12508059/CASSANDRA-3652-1.0.txt",,,,,,,,,,,,,,1.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,221485,,,Thu Dec 22 20:32:44 UTC 2011,,,,,,,,,,"0|i0gmlb:",95090,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"20/Dec/11 08:41;scode;Patch attached.;;;","22/Dec/11 20:32;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Truncate shouldn't rethrow timeouts as UA,CASSANDRA-3651,12535793,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,20/Dec/11 07:02,16/Apr/19 09:32,14/Jul/23 05:52,22/Dec/11 16:41,1.1.0,,,,,,0,,,,"Truncate is a very easy operation to timeout, but the timeouts rethrow as UnavailableException which is somewhat confusing.  Instead it should throw TimedOutException.",,cherro,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/11 16:07;brandon.williams;0001-Update-thrift-definition.txt;https://issues.apache.org/jira/secure/attachment/12508405/0001-Update-thrift-definition.txt","20/Dec/11 12:24;brandon.williams;0002-truncate-throws-TOE-on-timeout.txt;https://issues.apache.org/jira/secure/attachment/12508069/0002-truncate-throws-TOE-on-timeout.txt",,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,221476,,,Thu Dec 22 17:33:38 UTC 2011,,,,,,,,,,"0|i0gmkv:",95088,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"22/Dec/11 15:54;jbellis;Doesn't updating the Thrift signature mean (Java) clients now have to catch or rethrow TOE where before they did not?  If so we should probably do this in 1.1.;;;","22/Dec/11 16:02;brandon.williams;Yes. :(  Checked exceptions strike again.;;;","22/Dec/11 16:26;jbellis;+1;;;","22/Dec/11 16:41;brandon.williams;Committed.;;;","22/Dec/11 17:33;hudson;Integrated in Cassandra #1264 (See [https://builds.apache.org/job/Cassandra/1264/])
    Truncate throws TOE on timeout instead of UA.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-3651

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1222340
Files : 
* /cassandra/trunk/interface/cassandra.thrift
* /cassandra/trunk/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
* /cassandra/trunk/interface/thrift/gen-java/org/apache/cassandra/thrift/Constants.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/CassandraServer.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CASSANDRA-2335 was properly resolved - artifact:pom doesn't support the ""groupId"" attribute",CASSANDRA-3650,12535787,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,somerandomperson,somerandomperson,20/Dec/11 06:02,16/Apr/19 09:32,14/Jul/23 05:52,20/Dec/11 06:05,,,,,,,0,,,,"CASSANDRA-2335 was about a build error that windows users will see saying ""artifact:pom doesn't support the ""groupId"" attribute"". The fix for this is described in http://wiki.apache.org/cassandra/RunningCassandraInEclipse#artifact:pom_error and was outlined in CASSANDRA-2335. This bug was originally filed in the mistaken belief that the issue wasn't noted, when it had been.",Windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,221470,,,Tue Dec 20 06:05:56 UTC 2011,,,,,,,,,,"0|i0gmkf:",95086,,ben.coverston@datastax.com,,ben.coverston@datastax.com,Low,,,,,,,,,,,,,,,,,"20/Dec/11 06:05;somerandomperson;Next time read the build instructions correctly. This error is noted at the end.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
parsing of chunk_length_kb silently overflows,CASSANDRA-3644,12535605,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,18/Dec/11 01:32,16/Apr/19 09:32,14/Jul/23 05:52,22/Dec/11 20:40,1.0.7,,,,,,0,,,,"Not likely to trigger for ""real"" values; I noticed because some other bug caused the chunk length setting to be corrupted somehow and take on some huge value having nothing to do with what I asked for in my schema update (not yet identified why; separate issue).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/11 06:10;scode;CASSANDRA-3644-1.0-v2.txt;https://issues.apache.org/jira/secure/attachment/12508052/CASSANDRA-3644-1.0-v2.txt","18/Dec/11 01:33;scode;CASSANDRA-3644-1.0.txt;https://issues.apache.org/jira/secure/attachment/12507823/CASSANDRA-3644-1.0.txt",,,,,,,,,,,,,2.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,221289,,,Thu Dec 22 20:40:13 UTC 2011,,,,,,,,,,"0|i0gmgv:",95070,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Dec/11 06:34;jbellis;Patch does not apply to 1.0 branch (I assume from the name that's what it's from?);;;","20/Dec/11 06:10;scode;My apologies. I must have accidentally taken the wrong branch. v2 attached, against 1.0.;;;","22/Dec/11 20:40;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inconsistent/corrupt counters w/ broken shards never converge,CASSANDRA-3641,12535364,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,scode,scode,scode,15/Dec/11 20:09,16/Apr/19 09:32,14/Jul/23 05:52,02/Jan/12 17:21,1.1.0,,,,,,0,,,,"We ran into a case (which MIGHT be related to CASSANDRA-3070) whereby we had counters that were corrupt (hopefully due to CASSANDRA-3178). The corruption was that there would exist shards with the *same* node_id, *same* clock id, but *different* counts.

The counter column diffing and reconciliation code assumes that this never happens, and ignores the count. The problem with this is that if there is an inconsistency, the result of a reconciliation will depend on the order of the shards.

In our case for example, we would see the value of the counter randomly fluctuating on a CL.ALL read, but we would get consistent (whatever the node had) on CL.ONE (submitted to one of the nodes in the replica set for the key).

In addition, read repair would not work despite digest mismatches because the diffing algorithm also did not care about the counts when determining the differences to send.

I'm attaching patches that fixes this. The first patch is against our 0.8 branch, which is not terribly useful to people, but I include it because it is the well-tested version that we have used on the production cluster which was subject to this corruption.

The other patch is against trunk, and contains the same change.

What the patch does is:

* On diffing, treat as DISJOINT if there is a count discrepancy.
* On reconciliation, look at the count and *deterministically* pick the higher one, and:
** log the fact that we detected a corrupt counter
** increment a JMX observable counter for monitoring purposes

A cluster which is subject to such corruption and has this patch, will fix itself with and AES + compact (or just repeated compactions assuming the replicate-on-compact is able to deliver correctly).
",,feestend,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3670,,CASSANDRA-3070,CASSANDRA-3178,,,,,"15/Dec/11 20:12;scode;3641-0.8-internal-not-for-inclusion.txt;https://issues.apache.org/jira/secure/attachment/12507575/3641-0.8-internal-not-for-inclusion.txt","15/Dec/11 20:12;scode;3641-trunk.txt;https://issues.apache.org/jira/secure/attachment/12507574/3641-trunk.txt","24/Dec/11 03:15;scode;CASSANDRA-3641-trunk-nojmx.txt;https://issues.apache.org/jira/secure/attachment/12508582/CASSANDRA-3641-trunk-nojmx.txt",,,,,,,,,,,,3.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,221048,,,Mon Jan 02 17:21:34 UTC 2012,,,,,,,,,,"0|i0gmfb:",95063,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"17/Dec/11 00:44;scode;Sorry, this depends on CASSANDRA-3603. I'll submit the patch there in a minute.;;;","19/Dec/11 10:47;slebresne;The fix lgtm, but I'd be in favor of removing the JMX reporting. Logging at ERROR seems enough and I fear a JMX counter will confuse users more than anything else (you can always use a specific log4j logger if you're so inclined to monitor logged errors through JMX).

Nit: I'd also change the comment from ""We should never see shards w/ same id+clock but different counts"" to ""We should never see *non-delta* shards w/ same id+clock but different counts"", just to make sure someone reading this comment too quickly don't leave with the wrong info. ;;;","19/Dec/11 17:36;scode;I'll fix the comment (it was written before I understood fully the role of deltas).

As for the JMX counter: I kind of see your concern, but at the same time - most people that have monitoring of Cassandra at all will have setups to easily monitor/graph/alert on JMX exposed values and I really think it's a shame if we can't add additional instrumentation because it would confuse users.

How about putting it somewhere else, where it's clearly nothing you need to worry about normally? I actually had an original patch before I submitted upstream where I had created a separate MBean I called ""RedFlags"" because I found no good place to put the counter. The idea was that it felt completely overkill to have a dedicated MBean for the purpose, but at the same time I really wanted it accounted for. RedFlags was intended as a place to put counters that you essentially always expect to be exactly 0 during healthy production use.

I could see putting more stuff there like exception counts in places where any exception indicates a sever problem, or a count of out of disk space conditions preventing or affecting (different bucket) compaction, or a count of GC pauses above a certain threshold, etc.

If you agree I'll volunteer to go through and add some things I can think of, along with this count.

Else I can certainly re-submit without the JMX counter. Or just submit a separate JIRA for it (but that's only worth it if you might be okay with a RedFlags style approach and it's not just this one counter).
;;;","20/Dec/11 12:19;slebresne;Let's open a separate ticket to discuss that. So far we've use the log only for recording errors so let's keep it at that for this ticket.;;;","24/Dec/11 03:15;scode;New version attached. Rebased to current trunk, and no JMX. Otherwise identical.;;;","02/Jan/12 17:21;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
rangeSlice may iterate the whole memtable while just query one row . This may seriously affect the  performance .,CASSANDRA-3638,12535297,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,jonma,jonma,15/Dec/11 13:12,16/Apr/19 09:32,14/Jul/23 05:52,23/Dec/11 16:25,1.1.0,,,,,,0,,,,"RangeSliceVerbHandler may  just only query one row , but cassandra may iterate the whole memtable .
the problem is in ColumnFamilyStore.getRangeSlice() method .


{color:red} // this iterator may iterate the whole memtable!!{color}
{code:title=ColumnFamilyStore.java|borderStyle=solid}
 public List<Row> getRangeSlice(ByteBuffer superColumn, final AbstractBounds range, int maxResults, IFilter columnFilter)
    throws ExecutionException, InterruptedException
    {
    ...
        DecoratedKey startWith = new DecoratedKey(range.left, null);
        DecoratedKey stopAt = new DecoratedKey(range.right, null);

        QueryFilter filter = new QueryFilter(null, new QueryPath(columnFamily, superColumn, null), columnFilter);
        int gcBefore = (int)(System.currentTimeMillis() / 1000) - metadata.getGcGraceSeconds();

        List<Row> rows;
        ViewFragment view = markReferenced(startWith, stopAt);
        try
        {
            CloseableIterator<Row> iterator = RowIteratorFactory.getIterator(view.memtables, view.sstables, startWith, stopAt, filter, getComparator(), this);
            rows = new ArrayList<Row>();

            try
            {
                // pull rows out of the iterator
                boolean first = true;
                while (iterator.hasNext()) // this iterator may iterate the whole memtable!!               
               {
                    ....
                }
            }
          .....
        }
       .....
        return rows;
    }

{code} 

{color:red} // Just only query one row ,but returned a sublist of columnFamiles   {color}
{code:title=Memtable.java|borderStyle=solid}
// Just only query one row ,but returned a sublist of columnFamiles     
public Iterator<Map.Entry<DecoratedKey, ColumnFamily>> getEntryIterator(DecoratedKey startWith)
    {
        return columnFamilies.tailMap(startWith).entrySet().iterator();
    }
{code} 


{color:red} // entry.getKey() will never bigger or equal to startKey, and then iterate the whole sublist of memtable {color}             
{code:title=RowIteratorFactory.java|borderStyle=solid}
 public IColumnIterator computeNext()
        {
            while (iter.hasNext())
            {
                Map.Entry<DecoratedKey, ColumnFamily> entry = iter.next();
                IColumnIterator ici = filter.getMemtableColumnIterator(entry.getValue(), entry.getKey(), comparator);
                // entry.getKey() will never bigger or equal to startKey, and then iterate the whole sublist of memtable             
                if (pred.apply(ici))  
                    return ici;
            }
            return endOfData();
{code} ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/11 14:25;slebresne;3638.patch;https://issues.apache.org/jira/secure/attachment/12507907/3638.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220981,,,Fri Dec 23 17:18:28 UTC 2011,,,,,,,,,,"0|i0gme7:",95058,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"15/Dec/11 15:54;jbellis;getRangeSlice is the ""scan a lot of rows"" method.  getColumnFamily is the ""scan a single row"" method.;;;","16/Dec/11 01:10;jonma;I was cheated by the name also . I debug RangeSliceVerbHandler.java , I find the range  contains only one key . ;;;","19/Dec/11 13:09;jonma;
{code:title=RowIteratorFactory.java|borderStyle=solid}
public IColumnIterator computeNext()
        {
            while (iter.hasNext())
            {
                Map.Entry<DecoratedKey, ColumnFamily> entry = iter.next();
                IColumnIterator ici = filter.getMemtableColumnIterator(entry.getValue(), entry.getKey(), comparator);
                if (pred.apply(ici))  
                    return ici;
            }
            return endOfData();
{code} 
The 'iter' is a submap of memtable's columnFamilies  . If pred.apply(ici) == false , 'iter' will iterate to the end .
Suppose 'iter' is  [ c ,d ,e ,f ] ,  pred.startKey is a ,and endKey is b , pred.apply(ici)  will always be false . 
In fact ,in this case , 'iter' never need to iterate , and should directly return endOfData() . This is the problem!
So , can anybody review the code too ?

;;;","19/Dec/11 14:25;slebresne;I think you're right. And this go way back I believe, we've never used the stopAt bound to  stop iteration early in the range slice case.

Patch attached to fix this (against trunk).;;;","19/Dec/11 15:36;jbellis;+1;;;","23/Dec/11 16:25;slebresne;Committed, thanks;;;","23/Dec/11 17:18;hudson;Integrated in Cassandra #1267 (See [https://builds.apache.org/job/Cassandra/1267/])
    Optimize memtable iteration during range scan
patch by slebresne; reviewed by jbellis for CASSANDRA-3638

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1222728
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/RowIteratorFactory.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra 1.0.x breakes APT on debian OpenVZ,CASSANDRA-3636,12535263,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,zenek_kraweznik0,zenek_kraweznik0,15/Dec/11 07:51,16/Apr/19 09:32,14/Jul/23 05:52,23/Jul/12 17:07,1.0.11,1.1.3,,Packaging,,,0,,,,"During upgrade from 1.0.6
{code}Setting up cassandra (1.0.6) ...
*error: permission denied on key 'vm.max_map_count'*
dpkg: error processing cassandra (--configure):
 subprocess installed post-installation script returned error exit status 255
Errors were encountered while processing:
 cassandra
{code}","Debian Linux (stable), OpenVZ container",hudson,joschi,mohitz,patrik.modesto,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/12 02:35;thepaul;3636.patch.txt;https://issues.apache.org/jira/secure/attachment/12527987/3636.patch.txt",,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220947,,,Fri Aug 03 20:48:13 UTC 2012,,,,,,,,,,"0|i0gmd3:",95053,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"09/Jan/12 21:53;brandon.williams;What version of openvz is this?  As far as I can tell, there shouldn't be a problem changing this in a guest.;;;","13/Jan/12 13:22;zenek_kraweznik0;This must be increasec on host, not on guest. This is kernel shared memory modifications, guests haven't their own kernel.

This problem apperears on debian squeeze openvz kernel and on newest openvz patches;;;","19/Jan/12 09:11;saulius.grigaitis;I have exactly same version with cassandra 1.0.6 and 1.0.7 version (1.0.5 version works fine)

Installing from debian packanges on ubuntu 10.04 64bits:

Installing new version of config file /etc/cassandra/cassandra.yaml ...
error: permission denied on key 'vm.max_map_count'
dpkg: error processing cassandra (--configure):
 subprocess installed post-installation script returned error exit status 255

uname -a
Linux server 2.6.32-042stab044.11 #1 SMP Wed Dec 14 16:02:00 MSK 2011 x86_64 GNU/Linux;;;","19/Jan/12 15:46;thepaul;Saulius- are you also running openvz? What version, if so?;;;","20/Jan/12 09:13;saulius.grigaitis;Paul > yes, I'm running openvz. Openvz kernel version is 2.6.32-042stab044.11 .;;;","21/Jan/12 00:24;zenek_kraweznik0;Cassandra is running on openvz, but post-install procedure is not completed, but changing this sysctl in openvz container is impossible.

This is not a bug with OpenVZ stricte, so I set components to packaging.

There is line in cassandra.postinst script:
sysctl -p /etc/sysctl.d/cassandra.conf

apt tries to run this script before install/remove any packages and stops on this step.

This script is breaking apt.

changing ""/etc/sysctl.d/cassandra.conf"" to ""sysctl -p /etc/sysctl.d/cassandra.conf || rm /etc/sysctl.d/cassandra.conf"" could solve this.;;;","12/Apr/12 10:02;tiotempestade;Solved it! Just unpack the .deb and comment the sysctl -p /etc/sysctl.d/cassandra.conf line in postinst.

It is already set to vm.max_map_count = 1048575 as the script tries to do..


After that rebuild the .deb and proceed with Datastax's installation.;;;","13/Apr/12 07:49;patrik.modesto;Hi, it's still a problem for cassandra 1.0.9 and openvz 3.0.23 on Debian Squeeze.;;;","13/Apr/12 07:52;patrik.modesto;Added latest stable version of Cassandra.;;;","14/May/12 17:04;jbellis;So...  should we just log a warning if the sysctl fails instead of failing entirely?;;;","14/May/12 21:33;thepaul;Not much else we _can_ do.;;;","18/May/12 02:35;thepaul;This change should allow the deb install to succeed. Patrik, Tio, Zenek, Saulius, or someone else with this problem, could you try applying the patch and installing the deb?

Change also available on the 3636 branch on my github:

https://github.com/thepaul/cassandra/tree/3636;;;","21/May/12 16:29;brandon.williams;Committed.;;;","23/Jul/12 09:00;joschi;The fix seems to have been lost during a merge.

The [trunk version of debian/cassandra.postinst|https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blob;f=debian/cassandra.postinst] doesn't contain the changes from Paul's fix in commit [48438ffc61bce2396fa2eac2f88fa66a6e6a69cd|https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commitdiff;h=48438ffc61bce2396fa2eac2f88fa66a6e6a69cd].;;;","23/Jul/12 17:04;hudson;Integrated in Cassandra #1768 (See [https://builds.apache.org/job/Cassandra/1768/])
    merge CASSANDRA-3636 from 1.0 to 1.1 cassandra 1.0.x breakes APT on debian OpenVZ (Revision ff64c5d11e9e66dfa42a1ca72572a65fb7480f37)

     Result = ABORTED
dbrosius : 
Files : 
* debian/cassandra.postinst
;;;","23/Jul/12 17:07;brandon.williams;bq. The trunk version of debian/cassandra.postinst doesn't contain the changes from Paul's fix in commit 48438ffc61bce2396fa2eac2f88fa66a6e6a69cd.

Yes it does.;;;","03/Aug/12 14:15;mohitz;I am still facing this issue with cassandra 1.1.2. I downloaded the debian file and unpacked it to look at the postinst script. I don't see Paul's fix in there.;;;","03/Aug/12 17:20;brandon.williams;You're looking at the wrong branch, in that case, because it's not in 1.1.2.;;;","03/Aug/12 20:48;mohitz;The fix version here said 1.1.1 incorrectly, so i checked 1.1.2. Thanks for correcting it.  ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
using an ant builder in Eclipse is painful,CASSANDRA-3632,12535199,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,14/Dec/11 20:09,16/Apr/19 09:32,14/Jul/23 05:52,19/Dec/11 15:59,,,,Legacy/Tools,Packaging,,2,,,,"The {{generate-eclipse-files}} target creates project files that use an Ant builder.  Besides being painfully slow (I've had the runs stack up behind frequent saves), many of Eclipses errors and warnings do not show unless an internal builder is used.",,ardot,burkhardt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/11 20:09;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3632-remove-ant-builder-restore-java-builder.txt;https://issues.apache.org/jira/secure/attachment/12507423/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3632-remove-ant-builder-restore-java-builder.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220883,,,Mon Jun 18 23:45:23 UTC 2012,,,,,,,,,,"0|i0gmbj:",95046,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"19/Dec/11 15:23;tjake;+1!;;;","19/Dec/11 15:59;urandom;committed; closing;;;","19/Dec/11 17:27;hudson;Integrated in Cassandra #1260 (See [https://builds.apache.org/job/Cassandra/1260/])
    remove ant builder; restore java builder

Patch by eevans; reviewed by Rick Shaw for CASSANDRA-3632

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1220838
Files : 
* /cassandra/trunk/build.xml
;;;","18/Jun/12 23:45;burkhardt;For more general usage of ant, here is a tutorial for review:
http://i-proving.com/2005/10/31/ant-tutorial/ ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrapping nodes don't ensure schema is ready before continuing,CASSANDRA-3629,12535105,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,14/Dec/11 07:40,16/Apr/19 09:32,14/Jul/23 05:52,14/Dec/11 21:25,1.0.7,,,,,,0,,,,"A bootstrapping node will assume that after it has slept for RING_DELAY it has all of the schema migrations and can continue the bootstrap process.  However, with a large enough amount of migrations this is not sufficient and causes problems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/11 07:41;brandon.williams;0001-Wait-until-the-highest-known-schema-has-been-reached.txt;https://issues.apache.org/jira/secure/attachment/12507313/0001-Wait-until-the-highest-known-schema-has-been-reached.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220789,,,Thu Apr 05 16:45:28 UTC 2012,,,,,,,,,,"0|i0gm9b:",95036,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"14/Dec/11 15:38;jbellis;+1

should we put this in 0.8.9 as well?;;;","14/Dec/11 18:45;brandon.williams;bq. should we put this in 0.8.9 as well?

The 0.8.9 ship looks sailed unless we want to reroll for this, do you mean 0.8.10?

Committed to 1.0, the 0.8 backport isn't trivial in any case, so leaving the ticket open for that.;;;","14/Dec/11 21:25;jbellis;If it's a big backport, let's leave it as a known limitation of 0.8.  We need to keep 0.8 stable more than we need this fixed there.;;;","05/Apr/12 10:02;jbellis;The easiest workaround for this is CASSANDRA-3600, which is also 1.0-only. That much should be safe to backport...;;;","05/Apr/12 16:45;j.casares;A valid workaround is to save your current schema, drain and stop all the nodes, move all Migration and Schema sstables from all nodes to another directory (for temporary backup), stop all nodes, restart one seed node, paste the old schema (to create the first mutation and recreate the schema), restart that node one more time, then start the rest of the nodes.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IN (...) SELECTs don't honor KEY keyword,CASSANDRA-3627,12535058,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,13/Dec/11 23:25,16/Apr/19 09:32,14/Jul/23 05:52,05/Jan/12 23:11,,,,Legacy/CQL,,,0,cql,,,"The WHERE clause of a SELECT ... IN (...) will not work with the KEY keyword, (but does with named/aliased keys).",,psanford,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220742,,,Thu Jan 05 23:11:17 UTC 2012,,,,,,,,,,"0|i0gm7z:",95030,,,,,Normal,,,,,,,,,,,,,,,,,"05/Jan/12 23:11;urandom;I'm not sure what I was seeing when I opened this; I am not able to reproduce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Nodes can get stuck in UP state forever, despite being DOWN",CASSANDRA-3626,12535056,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,scode,scode,13/Dec/11 23:18,16/Apr/19 09:32,14/Jul/23 05:52,15/Dec/11 19:15,0.8.10,1.0.7,,,,,0,,,,"This is a proposed phrasing for an upstream ticket named ""Newly discovered nodes that are down get stuck in UP state forever"" (will edit w/ feedback until done):

We have a observed a problem with gossip which, when you are bootstrapping a new node (or replacing using the replace_token support), any node in the cluster which is Down at the time the node is started, will be assumed to be Up and then *never ever* flapped back to Down until you restart the node.

This has at least two implications to replacing or bootstrapping new nodes when there are nodes down in the ring:

* If the new node happens to select a node listed as (UP but in reality is DOWN) as a stream source, streaming will sit there hanging forever.
* If that doesn't happen (by picking another host), it will instead finish bootstrapping correctly, and begin servicing requests all the while thinking DOWN nodes are UP, and thus routing requests to them, generating timeouts.

The way to get out of this is to restart the node(s) that you bootstrapped.

I have tested and confirmed the symptom (that the bootstrapped node things other nodes are Up) using a fairly recent 1.0. The main debugging effort happened on 0.8 however, so all details below refer to 0.8 but are probably similar in 1.0.

Steps to reproduce:

* Bring up a cluster of >= 3 nodes. *Ensure RF is < N*, so that the cluster is operative with one node removed.
* Pick two random nodes A, and B. Shut them *both* off.
* Wait for everyone to realize they are both off (for good measure).
* Now, take node A and nuke it's data directories and re-start it, such that it comes up w/ normal bootstrap (or use replace_token; didn't test that but should not affect it).
* Watch how node A starts up, all the while believing node B is down, even though all other nodes in the cluster agree that B is down and B is in fact still turned off.

The mechanism by which it initially goes into Up state is that the node receives a gossip response from any other node in the cluster, and GossipDigestAck2VerbHandler.doVerb() calls Gossiper.applyStateLocally().

Gossiper.applyStateLocally() doesn't have any local endpoint state for the cluster, so the else statement at the end (""it's a new node"") gets triggered and handleMajorStateChange() is called. handleMajorStateChange() always calls markAlive(), unless the state is a dead state (but ""dead"" here does not mean ""not up"", but refers to joining/hibernate etc).

So at this point the node is up in the mind of the node you just bootstrapped.

Now, in each gossip round doStatusCheck() is called, which iterates over all nodes (including the one falsly Up) and among other things, calls FailureDetector.interpret() on each node.

FailureDetector.interpret() is meant to update its sense of Phi for the node, and potentially convict it. However there is a short-circuit at the top, whereby if we do not yet have any arrival window for the node, we simply return immediately.

Arrival intervals are only added as a result of a FailureDetector.report() call, which never happens in this case because the initial endpoint state we added, which came from a remote node that was up, had the latest version of the gossip state (so Gossiper.reportFailureDetector() will never call report()).

The result is that the node can never ever be convicted.

Now, let's ignore for a moment the problem that a node that is actually Down will be thought to be Up temporarily for a little while. That is sub-optimal, but let's aim for a fix to the more serious problem in this ticket - which is that is stays up forever.

Considered solutions:

* When interpret() gets called and there is no arrival window, we could add a faked arrival window far back in time to cause the node to have history and be marked down. This ""works"" in the particular test case. The problem is that since we are not ourselves actively trying to gossip to these nodes with any particular speed, it might take a significant time before we get any kind of confirmation from someone else that it's actually Up in cases where the node actually *is* Up, so it's not clear that this is a good idea.

* When interpret() gets called and there is no arrival window, we can simply convict it immediately. This has roughly similar behavior as the previous suggestion.

* When interpret() gets called and there is no arrival window, we can add a faked arrival window at the current time, which will allow it to be treated as Up until the usual time has passed before we exceed the Phi conviction threshold.

* When interpret() gets called and there is no arrival window, we can immediately convict it, *and* schedule it for immediate gossip on the next round in order to try to ensure they go Up quickly if they are indeed up. This has an effect of O(n) gossip traffic, as a special case once during node start-up. While theoretically a problem, I personally thing we can ignore it for now since it won't be a significant problem any time soon. However, this is more complicated since the way we queue up messages is asynchronously to background connection attempts. We'd have to make sure the initial gossip message actually gets sent on an open TCP connection (I haven't confirmed whether this will be the case or not).

The first three are simple to implement, possibly the fourth. But in all cases, I am worried about potential negative consequences that I am not seeing.

Thoughts?
",,brandon.williams,kzadorozhny,lenn0x,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/11 19:37;brandon.williams;3626.txt;https://issues.apache.org/jira/secure/attachment/12507408/3626.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220740,,,Thu Dec 15 22:28:47 UTC 2011,,,,,,,,,,"0|i0gm7j:",95028,,scode,,scode,Normal,,,,,,,,,,,,,,,,,"14/Dec/11 19:37;brandon.williams;bq. Now, let's ignore for a moment the problem that a node that is actually Down will be thought to be Up temporarily for a little while.

This is perfectly fine since this is what the SS.RING_DELAY stabilization period is designed to suss out - the correct ring state.

bq. Steps to reproduce:

This can be simplified to ""start two nodes, shut one down, add a third.""

bq. Considered solutions:

ISTM the simplest and most correct thing to do in this case is report() new nodes.  Patch to do so.;;;","14/Dec/11 19:38;brandon.williams;FWIW I went back to 0.7 to repro this and the bug is there - so we've had this for quite some time.;;;","15/Dec/11 05:37;scode;I agree off the top of my head, but I'm kinda suspicious as to why I didn't already arrive at that conclusion... I'll have another look at the code tomorrow and see if I can figure out some reason why this is not a good idea, but right now it seems like a +1 to me.;;;","15/Dec/11 18:56;scode;+1. :);;;","15/Dec/11 19:15;brandon.williams;Committed.;;;","15/Dec/11 22:28;hudson;Integrated in Cassandra-0.8 #419 (See [https://builds.apache.org/job/Cassandra-0.8/419/])
    Prevent new nodes from thinking down nodes are up forever.
Patch by brandonwilliams, reviewed by Peter Schuller for CASSANDRA-3626

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1214916
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted Handoff - related OOM,CASSANDRA-3624,12535015,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,marcuse,marcuse,13/Dec/11 20:34,16/Apr/19 09:32,14/Jul/23 05:52,04/Jan/12 16:04,1.0.7,,,,,,1,hintedhandoff,,,"One of our nodes had collected alot of hints for another node, so when the dead node came back and the row mutations were read back from disk, the node died with an OOM-exception (and kept dying after restart, even with increased heap (from 8G to 12G)). The heap dump contained alot of SuperColumns and our application does not use those (but HH does). 

I'm guessing that each mutation is big so that PAGE_SIZE*<mutation_size> does not fit in memory (will check this tomorrow)

A simple fix (if my assumption above is correct) would be to reduce the PAGE_SIZE in HintedHandOffManager.java to something like 10 (or even 1?) to reduce the memory pressure. The performance hit would be small since we are doing the hinted handoff throttle delay sleep before sending every *mutation* anyway (not every page), thoughts?

If anyone runs in to the same problem, I got the node started again by simply removing the HintsColumnFamily* files.",,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/11 02:58;jbellis;3624-rebased.txt;https://issues.apache.org/jira/secure/attachment/12508815/3624-rebased.txt","14/Dec/11 03:42;jbellis;3624.txt;https://issues.apache.org/jira/secure/attachment/12507299/3624.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220699,,,Wed Jan 04 16:04:14 UTC 2012,,,,,,,,,,"0|i0gm6v:",95025,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"14/Dec/11 03:36;jbellis;That makes sense.  (How big are your mutations?)

We added adaptive page sizing back in CASSANDRA-2652, but apparently removed it for the CASSANDRA-2045 redesign.;;;","14/Dec/11 03:42;jbellis;Patch to add back adaptive page sizing, and drops the default size to 128 columns.;;;","14/Dec/11 03:46;jbellis;bq. The performance hit would be small since we are doing the hinted handoff throttle delay sleep before sending every mutation anyway 

True, but this is likely to change (see Jake's comments to CASSANDRA-3554).;;;","22/Dec/11 21:06;jbellis;Marcus, are you able to test the attached patch?;;;","23/Dec/11 06:10;marcuse;not yet, i might be able to do it today though

this happened in a production cluster so i will have to try to reproduce somewhere else;;;","23/Dec/11 08:49;hsn;I have this problem too but i do not have large rows, i have huge number of small rows (max 180 bytes serialized);;;","28/Dec/11 10:47;hsn;My OOM during HH must have been caused by something else, most likely background compaction. I tried to use HH to deliver 2 million rows on 750MB heap and it worked fine. ;;;","29/Dec/11 02:58;jbellis;rebased;;;","04/Jan/12 15:31;brandon.williams;+1;;;","04/Jan/12 15:50;brandon.williams;minor nit: typo in 'behavr' in the comment;;;","04/Jan/12 16:04;jbellis;fixed typo and committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenBitSet can allocate more bytes than it needs,CASSANDRA-3618,12534885,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,13/Dec/11 00:52,16/Apr/19 09:32,14/Jul/23 05:52,14/Dec/11 15:43,1.0.7,,,,,,0,,,,"CASSANDRA-2466 changed OpenBitSet to break big long arrays into pages. However, it always allocate full pages, each page being of size 4096 * 8 bytes. This means that we almost always allocate too much bytes, and for a row that has 1 column, the associated row bloom filter allocates 32760 more bytes than it should.

This has a significant impact on performance. In a small test using the SSTableSimpleUnsortedWriter to generate rows with 1 column, 0.8 is about twice as fast as 1.0 because of that (the difference shrink when there is more columns obviously).",,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/11 00:56;slebresne;0001-Fix-openBitSet.patch;https://issues.apache.org/jira/secure/attachment/12507105/0001-Fix-openBitSet.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220569,,,Tue Dec 13 16:39:07 UTC 2011,,,,,,,,,,"0|i0gm4f:",95014,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"13/Dec/11 00:56;slebresne;Attached patch to avoid the over-allocation.

Note that it would probably be cleaner to reuse BigLongArray in OpenBitSet to deal with the ""paging"" in only one place, but that would involve far more changes in OpenBitSet than I'm confortable doing in 1.0.;;;","13/Dec/11 16:39;jbellis;Committed.

(This affects 1.0.1+, introduced by CASSANDRA-2466.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Temp SSTable and file descriptor leak,CASSANDRA-3616,12534843,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,eparusel,eparusel,12/Dec/11 22:07,16/Apr/19 09:32,14/Jul/23 05:52,19/Dec/11 09:28,1.0.7,,,,,,0,,,,"Discussion about this started in CASSANDRA-3532.  It's on it's own ticket now.

Anyhow:
The nodes in my cluster are using a lot of file descriptors, holding open tmp files. A few are using 50K+, nearing their limit (on Solaris, of 64K).

Here's a small snippet of lsof:
java 828 appdeployer *162u VREG 181,65540 0 333884 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776518-Data.db
java 828 appdeployer *163u VREG 181,65540 0 333502 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776452-Data.db
java 828 appdeployer *165u VREG 181,65540 0 333929 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776527-Index.db
java 828 appdeployer *166u VREG 181,65540 0 333859 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776514-Data.db
java 828 appdeployer *167u VREG 181,65540 0 333663 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776480-Data.db
java 828 appdeployer *168u VREG 181,65540 0 333812 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776506-Index.db

I spot checked a few and found they still exist on the filesystem too:
rw-rr- 1 appdeployer appdeployer 0 Dec 12 07:16 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776506-Index.db


After more investigation, it seems to happen during a CompactionTask.
I waited until I saw some -tmp- files hanging around in the data dir:

-rw-r--r--   1 appdeployer appdeployer       0 Dec 12 21:47:10 2011 messages_meta-tmp-hb-788904-Data.db
-rw-r--r--   1 appdeployer appdeployer       0 Dec 12 21:47:10 2011 messages_meta-tmp-hb-788904-Index.db

and then found this in the logs:
 INFO [CompactionExecutor:18839] 2011-12-12 21:47:07,173 CompactionTask.java (line 113) Compacting [SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760408-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760413-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760409-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-788314-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760407-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760412-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760410-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760411-Data.db')]

INFO [CompactionExecutor:18839] 2011-12-12 21:47:10,461 CompactionTask.java (line 218) Compacted to [/data1/cassandra/data/MA_DDR/messages_meta-hb-788896-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788897-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788898-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788899-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788900-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788901-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788902-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788903-Data.db,].  83,899,295 to 83,891,657 (~99% of original) bytes for 75,662 keys at 24.332518MB/s.  Time: 3,288ms.

Note that the timestamp of the 2nd log line matches the last modified time of the files, and has IDs leading up to, *but not including 788904*.

I thought this might be relavent information, but I haven't found the specific cause yet.","1.0.5 + CASSANDRA-3532 patch
Solaris 10",bryceg,eparusel,hsn,jalkanen,jborgstrom,jonma,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/11 19:47;slebresne;3616.patch;https://issues.apache.org/jira/secure/attachment/12507568/3616.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220527,,,Mon Dec 19 09:28:15 UTC 2011,,,,,,,,,,"0|i0gm3b:",95009,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"14/Dec/11 16:56;jborgstrom;Just a quick ""me too"".

After upgrading from 1.0.3 to 1.0.6 I see a bunch of left over 0 byte tmp files:
{code}
-rw-r--r--  1 cassandra 215          0 Dec 14 16:25 LogTokenIndex-tmp-hc-3175-Data.db
-rw-r--r--  1 cassandra 215          0 Dec 14 16:25 LogTokenIndex-tmp-hc-3175-Index.db
-rw-r--r--  1 cassandra 215          0 Dec 14 16:33 LogToken-tmp-hc-3093-Data.db
-rw-r--r--  1 cassandra 215          0 Dec 14 16:33 LogToken-tmp-hc-3093-Index.db
-rw-r--r--  1 cassandra 215          0 Dec 14 16:37 LogToken-tmp-hc-3104-Data.db
-rw-r--r--  1 cassandra 215          0 Dec 14 16:37 LogToken-tmp-hc-3104-Index.db
-rw-r--r--  1 cassandra 215          0 Dec 14 16:37 LogToken-tmp-hc-3105-Data.db
-rw-r--r--  1 cassandra 215          0 Dec 14 16:37 LogToken-tmp-hc-3105-Index.db
{code}
The timestamp suggests that they were created during a nodetool repair session.;;;","15/Dec/11 16:17;jbellis;Eric, do you also see correlation w/ repair operations?;;;","15/Dec/11 16:32;eparusel;I haven't run a repair lately (no deletes at this time, I plan on setting up scheduled repairs though), but can run one to find out in the next few hours.

So far I've only correlated it with compaction.
I should note we're using LeveledCompactionStrategy.;;;","15/Dec/11 18:29;brandon.williams;I can reproduce this with SizeTieredStrategy.;;;","15/Dec/11 18:35;jbellis;Just with compaction?  Did you get a debug log?;;;","15/Dec/11 19:05;brandon.williams;CASSANDRA-3532 is the culprit here.;;;","15/Dec/11 19:47;slebresne;I believe the problem is that when compaction is over we were creating an empty (and useless) writer. Previously we were just deleting it right away because the 'cleanupIfNeeded' was in the finally.

Patch attached to not create it in the first place.;;;","15/Dec/11 20:41;jbellis;+1;;;","16/Dec/11 21:02;eparusel;1.0.6 + 3616.patch fixes this issue for me, thanks!;;;","19/Dec/11 09:28;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLog BufferOverflowException,CASSANDRA-3615,12534826,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,rbranson,rbranson,rbranson,12/Dec/11 19:41,16/Apr/19 09:32,14/Jul/23 05:52,28/Dec/11 18:27,1.1.0,,,,,,1,,,,"Reported on mailing list http://mail-archives.apache.org/mod_mbox/cassandra-dev/201112.mbox/%3CCAJHHpg2Rw_BWFJ9DycRGSYkmwMwrJDK3%3Dzw3HwRoutWHbUcULw%40mail.gmail.com%3E

ERROR 14:07:31,215 Fatal exception in thread
Thread[COMMIT-LOG-WRITER,5,main]
java.nio.BufferOverflowException
at java.nio.Buffer.nextPutIndex(Buffer.java:501)
at java.nio.DirectByteBuffer.putInt(DirectByteBuffer.java:654)
at
org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:259)
at
org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:568)
at
org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
at java.lang.Thread.run(Thread.java:662)
 INFO 14:07:31,504 flushing high-traffic column family CFS(Keyspace='***',
ColumnFamily='***') (estimated 103394287 bytes)

It happened during a fairly standard load process using M/R.",,tivv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/11 05:54;rbranson;3615.txt;https://issues.apache.org/jira/secure/attachment/12508049/3615.txt","18/Dec/11 09:45;pkolaczk;cl-buffer-overflow.patch;https://issues.apache.org/jira/secure/attachment/12507829/cl-buffer-overflow.patch",,,,,,,,,,,,,2.0,rbranson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220510,,,Wed Jan 04 13:59:22 UTC 2012,,,,,,,,,,"0|i0gm33:",95008,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"15/Dec/11 10:06;tivv;I've got similar problem on 1.0.5:


ERROR [COMMIT-LOG-WRITER] 2011-12-13 21:11:57,004 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.nio.BufferOverflowException
        at java.nio.Buffer.nextPutIndex(Buffer.java:518)
        at java.nio.DirectByteBuffer.putInt(DirectByteBuffer.java:664)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:244)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:567)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Thread.java:679)

It seems that there is an incosistency between space checker and  actual write
org.apache.cassandra.db.commitlog.CommitLogSegment#hasCapacityFor checks for serialized length + ENTRY_OVERHEAD_SIZE = 4 + 8 + 8

At the same time, write also writes END_OF_SEGMENT_MARKER int, so ENTRY_OVERHEAD_SIZE should be 4+8+8+4
;;;","16/Dec/11 05:31;jbellis;sounds like write should only add END_OF_SEGMENT_MARKER if it's not already at EOF;;;","16/Dec/11 05:34;jbellis;But that's trunk-only, not 1.0.5...  There is no CLS:244 in 1.0.5. I don't think you're running the version you think you're running.;;;","16/Dec/11 09:20;tivv;Ooops. It seems that while fixing https://issues.apache.org/jira/browse/CASSANDRA-3573 I've deployed trunk. Will try to downgrade now since trunk does not seem to perform well for me.;;;","18/Dec/11 09:45;pkolaczk;A patch, increasing the segment overhead constant.;;;","19/Dec/11 18:48;rbranson;Any hints on how to reproduce this one?;;;","20/Dec/11 05:54;rbranson;I didn't have any luck with Piotr's patch specifically fixing the issue, but it's fairly close to finding the actual issue. 

Since the end-of-commit-log write is only necessary if it can actually write another mutation out to the log, it's unnecessary for cases that would trigger this BufferOverflowException.

Attached patch fixes the exception and includes a test case to reproduce & prevent regression.;;;","28/Dec/11 18:27;jbellis;committed Rick's patch;;;","04/Jan/12 11:43;tivv;I am sorry, but I don't see it committed in https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogSegment.java
;;;","04/Jan/12 13:59;brandon.williams;bq. I am sorry, but I don't see it committed in https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogSegment.java

That is because the github mirror is still tracking svn, and we recently switched to git.  Try here:

https://git-wip-us.apache.org/repos/asf?p=cassandra.git;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fatal exception in thread Thread[MigrationStage:1,5,main] (LeveledCompaction)",CASSANDRA-3614,12534825,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,asuffield-bossa,asuffield-bossa,12/Dec/11 19:36,16/Apr/19 09:32,14/Jul/23 05:52,12/Dec/11 20:30,1.0.7,,,,,,0,,,,"ERROR 19:29:39,712 Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:156)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:488)
        at org.apache.cassandra.db.DataTracker.removeAllSSTables(DataTracker.java:257)
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:267)
        at org.apache.cassandra.db.Table.unloadCf(Table.java:361)
        at org.apache.cassandra.db.Table.dropCf(Table.java:343)
        at org.apache.cassandra.db.migration.DropColumnFamily.applyModels(DropColumnFamily.java:87)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:156)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:73)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/11 20:00;jbellis;3614.txt;https://issues.apache.org/jira/secure/attachment/12507047/3614.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220509,,,Mon Dec 12 20:30:52 UTC 2011,,,,,,,,,,"0|i0gm2n:",95006,,asuffield,,asuffield,Normal,,,,,,,,,,,,,,,,,"12/Dec/11 19:38;asuffield-bossa;(This is the Debian build of the 1.0.6 RC that Sylvain just put up);;;","12/Dec/11 19:53;asuffield-bossa;Bigger log chunk. This has got something to do with schema migrations. I had just created the CF, and was trying to do some operations on it. Probably a truncate followed by a batch insertion of a few dozen rows.

 INFO [MigrationStage:1] 2011-12-12 19:42:48,182 Migration.java (line 119) Applying migration 784303e0-24f9-11e1-0000-0282f5487ffc Add column family: org.a
pache.cassandra.config.CFMetaData@e90e23[cfId=1083,ksName=CoreQA,cfName=Deal,cfType=Standard,comparator=org.apache.cassandra.db.marshal.UTF8Type,subcolumnc
omparator=<null>,comment=,rowCacheSize=1.0,keyCacheSize=1.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.ca
ssandra.db.marshal.UTF8Type,keyValidator=org.apache.cassandra.db.marshal.UTF8Type,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSe
conds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@9f2588,mergeSh
ardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStra
tegyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO [MigrationStage:1] 2011-12-12 19:42:48,183 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-Migrations@5834000(29724/37155 serialized/li
ve bytes, 1 ops)
 INFO [MigrationStage:1] 2011-12-12 19:42:48,183 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-Schema@3824284(25583/31978 serialized/live b
ytes, 8 ops)
 INFO [FlushWriter:2] 2011-12-12 19:42:48,184 Memtable.java (line 240) Writing Memtable-Migrations@5834000(29724/37155 serialized/live bytes, 1 ops)
 INFO [FlushWriter:2] 2011-12-12 19:42:48,219 Memtable.java (line 277) Completed flushing /var/lib/cassandra/data/system/Migrations-hc-135-Data.db (29788 b
ytes)
 INFO [FlushWriter:2] 2011-12-12 19:42:48,220 Memtable.java (line 240) Writing Memtable-Schema@3824284(25583/31978 serialized/live bytes, 8 ops)
 INFO [CompactionExecutor:3] 2011-12-12 19:42:48,221 CompactionTask.java (line 113) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/Migratio
ns-hc-132-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/Migrations-hc-133-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/Mig
rations-hc-134-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/Migrations-hc-135-Data.db')]
 INFO [FlushWriter:2] 2011-12-12 19:42:48,247 Memtable.java (line 277) Completed flushing /var/lib/cassandra/data/system/Schema-hc-135-Data.db (25733 bytes
)
 INFO [CompactionExecutor:3] 2011-12-12 19:42:48,371 CompactionTask.java (line 218) Compacted to [/var/lib/cassandra/data/system/Migrations-hc-136-Data.db,
].  1,521,844 to 1,521,704 (~99% of original) bytes for 1 keys at 9.674733MB/s.  Time: 150ms.
 INFO [CompactionExecutor:3] 2011-12-12 19:42:48,372 CompactionTask.java (line 113) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/Schema-h
c-135-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/Schema-hc-134-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/Schema-hc-1
33-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/Schema-hc-132-Data.db')]
 INFO [CompactionExecutor:3] 2011-12-12 19:42:48,482 CompactionTask.java (line 218) Compacted to [/var/lib/cassandra/data/system/Schema-hc-136-Data.db,].  
1,187,815 to 1,187,488 (~99% of original) bytes for 83 keys at 10.389695MB/s.  Time: 109ms.
 INFO [MigrationStage:1] 2011-12-12 19:43:58,220 Migration.java (line 119) Applying migration a20e7a60-24f9-11e1-0000-0282f5487ffc Drop column family: Core
QA.Deal
 INFO [MigrationStage:1] 2011-12-12 19:43:58,221 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-Migrations@19716308(29025/36281 serialized/l
ive bytes, 1 ops)
 INFO [MigrationStage:1] 2011-12-12 19:43:58,224 ColumnFamilyStore.java (line 692) Enqueuing flush of Memtable-Schema@13515618(25227/31533 serialized/live 
bytes, 8 ops)
 INFO [FlushWriter:3] 2011-12-12 19:43:58,225 Memtable.java (line 240) Writing Memtable-Migrations@19716308(29025/36281 serialized/live bytes, 1 ops)
 INFO [FlushWriter:3] 2011-12-12 19:43:58,248 Memtable.java (line 277) Completed flushing /var/lib/cassandra/data/system/Migrations-hc-138-Data.db (29089 b
ytes)
 INFO [FlushWriter:3] 2011-12-12 19:43:58,249 Memtable.java (line 240) Writing Memtable-Schema@13515618(25227/31533 serialized/live bytes, 8 ops)
 INFO [FlushWriter:3] 2011-12-12 19:43:58,275 Memtable.java (line 277) Completed flushing /var/lib/cassandra/data/system/Schema-hc-138-Data.db (25377 bytes)
ERROR [MigrationStage:1] 2011-12-12 19:43:58,276 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:156)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:488)
        at org.apache.cassandra.db.DataTracker.removeAllSSTables(DataTracker.java:257)
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:267)
        at org.apache.cassandra.db.Table.unloadCf(Table.java:361)
        at org.apache.cassandra.db.Table.dropCf(Table.java:343)
        at org.apache.cassandra.db.migration.DropColumnFamily.applyModels(DropColumnFamily.java:87)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:156)
        at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:846)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [pool-2-thread-25] 2011-12-12 19:43:58,277 Cassandra.java (line 3878) Internal error processing system_drop_column_family
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at org.apache.cassandra.thrift.CassandraServer.applyMigrationOnStage(CassandraServer.java:861)
        at org.apache.cassandra.thrift.CassandraServer.system_drop_column_family(CassandraServer.java:903)
        at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family.process(Cassandra.java:3872)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.thrift.CassandraServer.applyMigrationOnStage(CassandraServer.java:853)
        ... 7 more
Caused by: java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:156)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:488)
        at org.apache.cassandra.db.DataTracker.removeAllSSTables(DataTracker.java:257)
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:267)
        at org.apache.cassandra.db.Table.unloadCf(Table.java:361)
        at org.apache.cassandra.db.Table.dropCf(Table.java:343)
        at org.apache.cassandra.db.migration.DropColumnFamily.applyModels(DropColumnFamily.java:87)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:156)
        at org.apache.cassandra.thrift.CassandraServer$2.call(CassandraServer.java:846)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
 INFO [MutationStage:62] 2011-12-12 19:43:59,004 NodeId.java (line 206) Saved local node id: 0d098030-24e6-11e1-0000-0282f5487fff
ERROR [ReadStage:60] 2011-12-12 19:44:00,322 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadStage:60,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown table/cf pair (CoreQA.Deal)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:60)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException: Unknown table/cf pair (CoreQA.Deal)
        at org.apache.cassandra.db.Table.getColumnFamilyStore(Table.java:159)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:48)
        ... 4 more
;;;","12/Dec/11 20:00;jbellis;Thanks for testing -- attached patch should fix that.;;;","12/Dec/11 20:01;jbellis;(Affects at least back to 1.0.3 with CASSANDRA-3399, possibly older.);;;","12/Dec/11 20:10;slebresne;+1;;;","12/Dec/11 20:22;asuffield-bossa;That fixed it. Thanks for quick response.;;;","12/Dec/11 20:30;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL inserting blank key.,CASSANDRA-3612,12534669,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,samal_,samal_,11/Dec/11 18:01,16/Apr/19 09:32,14/Jul/23 05:52,28/Mar/12 17:06,1.0.9,1.1.0,,Legacy/CQL,,,0,cql,,,"One of our application bug inserted blank key into cluster causing assertion error on key. After checking the root cause, I found it is the bug with CQL and reproducible. Client cassandra-node and cqlsh-1.0.6.
Blank key only work when one column provided.
 
{}
cqlsh> insert into login (KEY,email)values('','');
cqlsh> select * from login;
u'' | u'email',u'' 
cqlsh> insert into login (KEY,email,verified)values('','','');
Request did not complete within rpc_timeout.
cqlsh> insert into login (KEY,verified)values('','');
Request did not complete within rpc_timeout.
cqlsh> insert into login (KEY,email)values('','');
cqlsh> 
cqlsh> select * from login;
u'' | u'email',u'' | u'uid',None
cqlsh> select * from login;
u'' | u'email',u'' | u'uid',None
cqlsh> select * from login;
u'' | u'email',u'' | u'uid',None
cqlsh> 
cqlsh> select * from login;
u'' | u'email',u'' | u'uid',None
u'samalgorai@gmail.com' | u'email',u'samalgorai@gmail.com' | u'password',u'388ad1c312a488ee9e12998fe097f2258fa8d5ee' | u'uid',UUID('05ea41dc-241f-11e1-8521-3da59237b189') | u'verified',u'0'
cqlsh> quit;
{/}

http://pastebin.com/HJn5fHhH",Linux ubuntu 3.0.0-12-generic #20-Ubuntu SMP Fri Oct 7 14:56:25 UTC 2011 x86_64 x86_64 x86_64 GNU/Linux,skamio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220390,,,Wed Mar 28 17:06:08 UTC 2012,,,,,,,,,,"0|i0gm1r:",95002,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"11/Dec/11 19:01;brandon.williams;Adding the exception from pastebin for posterity.

{noformat}

s.  Time: 292ms.
ERROR 22:39:02,137 Fatal exception in thread Thread[MutationStage:1,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.ColumnFamilyStore.markReferenced(ColumnFamilyStore.java:1236)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:83)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:62)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1275)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1161)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1123)
	at org.apache.cassandra.db.Table.readCurrentIndexedColumns(Table.java:504)
	at org.apache.cassandra.db.Table.apply(Table.java:441)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:256)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:440)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1258)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR 22:42:02,786 Fatal exception in thread Thread[MutationStage:34,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.ColumnFamilyStore.markReferenced(ColumnFamilyStore.java:1236)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:83)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:62)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1275)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1161)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1123)
	at org.apache.cassandra.db.Table.readCurrentIndexedColumns(Table.java:504)
	at org.apache.cassandra.db.Table.apply(Table.java:441)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:256)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:440)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1258)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR 22:45:40,752 Fatal exception in thread Thread[MutationStage:36,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.ColumnFamilyStore.markReferenced(ColumnFamilyStore.java:1236)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:83)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:62)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1275)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1161)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1123)
	at org.apache.cassandra.db.Table.readCurrentIndexedColumns(Table.java:504)
	at org.apache.cassandra.db.Table.apply(Table.java:441)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:256)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:440)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1258)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR 22:46:02,674 Fatal exception in thread Thread[MutationStage:37,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.ColumnFamilyStore.markReferenced(ColumnFamilyStore.java:1236)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:83)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:62)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1275)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1161)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1123)
	at org.apache.cassandra.db.Table.readCurrentIndexedColumns(Table.java:504)
	at org.apache.cassandra.db.Table.apply(Table.java:441)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:256)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:440)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1258)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat};;;","23/Feb/12 23:01;jbellis;We shouldn't allow blank keys at all, so there's a validation failure in QueryProcessor here.  Unsure why markReferenced doesn't fail the assert for the first insert though.

(If the fix is involved let's push to 1.1.)
;;;","24/Feb/12 20:48;thepaul;Ok, finally reproduced.  This isn't because there is more than one column with the blank key, it's because your second column (""verified"") must have an index on it. It's the ""read mutated indexes"" code path here that invokes the assertion for non-empty keys. You'll also see a similar traceback when trying to DELETE the row with the empty key.

And yes, QueryProcessor is missing validation for these write operations here (INSERT, UPDATE, DELETE, and each of those inside BATCH statements (would take a different validation path). QueryProcessor does have validation for most everything else, though.;;;","24/Feb/12 22:43;thepaul;This particular problem is fixed in my github fork, in the 3612-1.0 and 3612-1.1 branches.

https://github.com/thepaul/cassandra/commit/5dc73e765f - 3612-1.0
https://github.com/thepaul/cassandra/commit/eab19b0979 - 3612-1.1

However, it looks like problems of a similar nature still exist in cql3, in the 1.1 series. I'll fix that and include those changes along with this ticket.;;;","27/Feb/12 17:43;thepaul;ok, 3612-1.1 branch updated (now at https://github.com/thepaul/cassandra/commit/d921a395ae).;;;","28/Mar/12 16:49;xedin;+1;;;","28/Mar/12 17:06;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad code in org.apache.cassandra.cql.QueryProcessor,CASSANDRA-3604,12534571,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,zolyfarkas,zolyfarkas,10/Dec/11 01:26,16/Apr/19 09:32,14/Jul/23 05:52,12/Dec/11 19:51,1.1.0,,,,,,0,,,,"line 206:
            if (rows.get(0).key.key.equals(startKey))
                rows.remove(0);

the equals will always return false because object of different types are compared",all,,,,,,,,,,,,,,,,,,,,,,300,300,,0%,300,300,,,,,,,,,,,,,,,,"12/Dec/11 18:26;slebresne;3604.patch;https://issues.apache.org/jira/secure/attachment/12507025/3604.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220293,,,Mon Dec 12 20:26:36 UTC 2011,,,,,,,,,,"0|i0glyf:",94987,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"10/Dec/11 01:28;zolyfarkas;similar issue also on line:214:
            if (rows.get(lastIndex).key.key.equals(finishKey))
                rows.remove(lastIndex);;;;","10/Dec/11 02:34;jbellis;All the objects in question are ByteBuffers.;;;","10/Dec/11 02:55;zolyfarkas;that is not correct:

     if (rows.get(0).key.key.equals(startKey))
                rows.remove(0);

rows is of type List<org.apache.cassandra.db.Row>

org.apache.cassandra.db.Row.key is of type DecoratedKey<?>

DecoratedKey<?>.key is of type ByteBuffer

startKey is of type RowPosition 

as such

ByteBuffer is compared with a RowPosition

can you please explain how startKey is of type ByteBuffer ?;;;","10/Dec/11 02:55;zolyfarkas;see my comment;;;","10/Dec/11 03:47;jbellis;""ByteBuffer startKey"", line 155;;;","10/Dec/11 04:19;zolyfarkas;OK, I believe we are looking at different versions I see the issue in:

trunk: Revision 1212726 last modified Dec 06

        ByteBuffer startKeyBytes = (select.getKeyStart() != null)
Line 155:                                   ? select.getKeyStart().getByteBuffer(keyType)
                                   : null;

        ByteBuffer finishKeyBytes = (select.getKeyFinish() != null)
                                    ? select.getKeyFinish().getByteBuffer(keyType)
                                    : null;

THe actual definition
        RowPosition startKey = RowPosition.forKey(startKeyBytes, p), finishKey = RowPosition.forKey(finishKeyBytes, p);
;;;","10/Dec/11 04:21;zolyfarkas;re-opened it agains release 1.1, I see this issue in trunk, se revision detail in my previous comment.;;;","12/Dec/11 18:26;slebresne;Good catch, patch attached.;;;","12/Dec/11 19:34;jbellis;+1;;;","12/Dec/11 19:51;slebresne;Committed, thanks;;;","12/Dec/11 20:26;hudson;Integrated in Cassandra #1253 (See [https://builds.apache.org/job/Cassandra/1253/])
    Fix typo introduced by #1034 in QueryProcessor
patch by slebresne; reviewed by jbellis for CASSANDRA-3604

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1213394
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/cql/QueryProcessor.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CounterColumn and CounterContext use a log4j logger instead of using slf4j like the rest of the code base,CASSANDRA-3603,12534529,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,09/Dec/11 19:29,16/Apr/19 09:32,14/Jul/23 05:52,17/Dec/11 04:52,1.0.7,,,,,,0,,,,"(Will submit patch but not now, no time.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/11 00:48;scode;CASSANDRA-3603-trunk.txt;https://issues.apache.org/jira/secure/attachment/12507748/CASSANDRA-3603-trunk.txt",,,,,,,,,,,,,,1.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220251,,,Sat Dec 24 03:12:51 UTC 2011,,,,,,,,,,"0|i0glxz:",94985,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"17/Dec/11 00:48;scode;Trivial patch applied.;;;","17/Dec/11 04:52;jbellis;re-organized imports to match coding style and committed;;;","24/Dec/11 03:12;scode;My apologies. Looks like I accidentally nuked projectCodeStyle.xml in the wc without realizing it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get_count NullPointerException with counters,CASSANDRA-3601,12534422,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,ghinkle,ghinkle,09/Dec/11 02:05,16/Apr/19 09:32,14/Jul/23 05:52,09/Dec/11 08:54,1.0.6,,,,,,0,counters,,,get_count doesn't currently work for counter columns or super counter columns. The fix seems to be pretty simple.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/11 02:08;ghinkle;trunk-3601.txt;https://issues.apache.org/jira/secure/attachment/12506696/trunk-3601.txt",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220144,,,Fri Dec 09 08:54:14 UTC 2011,,,,,,,,,,"0|i0glwv:",94980,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"09/Dec/11 02:08;ghinkle;Patch to handle counter_columns or super_counter_columns;;;","09/Dec/11 02:24;ghinkle;Incidentally, this is the stack trace for the problem.

ERROR 21:23:33,504 Internal error processing get_count
java.lang.NullPointerException
	at org.apache.cassandra.thrift.CassandraServer.get_count(CassandraServer.java:458)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_count.process(Cassandra.java:3075)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680);;;","09/Dec/11 08:54;slebresne;+1, committed. Thanks Greg.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index Scan's will span across multiple DC's,CASSANDRA-3598,12534412,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,08/Dec/11 22:47,16/Apr/19 09:32,14/Jul/23 05:52,09/Dec/11 18:02,0.7.4,0.8.9,1.0.6,,,,0,,,,"Looks like we send requests to all the nodes provided by StorageService.instance.getLiveNaturalEndpoints(keyspace, range.right);
We dont filter it based on blockedFor (Consistency levels).

In a multi DC setup this will cause unnecessary load on the other DC. And even within a DC we might query more nodes than needed.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/11 17:45;vijay2win@yahoo.com;0001-super-simple-patch-3598.patch;https://issues.apache.org/jira/secure/attachment/12506760/0001-super-simple-patch-3598.patch",,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220134,,,Fri Dec 09 18:56:24 UTC 2011,,,,,,,,,,"0|i0glvr:",94975,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"09/Dec/11 17:45;vijay2win@yahoo.com;one line change fot this ticket.;;;","09/Dec/11 18:02;jbellis;Also fixed range scans similarly.  Note that neither will have any effect unless read_repair_chance < 1.;;;","09/Dec/11 18:05;vijay2win@yahoo.com;Thanks! and we do...;;;","09/Dec/11 18:11;jbellis;(Checked 0.7 branch to follow up -- I fixed this there back in March, but screwed up the merge forward.);;;","09/Dec/11 18:56;hudson;Integrated in Cassandra-0.8 #414 (See [https://builds.apache.org/job/Cassandra-0.8/414/])
    range and index scans now only send requests to enough replicas to satisfy requested CL + RR
patch by Vijay and jbellis for CASSANDRA-3598

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1212552
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: DESCRIBE output for a columnfamily does not work as input to same C* instance,CASSANDRA-3596,12534401,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,08/Dec/11 21:07,16/Apr/19 09:32,14/Jul/23 05:52,08/Dec/11 21:55,1.0.6,,,Legacy/Tools,,,0,cqlsh,,,"The {{DESCRIBE COLUMNFAMILY}} cqlsh command produces output that is intended to be usable as valid CQL (at least, when given to another Cassandra instance of the same version). But the output yields errors when run:

{noformat}
cqlsh> USE blah;
cqlsh:blah> CREATE COLUMNFAMILY cf1 (c1 int PRIMARY KEY, c2 varchar);
cqlsh:blah> DESCRIBE COLUMNFAMILY cf1;

CREATE COLUMNFAMILY cf1 (
  c1 int PRIMARY KEY,
  c2 text
) WITH
  comment='' AND
  comparator=text AND
  row_cache_provider='ConcurrentLinkedHashCacheProvider' AND
  key_cache_size=200000.000000 AND
  row_cache_size=0.000000 AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  row_cache_save_period_in_seconds=0 AND
  key_cache_save_period_in_seconds=14400 AND
  replication_on_write=True;

cqlsh:blah> CREATE COLUMNFAMILY cf1 (
        ...   c1 int PRIMARY KEY,
        ...   c2 text
        ... ) WITH
        ...   comment='' AND
        ...   comparator=text AND
        ...   row_cache_provider='ConcurrentLinkedHashCacheProvider' AND
        ...   key_cache_size=200000.000000 AND
        ...   row_cache_size=0.000000 AND
        ...   read_repair_chance=0.100000 AND
        ...   gc_grace_seconds=864000 AND
        ...   default_validation=text AND
        ...   min_compaction_threshold=4 AND
        ...   max_compaction_threshold=32 AND
        ...   row_cache_save_period_in_seconds=0 AND
        ...   key_cache_save_period_in_seconds=14400 AND
        ...   replication_on_write=True;
Bad Request: replication_on_write is not a valid keyword argument for CREATE COLUMNFAMILY
{noformat}

So it needs to do a better job of determining which CF attributes are valid for which C* versions.",Cqlsh running against a recent Cassandra build,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/11 21:44;thepaul;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3596-spell-replicate_on_write-right.txt;https://issues.apache.org/jira/secure/attachment/12506661/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3596-spell-replicate_on_write-right.txt",,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,220123,,,Thu Dec 08 21:55:21 UTC 2011,,,,,,,,,,"0|i0glv3:",94972,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"08/Dec/11 21:23;jbellis;It looks like this is just a bug of describe spitting out a setting that CQL doesn't know how to handle (in any version).;;;","08/Dec/11 21:24;jbellis;... So the fix is to add support for replicate_on_write to CQL, not to band-aid it in cqlsh.;;;","08/Dec/11 21:38;thepaul;Yeah, actually, it looks like the fix is just to spell ""replicate_on_write"" correctly.;;;","08/Dec/11 21:55;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh: HELP for DELETE_USING and DELETE_COLUMNS broken,CASSANDRA-3588,12534260,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,07/Dec/11 21:59,16/Apr/19 09:32,14/Jul/23 05:52,08/Dec/11 16:30,1.0.6,,,Legacy/Tools,,,0,cqlsh,,,"type ""HELP DELETE_USING"" or ""HELP DELETE_COLUMNS"" in cqlsh. both of those are referred to by the HELP index, by tab-completion after ""HELP"", and in the help text for DELETE.

nothing is shown but a new command prompt.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/11 22:00;thepaul;0001-cqlsh-fix-HELP-for-DELETE_USING-DELETE_COLUMNS.txt;https://issues.apache.org/jira/secure/attachment/12506527/0001-cqlsh-fix-HELP-for-DELETE_USING-DELETE_COLUMNS.txt",,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219982,,,Thu Dec 08 16:30:16 UTC 2011,,,,,,,,,,"0|i0glrj:",94956,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"07/Dec/11 22:00;thepaul;add the missing 'print' calls for those two help functions.;;;","08/Dec/11 16:30;urandom;committed; thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check for 0.0.0.0 is incorrect,CASSANDRA-3584,12534221,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,07/Dec/11 17:36,16/Apr/19 09:32,14/Jul/23 05:52,07/Dec/11 20:55,1.0.6,,,,,,0,,,,"As noted by Jake in the comments to CASSANDRA-3214, we are using == for a String comparison.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/11 17:38;jbellis;3584.txt;https://issues.apache.org/jira/secure/attachment/12506487/3584.txt","07/Dec/11 18:46;tjake;3584_v2.txt;https://issues.apache.org/jira/secure/attachment/12506495/3584_v2.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219943,,,Wed Dec 07 20:55:05 UTC 2011,,,,,,,,,,"0|i0glpr:",94948,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"07/Dec/11 17:38;jbellis;fix attached;;;","07/Dec/11 18:46;tjake;Added similar check to getSubSplits otherwise 0.0.0.0 can not be used.;;;","07/Dec/11 19:24;jbellis;+1 after fixing whitespace in your operators :);;;","07/Dec/11 20:55;tjake;committed with space fix;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UserInterruptedException is poorly encapsulated,CASSANDRA-3582,12534138,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,07/Dec/11 05:05,16/Apr/19 09:32,14/Jul/23 05:52,08/Dec/11 20:33,1.1.0,,,,,,0,compaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/11 15:29;jbellis;3582-v2.txt;https://issues.apache.org/jira/secure/attachment/12506458/3582-v2.txt","07/Dec/11 05:06;jbellis;3582.txt;https://issues.apache.org/jira/secure/attachment/12506393/3582.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219860,,,Thu Dec 08 21:51:18 UTC 2011,,,,,,,,,,"0|i0glov:",94944,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"07/Dec/11 05:06;jbellis;patch renames UIE to CompactionInterruptedException and moves log special casing into CompactionExecutor;;;","07/Dec/11 15:29;jbellis;v2 extracts DebuggableThreadPoolExecutor.handleOrLog as well;;;","08/Dec/11 16:09;slebresne;Is there a reason for having CompactionExecutor extend TheadPoolExecutor instead of extending DebuggableThreadPoolExecutor and overriding handleOrLog (which would allow to have it protected, and have extractThrowable private)?
;;;","08/Dec/11 16:23;jbellis;CE doesn't need any of the other DTPE baggage (primarily the blocking submit), so it feels simpler to do a clean TPE extension and avoid having to compare to DTPE to tell ""what behavior exactly am I going to get from this?"";;;","08/Dec/11 17:14;slebresne;Feels to me that DPTE is a very thin layer on top of TPE and I've mostly been more confused by TPE default behavior than by what DPTE adds on top of that. And it also does feel to me that just overriding handleOrLog. But it's a nit. +1 in any case.;;;","08/Dec/11 20:33;jbellis;committed;;;","08/Dec/11 20:34;jbellis;(logExceptionsAfterExecute and the methods it calls need to be static since we use them from DSTPE too, which doesn't and can't share a common ancestor);;;","08/Dec/11 21:51;hudson;Integrated in Cassandra #1249 (See [https://builds.apache.org/job/Cassandra/1249/])
    improve UserInterruptedException encapsulation (and renamed to CompactionInterruptedException)
patch by jbellis; reviewed by slebresne for CASSANDRA-3582

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1212092
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionInterruptedException.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionTask.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/UserInterruptedException.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/SecondaryIndexBuilder.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't assume the Table instance has been open when dropping a keyspace,CASSANDRA-3580,12534054,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,06/Dec/11 17:03,16/Apr/19 09:32,14/Jul/23 05:52,10/Dec/11 19:17,0.8.9,1.0.6,,,,,0,,,,"DropKeyspace assumes that the Table had been open (rather, it checks that Table.clear() don't return null). It has been seen however that in the case of a fat client (the sstableloader in that case, but it would be true for any fat client) the Table may not have been open. Let's just remove that assertion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/11 17:05;slebresne;0001-Super-awesome-fix.patch;https://issues.apache.org/jira/secure/attachment/12506279/0001-Super-awesome-fix.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219780,,,Sat Dec 10 20:22:24 UTC 2011,,,,,,,,,,"0|i0glnr:",94939,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"10/Dec/11 19:17;jbellis;committed;;;","10/Dec/11 20:22;hudson;Integrated in Cassandra-0.8 #417 (See [https://builds.apache.org/job/Cassandra-0.8/417/])
    remove invalid assertion that table was opened before dropping it
patch by slebresne; reviewed by jbellis for CASSANDRA-3580

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1212849
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/DropKeyspace.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in hintedhandoff - 1.0.5,CASSANDRA-3579,12534022,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,ramesh25,ramesh25,06/Dec/11 13:11,16/Apr/19 09:32,14/Jul/23 05:52,11/Jan/12 19:53,1.0.7,,,,,,0,,,,"We are running a 8 node cassandra cluster running cassandra 1.0.5.
All our CF use leveled compaction.  We ran a test where we did a lot
of inserts for 3 days. After that we started to run tests where some
of the reads could ask for information that was inserted a while back.
In this scenario we are seeing this assertion error in HintedHandoff.

ERROR [HintedHandoff:3] 2011-12-05 15:42:04,324
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[HintedHandoff:3,1,main]
java.lang.RuntimeException: java.lang.RuntimeException:
java.util.concurrent.ExecutionException: java.lang.AssertionError:
originally calculated column size of 470937164 but now it is 470294247
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException:
java.util.concurrent.ExecutionException: java.lang.AssertionError:
originally calculated column size of 470937164 but now it is 470294247
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:330)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:353)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
Caused by: java.util.concurrent.ExecutionException:
java.lang.AssertionError: originally calculated column size of
470937164 but now it is 470294247
       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
       at java.util.concurrent.FutureTask.get(FutureTask.java:83)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:326)
       ... 6 more
Caused by: java.lang.AssertionError: originally calculated column size
of 470937164 but now it is 470294247
       at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
       at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
       at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:158)
       at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       ... 3 more
ERROR [HintedHandoff:3] 2011-12-05 15:42:04,333
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[HintedHandoff:3,1,main]
java.lang.RuntimeException: java.lang.RuntimeException:
java.util.concurrent.ExecutionException: java.lang.AssertionError:
originally calculated column size of 470937164 but now it is 470294247
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException:
java.util.concurrent.ExecutionException: java.lang.AssertionError:
originally calculated column size of 470937164 but now it is 470294247
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:330)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:353)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
Caused by: java.util.concurrent.ExecutionException:
java.lang.AssertionError: originally calculated column size of
470937164 but now it is 470294247
       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
       at java.util.concurrent.FutureTask.get(FutureTask.java:83)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:326)
       ... 6 more
Caused by: java.lang.AssertionError: originally calculated column size
of 470937164 but now it is 470294247
       at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
       at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
       at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:158)
       at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       ... 3 more
ERROR [CompactionExecutor:9931] 2011-12-05 15:42:04,333
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[CompactionExecutor:9931,1,main]
java.lang.AssertionError: originally calculated column size of
470937164 but now it is 470294247
       at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
       at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
       at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:158)
       at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)","RHEL 6.1 64 bit, 32 GB RAM, 8 GB allocated to JVM, running XFS filesystem for commit/data directories",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/12 19:41;jbellis;3579-fix-text.txt;https://issues.apache.org/jira/secure/attachment/12510227/3579-fix-text.txt","06/Jan/12 22:39;jbellis;3579-v2.txt;https://issues.apache.org/jira/secure/attachment/12509729/3579-v2.txt","04/Jan/12 11:03;slebresne;3579.patch;https://issues.apache.org/jira/secure/attachment/12509396/3579.patch",,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219748,,,Wed Jan 11 19:53:32 UTC 2012,,,,,,,,,,"0|i0glnb:",94937,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"07/Dec/11 01:31;maedhroz;FWIW, I've seen the same error when bringing a node back up after a brief (5 minute) down-time.

ERROR [HintedHandoff:3] 2011-12-06 16:42:58,822 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:3,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:301)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:353)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722);;;","07/Dec/11 02:31;jbellis;Caleb, your assertion means you had some hint corruption from 1.0.0 (CASSANDRA-3466).  You should remove your Hint column families from the system/ keyspace.;;;","08/Dec/11 16:44;jbellis;Ramesh, are you using counters?  If so, this may be the same as CASSANDRA-3481.;;;","08/Dec/11 20:37;ramesh25;No we are not using counters.;;;","20/Dec/11 17:30;cumarana;I ran into this problem as well. I don't know if it is related, but I started to see Gossip errors for the node being down and up constantly even when I'm not writing to Cassandra. Restarting Cassandra fixed the problem.


INFO [CompactionExecutor:648] 2011-12-19 20:41:17,399 CompactionController.java (line 133) Compacting large row system/HintsColumnFamily:
77777777777777777777777777777770 (73427721 bytes) incrementally  INFO [FlushWriter:99] 2011-12-19 20:41:17,410 Memtable.java (line 275) Completed flushing /data/data/system/HintsColumnFamily-hb-141-Data
.db (3022 bytes)
ERROR [CompactionExecutor:648] 2011-12-19 20:41:19,445 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Compaction Executor:648,1,main]
java.lang.AssertionError: originally calculated column size of 66102951 but now it is 66009419
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:158)
        at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [HintedHandoff:1] 2011-12-19 20:41:19,445 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:1,1 ,main]
java.lang.RuntimeException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calc ulated column size of 66102951 but now it is 66009419
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column siz e of 66102951 but now it is 66009419
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:330)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
    at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:353)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: originally calculated column size of 66102951 but now it is66009419
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:326)
        ... 6 more
Caused by: java.lang.AssertionError: originally calculated column size of 66102951 but now it is 66009419
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:158)
        at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more

;;;","28/Dec/11 18:16;jbellis;The fact that all these stack traces show that the row size got *smaller*, and that HH sets a time to live on its hints starting with CASSANDRA-2034, makes me suspect that somehow columns are expiring in between the first and second LCR passes.  But I don't see anything obviously wrong with how we are using controller.gcBefore to theoretically prevent that.;;;","04/Jan/12 11:03;slebresne;I believe I know what is going on. This is bug with CF having gc_grace == 0 (which hints have). The problem lies in the following line of removeDeleted:
{noformat}
if ((c.isMarkedForDelete() && c.getLocalDeletionTime() <= gcBefore)
    || c.timestamp() <= cf.getMarkedForDeleteAt())
{
    iter.remove();
}
{noformat}
and more precisely the first condition. When that is executed, we have gcbefore <= now but we *can* have gcbefore == now if gc_grace == 0 (and since the resolution is the second, it's not even a very unlikely race).

Now for expiring columns it is further possible that localExpirationTime == gcbefore == now. When that happens, c.isMarkedForDelete() will return false (thus the column will be kept) because this method is defined as:
{noformat}
public boolean isMarkedForDelete()
{
    return (int) (System.currentTimeMillis() / 1000 ) > localExpirationTime;
}
{noformat}

However, during the second pass, now has changed and it is possible that we have now > gcbefore. But since gcbefore == localExpirationTime, this means that isMarkedForDelete() will be true *and* getLocalDeletionTime() <= gcbefore, so the column will be considered tombstone and gcable.

In other word, the current code does not respect the condition that at all time a column is considered gcable only if it is considered deleted.

A rather simple fix consist in changing the condition in removeDeleted to be
{noformat}
if ((c.isMarkedForDelete() && c.getLocalDeletionTime() < gcBefore)
    || c.timestamp() <= cf.getMarkedForDeleteAt())
{
    iter.remove();
}
{noformat}
Note the strict lesser than operator for the first condition. It fixes it because since we know that we always have gcbefore <= now, localExpirationTime < gcBefore always imply that isMarkedForDelete() is true.

Attached patch to make that fix (with hopefully useful comments).
;;;","05/Jan/12 23:30;jbellis;I think this change is an improvement, and I also think it's a good thing to make the semantics of gcBefore match what its name implies, i.e., we collect tombstones from *before* that epoch and not before-or-equal-to, but:

- It's slightly more intuitive for 0 gcgs to mean ""this is always gc'd immediately,"" not ""gc'd after one second.""  In practice, I can't see how this could matter, but it does bother my OCD. :)
- A search for < gcBefore, <= gcBefore, > gcBefore, >= gcBefore shows some of each.  We're not consistent in whether we treat it as before or before-or-equal-to, and we need to be.
- The isMarkedForDelete behavior you point out scares me, because that's going to change during the two LCR passes as well.  I suspect that's going to cause subtle bugs if we don't create a fixed point in time for which we treat a compaction as happening.  (Maybe just replace controller.gcBefore with controller.compactionTime and derive gcBefore from that as a method, instead of a field.);;;","05/Jan/12 23:32;jbellis;We have the same problem in 0.7 and 0.8 for any CF with 0 gcgs.  For those versions IMO we should just say ""don't do that.""
;;;","06/Jan/12 08:57;slebresne;bq. It's slightly more intuitive for 0 gcgs to mean ""this is always gc'd immediately,""

True, though gc is more a an internal detail in that it doesn't affect whether a column is returned by queries or not. It only affect whether it will be removed by a compaction, but that is already so dependent of other timings than it cannot matter. Or to put it otherwise, that one doesn't bother my own OCD. But it doesn't really bother me either if gcBefore means before-or-equal so ...

bq. A search for < gcBefore, <= gcBefore, > gcBefore, >= gcBefore shows some of each.

One of the goal of this patch was to actually adds consistency and I though I had eliminated all '<=' but maybe I missed one. I forgot to search for '>' and '>=' however. I'll fix.

bq. The isMarkedForDelete behavior you point out scares me, because that's going to change during the two LCR passes as well. I suspect that's going to cause subtle bugs if we don't create a fixed point in time for which we treat a compaction as happening. (Maybe just replace controller.gcBefore with controller.compactionTime and derive gcBefore from that as a method, instead of a field.)

It's an option, and it was even my first idea. But it means that isMarkedForDelete will take a timestamp, and it's called a lot in the code base. That and the other related change, I'd be tempted to commit the patch as is for 1.0 and maybe do the switch to 'compactionTime' for 1.1 onward.
;;;","06/Jan/12 22:30;jbellis;bq. I'd be tempted to commit the patch as is for 1.0 and maybe do the switch to 'compactionTime' for 1.1 onward

All right, for 1.0 let's just audit for > >= gcBefore correctness.

bq. isMarkedForDelete will take a timestamp, and it's called a lot in the code base

The more I think about it the less I like ""boolean isMarkedForDelete()""...  I like ""getExpirationTime()"" a lot better.  That allows combining iMFD and getLDT into a single method (that can return Integer.MAX_VALUE for Column) that doesn't need any parameters, but doesn't change value during compaction.;;;","06/Jan/12 22:39;jbellis;v2 changes two >= instances to >.;;;","09/Jan/12 16:58;slebresne;+1 on v2, committed on 1.0 branch. How do we merge --record-only with git again?

bq. The more I think about it the less I like ""boolean isMarkedForDelete()""... I like ""getExpirationTime()"" a lot better.

Agreed, that's much cleaner. I'll do that for 1.1.;;;","09/Jan/12 17:12;jbellis;bq. How do we merge --record-only with git again

The closest I found is {{git merge X --strategy=ours}}.;;;","10/Jan/12 17:37;jbellis;Created CASSANDRA-3716 for the 1.1 followup;;;","11/Jan/12 19:09;jbellis;this is causing the SystemTableTest failure mentioned in CASSANDRA-3727;;;","11/Jan/12 19:41;jbellis;Switching from <= to < means that getColumnFamily will now include tombstones for just-deleted column with gcgs == 0.

For clients this is harmless (since we drop tombstones in the coordinator after RR), but internal code needs to be more careful.  patch attached to have loadTokens expunge all tombstones.;;;","11/Jan/12 19:48;slebresne;+1;;;","11/Jan/12 19:53;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TimeoutException When using QuorumEach or ALL consistency on Multi-DC,CASSANDRA-3577,12533983,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,06/Dec/11 03:30,16/Apr/19 09:32,14/Jul/23 05:52,08/Dec/11 20:30,0.8.9,1.0.6,1.1.0,,,,0,,,,"Currently we have 
1) StorageProxy.sendMessages() sending messages to the first node in the other DC...  
2) A node in the other DC will remove the ForwardHeader and sendRR (Adding a MessageID to the Queue).
3) The receiving node receives the mutation, updates and sends the response to the Original Co-ordinator.
4) Co-Ordinator now checks for the MessageID (which it never had)

All the Quorum_Each updates fail in the co-ordinator, this issue started showing up after CASSANDRA-3472 the code was introduced in CASSANDRA-2138 .

Simple Fix is to remove the optimization in 0.8 and fix it in 1.x because it seems to me like it needs a change to the Message service version.

Possible Solution: We might want send the message ID's to be used by the all the nodes in other DC (Which is currently generated by the node which receives the Forward request see: (2) ).",JVM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/11 18:06;vijay2win@yahoo.com;0001-Mutation-Optimization-for-MultiDC-v2.patch;https://issues.apache.org/jira/secure/attachment/12506491/0001-Mutation-Optimization-for-MultiDC-v2.patch","07/Dec/11 08:20;vijay2win@yahoo.com;0001-Mutation-Optimization-for-MultiDC.patch;https://issues.apache.org/jira/secure/attachment/12506416/0001-Mutation-Optimization-for-MultiDC.patch","06/Dec/11 06:35;vijay2win@yahoo.com;0001-removing-mutation-MultiDC-optimization.patch;https://issues.apache.org/jira/secure/attachment/12506219/0001-removing-mutation-MultiDC-optimization.patch","08/Dec/11 17:35;jbellis;3577-v3.txt;https://issues.apache.org/jira/secure/attachment/12506620/3577-v3.txt","06/Dec/11 03:42;jbellis;3577.txt;https://issues.apache.org/jira/secure/attachment/12506214/3577.txt",,,,,,,,,,5.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219709,,,Sat Dec 10 15:51:36 UTC 2011,,,,,,,,,,"0|i0glmf:",94933,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"06/Dec/11 03:42;jbellis;All we need to do is forward with the original id, no?  Patch attached to do that.;;;","06/Dec/11 03:42;jbellis;(Patch is against 0.8.);;;","06/Dec/11 03:48;vijay2win@yahoo.com;But when the Co-Ordinator receives the response with the message ID the message is already removed because ResponseVerbHandler does
MessagingService.instance().removeRegisteredCallback(id);
We wont have the ID there.



;;;","06/Dec/11 04:25;jbellis;You're right, we switched to using unique message IDs per target in CASSANDRA-2058 so that we can track timeouts for the dynamic snitch, so my patch won't work.

I agree that pre-generating extra IDs on the coordinator is the easiest fix, and also that we should just disable this behavior in 0.8 (which was the case until CASSANDRA-3472 anyway).;;;","06/Dec/11 06:35;vijay2win@yahoo.com;removing mutation optimization for .8, i will work on the update to 1.1 shortly. Thanks!;;;","06/Dec/11 13:28;jbellis;committed .8 patch w/ comment pointing to this issue;;;","06/Dec/11 14:23;hudson;Integrated in Cassandra-0.8 #409 (See [https://builds.apache.org/job/Cassandra-0.8/409/])
    remove nonlocal DC write optimization
patch by Vijay; reviewed by jbellis for CASSANDRA-3577

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1210902
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
;;;","07/Dec/11 08:18;vijay2win@yahoo.com;Testing took some additional time, This patch is on 1.1 with an updated MessagingService.version to handle both older version and new version mutations.;;;","07/Dec/11 15:23;jbellis;Patch doesn't apply to latest trunk for me, can you rebase?;;;","07/Dec/11 18:06;vijay2win@yahoo.com;Sorry, Rebased to the the trunk. Thanks!;;;","08/Dec/11 17:35;jbellis;v3 attached.  Some cleanup of StorageProxy, switches to FastBAIS, and does a version check on the receiving side as well as the sending (since we do have released versions in the wild sending out ""bad"" FORWARD_HEADERs).;;;","08/Dec/11 18:49;vijay2win@yahoo.com;+1 Thanks!;;;","08/Dec/11 20:30;jbellis;committed;;;","08/Dec/11 21:51;hudson;Integrated in Cassandra #1249 (See [https://builds.apache.org/job/Cassandra/1249/])
    multi-dc replication optimization supporting CL > ONE
patch by Vijay and jbellis for CASSANDRA-3577

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1212088
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/RowMutationVerbHandler.java
* /cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
;;;","10/Dec/11 15:51;jbellis;This can actually cause the more subtle problem of CASSANDRA-3585: Node A (DC1) sends a write to node B (DC2), which forwards to node C (DC2).  Node C replies to node A with the message ID it received from node B.  If the message generation on A and B is far enough apart, then A will not have a callback for the reply and all you will see happen is the write timeout (at CL > ONE).  But if A *does* have a callback (for a different operation) waiting, then A will try to apply the mutation response to that callback, which (if the callback is for a read) will result in the error see in that ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in DK,CASSANDRA-3574,12533950,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,05/Dec/11 22:42,16/Apr/19 09:32,14/Jul/23 05:52,06/Dec/11 17:08,1.1.0,,,,,,0,,,,"When running the dtests:

{noformat}
ERROR [pool-2-thread-1] 2011-12-05 16:22:53,940 Cassandra.java (line 4082) Internal error processing execute_cql_query
java.lang.AssertionError
        at org.apache.cassandra.db.DecoratedKey.<init>(DecoratedKey.java:56)
        at org.apache.cassandra.dht.RandomPartitioner.decorateKey(RandomPartitioner.java:47)
        at org.apache.cassandra.cql.QueryProcessor.multiRangeSlice(QueryProcessor.java:161)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:549)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1249)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

I suspect CASSANDRA-1034 is to blame.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/11 15:26;slebresne;3574.patch;https://issues.apache.org/jira/secure/attachment/12506263/3574.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219676,,,Tue Dec 06 18:25:10 UTC 2011,,,,,,,,,,"0|i0gllb:",94928,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"06/Dec/11 15:26;slebresne;I though I had fixed that already but apparently not. Patch attached.;;;","06/Dec/11 16:08;jbellis;+1;;;","06/Dec/11 17:08;slebresne;Committed, thanks;;;","06/Dec/11 18:25;hudson;Integrated in Cassandra #1240 (See [https://builds.apache.org/job/Cassandra/1240/])
    Fix assertion error in DK
patch by slebresne; reviewed by jbellis for CASSANDRA-3574

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211030
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/cql/QueryProcessor.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When Snappy compression is not available on the platform, trying to enable it introduces problems",CASSANDRA-3573,12533891,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,tivv,tivv,05/Dec/11 15:20,16/Apr/19 09:32,14/Jul/23 05:52,05/Dec/11 20:30,1.0.6,,,,,,0,,,,"I've tried to enable compression for some column families in my cluster using Snappy compression.
It does not work and I am having problems with schema updates to remove it (a lot of UNREACHABLE nodes during scema update).

In log I have the next:


ERROR [FlushWriter:961] 2011-12-05 17:16:33,383 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Flu
shWriter:961,5,main]
java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy
        at org.apache.cassandra.io.compress.SnappyCompressor.initialCompressedBufferLength(SnappyCompressor.java:39)
        at org.apache.cassandra.io.compress.CompressedSequentialWriter.<init>(CompressedSequentialWriter.java:63)
        at org.apache.cassandra.io.compress.CompressedSequentialWriter.open(CompressedSequentialWriter.java:34)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:91)
        at org.apache.cassandra.db.ColumnFamilyStore.createFlushWriter(ColumnFamilyStore.java:1850)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:250)
        at org.apache.cassandra.db.Memtable.access$400(Memtable.java:47)
        at org.apache.cassandra.db.Memtable$4.runMayThrow(Memtable.java:291)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)

It looks like Snappy can't initialize because it does not have native library for my platform. It would be great if:
1) A check be done on schema update if Snappy can be used
2) If it is enabled and can't be used it would still work without compression writes (but may be outputting some errors to indicate the situation)
 ","FreeBSD
",tivv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/11 19:34;xedin;CASSANDRA-3573.patch;https://issues.apache.org/jira/secure/attachment/12506148/CASSANDRA-3573.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219617,,,Mon Dec 05 20:30:01 UTC 2011,,,,,,,,,,"0|i0glkn:",94925,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"05/Dec/11 15:40;tjake;If you build snappy-java directly on those machines you can tell snappy to use that library with the following:

cassandra -Djava.library.path=(path to the installed snappyjava lib) -Dorg.xerial.snappy.use.systemlib=true;;;","05/Dec/11 16:24;tivv;In case when someone get int othis trap, I could replace Snappy compression class with NOOP one and restart: 

public class SnappyCompressor implements ICompressor
{
    public static final SnappyCompressor instance = new SnappyCompressor();

    public static SnappyCompressor create(Map<String, String> compressionOptions)
    {
        // no specific options supported so far
        return instance;
    }

    public int initialCompressedBufferLength(int chunkLength)
    {
        return chunkLength;
    }

    public int compress(byte[] input, int inputOffset, int inputLength, ICompressor.WrappedArray output, int outputOffset) throws IOException
    {
        System.arraycopy(input, inputOffset, output.buffer, outputOffset, inputLength);
        return inputLength;
    }

    public int uncompress(byte[] input, int inputOffset, int inputLength, byte[] output, int outputOffset) throws IOException
    {
        System.arraycopy(input, inputOffset, output, outputOffset, inputLength);
        return inputLength;
    }
}
;;;","05/Dec/11 17:01;jbellis;Sounds like we should try to instantiate the compressor during validateCfDef to make sure it's not going to cause problems later.;;;","05/Dec/11 18:21;tivv;I'm afraid the patch won't help because Snappy compressor don't initialize snappy on init. This must be changed too.;;;","05/Dec/11 18:25;xedin;you are right, I will add validation to SnappyCompressor create method to ensure that Snappy lib is in place.;;;","05/Dec/11 20:06;brandon.williams;We probably need to bump the thrift rev too, but +1 otherwise.;;;","05/Dec/11 20:08;brandon.williams;bq. We probably need to bump the thrift rev too

Nevermind.;;;","05/Dec/11 20:30;xedin;Committed with grammar change thrown/threw.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cache saving broken on windows,CASSANDRA-3566,12533701,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,jbellis,jbellis,02/Dec/11 23:26,16/Apr/19 09:32,14/Jul/23 05:52,06/Dec/11 20:52,1.1.0,,,,,,0,windows,,,CASSANDRA-1740 broke cache saving on Windows.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/11 16:22;slebresne;3566.patch;https://issues.apache.org/jira/secure/attachment/12506274/3566.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219428,,,Tue Dec 06 20:52:16 UTC 2011,,,,,,,,,,"0|i0glhj:",94911,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"02/Dec/11 23:26;jbellis;All the cache tests fail with an exception like this:

{noformat}
    [junit] Testcase: testKeyCacheLoad(org.apache.cassandra.db.KeyCacheTest):   Caused an ERROR
    [junit] java.lang.RuntimeException: java.io.IOException: Unable to rename build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache7847563715233372577.tmp to build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache
    [junit] java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.IOException: Unable to rename build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache7847563715233372577.tmp to build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache
    [junit]     at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    [junit]     at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    [junit]     at org.apache.cassandra.db.KeyCacheTest.testKeyCacheLoad(KeyCacheTest.java:87)
    [junit] Caused by: java.lang.RuntimeException: java.io.IOException: Unable to rename build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache7847563715233372577.tmp to build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache
    [junit]     at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:37)
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit]     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit]     at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit]     at java.lang.Thread.run(Thread.java:662)
    [junit] Caused by: java.io.IOException: Unable to rename build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache7847563715233372577.tmp to build\test\cassandra\saved_caches\KeyCacheSpace-Standard3-KeyCache
    [junit]     at org.apache.cassandra.cache.AutoSavingCache$Writer.saveCache(AutoSavingCache.java:265)
    [junit]     at org.apache.cassandra.db.compaction.CompactionManager$10.runMayThrow(CompactionManager.java:907)
    [junit]     at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.db.KeyCacheTest FAILED
{noformat}
;;;","02/Dec/11 23:27;jbellis;I'd be totally fine with just ripping the cancellation code out of cache saving.  Cache saving will be very fast compared to most actual compactions.;;;","06/Dec/11 16:22;slebresne;I'm totally with that too. Patch attach to rip that part.;;;","06/Dec/11 20:52;jbellis;+1 (went ahead and committed since this has been bugging me);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL CF creation skips most of the validation code,CASSANDRA-3565,12533699,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,02/Dec/11 23:08,16/Apr/19 09:32,14/Jul/23 05:52,02/Dec/11 23:42,1.0.6,,,Legacy/CQL,,,0,cql,,,"Most validation is done by ThriftValidation.validateCfDef, which we call from QP when creating an index but not on CF creation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/11 23:09;jbellis;3565.txt;https://issues.apache.org/jira/secure/attachment/12505941/3565.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219426,,,Fri Dec 02 23:42:45 UTC 2011,,,,,,,,,,"0|i0glh3:",94909,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"02/Dec/11 23:09;jbellis;patch adds validateCfDef to QP CF creation.;;;","02/Dec/11 23:42;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Packaging should increase vm.max_map_count to accommodate leveled compaction,CASSANDRA-3563,12533686,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,02/Dec/11 20:54,16/Apr/19 09:32,14/Jul/23 05:52,08/Dec/11 21:31,1.0.6,,,,,,0,,,,"As the title says, leveled can create a lot of files and you can run into an IOError trying to mmap all of them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/11 21:40;brandon.williams;0001-increase-debian-limit.txt;https://issues.apache.org/jira/secure/attachment/12505934/0001-increase-debian-limit.txt","07/Dec/11 21:44;thepaul;0002-fix-sysctl.d-installation.txt;https://issues.apache.org/jira/secure/attachment/12506522/0002-fix-sysctl.d-installation.txt",,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219413,,,Thu Dec 08 21:31:29 UTC 2011,,,,,,,,,,"0|i0glg7:",94905,,thepaul,,thepaul,Normal,,,,,,,,,,,,,,,,,"07/Dec/11 21:44;thepaul;This won't work as is; dh_install doesn't support renaming a file while putting it into a directory (it will make a directory named /etc/sysctl.d/cassandra.conf/ and put cassandra-sysctl.conf in it).

Attachment applies on top of original patch, does file rename in debian/rules.

+1 with this fix;;;","07/Dec/11 21:59;brandon.williams;Committed.;;;","08/Dec/11 21:23;thepaul;Looks like debian/cassandra-sysctl.conf didn't actually get committed.;;;","08/Dec/11 21:31;brandon.williams;You're right.  Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compression chunk_length_kb is not correctly returned for thrift/avro,CASSANDRA-3558,12533603,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,02/Dec/11 11:59,16/Apr/19 09:32,14/Jul/23 05:52,06/Dec/11 12:31,1.0.6,,,,,,0,compression,,,"CASSANDRA-3492 fixed the interpretation of chunk_length_kb as a size in bytes but infortunately forgot to convert it back to kb when returning it for thrift/avro. In particular, this means that a {{describe cf}} would return things like {{chunk_length_kb: 65535}}.

I'm afraid that because migration uses Avro this is kind of a problem. One may have to issue an 'update column family' with the right chunk_length_kb to be sure to be in a safe place.",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/11 12:16;slebresne;0001-Correctly-handle-chunk_length_in_kb.patch;https://issues.apache.org/jira/secure/attachment/12505878/0001-Correctly-handle-chunk_length_in_kb.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219331,,,Tue Dec 06 12:31:40 UTC 2011,,,,,,,,,,"0|i0gldz:",94895,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"02/Dec/11 12:16;slebresne;Fix attached. I've added a test for this in the distributed tests (at https://github.com/riptano/cassandra-dtest).

We could have a unit test for this but without CASSANDRA-3559, it's more pain that it should be.;;;","06/Dec/11 12:31;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commit Log segments are not recycled,CASSANDRA-3557,12533585,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,rbranson,rbranson,rbranson,02/Dec/11 09:19,16/Apr/19 09:32,14/Jul/23 05:52,02/Dec/11 23:30,1.1.0,,,,,,0,,,,Cassandra never recycles segments created after recovery.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/11 15:57;rbranson;3557.txt;https://issues.apache.org/jira/secure/attachment/12505900/3557.txt",,,,,,,,,,,,,,1.0,rbranson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219313,,,Sat Dec 03 01:06:09 UTC 2011,,,,,,,,,,"0|i0gldj:",94893,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"02/Dec/11 22:38;jbellis;I'm still not a fan of creating a new Segment object on recycle.  Feels like in this case it makes things more fragile rather than less.  Otherwise lgtm.;;;","02/Dec/11 23:21;rbranson;There are a few reasons:

* There is some ""reset"" logic that would need to be duplicated from the constructor into the recycle() method. Resetting this mutable state seems fragile vs just discarding immutable state.
* Current code from the constructor for existing files works for both the case of recycling recovered segments and recycling discarded segments. Moving this out means we'd have to split this logic and duplicate it.
* Naming/renaming logic would have to be reworked and likely duplicated in some places, and the segment MUST be renamed for several reasons:
** ReplayPosition objects reference CommitLogSegment's by their id, which is stored in the filename
** CommitLog.recover explicitly replays segments in order of their id, and not giving them a fresh id would break this contract.
* If we re-use CommitLogSegment objects, leaked references could unintentionally operate on the wrong segment file. A CLS object could reference one segment at one point in time, and another at a future point in time. This seems dangerous to me. The patch as it sits rejects any mutations after the segment is closed and any modifications to the ""dirty"" table are meaningless.;;;","02/Dec/11 23:30;jbellis;makes sense, committed;;;","03/Dec/11 01:06;hudson;Integrated in Cassandra #1235 (See [https://builds.apache.org/job/Cassandra/1235/])
    fix commitlog segment recycling
patch by Rick Branson; reviewed by jbellis for CASSANDRA-3557

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1209779
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogAllocator.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogSegment.java
* /cassandra/trunk/test/unit/org/apache/cassandra/SchemaLoader.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool info reports inaccurate datacenter/rack for localhost,CASSANDRA-3556,12533581,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,rbranson,rbranson,rbranson,02/Dec/11 08:10,16/Apr/19 09:32,14/Jul/23 05:52,02/Dec/11 18:10,0.8.9,1.0.6,,Tool/nodetool,,,0,,,,The datacenter & rack information provided by 'nodetool info' is incorrect when using 'nodetool -h localhost info'. This is because the IP address passed to the EndpointSnitch to determine the datacenter & rack is sourced from the host parameter provided to nodetool and not the actual endpoint address used in the ring.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/11 16:03;rbranson;3556-v2.txt;https://issues.apache.org/jira/secure/attachment/12505902/3556-v2.txt","02/Dec/11 08:12;rbranson;3556.txt;https://issues.apache.org/jira/secure/attachment/12505857/3556.txt",,,,,,,,,,,,,2.0,rbranson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219309,,,Fri Dec 02 18:22:46 UTC 2011,,,,,,,,,,"0|i0gld3:",94891,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"02/Dec/11 08:13;rbranson;Attached patch resolves the issue by looking up the local endpoint's ""real"" hostname by token.;;;","02/Dec/11 14:09;jbellis;""fall back if we can't find it for some reason"" is shouldn't-ever-happen territory, right?  Would prefer to throw assertionerror, if so.;;;","02/Dec/11 16:03;rbranson;betta? :);;;","02/Dec/11 18:10;jbellis;committed;;;","02/Dec/11 18:22;hudson;Integrated in Cassandra-0.8 #408 (See [https://builds.apache.org/job/Cassandra-0.8/408/])
    use cannonical host for local node in nodetool info
patch by Rick Branson; reviewed by jbellis for CASSANDRA-3556

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1209608
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/NodeProbe.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hints are not replayed unless node was marked down,CASSANDRA-3554,12533568,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,02/Dec/11 05:46,16/Apr/19 09:32,14/Jul/23 05:52,22/Dec/11 21:34,1.0.7,,,,,,1,hintedhandoff,jmx,,"If B drops a write from A because it is overwhelmed (but not dead), A will hint the write.  But it will never get notified that B is back up (since it was never down), so it will never attempt hint delivery.",,brandon.williams,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Dec/11 04:53;jbellis;0001-cleanup.patch;https://issues.apache.org/jira/secure/attachment/12507477/0001-cleanup.patch","15/Dec/11 04:47;jbellis;0002-deliver.patch;https://issues.apache.org/jira/secure/attachment/12507476/0002-deliver.patch","22/Dec/11 20:29;jbellis;3554-1.0-v2.txt;https://issues.apache.org/jira/secure/attachment/12508433/3554-1.0-v2.txt","19/Dec/11 23:34;jbellis;3554-1.0.txt;https://issues.apache.org/jira/secure/attachment/12507999/3554-1.0.txt",,,,,,,,,,,4.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219296,,,Wed Mar 21 11:59:05 UTC 2012,,,,,,,,,,"0|i0glc7:",94887,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"02/Dec/11 05:48;jbellis;Unclear how we should tell when it's a good idea to re-attempt delivery in this scenario.

Possibly the best solution is to just make FD smarter and mark nodes as ""effectively down"" in this situation.  A ""local"" FD as in CASSANDRA-3533 could address this.;;;","02/Dec/11 08:40;slebresne;Couldn't B handles this? When it drops writes from A, it could record it. Then B could have a scheduled tasks that looks for locally dropped writes and decide if it's ok to get hints based on the mutation stage queue. It could then request the hint delivery from A.;;;","02/Dec/11 14:11;jbellis;That could work, but I cringe at adding both ""pull"" and ""push"" modes for hint delivery, which has historically been a source of enough bugs that more complexity is counterindicated.;;;","02/Dec/11 16:07;brandon.williams;At risk of heresy, would bringing the hourly scan back be so bad now that our hint model doesn't suck like it did the first time we did that? ;;;","02/Dec/11 16:11;tjake;Can we keep a running tally of hints per endpoint when they are written, when they reach a threshold we deliver them? + hourly scan :);;;","02/Dec/11 16:31;jbellis;The problem I have with a ""brute force"" hourly scan or hint threshold is that you're likely to run into the same overload scenario that caused the hinting in the first place.;;;","03/Dec/11 00:27;appodictic;Could we use the badness detector in dynamic switch?;;;","05/Dec/11 15:22;tjake;The other problem is even if we fix the replay issue it's still terribly slow due to excessive throttling

I like the idea of changing from a push to pull mode for hint delivery. Similar to how mysql replication is client pull.  Clients know how swamped they are and can throttle their own delivery.


;;;","05/Dec/11 17:03;jbellis;The problem with a pull model is that the node doing the pulling usually won't know ""I was down, therefore I should ask for hints.""  (The exception is on restart, but hints due to overload conditions or GC pauses are much more common.);;;","05/Dec/11 17:07;tjake;Right, however its never going to know if hints are on a coordinator node due to the coordinator needed to drop some messages (backpressure?)

So either the clients can poll all nodes slowly and fetch hints or we perhaps gossip hints available flag so nodes know when hints are there to read?;;;","06/Dec/11 01:33;jbellis;Right.  So, not clearly simpler than doing something to our existing push model, but much more code churn.  I'd rather stick with push.;;;","07/Dec/11 16:19;appodictic;How expensive is the process of ""1) Wake Up. 2)Check for hints. 3) try to deliver them."" As for parts 1 & 2 I would think we can do this more often then hourly, maybe a background thread that sleeps for N seconds and attempts again.  

{quote}
The other problem is even if we fix the replay issue it's still terribly slow due to excessive throttling{quote} Side note. This throttle can not currently be adjusted at runtime. This should be JMX able. The default may be too low. Historically the problem was the hint sending node got hammered. In my mind the throttle was protecting that system.  ;;;","15/Dec/11 04:47;jbellis;Brute force fix attached to check for hints-to-deliver every 10 minutes.  If a hint cannot be replayed because of timing out, we abort (to that target).  Reduces default hint throttle delay to 1ms.;;;","15/Dec/11 04:52;jbellis;0001 does some related cleanup, including moving the mbean method StorageService.deliverHints to HHOM.scheduleHintDelivery.;;;","19/Dec/11 22:57;jbellis;combined patch against 1.0;;;","19/Dec/11 23:34;jbellis;updated 1.0 rebase that is pre-1034 friendly;;;","20/Dec/11 12:06;brandon.williams;I'm not sure how exactly, but obviously the keys being passed here are not quite what we think they are:

{noformat}
DEBUG 11:57:03,907 Started scheduleAllDeliveries
DEBUG 11:57:03,907 deliverHints to /7fff:ffff:ffff:ffff:ffff:ffff:ffff:fffe
DEBUG 11:57:03,908 deliverHints to /5555:5555:5555:5555:5555:5555:5555:5554
DEBUG 11:57:03,908 Checking remote(/7fff:ffff:ffff:ffff:ffff:ffff:ffff:fffe) schema before delivering hints
DEBUG 11:57:03,908 Finished scheduleAllDeliveries
ERROR 11:57:03,909 Fatal exception in thread Thread[HintedHandoff:3,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.waitForSchemaAgreement(HintedHandOffManager.java:206)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:238)
        at org.apache.cassandra.db.HintedHandOffManager.access$200(HintedHandOffManager.java:84)
        at org.apache.cassandra.db.HintedHandOffManager$3.runMayThrow(HintedHandOffManager.java:383)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}
;;;","22/Dec/11 20:29;jbellis;You're right, we switched from InetAddress as the key to Tokens.  v2 attached.;;;","22/Dec/11 21:18;brandon.williams;+1;;;","22/Dec/11 21:34;jbellis;committed;;;","21/Mar/12 11:59;jonma;+1 
""shedule deliver hints"" should have been more ealier brought in .
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
value validator in the cli does not pick up,CASSANDRA-3553,12533553,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,cywjackson,cywjackson,02/Dec/11 00:49,16/Apr/19 09:32,14/Jul/23 05:52,02/Dec/11 14:39,1.0.6,,,Legacy/Tools,,,0,cli,,,"the summary is probably confusing, so here is an example:

{noformat}
[default@testks] describe testcf;
    ColumnFamily: testcf
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.LongType
      Columns sorted by: org.apache.cassandra.db.marshal.LongType
{noformat}

notice both column and column value are under LongType

in the cli (without assume):

[default@testks] set testcf['foo'][1293843587]=30; 
null
InvalidRequestException(why:(Expected 8 or 0 byte long (2)) [testks][testcf][1293843587] failed validation)
        at org.apache.cassandra.thrift.Cassandra$insert_result.read(Cassandra.java:15198)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_insert(Cassandra.java:858)
        at org.apache.cassandra.thrift.Cassandra$Client.insert(Cassandra.java:830)
        at org.apache.cassandra.cli.CliClient.executeSet(CliClient.java:902)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:216)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)

so the above so the value cannot be validated, so now lets try to change the column name also:

[default@testks] set testcf['foo'][30]=30;               
null
InvalidRequestException(why:(Expected 8 or 0 byte long (2)) [testks][testcf][30] failed validation)
        at org.apache.cassandra.thrift.Cassandra$insert_result.read(Cassandra.java:15198)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_insert(Cassandra.java:858)
        at org.apache.cassandra.thrift.Cassandra$Client.insert(Cassandra.java:830)
        at org.apache.cassandra.cli.CliClient.executeSet(CliClient.java:902)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:216)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)

now lets set value with 8 characters:

[default@testks] set testcf['foo'][30]=12345678;  
Value inserted.

so that shows column is fine, only the value part is not fine. put it in assume or long() works:

[default@testks] assume testcf validator as long;
Assumption for column family 'testcf' added successfully.
[default@testks] set testcf['foo'][30]=30;       
Value inserted.


or (restart to a new session to un-assume):

[default@testks] set testcf['foo'][30]=long(30);         
Value inserted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/11 14:26;xedin;CASSANDRA-3553-1-0-specific-changes.patch;https://issues.apache.org/jira/secure/attachment/12505895/CASSANDRA-3553-1-0-specific-changes.patch","02/Dec/11 12:04;xedin;CASSANDRA-3553.patch;https://issues.apache.org/jira/secure/attachment/12505876/CASSANDRA-3553.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219281,,,Fri Dec 02 14:39:48 UTC 2011,,,,,,,,,,"0|i0glbr:",94885,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"02/Dec/11 12:04;xedin;it wasn't peaking correct default validation class, just converting everything to bytes...;;;","02/Dec/11 14:13;jbellis;For consistency with the rest of the CLI we should use defaultValidator.fromString(columnValue) even for BytesType.  (Which means we should probably make this change only in 1.0, since 0.8 users may be relying on the old, broken behavior.);;;","02/Dec/11 14:26;xedin;I agree, here is patch with 1.0 specific changes.;;;","02/Dec/11 14:31;jbellis;+1;;;","02/Dec/11 14:39;xedin;Committed to 1.0 branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timeout exception for quorum reads after upgrade from 1.0.2 to 1.0.5,CASSANDRA-3551,12533530,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,apache.zli,apache.zli,01/Dec/11 22:58,16/Apr/19 09:32,14/Jul/23 05:52,07/Dec/11 17:35,1.0.6,,,,,,1,,,,"I upgraded from 1.0.2 to 1.0.5. For some column families always got TimeoutException. I turned on debug and increase rpc_timeout to 1 minute, but still got timeout. I believe it is bug on 1.0.5.

ConsistencyLevel is QUORUM, replicate factor is 3. 

Here are partial logs. 


DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,717 StorageProxy.java (line 813) RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=SlicePre
dicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00 0C 00 0
2 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00 0C 
00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/999/99999], max_keys=1024}
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,718 StorageProxy.java (line 1012) restricted ranges for query [PROD/US/000/0,PROD/US/999/99999] are [[PROD/US/000/0,PROD/US/300/~], (PROD/US/300/~,PROD/
US/600/~], (PROD/US/600/~,PROD/US/999/99999]]
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,720 VoxeoStrategy.java (line 157) ReplicationFactor 3
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,720 VoxeoStrategy.java (line 33) PROD/US/300/~
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,721 VoxeoStrategy.java (line 96) End region for token PROD/US/300/~ PROD/US/300/~ 10.92.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,721 VoxeoStrategy.java (line 96) End region for token PROD/US/300/~ PROD/US/600/~ 10.72.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,721 VoxeoStrategy.java (line 96) End region for token PROD/US/300/~ PROD/US/999/~ 10.8.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,723 VoxeoStrategy.java (line 157) ReplicationFactor 3
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,724 ReadCallback.java (line 77) Blockfor/repair is 2/false; setting up requests to /10.92.208.103,/10.72.208.103
DEBUG [WRITE-/10.92.208.103] 2011-12-01 22:25:39,725 OutboundTcpConnection.java (line 206) attempting to connect to /10.92.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,726 StorageProxy.java (line 859) reading RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=
SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00
 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 7
2 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/300/~], max_keys=1024} from /10.92.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,726 StorageProxy.java (line 859) reading RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=
SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00
 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 7
2 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/300/~], max_keys=1024} from /10.72.208.103
DEBUG [WRITE-/10.8.208.103] 2011-12-01 22:25:39,727 OutboundTcpConnection.java (line 206) attempting to connect to /10.8.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,727 StorageProxy.java (line 859) reading RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=
SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00
 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 7
2 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/300/~], max_keys=1024} from /10.8.208.103
DEBUG [ReadStage:1] 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 0 of 1024: active:false:1@1322777621601000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 1 of 1024: name:false:4@1322777621601000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 2 of 1024: providerData:false:2283@1321549067179000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 3 of 1024: providerID:false:1@1322777621601000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,732 SliceQueryFilter.java (line 123) collecting 4 of 1024: timestamp:false:13@1322777621601000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,732 SliceQueryFilter.java (line 123) collecting 5 of 1024: vendorData:false:2364@1322777621601000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,733 ColumnFamilyStore.java (line 1331) scanned DecoratedKey(PROD/US/001/1, 50524f442f55532f3030312f31)
DEBUG [ReadStage:1] 2011-12-01 22:25:39,733 RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=Row(key=DecoratedKey(PROD/US/001/1, 50524f442f55532f3030312f31), cf=ColumnFamily(dataP
rovider [active:false:1@1322777621601000,name:false:4@1322777621601000,providerData:false:2283@1321549067179000,providerID:false:1@1322777621601000,timestamp:false:13@1322777621601000,vendorData:f
alse:2364@1322777621601000,]))} to 72@/10.72.208.103
DEBUG [RequestResponseStage:1] 2011-12-01 22:25:39,734 ResponseVerbHandler.java (line 44) Processing response on a callback from 72@/10.72.208.103
DEBUG [RequestResponseStage:2] 2011-12-01 22:25:39,887 ResponseVerbHandler.java (line 44) Processing response on a callback from 71@/10.92.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,889 SliceQueryFilter.java (line 123) collecting 0 of 2147483647: active:false:1@1322777621601000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 1 of 2147483647: name:false:4@1322777621601000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 2 of 2147483647: providerData:false:2283@1321549067179000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 3 of 2147483647: providerID:false:1@1322777621601000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 4 of 2147483647: timestamp:false:13@1322777621601000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,891 SliceQueryFilter.java (line 123) collecting 5 of 2147483647: vendorData:false:2364@1322777621601000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,892 StorageProxy.java (line 867) range slices read DecoratedKey(PROD/US/001/1, 50524f442f55532f3030312f31)
DEBUG [RequestResponseStage:3] 2011-12-01 22:25:39,936 ResponseVerbHandler.java (line 44) Processing response on a callback from 73@/10.8.208.103
DEBUG [ScheduledTasks:1] 2011-12-01 22:26:19,788 LoadBroadcaster.java (line 86) Disseminating load info ...
DEBUG [pool-2-thread-1] 2011-12-01 22:26:39,904 StorageProxy.java (line 874) Range slice timeout: java.util.concurrent.TimeoutException: Operation timed out.
","Linux, Cassandra 1.0.5",jalkanen,kzadorozhny,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/11 17:07;slebresne;3551.patch;https://issues.apache.org/jira/secure/attachment/12506485/3551.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219258,,,Wed Dec 14 19:54:21 UTC 2011,,,,,,,,,,"0|i0glav:",94881,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"01/Dec/11 23:08;jbellis;any exceptions on the other nodes?;;;","02/Dec/11 09:57;jalkanen;I am seeing this too; switching to ConsistencyLevel.ONE helps, but does not solve the problem completely, i.e. queries fail less often.;;;","02/Dec/11 10:23;slebresne;More infos on you respective setups could help. For instance:
* you said, 'For some column families'. Is there something specific to those column families ? Are they using compression? leveled compaction?
* Janne: you're seeing it too, but on which version exactly did you definitively not see this problem and on which are you definitively seeing it? Is it 1.0.2 and 1.0.5 respectively as for Zhong?
* As Jonathan said, are you seeing any error in any node logs?;;;","02/Dec/11 11:28;jalkanen;1.0.5, RF 3, 3 node cluster on EC2.  I upgraded just recently directly from 0.6.13, so I have not been on any earlier 1.0.x version.  No compression, just a straightforward upgrade with minimal tuning to the cassandra.yaml file.  2GB heap, maybe ~1GB in use.  Happens with column families which have 20 rows, CFs which have 10000 rows and more.  Happens when trying to read 100 rows at a time, happens when trying to read 10k rows at a time.  The only factor that I've noticed while trying to tune that has any effect is changing the CL.

No errors in node logs, no anomalies in system monitoring (like suddenly increased disk latency).  Only cassandra's storageproxy latency goes way up (hundreds of milliseconds), before failure.

Here is the exception from hector:

Caused by: me.prettyprint.hector.api.exceptions.HTimedOutException: TimedOutException()
	at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:42)
	at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:163)
	at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:145)
	at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:101)
	at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:233)
	at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:131)
	at me.prettyprint.cassandra.service.KeyspaceServiceImpl.getRangeSlices(KeyspaceServiceImpl.java:167)
	at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:67)
	at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:63)
	at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)
	at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:85)
	at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery.execute(ThriftRangeSlicesQuery.java:62)

Here's the CF definition:

    ColumnFamily: XXXX
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Row Cache Provider: org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider
      Key cache size / save period in seconds: 200000.0/14400
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
;;;","02/Dec/11 18:31;apache.zli;There is no exceptions on other nodes. 

I might be wrong about 'For some column families'. I saw another column family failed with Range Slice too. It works for insert. It might work for others retrieve command. I need test more when I have time. There is no compression, one row data only.



 ColumnFamily: dataProvider
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds / keys to save : 1024.0/0/all
      Row Cache Provider: org.apache.cassandra.cache.SerializingCacheProvider
      Key cache size / save period in seconds: 1024.0/14400
      GC grace seconds: 432000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Column Metadata:
        Column Name: active
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
          Index Name: dataProvider_active_idx
          Index Type: KEYS
        Column Name: object
          Validation Class: org.apache.cassandra.db.marshal.BytesType
        Column Name: providerData
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
          Index Name: dataProvider_providerData_idx
          Index Type: KEYS
        Column Name: providerID
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
          Index Name: dataProvider_providerID_idx
          Index Type: KEYS
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
 ;;;","06/Dec/11 18:06;dccwilliams;I hit a version of this problem...

I upgraded a production cluster from 1.0.3 (from a non-official version patched for CASSANDRA-3510) to 1.0.5. The aim was to pass CASSANDRA-3440.

This generated a timeout storm on range slices and I have reverted. 

Notes:

1/ The 1.0.5 node CPUs all showed tiny load - in fact, they seemed to be substantially less loaded than the 1.0.3 nodes were/are again

2/ The system.log files on the 1.0.5 nodes didn't record any errors

3/ range_slice timeout storm experienced in application layer. Example log trace below

org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out
        at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:129) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204) ~[libthrift-0.6.1.jar:0.6.1]
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_slice(Cassandra.java:560) ~[cassandra-thrift-1.0.1.jar:1.0.1]
        at org.apache.cassandra.thrift.Cassandra$Client.get_slice(Cassandra.java:542) ~[cassandra-thrift-1.0.1.jar:1.0.1]
        at org.scale7.cassandra.pelops.Selector$3.execute(Selector.java:683) ~[scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at org.scale7.cassandra.pelops.Selector$3.execute(Selector.java:680) ~[scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at org.scale7.cassandra.pelops.Operand.tryOperation(Operand.java:86) [scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at org.scale7.cassandra.pelops.Operand.tryOperation(Operand.java:66) [scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at org.scale7.cassandra.pelops.Selector.getColumnOrSuperColumnsFromRow(Selector.java:680) [scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at org.scale7.cassandra.pelops.Selector.getColumnsFromRow(Selector.java:689) [scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at org.scale7.cassandra.pelops.Selector.getColumnsFromRow(Selector.java:676) [scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at org.scale7.cassandra.pelops.Selector.getColumnsFromRow(Selector.java:562) [scale7-pelops-1.3-1.0.x-SNAPSHOT.jar:na]
        at com.fightmymonster.game.Monsters.getMonster(Monsters.java:92) [fmmServer.jar:na]
        at com.fightmymonster.rmi.monsters.GetMonster.doWork(GetMonster.java:25) [fmmServer.jar:na]
        at org.wyki.networking.starburst.SyncRmiOperation.run(SyncRmiOperation.java:50) [fmmServer.jar:na]
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_22]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_22]
        at java.lang.Thread.run(Thread.java:662) [na:1.6.0_22]
Caused by: java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.6.0_22]
        at java.net.SocketInputStream.read(SocketInputStream.java:129) ~[na:1.6.0_22]
        at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:127) ~[libthrift-0.6.1.jar:0.6.1]
        ... 23 common frames omitted;;;","07/Dec/11 17:07;slebresne;This is due to CASSANDRA-3440. More precisely, the fact that in RowRepairResolver it has changed the message from the mutation verb to the read_repair one. The problem is that ReadRepairVerbHandler does not respond anything, but the RowRepairResolver is waiting for a response.

After looking, I haven't found any part of the code using the read_repair verb handler except for the RowRepairResolver (which would mean that before CASSANDRA-3440 it wasn't used at all, so it's worth having someone else double checking I didn't missed anything), so a simple fix is to make the ReadRepairVerbHandler return an acknowledgment. Attaching a patch for that.;;;","07/Dec/11 17:29;jbellis;To be explicit: this only affects queries at CL > ONE.

bq. I haven't found any part of the code using the read_repair verb handler except for the RowRepairResolver, which would mean that before CASSANDRA-3440 it wasn't used at all

Right, it was used for a while, then we switched to MUTATION Verb to get the reply for ""free"" when we changed StorageProxy to wait for repair acks, but we never cleared out the Verb or RRVH.

+1 on the fix.;;;","07/Dec/11 17:35;slebresne;Committed, thanks;;;","14/Dec/11 19:54;jalkanen;Confirmed fixed in 1.0.6. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in AntiEntropyService$RepairSession.completed(),CASSANDRA-3548,12533435,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,amorton,amorton,01/Dec/11 11:35,16/Apr/19 09:32,14/Jul/23 05:52,02/Dec/11 10:54,1.0.6,,,,,,0,,,,"This may be related to CASSANDRA-3519 (cluster it was observed on is still 1.0.1), however i think there is still a race condition.

Observed on a 2 DC cluster, during a repair that spanned the DC's.  

{noformat}
INFO [AntiEntropyStage:1] 2011-11-28 06:22:56,225 StreamingRepairTask.java (line 136) [streaming task #69187510-1989-11e1-0000-5ff37d368cb6] Forwarding streaming repair of 8602 
ranges to /10.6.130.70 (to be streamed with /10.37.114.10)
...
 INFO [AntiEntropyStage:66] 2011-11-29 11:20:57,109 StreamingRepairTask.java (line 253) [streaming task #69187510-1989-11e1-0000-5ff37d368cb6] task succeeded
ERROR [AntiEntropyStage:66] 2011-11-29 11:20:57,109 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[AntiEntropyStage:66,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.completed(AntiEntropyService.java:712)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession$Differencer$1.run(AntiEntropyService.java:912)
        at org.apache.cassandra.streaming.StreamingRepairTask$2.run(StreamingRepairTask.java:186)
        at org.apache.cassandra.streaming.StreamingRepairTask$StreamingRepairResponse.doVerb(StreamingRepairTask.java:255)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
{noformat}

One of the nodes involved in the repair session failed, e.g. (Not sure if this is from the same repair session as the streaming task above, but it illustrates the issue)

{noformat}
ERROR [AntiEntropySessions:1] 2011-11-28 19:39:52,507 AntiEntropyService.java (line 688) [repair #2bf19860-197f-11e1-0000-5ff37d368cb6] session completed with the following error
java.io.IOException: Endpoint /10.29.60.10 died
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.failedNode(AntiEntropyService.java:725)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.convict(AntiEntropyService.java:762)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:192)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
ERROR [GossipTasks:1] 2011-11-28 19:39:52,507 StreamOutSession.java (line 232) StreamOutSession /10.29.60.10 failed because {} died or was restarted/removed
ERROR [GossipTasks:1] 2011-11-28 19:39:52,571 Gossiper.java (line 172) Gossip error
java.util.ConcurrentModificationException
        at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:782)
        at java.util.ArrayList$Itr.next(ArrayList.java:754)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:190)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)

{noformat}

When a node is marked as failed AntiEntropyService.RepairSession.forceShutdown() clears the activejobs map. But the jobs to other nodes will continue, and will eventually call completed(). 

RepairSession.terminated should stop completed() from checking the map, but there is a race between the map been cleared and if there is an error in finally block it wont be set. 
","Free BSD 8.2, JVM vendor/version: OpenJDK 64-Bit Server VM/1.6.0",slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/11 11:43;amorton;0001-3548.patch;https://issues.apache.org/jira/secure/attachment/12505756/0001-3548.patch","01/Dec/11 12:03;slebresne;3548-v2.patch;https://issues.apache.org/jira/secure/attachment/12505760/3548-v2.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219163,,,Fri Dec 02 10:54:38 UTC 2011,,,,,,,,,,"0|i0gl9j:",94875,,amorton,,amorton,Low,,,,,,,,,,,,,,,,,"01/Dec/11 11:43;amorton;check for null;;;","01/Dec/11 12:03;slebresne;We also have the same race in rendezvous with the jobs. Attaching v2 that fix both.;;;","01/Dec/11 20:16;amorton;Much nicer. I'm not able to test in place on the cluster but +1.

thanks.;;;","02/Dec/11 10:54;slebresne;bq. I'm not able to test in place on the cluster

You would probably have a hard time reproducing this anyway.

Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race between cf flush and  its secondary indexes flush,CASSANDRA-3547,12533432,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,01/Dec/11 11:19,16/Apr/19 09:32,14/Jul/23 05:52,02/Dec/11 10:50,1.0.6,,,Feature/2i Index,,,0,,,,"When a CF with indexes is flushed, it's indexes are flushed too. In particular their memtable is switched, but without making the old memtable frozen. This can conflict with a concurrent flush of the index itself, as reported on the user list by Michael Vaknine:
{noformat}
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 java.lang.AssertionError
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.ColumnFamilyStore.maybeSwitchMemtable(ColumnFamilyStore.java:671)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:745)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.ColumnFamilyStore.forceBlockingFlush(ColumnFamilyStore.java:750)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.index.keys.KeysIndex.forceBlockingFlush(KeysIndex.java:119)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.index.SecondaryIndexManager.flushIndexesBlocking(SecondaryIndexManager.java:258)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.index.SecondaryIndexManager.maybeBuildSecondaryIndexes(SecondaryIndexManager.java:123)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:151)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/11 11:29;slebresne;3547.patch;https://issues.apache.org/jira/secure/attachment/12505754/3547.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219160,,,Fri Dec 02 10:50:02 UTC 2011,,,,,,,,,,"0|i0gl8v:",94872,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"01/Dec/11 11:29;slebresne;Patch to freeze every memtable we actually flush.;;;","01/Dec/11 11:39;slebresne;Actually I see no reason this wouldn't affect 0.8. The patch is against 1.0 but it will be trivial to rebase to 0.8 if needed.;;;","01/Dec/11 21:24;jbellis;+1, and I think we do need this for 0.8 as well;;;","01/Dec/11 21:24;jbellis;However...  0.8 is stable enough, and this bug is rare enough, and flush code is tricky enough, that maybe we should leave it be.  I could go either way.;;;","02/Dec/11 10:50;slebresne;You know what, I agree on 0.8. It's the first time anyone reports this and it's not like it corrupt data or anything bad, so I've committed to 1.0 only.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted handoffs isn't delivered if/when HintedHandOffManager ends up in invalid state.,CASSANDRA-3546,12533423,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,fredrikl74,fredrikl74,01/Dec/11 10:42,16/Apr/19 09:32,14/Jul/23 05:52,02/Dec/11 10:45,1.0.6,,,,,,0,,,,"Running Cassandra 1.0.3.
I've done some testing with 2 nodes (node A, node B), replication factor 2.
I take node A down, writing some data to node B and then take node A up.
Sometimes hints aren't delivered when node A comes up.

I've done some debugging in org.apache.cassandra.db.HintedHandOffManager and sometimes node B ends up in a strange state in method org.apache.cassandra.db.HintedHandOffManager.deliverHints(final InetAddress to), where org.apache.cassandra.db.HintedHandOffManager.queuedDeliveries already has node A in it's Set and therefore no hints will ever be delivered to node A.

The only reason for this that I can see is that in org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(InetAddress endpoint) the hintStore.isEmpty() check returns true and the endpoint (node A)  isn't removed from org.apache.cassandra.db.HintedHandOffManager.queuedDeliveries. Then no hints will ever be delivered again until node B is restarted.

During what conditions will hintStore.isEmpty() return true?
Shouldn't the hintStore.isEmpty() check be inside the try {} finally{} clause, removing the endpoint from queuedDeliveries in the finally block?

{code}
public void deliverHints(final InetAddress to)
{
    logger_.debug(""deliverHints to {}"", to);
    if (!queuedDeliveries.add(to))
        return;
    .......
}
{code}

{code}
private void deliverHintsToEndpoint(InetAddress endpoint) 
    throws IOException, DigestMismatchException, InvalidRequestException, TimeoutException, InterruptedException
{
     ColumnFamilyStore hintStore = Table.open(Table.SYSTEM_TABLE).getColumnFamilyStore(HINTS_CF);
     if (hintStore.isEmpty())
         return; // nothing to do, don't confuse users by logging a no-op handoff
     try
     {
         ......
     }
     finally
     {
         queuedDeliveries.remove(endpoint);
     }
}
{code} ",,fredrikl74,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/11 12:26;slebresne;3546.patch;https://issues.apache.org/jira/secure/attachment/12505762/3546.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219151,,,Fri Dec 02 10:45:30 UTC 2011,,,,,,,,,,"0|i0gl8n:",94871,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"01/Dec/11 11:42;slebresne;Patch attached, thanks Fredrik.;;;","01/Dec/11 12:21;fredrikl74;The patch is broken, hintStore must be declared outside the try {} finally {} clause.

;;;","01/Dec/11 12:26;slebresne;Oops, that'll teach me to not even compile before submitting. Patch updated.;;;","01/Dec/11 19:00;jbellis;+1;;;","02/Dec/11 10:45;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE on startup when there are permissions issues with directories,CASSANDRA-3544,12533346,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,thobbs,thobbs,30/Nov/11 20:55,16/Apr/19 09:32,14/Jul/23 05:52,16/Dec/11 21:31,1.0.7,,,,,,0,,,,"If the directories used by cassandra for data, commitlog, and saved caches aren't readable due to permissions, you get an NPE on startup.  In particular, if none of them are readable, you'll see something like this:

{noformat}
ERROR 14:50:11,945 Exception encountered during startup
java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:391)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:147)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:391)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:147)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
Exception encountered during startup: null
{noformat}

This traceback happens when the saved_caches directory isn't readable, but you can get different ones if only the data or commitlog directories aren't readable.

We should check the permissions of these directories before trying to list their contents.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/11 18:34;yukim;cassandra-1.0-3544.txt;https://issues.apache.org/jira/secure/attachment/12507714/cassandra-1.0-3544.txt",,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219074,,,Fri Dec 16 21:31:51 UTC 2011,,,,,,,,,,"0|i0gl7r:",94867,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"07/Dec/11 17:02;yukim;Here is my first attempt. It checks all(data, commitlog, saved cache) directories' existence and permissions on startup.

If check fails, cassandra stops starting.;;;","08/Dec/11 20:52;brandon.williams;The problem with this patch is it asserts the directories exist, where before we would try to create them if they didn't.;;;","16/Dec/11 18:34;yukim;You're right. I changed to assert only when directory exists.;;;","16/Dec/11 21:31;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commit Log Allocator deadlock after first start with empty commitlog directory,CASSANDRA-3543,12533334,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,rbranson,brandon.williams,brandon.williams,30/Nov/11 19:22,16/Apr/19 09:32,14/Jul/23 05:52,02/Dec/11 22:10,1.1.0,,,,,,0,,,,"While testing CASSANDRA-3541 at some point stress completely timed out.  I proceeded to shut the cluster down and 2/3 JVMs hang infinitely.  After a while, one of them logged:

{noformat}
WARN 19:07:50,133 Some hints were not written before shutdown.  This is not supposed to happen.  You should (a) run repair, and (b) file a bug report
{noformat}",,colinkuo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/11 20:55;rbranson;3543.txt;https://issues.apache.org/jira/secure/attachment/12505929/3543.txt","30/Nov/11 19:23;brandon.williams;hung_stack.txt;https://issues.apache.org/jira/secure/attachment/12505649/hung_stack.txt",,,,,,,,,,,,,2.0,rbranson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,219062,,,Fri Dec 02 23:17:56 UTC 2011,,,,,,,,,,"0|i0gl73:",94864,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"30/Nov/11 19:23;brandon.williams;Here is a thread dump.;;;","30/Nov/11 20:31;rbranson;This is a bug in the new commit log allocator from #3411. The MutationStage threads are all blocked because the CommitLogExecutor queue is full. The COMMIT-LOG-WRITER thread which drains this queue is blocking on fetchSegment() which waits on CommitLogAllocator to push newly created segments onto this queue. Brandon also stated that only 1 commit log segment existed afterwards.;;;","30/Nov/11 22:12;rbranson;Steps to reproduce:

1) $ rm -rf /var/lib/cassandra/*
2) Start Cassandra
3) $ stress -F1 -n 999999999

The stress will run until requests start timing out, on my box it was ~400,000.;;;","30/Nov/11 22:38;rbranson;A workaround is to restart Cassandra immediately after it comes up the first time with an empty commitlog directory.;;;","02/Dec/11 22:10;jbellis;committed;;;","02/Dec/11 23:17;hudson;Integrated in Cassandra #1234 (See [https://builds.apache.org/job/Cassandra/1234/])
    enableReserveSegmentCreation even when there is nothing to replay
patch by Rick Branson; reviewed by jbellis for CASSANDRA-3543

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1209724
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong check of partitioner for secondary indexes,CASSANDRA-3540,12533155,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,yukim,slebresne,slebresne,29/Nov/11 17:57,16/Apr/19 09:32,14/Jul/23 05:52,08/Dec/11 12:06,1.0.6,,,Feature/2i Index,,,0,,,,"CASSANDRA-3407 doesn't handle the fact that secondary indexes have a specific partitioner (LocalPartitioner). This result in the following error when starting nodes in 1.0.4:
{noformat}
java.lang.RuntimeException: Cannot open /var/lib/cassandra/data/Index/AttractionLocationCategoryDateIdx.AttractionLocationCategoryDateIdx_09partition_idx-h-1 because partitioner does not match org.apache.cassandra.dht.LocalPartitioner
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/11 08:22;yukim;0001-Add-tests-for-opening-index-sstables.patch;https://issues.apache.org/jira/secure/attachment/12506586/0001-Add-tests-for-opening-index-sstables.patch","08/Dec/11 08:22;yukim;0002-Fix-SSTableMetadata-to-write-correct-partitioner.patch;https://issues.apache.org/jira/secure/attachment/12506587/0002-Fix-SSTableMetadata-to-write-correct-partitioner.patch","06/Dec/11 06:48;yukim;cassandra-1.0-3540-v2.txt;https://issues.apache.org/jira/secure/attachment/12506223/cassandra-1.0-3540-v2.txt","01/Dec/11 08:25;yukim;cassandra-1.0-3540.txt;https://issues.apache.org/jira/secure/attachment/12505740/cassandra-1.0-3540.txt",,,,,,,,,,,4.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,218883,,,Thu Dec 08 12:06:20 UTC 2011,,,,,,,,,,"0|i0gl5z:",94859,,slebresne,,slebresne,Critical,,,,,,,,,,,,,,,,,"01/Dec/11 08:25;yukim;This bug is caused by SSTableMetadata providing node's partitioner when reading old version (prior hc) of sstable.

Attached patch let SSTableMetadata to use empty string("""") instead of DatabaseDescriptor.getPartitioner as default. When older version of sstable is read, SSTableMetadata provides empty string for partitioner, and check for partitioner is skipped.;;;","01/Dec/11 10:03;slebresne;Some nits:
  * We can give then partitioner as an argument to Collector.finalizeMetadata() instead of adding a new field.
  * I think I slightly prefer using null when there is no partitioner available. I understand that """" make SSTableMetadata slightly simpler but in fact we shouldn't ever write a SSTableMetadata with no partitioner (we can only read one). So we could simply assert that partitioner != null in the serialize method.;;;","01/Dec/11 16:44;jbellis;I'd also like to add a unit test (probably using clearUnsafe) to make sure we're exercising the logic and triggering the original 3407 bug.;;;","06/Dec/11 06:48;yukim;Updated patch to give partitioner an arg to Collector#finalizeMetadata().
I also added test to SSTableReaderTest which fails when run against current 1.0 branch and  succeeds after this patch.;;;","06/Dec/11 13:17;slebresne;The patch lgtm. Regarding the tests, I think it would be worth testing if we're correctly able to load old sstable, those without a saved partitioner, which I don't think the attached test does, does it (but the current test is good, we should keep it) ? Typically we could have a similar test than in the patch, but that nukes the statistics component of the sstables before reloading them . ;;;","08/Dec/11 08:22;yukim;Sylvain,

I separate the patch into two parts.

0001 just adds tests to reproduce CASSANDRA-3407, one test from previous patch (flush and open) and new test to simulate upgrading from previous version of SSTable. Patch generates Indexed1 SSTable which I created using v1.0.3 (SSTable version is ""hb"") under test/data/legacy-sstables/hb.
When testing, those sstables are copied into unit test data location just like ScrubTest does.

Note that it also generates Standard1 sstable in order to let LegacySSTableTest pass.

0002 contains fix same as previously submitted patch.

Only applying 0001 patch let SSTableReaderTest in current 1.0 branch fail, and you can see it success after applying 0002.;;;","08/Dec/11 12:06;slebresne;I would have been fine with just nuking the metadata component :) but that's great like that.

+1, committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion error when forwarding to local nodes,CASSANDRA-3539,12533131,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,29/Nov/11 15:59,16/Apr/19 09:32,14/Jul/23 05:52,29/Nov/11 17:04,1.0.5,,,,,,0,,,,"CASSANDRA-3530 introduces a regression as reported on irc:
{quote}
Started a rolling upgrade from 1.0.3 to 1.0.4 now all boxes are constantly spitting out this assert: at 
org.apache.cassandra.db.RowMutationVerbHandler.forwardToLocalNodes(RowMutationVerbHandler.java:71)
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/11 16:01;slebresne;3539.patch;https://issues.apache.org/jira/secure/attachment/12505485/3539.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,218859,,,Tue Nov 29 17:04:09 UTC 2011,,,,,,,,,,"0|i0gl5j:",94857,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"29/Nov/11 16:01;slebresne;The problem was that we were writing a FOWARD_HEADER with an empty byte array in case there were no local nodes to forward to.;;;","29/Nov/11 16:05;jbellis;+1;;;","29/Nov/11 17:04;slebresne;Committed, thanks

(for those running into this, note that you'll have to update *all* nodes before stopping seeing the exception showing up);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExpiringMap timer is not exception-proof,CASSANDRA-3537,12533074,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jonma,jonma,29/Nov/11 07:24,16/Apr/19 09:32,14/Jul/23 05:52,24/Feb/12 15:26,1.1.0,,,,,,0,,,,"I have 4 cassandra nodes ,and I put about 30G data to db for every nodes . It's just 4 days before I start the cluster ,but now every 4 nodes have the same problem ,JVM heap is full  ,and  GC take no effect ,There must be some memory leak . Jmap the memory as follow:

Object Histogram:

num 	  #instances	#bytes	Class description
--------------------------------------------------------------------------
1:		15793606	758093088	java.nio.HeapByteBuffer
2:		2153811	320138208	java.lang.Object[]
3:		6163192	197222144	org.apache.cassandra.db.Column
4:		2543836	175890256	int[]
5:		2168816	155397192	long[]
6:		2078123	116374888	org.cliffc.high_scale_lib.ConcurrentAutoTable$CAT
7:		1847111	73884440	java.math.BigInteger
8:		1234243	59243664	java.util.Hashtable
9:		1770829	58233000	char[]
10:		1770627	56660064	java.lang.String
11:		1665886	39981264	org.apache.cassandra.db.DecoratedKey
12:		692706	38791536	org.cliffc.high_scale_lib.NonBlockingHashMap$CHM
13:		1234274	37172088	java.util.Hashtable$Entry[]
14:		1133541	36273312	java.net.Inet4Address
15:		738528	35449344	org.apache.cassandra.service.ReadCallback
16:		2078118	33249888	org.cliffc.high_scale_lib.Counter
17:		1373886	32973264	org.apache.cassandra.db.ReadResponse
18:		1234023	29616552	org.apache.cassandra.net.Message
19:		1234019	29616456	org.apache.cassandra.net.Header
20:		1846185	29538960	org.apache.cassandra.dht.BigIntegerToken
21:		891378	28524096	org.apache.cassandra.utils.ExpiringMap$CacheableObject
22:		692706	27708240	org.cliffc.high_scale_lib.NonBlockingHashMap
23:		1148252	27558048	java.util.Collections$SynchronizedSet
24:		541977	26014896	org.apache.cassandra.db.SliceFromReadCommand
25:		998001	23952024	java.util.concurrent.ConcurrentSkipListMap$Node
26:		928792	22291008	java.util.ArrayList
27:		692715	22166880	java.util.concurrent.atomic.AtomicReferenceFieldUpdater$AtomicReferenceFieldUpdaterImpl
28:		891378	21393072	org.apache.cassandra.net.CallbackInfo
29:		1148247	18371952	java.util.Hashtable$KeySet
30:		731859	17564616	org.apache.cassandra.db.Row
31:		529991	16959712	org.apache.cassandra.db.ArrayBackedSortedColumns
32:		691425	16594200	org.apache.cassandra.db.AbstractColumnContainer$DeletionInfo
33:		648580	15565920	org.apache.cassandra.db.filter.QueryPath
34:		648338	15560112	org.apache.cassandra.service.RowDigestResolver
35:		971376	15542016	java.util.concurrent.atomic.AtomicInteger
36:		837418	13398688	org.apache.cassandra.utils.SimpleCondition
37:		535614	12854736	org.apache.cassandra.db.ColumnFamily
38:		725634	11610144	java.util.concurrent.atomic.AtomicReference
39:		195117	9365616	org.apache.cassandra.db.ThreadSafeSortedColumns
40:		281921	9021472	java.util.concurrent.ConcurrentSkipListMap$HeadIndex
41:		277679	8885728	java.util.concurrent.locks.ReentrantLock$NonfairSync
42:		314424	7546176	java.util.concurrent.ConcurrentSkipListMap$Index
43:		275186	6604464	java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject
44:		270280	6486720	java.util.concurrent.LinkedBlockingQueue$Node
45:		219553	5269272	org.apache.cassandra.io.sstable.IndexSummary$KeyPosition
46:		106436	5108928	java.util.TreeMap
47:		122185	4887400	org.apache.cassandra.db.ExpiringColumn
48:		189968	4559232	org.apache.cassandra.db.SuperColumn
49:		275659	4410544	java.util.concurrent.locks.ReentrantLock
50:		90213	4330224	java.util.concurrent.LinkedBlockingQueue
51:		107026	4281040	java.util.TreeMap$Entry
52:		30501	4222056	* ConstMethodKlass","当前堆大小: 
5,815,955 Kb
堆大小的最大值: 
6,045,696 Kb
分配的内存: 
6,045,696 Kb
暂挂结束操作: 
0 个对象
垃圾收集器: 
Name = 'ParNew', Collections = 3,294, Total time spent = 2 minutes
垃圾收集器: 
Name = 'ConcurrentMarkSweep', Collections = 5,909, Total time spent = 2 hours 17 minutes
 
操作系统: 
Linux 2.6.32.12-0.7-default
体系结构: 
amd64
处理器的数目: 
16
分配的虚拟内存: 
42,748,416 Kb
物理内存总量: 
24,568,836 Kb
可用物理内存: 
 7,136,380 Kb
交换空间总量: 
 2,104,472 Kb
可用交换空间: 
 1,970,800 Kb
 
VM 参数: 
-ea -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms6G -Xmx6G -Xmn2400M -XX:+HeapDumpOnOutOfMemoryError -Xss128k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=1 -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Djava.net.preferIPv4Stack=true -Dcom.sun.management.jmxremote.port=9000 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dpasswd.properties=/opt/obs/cassandra/conf/passwd.properties -Dpasswd.mode=MD5 -Dlog4j.configuration=log4j-server.properties -Dlog4j.defaultInitOverride=true 
类路径: 
/opt/obs/cassandra/conf:/opt/obs/cassandra/build/classes/main:/opt/obs/cassandra/build/classes/thrift:/opt/obs/cassandra/lib/antlr-3.2.jar:/opt/obs/cassandra/lib/apache-cassandra-1.0.0.jar:/opt/obs/cassandra/lib/apache-cassandra-clientutil-1.0.0.jar:/opt/obs/cassandra/lib/apache-cassandra-thrift-1.0.0.jar:/opt/obs/cassandra/lib/avro-1.4.0-fixes.jar:/opt/obs/cassandra/lib/avro-1.4.0-sources-fixes.jar:/opt/obs/cassandra/lib/cassandra_simple_authentication.jar:/opt/obs/cassandra/lib/commons-cli-1.1.jar:/opt/obs/cassandra/lib/commons-codec-1.2.jar:/opt/obs/cassandra/lib/commons-lang-2.4.jar:/opt/obs/cassandra/lib/compress-lzf-0.8.4.jar:/opt/obs/cassandra/lib/concurrentlinkedhashmap-lru-1.2.jar:/opt/obs/cassandra/lib/guava-r08.jar:/opt/obs/cassandra/lib/high-scale-lib-1.1.2.jar:/opt/obs/cassandra/lib/jackson-core-asl-1.4.0.jar:/opt/obs/cassandra/lib/jackson-mapper-asl-1.4.0.jar:/opt/obs/cassandra/lib/jamm-0.2.5.jar:/opt/obs/cassandra/lib/jline-0.9.94.jar:/opt/obs/cassandra/lib/json-simple-1.1.jar:/opt/obs/cassandra/lib/libthrift-0.6.jar:/opt/obs/cassandra/lib/log4j-1.2.16.jar:/opt/obs/cassandra/lib/servlet-api-2.5-20081211.jar:/opt/obs/cassandra/lib/slf4j-api-1.6.1.jar:/opt/obs/cassandra/lib/slf4j-log4j12-1.6.1.jar:/opt/obs/cassandra/lib/snakeyaml-1.6.jar:/opt/obs/cassandra/lib/snappy-java-1.0.3.jar",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/12 17:00;jbellis;3537-v2.txt;https://issues.apache.org/jira/secure/attachment/12515612/3537-v2.txt","23/Feb/12 17:46;jbellis;3537-v4.txt;https://issues.apache.org/jira/secure/attachment/12515768/3537-v4.txt","22/Feb/12 10:59;slebresne;3537.txt;https://issues.apache.org/jira/secure/attachment/12515573/3537.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,218802,,,Fri Feb 24 15:26:26 UTC 2012,,,,,,,,,,"0|i0gl4n:",94853,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"29/Nov/11 07:34;jbellis;Reduce the memtable_total_space_in_mb setting.;;;","30/Nov/11 02:00;jonma;Thank u very much . But I have a test tonight  ,I changed my 4 cassandra nodes'  max heap size and memtable_total_space_in_mb setting as follow:
node A: heap size :6G  , memtable_total_space_in_mb = 1G (as you say ,I reduce the memtable_total_space_in_mb setting)
node B: 8G   ,  default ;
node C: 5G   ,  default ;
node D: 8G   ,  1G  ;
Then I restart the cluster, and begin to push data to it . This morning I have a frustrated test result ,  node A heap is full  ,node B OK , node C OK , node D heap is full . 

I don't know how to explain it .

;;;","30/Nov/11 02:11;jbellis;what liveRatio is being logged for your workload?

can you reproduce using a synthetic Stress workload?  (stress tool available w/ source distribution);;;","30/Nov/11 03:50;jonma;In system.log ,I find this : 
<164> 4 2011-11-29T15:29:57.890714+00:00   WARN [MutationStage:10] 2011-11-29 15:29:57,890 Memtable.java (line 142) MemoryMeter uninitialized (jamm not specified as java agent); assuming liveRatio of 10.0.  Usually this means cassandra-env.sh disabled jamm because you are using a buggy JRE; upgrade to the Sun JRE instead .
 
I have no idea this will affect the test result ,or not . But all of my 4 cassandra nodes have the same log . I checked my system physical memory size : node A: 16 G ,ulimit -v :14G   node B:  32G , ulimit 28G ,  node C :48G ,ulimit -v 42G  node D:24G ulimit -v 21G , 

The ulimit -v setting will result the problem ?

ps : There is another java application run on my machine  . It result the log WARN .
 I haven't use Stress as workload ,I use a workload like YCSB based on Hector .
 There are 8 user CF in the system.




;;;","30/Nov/11 05:02;jbellis;You should upgrade to the Sun/Oracle JVM as it suggests.;;;","30/Nov/11 05:17;jonma;In fact ,my jvm is Sun JVM, Java HotSpot(TM) 64-Bit Server VM  19.1-b02 .;;;","30/Nov/11 05:32;jbellis;Then you'll have to come up with an alternative explanation for why jamm isn't being used.;;;","30/Nov/11 05:45;jonma;
If I terminate the another java application ,it will not have the log WARN .
I think I should terminate the another java application first , then do a new test again.  Thanks.
========================================
ps : There is another java application run on my machine . It result the log WARN .
I haven't use Stress as workload ,I use a workload like YCSB based on Hector .
.......;;;","02/Dec/11 02:04;jonma;After  I set jamm correctly , I do a new test , and haven't find the memory problem in 20 hours  .But I could't confirm it's  ""jamm not specified as java agent""  result the heap problem . I analysised the dump memory info of the node which had the heap problem , they nearly have the same thing :  too many Message objects and ralated objects like below,although I have stoped reading and writing data for a few minutes and  empty all memtables  .
#instances	#bytes	Class description
---------------------------------------------
900654	21615696	org.apache.cassandra.net.Message
900653	21615672	org.apache.cassandra.net.Header
341351	16384848	org.apache.cassandra.service.ReadCallback
338502	13540080	org.apache.cassandra.db.RangeSliceCommand
338500	10832000	org.apache.cassandra.thrift.SliceRange
338502	8124048	org.apache.cassandra.thrift.SlicePredicate
343328	5493248	org.apache.cassandra.utils.SimpleCondition


Is there someone can tell me ,whether there are some relationship between ""jamm not specified as java agent"" and so many 
objects like above  ?
 ;;;","02/Dec/11 06:45;jonma;Heap memory can not be GC again in my lasted test , although I have set jamm correctly . Jmap the memory ,find the same thing  ,too many Message objects and ralated objects  in the memory ! 
I have empty all memtables ,in normal condition , heap memory used not more than 0.5G ,but now more than 2.8G . I think only memory leak will result this .

{code:xml} 
Object Histogram:

num 	  #instances	#bytes	Class description
--------------------------------------------------------------------------
1:		9101138	927302160	byte[]
2:		9998059	479906832	java.nio.HeapByteBuffer
3:		875398	221844128	long[]
4:		4302698	137686336	org.apache.cassandra.db.Column
5:		869859	137476712	java.lang.Object[]
6:		1794458	101193096	int[]
7:		1515578	60623120	java.math.BigInteger
8:		834628	46739168	org.cliffc.high_scale_lib.ConcurrentAutoTable$CAT
9:		1644330	39463920	java.util.concurrent.ConcurrentSkipListMap$Node
10:		1461787	35082888	org.apache.cassandra.db.DecoratedKey
11:		513576	24651648	java.util.Hashtable
12:		741037	24559960	char[]
13:		1512396	24198336	org.apache.cassandra.dht.BigIntegerToken
14:		741097	23715104	java.lang.String
15:		732184	17572416	org.apache.cassandra.io.sstable.IndexSummary$KeyPosition
16:		513594	15637344	java.util.Hashtable$Entry[]
17:		278209	15579704	org.cliffc.high_scale_lib.NonBlockingHashMap$CHM
18:		462641	14804512	java.net.Inet4Address
19:		578801	13891224	java.util.concurrent.ConcurrentSkipListMap$Index
20:		834627	13354032	org.cliffc.high_scale_lib.Counter
21:		556415	13353960	org.apache.cassandra.db.ReadResponse
22:		267483	12839184	org.apache.cassandra.service.ReadCallback
23:		513250	12318000	org.apache.cassandra.net.Message
24:		513248	12317952	org.apache.cassandra.net.Header
25:		501596	12038304	org.apache.cassandra.db.AbstractColumnContainer$DeletionInfo
26:		244644	11742912	org.apache.cassandra.db.ThreadSafeSortedColumns
27:		365024	11680768	java.util.concurrent.ConcurrentSkipListMap$HeadIndex
28:		464800	11155200	java.util.Collections$SynchronizedSet
29:		278209	11128360	org.cliffc.high_scale_lib.NonBlockingHashMap
30:		342741	10967712	org.apache.cassandra.utils.ExpiringMap$CacheableObject
31:		199478	9574944	org.apache.cassandra.db.SliceFromReadCommand
32:		278218	8902976	java.util.concurrent.atomic.AtomicReferenceFieldUpdater$AtomicReferenceFieldUpdaterImpl
33:		348179	8356296	org.apache.cassandra.db.ColumnFamily
34:		342741	8225784	org.apache.cassandra.net.CallbackInfo
35:		255241	8167712	org.apache.cassandra.db.ArrayBackedSortedColumns
36:		334724	8033376	java.util.ArrayList
37:		499963	7999408	java.util.concurrent.atomic.AtomicReference
38:		319779	7674696	org.apache.cassandra.db.Row
39:		187484	7499360	org.apache.cassandra.db.Memtable$6
40:		464794	7436704	java.util.Hashtable$KeySet
41:		187496	5999872	java.util.TreeMap$KeyIterator
42:		372748	5963968	java.util.concurrent.atomic.AtomicInteger
43:		236877	5685048	org.apache.cassandra.db.filter.QueryPath
44:		236733	5681592	org.apache.cassandra.service.RowDigestResolver
45:		300385	4806160	org.apache.cassandra.utils.SimpleCondition
46:		30726	4245920	* ConstMethodKlass
47:		30726	3696240	* MethodKlass
48:		151726	3641424	org.apache.cassandra.db.SuperColumn
49:		3134	3350536	* ConstantPoolKlass
50:		95622	3059904	java.util.concurrent.locks.ReentrantLock$NonfairSync
51:		45910	2497952	* SymbolKlass
52:		3134	2276040	* InstanceKlassKlass
53:		94544	2269056	java.util.concurrent.LinkedBlockingQueue$Node
54:		93130	2235120	java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject
55:		133398	2134368	java.util.concurrent.ConcurrentSkipListMap$Values
56:		2813	2099368	* ConstantPoolCacheKlass
57:		37698	1809504	java.util.TreeMap
58:		37783	1511320	java.util.TreeMap$Entry
59:		93602	1497632	java.util.concurrent.locks.ReentrantLock
60:		30768	1476864	java.util.concurrent.LinkedBlockingQueue
61:		85156	1362496	java.util.concurrent.ConcurrentSkipListMap$EntrySet
62:		41451	1326432	org.apache.cassandra.service.RowRepairResolver
63:		41141	1316512	java.util.concurrent.ConcurrentHashMap$HashEntry
64:		32885	1315400	org.apache.cassandra.service.WriteResponseHandler
65:		3110	1277672	* MethodDataKlass
66:		39780	1272960	com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node
67:		31556	1262240	org.apache.cassandra.net.AsyncResult
68:		30745	1229800	org.apache.cassandra.db.RangeSliceCommand
69:		37628	1204096	org.apache.cassandra.db.SliceByNamesReadCommand
70:		44528	1068672	com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue
71:		31974	1023168	java.util.concurrent.locks.AbstractQueuedSynchronizer$Node
72:		41435	994440	org.apache.cassandra.service.AsyncRepairCallback
73:		30821	986272	org.apache.cassandra.thrift.SliceRange
74:		30756	984192	java.util.RandomAccessSubList
75:		30745	983840	org.apache.cassandra.service.RangeSliceResponseResolver
76:		29115	931680	org.apache.cassandra.db.DeletedColumn
77:		21676	867040	org.apache.cassandra.db.ExpiringColumn
78:		35339	848136	java.lang.Long
79:		35204	844896	org.apache.cassandra.utils.Pair
80:		30924	742176	java.util.BitSet
81:		30821	739704	org.apache.cassandra.thrift.SlicePredicate
82:		30787	738888	org.apache.cassandra.dht.Bounds
83:		37673	602768	java.util.TreeSet
84:		37654	602464	java.util.TreeMap$KeySet
85:		31598	505568	java.util.concurrent.atomic.AtomicBoolean
86:		6575	420800	java.nio.DirectByteBufferR
87:		2004	416400	java.util.concurrent.ConcurrentHashMap$HashEntry[]
88:		3433	357032	java.lang.Class
89:		8976	287232	java.lang.ref.WeakReference
90:		4999	269008	short[]{code} 
;;;","05/Dec/11 03:02;jonma;I spend a whole week analyse this problem , Now I find this problem is only caused by {color:red} 
the cache in ExpiringMap {color} 
 which is a NonBlockingHashMap . Why this cache can make the heap memory full and never be GC ?;;;","05/Dec/11 05:55;jbellis;That is where callbacks for ongoing requests are stored.;;;","05/Dec/11 06:19;jonma;Yes. 
{code:title=ExpiringMap.java|borderStyle=solid}
/**
     *
     * @param expiration the TTL for objects in the cache in milliseconds
     */
    public ExpiringMap(long expiration, final Function<Pair<K,V>, ?> postExpireHook)
    {
        ....
        timer = new Timer(""EXPIRING-MAP-TIMER-"" + (++counter), true);
        TimerTask task = new TimerTask()
        {
            public void run()
            {
                long start = System.currentTimeMillis();
                for (Map.Entry<K, CacheableObject<V>> entry : cache.entrySet())
                {
                    if (entry.getValue().isReadyToDie(start))
                    {
                        cache.remove(entry.getKey());
                        if (postExpireHook != null)
                            postExpireHook.apply(new Pair<K, V>(entry.getKey(), entry.getValue().getValue()));
                    }
                }
            }
        };
        timer.schedule(task, expiration / 2, expiration / 2);
    }
{code} 

The code above exist a bug . If a exception occured in the run method ,the timer will be killed and no longer to expire the value in cache .
I find that if there is no ""EXPIRING-MAP-TIMER- x"" thread in the memory  when there is a  heap problem , and if there is a ""EXPIRING-MAP-TIMER- x"" thread in the memory   there is no heap problem .;;;","05/Dec/11 14:10;jbellis;That's true, but unless you're seeing exceptions in the log it should be a non-issue.;;;","06/Dec/11 11:36;jonma;{quote}
Jonathan Ellis added a comment - 05/Dec/11 14:10 
That's true, but unless you're seeing exceptions in the log it should be a non-issue.
{quote}
2011-12-06 06:50:23,383 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[EXPIRING-MAP-TIMER-1,5,main]
java.lang.AssertionError: /xxx.xxx.xx.xx
      at org.apache.cassandra.service.StorageProxy.scheduleLocalHint(StorageProxy.java:337)
      at org.apache.cassandra.net.MessagingService.scheduleMutationHint(MessagingService.java:199)
      at org.apache.cassandra.net.MessagingService.access$500(MessagingService.java:62)
      at org.apache.cassandra.net.MessagingService$2.apply(MessagingService.java:173)
      at org.apache.cassandra.net.MessagingService$2.apply(MessagingService.java:150)
      at org.apache.cassandra.utils.ExpiringMap$1.run(ExpiringMap.java:89)
      at java.util.TimerThread.mainLoop(Timer.java:512)
      at java.util.TimerThread.run(Timer.java:462);;;","06/Dec/11 13:35;jbellis;That's CASSANDRA-3440, you should upgrade.;;;","07/Dec/11 03:38;jonma;Thanks very much . Helps a lot .;;;","22/Feb/12 10:59;slebresne;Simple patch attached to make expiring map more exception proof.;;;","22/Feb/12 11:15;jonma;
{quote}
Simple patch attached to make expiring map more exception proof.
{quote}
Please catch Throwable  , not just only catch Exception. It may have Errors in try-catch block.;;;","22/Feb/12 14:51;jbellis;catching Throwable or Error is generally Bad because most Errors are not recoverable. OutOfMemoryError is the most common. We have a global exception hook set up to shut down the process for those.;;;","22/Feb/12 17:00;jbellis;v2 attached that address this for DSTPE in general.  Also fixes a double-log of RuntimeExceptions from ThreadPoolExecutor in our DTPE afterExecute code.

The culprit for this last part is this fragment from TPE.runTask:
{code}
.               try {
                    task.run();
                    ran = true;
                    afterExecute(task, null);
                    ++completedTasks;
                } catch (RuntimeException ex) {
                    if (!ran)
                        afterExecute(task, ex);
                    throw ex;
                }
{code}

That is, for ""raw"" exceptions thrown by the Task (and not wrapped inside FutureTask), runTask will already re-throw the exception, allowing our default uncaught exception handler to log it.  So we don't want to double-log that in DTPE.;;;","22/Feb/12 17:59;slebresne;I like the idea of using DSTPE. I'm not sure I understand why LoggingScheduledFuture is useful though. STPE already wraps runnables into FutureTask, which catches Throwable and turn them into ExecutionException. So DSTPE.afterExecute() should already get everything.

However, I think we may need the kind of wrapper LoggingScheduledFuture implements for DPTE, since if we use the TPE.execute() method (which we do), then the runnable is not wrapped by a FutureTask and thrown exception will be thrown in TPE.runTask. And since TPE.runTask only catches RuntimeException...

As a side note, I notice your patch is against 1.1 which I'm good with but we may want to change the fix version then.;;;","22/Feb/12 19:24;jbellis;bq. I'm not sure I understand why LoggingScheduledFuture is useful though

The problem I'm trying to solve is, the core of STPE.ScheduledFutureTask:

{code}
.       private void runPeriodic() {
            boolean ok = ScheduledFutureTask.super.runAndReset();
            boolean down = isShutdown();
            // Reschedule if not cancelled and not shutdown or policy allows
            if (ok && (!down ||
                       (getContinueExistingPeriodicTasksAfterShutdownPolicy() &&
                        !isStopped()))) {
                long p = period;
                if (p > 0)
                    time += p;
                else
                    time = triggerTime(-p);
                ScheduledThreadPoolExecutor.super.getQueue().add(this);
            }
            // This might have been the final executed delayed
            // task.  Wake up threads to check.
            else if (down)
                interruptIdleWorkers();
        }
{code}

In other words, ""If any execution of the task encounters an exception, subsequent executions are suppressed.""

That said, you're right that a second wrapper doesn't solve this problem.  Not sure what to do here.  Use reflection to reach into the SFT in afterExecute, and re-schedule the original Runnable?

bq. if we use the TPE.execute() method (which we do), then the runnable is not wrapped by a FutureTask and thrown exception will be thrown in TPE.runTask

That's fine though, since TPE.execute is inherently non-repeating.;;;","23/Feb/12 09:48;slebresne;bq. In other words, ""If any execution of the task encounters an exception, subsequent executions are suppressed.""

I see.

bq. Use reflection to reach into the SFT in afterExecute, and re-schedule the original Runnable?

That would be one option I suppose. Another would be to override all the schedule() method to wrap the runnable into a wrapper that would resubmit the task on exception (we would avoid reflection, not sure it's simpler though).

bq. That's fine though, since TPE.execute is inherently non-repeating.

True.;;;","23/Feb/12 14:14;jbellis;bq. Another would be to override all the schedule() method to wrap the runnable 

That's going to require reflection as well since SFT is private. :);;;","23/Feb/12 14:23;slebresne;No I meant that we would wrap the initial Runnable into another Runnable that catches every exception of the first one and log them. In other words, as far as STPE is concerned, our runnable would never ever fail (and thus would never be suppressed).;;;","23/Feb/12 17:46;jbellis;That makes sense.  v4 attached with this approach.;;;","23/Feb/12 17:47;jbellis;(And yes, I think this should target 1.1 instead of 1.0.9 since messing with executors scares me a bit.);;;","24/Feb/12 09:59;slebresne;+1 (nit: the overriden schedule method misses their @Override);;;","24/Feb/12 15:26;jbellis;Added @Override and committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion error during bootstraping cassandra,CASSANDRA-3536,12532941,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,ramesh25,ramesh25,28/Nov/11 17:44,16/Apr/19 09:32,14/Jul/23 05:52,02/Dec/11 15:47,1.0.6,,,,,,1,compaction,,," I have a 3 node cassandra cluster. I have RF set to 3 and do reads
and writes using QUORUM.

Here is my initial ring configuration

[root@CAP4-CNode1 ~]# /root/cassandra/bin/nodetool -h localhost ring
Address         DC          Rack        Status State   Load
Owns    Token

       113427455640312821154458202477256070484
10.19.104.11    datacenter1 rack1       Up     Normal  1.66 GB
33.33%  0
10.19.104.12    datacenter1 rack1       Up     Normal  1.06 GB
33.33%  56713727820156410577229101238628035242
10.19.104.13    datacenter1 rack1       Up     Normal  1.61 GB
33.33%  113427455640312821154458202477256070484

I want to add 10.19.104.14 to the cluster.

I edited the 10.19.104.14 cassandra.yaml file and set the token to
127605887595351923798765477786913079296 and set auto_bootstrap to
true.

When I started cassandra I am getting Assertion Error.  

thanks
Ramesh




[root@CAP4-CNode4 cassandra]#  INFO 10:29:46,093 Logging initialized
 INFO 10:29:46,099 JVM vendor/version: Java HotSpot(TM) 64-Bit Server
VM/1.6.0_25
 INFO 10:29:46,100 Heap size: 8304721920/8304721920
 INFO 10:29:46,100 Classpath:
bin/../conf:bin/../build/classes/main:bin/../build/classes/thrift:bin/../lib/antlr-3.2.jar:bin/../lib/apache-cassandra-1.0.2.jar:bin/../lib/apache-cassandra-clientutil-1.0.2.jar:bin/../lib/apache-cassandra-thrift-1.0.2.jar:bin/../lib/avro-1.4.0-fixes.jar:bin/../lib/avro-1.4.0-sources-fixes.jar:bin/../lib/commons-cli-1.1.jar:bin/../lib/commons-codec-1.2.jar:bin/../lib/commons-lang-2.4.jar:bin/../lib/compress-lzf-0.8.4.jar:bin/../lib/concurrentlinkedhashmap-lru-1.2.jar:bin/../lib/guava-r08.jar:bin/../lib/high-scale-lib-1.1.2.jar:bin/../lib/jackson-core-asl-1.4.0.jar:bin/../lib/jackson-mapper-asl-1.4.0.jar:bin/../lib/jamm-0.2.5.jar:bin/../lib/jline-0.9.94.jar:bin/../lib/jna.jar:bin/../lib/json-simple-1.1.jar:bin/../lib/libthrift-0.6.jar:bin/../lib/log4j-1.2.16.jar:bin/../lib/mx4j-examples.jar:bin/../lib/mx4j-impl.jar:bin/../lib/mx4j.jar:bin/../lib/mx4j-jmx.jar:bin/../lib/mx4j-remote.jar:bin/../lib/mx4j-rimpl.jar:bin/../lib/mx4j-rjmx.jar:bin/../lib/mx4j-tools.jar:bin/../lib/servlet-api-2.5-20081211.jar:bin/../lib/slf4j-api-1.6.1.jar:bin/../lib/slf4j-log4j12-1.6.1.jar:bin/../lib/snakeyaml-1.6.jar:bin/../lib/snappy-java-1.0.4.1.jar:bin/../lib/jamm-0.2.5.jar
 INFO 10:29:48,713 JNA mlockall successful
 INFO 10:29:48,726 Loading settings from
file:/root/apache-cassandra-1.0.2/conf/cassandra.yaml
 INFO 10:29:48,883 DiskAccessMode 'auto' determined to be mmap,
indexAccessMode is mmap
 INFO 10:29:48,898 Global memtable threshold is enabled at 2640MB
 INFO 10:29:49,203 Couldn't detect any schema definitions in local storage.
 INFO 10:29:49,204 Found table data in data directories. Consider
using the CLI to define your schema.
 INFO 10:29:49,220 Creating new commitlog segment
/var/lib/cassandra/commitlog/CommitLog-1321979389220.log
 INFO 10:29:49,227 No commitlog files found; skipping replay
 INFO 10:29:49,230 Cassandra version: 1.0.2
 INFO 10:29:49,230 Thrift API version: 19.18.0
 INFO 10:29:49,230 Loading persisted ring state
 INFO 10:29:49,235 Starting up server gossip
 INFO 10:29:49,259 Enqueuing flush of
Memtable-LocationInfo@122130810(192/240 serialized/live bytes, 4 ops)
 INFO 10:29:49,260 Writing Memtable-LocationInfo@122130810(192/240
serialized/live bytes, 4 ops)
 INFO 10:29:49,317 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-1-Data.db (300 bytes)
 INFO 10:29:49,340 Starting Messaging Service on port 7000
 INFO 10:29:49,349 JOINING: waiting for ring and schema information
 INFO 10:29:50,759 Applying migration
4b0e20f0-1511-11e1-0000-c11bc95834d7 Add keyspace: MSA, rep
strategy:SimpleStrategy{}, durable_writes: true
 INFO 10:29:50,761 Enqueuing flush of
Memtable-Migrations@1507565381(6744/8430 serialized/live bytes, 1 ops)
 INFO 10:29:50,761 Writing Memtable-Migrations@1507565381(6744/8430
serialized/live bytes, 1 ops)
 INFO 10:29:50,761 Enqueuing flush of
Memtable-Schema@1498835564(2889/3611 serialized/live bytes, 3 ops)
 INFO 10:29:50,776 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-1-Data.db (6808 bytes)
 INFO 10:29:50,777 Writing Memtable-Schema@1498835564(2889/3611
serialized/live bytes, 3 ops)
 INFO 10:29:50,797 Completed flushing
/var/lib/cassandra/data/system/Schema-h-1-Data.db (3039 bytes)
 INFO 10:29:50,814 Applying migration
4b6f2cb0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@1639d811[cfId=1000,ksName=MSA,cfName=modseq,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=5000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@2f984f7d,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:50,815 Enqueuing flush of
Memtable-Migrations@948613108(7482/9352 serialized/live bytes, 1 ops)
 INFO 10:29:50,816 Writing Memtable-Migrations@948613108(7482/9352
serialized/live bytes, 1 ops)
 INFO 10:29:50,816 Enqueuing flush of
Memtable-Schema@421910828(3294/4117 serialized/live bytes, 3 ops)
 INFO 10:29:50,831 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-2-Data.db (7546 bytes)
 INFO 10:29:50,832 Writing Memtable-Schema@421910828(3294/4117
serialized/live bytes, 3 ops)
 INFO 10:29:50,846 Completed flushing
/var/lib/cassandra/data/system/Schema-h-2-Data.db (3444 bytes)
 INFO 10:29:50,854 Applying migration
4b8c9fc0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@1bd97d0d[cfId=1001,ksName=MSA,cfName=msgid,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@63a0eec3,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO 10:29:50,855 Enqueuing flush of
Memtable-Migrations@1520138062(7750/9687 serialized/live bytes, 1 ops)
 INFO 10:29:50,856 Writing Memtable-Migrations@1520138062(7750/9687
serialized/live bytes, 1 ops)
 INFO 10:29:50,856 Enqueuing flush of
Memtable-Schema@347459675(3630/4537 serialized/live bytes, 3 ops)
 INFO 10:29:50,878 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-3-Data.db (7814 bytes)
 INFO 10:29:50,879 Writing Memtable-Schema@347459675(3630/4537
serialized/live bytes, 3 ops)
 INFO 10:29:50,894 Completed flushing
/var/lib/cassandra/data/system/Schema-h-3-Data.db (3780 bytes)
 INFO 10:29:50,900 Applying migration
4ba1ae60-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@6a095b8a[cfId=1002,ksName=MSA,cfName=participants,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@c58f769,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO 10:29:50,900 Enqueuing flush of
Memtable-Migrations@618337492(8194/10242 serialized/live bytes, 1 ops)
 INFO 10:29:50,901 Writing Memtable-Migrations@618337492(8194/10242
serialized/live bytes, 1 ops)
 INFO 10:29:50,902 Enqueuing flush of
Memtable-Schema@724860211(4020/5025 serialized/live bytes, 3 ops)
 INFO 10:29:50,917 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-4-Data.db (8258 bytes)
 INFO 10:29:50,918 Writing Memtable-Schema@724860211(4020/5025
serialized/live bytes, 3 ops)
 INFO 10:29:50,925 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-3-Data.db')]
 INFO 10:29:50,934 Completed flushing
/var/lib/cassandra/data/system/Schema-h-4-Data.db (4170 bytes)
 INFO 10:29:50,935 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-3-Data.db')]
 INFO 10:29:50,940 Applying migration
4bb4e840-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@318c69a9[cfId=1003,ksName=MSA,cfName=subinfo,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=5000.0,keyCacheSize=5000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=14400,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@796cefa8,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:50,941 Enqueuing flush of
Memtable-Migrations@1682081063(8618/10772 serialized/live bytes, 1
ops)
 INFO 10:29:50,941 Writing Memtable-Migrations@1682081063(8618/10772
serialized/live bytes, 1 ops)
 INFO 10:29:50,941 Enqueuing flush of
Memtable-Schema@1083461053(4427/5533 serialized/live bytes, 3 ops)
 INFO 10:29:50,977 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-5-Data.db (8682 bytes)
 INFO 10:29:50,978 Writing Memtable-Schema@1083461053(4427/5533
serialized/live bytes, 3 ops)
 INFO 10:29:50,991 Compacted to
[/var/lib/cassandra/data/system/Schema-h-5-Data.db,].  14,433 to
14,106 (~97% of original) bytes for 5 keys at 0.269051MB/s.  Time:
50ms.
 INFO 10:29:50,995 Completed flushing
/var/lib/cassandra/data/system/Schema-h-7-Data.db (4577 bytes)
 INFO 10:29:51,000 Applying migration
4bc6e9a0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@20b00ec2[cfId=1004,ksName=MSA,cfName=transactions,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=0.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@698f352,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:51,001 Enqueuing flush of
Memtable-Migrations@596545504(9027/11283 serialized/live bytes, 1 ops)
 INFO 10:29:51,002 Writing Memtable-Migrations@596545504(9027/11283
serialized/live bytes, 1 ops)
 INFO 10:29:51,003 Enqueuing flush of
Memtable-Schema@1686621532(4835/6043 serialized/live bytes, 3 ops)
 INFO 10:29:51,029 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-7-Data.db (9091 bytes)
 INFO 10:29:51,029 Writing Memtable-Schema@1686621532(4835/6043
serialized/live bytes, 3 ops)
 INFO 10:29:51,031 Compacted to
[/var/lib/cassandra/data/system/Migrations-h-6-Data.db,].  30,426 to
30,234 (~99% of original) bytes for 1 keys at 0.272013MB/s.  Time:
106ms.
 INFO 10:29:51,044 Completed flushing
/var/lib/cassandra/data/system/Schema-h-8-Data.db (4985 bytes)
 INFO 10:29:51,049 Applying migration
4bd76460-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@4ab4faeb[cfId=1005,ksName=MSA,cfName=uid,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1500000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@2fc5809e,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:51,050 Enqueuing flush of
Memtable-Migrations@1333730706(9421/11776 serialized/live bytes, 1
ops)
 INFO 10:29:51,050 Writing Memtable-Migrations@1333730706(9421/11776
serialized/live bytes, 1 ops)
 INFO 10:29:51,051 Enqueuing flush of
Memtable-Schema@577668356(5236/6545 serialized/live bytes, 3 ops)
 INFO 10:29:51,065 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-9-Data.db (9485 bytes)
 INFO 10:29:51,066 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-6-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-9-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-7-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-5-Data.db')]
 INFO 10:29:51,066 Writing Memtable-Schema@577668356(5236/6545
serialized/live bytes, 3 ops)
 INFO 10:29:51,081 Completed flushing
/var/lib/cassandra/data/system/Schema-h-9-Data.db (5386 bytes)
 INFO 10:29:51,083 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-5-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-9-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-8-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-7-Data.db')]
 INFO 10:29:51,114 Compacted to
[/var/lib/cassandra/data/system/Schema-h-10-Data.db,].  29,054 to
28,727 (~98% of original) bytes for 8 keys at 0.913207MB/s.  Time:
30ms.
 INFO 10:29:51,144 Compacted to
[/var/lib/cassandra/data/system/Migrations-h-10-Data.db,].  57,492 to
57,300 (~99% of original) bytes for 1 keys at 0.700584MB/s.  Time:
78ms.
 INFO 10:29:51,410 Node /10.19.104.13 is now part of the cluster
 INFO 10:29:51,412 InetAddress /10.19.104.13 is now UP
 INFO 10:29:51,414 Enqueuing flush of
Memtable-LocationInfo@709342045(35/43 serialized/live bytes, 1 ops)
 INFO 10:29:51,415 Writing Memtable-LocationInfo@709342045(35/43
serialized/live bytes, 1 ops)
 INFO 10:29:51,428 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-2-Data.db (89 bytes)
 INFO 10:29:51,439 Node /10.19.104.12 is now part of the cluster
 INFO 10:29:51,439 InetAddress /10.19.104.12 is now UP
 INFO 10:29:51,441 Enqueuing flush of
Memtable-LocationInfo@1292444743(35/43 serialized/live bytes, 1 ops)
 INFO 10:29:51,441 Writing Memtable-LocationInfo@1292444743(35/43
serialized/live bytes, 1 ops)
 INFO 10:29:51,455 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-3-Data.db (89 bytes)
 INFO 10:29:51,456 Node /10.19.104.11 is now part of the cluster
 INFO 10:29:51,457 InetAddress /10.19.104.11 is now UP
 INFO 10:29:51,459 Enqueuing flush of
Memtable-LocationInfo@1891328597(20/25 serialized/live bytes, 1 ops)
 INFO 10:29:51,459 Writing Memtable-LocationInfo@1891328597(20/25
serialized/live bytes, 1 ops)
 INFO 10:29:51,471 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-4-Data.db (74 bytes)
 INFO 10:29:51,473 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-3-Data.db')]
 INFO 10:29:51,497 Compacted to
[/var/lib/cassandra/data/system/LocationInfo-h-5-Data.db,].  552 to
444 (~80% of original) bytes for 3 keys at 0.018410MB/s.  Time: 23ms.
 INFO 10:30:19,349 JOINING: getting bootstrap token
 INFO 10:30:19,352 Enqueuing flush of
Memtable-LocationInfo@225265367(36/45 serialized/live bytes, 1 ops)
 INFO 10:30:19,353 Writing Memtable-LocationInfo@225265367(36/45
serialized/live bytes, 1 ops)
 INFO 10:30:19,364 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-7-Data.db (87 bytes)
 INFO 10:30:19,374 JOINING: sleeping 30000 ms for pending range setup
 INFO 10:30:49,375 JOINING: Starting to bootstrap...
ERROR 10:31:13,444 Fatal exception in thread Thread[Thread-49,5,main]
java.lang.AssertionError
       at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:178)
       at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
       at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:466)
       at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:275)
       at org.apache.cassandra.db.DataTracker.addSSTables(DataTracker.java:237)
       at org.apache.cassandra.db.DataTracker.addStreamedSSTable(DataTracker.java:242)
       at org.apache.cassandra.db.ColumnFamilyStore.addSSTable(ColumnFamilyStore.java:922)
       at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:141)
       at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:102)
       at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
       at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)","RHEL6 linux x86-64 guest on ESXi 5.0 (host),  kernel: 2.6.32-71.24.1.el6.x86_64, 8 cpu, 20GB RAM",eparusel,joelastpass,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/11 05:30;jbellis;3536.txt;https://issues.apache.org/jira/secure/attachment/12505850/3536.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,218673,,,Fri Dec 02 15:47:18 UTC 2011,,,,,,,,,,"0|i0gl47:",94851,,bcoverston,,bcoverston,Normal,,,,,,,,,,,,,,,,,"29/Nov/11 18:28;eparusel;I may have a similar, or the same problem.

On a node where I was running repair (with no other load), I am seeing these exceptions:

ERROR [Thread-90] 2011-11-29 17:44:22,753 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Thread-90,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:178)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:481)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:275)
        at org.apache.cassandra.db.DataTracker.addSSTables(DataTracker.java:237)
        at org.apache.cassandra.db.DataTracker.addStreamedSSTable(DataTracker.java:242)
        at org.apache.cassandra.db.ColumnFamilyStore.addSSTable(ColumnFamilyStore.java:920)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:141)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:103)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81);;;","02/Dec/11 05:30;jbellis;LeveledManifest.promote only really knows how to handle compaction -- swapping out one set of sstables, for another.  When adding a new sstable, we need to call LM.add instead.  Refactored DataTracker to do this correctly for streaming (and load-new-sstables-from-jmx); in the process, got rid of addStreamedSSTable (becomes just addSSTables(singletonlist) and added addInitialSSTables, to add to the tracker with no notifications (since Strategy creation loads the manifest separately).;;;","02/Dec/11 05:52;bcoverston;+1;;;","02/Dec/11 15:32;joelastpass;Thanks Jonathan, the patch works for me.;;;","02/Dec/11 15:47;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction cleanupIfNecessary costly when many files in data dir,CASSANDRA-3532,12532766,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,eparusel,eparusel,eparusel,25/Nov/11 22:58,16/Apr/19 09:32,14/Jul/23 05:52,02/Dec/11 21:01,1.0.6,,,,,,1,compaction,,,"From what I can tell SSTableWriter.cleanupIfNecessary seems increasingly costly as the number of files in the data dir increases.
It calls SSTable.componentsFor(descriptor, Descriptor.TempState.TEMP) which lists all files in the data dir to find matching components.

Am I roughly correct that   (cleanupCost = SSTable count * data dir size)?


We had been doing write load testing with default compaction throttling (16MB/s) and LeveledCompaction.
Unfortunately we haven't been keeping tabs on sstable counts and it grew out of control.

On a system with 300,000 sstables (!) here is an example of our compaction rate.  Note that as you're probably aware cleanupIfNecessary is included in the timing:

 INFO [CompactionExecutor:48] 2011-11-25 22:25:30,353 CompactionTask.java (line 213) Compacted to [/data1/cassandra/data/MA_DDR/indexes_03-hc-5369-Data.db,].  5,821,590 to 5,306,354 (~91% of original) bytes for 123 keys at 0.163755MB/s.  Time: 30,903ms.

Here's a slightly larger one:
 INFO [CompactionExecutor:43] 2011-11-25 22:23:28,956 CompactionTask.java (line 213) Compacted to [/data1/cassandra/data/MA_DDR/indexes_03-hc-5336-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5337-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5338-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5339-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5340-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5341-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5342-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5343-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5344-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5345-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5346-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5347-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5348-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5349-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5350-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5351-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5352-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5353-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5354-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5355-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5356-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5357-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5358-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5359-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5360-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5361-Data.db,].  140,706,512 to 137,990,868 (~98% of original) bytes for 2,181 keys at 0.338627MB/s.  Time: 388,623ms.


This is with compaction throttling set to 0 (Off).


So I believe because of this it's going to take a very long time to recover from having so many small sstables. 
It might be notable that we're using Solaris 10, possibly listFiles() is faster on other platforms?

Is it feasible to keep track of the temp files and just delete them rather than searching for them for each SSTable using SSTable.componentsFor()?



Here's the stack trace for the CompactionExecutor:14 thread that appears to be occupying the majority of the cpu time on this node:

Name: CompactionExecutor:14
State: RUNNABLE
Total blocked: 3  Total waited: 1,610,714

Stack trace: 
 java.io.UnixFileSystem.getBooleanAttributes0(Native Method)
java.io.UnixFileSystem.getBooleanAttributes(Unknown Source)
java.io.File.isDirectory(Unknown Source)
org.apache.cassandra.io.sstable.SSTable$3.accept(SSTable.java:204)
java.io.File.listFiles(Unknown Source)
org.apache.cassandra.io.sstable.SSTable.componentsFor(SSTable.java:200)
org.apache.cassandra.io.sstable.SSTableWriter.cleanupIfNecessary(SSTableWriter.java:289)
org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:189)
org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:57)
org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:134)
org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:114)
java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
java.util.concurrent.FutureTask.run(Unknown Source)
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
java.lang.Thread.run(Unknown Source)

No matter where I click in the busy Compaction thread timeline in YourKit it's in Running state and showing this above trace, except for short periods of time where it's actually compacting :)

Thanks,
Eric","Solaris 10, 1.0.4 release candidate",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/11 03:49;jbellis;3532-v2.txt;https://issues.apache.org/jira/secure/attachment/12505848/3532-v2.txt","02/Dec/11 20:22;jbellis;3532-v3.txt;https://issues.apache.org/jira/secure/attachment/12505926/3532-v3.txt","28/Nov/11 19:00;eparusel;3532.txt;https://issues.apache.org/jira/secure/attachment/12505369/3532.txt",,,,,,,,,,,,3.0,eparusel,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,218499,,,Mon Dec 12 22:07:38 UTC 2011,,,,,,,,,,"0|i0gl2f:",94843,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"26/Nov/11 07:04;jbellis;Looks like leveled compaction means that sstable creation can be part of the critical path now:

{noformat}
.   /**
     * Discovers existing components for the descriptor. Slow: only intended for use outside the critical path.
     */
    static Set<Component> componentsFor(final Descriptor desc, final Descriptor.TempState matchState)
{noformat}

bq. Is it feasible to keep track of the temp files and just delete them rather than searching for them for each SSTable using SSTable.componentsFor()?

Simplest would be to just check File.exists on the limited set of possible temp file names.  Next simplest and slightly more performant would be to move the cleanup out of the finally blocks, and into a catch block: the cleanup is a no-op if everything went well.;;;","27/Nov/11 10:21;eparusel;Thanks.
Do I need to try to delete ~all~ components, for that descriptor.asTemporary()?  Or just specific ones?
A Component of type BITMAP_INDEX requires an id, so I'm not sure how I'd find this out without listing the directory contents.;;;","28/Nov/11 14:32;jbellis;BITMAP_INDEX type was never completed (CASSANDRA-1472) so I'm fine with removing that code and doing whatever is simplest.  We can get more sophisticated if/when we get back to working on 1472.;;;","28/Nov/11 19:00;eparusel;Here's a small patch, and a few notes:

- I wasn't sure how to best document the interaction with BITMAP_INDEX, hope a TODO there is ok.
- I created a copy of descriptor.asTemporary(true).  Is descriptor always guaranteed to be temporary==true?  It left me a little uneasy.
- I used SSTable.delete (while checking ahead of time that the file exists), because I wasn't sure if ordering of deletes was important.
- SSTableReader.open() is the other method that calls SSTable.componentsFor, I only mention this because it may be a suboptimal call as well.;;;","29/Nov/11 00:44;eparusel;I applied the patch in our test environment against the 1.0.4 revision, and compaction is humming along nicely now.
On the (otherwise idle) node I'm watching that had 250000 data/ files, file count is decreasing by 1000-1900/minute.;;;","02/Dec/11 03:49;jbellis;v2 attached, with the new approach moved into componentsFor, so that open() can take advantage of the improvement too. Also, componentsFor now respects the temporary-ness of the descriptor passed, so a separate TempState enum is unnecessary.

Also renamed cleanupIfNecessary to abort, and moved to catch block as discussed above.;;;","02/Dec/11 17:37;eparusel;Thank you Jonathan -- I applied the patch and it works for me.
Cheers;;;","02/Dec/11 18:45;slebresne;* Wrapping exceptions into RuntimeException() blindly will confuse the catcher of UserInterruptedException in DTPE.logExceptionsAfterExecute. We could make make that catcher unwrap the full exception, but truth is I'm not a fan of wrapping exception needlessly. Maybe we could just add a silence method somewhere:
{noformat}
public void silence(Exception e)
{
    if (e instanceof RuntimeException)
        throw (RuntimeException) e;
    else
        throw new RuntimeException(e);
}
{noformat}
and use that (as we already do in WrappedRunnable actually).
* We could remove the bitmap indexes type while were at it (it'd be one less type to check).
;;;","02/Dec/11 20:22;jbellis;added FBUtilities.unchecked(Exception) as suggested, and removed BITMAP component.;;;","02/Dec/11 20:48;slebresne;+1;;;","02/Dec/11 21:01;jbellis;committed;;;","02/Dec/11 21:39;eparusel;Patch 3532-v3.txt applied here, works for me.  Thanks again!;;;","12/Dec/11 07:56;eparusel;Hmm, I might have spoken too soon.  This could also be a separate bug however.

The nodes in my cluster are using a lot of file descriptors, holding open tmp files.  A few are using 50K+, nearing their limit (on Solaris, of 64K).

Here's a small snippet of lsof:
java        828 appdeployer *146u  VREG          181,65540          0     333376 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776429-Data.db
java        828 appdeployer *147u  VREG          181,65540          0     332952 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776359-Data.db
java        828 appdeployer *148u  VREG          181,65540          0     333079 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776380-Index.db
java        828 appdeployer *149u  VREG          181,65540          0     333080 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776380-Data.db
java        828 appdeployer *150u  VREG          181,65540          0     333224 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776403-Index.db
java        828 appdeployer *151u  VREG          181,65540          0     333025 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776372-Data.db
java        828 appdeployer *152u  VREG          181,65540          0     333225 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776403-Data.db
java        828 appdeployer *154u  VREG          181,65540          0     333858 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776514-Index.db
java        828 appdeployer *155u  VREG          181,65540          0     333426 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776438-Data.db
java        828 appdeployer *156u  VREG          181,65540          0     333326 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776421-Data.db
java        828 appdeployer *157u  VREG          181,65540          0     333553 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776460-Data.db
java        828 appdeployer *158u  VREG          181,65540          0     333501 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776452-Index.db
java        828 appdeployer *159u  VREG          181,65540          0     333597 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776468-Index.db
java        828 appdeployer *160u  VREG          181,65540          0     333598 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776468-Data.db
java        828 appdeployer *162u  VREG          181,65540          0     333884 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776518-Data.db
java        828 appdeployer *163u  VREG          181,65540          0     333502 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776452-Data.db
java        828 appdeployer *165u  VREG          181,65540          0     333929 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776527-Index.db
java        828 appdeployer *166u  VREG          181,65540          0     333859 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776514-Data.db
java        828 appdeployer *167u  VREG          181,65540          0     333663 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776480-Data.db
java        828 appdeployer *168u  VREG          181,65540          0     333812 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776506-Index.db

I spot checked a few and found they still exist on the filesystem too:
-rw-r--r--   1 appdeployer appdeployer       0 Dec 12 07:16 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776506-Index.db;;;","12/Dec/11 13:59;jbellis;That does sound like a separate problem.  What do you see when you grep the log for messages_meta-tmp-hb-776506?;;;","12/Dec/11 16:07;eparusel;Sure.  Let me know if you'd prefer a separate ticket.

I don't see anything in the logs matching ""776506"".  Any suggestions as to which class(es) I could turn on DEBUG log level for (via JMX), if that would help troubleshoot?;;;","12/Dec/11 16:48;jbellis;Yes, separate ticket.

org.apache.cassandra.db.compaction, org.apache.cassandra.db.Memtable, org.apache.cassandra.db.DataTracker to start with;;;","12/Dec/11 22:07;eparusel;Separate ticket created: CASSANDRA-3616;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix crack-smoking in ConsistencyLevelTest,CASSANDRA-3531,12532705,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,25/Nov/11 11:37,16/Apr/19 09:32,14/Jul/23 05:52,04/Jan/12 03:17,1.0.7,,,Legacy/Testing,,,0,,,,"First, let's note that this test fails in current 1.0 branch. It was ""broken"" (emphasis on the quotes) by CASSANDRA-3529. But it's not CASSANDRA-3529 fault, it's only that the use of NonBlockingHashMap changed the order of the tables returned by Schema.instance.getNonSystemTables(). *And*,  it turns out that ConsistencyLevelTest bails out as soon as it has found one keyspace with rf >= 2 due to a misplaced return. So it use to be that ConsistencyLevelTest was only ran for Keyspace5 (whose RF is 2) for which the test work. But for any RF > 2, the test fails.

The reason of this failing is that the test creates a 3 node cluster for whom only 1 node is alive as far as the failure detector is concerned. So for RF=3 and CL=QUORUM, the writes are unavailable (the failure detector is queried), while for reads we ""pretend"" two nodes are alive so we end up with a case where isWriteUnavailable != isReadUnavailable.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,218438,,,Wed Jan 04 03:17:20 UTC 2012,,,,,,,,,,"0|i0gl1z:",94841,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"25/Nov/11 17:47;jbellis;I did some minimal-necessary work to get CLT to pass in trunk, but it sounds like you have deeper changes in mind.;;;","03/Jan/12 14:19;slebresne;The deeper change I have in mind consists roughly in removing that test. It's trying to tests the result of WriteHandler.assureSufficientLiveNodes() but that method depends on the result of the FailureDetector. The problem is that I don't think we really have a good way to create real multi-nodes cluster in the unit test. Maybe we can ""fake"" live nodes but I'm not sure how and in the end it makes me wonder what the tests is really testing if we're starting to fake too much stuff. It seems to me that the distributed tests are probably a better place to do that kind of thing.

In any case, It's really annoying to have unit tests failure, especially in the 1.0 branch. And as said in the description, that test never really worked anyway so any opposition to at least commenting it for now? ;;;","04/Jan/12 03:17;jbellis;You're right: while it's reasonable to unit-test assureSufficientLiveNodes, the right place to do that is just with IWriteResponseHandler objects instead of mocking up a ring.  This test also pre-dated NTS and the different IWRH for that, so it's pretty fragile.

Went ahead and deleted it per your suggestion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Header class not thread safe, but mutated by multiple threads",CASSANDRA-3530,12532664,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,sgbridges,sgbridges,25/Nov/11 00:11,16/Apr/19 09:32,14/Jul/23 05:52,25/Nov/11 09:55,1.0.4,,,,,,0,,,,"With Cassandra 1.0.3 we are getting exceptions like,

Fatal exception in thread Thread[WRITE-/xx.xx.xx.xx,5,main]java.util.ConcurrentModificationException        
        at java.util.Hashtable$Enumerator.next(Unknown Source)
        at org.apache.cassandra.net.Header.serializedSize(Header.java:97)        
        at org.apache.cassandra.net.OutboundTcpConnection.messageLength(OutboundTcpConnection.java:164)
        at org.apache.cassandra.net.OutboundTcpConnection.write(OutboundTcpConnection.java:154)        
        at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:115)        
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:94)

and,

ERROR [WRITE-/xx.xx.xx.xx] 2011-11-24 22:08:28,981 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[WRITE-/10.30.12.79,5,main]java.lang.NullPointerException        
        at org.apache.cassandra.net.Header.serializedSize(Header.java:101)
        at org.apache.cassandra.net.OutboundTcpConnection.messageLength(OutboundTcpConnection.java:164)
        at org.apache.cassandra.net.OutboundTcpConnection.write(OutboundTcpConnection.java:154)
        at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:115)        
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:94)

It looks like Header is not thread safe, but the same header instance is modified concurrently while being sent to several threads in StorageProxy.sendMessages. 

This bug eventually causes the node to OOM, as it kills the OutboundTcpConnection thread, which means nothing is dequeing from queue.
",,eparusel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/11 07:58;jbellis;3530-0.8.txt;https://issues.apache.org/jira/secure/attachment/12505074/3530-0.8.txt","25/Nov/11 07:43;jbellis;3530-v2.txt;https://issues.apache.org/jira/secure/attachment/12505073/3530-v2.txt","25/Nov/11 00:15;sgbridges;CASSANDRA-3530.patch;https://issues.apache.org/jira/secure/attachment/12505045/CASSANDRA-3530.patch",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,218397,,,Fri Nov 25 15:53:13 UTC 2011,,,,,,,,,,"0|i0gl1j:",94839,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"25/Nov/11 00:15;sgbridges;Patch makes Header nearly immutable.  setDetails and removeDetails now return a copy of the Header with the modifications, rather than modifying the original.;;;","25/Nov/11 03:38;jbellis;Are you running in a multi-DC environment?;;;","25/Nov/11 03:39;jbellis;bq. Patch makes Header nearly immutable

What are the changes in OutboundTcpConnection trying to do?;;;","25/Nov/11 04:22;sbridges;We are running in a multi dc environment.

The changes in OutboundTcpConnection prevent the thread from dieing in case of a RuntimeException.  When the thread dies the node eventually runs out of memory as the queue never gets emptied.;;;","25/Nov/11 07:43;jbellis;v2 attached (against 1.0 branch):

- simplifies OTC change to just catch Exception in existing try/catch in writeConnected
- some style updates to Header/Message changes, but functionality is unchanged
- updated StorageProxy to only need one header copy instead of the horribly inefficient process used previously;;;","25/Nov/11 07:49;jbellis;This affects 0.8 as well (and would have affected 0.7 if not for CASSANDRA-3472).;;;","25/Nov/11 07:58;jbellis;version of v2 for 0.8.  Omitted the OTC change since in 0.8 it's just shoving a bytebuffer over the wire, there's not really any non-IOExceptions to worry about.;;;","25/Nov/11 10:17;hudson;Integrated in Cassandra-0.8 #404 (See [https://builds.apache.org/job/Cassandra-0.8/404/])
    avoid race in OutboundTcpConnection in multi-DC setups
patch by jbellis; reviewed by slebresne for CASSANDRA-3530

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1206098
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/RowMutationVerbHandler.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/Header.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/Message.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
;;;","25/Nov/11 14:44;jbellis;Looking at this in the morning, I think I was wrong about it affecting 0.8, since all the Message wrangling (including the Message -> byte[] conversion in sendOneWay) happens on the StorageProxy thread.  I think we should revert from 0.8.8.;;;","25/Nov/11 15:33;slebresne;There is no StorageProxy thread per-se, is there? Why wouldn't 0.8 be subject to the same concurrency bug as reported on that issue?;;;","25/Nov/11 15:43;jbellis;Because in 0.8 there is only one thread touching any given Message in StorageProxy + MessagingService.  Once OTC gets it, MS has already turned it into a byte[] (which OTC then copies to its Socket buffer).

To avoid the unnecessary intermediate byte[], for 1.0 we switch to OTC getting the Message objects.  So in the multi-DC case you can have a 2nd thread (the OTC one) sending a Message, while the SP thread updates its Header.;;;","25/Nov/11 15:48;slebresne;Ok. I'm fine reverting this then, though I would also be fine keeping it in. I prefer the new copying methods, and it does improve the code a bit in StorageProxy, and I don't see many chance for this to introduce new bugs (but obviously there is always one).;;;","25/Nov/11 15:53;jbellis;I'm going to revert then; if I hadn't incorrectly thought this bug affected 0.8, it would be clear we shouldn't backport it just to clean things up a bit.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException in Table.all(),CASSANDRA-3529,12532653,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,24/Nov/11 20:36,16/Apr/19 09:32,14/Jul/23 05:52,24/Nov/11 20:54,1.0.4,,,,,,0,,,,"{noformat}
    [junit] java.util.ConcurrentModificationException
    [junit]     at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
    [junit]     at java.util.HashMap$KeyIterator.next(HashMap.java:828)
    [junit]     at com.google.common.collect.Iterators$8.next(Iterators.java:750)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1509)
    [junit]     at org.apache.cassandra.db.MeteredFlusher.countFlushingBytes(MeteredFlusher.java:118)
    [junit]     at org.apache.cassandra.db.MeteredFlusher.run(MeteredFlusher.java:45)
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit]     at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
    [junit]     at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
    [junit]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
    [junit]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
    [junit]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit]     at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/11 20:38;jbellis;3529.txt;https://issues.apache.org/jira/secure/attachment/12505036/3529.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,218386,,,Thu Nov 24 20:54:31 UTC 2011,,,,,,,,,,"0|i0gl13:",94837,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"24/Nov/11 20:38;jbellis;Patch to make Schema.tables a concurrent map;;;","24/Nov/11 20:54;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fatal exception in thread Thread ,CASSANDRA-3522,12532356,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,grosendorf,grosendorf,22/Nov/11 15:55,16/Apr/19 09:32,14/Jul/23 05:52,28/Nov/11 15:22,1.0.3,,,,,,0,,,,"Seeing this recurring exception on all machines in a 5 node cluster.  Recently upgraded to 1.0.0 from 0.7.2. 

ERROR [MutationStage:20] 2011-11-22 15:25:55,758 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:20,5,main]
java.lang.AssertionError
	at org.apache.cassandra.locator.TokenMetadata.getToken(TokenMetadata.java:273)
	at org.apache.cassandra.service.StorageProxy$4.run(StorageProxy.java:350)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
",Ubuntu 10.04 LTS - 8GB mem/ 360 GB disk - Hosted by RS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,218089,,,Tue Nov 22 21:45:18 UTC 2011,,,,,,,,,,"0|i0gkxz:",94823,,,,,Low,,,,,,,,,,,,,,,,,"22/Nov/11 16:05;jbellis;please upgrade to 1.0.3 to verify that this isn't an already-fixed bug like CASSANDRA-2961;;;","22/Nov/11 21:17;grosendorf;Upgraded to 1.0.3, still getting the same exception on all machines.  Disk usage is also going up quickly on all the servers.;;;","22/Nov/11 21:26;jbellis;What does nodetool ring look like?;;;","22/Nov/11 21:30;grosendorf;Looks like this on one node:

Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               141784319550391032739561396922763706368     
10.183.65.16    datacenter1 rack1       Up     Normal  129.55 GB       16.67%  0                                           
10.183.47.195   datacenter1 rack1       Up     Normal  181.65 GB       33.33%  56713727820156407428984779325531226112      
10.182.96.44    datacenter1 rack1       Up     Normal  165.17 GB       16.67%  85070591730234615865843651857942052864      
10.179.64.134   datacenter1 rack1       Up     Normal  128.11 GB       16.67%  113427455640312814857969558651062452224     
10.183.65.18    datacenter1 rack1       Up     Normal  140.6 GB        16.67%  141784319550391032739561396922763706368 

But on another node, it all of a sudden thinks that a node that I decommissioned yesterday is still part of the ring, just down:

Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               141784319550391032739561396922763706368     
10.183.65.16    datacenter1 rack1       Up     Normal  129.55 GB       16.67%  0                                           
10.183.47.195   datacenter1 rack1       Up     Normal  181.65 GB       33.33%  56713727820156407428984779325531226112      
10.182.96.44    datacenter1 rack1       Up     Normal  165.17 GB       16.67%  85070591730234615865843651857942052864      
10.179.64.134   datacenter1 rack1       Up     Normal  128.11 GB       16.67%  113427455640312814857969558651062452224     
10.179.77.102   datacenter1 rack1       Down   Normal  ?               7.61%   126376470034382387444230631795475767049     
10.183.65.18    datacenter1 rack1       Up     Normal  140.6 GB        9.06%   141784319550391032739561396922763706368

;;;","22/Nov/11 21:31;grosendorf;Also, since the upgrade from 1.0.0 to 1.0.3 I'm getting Hinted Handoff Exceptions:


 INFO [HintedHandoff:4] 2011-11-22 21:28:48,046 HintedHandOffManager.java (line 268) Started hinted handoff for token: 113427455640312814857969558651062452224 with IP: /10.179.64.134
ERROR [HintedHandoff:4] 2011-11-22 21:28:48,077 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:4,1,main]
java.lang.AssertionError
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:301)
	at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
	at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:353)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
;;;","22/Nov/11 21:37;jbellis;The HH assertion means you had some hint corruption from 1.0.0 (CASSANDRA-3466).  You should remove your Hint column families from the system/ keyspace.;;;","22/Nov/11 21:45;grosendorf;Ok, removing the Hint CFs took care of the Hinted Handoff exception. I ran nodetool removetoken on that errant token from the machine that I decommissioned yesterday, so nodetool ring is the same on all machines.  I did a full cluster restart, and so far I'm not seeing any exceptions.  
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader throwing exceptions when loading snapshot data from compressed CFs,CASSANDRA-3521,12532297,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,pvelas,pvelas,22/Nov/11 09:33,16/Apr/19 09:32,14/Jul/23 05:52,25/Nov/11 09:20,1.0.4,,,,,,0,compression,,,"Loaded data from snapshot then enabled  `sstable_compression: org.apache.cassandra.io.compress.SnappyCompressor`
Then flush, scrub and compact. I can see actual CompressionRatio in JMX Console and access my data without problems..

Now I snapshot compressed keyspace and when trying to load snapshot to another single node or different Keyspace (the same super CF structure with compression options enabled, even try to truncate snapshoted CFs.) I cant retrieve any records . 


sstableloader command with debug mode dont throw any errors and shows its streaming 

{quote}
sstableloader-cassandra_2/bin/sstableloader --debug Impressions_compressed/
{quote}


Node logs contains repeating the errors bellow.

{quote}
ERROR [Thread-319] 2011-11-22 09:56:01,931 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Thread-319,5,main]
java.lang.AssertionError: attempted to delete non-existing file HidSaid-tmp-hb-260-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
        at org.apache.cassandra.streaming.IncomingStreamReader.retry(IncomingStreamReader.java:170)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:92)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)
 INFO [Thread-320] 2011-11-22 09:56:02,492 StreamInSession.java (line 120) Streaming of file Impressions_compressed/HidSaid-hb-9-Data.db sections=1 progress=0/5616749 - 0% from org.apache.cassandra.streaming.StreamInSession@3cc62c07 failed: requesting a retry.
ERROR [Thread-320] 2011-11-22 09:56:02,493 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Thread-320,5,main]
java.lang.AssertionError: attempted to delete non-existing file HidSaid-tmp-hb-261-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
        at org.apache.cassandra.streaming.IncomingStreamReader.retry(IncomingStreamReader.java:170)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:92)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)
{quote}

Hope its enough if you need more info just tell me what you need to reproduce this bug.","One node cluster (same problem with 4 nodes ) and dedicated machine for sstable loader.
node and loader running jdk1.6.0_29 and cassandra-1.0.3",pvelas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/11 06:14;yukim;3521.txt;https://issues.apache.org/jira/secure/attachment/12505061/3521.txt",,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,218030,,,Fri Nov 25 09:20:19 UTC 2011,,,,,,,,,,"0|i0gkxj:",94821,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"22/Nov/11 16:07;jbellis;If sstableloader says it's streaming, what does ""I can't load any data"" mean?;;;","22/Nov/11 17:34;pvelas;progress line is updating and bitrate is changing but it never ends. And my CF is still empty.

{noformat}
sstableloader-cassandra_2/bin/sstableloader --debug Impressions_compressed
Starting client (and waiting 30 seconds for gossip) ...
Streaming revelant part of Impressions_compressed/Hid-hb-9-Data.db Impressions_compressed/Impression-hb-11-Data.db Impressions_compressed/HidSaid-hb-9-Data.db to [/10.20.30.135]

progress: [/10.20.30.135 0/3 (3851)] [total: 3851 - 7MB/s (avg: 6MB/s)]
{noformat}

{noformat}
[default@Impressions_compressed] list Impression;
Using default limit of 100

0 Row Returned.
{noformat}



When I tried to list files from OS point of view (loading only one CF right now)
Its looks like some loop . Streaming data to file and when it reach 200 MB start with new file and increase by 1.

{noformat}
[root@cass101 Impressions_compressed]# ls -sh1
total 193M
 28K Impression-tmp-hb-59-CompressionInfo.db
193M Impression-tmp-hb-59-Data.db
   0 Impression-tmp-hb-59-Index.db

[root@cass101 Impressions_compressed]# ls -sh1
total 2.0M
4.0K Impression-tmp-hb-60-CompressionInfo.db
2.0M Impression-tmp-hb-60-Data.db
   0 Impression-tmp-hb-60-Index.db
{noformat}

Hope now its better described .;;;","22/Nov/11 18:54;pvelas;Better formulation would be ""I cant retrieve any records with cassandra-cli."";;;","25/Nov/11 06:14;yukim;Thanks for the detail, Peter.
Current version of sstableloader seems it does not pick up sstable's compression info before start streaming, and that causes receiver node handle streaming data incorrectly.
Attached patch let sstableloader to pick up compression information if available.;;;","25/Nov/11 09:20;slebresne;+1, committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit test are hanging on 0.8 branch,CASSANDRA-3520,12532289,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,22/Nov/11 08:42,16/Apr/19 09:32,14/Jul/23 05:52,28/Nov/11 16:39,0.8.8,,,Legacy/Testing,,,0,,,,"As the summary says, the unit test on current 0.8 are just hanging after CliTest (it's apparently not the case on windows, but it is on Linux and MacOSX).
Not sure what's going on, but what I can tell is that it's enough to run CliTest to have it hang after the test successfully pass (i.e, JUnit just wait indefinitely for the VM to exit). Even weirder, it seems that it is the counter increment in the CliTest that make it hang, if you comment those statement, it stop hanging. However, nothing seems to go wrong with the increment itself (the test passes) and it doesn't even trigger anything (typically sendToHintedEndpoint is not called because there is only one node).
Looking at the stack when the VM is hanging (attached), there is nothing specific to counters in there, and nothing that struck me at odd (but I could miss something). There do is a few thrift thread running (CASSANDRA-3335), but why would that only be a problem for the tests in that situation is a mystery to me.",Linux,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/11 15:22;slebresne;0001-Use-durable-writes-for-system-ks.patch;https://issues.apache.org/jira/secure/attachment/12505117/0001-Use-durable-writes-for-system-ks.patch","28/Nov/11 11:13;slebresne;3520.patch;https://issues.apache.org/jira/secure/attachment/12505320/3520.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,218022,,,Mon Nov 28 16:39:05 UTC 2011,,,,,,,,,,"0|i0gkx3:",94819,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"22/Nov/11 14:04;jbellis;Does it hang if you just run CliTest, or only for the whole suite?;;;","22/Nov/11 14:08;slebresne;Only CliTest. I honestly didn't really try to run other tests (and CliTest is like the 3rd test), so possibly there is other tests failing.;;;","22/Nov/11 15:34;jbellis;On Windows CliTest on 0.8 times out, after all the tests pass, but it does not hang.;;;","22/Nov/11 15:34;jbellis;(My 0.8 checkout was on the 0.8.6 tag when I reported that CliTest worked completely.);;;","25/Nov/11 15:22;slebresne;So, some form of progress here. The hanging can be bisected to r1185961 (svn). And it is actually due to the switch to non-durable writes for the system keyspace. But I don't know why yet. In particular 1.0 and trunk also use non-durable writes and have no such problem.

I'm attaching a small patch to re-enable durable writes. I think we should figure out what is going on, but if we want to go ahead with the release of 0.8.8 in the meantime, we could apply that.

Last info, CliTest is not the only one to hang, CleanupTest and AntiEntropyServiceCounterTest.java are also hanging.
;;;","25/Nov/11 15:56;slebresne;btw, how sure are we that using non-durable writes for the system keyspace is a good idea? It doesn't seem great for hints at least. Moreover on 1.0, when will the CF be flushed if durable_writes is false unless we do a manual flush?;;;","25/Nov/11 16:54;jbellis;Setting durable_writes back to true does not fix CliTest timing out on windows.  However, I can't think a good reason to have it off for the system KS so I committed 0001.;;;","25/Nov/11 17:13;hudson;Integrated in Cassandra-0.8 #406 (See [https://builds.apache.org/job/Cassandra-0.8/406/])
    set system keyspace back to durable_writes
patch by slebresne; reviewed by jbellis for CASSANDRA-3520

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1206257
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/KSMetaData.java
;;;","25/Nov/11 17:42;jbellis;I can confirm that r1185960 is the last revision that doesn't timeout CliTest.  (r1185961 doesn't compile, and r1185963 times out.);;;","28/Nov/11 09:50;slebresne;I was a bit fast at calling it victory. I was testing with CleanupTest that was hanging as well, and starting from the same revision, so I assumed that the problem was the exact same as for CliTest. Turns out setting durable writes does fix both CleanupTest and AntiEntropyServiceCounterTest, but CliTest is still hanging (it's now the only one to do so however).;;;","28/Nov/11 11:13;slebresne;So, the whole problem is due to our handling of non durable writes in the shutdown hook. For those, we flush the CFS as part of shutdown. However, flush tries to grab a commitlog context, which blocks because the commit log has been shutdown *before* all this (and for some reason, executor.submit() don't throw any exception if the executor is shutdown).

The reason why r1185960 was triggering this is that it actually fixed a bug by which previously to this commit, adding a new column family to a keyspace would reset the durableWrites option to true, hence hiding the bug as far as CliTest is concerned.

One simple solution is to move the commit log shutdown after the flushes of the non-durable CFs (which 1.0 does, and that's why it isn't affected). Truth is, it doesn't feel like the right fix in that non-durable CF shouldn't query the commit log at all, even during flushes. However, changing that introduces the possibility to have some CL segment retained forever when upgrading a keyspace from non-durable to durable if we're not careful. So overall just pushing the CL shutdown down in the shutdown hook to match 1.0 seems good enough, at least for 0.8. Attaching a patch to do just that. We can then look at making things cleaner with respect to flushing non-durable CFS in 1.0/trunk if we so wish.

Note that while having a non-durable system keyspace was not directly the problem, I think it was a fairly bad idea, and we should leave it to durable for 0.8 and turn it back to durable for 1.0 and trunk.
;;;","28/Nov/11 14:28;jbellis;+1, but can we add a system test w/ a non-durable ks to help prevent regressions?

Edit: never mind, that's what we're already doing in NoCommitlogSpace, hence the continuing clitest timeout;;;","28/Nov/11 15:21;hudson;Integrated in Cassandra-0.8 #407 (See [https://builds.apache.org/job/Cassandra-0.8/407/])
    Shutdown CL after having flushed non-durable CF
patch by slebresne; reviewed by jbellis for CASSANDRA-3520

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1207262
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
;;;","28/Nov/11 16:39;slebresne;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException in FailureDetector,CASSANDRA-3519,12532284,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,22/Nov/11 07:38,16/Apr/19 09:32,14/Jul/23 05:52,22/Nov/11 10:11,0.8.8,1.0.4,,,,,0,,,,"Noticed in a 2 DC cluster, error was on node in DC 2 streaming to a node in DC 1. 

{code:java}

INFO [GossipTasks:1] 2011-11-20 18:36:05,153 Gossiper.java (line 759) InetAddress /10.6.130.70 is now dead.
ERROR [GossipTasks:1] 2011-11-20 18:36:25,252 StreamOutSession.java (line 232) StreamOutSession /10.6.130.70 failed because {} died or was restarted/removed
ERROR [AntiEntropySessions:21] 2011-11-20 18:36:25,252 AntiEntropyService.java (line 688) [repair #7fb5b1b0-11f1-11e1-0000-baed0a2090fe] session completed with the following err
or
java.io.IOException: Endpoint /10.6.130.70 died
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.failedNode(AntiEntropyService.java:725)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.convict(AntiEntropyService.java:762)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:192)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
ERROR [GossipTasks:1] 2011-11-20 18:36:25,256 Gossiper.java (line 172) Gossip error
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:190)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
ERROR [AntiEntropySessions:21] 2011-11-20 18:36:25,256 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[AntiEntropySessions:21,5,RMI Runtime]
java.lang.RuntimeException: java.io.IOException: Endpoint /10.6.130.70 died
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Endpoint /10.6.130.70 died
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.failedNode(AntiEntropyService.java:725)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.convict(AntiEntropyService.java:762)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:192)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        ... 3 more
ERROR [RMI TCP Connection(3634)-10.29.60.10] 2011-11-20 18:36:25,256 StorageService.java (line 1712) Repair session 7fb5b1b0-11f1-11e1-0000-baed0a2090fe failed.
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.IOException: Endpoint /10.6.130.70 died
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.service.StorageService.forceTableRepair(StorageService.java:1708)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1264)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1359)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.io.IOException: Endpoint /10.6.130.70 died
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
Caused by: java.io.IOException: Endpoint /10.6.130.70 died
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.failedNode(AntiEntropyService.java:725)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.convict(AntiEntropyService.java:762)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:192)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        ... 3 more
 INFO [GossipStage:1] 2011-11-20 18:36:28,173 Gossiper.java (line 777) Node /10.6.130.70 has restarted, now UP
 INFO [GossipStage:1] 2011-11-20 18:36:28,175 Gossiper.java (line 745) InetAddress /10.6.130.70 is now UP
 INFO [GossipStage:1] 2011-11-20 18:36:28,175 StorageService.java (line 885) Node /10.6.130.70 state jump to normal
{code}

FailureDetector uses a normal ArrayList for the listeners.","Free BSD 8.2 
/java -version
java version ""1.6.0_07""
Diablo Java(TM) SE Runtime Environment (build 1.6.0_07-b02)
Diablo Java HotSpot(TM) 64-Bit Server VM (build 10.0-b23, mixed mode)

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/11 07:54;amorton;3519.patch;https://issues.apache.org/jira/secure/attachment/12504713/3519.patch",,,,,,,,,,,,,,1.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,218017,,,Tue Nov 22 12:02:39 UTC 2011,,,,,,,,,,"0|i0gkwn:",94817,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"22/Nov/11 07:54;amorton;Use a CopyOnWriteArrayList in the FailureDetector to track listeners, like the Gossiper does. ;;;","22/Nov/11 10:11;slebresne;+1, commmitted (to 0.8 and up);;;","22/Nov/11 12:02;hudson;Integrated in Cassandra-0.8 #400 (See [https://builds.apache.org/job/Cassandra-0.8/400/])
    Fix ConcurrentModificationException in FailureDetector
patch by amorton; reviewed by slebresne for CASSANDRA-3519

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1204884
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/FailureDetector.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CounterColumnFamily Compaction error (ArrayIndexOutOfBoundsException),CASSANDRA-3514,12532195,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,efalcao,efalcao,21/Nov/11 19:13,16/Apr/19 09:32,14/Jul/23 05:52,23/Nov/11 08:04,0.8.8,1.0.4,,,,,0,compaction,,,"On a single node, I'm seeing the following error when trying to compact a CounterColumnFamily. This appears to have started with version 1.0.3.

nodetool -h localhost compact TRProd MetricsAllTime
Error occured during compaction
java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:250)
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1471)
	at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1523)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ArrayIndexOutOfBoundsException
	at org.apache.cassandra.utils.ByteBufferUtil.arrayCopy(ByteBufferUtil.java:292)
	at org.apache.cassandra.db.context.CounterContext$ContextState.copyTo(CounterContext.java:792)
	at org.apache.cassandra.db.context.CounterContext.removeOldShards(CounterContext.java:709)
	at org.apache.cassandra.db.CounterColumn.removeOldShards(CounterColumn.java:260)
	at org.apache.cassandra.db.CounterColumn.mergeAndRemoveOldShards(CounterColumn.java:306)
	at org.apache.cassandra.db.CounterColumn.mergeAndRemoveOldShards(CounterColumn.java:271)
	at org.apache.cassandra.db.compaction.PrecompactedRow.removeDeletedAndOldShards(PrecompactedRow.java:86)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:102)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:133)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:102)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:87)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:116)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:99)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:172)
	at org.apache.cassandra.db.compaction.CompactionManager$4.call(CompactionManager.java:277)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/11 19:05;slebresne;3514.patch;https://issues.apache.org/jira/secure/attachment/12504782/3514.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,217931,,,Wed Nov 23 08:15:02 UTC 2011,,,,,,,,,,"0|i0gkuf:",94807,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,"22/Nov/11 01:57;efalcao;This also just started happening on another node (without user-triggered major compaction). Not sure what's causing this (old 0.8.* counter data perhaps?);;;","22/Nov/11 19:05;slebresne;That is a bug, attaching patch to fix. Note that there is actually two bugs, so the patch fix both and adds unit tests for each of them.;;;","22/Nov/11 19:07;slebresne;Note that this is due to CASSANDRA-3178 so this affects the 0.8 branch, but not any released version.;;;","23/Nov/11 01:05;yukim;+1;;;","23/Nov/11 08:04;slebresne;Committed, thanks;;;","23/Nov/11 08:15;hudson;Integrated in Cassandra-0.8 #402 (See [https://builds.apache.org/job/Cassandra-0.8/402/])
    Fix array out of bounds error in counter shard removal
patch by slebresne; reviewed by yukim for CASSANDRA-3514

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1205316
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/context/CounterContext.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/context/CounterContextTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect query results due to invalid SSTable.maxTimestamp,CASSANDRA-3510,12532118,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,amorton,amorton,21/Nov/11 07:15,16/Apr/19 09:32,14/Jul/23 05:52,22/Nov/11 09:40,1.0.4,,,,,,0,,,,"related to CASSANDRA-3446

(sorry this is so long, took me a bit to work through it all and there is a lot of new code :) )
 
h1. Summary

SSTable.maxTimestamp for files created before 1.0 defaults to Long.MIN_VALUE, and this means the wrong data is returned from queries. 
 
h2. Details 

Noticed on a cluster that was upgraded from 0.8.X to 1.X, it then had trouble similar to CASSANDRA-3446. It was rolled back to 0.8 and the migrated to 1.0.3. 

4 Node cluster, all files upgraded to ""hb"" format. 

In a super CF there are situations where a get for a sub columns returns a different value than a get for the column. .e.g. 

{noformat}
[default@XXX] get Users[ascii('username')]['meta']['password'];
=> (column=password, value=3130323130343130, timestamp=1307352647576000)

[default@XX] get Users[ascii('username')]['meta'];     
(snip)       
=> (column=password, value=3034323131303034, timestamp=1319563673493000)
{noformat}

The correct value is the second one. 

I added logging after line 109 in o.a.c.db.CollectionController.collectTimeOrderedData() to log the sstable name and the file max timestamp, this is what I got:

{code:java}
for (SSTableReader sstable : view.sstables)
{
    long currentMaxTs = sstable.getMaxTimestamp();
    logger.debug(String.format(""Got sstable %s and max TS %d"", sstable, currentMaxTs));
    reduceNameFilter(reducedFilter, container, currentMaxTs);
{code}

{noformat}
DEBUG 14:08:46,012 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12348-Data.db') and max TS 1321824847534000
DEBUG 14:08:47,231 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12346-Data.db') and max TS 1321813380793000
DEBUG 14:08:49,879 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12330-Data.db') and max TS -9223372036854775808
DEBUG 14:08:49,880 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12325-Data.db') and max TS -9223372036854775808
{noformat}

The key I was reading is present in files 12330 and 12325, the first contains the *old / wrong* value with timestamp 1307352647576000 above. The second contains the *new / correct* value with timestamp 1319563673493000.

**Updated:** Incorrect, it was a later file that had the correct value, see the first comment. 

When CollectionController.collectTimeOrderedData() processes the 12325 file (after processing the 12330 file) while looping over the sstables the call to reduceNameFilter() removes the column  from the filter because the column read from the 12330 file has a time stamp of 1307352647576000 and the 12325 file incorrectly has a max time stamp of -9223372036854775808 .

SSTableMetadata is reading the max time stamp from the stats file, but it is Long.MIN_VALUE. I think this happens because scrub creates the SSTableWriter using cfs.createCompactionWriter() which sets the maxTimestamp in the meta data collector according to the maxTimestamp in the meta data for the file(s) that will be scrubbed / compacted. But for pre 1.0 format files the default in SSTableMetadata is Long.MIN_VALUE, (see SSTableMetaData.deserialize() and the ctor). So scrubbing a pre 1.0 file will write stats files that have maxTimestamp as Long.MIN_VALUE.

During scrubbing the SSTableWriter does not update the maxTimestamp because append(AbstractCompactedRow) is called which expects the that cfs.createCompactionWriter() was able to set the correct maxTimestamp on the meta data. Compaction also uses append(AbstractCompactedRow) so may create an SSTable with an incorrect maxTimestamp if one of the input files started life as a pre 1.0 file and has a bad maxTimestamp. 

It looks like the only time the maxTimestamp is calculated is when the SSTable is originally written. So the error from the old files will be carried along. 

e.g. If the files a,b and c have the maxTimestamps 10, 100 and Long.MIN_VALUE compaction will write a SSTable with maxTimestamp 100. However file c may actually contain columns with a timestamp > 100 which will be in the compacted file.

h1. Reproduce

1. Start a clean 0.8.7

2. Add a schema (details of the schema do not matter):
{noformat}
[default@unknown] create keyspace dev;   
5f834620-140b-11e1-0000-242d50cf1fdf
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] 
[default@unknown] use dev;
Authenticated to keyspace: dev
[default@dev] 
[default@dev] create column family super_dev with column_type = 'Super' 
...	and key_validation_class = 'AsciiType' and comparator = 'AsciiType' and 
...	subcomparator = 'AsciiType' and default_validation_class = 'AsciiType';
60490720-140b-11e1-0000-242d50cf1fdf
Waiting for schema agreement...
... schemas agree across the cluster
{noformat}

3. Shutdown 0.8.7

4. Start 1.0.3 using the same data. Check the schema version loaded, example below shows the wrong schema is loaded. I stepped the code and the wrong value was read from Migration.getLastMigrationId() due to this bug. 

{noformat}
 INFO [main] 2011-11-21 19:39:08,546 DatabaseDescriptor.java (line 501) Loading schema version 5f834620-140b-11e1-0000-242d50cf1fdf
{noformat}

5. Check the schema using the 1.0.3 CLI 

{noformat}
[default@unknown] use dev;
Authenticated to keyspace: dev
[default@dev] describe;
Keyspace: dev:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [datacenter1:1]
  Column Families:
[default@dev] 
{noformat}

6. I then did a 1.0.3 scrub and re-started. The correct schema version was read, but stepping the code both Schema SSTables had Long.MIN_VALUE as the maxTimestamp so I think it was only the random order of the files that made it work. 

{noformat}
DEBUG 19:52:30,744 Got sstable SSTableReader(path='/var/lib/cassandra/data/system/Schema-hb-4-Data.db') and max TS -9223372036854775808
DEBUG 19:52:30,744 Got sstable SSTableReader(path='/var/lib/cassandra/data/system/Schema-hb-3-Data.db') and max TS -9223372036854775808
{noformat}

h1. Fixes

Not sure, (wanted to get the ticket opened and find out if I was imagining things), guessing...

Use Long.MIN_VALUE as a magic maxTimestamp that means the value is not know. This would not fix issues where the incorrect maxTimestamp been included in compaction. 
 
Looking at making scrub re-calculate the maxTimestamp.

Also wondering if the maxTimestamp should default to Long.MAX_VALUE if read from a file format that does not support maxTimestamp ?
",,jborgstrom,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Nov/11 10:44;amorton;0001-3510-ignore-maxTimestamp-if-Long.MIN_VALUE.patch;https://issues.apache.org/jira/secure/attachment/12504489/0001-3510-ignore-maxTimestamp-if-Long.MIN_VALUE.patch","21/Nov/11 10:44;amorton;0002-3510-update-maxTimestamp-during-repair.patch;https://issues.apache.org/jira/secure/attachment/12504490/0002-3510-update-maxTimestamp-during-repair.patch","21/Nov/11 11:43;slebresne;3510.patch;https://issues.apache.org/jira/secure/attachment/12504493/3510.patch",,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,217854,,,Tue Nov 22 09:40:36 UTC 2011,,,,,,,,,,"0|i0gksn:",94799,,amorton,,amorton,Critical,,,,,,,,,,,,,,,,,"21/Nov/11 09:16;amorton;To be clear, I do not think this is an issue with super CF's. The Schema CF is a standard, it was noticed on a super CF. 

I added some more logging and removed the call to cut the query short by checking maxTimestamp in CollationController.collectTimeOrderedData(). The query returned the result value (password col with timestamp 1319563673493000) and this was the output 

{code:java}
for (SSTableReader sstable : view.sstables)
{
    long currentMaxTs = sstable.getMaxTimestamp();
    logger.debug(String.format(""Got sstable %s and max TS %d"", sstable, currentMaxTs));
//                reduceNameFilter(reducedFilter, container, currentMaxTs);
//                if (((NamesQueryFilter) reducedFilter.filter).columns.isEmpty())
//                    break;

    IColumnIterator iter = reducedFilter.getSSTableColumnIterator(sstable);
    iterators.add(iter);
    if (iter.getColumnFamily() != null)
    {
        container.delete(iter.getColumnFamily());
        sstablesIterated++;
        while (iter.hasNext())
        {
            IColumn col = iter.next();
            if (col instanceof SuperColumn)
            {
              for (IColumn subcol : ((SuperColumn)col).columns)
              {
                if (subcol.name().equals(ByteBufferUtil.bytes(""password"")))
                  logger.debug(String.format(""Add Sub Column %s"", subcol.getString(cfs.metadata.subcolumnComparator)));
              }
            }
            else
            {
              logger.debug(String.format(""Add Column %s"", col.getString(cfs.metadata.comparator)));
            }
            container.addColumn(col);
        }
{code}


{noformat}
DEBUG 22:05:20,748 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12348-Data.db') and max TS 1321824847534000
DEBUG 22:05:20,749 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12346-Data.db') and max TS 1321813380793000
DEBUG 22:05:20,753 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12330-Data.db') and max TS -9223372036854775808
DEBUG 22:05:20,754 Add Sub Column password:false:8@1307352647576000
DEBUG 22:05:20,755 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12325-Data.db') and max TS -9223372036854775808
DEBUG 22:05:20,757 Add Sub Column password:false:8@1307352647576000
DEBUG 22:05:20,758 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12327-Data.db') and max TS -9223372036854775808
DEBUG 22:05:20,760 Add Sub Column password:false:8@1307352647576000
DEBUG 22:05:20,761 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12328-Data.db') and max TS -9223372036854775808
DEBUG 22:05:20,762 Add Sub Column password:false:8@1319563673493000
DEBUG 22:05:20,763 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12326-Data.db') and max TS -9223372036854775808
DEBUG 22:05:20,765 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12331-Data.db') and max TS -9223372036854775808
DEBUG 22:05:20,767 Add Sub Column password:false:8@1307352647576000
DEBUG 22:05:20,768 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12332-Data.db') and max TS -9223372036854775808
DEBUG 22:05:20,774 Read: 27 ms.
{noformat}

;;;","21/Nov/11 10:44;amorton;patch 0001 is a proof of concept hack based on my first comment, it generated this output when using the extra logging 

{noformat}
DEBUG [ReadStage:1] 2011-11-21 23:12:28,578 CollationController.java (line 77) collectTimeOrderedData
DEBUG [ReadStage:1] 2011-11-21 23:12:28,578 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12348-Data.db') and max TS 1321824847534000
DEBUG [ReadStage:1] 2011-11-21 23:12:28,578 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12346-Data.db') and max TS 1321813380793000
DEBUG [ReadStage:1] 2011-11-21 23:12:28,583 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12330-Data.db') and max TS -9223372036854775808
DEBUG [ReadStage:1] 2011-11-21 23:12:28,584 CollationController.java (line 130) Add Sub Column password:false:8@1307352647576000
DEBUG [ReadStage:1] 2011-11-21 23:12:28,585 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12325-Data.db') and max TS -9223372036854775808
DEBUG [ReadStage:1] 2011-11-21 23:12:28,587 CollationController.java (line 130) Add Sub Column password:false:8@1307352647576000
DEBUG [ReadStage:1] 2011-11-21 23:12:28,588 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12327-Data.db') and max TS -9223372036854775808
DEBUG [ReadStage:1] 2011-11-21 23:12:28,590 CollationController.java (line 130) Add Sub Column password:false:8@1307352647576000
DEBUG [ReadStage:1] 2011-11-21 23:12:28,591 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12328-Data.db') and max TS -9223372036854775808
DEBUG [ReadStage:1] 2011-11-21 23:12:28,592 CollationController.java (line 130) Add Sub Column password:false:8@1319563673493000
DEBUG [ReadStage:1] 2011-11-21 23:12:28,593 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12326-Data.db') and max TS -9223372036854775808
DEBUG [ReadStage:1] 2011-11-21 23:12:28,595 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12331-Data.db') and max TS -9223372036854775808
DEBUG [ReadStage:1] 2011-11-21 23:12:28,596 CollationController.java (line 130) Add Sub Column password:false:8@1307352647576000
DEBUG [ReadStage:1] 2011-11-21 23:12:28,597 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hb-12332-Data.db') and max TS -9223372036854775808
DEBUG [pool-2-thread-1] 2011-11-21 23:12:28,604 StorageProxy.java (line 694) Read: 26 ms.
{noformat}


patch 0002 makes scrub update the maxTimestamp and when I ran my test afterwards it created this output:

**NOTE** patch 0002 has incorrect file name, it modifies scrub.

{noformat}

DEBUG [ReadStage:33] 2011-11-21 23:20:32,032 CollationController.java (line 77) collectTimeOrderedData
DEBUG [ReadStage:33] 2011-11-21 23:20:32,033 CollationController.java (line 111) Got sstable SSTableReader(path='/private/var/lib/cassandra/data/fmm/Users-hc-12357-Data.db') and max TS 1321824847534000
DEBUG [ReadStage:33] 2011-11-21 23:20:32,033 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hc-12352-Data.db') and max TS 1321813380793000
DEBUG [ReadStage:33] 2011-11-21 23:20:32,063 CollationController.java (line 111) Got sstable SSTableReader(path='/private/var/lib/cassandra/data/fmm/Users-hc-12356-Data.db') and max TS 1321560509938000
DEBUG [ReadStage:33] 2011-11-21 23:20:32,064 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hc-12349-Data.db') and max TS 1319813295567000
DEBUG [ReadStage:33] 2011-11-21 23:20:32,105 CollationController.java (line 130) Add Sub Column password:false:8@1319563673493000
DEBUG [ReadStage:33] 2011-11-21 23:20:32,105 CollationController.java (line 111) Got sstable SSTableReader(path='/var/lib/cassandra/data/fmm/Users-hc-12350-Data.db') and max TS 1318523190222000
DEBUG [pool-2-thread-2] 2011-11-21 23:20:32,106 StorageProxy.java (line 694) Read: 74 ms.
{noformat};;;","21/Nov/11 11:43;slebresne;The fix of the first patch looks. For the second patch, even though this is an option, I think I'd rather make this parts of compaction, because I don't like too much having potential subtle bug that needs scrub to be run (without really anything telling you that you potentially have a problem btw).

Attaching a patch that does this (make it work for compaction in general as long as we're not using an EchoedRow (which makes it work for scrub in particular)). The patch also adds a few comments and a unit test. It includes the fix of the first patch. ;;;","21/Nov/11 15:08;jbellis;bq. 

{noformat}
+         * However, for old sstables without timestamp, we still want to update the timestamp (and we know
+         * that in this case we will not use EchoedRow).
{noformat}

Where is the ""don't use echoedrow for old sstables"" logic?;;;","21/Nov/11 15:30;slebresne;bq. Where is the ""don't use echoedrow for old sstables"" logic?

In CompactionController.needDeserialize(), but I agree that the comment could be improved to recall it.;;;","21/Nov/11 15:41;jbellis;Referring to the isLatestVersion check?;;;","21/Nov/11 16:57;slebresne;bq. Referring to the isLatestVersion check?

Yes.;;;","21/Nov/11 19:14;amorton;Thanks, will test shortly. 
;;;","21/Nov/11 19:57;amorton;Tested against the current 1.0 head, with the test case I had and the query works as expected. 

I agree doing it in compaction is safer, putting it in repair just got me there faster last night. 

Thanks for cleaning up the patch. 
+1;;;","21/Nov/11 20:05;jbellis;+1;;;","22/Nov/11 09:40;slebresne;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli shows org.apache.Cassandra.XXX in example help for replication strategy but it should be cassandra with a lowercase c,CASSANDRA-3509,12532000,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tommysdk,mdennis,mdennis,18/Nov/11 21:49,16/Apr/19 09:32,14/Jul/23 05:52,05/Mar/12 23:47,1.1.0,,,,,,0,,,,"copying and pasting the example doesn't result in a working example and noticing the ""C"" -v- ""c"" is something easy to overlook ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/12 21:17;tommysdk;CASSANDRA-3509.PATCH;https://issues.apache.org/jira/secure/attachment/12517126/CASSANDRA-3509.PATCH",,,,,,,,,,,,,,1.0,tommysdk,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,217736,,,Mon Mar 05 23:47:48 UTC 2012,,,,,,,,,,"0|i0gks7:",94797,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"05/Mar/12 21:17;tommysdk;Submitted patch to cli help containing fix for incorrect package names (Cassandra should be lower-cased).;;;","05/Mar/12 21:18;tommysdk;Apply patch on current trunk.;;;","05/Mar/12 23:47;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
requiring --debug to see stack traces for failures in cassandra-cli is a terrible idea (aka silent failure is never a valid option),CASSANDRA-3508,12531999,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,mdennis,mdennis,18/Nov/11 21:47,16/Apr/19 09:32,14/Jul/23 05:52,21/Nov/11 18:26,1.0.4,,,Legacy/Tools,,,0,,,,"this manifests itself in cassandra-cli by returning null to the user.  In order to see what the problem was (and in many cases, just to know there was a problem at all) requires running cassandra-cli with ""--debug""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Nov/11 18:03;xedin;CASSANDRA-3508-v2.patch;https://issues.apache.org/jira/secure/attachment/12504525/CASSANDRA-3508-v2.patch","18/Nov/11 22:50;xedin;CASSANDRA-3508.patch;https://issues.apache.org/jira/secure/attachment/12504282/CASSANDRA-3508.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,217735,,,Mon Nov 21 18:26:48 UTC 2011,,,,,,,,,,"0|i0gkrr:",94795,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Nov/11 22:50;xedin;removes explicit --debug requirement and prints stack-traces in cases where it's necessary.;;;","21/Nov/11 15:21;jbellis;We need to be a little judicious here.  Simply printing stacktraces for *all* exceptions is not right either.  For instance, it looks to me like

{code}
             sessionState.err.println(e.getWhy());
 
-            if (sessionState.debug)
-                e.printStackTrace();
+            e.printStackTrace(sessionState.err);
{code}

{code}
-            if (sessionState.debug)
-                e.printStackTrace();
-            
             sessionState.err.println(""Login failure. Did you specify 'keyspace', 'username' and 'password'?"");
+            e.printStackTrace(sessionState.err);
{code}

should not be stacktraces by default.;;;","21/Nov/11 15:47;xedin;I agree that we can remove it from the first one (describeRing) but I think we should keep stack-trace for login as it could be handy to have.;;;","21/Nov/11 17:45;jbellis;I'd be okay with keeping --debug around for that.;;;","21/Nov/11 18:03;xedin;this patch keeps the --debug option and shows exceptions in the places your mentioned only when that option is set.;;;","21/Nov/11 18:10;jbellis;+1;;;","21/Nov/11 18:26;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cassandra.bat does not use SETLOCAL, can cause classpath issues",CASSANDRA-3506,12531982,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,18/Nov/11 19:06,16/Apr/19 09:32,14/Jul/23 05:52,18/Nov/11 21:08,1.0.4,,,,,,0,,,,"In bin/cassandra.bat, we don't use SETLOCAL (although we do use ENDLOCAL for some reason), so modifications to the classpath within the batch script persist.  This means that if you run cassandra-1.0.0/bin/cassandra.bat, kill the process, and then run cassandra-1.0.3/bin/cassandra.bat, the 1.0.0 jars will still be in the classpath. ",Windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/11 19:12;thobbs;3506.txt;https://issues.apache.org/jira/secure/attachment/12504248/3506.txt",,,,,,,,,,,,,,1.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,217718,,,Fri Nov 18 21:08:38 UTC 2011,,,,,,,,,,"0|i0gkqn:",94790,,bcoverston,,bcoverston,Low,,,,,,,,,,,,,,,,,"18/Nov/11 19:12;thobbs;Attached patch adds the same conditional setlocal to cassandra.bat that cassandra-cli.bat uses.;;;","18/Nov/11 19:25;bcoverston;Reviewed patch +1.;;;","18/Nov/11 21:08;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IncomingStreamReader uses socket.getRemoteSocketAddress() which might be diffrent than FB.getBroadcastAddress(),CASSANDRA-3503,12531835,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,17/Nov/11 19:57,16/Apr/19 09:32,14/Jul/23 05:52,17/Nov/11 22:17,1.1.0,,,,,,0,,,,"We can add BCA to the streaming so the receiver can use this to StreamInSession.get(bca, sid)

Currently this causes the repairs to hang when the bca is diffrent than LocalAddress.",JVM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/11 21:26;vijay2win@yahoo.com;0001-fixing-the-bca-lookup-for-streaming-v2.patch;https://issues.apache.org/jira/secure/attachment/12504114/0001-fixing-the-bca-lookup-for-streaming-v2.patch","17/Nov/11 21:00;vijay2win@yahoo.com;0001-fixing-the-bca-lookup-for-streaming.patch;https://issues.apache.org/jira/secure/attachment/12504113/0001-fixing-the-bca-lookup-for-streaming.patch",,,,,,,,,,,,,2.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,217571,,,Thu Nov 17 23:27:05 UTC 2011,,,,,,,,,,"0|i0gkpj:",94785,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"17/Nov/11 21:00;vijay2win@yahoo.com;Attached patch add's InetAddress to the Header (Simple approach).

Alternative approach will be converting the StreamInSession.context into a UUID.;;;","17/Nov/11 21:26;vijay2win@yahoo.com;Update the serializer type based on the feedback from IRS.;;;","17/Nov/11 22:17;brandon.williams;Committed, thanks!;;;","17/Nov/11 23:27;hudson;Integrated in Cassandra #1212 (See [https://builds.apache.org/job/Cassandra/1212/])
    Streaming uses BroadcastAddress instead of the remote socket.
Patch by Vijay, reviewed by brandonwilliams for CASSANDRA-3503

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1203394
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/IncomingStreamReader.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/StreamHeader.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move allows you to move to tokens > 2**127,CASSANDRA-3501,12531820,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,nickmbailey,nickmbailey,17/Nov/11 17:55,16/Apr/19 09:32,14/Jul/23 05:52,18/Nov/11 22:56,1.0.4,,,Legacy/Tools,,,1,,,,Currently you can move to tokens greater than what should be the max token in RP.,,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3880,,,,,,,,,"18/Nov/11 22:31;jbellis;3501-v2.txt;https://issues.apache.org/jira/secure/attachment/12504280/3501-v2.txt","17/Nov/11 23:31;jbellis;3501.txt;https://issues.apache.org/jira/secure/attachment/12504138/3501.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,217556,,,Fri Nov 18 22:56:50 UTC 2011,,,,,,,,,,"0|i0gkon:",94781,,nickmbailey,,nickmbailey,Low,,,,,,,,,,,,,,,,,"17/Nov/11 23:31;jbellis;Patch to bounds-check RandomPartitioner TokenFactor.;;;","18/Nov/11 22:31;jbellis;v2 introduces RP.MAXIMUM constant;;;","18/Nov/11 22:38;nickmbailey;+1;;;","18/Nov/11 22:56;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh ASSUME doesn't work when using SELECT keyspace.cfname syntax,CASSANDRA-3500,12531818,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,17/Nov/11 17:22,16/Apr/19 09:32,14/Jul/23 05:52,28/Nov/11 21:19,1.0.5,,,Legacy/Tools,,,0,cqlsh,,,"After assigning an ASSUME type to some columnfamily CF in keyspace K, if a SELECT is subsequently done on CF while the session is using a different keyspace, the ASSUME does not take effect:

{noformat}
cqlsh> USE ks;
cqlsh:ks> CREATE COLUMNFAMILY cf (key int PRIMARY KEY, col int);
cqlsh:ks> INSERT INTO cf (key, col) VALUES (99, 1633837924);
cqlsh:ks> ASSUME cf(col) VALUES ARE ascii;
cqlsh:ks> SELECT * FROM cf;
 KEY |  col |
  99 | abcd |

cqlsh:ks> USE system;
cqlsh:system> SELECT * FROM ks.cf;
 KEY |        col |
  99 | 1633837924 |

{noformat}

the output from both {{SELECT}}s there should be the same.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/11 17:24;thepaul;3500.patch.txt;https://issues.apache.org/jira/secure/attachment/12504081/3500.patch.txt",,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,217554,,,Mon Nov 28 21:19:22 UTC 2011,,,,,,,,,,"0|i0gko7:",94779,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"17/Nov/11 17:24;thepaul;fixerated;;;","28/Nov/11 21:19;brandon.williams;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load from `nodetool ring` does not update after cleanup.,CASSANDRA-3496,12531527,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,cywjackson,bcoverston,bcoverston,15/Nov/11 20:18,16/Apr/19 09:32,14/Jul/23 05:52,16/Nov/11 18:10,1.0.4,,,,,,0,,,,"Repro:
Bring up a node.

Insert 1M rows:
127.0.0.1       datacenter1 rack1       Up     Normal  406.92 MB       100.00% 77747037169725419723056812679314618801
(Already looks wrong, 406.92 is higher than I'm used to seeing from a single run of stress)

Bootstrap a second node into the cluster:

162877269496252595336256012556853953561
127.0.0.1       datacenter1 rack1       Up     Normal  407.03 MB       49.96%  77747037169725419723056812679314618801
127.0.0.2       datacenter1 rack1       Up     Normal  157.91 MB       50.04%  162877269496252595336256012556853953561

Cleanup
162877269496252595336256012556853953561
127.0.0.1       datacenter1 rack1       Up     Normal  551.2 MB       49.96%  77747037169725419723056812679314618801
127.0.0.2       datacenter1 rack1       Up     Normal  157.91 MB       50.04%  162877269496252595336256012556853953561

Looks like each operation that adds and removes SSTables only adds to the total and doesn't remove the old sstables from the total size count.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/11 20:29;jbellis;3496.txt;https://issues.apache.org/jira/secure/attachment/12503789/3496.txt",,,,,,,,,,,,,,1.0,cywjackson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,217263,,,Wed Nov 16 18:10:07 UTC 2011,,,,,,,,,,"0|i0gkmf:",94771,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"15/Nov/11 20:27;jbellis;Jackson pointed out that doing it in the current order, the data will be removed by the time we call length() on it:

{code}
.           sstable.markCompacted();
            sstable.releaseReference();
            liveSize.addAndGet(-sstable.bytesOnDisk());
{code};;;","15/Nov/11 20:29;jbellis;patch to reduce size before releasing reference;;;","15/Nov/11 20:34;cywjackson;my test case (which seems difference from Ben's?):
1) put a break between the releaseReference and releaseReference
2) insert to a CF, flush, insert again, flush, now you have 2 sstables
3) force major compaction

with the following break/sleep in the test:

{code}
            sstable.releaseReference();
            try
            {
                logger.debug(""xxxxxxxxxxxx zz for 10s"");
                Thread.sleep(7000);
            }
            
            catch (InterruptedException e)
            {
                // TODO Auto-generated catch block
                e.printStackTrace();
            }
            long bytesOnDisk = sstable.bytesOnDisk();
            logger.debug(""xxxxxxxxxx wake up -------- size to remove from livesize"" + bytesOnDisk);
liveSize.addAndGet(-bytesOnDisk);
{code}

the following is observed in the log:
{noformat}
DEBUG [CompactionExecutor:6] 2011-11-15 12:29:43,050 DataTracker.java (line 345) removing /var/lib/cassandra/data/testsize/testcfsize-hb-1 from list of files tracked for testsize.testcfsize
DEBUG [CompactionExecutor:6] 2011-11-15 12:29:43,050 SSTableReader.java (line 742) Marking /var/lib/cassandra/data/testsize/testcfsize-hb-1-Data.db compacted
DEBUG [CompactionExecutor:6] 2011-11-15 12:29:43,050 MmappedSegmentedFile.java (line 139) All segments have been unmapped successfully
DEBUG [CompactionExecutor:6] 2011-11-15 12:29:43,051 MmappedSegmentedFile.java (line 139) All segments have been unmapped successfully
DEBUG [CompactionExecutor:6] 2011-11-15 12:29:43,051 DataTracker.java (line 353) xxxxxxxxxxxx zz for 10s
DEBUG [NonPeriodicTasks:1] 2011-11-15 12:29:43,051 FileUtils.java (line 51) Deleting testcfsize-hb-1-Index.db
DEBUG [NonPeriodicTasks:1] 2011-11-15 12:29:43,051 FileUtils.java (line 51) Deleting testcfsize-hb-1-Filter.db
DEBUG [NonPeriodicTasks:1] 2011-11-15 12:29:43,052 FileUtils.java (line 51) Deleting testcfsize-hb-1-Digest.sha1
DEBUG [NonPeriodicTasks:1] 2011-11-15 12:29:43,052 FileUtils.java (line 51) Deleting testcfsize-hb-1-Statistics.db
DEBUG [NonPeriodicTasks:1] 2011-11-15 12:29:43,052 SSTable.java (line 144) Deleted /var/lib/cassandra/data/testsize/testcfsize-hb-1
DEBUG [CompactionExecutor:6] 2011-11-15 12:29:50,051 DataTracker.java (line 363) xxxxxxxxxx wake up -------- size to remove from livesize0
{noformat}

cfstats:
                Column Family: testcfsize
                SSTable count: 1
                Space used (live): 14395
                Space used (total): 5491
;;;","16/Nov/11 10:35;slebresne;patch lgtm, +1;;;","16/Nov/11 18:10;jbellis;Committed.

(Tried to catch the bug w/ a check on live size in RecoveryManagerTruncateTest, but I couldn't reproduce the race without adding sleeps inside the deletion task.  So I just committed the patch as above.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh complains when you try to do UPDATE with counter columns,CASSANDRA-3493,12531370,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,14/Nov/11 21:09,16/Apr/19 09:32,14/Jul/23 05:52,14/Nov/11 21:31,1.0.3,,,Legacy/Tools,,,0,,,,"trying to do a counter column UPDATE in cqlsh causes an ""Invalid syntax"" error:

{noformat}
cqlsh:foo> update brongo SET boo = boo+1 where key='hi';
Invalid syntax at line 1, char 28
  update brongo SET boo = boo+1 where key='hi';
                             ^
{noformat}

This is cause cqlsh's lexer doesn't know that + and - are valid operators in CQL. Don't worry, I'm not trying to make cqlsh be able to parse all CQL with exactness- it tries, in order to provide the best tab completion, but when it fails to parse it can still pass on CQL text to the server. This case is different because it's the lexer that can't understand the operators, before we even get to the parser. We do need a working and correct lexer, along with at least minimal parsing capability, in order to reliably split up statements, tell when the user is changing the keyspace, or SELECTing on a columnfamily with ASSUMEd types.

Also, the parser should be tweaked in a manner similar to CASSANDRA-3418.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/11 21:15;thepaul;3493.patch.txt;https://issues.apache.org/jira/secure/attachment/12503676/3493.patch.txt",,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,217106,,,Mon Nov 14 21:31:47 UTC 2011,,,,,,,,,,"0|i0gkl3:",94765,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"14/Nov/11 21:29;jbellis;+1;;;","14/Nov/11 21:31;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compression option chunk_length is not converted into KB as it should,CASSANDRA-3492,12531368,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,slebresne,slebresne,14/Nov/11 20:58,16/Apr/19 09:32,14/Jul/23 05:52,15/Nov/11 07:41,1.0.3,,,,,,0,compression,,,,,mck,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/11 20:58;slebresne;0001-Fix-chunk-length-option.patch;https://issues.apache.org/jira/secure/attachment/12503674/0001-Fix-chunk-length-option.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,217104,,,Tue Nov 15 09:08:38 UTC 2011,,,,,,,,,,"0|i0gkkn:",94763,,xedin,,xedin,Critical,,,,,,,,,,,,,,,,,"14/Nov/11 20:58;slebresne;Oups;;;","14/Nov/11 21:11;xedin;+1;;;","14/Nov/11 21:32;mck;Does this involve a `nodetool compact/scrub` to repair existing sstables?
(the answer is yes).;;;","15/Nov/11 07:41;slebresne;Committed;;;","15/Nov/11 09:08;mck;Looks good. Back to running w/ Xmx8g :-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Recursion bug in CollationController,CASSANDRA-3491,12531347,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,tjake,tjake,14/Nov/11 17:54,16/Apr/19 09:32,14/Jul/23 05:52,14/Nov/11 21:30,1.0.3,,,,,,0,,,,"The following stack trace seems to indicate a recursion bug in CollationController

Where the stats collection mutation is itself having stats collected on and so fourth

http://pastebin.com/raw.php?i=35Rt7ryB",,jborgstrom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/11 18:10;jbellis;3491.txt;https://issues.apache.org/jira/secure/attachment/12503646/3491.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,217083,,,Mon Nov 14 21:30:20 UTC 2011,,,,,,,,,,"0|i0gkk7:",94761,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"14/Nov/11 18:10;jbellis;Patch to revert CASSANDRA-2503.  Clearly putting that in 1.0.x was a mistake (mine).;;;","14/Nov/11 21:03;slebresne;+1;;;","14/Nov/11 21:30;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EncryptionOptions should be instantiated,CASSANDRA-3489,12531262,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,13/Nov/11 16:32,16/Apr/19 09:32,14/Jul/23 05:52,01/Dec/11 20:37,1.0.6,,,,,,0,,,,"As the title says, otherwise you get an NPE when the options are missing from the yaml.  It's included in my second patch on CASSANDRA-3045 and is a one line fix.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3599,,,"28/Nov/11 22:10;brandon.williams;3489.txt;https://issues.apache.org/jira/secure/attachment/12505406/3489.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,216999,,,Thu Dec 01 20:37:56 UTC 2011,,,,,,,,,,"0|i0gkjb:",94757,,,,,Low,,,,,,,,,,,,,,,,,"13/Nov/11 16:41;jbellis;There's a bunch of ""if encryption options is null then ignore it"" special cases already, if you're going to instantiate a default instead then let's get rid of those.

May also need to be applied to 0.8 unless aforesaid special cases cover everything.;;;","28/Nov/11 22:10;brandon.williams;I could only find the special case added the first time we fixed this back in 0.8 for CASSANDRA-3007.  Attached patch removes that and instantiates the default instead.;;;","01/Dec/11 20:33;jbellis;Hmm.  I thought the other place was OTC, but that's going to NPE in the current code base.  So +1 for this patch.;;;","01/Dec/11 20:34;jbellis;(Checked, and 0.8 OTC does have the null check.  So we're good there.);;;","01/Dec/11 20:37;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossiper.addSavedEndpoint should never add itself,CASSANDRA-3485,12531187,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,11/Nov/11 22:22,16/Apr/19 09:32,14/Jul/23 05:52,09/Dec/11 21:20,0.8.9,1.0.6,,,,,0,,,,"Somehow, people are running into a situation where nodes are adding themselves to the persisted ring cache.  Since SS is initialized after the Gossiper and calls addSavedEndpoint on it, which inits the nodes with a generation of zero, this ends up with nodes using a generation of zero and thus never being marked as alive.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1518,,,,,,"13/Nov/11 21:38;brandon.williams;3485.txt;https://issues.apache.org/jira/secure/attachment/12503554/3485.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,216924,,,Sat Dec 10 11:32:26 UTC 2011,,,,,,,,,,"0|i0gkhj:",94749,,thepaul,,thepaul,Normal,,,,,,,,,,,,,,,,,"11/Nov/11 22:26;brandon.williams;Note that you can get around this with -Dcassandra.load_ring_state=false;;;","13/Nov/11 21:38;brandon.williams;Patch to prevent adding our own endpoint to saved endpoints, and if detected, remove it.;;;","09/Dec/11 20:20;thepaul;+1.;;;","09/Dec/11 21:20;brandon.williams;Committed.;;;","10/Dec/11 11:32;hudson;Integrated in Cassandra-0.8 #416 (See [https://builds.apache.org/job/Cassandra-0.8/416/])
    Prevent gossiper from adding itself to saved endpoints.
Patch by brandonwilliams reviewed by Paul Cannon for CASSANDRA-3485.

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1212624
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bizarre Compaction Manager Behaviour,CASSANDRA-3484,12531167,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,dhendry,dhendry,11/Nov/11 19:04,16/Apr/19 09:32,14/Jul/23 05:52,11/Nov/11 20:01,1.0.3,,,,,,0,,,,"It seems the CompactionManager has gotten itself into a bad state. My 1.0.2 node has been up for 20 hours now - checking via JMX, the compaction manager is reporting that it has completed 14,797,412,000 tasks. Yep, thats right 14 billion tasks and increasing at a rate of roughly 208,400/second. 

I should point out that I am currently running a major compaction on the node. My theory is that this problem was introduced by CASSANDRA-3363. It looks like SizeTieredCompactionStrategy.getBackgroundTasks() returns a set of task without consideration for any in-progress compactions. Compactions are only kicked off if task.markSSTablesForCompaction() returns true (CompactionManager line 127) but the task resubmission is based only on the task list not being empty (CompactionManager line 141). Should the logic not be to only reschedule if a task has actually been executed?

I am just waiting now for the major compaction to finish to see if the problem goes away as would be suggested by my theory.","RHEL 6
java version ""1.6.0_26""
6 node cluster (5 nodes 0.8.6, 1 node 1.0.2 minus CASSANDRA-2503)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/11 19:19;dhendry;3484.txt;https://issues.apache.org/jira/secure/attachment/12503407/3484.txt","11/Nov/11 19:07;dhendry;compaction.png;https://issues.apache.org/jira/secure/attachment/12503403/compaction.png",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,216905,,,Fri Nov 11 20:01:39 UTC 2011,,,,,,,,,,"0|i0gkh3:",94747,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"11/Nov/11 19:07;dhendry;JMX evidence;;;","11/Nov/11 19:19;dhendry;Patch to only reschedule another compaction check when the active check resulted in a task being executed;;;","11/Nov/11 19:24;jbellis;I think the patch on #2407 would also fix this.;;;","11/Nov/11 19:33;dhendry;Yes it would. The issue seemed to result in some pretty significant temporary performance degradation. Any chance of getting 2407 into 1.0.3 instead of 1.1?;;;","11/Nov/11 19:47;slebresne;We can commit either this patch or the one on CASSANDRA-2407 for this issue (since they both fix the issue here). But the goal of #2407 is a bit different so we just should leave that issue solve the problem it want to solve.;;;","11/Nov/11 20:01;slebresne;Alright, +1 on the patch here, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support bringing up a new datacenter to existing cluster without repair,CASSANDRA-3483,12531095,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,scode,lenn0x,lenn0x,11/Nov/11 04:59,16/Apr/19 09:32,14/Jul/23 05:52,30/Jan/12 15:24,1.1.0,,,,,,2,,,,"Was talking to Brandon in irc, and we ran into a case where we want to bring up a new DC to an existing cluster. He suggested from jbellis the way to do it currently was set strategy options of dc2:0, then add the nodes. After the nodes are up, change the RF of dc2, and run repair. 

I'd like to avoid a repair as it runs AES and is a bit more intense than how bootstrap works currently by just streaming ranges from the SSTables. Would it be possible to improve this functionality (adding a new DC to existing cluster) than the proposed method? We'd be happy to do a patch if we got some input on the best way to go about it.
",,brandon.williams,cywjackson,harishd,marcuse,psanford,stuhood,vijay2win@yahoo.com,yulinyen,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3807,,,CASSANDRA-3833,,,,,,,,"30/Jan/12 14:49;jbellis;3483-cleanup.txt;https://issues.apache.org/jira/secure/attachment/12512421/3483-cleanup.txt","30/Jan/12 12:47;slebresne;3483-v3.patch;https://issues.apache.org/jira/secure/attachment/12512409/3483-v3.patch","30/Nov/11 18:29;scode;CASSANDRA-3483-0.8-prelim.txt;https://issues.apache.org/jira/secure/attachment/12505645/CASSANDRA-3483-0.8-prelim.txt","05/Dec/11 01:40;scode;CASSANDRA-3483-1.0.txt;https://issues.apache.org/jira/secure/attachment/12506076/CASSANDRA-3483-1.0.txt","24/Dec/11 04:08;scode;CASSANDRA-3483-trunk-noredesign.txt;https://issues.apache.org/jira/secure/attachment/12508583/CASSANDRA-3483-trunk-noredesign.txt","28/Jan/12 05:31;scode;CASSANDRA-3483-trunk-rebase2.txt;https://issues.apache.org/jira/secure/attachment/12512290/CASSANDRA-3483-trunk-rebase2.txt","28/Jan/12 07:06;scode;CASSANDRA-3483-trunk-refactored-v1.txt;https://issues.apache.org/jira/secure/attachment/12512294/CASSANDRA-3483-trunk-refactored-v1.txt","28/Jan/12 10:00;scode;CASSANDRA-3483-trunk-refactored-v2.txt;https://issues.apache.org/jira/secure/attachment/12512299/CASSANDRA-3483-trunk-refactored-v2.txt",,,,,,,8.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,216833,,,Mon Jan 30 18:12:02 UTC 2012,,,,,,,,,,"0|i0gkgn:",94745,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"11/Nov/11 05:43;lenn0x;Some discussion from irc:

{noformat}
23:43 < goffinet> has datastax ever had a customer add a new datacenter to an existing cluster? No docs or info on web suggest anyone has done this before
23:44 < driftx> yeah
23:44 < goffinet> how is it done? we are running a case where if i modify strategy options before adding nodes, writes will fail since no endpoints for DC have been added
23:44 < goffinet> we were expecting this might work because we want to bootstrap the new DC to the existing cluster
23:44 < goffinet> take on writes + stream data with RF factor
23:45 < driftx> general best practice is (jbellis can correct if I'm outdated) add the dc at rf:0, add the nodes/update snitch, repair
23:45 < driftx> err, update rf, repair
23:46 < goffinet> yeah mind if i open up a jira? that seems extreme to make the cluster do that .. ?
23:46 < goffinet> or is repair smart enough to just stream ranges instead of AES?
23:46 < driftx> 'instead of AES?' that's what repair is, but if just streams ranges
23:46 < driftx> s/if/it/
23:47 < goffinet> right but AES builds merkle tree, scans through all data ?
23:47 < goffinet> isn't bootstrap a different operation?
23:47 < goffinet> when streaming just sstables
23:47 < driftx> yeah, it is
23:47 < goffinet> yeah thats more heavy. dont understand why we couldnt use that instead
23:47 < goffinet> like bootstrap
23:48 < stuhood> now that i think about it, it doesn't really make sense that a CL.ONE write fails if a DC isn't available
23:48 < stuhood> independent of the bootstrap case, that sounds like the real issue
23:49 < stuhood> goffinet: ^
23:50 < driftx> hmm, yeah that doesn't
23:50 < driftx> but the problem with bootstrapping a dc is the first node you bootstrap gets everything
23:50 < goffinet> stuhood: yeah. it was complaining about not enough endpoints 
23:50 < goffinet> driftx: why is that? if you are doubling the cluster, and assign the tokens manually ?
23:51 < driftx> still have to do them 2 mins apart, and they're probably going to be part of the same replica set which I think is troublesome too
23:51 < goffinet> driftx: maybe we can make repair a bit more intelligent? if no data exists on the node .. just stream the ranges instead of using AES
23:52 < driftx> problem is we're pushing AES to do the entire replica set (which is nearly does now)
23:52 < stuhood> goffinet: it shouldn't be as heavyweight as you're thinking
23:53 < goffinet> stuhood: but we have a way currently that is less heavy
23:53 < goffinet> i dont understand why we couldnt use that method
23:53 < stuhood> not implemented =)
23:53 < goffinet> don't cut corners :)
23:53 < stuhood> human time vs cpu time =P
23:54 < driftx> you could almost do something like #3452 and then have a jmx call to say 'ok, finish'
23:54 < CassBotJr> https://issues.apache.org/jira/browse/CASSANDRA-3452 : Create an 'infinite bootstrap' mode for sampling live traffic
23:54 < driftx> except the first one that tries is going to have every node pound it with all the writes
23:54 < goffinet> driftx: ill make a jira ticket so we can discuss there, it doesn't seem like it would be too much trouble to support this use case
23:54 < goffinet> we'd be happy to write the patch after some input
23:55 < driftx> trickier than it sounds I'll bet, but sgtm
23:57 < stuhood> alternatively, is now the right time to add back group bootstrap?
23:58 < stuhood> so you'd 1) add the dc to the strategy, 2) do a group bootstrap of the entire dc
23:58 < stuhood> would also have to fix the CL.ONE problem though.
23:59 < goffinet> how did group bootstrap work again?
23:59 < driftx> #2434 is relevant
23:59 < CassBotJr> https://issues.apache.org/jira/browse/CASSANDRA-2434 : range movements can violate consistency
--- Day changed Fri Nov 11 2011
00:00 < stuhood> goffinet: bootstrapping many nodes at once without the 2 minute wait
00:01 < goffinet> why was it removed?
00:01 < stuhood> used zookeeper
00:01 < goffinet> oh.
00:01 < stuhood> but come to think of it, removing the 2 minute wait would seem to be relatively easy
00:02 < goffinet> stuhood, i thought the 2 minute wait was just waiting for ring state to settle?
00:02 < goffinet> before it streamed from nodes
00:02 < stuhood> goffinet: yea: you could form a ""group"" bootstrap by inverting things and waiting until you -hadn't- seen a new node in 2-10 minutes before you chose a token and started bootstrapping
00:03 < stuhood> so, not terribly simple, but.
00:04 < stuhood> you'd basically have a bunch of nodes sitting around waiting until no new nodes started, and then they have to deterministically choose tokens.
00:05 < goffinet> yes
00:05 < stuhood> well, alternatively, you wouldn't need a new way to deterministically choose tokens
00:05 < stuhood> (easier)
00:05 < stuhood> no… scratch that. you would need a way
00:05 < stuhood> for this DC case, all of the nodes are entering an empty ring
00:06 < stuhood> so the group would need to choose something balanced
00:06 < goffinet> empty ring?
00:06 < stuhood> yea, essentially… there are no tokens in that dc
00:06 < goffinet> but we were going to provide the tokens manually?
00:06 < goffinet> were you thinking of making it automatic?
00:07 < stuhood> yea. fixing bootstrapping groups of nodes would make automatic safe again
00:08 < stuhood> so… whatever state a node is in when it is sitting and waiting for enough information to choose a token, it should just stay that way and watch what other nodes enter that state
00:08 < goffinet> so i have a question about the 120 second window you have to wait..
00:09 < stuhood> mm
00:09 < driftx> hmm, what if they started up at rf:0 but stayed in some dead state (hibernate might work) without doing anything until you changed the rf, then actually bootstrapped?
00:09 < goffinet> so imagine i startup all the nodes in DC2 at same time, does join_ring=false not grab gossip info at all? I was thinking it would be good if we could just start gossip on all nodes, but until operator says 'go' then i could bootstrap them all at same time
00:09 < goffinet> since i would only have to wait at most 120 seconds before kicking them all off
00:10 < stuhood> driftx: yea, that could work too… but you'd still need to choose tokens. (also, the rf=0 thing shouldn't be necessary, right? that's the CL.ONE bug)
00:11 < driftx> well, you really want to choose tokens anyway
00:11 < stuhood> goffinet: it does get gossip… i think that's basically equivalent to the pre-join state
00:11 < driftx> I guess you don't need rf=0 if all the nodes are in hibernate
00:12 < goffinet> yeah i think you do need hibernate in this case, because if i set tokens upfront, i want all nodes to know about ATL ones too
00:12 < goffinet> before i kick off bootstrap
00:12 < stuhood> driftx: i'm confused… what is the difference between rf=0 and not being there?
00:12 < stuhood> is that a workaround for the CL.ONE bug?
00:13 < driftx> you know there's a dc with rf:0, can add one with impacting anything
00:13 < driftx> err, without
00:14 ?? boaz__ (0819c319@gateway/web/freenode/ip.8.25.195.25) has joined #cassandra-dev
00:14 < stuhood> so what was the point of adding it? that's why i'm confused...
00:14 < goffinet> im fine with rf:0, its so you can add the nodes to the cluster before calling repair
00:14 < goffinet> before you add nodes
00:15 < driftx> because the dc is in the schema
00:15 < driftx> so you need it there to have nodes be in it
00:15 < stuhood> ah
00:16 < goffinet> driftx: any reason why we couldnt just fix that? so dc2:3 wont throw an error if nodes are down?
00:16 < goffinet> that way you would needed to do two steps
00:16 < goffinet> dc2:0, add nodes, dc2:3
00:16 < goffinet> wouldn't*
00:16 < driftx> I don't understand, you can already do that
00:17 < driftx> you just have to repair afterwards
00:17 < goffinet> it throws an error currently? if you set dc2:3 and no nodes exist for dc2
00:17 < goffinet> we'll double check on that
00:18 < goffinet> for writes
00:18 < driftx> oh, it does
00:19 < driftx> but only for writes
00:19 < goffinet> yeah
00:19 < goffinet> so thats fine, thats fixable
00:19 < goffinet> im just curious about a) how can we bootstrap nodes without 120s delays between N nodes b) stream from DC1 without AES
00:21 < stuhood> goffinet: if you figure out a, i don't think b is necessary?
00:22 < stuhood> assuming they are aware of the other joining nodes, and can all join the same range
00:22 < stuhood> that would be the keystone for some kind of group bootstrap
00:23 < goffinet> let me test out join_ring, because im curious. if join_ring=false still gossips but doesnt offically join.. it would be nice if node 2 in DC2 knew about that node too somehow?
00:23 < driftx> that's why I proposed cheating, add them all as non-members, then ask them to bootstrap
00:23 < goffinet> because then .. i could just run a command on each node at same time
00:23 < goffinet> since they all know about each other in a hibernate state
00:23 < goffinet> driftx: yes i like that
00:24 < driftx>     private void joinTokenRing(int delay) throws IOException, org.apache.cassandra.config.ConfigurationException
00:24 < driftx>     {
00:24 < driftx>         logger_.info(""Starting up server gossip"");
00:24 < driftx> they don't use gossip with join_ring off
00:24 < stuhood> but will that actually allow them to all join the same range?
00:24 < goffinet> okay cool, yeah we would need to make it join in that special state then
00:25 < stuhood> i think there is an edgecase here… if multiple nodes are joining the same range, and one of them fails, then should they all fail?
00:25 < driftx> no, it basically saves you server startup time that is not ring-related :)
00:25 < goffinet> stuhood, they all know the tokens ahead of time?
00:25 < goffinet> they just need to know the current global state of things
00:25 < stuhood> goffinet: right, but if they are streaming the range that they will be responsible for...
00:26 ?? mw1 (~Adium@8.25.195.29) has quit (Quit: Leaving.)
00:26 < stuhood> Joining nodes don't stick around if they fail
00:26 < goffinet> they shouldnt be allowed to do that until they joined ?
00:26 < stuhood> nah, you stream while you are joining… unless you are talking about repair
00:26 < goffinet> stuhood: was that removed? i thought u had to still remove the node
00:26 < goffinet> using the new options in 1.0
00:26 < stuhood> don't know about 1.0
00:27 < driftx> no, a failed non-member is just a fat client and disappears
00:27 < goffinet> but i thought there was a timeout for fat client ?
00:27 < goffinet> is it 30s or something?
00:27 < driftx> yes
00:28 < goffinet> so nodes that arent fat clients, why might we remove them ? if we didnt..
00:28 < goffinet> and let the operator do it
00:28 < goffinet> or have a larger timeout
00:28 < goffinet> might make this a non-issue
00:28 < driftx> what does a larger timeout/keeping them around buy you?
00:29 < goffinet> because if they go away, and i bootstrap after they failed, wont my view of ring be skewed?
00:29 < stuhood> driftx: i guess in this case, the node would resume bootstrapping from where it left off
00:29 < driftx> it would've missed writes in the meantime and require a repair afterwards anyway
00:29 < stuhood> sorry… ""resume"" in the sense of ""start over"", but yea
00:31 < stuhood> that would be a pretty big change, but it might make sense
00:31 < goffinet> stuhood: what would you change
00:31 < stuhood> what you said, about nodes in joining staying in joining
00:31 < stuhood> so if the machine restarts, it begins joining at the same position again
00:33 < goffinet> if we supported that + letting nodes gossip in hibernate, would allow us to add capacity at operator control
{noformat};;;","16/Nov/11 20:49;slebresne;I think it wouldn't be crazy and actually very (very) simple to add a new nodetool command (rebuild?) that would basically have the node asks the other replicas to stream all there data to him (for the correct ranges obviously). In other words, a command that force the streaming part of bootstrap without all the join ring part. Or another way to say is to have the streaming part of a repair but without the validation part.

The method to add a new DC would be the same as today except that repair would be replaced by this new operation.;;;","16/Nov/11 20:52;brandon.williams;We would still need to put the nodes into a 'bootstrap' state to get incoming writes forwarded to them, otherwise you have to repair in the end anyway.;;;","17/Nov/11 04:27;lenn0x;Yeah as Brandon mentioned, we would still want to go into the bootstrap state to get those writes. This would also allow us to add capacity in the same way, if we manually pick tokens (auto bootstrap is kinda worthless IMO) to existing DC as well. We can just fire off the bootstrap command from nodetool as needed.;;;","17/Nov/11 06:30;lenn0x;Sorry, I had to re-read what Sylvain said for it to 'click'. So the process he proposes is as follows with RF of 3:

1. strategy options dc2:0
2. bring up new nodes in dc2 with auto_bootstrap off and token set
3. set strategy options dc2:3
4. run 'rebuild' on each node in dc2

this would handle the writes part.

i was kinda hoping though that we could modify the gossip state because I could very well see this playing into the case where you weren't adding DCs but wanted to add lots of nodes (60-100 like we do currently) ... and wanted to have them all added to existing DC.. having the bootstrap defined that way, would allow us to bootstrap nodes as we please in existing DCs, bring them all to the ring at once to have a consistent state without taking on traffic until they transitioned states (joining/normal). Where as this proposal wouldn't be able to satisfy that use case.

;;;","17/Nov/11 09:18;slebresne;So you're proposing to add support for bootstrapping multiple nodes together. I'm not against that, it would be nice and that would give you 90% of what this ticket is about (you'd have to add the ability to multi-boostrap *and* add a DC/augment the replication faction at the same time). But it is orders of magnitude more complicated than what I'm suggesting. Which is not a problem in itself given it's a broader solution, but it means we'll have multi-node boostrap at best for 1.1, while I'm pretty sure I can get the 'rebuild' command wrote in like an hour (and I see no reason why it couldn't be put in the 1.0 series).

;;;","18/Nov/11 00:22;lenn0x;Sylvain, it looks like a state 'HIBERNATE' exists in GOSSIP already based on recovering a dead node in 1.0. Do you have a preference on the name of the new state if we attempted adding multiple nodes patch? WAITING? STANDBY?;;;","18/Nov/11 04:22;scode;I was thinking of GHOST too after I realized we'll need a state where the node receives neither writes nor reads (more details here later).;;;","21/Nov/11 21:03;jbellis;So, am I understanding correctly that we're talking about two different scenarios?

- Add new DC without repair
- Add many nodes to existing DC without RING_DELAY in between

I think Sylvain's proposal addresses the first nicely.  So what I need help with is understanding what problem you're trying to solve with the second part.  Dealing with overlapping ranges in node movement basically requires a rewrite of that subsystem (CASSANDRA-2434).  But I suspect there is a ""good enough"" solution that we could find if I understood better what your pain point is here.;;;","22/Nov/11 05:57;lenn0x;Jonathan,

You are correct. Sylvain's proposal does satisfy this ticket. It doesn't solve the case of (2) where if you want to add lots of nodes in an existing DC, and you know the tokens they should be at, and want to join them all at once.

Our use case is, we actually add 60-100 nodes in one big capacity add. We would like to avoid the 120 second per node time frame. It's not a deal breaker though. We actually realized though if we are adding that many nodes to our cluster, with a large cluster already, we need to rebalance heavily anyway.  Peter is almost done with the 'rebuild' patch, I'm assigning him to this ticket. 

Our next big focus is improving the rebalancing of a cluster. We have a very large cluster and after adding 100 nodes every month or so, this becomes painful. Almost all of our nodes have over 600GB+ each. We have an application that will require us to be rebalancing at all times to reduce our hot spots.

;;;","23/Nov/11 00:51;scode;We ended up going for the simpler rebuild patch as Chris hinted at. I'll quote myself:

{quote}
I've been looking at this some more.

Here's the proposal so far:

We introduce a GHOST state, in which a node receives neither reads nor writes. This allows us to bring in a group of nodes in the ring without suffering any ill effects. It is completely invisible to reads and writes, and will never count towards e.g. consistency level.

Once all ghosts are ready, we can start bootstrapping, taking ghost nodes into accounts for purposes of determining which range we are responsible for, but streaming only from non-ghost nodes. This accomplishes the goal of not transferring more data than necessary.

In order to avoid a bootstrapping node from taking writes for more than it's eventual share, we'd have to make the write endpoints be aware of ghost nodes. This is dosable, but not critical since we're bisecting a range that was previously handled by a single node anyway so the traffic would be managable. It would just be cleaner to not have to cleanup afterwards.

Once we transition from bootstrapping to being ""up"", we have a bigger problem however. If the read paths and write paths are only aware of non-ghost nodes, the read/write paths would think that these nodes had more ownership than they really do.

So we must really be taking into account the other nodes in the read/write path as well - but only when determining ownership of a completely bootstrapped node that was previously a ghost. That means we must distinguish between a ""normally up"" node and one that's been bootstraped from ghost state (call it ""ghost strapped"").

This suddenly gets complex. The process of group bootstrap would then be:

Add a bunch of nodes in GHOST state
Bootstrap all of them, each of them going into GHOSTSTRAPPED state
Once all are GHOSTSTRAPPED, we can safely transntion from GHOSTSTRAPPED to normal/up
Is there a simpler solution?
{quote}

After some additional discussion we felt this was adding to much complexity and potential edge cases/bugs that it became more cost-effective to just go with the simple rebuild for our immediate needs, hoping to address the problem of adding lots of nodes to a DC separately in some other way.

A patch is forthcoming soon.
;;;","30/Nov/11 18:29;scode;Here is a patch rebased against 0.8 for cursory review. I do not expect this to go into 0.8, and in fact I have not tested this patch other than build against vanilla 0.8 (the original patch is tested, but against our internal 0.8).

If there are no concerns with the overall implementation, I'll submit a rebased version for 1.0/trunk.

There are two components of the change:

* Breaking out the streaming part of BootStrapper into a separate RangeStreamer. Change BootStrapper to use that.

* Implement the rebuild command on top of RangeStreamer.

There are two ways to invoke rebuild:

{code}
nodetool rebuild
nodetool rebuild nameofdc
{code}

The first form streams from nearest endpoints, while the latter streams from nearest endpoints in the specified data center.

;;;","30/Nov/11 18:40;brandon.williams;bq. If there are no concerns with the overall implementation, I'll submit a rebased version for 1.0/trunk.

This looks good to me, I like the RangeStreamer abstraction.;;;","05/Dec/11 01:40;scode;Attached is a version rebased against 1.0 (and tested).;;;","05/Dec/11 16:09;jjordan;Once this option is in, is this the procedure for running rebuild (with 4 changed to 'rebuild dc1')?

{quote}
1. strategy options dc2:0
2. bring up new nodes in dc2 with auto_bootstrap off and token set
3. set strategy options dc2:3
4. run 'rebuild' on each node in dc2
{quote}

Do we need to stagger issuing the rebuild commands or can they be run all at once?
;;;","05/Dec/11 19:28;scode;Yes, that looks good.

And yes, you can run rebuilds concurrently as long as you're comfortable with the amount of bandwidth you'll be pushing and the load you'll be putting on the source nodes.

However, if you expect to see reasonable performance and streaming at full speed to all nodes, you also need CASSANDRA-3494.

Regardless: I strongly recommend testing this with your exact version of Cassandra before trying it for real.
;;;","14/Dec/11 00:37;slebresne;I haven't applied the patch yet, it needs rebase and preferably against trunk since that is the likely target for this, but a few comments.

We could have more reuse of code between Boostrapper ant the rebuild command.  Typically:
* RangeStreamer.getAllRangeWithSourcesFor does essentially the same thing that Boostrapper.getRangesWithSources, so it would be nice to do some reuse.
* In rebuild, we essentially have the code of Boostrapper.getWorkMap, again would be nice to do some code reuse.

I think we should move all of those in RangeStreamer and ultimately Boostrapper.boostrap() should be just one call to rebuild with the right arguments (mostly the correct tokenMetada instance and the ""myRange"" collection).

A few nits:
* rebuild code could be simplified slightly by using StorageService.getLocalRanges()
* rebuild doesn't fully respect the code style.
;;;","14/Dec/11 01:52;scode;I'll get it rebased once it's otherwise okay.

As for re-use: I had intermediate versions that tried to do this, but ever time I ended up realizing that it was exploding in verbosity at the point where I was using the abstraction so it didn't actually help. However, I think there were a few changes towards the end after which I didn't re-evaluate.

I'll look at it again and see what I can do.;;;","14/Dec/11 01:52;scode;I'll get it rebased once it's otherwise okay.

As for re-use: I had intermediate versions that tried to do this, but ever time I ended up realizing that it was exploding in verbosity at the point where I was using the abstraction so it didn't actually help. However, I think there were a few changes towards the end after which I didn't re-evaluate.

I'll look at it again and see what I can do.;;;","24/Dec/11 04:08;scode;Attaching version rebased to trunk but not yet re-factored.;;;","25/Jan/12 21:24;jbellis;Peter, are you planning to follow up on Sylvain's comments still?;;;","28/Jan/12 05:31;scode;I do. I'm sorry for the delay, this has been nagging me for quite some time. It's not forgotten, I have just been inundated with urgent stuff to do.

I'm attaching a fresh rebase against current trunk and I hope to submit an improved version later tonight (keyword being ""hope"").;;;","28/Jan/12 07:06;scode;{{CASSANDRA\-3483\-trunk\-refactored\-v1.txt}} addresses the duplication between BootStrapper and RangeStreamer.

Next patch will address rebuild/getworkmap duplication.
;;;","28/Jan/12 07:07;scode;(It also contains the addition of a brace from CASSANDRA-3806; this is intentional to avoid pain.) ;;;","28/Jan/12 07:12;scode;I borked the unit test, will address that too.;;;","28/Jan/12 10:00;scode;{{CASSANDRA\-3483\-trunk\-refactored\-v2.txt}} I believe addresses the concerns, plus makes other improvements. I'm much more happy with this one.

It addresses CASSANDRA-3807 by supporting fetch ""consistency levels"" (though only ONE is currently usable without patching), and the filtering of hosts is abstracted out.

There is still some duplication between {{Bootstrapper.bootstrap()}} and {{StorageService.rebuild()}} in that both do the dance of iteration over tables to construct the final map. I am not really feeling that abstracting away that is a good idea to include in this ticket, though I think it's worthwhile doing at some point separately.

The unit test is fixed; my adjustment of it was wrong because I wasn't picking pending ranges (in the test).

I've tested both rebuild and bootstrap in a 3 node cluster.

I've added some more logging than what is typically the case; there have been several cases where I wished streaming was logged in more detail at INFO, particularly when bootstrapping or rebuilding. I think it's worthwhile to get that in while at it.;;;","30/Jan/12 12:47;slebresne;bq. It addresses CASSANDRA-3807 by supporting fetch ""consistency levels"" (though only ONE is currently usable without patching)

I think a first issue is that current bootstrap does not fail if no node is alive for a given range, which arguably it should. I'm good with doing that, though it would be worth backporting to 1.0 too so it may be worth splitting that to a separate patch (or rather just create one for the fix in 1.0).

However, that does not solve the problem of bootstrap possibly breaking the consistency contract. The problem being that if we transfer a range from a node that happens to be lacking behind in term of consistency, and we end up replacing a node that was not lacking behind, we could break some consistency contracts. To fix that, I really only see only one solution right off the bat (which doesn't mean there isn't other): it is to ensure that for each range, we transfer it from (at least) the node we will replace for this range.

I believe the FetchConsistencyLevel of this patch is making an attempt to fix this by allowing to fetch from more than one node. While it does make it less likely to break consistency, unless we fetch from all nodes (and thus the one we'll replace), we cannot be sure we won't break the consistency level for people that say write at CL.ALL and read at CL.ONE. Overall, I fully agree this is a problem that we should fix someone, but I'm not sure the FetchConsistencyLevel is the right solution and even if it is it's a complicated enough problem that it's worth it's own ticket. I would agree that the problem with rebuild is a little bit different, but since anyway the patch introduce FCL without using it, let's keep that for later if that's ok.

bq. There is still some duplication between Bootstrapper.bootstrap() and StorageService.rebuild() in that both do the dance of iteration over tables to construct the final map. I am not really feeling that abstracting away that is a good idea to include in this ticket

I think it is, at least for a good chunk of it. It's not very complicated, it clearly improves code readability and since the patch already refactor that code I don't see a good reason to push that to later, especially if we agree it's worthwhile.

Attaching a v3 that 1) remove FetchConsistencyLevel for the reasons above and 2) move most of the details of creating the multimaps in RangeStreamer.
;;;","30/Jan/12 14:04;jbellis;bq. Overall, I fully agree this is a problem that we should fix someone, but I'm not sure the FetchConsistencyLevel is the right solution and even if it is it's a complicated enough problem that it's worth it's own 

This is CASSANDRA-2434 isn't it?;;;","30/Jan/12 14:09;slebresne;bq. This is CASSANDRA-2434 isn't it?

it is.;;;","30/Jan/12 14:49;jbellis;cleanup patch addressing mostly typos and style.  only substantial code change was to RangeStreamer.getRangeFetchMap.  Also, moved OperationType.REBUILD to the end of the enum to make sure we don't break anything depending on ordinal.

+1 from me otherwise.;;;","30/Jan/12 15:24;slebresne;Committed v3 + Jonathan's cleanups (and a fix to the unit test).;;;","30/Jan/12 18:12;scode;For the record I never intended to fix the general problem of bootstrapping never violating consistency. But in retrospect it's obvious how my choice of naming would make it sound like I did :) I agree it's a problem for its own ticket.

Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flush Assertion Error - CF size changed during serialization,CASSANDRA-3482,12531051,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,dhendry,dhendry,10/Nov/11 21:05,16/Apr/19 09:32,14/Jul/23 05:52,11/Nov/11 17:56,1.0.3,,,,,,0,,,,"I have seen the following assert in the logs - there are no other suspicious or unexpected log messages.

INFO [FlushWriter:9] 2011-11-10 13:08:58,882 Memtable.java (line 237) Writing Memtable-UserData@1388955390(25676955/430716097 serialized/live bytes, 478913 ops)
ERROR [FlushWriter:9] 2011-11-10 13:08:59,513 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[FlushWriter:9,5,main]
java.lang.AssertionError: CF size changed during serialization: was 4 initially but 3 written
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:94)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeWithIndexes(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:177)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:264)
        at org.apache.cassandra.db.Memtable.access$400(Memtable.java:47)
        at org.apache.cassandra.db.Memtable$4.runMayThrow(Memtable.java:289)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Once the error occurs, further MemtablePostFlusher tasks are blocked:

nodetool tpstats:
  Pool Name                    Active   Pending      Completed   Blocked  All time blocked
  MemtablePostFlusher               1        18             16         0                 0

It *seems* that all further flushed for the particular CF (in this case UserData) will also result in the same assertion error. Restarting the node fixes the problem.","RHEL 6
java version ""1.6.0_26""
6 node cluster",jjordan,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/11 16:38;jbellis;3482.txt;https://issues.apache.org/jira/secure/attachment/12503383/3482.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,216789,,,Fri Nov 11 17:56:43 UTC 2011,,,,,,,,,,"0|i0gkg7:",94743,,slebresne,,slebresne,Critical,,,,,,,,,,,,,,,,,"10/Nov/11 21:13;dhendry;I should also point out that the number of columns being logged in the error, 3/4, is MUCH smaller than the expected column count based on the flushed bytes.

Just based on the usage pattern for this CF and reading the description for CASSANDRA-2503, I suspect (without any real evidence) they may be related.;;;","10/Nov/11 21:22;dhendry;Oh, and the problem crops up again with 1h of restarting the node so. ""Restarting the node fixes the problem"" isnt exactly true.;;;","11/Nov/11 15:40;dhendry;Pretty sure CASSANDRA-2503 introduced this problem. 

My experimental evidence at this point (since I have not traced through the code) is that 2 out of 2 times the node was started using 1.0.2 (release), the assert cropped up within 1 hour of going live (first time was after 59 mins, second was after 23 mins). I reverted the changes introduced in CASSANDRA-2503 (still running 1.0.2), and the node has now been up for 17 hours with no problems.;;;","11/Nov/11 16:38;jbellis;I bet you're right.  Patch attached to fix the race introduced by 2503.;;;","11/Nov/11 17:56;slebresne;+1, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"During repair, ""incorrect data size"" & ""Connection reset"" errors. Repair unable to complete.",CASSANDRA-3481,12530929,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,efalcao,efalcao,09/Nov/11 21:31,16/Apr/19 09:32,14/Jul/23 05:52,16/Nov/11 13:39,1.0.3,,,,,,0,connection,repair,,"This has been happening since 1.0.2. I wasn't on 1.0 for very long but I'm fairly certain repair was working ok. Repair worked decently for me in 0.8 (data bloat sucked). All my SSTables are version h.

On one node:

java.lang.AssertionError: incorrect row data size 596045 written to /mnt/cassandra/data/TRProd/Metrics1m-tmp-h-25036-Data.db; correct is 586675
	at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:253)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:146)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:87)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)

On the other node:

4999 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-24953-Data.db sections=1707 progress=0/1513497639 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25000-Data.db sections=635 progress=0/53400713 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25002-Data.db sections=570 progress=0/709993 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25003-Data.db sections=550 progress=0/449498 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25005-Data.db sections=516 progress=0/316301 - 0%], 6 sstables.
 INFO [StreamStage:1] 2011-11-09 19:45:22,795 StreamOutSession.java (line 203) Streaming to /10.38.69.192
ERROR [Streaming:1] 2011-11-09 19:47:47,964 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Streaming:1,1,main]
java.lang.RuntimeException: java.net.SocketException: Connection reset
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
	at com.ning.compress.lzf.ChunkEncoder.encodeAndWriteChunk(ChunkEncoder.java:133)
	at com.ning.compress.lzf.LZFOutputStream.writeCompressedBlock(LZFOutputStream.java:203)
	at com.ning.compress.lzf.LZFOutputStream.write(LZFOutputStream.java:97)
	at org.apache.cassandra.streaming.FileStreamTask.write(FileStreamTask.java:181)
	at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:145)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
ERROR [Streaming:1] 2011-11-09 19:47:47,970 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Streaming:1,1,main]
java.lang.RuntimeException: java.net.SocketException: Connection reset
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
	at com.ning.compress.lzf.ChunkEncoder.encodeAndWriteChunk(ChunkEncoder.java:133)
	at com.ning.compress.lzf.LZFOutputStream.writeCompressedBlock(LZFOutputStream.java:203)
	at com.ning.compress.lzf.LZFOutputStream.write(LZFOutputStream.java:97)
	at org.apache.cassandra.streaming.FileStreamTask.write(FileStreamTask.java:181)
	at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:145)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/11 18:19;slebresne;3481-v2.patch;https://issues.apache.org/jira/secure/attachment/12503265/3481-v2.patch","10/Nov/11 12:56;slebresne;3481.patch;https://issues.apache.org/jira/secure/attachment/12503201/3481.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,216667,,,Wed Nov 16 13:39:47 UTC 2011,,,,,,,,,,"0|i0gkfj:",94740,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"10/Nov/11 08:53;slebresne;What is in this 'Metrics' column family? Is it normal columns, some expiring ones, super columns, counters?;;;","10/Nov/11 12:56;slebresne;We do have a problem with counters that leads to that exact problem.

During streaming, we must ensure that whatever we deserialize from the wire has the same size once reserialized. For counters, whenever we should have clean deltas, instead we just 'mark' the delta to be clean in a way that doesn't change the size and let them be clean later. Problem is, if you stream a file that still have those 'marked' delta, it will remove them, changing the size of the data and thus triggering the exception at end.

Fixing this require knowing at deserialization that we are in this 'preserve the size' mode. Attaching a patch for that. Instead of adding a new flag to deserialization, I replaced 'fromRemote' boolean by an enum flag. The patch includes a unit test.
;;;","10/Nov/11 14:10;jbellis;is this 1.x only or 0.8 too?;;;","10/Nov/11 14:16;slebresne;It's 1.x only as far as the bug is concerned (it's really linked to single-pass streaming). That being said, the bulk of the patch, i.e, replacing the fromRemote boolean by a Flag, could be backported in 0.8 for purpose of making future merge easier. But I'm skeptical this is worth it.;;;","10/Nov/11 15:27;efalcao;Metrics1m is a Counter CF. I originally tried CounterColumns with TTL's but was advised against it. They have no expiration.

Let me know if I can be any more help. I might be able to try the patch a bit later in the day.;;;","10/Nov/11 18:03;jbellis;Shouldn't IncomingStreamReader use PRESERVE_SIZE instead of FROM_REMOTE?

What does the commented-out test do, and why is it commented out?

nit: this comment needs updating:

{noformat}
             // deserialize column with fromRemote false, in order to keep size of streamed column
{noformat}
;;;","10/Nov/11 18:19;slebresne;v2 attached

bq. Shouldn't IncomingStreamReader use PRESERVE_SIZE instead of FROM_REMOTE?

That would work too but we don't need to, because we won't echo the columns like in the other branch. So it's better to use FROM_REMOTE and have the counter delta cleaned right away rather than later (as this is slightly more efficient). I added a comment.

bq. What does the commented-out test do, and why is it commented out?

Oups, attached the wrong patch. I just copied a previous test but commented it out when I realized it was simpler to just add a few lines into the existing test. v2 removes the commented-out one.
;;;","10/Nov/11 18:27;jbellis;+1;;;","16/Nov/11 13:39;slebresne;Forgot to close this one but it's been committed already.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LoadBroadcaster never removes endpoints,CASSANDRA-3475,12530865,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,09/Nov/11 11:57,16/Apr/19 09:32,14/Jul/23 05:52,10/Nov/11 00:52,1.0.3,,,,,,0,lhf,,,As the title says.,,codevally,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/11 11:59;brandon.williams;3475.txt;https://issues.apache.org/jira/secure/attachment/12503062/3475.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,216603,,,Thu May 16 23:30:27 UTC 2013,,,,,,,,,,"0|i0gkd3:",94729,,,,,Low,,,,,,,,,,,,,,,,,"09/Nov/11 12:00;brandon.williams;Straightforward patch.;;;","09/Nov/11 20:20;jbellis;+1;;;","10/Nov/11 00:51;brandon.williams;Committed;;;","16/May/13 23:30;codevally;Still this happen top me with 1.0.11 version. The JMX LoadMap shows already decommission nodes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing null check in CQL QueryProcessor,CASSANDRA-3473,12530737,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,08/Nov/11 17:12,16/Apr/19 09:32,14/Jul/23 05:52,08/Nov/11 18:01,1.0.3,,,,,,0,CQL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/11 17:13;slebresne;0001-Missing-null-check-in-QueryProcessor.patch;https://issues.apache.org/jira/secure/attachment/12502931/0001-Missing-null-check-in-QueryProcessor.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,216475,,,Tue Nov 08 18:01:38 UTC 2011,,,,,,,,,,"0|i0gkc7:",94725,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"08/Nov/11 17:13;slebresne;That seems to be a 1.0 only problem introduced by CASSANDRA-2734.;;;","08/Nov/11 17:41;jbellis;+1

(Actually introduced by CASSANDRA-3424, updating Affects accordingly.);;;","08/Nov/11 18:01;slebresne;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Actually uses efficient cross DC writes,CASSANDRA-3472,12530704,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,08/Nov/11 10:36,16/Apr/19 09:32,14/Jul/23 05:52,08/Nov/11 15:29,0.8.8,1.0.3,,,,,0,,,,"CASSANDRA-2138 introduced the following code:
{noformat}
if (dataCenter.equals(localDataCenter) || StorageService.instance.useEfficientCrossDCWrites())
{
    // direct writes to local DC or old Cassadra versions
    for (InetAddress destination : messages.getValue())
        MessagingService.instance().sendRR(message, destination, handler);
}
else
{
    // Non-local DC. First endpoint in list is the destination for this group
{noformat}
A 'not' is missing on that useEfficientCrossDCWrites call (which does return true for any version >= 0.7.1).

A simple fix would be to add the missing !, but as said a comment, all this code should have been removed in 0.8 since it was detecting nodes before 0.7.1, but direct upgrade from pre-0.7.1 to 0.8+ is not supported. So let's just completely remove that code now.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/11 10:37;slebresne;3472.patch;https://issues.apache.org/jira/secure/attachment/12502905/3472.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,216442,,,Tue Nov 08 18:09:07 UTC 2011,,,,,,,,,,"0|i0gkbr:",94723,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"08/Nov/11 10:37;slebresne;Attached patch is against 0.8;;;","08/Nov/11 15:10;jbellis;+1;;;","08/Nov/11 15:29;slebresne;Committed;;;","08/Nov/11 18:09;hudson;Integrated in Cassandra-0.8 #396 (See [https://builds.apache.org/job/Cassandra-0.8/396/])
    Fix bug preventing the use of efficient cross-DC writes
patch by slebresne; reviewed by jbellis for CASSANDRA-3472

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1199284
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"get_slice, super column family with UUIDType as column comparator",CASSANDRA-3467,12530627,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,rbranson,alesl,alesl,07/Nov/11 20:33,16/Apr/19 09:32,14/Jul/23 05:52,08/Nov/11 18:03,1.0.3,,,Legacy/CQL,,,0,,,,"get_slice with more than one column selected by predicate fails, when comparator is set to (Lexical|Time)UUIDType and more than one column is being selected.

{code}
// Sample data:
create column family A with column_type = Super and comparator = LexicalUUIDType and subcomparator = UTF8Type and default_validation_class = UTF8Type;
set A[ascii('key')][lexicaluuid('b139337e-fb6d-41e1-a868-1db7f2a52a42')]['a'] = 'A';
set A[ascii('key')][lexicaluuid('b139337e-fb6d-41e1-a868-1db7f2a52a42')]['b'] = 'B';
set A[ascii('key')][lexicaluuid('b139337e-fb6d-41e1-a868-1db7f2a52a42')]['c'] = 'C';
set A[ascii('key')][lexicaluuid('b139337e-fb6d-41e1-a868-1db7f2a52a42')]['d'] = 'D';
set A[ascii('key')][lexicaluuid('b139337e-fb6d-41e1-a868-1db7f2a52a42')]['e'] = 'E';

// Failed call
$client->get_slice(
    'key', 
    new ColumnParent(array(
        'column_family'=>'A', 
        'super_column'=>base64_decode('sTkzfvttQeGoaB238qUqQg==')
    )), 
    new SlicePredicate(array(
        'column_names'=>array('a', 'b')
    )), 
    1
);

// Exception thrown
ERROR [ReadStage:302] 2011-11-07 21:29:30,339 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadStage:302,5,main]
java.lang.RuntimeException: java.lang.IndexOutOfBoundsException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1269)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IndexOutOfBoundsException
	at java.nio.Buffer.checkIndex(Buffer.java:520)
	at java.nio.HeapByteBuffer.getLong(HeapByteBuffer.java:391)
	at org.apache.cassandra.utils.UUIDGen.getUUID(UUIDGen.java:67)
	at org.apache.cassandra.db.marshal.LexicalUUIDType.compare(LexicalUUIDType.java:58)
	at org.apache.cassandra.db.marshal.LexicalUUIDType.compare(LexicalUUIDType.java:31)
	at java.util.TreeMap.put(TreeMap.java:530)
	at java.util.TreeSet.add(TreeSet.java:238)
	at java.util.AbstractCollection.addAll(AbstractCollection.java:305)
	at java.util.TreeSet.addAll(TreeSet.java:295)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:98)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:61)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1278)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1164)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1131)
	at org.apache.cassandra.db.Table.getRow(Table.java:378)
	at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:58)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:797)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1265)
	... 3 more

// This works though
$client->get_slice(
    'key', 
    new ColumnParent(array(
        'column_family'=>'A', 
        'super_column'=>base64_decode('sTkzfvttQeGoaB238qUqQg==')
    )), 
    new SlicePredicate(array(
        'column_names'=>array('a')
    )), 
    1
);

// This works too
$client->get_slice('key', 
	new ColumnParent(array(
		'column_family'=>'A', 
		'super_column'=>base64_decode('sTkzfvttQeGoaB238qUqQg==')
	)), 
	new SlicePredicate(array(
		'slice_range'=>new SliceRange(array(
			'start'=>'', 
			'finish'=>'',
			'reversed'=>false,
			'count'=>100
		))
	)), 1);
{code}

Regards
ales","ubuntu 11.04
java version ""1.6.0_26""
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)

php interface build with thrift 0.7.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/11 17:44;rbranson;3467-get-slice-on-scf-with-uuidtype-comparator.patch;https://issues.apache.org/jira/secure/attachment/12502938/3467-get-slice-on-scf-with-uuidtype-comparator.patch",,,,,,,,,,,,,,1.0,rbranson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,216365,,,Tue Nov 08 18:03:27 UTC 2011,,,,,,,,,,"0|i0gk9j:",94713,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"08/Nov/11 17:44;rbranson;Attached is a test case and a patch for the issue.;;;","08/Nov/11 18:03;jbellis;CASSANDRA-3446 fixed this in passing, but I committed the test to prevent future regressions.  Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted handoff not working after rolling upgrade from 0.8.7 to 1.0.2,CASSANDRA-3466,12530617,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jborgstrom,jborgstrom,07/Nov/11 19:16,16/Apr/19 09:32,14/Jul/23 05:52,11/Nov/11 19:54,1.0.3,,,,,,0,hintedhandoff,,,"While testing rolling upgrades from 0.8.7 to 1.0.2 on a test cluster I've noticed that hinted hand-off didn't always work properly. Hints generated on an upgraded node does not seem to be delivered to other newly upgraded nodes once they rejoin the ring. They only way I've found to get a node to deliver its hints is to restart it.

Here's some steps to reproduce this issue:

1. Install cassandra 0.8.7 on node1 and node2 using default settings.
2. Create keyspace foo with {replication_factor: 2}. Create column family bar
3. Shutdown node2 
4. Insert data into bar and verify that HintsColumnFamily on node2 contains hints
5. Start node2 and verify that hinted handoff is performed and HintsColumnFamily becomes empty again.

6. Upgrade and restart node1
7. Shutdown node2 
8. Insert data into bar and verify that HintsColumnFamily on node2 contains hints
9. Upgrade and start node2
10. Notice that hinted handoff is *not* performed when ""node2"" comes back. (Only if node1 is restarted)
",,hibou,hsn,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/11 19:02;jbellis;3466.txt;https://issues.apache.org/jira/secure/attachment/12503277/3466.txt","10/Nov/11 12:55;jborgstrom;CASSANDRA-3466-2.tar.gz;https://issues.apache.org/jira/secure/attachment/12503200/CASSANDRA-3466-2.tar.gz","09/Nov/11 14:49;jborgstrom;CASSANDRA-3466.tar.gz;https://issues.apache.org/jira/secure/attachment/12503083/CASSANDRA-3466.tar.gz",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,216355,,,Fri Nov 11 19:54:48 UTC 2011,,,,,,,,,,"0|i0gk93:",94711,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"07/Nov/11 22:57;jbellis;Do you observe the same behavior if both nodes start life as 1.x?

Is node2 down long enough for node1 failure detector to notice?  (i.e. shows as DOWN in nodetool ring);;;","08/Nov/11 07:40;jborgstrom;> Do you observe the same behavior if both nodes start life as 1.x?

No. Hinted handoff seems to work well if I either start with 1.0.2 nodes or restart all nodes once all nodes have been upgraded to 1.0.2. Probably something in the gossip state confusing hinted hand-offs.

> Is node2 down long enough for node1 failure detector to notice? (i.e. shows as DOWN in nodetool ring)

Yes. This from the node1 log:
{code}
Node2 is shut down
  INFO 19:39:08,331 InetAddress /127.0.0.2 is now dead.
Hint is triggered using set bar[x][x]='x'
  INFO 19:40:52,126 Node /127.0.0.2 has restarted, now UP
  INFO 19:40:52,127 InetAddress /127.0.0.2 is now UP
  INFO 19:40:52,127 Node /127.0.0.2 state jump to normal
Nothing happens, hint stays in HintsColumnFamily forever
{code}

Calling deliverHints() using JMX also does not seem to work. Restarting node1 will though...
;;;","08/Nov/11 08:41;jborgstrom;> Do you observe the same behavior if both nodes start life as 1.x?

I spoke too soon. I did some more testing with fresh 1.0.2 installs (and empty data directories).
{code}
1. Start node1 and node2 and create keyspace and cf.
2. Stop node2, wait until ""is now dead"".
3. Trigger hint using ""set bar[x][x]='x';""
4. Start node2, wait for hint delivery... Nothing happens
5. Restart node1, hints are finally delivered.
{code}
Now it gets really weird...
{code}
2. Stop node2, wait until ""is now dead"".
3. Trigger hint using ""set bar[x][x]='x';""
4. Start node2. Node1 attempts to deliver hints but fails with:
 INFO 09:29:35,153 Started hinted handoff for token: 23495828435496583962471242736585511198 with IP: /127.0.0.2
ERROR 09:29:35,212 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.AssertionError
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:301)
	at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
	at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:353)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)

On node1:
[default@system] list HintsColumnFamily;
Using default limit of 100
-------------------
RowKey: 11ad21c7d112e903d71645d1d951911e
=> (super_column=b2cd983009e311e10000fe8ebeead9cf,
     (column=6b6579, value=78, timestamp=1320740935987, ttl=864000)
     (column=6d75746174696f6e, value=0003666f6f00017800000001000003e801000003e880000000800000000000000000000001000178000004b134f65401d80000000139, timestamp=1320740935987, ttl=864000)
     (column=7461626c65, value=666f6f, timestamp=1320740935987, ttl=864000)
     (column=76657273696f6e, value=00000003, timestamp=1320740935987, ttl=864000))
=> (super_column=f929a36009e211e10000fe8ebeead9df,
     (column=6b6579, value=78, timestamp=1320740624538, ttl=864000)
     (column=7461626c65, value=666f6f, timestamp=1320740624538, ttl=864000))
1 Row Returned.
{code}

Is that a 0.8 hint showing up out of the blue on a pure 1.0.2 cluster? Or did I mess up my test somehow?
;;;","09/Nov/11 12:50;brandon.williams;I haven't been able to reproduce the assertion errors, but I did find what is preventing hint delivery in some cases:

{noformat}
        if (hintStore.getSSTables().isEmpty())
            return; // nothing to do, don't confuse users by logging a no-op handoff
{noformat}

If you're testing with a small enough amount of hints that the hints table never flushes, we never deliver the hints in the current memtable.;;;","09/Nov/11 13:00;hsn;Thats true. i did more testing in this area today. Deleted hints sstables from disk to be sure that no 0.8.x possible hints are there - but they should not be there anyway because i had snapshot from CF hints truncate around. And i could not get any hints delivered to second node (no exceptions today).

Thats probably deserves to roll new cassandra version soon, because it is very important to get hints properly delivered as soon as possible.;;;","09/Nov/11 13:19;brandon.williams;{noformat}
=> (super_column=f929a36009e211e10000fe8ebeead9df,
     (column=6b6579, value=78, timestamp=1320740624538, ttl=864000)
     (column=7461626c65, value=666f6f, timestamp=1320740624538, ttl=864000))
1 Row Returned.
{noformat}

The odd thing here is that you have the 'table' and 'key' subcolumns, but not the 'mutation' or 'version' subcolumns.  Since these are all applied at once I'm not sure how they could be missing.;;;","09/Nov/11 13:23;hsn;application is doing row delete and then row insert;;;","09/Nov/11 13:56;jborgstrom;> I haven't been able to reproduce the assertion errors, but I did find what is preventing hint delivery in some cases

Brandon, Did you verify that removing those lines of code actually fixes hint delivery? 

Instead of changing the code I just did a quick experiment with ""nodetool flush"" on the node holding the hints and then restarting the other node but that was not enough to trigger hints delivery:

{code}
Node1 notices that node2 is backup up
  INFO 14:41:50,752 Node /127.0.0.2 has restarted, now UP
  INFO 14:41:50,752 InetAddress /127.0.0.2 is now UP
  INFO 14:41:50,753 Node /127.0.0.2 state jump to normal
But no hints are delivered...

nodetool flush is used to make sure hints hit the disk on node1:

  INFO 14:42:32,675 Enqueuing flush of Memtable-Versions@1503666327(83/103 serialized/live bytes, 3 ops)
  INFO 14:42:32,675 Writing Memtable-Versions@1503666327(83/103 serialized/live bytes, 3 ops)
  INFO 14:42:32,681 Completed flushing /tmp/node1/data/data/system/Versions-h-1-Data.db (247 bytes)
  INFO 14:42:32,682 Enqueuing flush of Memtable-HintsColumnFamily@737188401(177/221 serialized/live bytes, 1 ops)
  INFO 14:42:32,682 Writing Memtable-HintsColumnFamily@737188401(177/221 serialized/live bytes, 1 ops)
  INFO 14:42:32,688 Completed flushing /tmp/node1/data/data/system/HintsColumnFamily-h-1-Data.db (277 bytes)
  INFO 14:42:32,691 Enqueuing flush of Memtable-bar@1831941861(17/21 serialized/live bytes, 1 ops)
  INFO 14:42:32,691 Writing Memtable-bar@1831941861(17/21 serialized/live bytes, 1 ops)
  INFO 14:42:32,694 Completed flushing /tmp/node1/data/data/foo/bar-h-1-Data.db (68 bytes)

Node2 is restarted once more to check if this will trigger hints delivery:
  INFO 14:42:54,650 InetAddress /127.0.0.2 is now dead.
  INFO 14:43:02,628 Node /127.0.0.2 has restarted, now UP
  INFO 14:43:02,629 InetAddress /127.0.0.2 is now UP
  INFO 14:43:02,629 Node /127.0.0.2 state jump to normal

Still nothing...  Restarting node 1 will deliver the hints within a few seconds though...
{code}

Regarding reproducing the assertion error it's a bit tricky. But after letting my two node test cluster performing hints delivery for each other a few times I was able to reproduce it once more. Is there anything special you would like me to test?


;;;","09/Nov/11 14:04;brandon.williams;bq. Brandon, Did you verify that removing those lines of code actually fixes hint delivery?

Yes.

bq. Instead of changing the code I just did a quick experiment with ""nodetool flush"" on the node holding the hints and then restarting the other node but that was not enough to trigger hints delivery

Hints aren't delivered immediately, there's up to a 60s random delay to stagger the replay.  Did you wait long enough to be sure it passed?;;;","09/Nov/11 14:16;jborgstrom;bq. Hints aren't delivered immediately, there's up to a 60s random delay to stagger the replay. Did you wait long enough to be sure it passed?

Yes, I've waited a couple of minutes now and still nothing. So I guess issuing ""nodetool flush"" isn't enough. I'll leave it running while I recompile with your changes and re-test.
;;;","09/Nov/11 14:47;jborgstrom;Brandon, you're right. After commenting out those two lines I can no longer reproduce the hint delivery problem.

The AssertionError is still there though and I managed to reproduce it with my two node setup.
I'll attach a tarball with my config, DEBUG-level log files and data directories shortly.
What I did was basically:

# Start a new cluster with a patched 1.0.2 and create keyspace and column family.
# Bring down node2, trigger hints on node1
# Bring up node2, notice that hints are delivered, YAY!
# Bring down node1, trigger hints on node2
# Bring up node1, noticed AssertionError in log file for node1   (node1/system.log:2036)

To me it looks like the assertionError on node1 is triggered when node2 connects to deliver its hints.;;;","09/Nov/11 14:49;jborgstrom;Logs, config files and data files from a setup that triggered AssertionError in HintedHandoffManager;;;","09/Nov/11 17:56;brandon.williams;Jonas,

Thanks for the data files.  Looking at them with sstable2json I see the all the subcolumns are actually there, so I think what you may be seeing is CASSANDRA-3446, could you try to reproduce with that patch applied?;;;","09/Nov/11 20:10;jborgstrom;bq. Jonas,

bq. Thanks for the data files. Looking at them with sstable2json I see the all the subcolumns are actually there, so I think what you may be seeing is CASSANDRA-3446, could you try to reproduce with that patch applied?

Jikes, that one looks pretty bad.

Anyway, with the patches in CASSANDRA-3446 in combination with the modifications of HintedHandOffManager.py mentioned earlier I've so far not been able to reproduce any of the issues I've observed earlier.

Thanks for your help!
;;;","10/Nov/11 12:55;jborgstrom;I've unfortunately managed to reproduce this AssertionError even with the CASSANDRA-3446 patches and the HintedHandOffManager.java modification.

I used the same two node setup as before initialized with:

{code}
create keyspace foo with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = {replication_factor:2};
use foo;
create column family bar with comparator=UTF8Type and key_validation_class=UTF8Type;
{code}

# Shutdown node2 and wait until it's detected as down
trigger hints on node1 (using random values):
{code}
use foo;
set bar[0][0]='1';
set bar[2][3]='1';
set bar[3][3]='1';
{code}
Verify that `list HintsColumnFamily` looks OK
# Start node2 and wait for hints delivery
Verify that `list HintsColumnFamily` is empty after hints delivery

After repeating step 1-2 a few times (sometimes once is enough) ""HintsColumnFamily"" will become ""corrupt"" after hints delivery instead of empty. And if node1 is restarted after that it will trigger the AssertionError.

The attached file contains DEBUG-log files, data directories and conf-directories from one test where HintsColumnFamily ended up looking like this:
{code}
[default@system] list HintsColumnFamily;
Using default limit of 100
-------------------
RowKey: 1b226da5af66854850abdcc6ab4ce9c6
=> (super_column=1f2860e00b9811e10000fe8ebeead9ff,
     (column=6b6579, value=33, timestamp=1320928378352, ttl=864000))

1 Row Returned.
Elapsed time: 6 msec(s).
{code}
;;;","10/Nov/11 15:12;slebresne;The system log indicates an assertion error on HintedHandoffManager line 298. But it's not an assertion on this line in 1.0.2. Would you mind trying to see if you can reproduce on the current 1.0 branch (or 1.0.2 patched with CASSANDRA-3446)? Just want to make sure you're not hitting something fixed already.;;;","10/Nov/11 15:26;jborgstrom;I'm running 1.0.2 with the 3446 patches *and* the following three lines removed from HintedHandOffManager.java (As discussed earlier in this ticket)
{code}
        if (hintStore.getSSTables().isEmpty())
            return; // nothing to do, don't confuse users by logging a no-op handoff
// An empty line
{code}
That should explain the line number mismatch (-3).
Without this change hinted handoff wouldn't work at all for me unless HintsColumnFamily is large enough to flow to disk before the other node gets back up.;;;","10/Nov/11 18:39;jbellis;So I should be looking at node1 HintsColumnFamily for the corruption, right?;;;","10/Nov/11 19:02;jbellis;Fix attached.  The problem is we were deleting the hint with the timestamp from the {{version}} subcolumn, but the subcolumn timestamps were generated independently so it's possible for there to be ""orphaned"" subcolumns left over.

Patch (1) uses same ts for all hint subcolumns and (2) creates the tombstone w/ the max ts from the subcolumns.

Also fixes the isEmpty delivery bug by checking memtable contents as well.;;;","11/Nov/11 19:47;brandon.williams;+1;;;","11/Nov/11 19:54;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make cqlsh look for a suitable python version,CASSANDRA-3457,12530367,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,04/Nov/11 19:39,16/Apr/19 09:32,14/Jul/23 05:52,02/Dec/11 03:10,1.0.6,,,Legacy/Tools,,,0,cqlsh,,,"On RHEL 5, which I guess we still want to support, the default ""python"" in the path is still 2.4. cqlsh does use a fair number of python features introduced in 2.5, like collections.defaultdict, functools.partial, generators. We can require RHEL 5 users to install a later python from EPEL, but we'd have to call it as 'python2.5', or 'python2.6', etc.

So rather than take the time to vet everything against python2.4, we may want to make a wrapper script for cqlsh that checks for the existence of python2.7, 2.6, and 2.5, and calls the appropriate one to run the real cqlsh.",,cburroughs,jplock,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/11 18:37;thepaul;3457.patch.txt;https://issues.apache.org/jira/secure/attachment/12505366/3457.patch.txt",,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,216105,,,Fri Dec 02 03:10:31 UTC 2011,,,,,,,,,,"0|i0gk53:",94693,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"17/Nov/11 17:35;thepaul;Just a note for future reference- the opinion has been expressed that requiring EPEL for cassandra rpms may not be an option. Might need to remove the 2.5 features from cqlsh and test it along with the python driver on 2.4.;;;","28/Nov/11 18:46;jbellis;{noformat}
for pyver in 2.6 2.7 2.5
{noformat}

Why that particular ordering?;;;","28/Nov/11 19:03;thepaul;bq. Why that particular ordering?

No strong reason. 2.6 is just a little more likely to be found than 2.7, over our expected set of install targets, and both 2.6 and 2.7 are little bits faster than 2.5.

Fine with changing to something else.;;;","28/Nov/11 19:11;tjake;tested on centos 6 which used 2.4 by default, with this patch it finds the right version so +1;;;","28/Nov/11 19:20;thepaul;One minor edit: if we change the shebang from {{\#\!/bin/bash}} to {{\#\!/bin/sh}}, it will work for Solaris too. none of the code actually needs bash.;;;","02/Dec/11 03:10;jbellis;committed, with bin/sh;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
estimated row sizes regression,CASSANDRA-3451,12530100,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,03/Nov/11 19:15,16/Apr/19 09:32,14/Jul/23 05:52,04/Nov/11 08:17,1.0.2,,,,,,0,,,,"CASSANDRA-2753 broke the histogram collection; it got the histogram for column count (which can go up to 2B) switched with the one for row size in bytes (which goes up to ~1.5PB).  So any row over 2GB, will break things.",,jborgstrom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/11 00:12;jbellis;3451.txt;https://issues.apache.org/jira/secure/attachment/12502242/3451.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215959,,,Fri Nov 04 08:17:46 UTC 2011,,,,,,,,,,"0|i0gk2f:",94681,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"04/Nov/11 00:04;jbellis;patch fixes the regression and also does a minor rename for clarity on the methods involved.;;;","04/Nov/11 07:01;springrider;""which can go up to 2B"" should be 2GB?;;;","04/Nov/11 08:17;slebresne;+1
Committed, but I've renamed defaultColumnSizeHistogram to defaultColumnCountHistogram for coherence with the rest of the code.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
maybeInit in ColumnFamilyRecordReader can cause rows to be empty but not null,CASSANDRA-3450,12530083,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,lannyripple,lannyripple,lannyripple,03/Nov/11 17:20,16/Apr/19 09:32,14/Jul/23 05:52,04/Nov/11 21:29,0.8.8,1.0.3,,,,,0,,,,"1) In {{ColumnFamilyRecordReader}} {{isPredicateEmpty}} needs bracing to correctly place the {{else if}} to the properly controlling {{if}}.

1a) {{isPredicateEmpty}} should use an || in the getSlice_range predicate rather than &&.

2) In {{ColumnFamilyRecordReader}} {{computeNext()}} calls {{maybeInit()}} and then if {{ros}} is not null it is indexed into.  {{maybeInit()}} could fetch new data, determine the associated slice predicate is empty, and end up removing all the rows if all columns turned out to be empty.  There is no check for {{rows.isEmpty()}} after the possible removal of all rows.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/11 16:23;lannyripple;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3450.txt;https://issues.apache.org/jira/secure/attachment/12502477/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3450.txt",,,,,,,,,,,,,,1.0,lannyripple,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215942,,,Tue Nov 08 16:03:09 UTC 2011,,,,,,,,,,"0|i0gk1z:",94679,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"04/Nov/11 21:29;brandon.williams;Committed, thanks!;;;","05/Nov/11 00:05;hudson;Integrated in Cassandra-0.8 #393 (See [https://builds.apache.org/job/Cassandra-0.8/393/])
    Fix empty row filtering and check if there are no rows returned.
Patch by Lanny Ripple, reviewed by brandonwilliams for CASSANDRA-3450

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1197786
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
;;;","08/Nov/11 14:00;tjake;Reverted this change due to a bug related to single node test failures.  Going to attempt a fresh fix at this and CASSANDRA-2855  ;;;","08/Nov/11 16:03;hudson;Integrated in Cassandra-0.8 #395 (See [https://builds.apache.org/job/Cassandra-0.8/395/])
    Revert CASSANDRA-3450

jake : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1199242
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing null check in digest retry part of read path,CASSANDRA-3449,12529992,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,03/Nov/11 03:10,16/Apr/19 09:32,14/Jul/23 05:52,04/Nov/11 08:45,1.0.2,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/11 03:13;jbellis;3449.txt;https://issues.apache.org/jira/secure/attachment/12502092/3449.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215851,,,Fri Nov 04 08:45:57 UTC 2011,,,,,,,,,,"0|i0gk1j:",94677,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"03/Nov/11 16:37;slebresne;I think it is possible for the row to be null but still being in a short read situation. Like if you resolve 2 nodes whose responses look like:
{noformat}
  node1:  1  del  3
  node2: del  2  del
{noformat}
and you've asked for two columns. The merging results in a null row (assuming all del have priority), but it's possible that node1 had a lot of perfectly good value after that 3, so we should retry.

I believe the right fix could be to just have a {{if (row != null)}} before the rows.add(row) ?;;;","04/Nov/11 08:45;slebresne;Ok, I went ahead and while committing CASSANDRA-3303 I added a null check just before the rows.add(row) line. My rational for ninja-shoving it was that this can't introduce a bug. Worst case scenario, we shouldn't do the check for short read code when the row is null in the first place and my added null check is useless. But as said above I believe we do need to do the check for short read in this case, so I'm closing this. But feel free to reopen if you spot a mistake in my reasoning.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem SliceByNamesReadCommand on super column family after flush operation,CASSANDRA-3446,12529885,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,rheostat,rheostat,02/Nov/11 16:15,16/Apr/19 09:32,14/Jul/23 05:52,08/Nov/11 17:55,1.0.3,,,,,,0,supercolumns,,,"I'm having a problem with doing a multiget_slice on a super column family
after its first flush. Updates to the column values work properly, but
trying to retrieve the updated values using a multiget_slice operation fail
to get the updated values. Instead they return the values from before the
flush. The problem is not apparent with standard column families.

I've seen this problem in Cassandra v1.0.0 and v1.0.1. The problem
is not present in Cassandra v0.7.6.

Steps to reproduce:

   1. Create one or more super column entries
   2. Verify the sub column values can be updated and that you can retrieve
   the new values
   3. Use nodetool to flush the column family or restart cassandra
   4. Update the sub column values
   5. Verify they have been updated using cassandra-cli
   6. Verify you *DO NOT* get the updated values when doing a
   multiget_slice; instead you get the old values from before the flush

You can get the most recent value by doing a flush followed by a major
compaction. However, future updates are not retrieved properly either.

With debug turned on, it looks like the multiget_slice query uses the
following command/consistency level:
SliceByNamesReadCommand(table='test_cassandra', key=666f6f,
columnParent='QueryPath(columnFamilyName='test', superColumnName='null',
columnName='null')', columns=[foo,])/QUORUM.

Cassandra-cli uses the following command/consistency level for a get_slice:
SliceFromReadCommand(table='test_cassandra', key='666f6f',
column_parent='QueryPath(columnFamilyName='test', superColumnName='null',
columnName='null')', start='', finish='', reversed=false,
count=1000000)/QUORUM

Notice the test program gets 'bar2' for the column values and cassandra-cli
gets 'bar3' for the column values:

tcpdump from test program using hector-core:1.0-1

16:46:07.424562 IP iam.47158 > iam.9160: Flags [P.], seq 55:138, ack 30,
win 257, options [nop,nop,TS val 27474096 ecr 27474095], length 83
E....#@.@.PK.........6#.....].8......{.....
..8...8.........multiget_slice................foo..........test................foo.........
16:46:07.424575 IP iam.9160 > iam.47158: Flags [.], ack 138, win 256,
options [nop,nop,TS val 27474096 ecr 27474096], length 0
E..4..@.@.<.........#..6].8..........(.....
..8...8.
16:46:07.428771 IP iam.9160 > iam.47158: Flags [P.], seq 30:173, ack 138,
win 256, options [nop,nop,TS val 27474097 ecr 27474096], length 143
@.@.<&........#..6].8................
............foo...............foo...............foo1.......bar2
........6h........foo2.......bar2
........I.....


tcpdump of cassandra-cli:

16:30:55.945123 IP iam.47134 > iam.9160: Flags [P.], seq 370:479, ack 5310,
win 387, options [nop,nop,TS val 27246226 ecr 27241207], length 109
E.....@.@.9q..........#..n.X\
.............
................get_range_slices..............test.........................................................d.........
16:30:55.945152 IP iam.9160 > iam.47134: Flags [.], ack 479, win 256,
options [nop,nop,TS val 27246226 ecr 27246226], length 0
E..4..@.@."".........#...\
...n.......(.....
........
16:30:55.949245 IP iam.9160 > iam.47134: Flags [P.], seq 5310:5461, ack
479, win 256, options [nop,nop,TS val 27246227 ecr 27246226], length 151
E.....@.@.""V........#...\
...n.............
....................get_range_slices...................foo..................foo...............foo1.......bar3
........&.........foo2.......bar3
........: .....","Linux iam 3.0.0-12-generic #20-Ubuntu SMP Fri Oct 7 14:56:25 UTC 2011 x86_64 x86_64 x86_64 GNU/Linux
java version ""1.6.0_23""
OpenJDK Runtime Environment (IcedTea6 1.11pre) (6b23~pre10-0ubuntu5)
OpenJDK 64-Bit Server VM (build 20.0-b11, mixed mode)
hector-core; 1.0-1
",jborgstrom,norru,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/11 21:39;jbellis;0001-fix-addColumn-and-collation-supercolumn-bugs.patch;https://issues.apache.org/jira/secure/attachment/12502817/0001-fix-addColumn-and-collation-supercolumn-bugs.patch","07/Nov/11 21:39;jbellis;0002-r-m-minTimestamp-method.patch;https://issues.apache.org/jira/secure/attachment/12502818/0002-r-m-minTimestamp-method.patch","04/Nov/11 19:19;rbranson;3446-test.patch;https://issues.apache.org/jira/secure/attachment/12502506/3446-test.patch",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215744,,,Thu Nov 10 14:28:49 UTC 2011,,,,,,,,,,"0|i0gk07:",94671,,slebresne,,slebresne,Critical,,,,,,,,,,,,,,,,,"02/Nov/11 18:40;jbellis;Can you post your columnfamily create statement?;;;","02/Nov/11 19:59;rheostat;create keyspace test_cassandra;
use test_cassandra;
create column family test1 with column_type = 'Super' and comparator =
'AsciiType';
create column family test2 with column_type = 'Standard' and comparator =
'AsciiType';
;;;","03/Nov/11 04:42;jbellis;Is this just a single-node cluster?;;;","03/Nov/11 04:49;rheostat;Yes, single node cluster.


On Wed, Nov 2, 2011 at 9:43 PM, Jonathan Ellis (Commented) (JIRA) <

;;;","04/Nov/11 19:19;rbranson;Adds a unit test to ColumnFamilyStoreTest to reproduce the issue.;;;","07/Nov/11 18:37;norru;I had a similar problem and spent a fair amount of time tracking it down. It appears related to a problem in collating data from Memtables and SStables, but only when the query involves SuperColumns

I may have found a fix. It did solve the problem for me but I haven't tested extensively for regressions or concurrency issues.

{code}
package org.apache.cassandra.db;

public class TreeMapBackedSortedColumns extends TreeMap<ByteBuffer, IColumn> implements ISortedColumns



    /*
     * If we find an old column that has the same name
     * the ask it to resolve itself else add the new column
    */
    public void addColumn(IColumn column, Allocator allocator)
    {
        ByteBuffer name = column.name();
        IColumn oldColumn = put(name, column);
        if (oldColumn != null)
        {
            if (oldColumn instanceof SuperColumn)
            {
                assert column instanceof SuperColumn;
                ((SuperColumn) oldColumn).putColumn((SuperColumn)column, allocator);
                // we need to restore the old value here or things won't work! --norru@scee.net
                put(name, oldColumn); // <--- here it is
            }
            else
            {
                // calculate reconciled col from old (existing) col and new col
                IColumn reconciledColumn = column.reconcile(oldColumn, allocator);
                put(name, reconciledColumn);
            }
        }
    }
{code}

Let me know if you need a proper patch. It's an one liner so it might be easier for you to add the line yourself.

Also let me know if you need more details. Cheers!;;;","07/Nov/11 18:57;jbellis;There's at least two problems here.  One is the one Nicola describes.  The other is that the name-based path in CollationController stops as soon as it finds one subcolumn in a given supercolumn.

Working on a patch for both.;;;","07/Nov/11 21:38;jbellis;There's a third bug involved, that can hide the second: SuperColumn.minTimestamp is calculated incorrectly.

Patch 01 adds tests for the two ""main"" bugs and fixes them.  Patch 02 removes SC.minTimestamp since part of the 01 fix is recognizing that we shouldn't short-circuit a SuperColumn read, since we don't know how many potential subcolumns there are without an exhaustive search.;;;","07/Nov/11 22:02;rheostat;I applied the patches to 1.0.2 and all looks good. Thanks a lot for the quick turn around and keep up the good work!!;;;","08/Nov/11 09:27;slebresne;+1, good catches.;;;","08/Nov/11 17:55;jbellis;committed;;;","10/Nov/11 14:28;jborgstrom;I'm not sure it's related at all but CASSANDRA-3466 is about the HintsColumnFamily mysteriously losing or gaining some subcolumns after a hints delivery and corresponding flush.

First assumption was that this was related to this ticket. But after some further testing with a patched 1.0.2 I was still able to reproduce the AssertionError.
So either these two tickets are not related at all or the proposed patches does not fix all cases.

If anyone's interested CASSANDRA-3466 contains an attachment with DEBUG-logs and data files from when I reproduced the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"recognize that ""SELECT first ... *"" isn't really ""SELECT *""",CASSANDRA-3445,12529869,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,02/Nov/11 14:48,16/Apr/19 09:32,14/Jul/23 05:52,03/Nov/11 19:00,1.1.0,,,Legacy/CQL,,,0,cql,,,"QueryProcessor includes the row key in ""first *"" because it mistakenly thinks the full row is being requested.  ""first *"" should really be treated like a slice.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/11 03:21;jbellis;3445-skeleton.txt;https://issues.apache.org/jira/secure/attachment/12502094/3445-skeleton.txt","03/Nov/11 11:14;xedin;CASSANDRA-3445.patch;https://issues.apache.org/jira/secure/attachment/12502128/CASSANDRA-3445.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215728,,,Thu Nov 03 19:19:00 UTC 2011,,,,,,,,,,"0|i0gjzr:",94669,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"03/Nov/11 03:21;jbellis;Would like to take the approach outlined here but there isn't actually a way to tell if the FIRST count was specified in the statement or not w/ the current antlr.  Re-assigning to Pavel for antlr help.;;;","03/Nov/11 11:14;xedin;Used Jonathan's skeleton with one modification - SelectStatement.isFullWildcard () was changed to use !hasFirstSet because I think that intention here is not to include the key when ""FIRST"" keyword is set.;;;","03/Nov/11 17:17;jbellis;+1;;;","03/Nov/11 19:00;xedin;Committed.;;;","03/Nov/11 19:19;hudson;Integrated in Cassandra #1185 (See [https://builds.apache.org/job/Cassandra/1185/])
    recognize that ""SELECT first ... *"" isn't really ""SELECT *""
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3445

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1197270
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/cql/Cql.g
* /cassandra/trunk/src/java/org/apache/cassandra/cql/QueryProcessor.java
* /cassandra/trunk/src/java/org/apache/cassandra/cql/SelectExpression.java
* /cassandra/trunk/src/java/org/apache/cassandra/cql/SelectStatement.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PerRowSecondaryIndexes skip the first column on update,CASSANDRA-3441,12529800,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tjake,tjake,tjake,02/Nov/11 03:03,16/Apr/19 09:32,14/Jul/23 05:52,02/Nov/11 13:57,1.0.2,,,,,,0,,,,PerRowSecondaryIndexes skip the initial field on apply,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/11 03:04;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3441-fix-lost-first-column-bug.txt;https://issues.apache.org/jira/secure/attachment/12501902/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3441-fix-lost-first-column-bug.txt",,,,,,,,,,,,,,1.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215659,,,Wed Nov 02 13:57:22 UTC 2011,,,,,,,,,,"0|i0gjxz:",94661,,jasonrutherglen,,jasonrutherglen,Normal,,,,,,,,,,,,,,,,,"02/Nov/11 03:08;jasonrutherglen;Looks good +1;;;","02/Nov/11 13:57;tjake;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
local writes timing out cause attempt to hint to self,CASSANDRA-3440,12529794,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,02/Nov/11 02:11,16/Apr/19 09:32,14/Jul/23 05:52,24/Nov/11 20:06,1.0.4,,,,,,0,hintedhandoff,,,"As reported by Ramash Natarajan on the mailing list:

{noformat}
We have a 8 node cassandra cluster running 1.0.1. After running a load
test for a day we are seeing this exception in system.log file.

ERROR [EXPIRING-MAP-TIMER-1] 2011-11-01 13:20:45,350
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[EXPIRING-MAP-TIMER-1,5,main]
java.lang.AssertionError: /10.19.102.12
       at org.apache.cassandra.service.StorageProxy.scheduleLocalHint(StorageProxy.java:339)
       at org.apache.cassandra.net.MessagingService.scheduleMutationHint(MessagingService.java:201)
       at org.apache.cassandra.net.MessagingService.access$500(MessagingService.java:64)
       at org.apache.cassandra.net.MessagingService$2.apply(MessagingService.java:175)
       at org.apache.cassandra.net.MessagingService$2.apply(MessagingService.java:152)
       at org.apache.cassandra.utils.ExpiringMap$1.run(ExpiringMap.java:89)
       at java.util.TimerThread.mainLoop(Timer.java:512)
       at java.util.TimerThread.run(Timer.java:462)
{noformat}",,hibou,jborgstrom,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/11 05:49;jbellis;3440-v2.txt;https://issues.apache.org/jira/secure/attachment/12504988/3440-v2.txt","05/Nov/11 05:17;jbellis;3440.txt;https://issues.apache.org/jira/secure/attachment/12502578/3440.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215653,,,Thu Nov 24 20:06:51 UTC 2011,,,,,,,,,,"0|i0gjxj:",94659,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"05/Nov/11 05:17;jbellis;patch to (1) allow retrying a write-to-self that timed out and (2) improve defense against cascading failure when nodes are overwhelmed but not dead.;;;","08/Nov/11 10:12;slebresne;I'm obviously missing something but I don't find how that assertion could be triggered in the first place.
More precisely, I don't see that a node can hint itself, since a callback in MessagingService is only put through sendRR which isn't called for local writes (unless OPTIMIZE_LOCAL_REQUESTS is false, which it shouldn't).;;;","24/Nov/11 04:43;jbellis;The assertion can be triggered by a read-repair mutation timing out.  Read-repair mutations (from RowRepairResolver.scheduleRepairs) are sent over MessagingService.;;;","24/Nov/11 05:49;jbellis;v2 retains the assertion, and moves read repair updates back to the READ_REPAIR message type, where they won't be hinted.  v2 retains the code to make HH more robust against causing coordinator OOMs.;;;","24/Nov/11 09:48;slebresne;Nit: it doesn't seem we use the row mutations saved in hintsInProgess, so maybe we could use a simple AtomicInteger, rather than a full concurrent map.

But otherwise patch looks good, +1.;;;","24/Nov/11 20:06;jbellis;Updated to use this instead and committed:

{code}
.   private static final Map<InetAddress, AtomicInteger> hintsInProgress = new MapMaker().concurrencyLevel(1).makeComputingMap(new Function<InetAddress, AtomicInteger>()
    {
        public AtomicInteger apply(InetAddress inetAddress)
        {
            return new AtomicInteger(0);
        }
    });
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader fails,CASSANDRA-3438,12529656,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,jeremypinkham,jeremypinkham,01/Nov/11 13:35,16/Apr/19 09:32,14/Jul/23 05:52,04/Nov/11 07:53,1.0.2,,,,,,0,,,,"Ticket at the request of driftx in IRC.  

I've attached the files I'm attempting to load (this is fabricated test data for 100 keys).  I generated this using SSTableSimpleUnsortedWriter. I have four dedicated nodes (mine, not ec2 or other cloud host) that I'm using, we'll call them A,B,C, and D.  I first start cassandra on node A and create the test keyspace and CF:

{code}
create keyspace test
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = {replication_factor : 1}
  and durable_writes = true;

use test;

create column family test
  with column_type = 'Super'
  and comparator = 'UTF8Type'
  and subcomparator = 'UTF8Type'
  and default_validation_class = 'UTF8Type'
  and key_validation_class = 'UTF8Type'
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and row_cache_keys_to_save = 2147483647
  and keys_cached = 200000.0
  and key_cache_save_period = 14400
  and read_repair_chance = 1.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'ConcurrentLinkedHashCacheProvider'
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy';
{code}

Then I perform a load using sstableloader from node D, which works fine with the following output:

{code}
Starting client (and waiting 30 seconds for gossip) ...
 INFO 09:12:17,660 Loading settings from file:/opt/apache-cassandra-1.0.1/conf/cassandra.yaml
 INFO 09:12:17,761 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 09:12:17,771 Global memtable threshold is enabled at 75MB
 INFO 09:12:17,917 Starting up client gossip
 INFO 09:12:17,934 Starting Messaging Service on port 7000
 INFO 09:12:19,941 Node /172.21.31.244 is now part of the cluster
 INFO 09:12:19,941 InetAddress /172.21.31.244 is now UP
 INFO 09:12:48,003 Opening test/test-h-1 (2884 bytes)
 INFO 09:12:48,017 JNA not found. Native methods will be disabled.
Streaming revelant part of test/test-h-1-Data.db to [/172.21.31.244]
 INFO 09:12:48,052 Stream context metadata [test/test-h-1-Data.db sections=1 progress=0/2884 - 0%], 1 sstables.
 INFO 09:12:48,053 Streaming to /172.21.31.244

progress: [/172.21.31.244 0/1 (0)] [total: 0 - 0MB/s (avg: 0MB/s)] INFO 09:12:48,103 Shutting down MessageService...
 INFO 09:12:48,103 Waiting for in-progress requests to complete
 INFO 09:12:48,104 MessagingService shutting down server thread.
{code}

Then I start over by shutting cassandra, deleting all of the data and commitlog dirs, starting cassandra on Node A and Node B and creating the same keyspace and CF.  When I run the loader against that, I get:

{code}
Starting client (and waiting 30 seconds for gossip) ...
 INFO 09:15:09,316 Loading settings from file:/opt/apache-cassandra-1.0.1/conf/cassandra.yaml
 INFO 09:15:09,417 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 09:15:09,427 Global memtable threshold is enabled at 75MB
 INFO 09:15:09,572 Starting up client gossip
 INFO 09:15:09,591 Starting Messaging Service on port 7000
 INFO 09:15:10,777 Node /172.21.31.244 is now part of the cluster
 INFO 09:15:10,778 InetAddress /172.21.31.244 is now UP
 INFO 09:15:10,780 Node /172.21.31.245 is now part of the cluster
 INFO 09:15:10,781 InetAddress /172.21.31.245 is now UP
 INFO 09:15:39,664 Opening test/test-h-1 (2884 bytes)
 INFO 09:15:39,691 JNA not found. Native methods will be disabled.
Streaming revelant part of test/test-h-1-Data.db to [/172.21.31.244, /172.21.31.245]
 INFO 09:15:39,730 Stream context metadata [test/test-h-1-Data.db sections=1 progress=0/274 - 0%], 1 sstables.
 INFO 09:15:39,731 Streaming to /172.21.31.244
 INFO 09:15:39,743 Stream context metadata [test/test-h-1-Data.db sections=2 progress=0/2610 - 0%], 1 sstables.
 INFO 09:15:39,743 Streaming to /172.21.31.245

progress: [/172.21.31.244 0/1 (0)] [/172.21.31.245 0/1 (0)] [total: 0 - 0MB/s (avg: 0MB/s)]Exception in thread ""MiscStage:1"" java.lang.AssertionError: Reference counter -1 for test/test-h-1-Data.db
	at org.apache.cassandra.io.sstable.SSTableReader.releaseReference(SSTableReader.java:715)
	at org.apache.cassandra.streaming.StreamOutSession.startNext(StreamOutSession.java:123)
	at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:59)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)
Exception in thread ""MiscStage:2"" java.lang.AssertionError: Reference counter -2 for test/test-h-1-Data.db
	at org.apache.cassandra.io.sstable.SSTableReader.releaseReference(SSTableReader.java:715)
	at org.apache.cassandra.streaming.StreamOutSession.close(StreamOutSession.java:150)
	at org.apache.cassandra.streaming.StreamOutSession.close(StreamOutSession.java:132)
	at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:67)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)
progress: [/172.21.31.244 1/1 (100)] [/172.21.31.245 1/1 (100)] [total: 100 - 0MB/s (avg: 0MB/s)]
Waiting for targets to rebuild indexes ...
{code}

After that, it never exists.  There are on errors in the logs on the server side for either node.  Additional tests with larger inputs that show the same general error show slightly different behavior, specifically the progress on all but the first node gets past 1/N.  For example, this is the last line on a test of a real data set that had 16 sstables: ""progress: [/172.21.31.244 16/16 (100)] [/172.21.31.245 1/16 (6)] [total: 19 - 0MB/s (avg: 3MB/s)]]""... and it never progresses from there, and avg drops to zero over time indicating nothing is happening.  

I haven't replicated on any previous versions. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/11 16:21;slebresne;3438.patch;https://issues.apache.org/jira/secure/attachment/12502173/3438.patch","01/Nov/11 13:36;jeremypinkham;test-h-1-Data.db;https://issues.apache.org/jira/secure/attachment/12501769/test-h-1-Data.db","01/Nov/11 13:36;jeremypinkham;test-h-1-Index.db;https://issues.apache.org/jira/secure/attachment/12501770/test-h-1-Index.db",,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215515,,,Fri Nov 04 07:53:55 UTC 2011,,,,,,,,,,"0|i0gjwn:",94655,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"03/Nov/11 16:21;slebresne;The sstableloader was just forgetting to acquire references. Patch attached to fix (the patch also attach a very minor cosmetic fix for the progress bar always looks like it did completed the transfer).;;;","03/Nov/11 17:33;jbellis;is this something we could add a unit test for?;;;","03/Nov/11 20:57;slebresne;Probably but painfully as everything that has to do with streaming. But I can have a look. In the meantime I would prefer committing this to be sure it makes 1.0.2 as I'm sure that not acquire references was bad.;;;","04/Nov/11 00:21;jbellis;+1 then;;;","04/Nov/11 07:53;slebresne;Committed (writing the unit test is on my todo list :));;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
invalidate / unregisterSSTables is confused,CASSANDRA-3437,12529618,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,01/Nov/11 04:07,16/Apr/19 09:32,14/Jul/23 05:52,07/Nov/11 17:34,1.0.3,,,,,,0,indexing,,,"invalidate doesn't call unregisterSSTables, and vice versa, making it easy to get yourself into a situation that ""shouldn't happen.""  This is causing test failures post-CASSANDRA-3116.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/11 02:01;jbellis;3437-v2.txt;https://issues.apache.org/jira/secure/attachment/12502634/3437-v2.txt","01/Nov/11 04:10;jbellis;3437.txt;https://issues.apache.org/jira/secure/attachment/12501732/3437.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215477,,,Mon Nov 07 17:34:23 UTC 2011,,,,,,,,,,"0|i0gjw7:",94653,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"01/Nov/11 04:10;jbellis;patch to make invalidate do the equivalent of unregisterSSTables and removes the latter as a separate method.  Uses in tests were replaced by calling clearUnsafe.  Did have to extract CFS.unregisterMBean as a separate method for KeyCacheTest's benefit.

Patch also renames IM.unregisterMBean -> IM.invalidate.

;;;","06/Nov/11 02:07;jbellis;v2 is against 1.0 branch.  Caught a comment change or two that v1 didn't but otherwise the substance is the same.;;;","07/Nov/11 11:00;slebresne;I haven't been to apply v2 cleanly on 1.0, so I haven't checked the unit tests in particular, but from reading the patch it lgtm.;;;","07/Nov/11 14:47;jbellis;p0 problem again?  Just updated 1.0 and it applies cleanly.;;;","07/Nov/11 15:23;slebresne;Not a p0 problem this time but somehow I messed up updating my 1.0 branches, not sure how. Anyway, it does apply, +1.;;;","07/Nov/11 17:34;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL Metadata has inconsistent schema nomenclature,CASSANDRA-3436,12529565,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,kreynolds,kreynolds,31/Oct/11 21:43,16/Apr/19 09:32,14/Jul/23 05:52,01/Nov/11 02:15,1.0.2,,,,,,0,cql,,,"The dumped object below shows that the default_name_type and the default_value_type are referenced inconsistently .. default_name_type should probably use the shortened version like everything else.

--- !ruby/object:CassandraCQL::Thrift::CqlResult 
rows: 
- !ruby/object:CassandraCQL::Thrift::CqlRow 
  columns: 
  - !ruby/object:CassandraCQL::Thrift::Column 
    name: id
    timestamp: -1
    value: test string
  - !ruby/object:CassandraCQL::Thrift::Column 
    name: test_column
    timestamp: 1320097088551000
    value: test
  key: test string
schema: !ruby/object:CassandraCQL::Thrift::CqlMetadata 
  default_name_type: org.apache.cassandra.db.marshal.UTF8Type
  default_value_type: UTF8Type
  name_types: 
    id: AsciiType
  value_types: 
    id: AsciiType
    test_column: UTF8Type
type: 1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215425,,,Tue Nov 01 02:15:16 UTC 2011,,,,,,,,,,"0|i0gjvr:",94651,,,,,Low,,,,,,,,,,,,,,,,,"01/Nov/11 02:15;jbellis;fixed in r1195768;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Describe ring is broken,CASSANDRA-3433,12529528,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,nickmbailey,nickmbailey,nickmbailey,31/Oct/11 18:30,16/Apr/19 09:32,14/Jul/23 05:52,31/Oct/11 20:07,1.0.2,,,Legacy/CQL,Legacy/Tools,,0,thrift,,,CASSANDRA-2882 broke describe_ring by causing the 'endpoints' field to contain the rpc address rather than the listen address. the rpc_address belongs in the 'rpc_endpoints' field. Hence the name.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/11 19:19;nickmbailey;0001-Don-t-use-rpc-address-for-endpoints-field-of-describ.patch;https://issues.apache.org/jira/secure/attachment/12501653/0001-Don-t-use-rpc-address-for-endpoints-field-of-describ.patch",,,,,,,,,,,,,,1.0,nickmbailey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215388,,,Mon Oct 31 20:07:28 UTC 2011,,,,,,,,,,"0|i0gju7:",94644,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"31/Oct/11 19:19;nickmbailey;Fixes the bug as well as adding rack information. ;;;","31/Oct/11 19:20;nickmbailey;Didn't change the thrift version number in that patch. Assuming it is committed, that should also be bumped.;;;","31/Oct/11 20:07;brandon.williams;Committed, thrift version bumped to 19.19;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CompressionMetadata is not shared across threads, we create a new one for each read",CASSANDRA-3427,12529483,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,31/Oct/11 13:57,16/Apr/19 09:32,14/Jul/23 05:52,31/Oct/11 15:44,1.0.2,,,,,,1,compression,,,"The CompressionMetada holds the compressed block offsets in memory. Without being absolutely huge, this is still of non-negligible size as soon as you have a bit of data in the DB. Reallocating this for each read is a very bad idea.

Note that this only affect range queries, since ""normal"" queries uses CompressedSegmentedFile that does reuse a unique CompressionMetadata instance.

( Background: http://thread.gmane.org/gmane.comp.db.cassandra.user/21362 )",,cburroughs,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/11 14:22;slebresne;0001-debugging.patch;https://issues.apache.org/jira/secure/attachment/12503620/0001-debugging.patch","31/Oct/11 15:16;slebresne;3427.patch;https://issues.apache.org/jira/secure/attachment/12501622/3427.patch","03/Nov/11 10:47;slebresne;3427_v2.patch;https://issues.apache.org/jira/secure/attachment/12502124/3427_v2.patch","03/Nov/11 04:55;mck;CASSANDRA-3427.patch;https://issues.apache.org/jira/secure/attachment/12502099/CASSANDRA-3427.patch","13/Nov/11 16:55;mck;jmx_jvm_memory-month.png;https://issues.apache.org/jira/secure/attachment/12503537/jmx_jvm_memory-month.png","13/Nov/11 16:55;mck;jmx_jvm_memory-week.png;https://issues.apache.org/jira/secure/attachment/12503536/jmx_jvm_memory-week.png",,,,,,,,,6.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215343,,,Mon Nov 14 21:01:45 UTC 2011,,,,,,,,,,"0|i0gjrj:",94632,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"31/Oct/11 15:01;mck;This is unfortunately a showstopper for our hadoop jobs querying our production cluster.

With 1.0.1 is there any workaround for this issue?
Is it correct that this ""compressed block offsets"" totals to 
  (<sstable-size> / <chunk_length>) * 8bytes

Therefore a change to a higher chunk_length should be an intermediate workaround?;;;","31/Oct/11 15:20;slebresne;Quite honestly, the best workaround is likely to apply the attached patch on top of 1.0.1 (you can wait for someone to review to get a bit more confidence).

Because yes, a bigger chunk_length would diminish the problem, but if you do enough range_queries you would likely still OOM and there is point after which a chunk_length too big is just counter-productive. ;;;","31/Oct/11 15:44;xedin;Committed.;;;","31/Oct/11 20:10;mck;Rolled out into production. Works a charm! Even on 200Gb sstables.

Sincere appreciations on this one.;;;","02/Nov/11 21:25;mck;Won't the cache here leak?
Many (most?) sstables are transient (gone after the next minor compaction), but this cache will just grow...;;;","02/Nov/11 21:30;xedin;Oh yes, it seems like I missed that one - we should remove entry from the cache when SSTable gets compacted out. What do you think, Sylvain?;;;","02/Nov/11 21:38;mck;Or use a ConcurrentLinkedHashMap w/ fixed capacity?;;;","02/Nov/11 21:43;xedin;yes but that will also imply that we should weight it in memory size instead of number of entries so need to use jamm which is calculation overhead, better just remove unused because we know precisely when to do that...;;;","03/Nov/11 04:55;mck;patch according to Pavel's suggestion;;;","03/Nov/11 10:47;slebresne;Ok, I think using an object cache is ugly (I know that it was my idea).

At first, I tried going with the natural idea, to add the compressionMetadata as a final field of SSTableReader and use that everywhere, ensuring we use one per sstable. Turned out that for SSTableWriter you need to have the metadata existing before the SSTableReader is created and that seemed like a bit of a mess so I backtracked and decided to go with an object cache in CompressionMetada, but more out of laziness than anything else.

That was wrong of me to be lazy. We don't need that object cache and if its use is going to leak out of CompressionMetada (like hard coding the addition of the notifier in the DataTracker constructor; which defeats the purpose of the notifier abstraction in the first place) then it's not even clean as far as code is concerned.

Attaching a v2 patch that remove the cache and do a slight modification of the initial idea, that is it just let CompressedSegmentedFile create the CompressionMetada and have the rest of the code use that. Turns out that once I plug my brain, it's only a few lines of code.
;;;","03/Nov/11 14:09;xedin;+1;;;","03/Nov/11 14:17;slebresne;Alright, committed this new version, thanks;;;","13/Nov/11 11:39;mck;Handling jvm memory since upgrading to cassandra-1.0 and enabling compression is still a headache.
Where i used to be able to run w/ Xmx8G i'm now struggling to run with Xmx20G (all caches are disabled) and during startup can hit
{noformat}java.lang.OutOfMemoryError: Java heap space
        at org.apache.cassandra.utils.BigLongArray.<init>(BigLongArray.java:53)
        at org.apache.cassandra.utils.BigLongArray.<init>(BigLongArray.java:39)
        at org.apache.cassandra.io.compress.CompressionMetadata.readChunkOffsets(CompressionMetadata.java:122){noformat}

I could keep increasing chunk_length (it's already at 256) but this seems awkward just to get a cluster running smoothly. At minimum the calculations for memory requirements for cassandra should be re-written if compression is to take such a large chunk of heap.;;;","13/Nov/11 13:33;jbellis;Does heap usage stay high-post startup?  Can you try forcing a full GC to check that?;;;","13/Nov/11 16:45;mck;bq. Does heap usage stay high-post startup? Can you try forcing a full GC to check that?
Yes. GC doesn't seem to help, i'll attach a munin graph that shows it over time. It was running for a number of days just under 20G, but you can see from that how ""squeezed"" it was.

(invoking full gc via jmx has no noticeable effect on heap used);;;","13/Nov/11 16:55;mck;0.8 was running on Xmx8G up until week 44.
at that point we upgraded to 1.0 and enabled compression. The very high memory usage at the beginning of week 44 was to handle the change from chunk_length 16 to 256.
Then for most of week 44 and week 45 we ran with Xmx16G, but this was very ""squeezed"". Now that's OOM, and raising it to 20G didn't help. Currently it's on 30G.

Also note we're always used -XX:CMSInitiatingOccupancyFraction=60 for this cluster.
(full java opts are ""-Xss128k -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42  -XX:SurvivorRatio=8  -XX:MaxTenuringThreshold=1  -XX:+UseParNewGC  -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:CMSInitiatingOccupancyFraction=60 -XX:+UseCMSInitiatingOccupancyOnly -Xmx30g -Xmx30g -Xmn800M   -XX:ParallelCMSThreads=4  -XX:+CMSIncrementalMode  -XX:+CMSIncrementalPacing  -XX:CMSIncrementalDutyCycleMin=0  -XX:CMSIncrementalDutyCycle=10"". the last 5 were added during week 44 to try and help, ref http://blog.mikiobraun.de/2010/08/cassandra-gc-tuning.html);;;","14/Nov/11 14:12;jbellis;What version exactly are you running now?  1.0.2?  1.0.0 + 3427?  Something else?;;;","14/Nov/11 14:25;slebresne;I've attached a tiny patch (0001-debugging.patch) that prints in the log the size of the long array we allocate for the chunk offsets. Would you mind trying with this and attach a log of when startup hits one of the OOM you pasted earlier (feel free to use a 8GB heap if it's easier to reproduce). I'd like to know if those offsets are indeed the problem.;;;","14/Nov/11 14:30;jbellis;I can get you a place to upload the heap dump from an OOM too.  (8GB would be best, since heap analysis requires ram proportional to the heap size.)

To rule out the obvious, have you tried running 1.0.x w/o compression?;;;","14/Nov/11 18:14;mck;version: 1.0.2 snapshot (pretty close to release date) + CASSANDRA-3197.
w/o compression: that would require a full compact/scrub. that takes close to 24hrs :-(
patch: i attach that and hopefully have output soon. a heap dump can be done at the same time...;;;","14/Nov/11 20:06;mck;startup log with debug (off 1.0.2 release){noformat}INFO  48:34,688 DatabaseDescriptor: Loading settings from file:/iad/finn/countstatistics/conf/cassandra-prod.yaml
INFO  48:34,782 DatabaseDescriptor: DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO  48:34,792 DatabaseDescriptor: Global memtable threshold is enabled at 512MB
INFO  48:34,890 AbstractCassandraDaemon: JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_24
INFO  48:34,891 AbstractCassandraDaemon: Heap size: 760414208/8506048512
INFO  48:34,891 AbstractCassandraDaemon: Classpath: /iad/finn/countstatistics/jar/countstatistics.jar:/iad/common/apps/cassandra/lib/jamm-0.2.5.jar
INFO  48:37,158 CLibrary: JNA mlockall successful
INFO  48:37,879 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/Versions-h-42 (256 bytes)
INFO  48:37,879 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/Versions-h-41 (256 bytes)
INFO  48:37,879 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/Versions-h-40 (256 bytes)
INFO  48:37,959 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/IndexInfo-h-3 (223 bytes)
INFO  48:38,001 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/Schema-h-15 (34257 bytes)
INFO  48:38,045 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/Migrations-h-15 (78524 bytes)
INFO  48:38,096 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/LocationInfo-h-150 (80 bytes)
INFO  48:38,096 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/LocationInfo-h-149 (628 bytes)
INFO  48:38,096 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/system/LocationInfo-h-151 (163 bytes)
INFO  48:38,192 DatabaseDescriptor: Loading schema version 1940c630-0be4-11e1-0000-d1695892b1ff
INFO  51:35,136 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191473 (38646535 bytes)
INFO  51:35,136 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-190467 (2284524668 bytes)
INFO  51:35,136 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191469 (254927460 bytes)
INFO  51:35,136 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191475 (30477008 bytes)
INFO  51:35,136 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-114136 (156044360682 bytes)
INFO  51:35,137 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191294 (4585008988 bytes)
INFO  51:35,137 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-190415 (15857295280 bytes)
INFO  51:35,137 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-183183 (196289440978 bytes)
INFO  51:35,137 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191472 (1346076 bytes)
INFO  51:35,137 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-190736 (4626053255 bytes)
INFO  51:35,137 SSTableReader: Opening /iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191435 (1188223188 bytes)
INFO  51:35,187 CompressionMetadata: Allocating chunks index for 5745 chunks for uncompressed size of 1470519 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191472-CompressionInfo.db)
INFO  51:35,421 CompressionMetadata: Allocating chunks index for 129646 chunks for uncompressed size of 33189311 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191475-CompressionInfo.db)
INFO  51:35,544 CompressionMetadata: Allocating chunks index for 165602 chunks for uncompressed size of 42393918 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191473-CompressionInfo.db)
INFO  51:37,171 CompressionMetadata: Allocating chunks index for 1091377 chunks for uncompressed size of 279392485 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191469-CompressionInfo.db)
INFO  51:41,148 CompressionMetadata: Allocating chunks index for 5086138 chunks for uncompressed size of 1302051278 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191435-CompressionInfo.db)
INFO  51:46,351 CompressionMetadata: Allocating chunks index for 9766541 chunks for uncompressed size of 2500234376 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-190467-CompressionInfo.db)
INFO  51:56,717 CompressionMetadata: Allocating chunks index for 19828434 chunks for uncompressed size of 5076078986 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-190736-CompressionInfo.db)
INFO  51:56,897 CompressionMetadata: Allocating chunks index for 19626358 chunks for uncompressed size of 5024347477 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-191294-CompressionInfo.db)
INFO  52:21,670 CompressionMetadata: Allocating chunks index for 67865822 chunks for uncompressed size of 17373650297 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-190415-CompressionInfo.db)
INFO  55:55,920 CompressionMetadata: Allocating chunks index for 666981588 chunks for uncompressed size of 170747286320 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-114136-CompressionInfo.db)
INFO  56:49,620 CompressionMetadata: Allocating chunks index for 840404671 chunks for uncompressed size of 215143595584 (/iad/finn/countstatistics/cassandra-data/countstatisticsCount/thrift_no_finntech_countstats_count_Count_neg8589045746818385983-h-183183-CompressionInfo.db)
ERROR 57:51,112 AbstractCassandraDaemon: Fatal exception in thread Thread[SSTableBatchOpen:8,5,main]
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.utils.BigLongArray.<init>(BigLongArray.java:53)
	at org.apache.cassandra.utils.BigLongArray.<init>(BigLongArray.java:39)
	at org.apache.cassandra.io.compress.CompressionMetadata.readChunkOffsets(CompressionMetadata.java:127)
        ...{noformat};;;","14/Nov/11 21:01;slebresne;That's the stupidest bug ever. It happens we interpret the chunk_length_in_kb not in kb but in bytes.
Anyway, I've created CASSANDRA-3492 to address this.
Turns out if you don't update the chunk_length you're fine because the default is ok, but I guess hitting this issue initially has put you in the wrong spot :(;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can create a Column Family with comparator CounterColumnType which is subsequently unusable,CASSANDRA-3422,12529368,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,kreynolds,kreynolds,29/Oct/11 16:46,16/Apr/19 09:32,14/Jul/23 05:52,07/Dec/11 16:10,0.8.9,1.0.6,,,,,0,,,,"It's probably the case that this shouldn't be allowed at all but one is currently allowed to create a Column Family with comparator CounterColumnType which then appears unusable.

CREATE COLUMNFAMILY comparator_cf_counter (id text PRIMARY KEY) WITH comparator=CounterColumnType

# Fails
UPDATE comparator_cf_counter SET 1=1 + 1 WHERE id='test_key'

Error => invalid operation for non commutative columnfamily comparator_cf_counter",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/11 15:50;slebresne;3422-v2.patch;https://issues.apache.org/jira/secure/attachment/12506266/3422-v2.patch","01/Nov/11 13:29;slebresne;3422.patch;https://issues.apache.org/jira/secure/attachment/12501768/3422.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215228,,,Wed Dec 07 17:51:13 UTC 2011,,,,,,,,,,"0|i0gjpb:",94622,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"30/Oct/11 13:21;kreynolds;One can also create a table with CounterColumnType as the row_key type, probably should disallow that.;;;","01/Nov/11 13:30;slebresne;Patch is against 1.0. I'll rebase against 0.8 for the commit.;;;","01/Nov/11 14:19;jbellis;Shouldn't this go in ThriftValidation.validateCfDef (which is already called by QP as well as the thrift side)?;;;","01/Nov/11 14:25;slebresne;bq. which is already called by QP

I don't see that. I see it called for CREATE_INDEX but not for CREATE_COLUMNFAMILY, for which it seems we never create a thrift CfDef (which is reasonable). I mean, all this is imho a complete mess, between CFM, thrift CfDef, avro CfDef, etc... seems there is nothing consistent in there.
I'm happy to do this differently but I failed to see the correct place for that.;;;","24/Nov/11 15:33;slebresne;Any opinion related to my previous comment?;;;","02/Dec/11 22:58;jbellis;You're right, it's all over the place.  CFMD is fine.  But it also needs to check that we don't use CCT as an individual column type.  (CASSANDRA-2614.);;;","06/Dec/11 15:50;slebresne;Attached v2 checks we don't mix counter and non-counters;;;","06/Dec/11 16:12;jbellis;+1;;;","07/Dec/11 17:51;hudson;Integrated in Cassandra-0.8 #411 (See [https://builds.apache.org/job/Cassandra-0.8/411/])
    Detect misuses of CounterColumnType
patch by slebresne; reviewed by jbellis for CASSANDRA-3422

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211486
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/CreateColumnFamilyStatement.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can create a table with DecimalType comparator but CQL explodes trying to actually use it.,CASSANDRA-3421,12529365,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ardot,kreynolds,kreynolds,29/Oct/11 16:25,16/Apr/19 09:32,14/Jul/23 05:52,03/Nov/11 04:53,1.0.2,,,,,,0,cql,,,"CREATE KEYSPACE CassandraCQLTestKeyspace WITH strategy_class='org.apache.cassandra.locator.SimpleStrategy' AND strategy_options:replication_factor=1

USE CassandraCQLTestKeyspace

CREATE COLUMNFAMILY comparator_cf_decimal (id text PRIMARY KEY) WITH comparator='DecimalType'

INSERT INTO comparator_cf_decimal (id, 15.333) VALUES ('test', 'test')

# This leads to failure
SELECT 15.333 FROM comparator_cf_decimal WHERE id = 'test' 

ERROR 12:22:56,909 Internal error processing execute_cql_query
java.nio.BufferUnderflowException
	at java.nio.Buffer.nextGetIndex(Buffer.java:480)
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:336)
	at org.apache.cassandra.cql.jdbc.JdbcDecimal.compose(JdbcDecimal.java:90)
	at org.apache.cassandra.db.marshal.DecimalType.compose(DecimalType.java:43)
	at org.apache.cassandra.db.marshal.DecimalType.compare(DecimalType.java:38)
	at org.apache.cassandra.db.marshal.DecimalType.compare(DecimalType.java:30)
	at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:351)
	at java.util.TreeMap.getEntry(TreeMap.java:322)
	at java.util.TreeMap.get(TreeMap.java:255)
	at org.apache.cassandra.db.TreeMapBackedSortedColumns.getColumn(TreeMapBackedSortedColumns.java:138)
	at org.apache.cassandra.db.AbstractColumnContainer.getColumn(AbstractColumnContainer.java:128)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:627)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1236)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
",,ardot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/11 18:50;ardot;3421-v2.txt;https://issues.apache.org/jira/secure/attachment/12502010/3421-v2.txt",,,,,,,,,,,,,,1.0,ardot,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215225,,,Thu Nov 03 04:53:32 UTC 2011,,,,,,,,,,"0|i0gjov:",94620,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"29/Oct/11 16:32;kreynolds;FYI, this is the only Type that does this, every other type appears to function properly including Float and Double;;;","01/Nov/11 13:49;ardot;This occurs because the {{compose}} method in {{DecimalType}} does not protect itself  by checking for an empty {{ByteBuffer}} argument which is what occurs if a single item is in the CF.;;;","01/Nov/11 14:21;jbellis;Can you include a unit test for the problem?;;;","01/Nov/11 14:53;ardot;Sure. Are you looking for a unit test for {{DecimalType}}, or for the problem as stated? I can see where there are various unit tests for data types, but there does not seem to be a package in {{test/unit}} for CQL unit tests? Did I miss it?;;;","01/Nov/11 14:56;jbellis;Just for DecimalType.

(We don't have any CQL unit tests in C* since we moved all the drivers out of tree.);;;","02/Nov/11 18:50;ardot;V2 contains fix and unit test;;;","03/Nov/11 04:53;jbellis;Thanks, Rick!

Committed with a minor change: switched from mark/reset to duplicate as a ""don't modify the original buffer"" approach.  mark/modify/reset can cause problems if another thread also tries to use the buffer in between.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counter decrements require a space around the minus sign but not around the plus sign,CASSANDRA-3418,12529307,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,kreynolds,kreynolds,28/Oct/11 19:45,16/Apr/19 09:32,14/Jul/23 05:52,11/Nov/11 16:50,1.0.3,,,,,,0,cql,,,"UPDATE validation_cf_counter SET test=test+1 WHERE id='test_key' => Success
UPDATE validation_cf_counter SET test=test + 1 WHERE id='test_key' => Success
UPDATE validation_cf_counter SET test=test - 1 WHERE id='test_key' => Success
UPDATE validation_cf_counter SET test=test-1 WHERE id='test_key' => Failure (line 1:38 no viable alternative at input 'test')

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/11 20:29;xedin;CASSANDRA-3418-fix.patch;https://issues.apache.org/jira/secure/attachment/12503788/CASSANDRA-3418-fix.patch","11/Nov/11 14:54;xedin;CASSANDRA-3418.patch;https://issues.apache.org/jira/secure/attachment/12503375/CASSANDRA-3418.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215167,,,Wed Nov 16 18:50:51 UTC 2011,,,,,,,,,,"0|i0gjnj:",94614,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"11/Nov/11 16:25;jbellis;+1

time to add QueryProcessorTest?;;;","11/Nov/11 16:50;xedin;Committed. It's not related to QueryProcessor tho;;;","14/Nov/11 20:12;thepaul;This doesn't do the right thing when minusing a negative value, like:

{noformat}
UPDATE validation_cf_counter SET test = test - -1 WHERE id='test_key';
{noformat}

It subtracts 1, instead of adding 1.;;;","14/Nov/11 20:27;jbellis;bq. It's not related to QueryProcessor tho

But that's the simplest way to add CQL tests I would think?  Open to other ideas.;;;","15/Nov/11 20:29;xedin;here is the patch that fixes problem that Paul found. 

@Jonathan: I agree, we probably should do the same thing for CQL as we did for CLI.;;;","16/Nov/11 18:02;thepaul;+1 for the fix;;;","16/Nov/11 18:50;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InvocationTargetException ConcurrentModificationException at startup,CASSANDRA-3417,12529294,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,scode,j.casares,j.casares,28/Oct/11 18:33,16/Apr/19 09:32,14/Jul/23 05:52,13/Feb/12 21:45,1.0.8,,,,,,0,,,,"I was starting up the new DataStax AMI where the seed starts first and 34 nodes would latch on together. So far things have been working decently for launching, but right now I just got this during startup.


{CODE}
ubuntu@ip-10-40-190-143:~$ sudo cat /var/log/cassandra/output.log 
 INFO 09:24:38,453 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_26
 INFO 09:24:38,456 Heap size: 1936719872/1937768448
 INFO 09:24:38,457 Classpath: /usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/avro-1.4.0-fixes.jar:/usr/share/cassandra/lib/avro-1.4.0-sources-fixes.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang-2.4.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.2.jar:/usr/share/cassandra/lib/guava-r08.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.4.0.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.4.0.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jline-0.9.94.jar:/usr/share/cassandra/lib/joda-time-1.6.2.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.6.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.6.1.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.6.1.jar:/usr/share/cassandra/lib/snakeyaml-1.6.jar:/usr/share/cassandra/lib/snappy-java-1.0.3.jar:/usr/share/cassandra/apache-cassandra-1.0.0.jar:/usr/share/cassandra/apache-cassandra-thrift-1.0.0.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/java/jna.jar:/etc/cassandra:/usr/share/java/commons-daemon.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar
 INFO 09:24:39,891 JNA mlockall successful
 INFO 09:24:39,901 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 09:24:40,057 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 09:24:40,069 Global memtable threshold is enabled at 616MB
 INFO 09:24:40,159 EC2Snitch using region: us-east, zone: 1d.
 INFO 09:24:40,475 Creating new commitlog segment /raid0/cassandra/commitlog/CommitLog-1319793880475.log
 INFO 09:24:40,486 Couldn't detect any schema definitions in local storage.
 INFO 09:24:40,486 Found table data in data directories. Consider using the CLI to define your schema.
 INFO 09:24:40,497 No commitlog files found; skipping replay
 INFO 09:24:40,501 Cassandra version: 1.0.0
 INFO 09:24:40,502 Thrift API version: 19.18.0
 INFO 09:24:40,502 Loading persisted ring state
 INFO 09:24:40,506 Starting up server gossip
 INFO 09:24:40,529 Enqueuing flush of Memtable-LocationInfo@1388314661(190/237 serialized/live bytes, 4 ops)
 INFO 09:24:40,530 Writing Memtable-LocationInfo@1388314661(190/237 serialized/live bytes, 4 ops)
 INFO 09:24:40,600 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-1-Data.db (298 bytes)
 INFO 09:24:40,613 Ec2Snitch adding ApplicationState ec2region=us-east ec2zone=1d
 INFO 09:24:40,621 Starting Messaging Service on /10.40.190.143:7000
 INFO 09:24:40,628 Joining: waiting for ring and schema information
 INFO 09:24:43,389 InetAddress /10.194.29.156 is now dead.
 INFO 09:24:43,391 InetAddress /10.85.11.38 is now dead.
 INFO 09:24:43,392 InetAddress /10.34.42.28 is now dead.
 INFO 09:24:43,393 InetAddress /10.77.63.49 is now dead.
 INFO 09:24:43,394 InetAddress /10.194.22.191 is now dead.
 INFO 09:24:43,395 InetAddress /10.34.74.58 is now dead.
 INFO 09:24:43,395 Node /10.34.33.16 is now part of the cluster
 INFO 09:24:43,396 InetAddress /10.34.33.16 is now UP
 INFO 09:24:43,397 Enqueuing flush of Memtable-LocationInfo@1629818866(20/25 serialized/live bytes, 1 ops)
 INFO 09:24:43,398 Writing Memtable-LocationInfo@1629818866(20/25 serialized/live bytes, 1 ops)
 INFO 09:24:43,417 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-2-Data.db (74 bytes)
 INFO 09:24:43,418 InetAddress /10.202.67.43 is now dead.
 INFO 09:24:43,419 InetAddress /10.116.215.81 is now dead.
 INFO 09:24:43,420 InetAddress /10.99.39.242 is now dead.
 INFO 09:24:43,421 InetAddress /10.80.110.28 is now dead.
 INFO 09:24:43,422 InetAddress /10.118.233.198 is now dead.
 INFO 09:24:43,423 InetAddress /10.40.177.173 is now dead.
 INFO 09:24:43,424 InetAddress /10.205.23.34 is now dead.
 INFO 09:24:43,425 InetAddress /10.101.41.8 is now dead.
 INFO 09:24:43,669 InetAddress /10.118.230.219 is now dead.
 INFO 09:24:43,670 InetAddress /10.80.41.192 is now dead.
 INFO 09:24:43,671 InetAddress /10.40.22.224 is now dead.
 INFO 09:24:43,672 InetAddress /10.39.107.114 is now dead.
 INFO 09:24:46,164 InetAddress /10.118.185.68 is now dead.
 INFO 09:24:46,166 InetAddress /10.84.205.93 is now dead.
 INFO 09:24:46,167 InetAddress /10.116.134.183 is now dead.
 INFO 09:24:46,670 InetAddress /10.118.179.67 is now dead.
 INFO 09:24:46,671 InetAddress /10.116.241.250 is now dead.
 INFO 09:24:48,441 InetAddress /10.118.94.62 is now dead.
 INFO 09:24:48,442 InetAddress /10.99.86.251 is now dead.
 INFO 09:24:50,176 InetAddress /10.113.42.21 is now dead.
 INFO 09:24:50,177 InetAddress /10.34.159.72 is now dead.
 INFO 09:24:50,178 InetAddress /10.32.79.134 is now dead.
 INFO 09:24:50,179 InetAddress /10.80.210.38 is now dead.
 INFO 09:24:50,180 InetAddress /10.34.70.73 is now dead.
 INFO 09:24:50,181 InetAddress /10.196.79.240 is now dead.
 INFO 09:25:01,713 InetAddress /10.82.210.172 is now dead.
 INFO 09:25:06,202 InetAddress /10.80.110.28 is now UP
 INFO 09:25:06,908 InetAddress /10.99.39.242 is now UP
 INFO 09:25:07,696 InetAddress /10.118.233.198 is now UP
 INFO 09:25:07,697 InetAddress /10.205.23.34 is now UP
 INFO 09:25:08,704 InetAddress /10.194.22.191 is now UP
 INFO 09:25:08,705 InetAddress /10.40.177.173 is now UP
 INFO 09:25:08,706 InetAddress /10.101.41.8 is now UP
 INFO 09:25:09,489 InetAddress /10.202.67.43 is now UP
 INFO 09:25:09,698 InetAddress /10.77.63.49 is now UP
 INFO 09:25:10,628 Joining: getting bootstrap token
 INFO 09:25:10,631 Enqueuing flush of Memtable-LocationInfo@1733057335(36/45 serialized/live bytes, 1 ops)
 INFO 09:25:10,631 Writing Memtable-LocationInfo@1733057335(36/45 serialized/live bytes, 1 ops)
 INFO 09:25:10,647 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-3-Data.db (87 bytes)
 INFO 09:25:10,649 Joining: sleeping 30000 ms for pending range setup
 INFO 09:25:10,689 InetAddress /10.85.11.38 is now UP
 INFO 09:25:10,708 InetAddress /10.34.74.58 is now UP
 INFO 09:25:10,912 InetAddress /10.194.29.156 is now UP
 INFO 09:25:11,261 Applying migration bb843dd0-0146-11e1-0000-b877c09da5ff Add keyspace: OpsCenter, rep strategy:SimpleStrategy{org.apache.cassandra.config.CFMetaData@55e29b99[cfId=1000,ksName=OpsCenter,cfName=pdps,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=300.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@105585dc,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@5ec736e4[cfId=1004,ksName=OpsCenter,cfName=rollups86400,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=2,maxCompactionThreshold=8,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@68e4e358,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@b09dc35[cfId=1003,ksName=OpsCenter,cfName=rollups7200,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=2,maxCompactionThreshold=8,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@3458213c,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@5ee04fd[cfId=1002,ksName=OpsCenter,cfName=rollups300,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=16,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@4d898115,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@7e79b177[cfId=1005,ksName=OpsCenter,cfName=events,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=OpsCenter raw event storage,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=8,maxCompactionThreshold=12,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@67723c7f,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@540523be[cfId=1006,ksName=OpsCenter,cfName=events_timeline,cfType=Standard,comparator=org.apache.cassandra.db.marshal.LongType,subcolumncomparator=<null>,comment=OpsCenter event timelines,rowCacheSize=0.0,keyCacheSize=5.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=8,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@1d6dba0a,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@ed0f59e[cfId=1007,ksName=OpsCenter,cfName=settings,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=OpsCenter settings,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=8,maxCompactionThreshold=12,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@38ad5fab,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@7e63f09e[cfId=1001,ksName=OpsCenter,cfName=rollups60,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@534a55e5,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]}, durable_writes: true
 INFO 09:25:11,273 Enqueuing flush of Memtable-Migrations@1767199109(12925/16156 serialized/live bytes, 1 ops)
 INFO 09:25:11,273 Writing Memtable-Migrations@1767199109(12925/16156 serialized/live bytes, 1 ops)
 INFO 09:25:11,274 Enqueuing flush of Memtable-Schema@1616586953(5820/7275 serialized/live bytes, 3 ops)
 INFO 09:25:11,358 Completed flushing /raid0/cassandra/data/system/Migrations-h-1-Data.db (12989 bytes)
 INFO 09:25:11,358 Writing Memtable-Schema@1616586953(5820/7275 serialized/live bytes, 3 ops)
 INFO 09:25:11,390 Completed flushing /raid0/cassandra/data/system/Schema-h-1-Data.db (5970 bytes)
 INFO 09:25:11,727 InetAddress /10.116.215.81 is now UP
 INFO 09:25:11,744 InetAddress /10.34.42.28 is now UP
 INFO 09:25:11,750 InetAddress /10.40.22.224 is now UP
 INFO 09:25:12,023 InetAddress /10.80.41.192 is now UP
 INFO 09:25:12,712 InetAddress /10.39.107.114 is now UP
 INFO 09:25:12,717 InetAddress /10.118.185.68 is now UP
 INFO 09:25:12,721 InetAddress /10.116.134.183 is now UP
 INFO 09:25:13,322 InetAddress /10.118.230.219 is now UP
 INFO 09:25:13,632 InetAddress /10.84.205.93 is now UP
 INFO 09:25:14,713 InetAddress /10.118.179.67 is now UP
 INFO 09:25:14,717 InetAddress /10.116.241.250 is now UP
 INFO 09:25:17,468 InetAddress /10.34.159.72 is now UP
 INFO 09:25:17,476 InetAddress /10.118.94.62 is now UP
 INFO 09:25:17,480 InetAddress /10.80.210.38 is now UP
 INFO 09:25:17,716 InetAddress /10.32.79.134 is now UP
 INFO 09:25:17,721 InetAddress /10.99.86.251 is now UP
 INFO 09:25:18,717 InetAddress /10.196.79.240 is now UP
 INFO 09:25:18,727 InetAddress /10.34.70.73 is now UP
 INFO 09:25:19,596 InetAddress /10.113.42.21 is now UP
 INFO 09:25:25,750 InetAddress /10.82.210.172 is now UP
 INFO 09:25:37,743 Enqueuing flush of Memtable-LocationInfo@288976631(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,744 Writing Memtable-LocationInfo@288976631(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,764 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-4-Data.db (89 bytes)
 INFO 09:25:37,773 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-1-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-3-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-4-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-2-Data.db')]
 INFO 09:25:37,776 Enqueuing flush of Memtable-LocationInfo@1950702248(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,777 Writing Memtable-LocationInfo@1950702248(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,821 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-5-Data.db (89 bytes)
 INFO 09:25:37,869 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-6-Data.db,].  548 to 443 (~80% of original) bytes for 3 keys at 0.006500MB/s.  Time: 65ms.
 INFO 09:25:38,740 Enqueuing flush of Memtable-LocationInfo@92917455(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,740 Writing Memtable-LocationInfo@92917455(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,757 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-8-Data.db (89 bytes)
 INFO 09:25:38,766 Enqueuing flush of Memtable-LocationInfo@1096488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,767 Writing Memtable-LocationInfo@1096488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,814 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-9-Data.db (89 bytes)
 INFO 09:25:38,816 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-6-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-9-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-8-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-5-Data.db')]
 INFO 09:25:38,823 Enqueuing flush of Memtable-LocationInfo@1734564525(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,823 Writing Memtable-LocationInfo@1734564525(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,893 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-10-Data.db (89 bytes)
 INFO 09:25:38,916 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-11-Data.db,].  710 to 548 (~77% of original) bytes for 3 keys at 0.005226MB/s.  Time: 100ms.
 INFO 09:25:39,538 Enqueuing flush of Memtable-LocationInfo@811507066(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,539 Writing Memtable-LocationInfo@811507066(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,555 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-13-Data.db (89 bytes)
 INFO 09:25:39,578 Enqueuing flush of Memtable-LocationInfo@1125690366(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,578 Writing Memtable-LocationInfo@1125690366(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,594 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-14-Data.db (89 bytes)
 INFO 09:25:39,596 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-11-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-10-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-14-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-13-Data.db')]
 INFO 09:25:39,613 Enqueuing flush of Memtable-LocationInfo@1870148830(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,614 Writing Memtable-LocationInfo@1870148830(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,652 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-16-Data.db (89 bytes)
 INFO 09:25:39,692 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-15-Data.db,].  815 to 653 (~80% of original) bytes for 3 keys at 0.006487MB/s.  Time: 96ms.
 INFO 09:25:39,731 Enqueuing flush of Memtable-LocationInfo@1279866611(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,731 Writing Memtable-LocationInfo@1279866611(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,747 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-18-Data.db (89 bytes)
 INFO 09:25:40,649 Starting to bootstrap...
 INFO 09:25:40,701 Finished streaming session 304272969286 from /10.205.23.34
 INFO 09:25:40,703 Enqueuing flush of Memtable-LocationInfo@1868577756(53/66 serialized/live bytes, 2 ops)
 INFO 09:25:40,703 Writing Memtable-LocationInfo@1868577756(53/66 serialized/live bytes, 2 ops)
 INFO 09:25:40,721 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-19-Data.db (163 bytes)
 INFO 09:25:40,722 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-19-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-15-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-18-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-16-Data.db')]
 INFO 09:25:40,726 Node /10.40.190.143 state jump to normal
 INFO 09:25:40,734 Enqueuing flush of Memtable-LocationInfo@641287650(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:40,735 Writing Memtable-LocationInfo@641287650(35/43 serialized/live bytes, 1 ops)
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
    at java.util.HashMap$EntryIterator.next(HashMap.java:834)
    at java.util.HashMap$EntryIterator.next(HashMap.java:832)
    at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:301)
    at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:293)
    at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1127)
    at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1084)
    at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:920)
    at org.apache.cassandra.service.StorageService.onChange(StorageService.java:805)
    at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:880)
    at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1027)
    at org.apache.cassandra.service.StorageService.setToken(StorageService.java:226)
    at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:573)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:460)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:381)
    at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:215)
    at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:238)
    ... 5 more
Cannot load daemon
Service exit with a return value of 3
 INFO 09:35:35,156 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_26
 INFO 09:35:35,159 Heap size: 1936719872/1937768448
 INFO 09:35:35,160 Classpath: /usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/avro-1.4.0-fixes.jar:/usr/share/cassandra/lib/avro-1.4.0-sources-fixes.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang-2.4.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.2.jar:/usr/share/cassandra/lib/guava-r08.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.4.0.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.4.0.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jline-0.9.94.jar:/usr/share/cassandra/lib/joda-time-1.6.2.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.6.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.6.1.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.6.1.jar:/usr/share/cassandra/lib/snakeyaml-1.6.jar:/usr/share/cassandra/lib/snappy-java-1.0.3.jar:/usr/share/cassandra/apache-cassandra-1.0.0.jar:/usr/share/cassandra/apache-cassandra-thrift-1.0.0.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/java/jna.jar:/etc/cassandra:/usr/share/java/commons-daemon.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar
 INFO 09:35:36,626 JNA mlockall successful
 INFO 09:35:36,636 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 09:35:36,757 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 09:35:36,769 Global memtable threshold is enabled at 616MB
 INFO 09:35:36,811 EC2Snitch using region: us-east, zone: 1d.
 INFO 09:35:37,030 Opening /raid0/cassandra/data/system/Schema-h-1 (5970 bytes)
 INFO 09:35:37,067 Opening /raid0/cassandra/data/system/Migrations-h-1 (12989 bytes)
 INFO 09:35:37,075 Opening /raid0/cassandra/data/system/LocationInfo-h-19 (163 bytes)
 INFO 09:35:37,075 Opening /raid0/cassandra/data/system/LocationInfo-h-18 (89 bytes)
 INFO 09:35:37,083 Opening /raid0/cassandra/data/system/LocationInfo-h-15 (653 bytes)
 INFO 09:35:37,085 Opening /raid0/cassandra/data/system/LocationInfo-h-16 (89 bytes)
 INFO 09:35:37,131 Loading schema version bb843dd0-0146-11e1-0000-b877c09da5ff
 INFO 09:35:37,372 Creating new commitlog segment /raid0/cassandra/commitlog/CommitLog-1319794537372.log
 INFO 09:35:37,384 Replaying /raid0/cassandra/commitlog/CommitLog-1319793880475.log
 INFO 09:35:37,416 Finished reading /raid0/cassandra/commitlog/CommitLog-1319793880475.log
 INFO 09:35:37,422 Enqueuing flush of Memtable-events@1830423861(164/205 serialized/live bytes, 5 ops)
 INFO 09:35:37,423 Writing Memtable-events@1830423861(164/205 serialized/live bytes, 5 ops)
 INFO 09:35:37,424 Enqueuing flush of Memtable-Versions@817138449(83/103 serialized/live bytes, 3 ops)
 INFO 09:35:37,472 Completed flushing /raid0/cassandra/data/OpsCenter/events-h-1-Data.db (230 bytes)
 INFO 09:35:37,479 Writing Memtable-Versions@817138449(83/103 serialized/live bytes, 3 ops)
 INFO 09:35:37,497 Completed flushing /raid0/cassandra/data/system/Versions-h-1-Data.db (247 bytes)
 INFO 09:35:37,497 Log replay complete, 4 replayed mutations
 INFO 09:35:37,509 Cassandra version: 1.0.0
 INFO 09:35:37,510 Thrift API version: 19.18.0
 INFO 09:35:37,510 Loading persisted ring state
 INFO 09:35:37,528 Starting up server gossip
 INFO 09:35:37,530 Enqueuing flush of Memtable-LocationInfo@1655441108(29/36 serialized/live bytes, 1 ops)
 INFO 09:35:37,530 Writing Memtable-LocationInfo@1655441108(29/36 serialized/live bytes, 1 ops)
 INFO 09:35:37,554 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-20-Data.db (80 bytes)
 INFO 09:35:37,555 Ec2Snitch adding ApplicationState ec2region=us-east ec2zone=1d
 INFO 09:35:37,562 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-16-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-18-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-19-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-20-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-15-Data.db')]
 INFO 09:35:37,566 Starting Messaging Service on /10.40.190.143:7000
 INFO 09:35:37,592 Using saved token 19444706681196483626478548996101040654
 INFO 09:35:37,593 Enqueuing flush of Memtable-LocationInfo@995684858(53/66 serialized/live bytes, 2 ops)
 INFO 09:35:37,593 Writing Memtable-LocationInfo@995684858(53/66 serialized/live bytes, 2 ops)
 INFO 09:35:37,616 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-22-Data.db (163 bytes)
 INFO 09:35:37,620 Node /10.40.190.143 state jump to normal
 INFO 09:35:37,639 Bootstrap/Replace/Move completed! Now serving reads.
 INFO 09:35:37,640 Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO 09:35:37,684 Binding thrift service to /0.0.0.0:9160
 INFO 09:35:37,687 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-21-Data.db,].  1,074 to 799 (~74% of original) bytes for 4 keys at 0.007620MB/s.  Time: 100ms.
 INFO 09:35:37,688 Using TFastFramedTransport with a max frame size of 15728640 bytes.
 INFO 09:35:37,692 Using synchronous/threadpool thrift server on /0.0.0.0 : 9160
 INFO 09:35:37,695 Listening for thrift clients...
 INFO 09:35:37,706 Node /10.118.230.219 is now part of the cluster
 INFO 09:35:37,707 InetAddress /10.118.230.219 is now UP
 INFO 09:35:37,708 Enqueuing flush of Memtable-LocationInfo@2035487037(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,709 Writing Memtable-LocationInfo@2035487037(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,725 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-24-Data.db (89 bytes)
 INFO 09:35:37,726 Node /10.34.42.28 is now part of the cluster
 INFO 09:35:37,727 InetAddress /10.34.42.28 is now UP
 INFO 09:35:37,729 Enqueuing flush of Memtable-LocationInfo@321887181(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,729 Writing Memtable-LocationInfo@321887181(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,747 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-25-Data.db (89 bytes)
 INFO 09:35:37,748 Node /10.77.63.49 has restarted, now UP
 INFO 09:35:37,748 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-24-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-22-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-25-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-21-Data.db')]
 INFO 09:35:37,748 InetAddress /10.77.63.49 is now UP
 INFO 09:35:37,749 Node /10.77.63.49 state jump to normal
 INFO 09:35:37,750 Node /10.34.70.73 is now part of the cluster
 INFO 09:35:37,750 InetAddress /10.34.70.73 is now UP
 INFO 09:35:37,752 Enqueuing flush of Memtable-LocationInfo@1354749546(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,752 Writing Memtable-LocationInfo@1354749546(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,789 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-26-Data.db,].  1,140 to 877 (~76% of original) bytes for 4 keys at 0.020399MB/s.  Time: 41ms.
 INFO 09:35:37,801 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-27-Data.db (89 bytes)
 INFO 09:35:37,801 Node /10.99.86.251 is now part of the cluster
 INFO 09:35:37,802 InetAddress /10.99.86.251 is now UP
 INFO 09:35:37,803 Enqueuing flush of Memtable-LocationInfo@793374785(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,804 Writing Memtable-LocationInfo@793374785(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,825 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-29-Data.db (89 bytes)
 INFO 09:35:37,826 Node /10.202.67.43 has restarted, now UP
 INFO 09:35:37,827 InetAddress /10.202.67.43 is now UP
 INFO 09:35:37,827 Node /10.202.67.43 state jump to normal
 INFO 09:35:37,828 Node /10.116.134.183 is now part of the cluster
 INFO 09:35:37,828 InetAddress /10.116.134.183 is now UP
 INFO 09:35:37,829 Enqueuing flush of Memtable-LocationInfo@1728699027(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,830 Writing Memtable-LocationInfo@1728699027(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,850 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-30-Data.db (89 bytes)
 INFO 09:35:37,852 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-30-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-27-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-26-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-29-Data.db')]
 INFO 09:35:37,853 Node /10.118.94.62 is now part of the cluster
 INFO 09:35:37,853 InetAddress /10.118.94.62 is now UP
 INFO 09:35:37,855 Enqueuing flush of Memtable-LocationInfo@2001229122(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,855 Writing Memtable-LocationInfo@2001229122(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,885 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-31-Data.db (89 bytes)
 INFO 09:35:37,886 Node /10.116.215.81 is now part of the cluster
 INFO 09:35:37,887 InetAddress /10.116.215.81 is now UP
 INFO 09:35:37,888 Enqueuing flush of Memtable-LocationInfo@1748800276(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,888 Writing Memtable-LocationInfo@1748800276(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,909 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-33-Data.db (89 bytes)
 INFO 09:35:37,910 Node /10.80.110.28 has restarted, now UP
 INFO 09:35:37,911 InetAddress /10.80.110.28 is now UP
 INFO 09:35:37,911 Node /10.80.110.28 state jump to normal
 INFO 09:35:37,912 Node /10.80.210.38 is now part of the cluster
 INFO 09:35:37,912 InetAddress /10.80.210.38 is now UP
 INFO 09:35:37,914 Enqueuing flush of Memtable-LocationInfo@1761382005(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,914 Writing Memtable-LocationInfo@1761382005(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,925 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-32-Data.db,].  1,144 to 982 (~85% of original) bytes for 4 keys at 0.014190MB/s.  Time: 66ms.
 INFO 09:35:37,927 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-35-Data.db (89 bytes)
 INFO 09:35:37,928 Node /10.40.177.173 has restarted, now UP
 INFO 09:35:37,929 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-31-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-32-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-33-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-35-Data.db')]
 INFO 09:35:37,929 InetAddress /10.40.177.173 is now UP
 INFO 09:35:37,929 Node /10.40.177.173 state jump to normal
 INFO 09:35:37,930 Node /10.101.41.8 has restarted, now UP
 INFO 09:35:37,931 InetAddress /10.101.41.8 is now UP
 INFO 09:35:37,931 Node /10.101.41.8 state jump to normal
 INFO 09:35:37,931 Node /10.205.23.34 has restarted, now UP
 INFO 09:35:37,932 InetAddress /10.205.23.34 is now UP
 INFO 09:35:37,932 Node /10.205.23.34 state jump to normal
 INFO 09:35:37,933 Node /10.118.185.68 is now part of the cluster
 INFO 09:35:37,933 InetAddress /10.118.185.68 is now UP
 INFO 09:35:37,934 Enqueuing flush of Memtable-LocationInfo@260440278(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,935 Writing Memtable-LocationInfo@260440278(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,970 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-36-Data.db (89 bytes)
 INFO 09:35:37,971 Node /10.116.241.250 is now part of the cluster
 INFO 09:35:37,972 InetAddress /10.116.241.250 is now UP
 INFO 09:35:37,973 Enqueuing flush of Memtable-LocationInfo@768673839(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,974 Writing Memtable-LocationInfo@768673839(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,003 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-38-Data.db (89 bytes)
 INFO 09:35:38,004 Node /10.113.42.21 is now part of the cluster
 INFO 09:35:38,005 InetAddress /10.113.42.21 is now UP
 INFO 09:35:38,007 Enqueuing flush of Memtable-LocationInfo@1610335061(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,008 Writing Memtable-LocationInfo@1610335061(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,014 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-37-Data.db,].  1,249 to 1,087 (~87% of original) bytes for 4 keys at 0.012196MB/s.  Time: 85ms.
 INFO 09:35:38,024 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-40-Data.db (89 bytes)
 INFO 09:35:38,024 Node /10.194.29.156 is now part of the cluster
 INFO 09:35:38,025 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-37-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-40-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-36-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-38-Data.db')]
 INFO 09:35:38,025 InetAddress /10.194.29.156 is now UP
 INFO 09:35:38,026 Enqueuing flush of Memtable-LocationInfo@1625488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,027 Writing Memtable-LocationInfo@1625488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,042 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-41-Data.db (89 bytes)
 INFO 09:35:38,043 Node /10.85.11.38 has restarted, now UP
 INFO 09:35:38,044 InetAddress /10.85.11.38 is now UP
 INFO 09:35:38,044 Node /10.85.11.38 state jump to normal
 INFO 09:35:38,045 Node /10.34.159.72 is now part of the cluster
 INFO 09:35:38,046 InetAddress /10.34.159.72 is now UP
 INFO 09:35:38,047 Enqueuing flush of Memtable-LocationInfo@747881713(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,048 Writing Memtable-LocationInfo@747881713(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,065 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-42-Data.db (89 bytes)
 INFO 09:35:38,067 Node /10.194.22.191 is now part of the cluster
 INFO 09:35:38,067 InetAddress /10.194.22.191 is now UP
 INFO 09:35:38,069 Enqueuing flush of Memtable-LocationInfo@709926392(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,069 Writing Memtable-LocationInfo@709926392(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,092 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-44-Data.db (89 bytes)
 INFO 09:35:38,093 Node /10.34.74.58 is now part of the cluster
 INFO 09:35:38,097 InetAddress /10.34.74.58 is now UP
 INFO 09:35:38,098 Enqueuing flush of Memtable-LocationInfo@1356841826(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,099 Writing Memtable-LocationInfo@1356841826(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,105 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-43-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-41-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-44-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-42-Data.db')]
 INFO 09:35:38,106 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-43-Data.db,].  1,354 to 1,192 (~88% of original) bytes for 4 keys at 0.014034MB/s.  Time: 81ms.
 INFO 09:35:38,144 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-46-Data.db (89 bytes)
 INFO 09:35:38,145 Node /10.40.22.224 is now part of the cluster
 INFO 09:35:38,146 InetAddress /10.40.22.224 is now UP
 INFO 09:35:38,147 Enqueuing flush of Memtable-LocationInfo@422797318(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,148 Writing Memtable-LocationInfo@422797318(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,155 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-47-Data.db,].  1,459 to 1,297 (~88% of original) bytes for 4 keys at 0.024738MB/s.  Time: 50ms.
 INFO 09:35:38,164 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-49-Data.db (89 bytes)
 INFO 09:35:38,165 Node /10.32.79.134 is now part of the cluster
 INFO 09:35:38,166 InetAddress /10.32.79.134 is now UP
 INFO 09:35:38,167 Enqueuing flush of Memtable-LocationInfo@1455093129(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,168 Writing Memtable-LocationInfo@1455093129(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,199 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-50-Data.db (89 bytes)
 INFO 09:35:38,200 Node /10.118.179.67 is now part of the cluster
 INFO 09:35:38,200 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-50-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-47-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-49-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-46-Data.db')]
 INFO 09:35:38,200 InetAddress /10.118.179.67 is now UP
 INFO 09:35:38,202 Enqueuing flush of Memtable-LocationInfo@1105436908(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,202 Writing Memtable-LocationInfo@1105436908(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,248 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-51-Data.db (89 bytes)
 INFO 09:35:38,249 Node /10.84.205.93 is now part of the cluster
 INFO 09:35:38,249 InetAddress /10.84.205.93 is now UP
 INFO 09:35:38,251 Enqueuing flush of Memtable-LocationInfo@1306980591(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,251 Writing Memtable-LocationInfo@1306980591(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,262 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-52-Data.db,].  1,564 to 1,402 (~89% of original) bytes for 4 keys at 0.021919MB/s.  Time: 61ms.
 INFO 09:35:38,294 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-54-Data.db (89 bytes)
 INFO 09:35:38,294 Node /10.34.33.16 has restarted, now UP
 INFO 09:35:38,295 InetAddress /10.34.33.16 is now UP
 INFO 09:35:38,296 Node /10.34.33.16 state jump to normal
 INFO 09:35:38,296 Node /10.39.107.114 is now part of the cluster
 INFO 09:35:38,297 InetAddress /10.39.107.114 is now UP
 INFO 09:35:38,298 Enqueuing flush of Memtable-LocationInfo@1038389338(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,299 Writing Memtable-LocationInfo@1038389338(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,311 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-55-Data.db (89 bytes)
 INFO 09:35:38,312 Node /10.196.79.240 is now part of the cluster
 INFO 09:35:38,312 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-52-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-55-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-54-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-51-Data.db')]
 INFO 09:35:38,313 InetAddress /10.196.79.240 is now UP
 INFO 09:35:38,314 Enqueuing flush of Memtable-LocationInfo@1850278722(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,315 Writing Memtable-LocationInfo@1850278722(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,354 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-56-Data.db (89 bytes)
 INFO 09:35:38,355 Node /10.99.39.242 has restarted, now UP
 INFO 09:35:38,356 InetAddress /10.99.39.242 is now UP
 INFO 09:35:38,356 Node /10.99.39.242 state jump to normal
 INFO 09:35:38,357 Node /10.118.233.198 has restarted, now UP
 INFO 09:35:38,358 InetAddress /10.118.233.198 is now UP
 INFO 09:35:38,358 Node /10.118.233.198 state jump to normal
 INFO 09:35:38,359 Node /10.82.210.172 is now part of the cluster
 INFO 09:35:38,359 InetAddress /10.82.210.172 is now UP
 INFO 09:35:38,364 Enqueuing flush of Memtable-LocationInfo@786665924(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,364 Writing Memtable-LocationInfo@786665924(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,439 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-58-Data.db (89 bytes)
 INFO 09:35:38,440 Node /10.80.41.192 is now part of the cluster
 INFO 09:35:38,440 InetAddress /10.80.41.192 is now UP
 INFO 09:35:38,442 Enqueuing flush of Memtable-LocationInfo@1647844754(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,442 Writing Memtable-LocationInfo@1647844754(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,451 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-57-Data.db,].  1,669 to 1,515 (~90% of original) bytes for 4 keys at 0.010470MB/s.  Time: 138ms.
 INFO 09:35:38,459 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-60-Data.db (89 bytes)
 INFO 09:35:38,459 Node /10.76.243.129 is now part of the cluster
 INFO 09:35:38,460 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-56-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-58-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-57-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-60-Data.db')]
 INFO 09:35:38,460 InetAddress /10.76.243.129 is now UP
 INFO 09:35:38,462 Enqueuing flush of Memtable-LocationInfo@585652261(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,462 Writing Memtable-LocationInfo@585652261(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,478 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-61-Data.db (89 bytes)
 INFO 09:35:38,486 Node /10.34.42.28 state jump to normal
 INFO 09:35:38,487 Node /10.34.70.73 state jump to normal
 INFO 09:35:38,488 Node /10.99.86.251 state jump to normal
 INFO 09:35:38,489 Node /10.118.94.62 state jump to normal
 INFO 09:35:38,489 Node /10.80.110.28 state jump to normal
 INFO 09:35:38,490 Node /10.80.210.38 state jump to normal
 INFO 09:35:38,491 Node /10.40.177.173 state jump to normal
 INFO 09:35:38,493 Node /10.101.41.8 state jump to normal
 INFO 09:35:38,493 Node /10.113.42.21 state jump to normal
 INFO 09:35:38,494 Node /10.85.11.38 state jump to normal
 INFO 09:35:38,495 Node /10.34.159.72 state jump to normal
 INFO 09:35:38,496 Node /10.34.74.58 state jump to normal
 INFO 09:35:38,497 Node /10.84.205.93 state jump to normal
 INFO 09:35:38,497 Node /10.118.179.67 state jump to normal
 INFO 09:35:38,498 Node /10.34.33.16 state jump to normal
 INFO 09:35:38,499 Node /10.196.79.240 state jump to normal
 INFO 09:35:38,500 Node /10.118.233.198 state jump to normal
 INFO 09:35:38,501 Node /10.80.41.192 state jump to normal
 INFO 09:35:38,502 Node /10.76.243.129 state jump to normal
 INFO 09:35:38,508 Node /10.118.185.68 state jump to normal
 INFO 09:35:38,524 Node /10.118.230.219 state jump to normal
 INFO 09:35:38,536 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-62-Data.db,].  1,782 to 1,620 (~90% of original) bytes for 4 keys at 0.020328MB/s.  Time: 76ms.
 INFO 09:35:38,537 Node /10.80.110.28 state jump to normal
 INFO 09:35:38,537 Node /10.40.177.173 state jump to normal
 INFO 09:35:38,538 Node /10.101.41.8 state jump to normal
 INFO 09:35:38,539 Node /10.116.241.250 state jump to normal
 INFO 09:35:38,540 Node /10.194.29.156 state jump to normal
 INFO 09:35:38,540 Node /10.34.74.58 state jump to normal
 INFO 09:35:38,541 Node /10.40.22.224 state jump to normal
 INFO 09:35:38,542 Node /10.32.79.134 state jump to normal
 INFO 09:35:38,543 Node /10.39.107.114 state jump to normal
 INFO 09:35:38,543 Node /10.99.39.242 state jump to normal
 INFO 09:35:38,550 Node /10.77.63.49 state jump to normal
 INFO 09:35:38,550 Node /10.34.42.28 state jump to normal
 INFO 09:35:38,551 Node /10.116.134.183 state jump to normal
 INFO 09:35:38,553 Node /10.76.243.129 state jump to normal
 INFO 09:35:38,557 Node /10.202.67.43 state jump to normal
 INFO 09:35:38,558 Node /10.118.94.62 state jump to normal
 INFO 09:35:38,562 Node /10.116.215.81 state jump to normal
 INFO 09:35:38,563 Node /10.80.210.38 state jump to normal
 INFO 09:35:38,564 Node /10.205.23.34 state jump to normal
 INFO 09:35:38,565 Node /10.39.107.114 state jump to normal
{CODE}",,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3892,CASSANDRA-3857,,,,,,,"27/Jan/12 20:44;jbellis;3417-2.txt;https://issues.apache.org/jira/secure/attachment/12512227/3417-2.txt","28/Jan/12 02:11;scode;3417-3.txt;https://issues.apache.org/jira/secure/attachment/12512273/3417-3.txt","29/Oct/11 02:28;jbellis;3417.txt;https://issues.apache.org/jira/secure/attachment/12501422/3417.txt","13/Feb/12 18:27;scode;CASSANDRA-3417-tokenmap-1.0-v1.txt;https://issues.apache.org/jira/secure/attachment/12514381/CASSANDRA-3417-tokenmap-1.0-v1.txt","12/Feb/12 00:31;scode;CASSANDRA-3417-tokenmap-v2.txt;https://issues.apache.org/jira/secure/attachment/12514252/CASSANDRA-3417-tokenmap-v2.txt","13/Feb/12 07:20;scode;CASSANDRA-3417-tokenmap-v3.txt;https://issues.apache.org/jira/secure/attachment/12514326/CASSANDRA-3417-tokenmap-v3.txt","02/Feb/12 01:09;scode;CASSANDRA-3417-tokenmap.txt;https://issues.apache.org/jira/secure/attachment/12512891/CASSANDRA-3417-tokenmap.txt",,,,,,,,7.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215154,,,Mon Feb 13 21:45:16 UTC 2012,,,,,,,,,,"0|i0gjn3:",94612,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"28/Oct/11 19:38;jbellis;1.0 branch has moved so FTR here is line 1127 in 1.0.0 StorageService.java:

{code}
.       // At this stage pendingRanges has been updated according to leave operations. We can
        // now continue the calculation by checking bootstrapping nodes.

        // For each of the bootstrapping nodes, simply add and remove them one by one to
        // allLeftMetadata and check in between what their ranges would be.
        for (Map.Entry<Token, InetAddress> entry : bootstrapTokens.entrySet())
{code};;;","28/Oct/11 19:45;jbellis;Doesn't look like there is an easy way to make Guava BiMap threadsafe. :(

(This has been broken for forever but you're seeing it now because autobootstrap is the default in 1.0: CASSANDRA-2447);;;","28/Oct/11 19:54;j.casares;Okay, in that case I can drop autobootstrap to false the way it used to be.

Fair workaround?;;;","28/Oct/11 20:46;jbellis;only if you don't care about adding nodes to the cluster;;;","28/Oct/11 21:35;brandon.williams;bq. Doesn't look like there is an easy way to make Guava BiMap threadsafe. 

synchronizedBiMap should be threadsafe and performant enough here I think.;;;","28/Oct/11 21:45;jbellis;Where's that located?  I can't find it.;;;","29/Oct/11 01:01;brandon.williams;In collect.Synchronized... which it turns out isn't public :(;;;","29/Oct/11 02:28;jbellis;o hai Maps.synchronizedBiMap;;;","29/Oct/11 21:20;brandon.williams;bq. o hai Maps.synchronizedBiMap

I'm a useful idiot.  +1;;;","31/Oct/11 16:11;jbellis;committed;;;","27/Jan/12 19:26;scode;How is this a correct fix?

http://google-collections.googlecode.com/svn/trunk/javadoc/com/google/common/collect/Maps.html#synchronizedBiMap(com.google.common.collect.BiMap

In order for concurrent iteration to be safe it's not enough to make individual atomic operations thread-safe. I didn't dig deep but the docs claims HashBiMap is just backed by two hash maps, and also specifically there is mention of needing to synchronize iterations. Which makes sense; imagine trying to implement locking across an iteration and the issues that involves.

Did anyone see the problem actually go away with this patch under circumstances where it was previously reliably re-producible?
;;;","27/Jan/12 19:56;jbellis;You're right, it looks like you need to manually synchronize for iteration.;;;","27/Jan/12 20:44;jbellis;patch to add synchronization blocks around bootstraptokens iteration;;;","28/Jan/12 02:11;scode;Attaching v3 which also adds synchronization in TokenMap.getTokenToEndpointMap() - both for bootstrapTokens and tokenToEndpointMap.

(x.putAll(y) is not an atomic observation from the perspective of y, even if it is from the perspective of x);;;","28/Jan/12 04:28;jbellis;commmitted v3, thanks!;;;","01/Feb/12 21:33;scode;Still happening with this fixed. Can't give details now because I'm noticing this while testing something else (JIRA:s coming for those). Thus is bootstrapping a ~ 180 cluster w/ trunk, with all nodes joining at the same time.

{code}
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
        at java.util.HashMap$EntryIterator.next(HashMap.java:834)
        at java.util.HashMap$EntryIterator.next(HashMap.java:832)
        at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:301)
        at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:293)
        at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalEndpoints(NetworkTopologyStrategy.java:91)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getAddressRanges(AbstractReplicationStrategy.java:154)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getAddressRanges(AbstractReplicationStrategy.java:181)
        at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1291)
        at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1272)
        at org.apache.cassandra.service.StorageService.onRemove(StorageService.java:1537)
        at org.apache.cassandra.gms.Gossiper.removeEndpoint(Gossiper.java:312)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:629)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}
;;;","02/Feb/12 00:19;scode;This is no longer the bootstrap tokens, but the token meta data being iterated over. I may be blind atm, but looks like it's the AbstractReplicationStrategy's tokenMetadata member.;;;","02/Feb/12 00:26;scode;Right, which is the same as StorageService.instance.tokenMetadata_ ({{AbstractReplicationStrategy.createReplicationStrategy()}} which gets modified in the gossip stage - and the remove endpoint happening in the periodic gossip task is thus un-safe.

This is the FatClient removal path. We've seen this before, but it's not immediately clear to me why a fresh cluster believes there's a fat client to kick out.;;;","02/Feb/12 00:33;scode;TokenMetadata definitely has read locks in other places around the token meta data, but entrySet() is just returning it flatly.;;;","02/Feb/12 00:33;scode;{{NTS.calculateNaturalEndpoint()}} is the only caller.;;;","02/Feb/12 00:36;scode;Looks (without careful verification) that it was introduced in 406f4b3c6eccddfb65aadfb2cfe423cfe8cef062 for CASSANDRA-1103.
;;;","02/Feb/12 01:09;scode;{{CASSANDRA\-3417\-tokenmap.txt}} attached.;;;","11/Feb/12 00:11;jbellis;Hmm, is switching from raw endpoints to endpoints-with-bootstrap-candidates fixing a bug, or introducing one?;;;","11/Feb/12 00:18;jbellis;I think it's introducing one, because reads assume that getNaturalEndpoints returns nodes that have the data, and SS.getWriteEndpoints adds in the pending ranges.;;;","11/Feb/12 08:37;scode;Hmm, what are you referring to? I certainly agree that if there is code that changes the endpoints used for reads to include bootstrap candidates, that's wrong. I'm not sure what part of this ticket or which other ticket you are referring to however.;;;","11/Feb/12 15:45;jbellis;Here's the core of getTokenToEndpointMap:

{code}
.           Map<Token, InetAddress> map = new HashMap<Token, InetAddress>(tokenToEndpointMap.size() + bootstrapTokens.size());
            map.putAll(tokenToEndpointMap);
            synchronized (bootstrapTokens)
            {
                map.putAll(bootstrapTokens);
            }
            return map;
{code};;;","11/Feb/12 20:21;scode;Ok. I can't believe getTokenToEndpointMap() returns something different than tokenToEndPointMap. I completely missed that. I will submit an updated patch which also fixes naming to make this much more obvious.;;;","12/Feb/12 00:31;scode;Attaching {{v2}}. I decided I would submit a minimal patch to just fix this one problem. I will shortly open a separate ticket for further work on TokenMap.

I realized I have to carefully audit every use-case of TokenMap to detect any issues of a similar type (the first thing I looked at showed a problem - the duplicate token check during bootstrap is not considering all tokens, but only normal+bootstrapping).

I apologize for not being more thorough in my original patch submission and confirming what the method was actually doing. I remember looking closely at the method name and vaguely noticing the bootstrap tokens but it never sank in.;;;","12/Feb/12 00:34;scode;Filed CASSANDRA-3892 for follow-up.
;;;","13/Feb/12 07:20;scode;{{v3}} slightly adjusted to not use joiningEndpoints.size() when constructing the copy w/o joining endpoints.
;;;","13/Feb/12 15:34;jbellis;I'm getting 3 of 5 hunks failing to apply to 1.0, did you switch branches?;;;","13/Feb/12 18:27;scode;Attaching {{CASSANDRA\-3417\-tokenmap\-1.0\-v1.txt}} which is for 1.0.

Apologies for the confusion; I only ever triggered and tested this on 1.1/trunk since that's what I was testing, despite this bug originally being against 1.0.

I haven't done real testing with this patch for 1.0. Right now I can't use the cluster I was testing with to easily go to 1.0 to test either. But, the fix seems correct to me regardless of branch given that the iteration is clearly over a map that is getting modified. The biggest risk is a typo or similar mistake which is more easily spotted by review anyway.
;;;","13/Feb/12 21:45;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
show schema fails,CASSANDRA-3415,12529245,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,hsn,hsn,28/Oct/11 13:13,16/Apr/19 09:32,14/Jul/23 05:52,20/Dec/11 10:58,0.8.10,1.0.7,,Legacy/Tools,,,0,,,,"following command breaks ""show schema"" cli command with error ""A long is exactly 8 bytes: 5""

create column family resultcache with column_type = 'Super' and comparator = 'LongType' and  key_validation_class = 'UTF8Type' and subcomparator = 'AsciiType' and replicate_on_write = false and rows_cached = 700 and keys_cached = 30000 and key_cache_save_period = 0 and column_metadata = [ {column_name: id, validation_class: LongType}, {column_name: name, validation_class: 'AsciiType'}, {column_name: crc32, validation_class: LongType}, {column_name: size, validation_class: LongType} ];",freebsd,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215105,,,Tue Dec 20 09:38:58 UTC 2011,,,,,,,,,,"0|i0gjm7:",94608,,,,,Low,,,,,,,,,,,,,,,,,"28/Oct/11 13:16;hsn;1.0.0 has same bug;;;","28/Oct/11 14:28;jbellis;please test 1.0.1 artifacts: http://people.apache.org/~slebresne/;;;","28/Oct/11 16:53;hsn;bug in 1.0.1 too;;;","03/Nov/11 06:26;jbellis;fixed in r1196956.  Thanks for the reproducible test case!;;;","03/Nov/11 06:26;jbellis;(note that the problem is only with the cli, the schema itself is fine.);;;","03/Nov/11 09:04;hudson;Integrated in Cassandra-0.8 #392 (See [https://builds.apache.org/job/Cassandra-0.8/392/])
    fix displaying cfdef entries for super columnfamilies
patch by jbellis for CASSANDRA-3415

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1196956
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
;;;","20/Dec/11 09:38;hsn;This is fixed in 0.8 and 1.0 branches. Close ticket;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not possible to change row_cache_provider on existing cf,CASSANDRA-3414,12529198,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,marcuse,marcuse,28/Oct/11 05:53,16/Apr/19 09:32,14/Jul/23 05:52,28/Oct/11 12:48,0.8.8,,,,,,0,,,,"row_cache_provider is not possible to change using update column family xyz with row_cache_provider='something' in 0.8

It does work in 1.0.0

Reason is that the field is not added to the avro record, patch attached fixes that",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/11 05:55;marcuse;3414.patch;https://issues.apache.org/jira/secure/attachment/12501241/3414.patch",,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,215058,,,Fri Oct 28 15:23:40 UTC 2011,,,,,,,,,,"0|i0gjlr:",94606,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"28/Oct/11 05:55;marcuse;add the row_cache_provider to the avro record;;;","28/Oct/11 12:48;jbellis;committed, thanks!;;;","28/Oct/11 15:23;hudson;Integrated in Cassandra-0.8 #389 (See [https://builds.apache.org/job/Cassandra-0.8/389/])
    fix updating CF row_cache_provider
patch by Marcus Eriksson; reviewed by jbellis for CASSANDRA-3414

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1190282
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inconsistent rejection of CL.ANY on reads,CASSANDRA-3410,12529076,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,27/Oct/11 16:32,16/Apr/19 09:32,14/Jul/23 05:52,07/Nov/11 17:31,1.1.0,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/11 16:32;jbellis;3410.txt;https://issues.apache.org/jira/secure/attachment/12501113/3410.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,214936,,,Mon Nov 07 18:24:30 UTC 2011,,,,,,,,,,"0|i0gjjz:",94598,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"07/Nov/11 16:55;slebresne;+1;;;","07/Nov/11 17:31;jbellis;committed;;;","07/Nov/11 18:24;hudson;Integrated in Cassandra #1191 (See [https://builds.apache.org/job/Cassandra/1191/])
    reject CL.ANY range scans as well as single/multi-row gets
patch by jbellis; reviewed by slebresne for CASSANDRA-3410

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1198828
Files : 
* /cassandra/trunk/NEWS.txt
* /cassandra/trunk/src/java/org/apache/cassandra/service/ReadCallback.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/CassandraServer.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/ThriftValidation.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFS reloading of the compaction strategy is done for every metadata update and is not thread safe,CASSANDRA-3409,12529068,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,27/Oct/11 15:50,16/Apr/19 09:32,14/Jul/23 05:52,27/Oct/11 16:21,1.0.1,,,,,,0,,,,"The reloading of the compaction strategy done during CFS.reload is not thread safe. In particular, this is a problem for leveled compactions. It could leads to some sstable not being added to the manifest and also breaks the 'only one leveledCompactionTask can run at any given time' assumption (which, at least without CASSANDRA-3408 can likely leads to blocking compactions completely).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/11 15:59;slebresne;3409.patch;https://issues.apache.org/jira/secure/attachment/12501107/3409.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,214928,,,Thu Oct 27 16:21:17 UTC 2011,,,,,,,,,,"0|i0gjjj:",94596,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"27/Oct/11 15:59;slebresne;There really is two problems:
* we reload the strategy every time we reload the CFS, i.e, for each upade_column_family, even if it's just changing gc_grace or something. This makes that bug much more problematic.
* the only easy way I see make the reload of stategy safe is to grab the compaction lock.

Patch attached that fix both (fixing the first problem makes grabing the lock not a big deal imho).;;;","27/Oct/11 16:05;jbellis;+1;;;","27/Oct/11 16:21;slebresne;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeveledCompactionTask is too fragile and can block compactions,CASSANDRA-3408,12529064,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,27/Oct/11 15:14,16/Apr/19 09:32,14/Jul/23 05:52,27/Oct/11 16:21,1.0.1,,,,,,0,,,,"If any error happens during a LeveledCompactionTask, it will just block every compaction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/11 15:21;slebresne;3408.patch;https://issues.apache.org/jira/secure/attachment/12501104/3408.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,214924,,,Thu Oct 27 16:21:32 UTC 2011,,,,,,,,,,"0|i0gjiv:",94593,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"27/Oct/11 15:58;jbellis;+1;;;","27/Oct/11 16:21;slebresne;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing partitioner causes interval tree build failure before the change can be detected,CASSANDRA-3407,12529058,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,apache.zli,apache.zli,27/Oct/11 14:33,16/Apr/19 09:32,14/Jul/23 05:52,16/Nov/11 22:09,1.0.4,,,,,,0,,,,"After installed 1.0.0 and changed config file cassandra.yaml, restart cassandra and got exception,

INFO 22:25:37,727 Opening /srv/opt/cassandra8/data/system/IndexInfo-g-121 (5428 bytes)
ERROR 22:25:37,753 Exception encountered during startup_type: 0},
java.lang.StackOverflowError, validation_class: UTF8Type, index_type: 0},
       at java.math.BigInteger.compareMagnitude(BigInteger.java:2477)
       at java.math.BigInteger.compareTo(BigInteger.java:2463)type: 0},
       at org.apache.cassandra.dht.BigIntegerToken.compareTo(BigIntegerToken.java:39)
       at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:83)
       at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:38)
       at java.util.Arrays.mergeSort(Arrays.java:1144)dex_type: 0},
       at java.util.Arrays.sort(Arrays.java:1079)dex_type: 0},
       at java.util.Collections.sort(Collections.java:117)},
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.findMinMedianMax(IntervalNode.java:102)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:43)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
.....

       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:38)
       at org.apache.cassandra.db.DataTracker$View.buildIntervalTree(DataTracker.java:522)
       at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:547)
       at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:268)
       at org.apache.cassandra.db.DataTracker.addSSTables(DataTracker.java:237)
       at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:216)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:315)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:285)
       at org.apache.cassandra.db.Table.initCf(Table.java:372)
       at org.apache.cassandra.db.Table.<init>(Table.java:320)
       at org.apache.cassandra.db.Table.open(Table.java:121)
       at org.apache.cassandra.db.Table.open(Table.java:104)
       at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:215)
       at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:150)
       at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
       at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Exception encountered during startup: null
",Linux  2.6.18,billa,marcuse,satishbabu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3470,,"28/Oct/11 21:46;thepaul;3407-assert-intervals.patch;https://issues.apache.org/jira/secure/attachment/12501376/3407-assert-intervals.patch","11/Nov/11 01:52;yukim;3407-v2.txt;https://issues.apache.org/jira/secure/attachment/12503322/3407-v2.txt","08/Nov/11 03:21;yukim;3407.txt;https://issues.apache.org/jira/secure/attachment/12502876/3407.txt","27/Oct/11 14:34;apache.zli;exception1.txt;https://issues.apache.org/jira/secure/attachment/12501096/exception1.txt","28/Oct/11 22:09;apache.zli;system.log;https://issues.apache.org/jira/secure/attachment/12501383/system.log","27/Oct/11 19:38;apache.zli;system.log;https://issues.apache.org/jira/secure/attachment/12501141/system.log","29/Oct/11 00:54;apache.zli;system.tar.gz;https://issues.apache.org/jira/secure/attachment/12501416/system.tar.gz",,,,,,,,7.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,214918,,,Wed Nov 16 22:09:01 UTC 2011,,,,,,,,,,"0|i0gjif:",94591,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Oct/11 14:34;apache.zli;Full logs;;;","27/Oct/11 15:00;jbellis;Superficially it looks like we might want to change that IntervalTree method to a loop instead of recursion;;;","27/Oct/11 16:16;slebresne;On my machine, I'm able to do 1156 recursive calls with -Xss128k (which we use by default) before getting a StackOverflowException. Is that expected that we create such huge intervalTree ?;;;","27/Oct/11 16:23;jbellis;Does our intervaltree self-balance or will it degenerate if nodes are added in the ""wrong"" order, e.g. sorted order in a standard binary tree?;;;","27/Oct/11 17:03;thepaul;They're immutable and recreated as necessary with new DataTracker.View object, so ongoing balancing isn't an issue. It looks like the worst case depth (with the right arrangement of intervals; insertion order doesn't matter) is equal to the number of SSTableReaders in a View.;;;","27/Oct/11 17:06;jbellis;Hmm.  We could have way more than 1156 SSTables in a view very easily.;;;","27/Oct/11 17:18;thepaul;To get that, you'd need all the SSTable intervals to be wholly contained by the next larger one, and none intersecting the center point of any larger intervals. It seems unlikely, but maybe there are patterns of usage which could have that effect.

Either way, it probably is worth changing both the tree creation and search methods to use iteration instead of recursion.;;;","27/Oct/11 17:23;jbellis;In the meantime, Zhong should be able to just use a larger stack as a workaround.;;;","27/Oct/11 17:38;slebresne;bq. It seems unlikely, but maybe there are patterns of usage which could have that effect

It sound to me like we don't understand what is going on.

Besides, if you look at the full log, the exception is triggered on the IndexInfo cf. It also happens at the very startup after he has loaded only 3 sstables (hence DataTracker likely have 3 sstables, not a shit load). I'm pretty sure it's a bug leading to infinite recursive calls, not a stack size problem.;;;","27/Oct/11 18:13;jbellis;Good point.;;;","27/Oct/11 18:24;jbellis;I added debug logging for the intervals in the tree to the 1.0 branch in r1189921; if Zhong can build with that and give us the log, that would help a lot.;;;","27/Oct/11 19:08;apache.zli;
I will try to build it and test it.

 		 	   		  
;;;","27/Oct/11 19:38;apache.zli;I turn on debug and this is full logs;;;","28/Oct/11 21:02;thepaul;So here's the culprit:

bq. Interval(DecoratedKey(145657158669597754039818762792796113368, 6b657973706163654c42534441544150524f445553), DecoratedKey(130897858884062407407634012854175581156, 6b65797370616365544553544441544150524f445553))

Notice that the min of that Interval (~1.45e38) is greater than the max (~1.30e38). The IntervalTree logic is wholly incapable of dealing with that, so we end up in an infinite loop. The only way I can find that this could happen is from an invalid SSTableReader object. I'll make a patch with some extra asserts; if you can run with that, Zhong, maybe we can at least see where that invalid data is coming from.;;;","28/Oct/11 21:46;thepaul;Zhong, please try building and running that same setup with this patch.;;;","28/Oct/11 22:09;apache.zli;patch applied and log attached.;;;","28/Oct/11 22:15;thepaul;Hmm, the patch doesn't seem to have taken effect. Are you running with java's -ea option to enable assertions?;;;","29/Oct/11 00:54;apache.zli;I checked -ea is on. I also tested with IndexInfo-g-121 files only and got same exception. It should be easier for you with those system files attached. ;;;jira-users","30/Oct/11 03:24;thepaul;I haven't been able to reproduce this still, even with the IndexInfo files. I guess let's figure out where things are going differently. You're using the ByteOrderedPartitioner, it looks like? And can you confirm, you're running a single node of Cassandra 1.0.0, with all its data/commitlog/saved_caches directories empty except for data/system/IndexInfo*.db, and you get this exception?;;;","31/Oct/11 15:38;apache.zli;Oh. Forgot to change the partitioner setting, it is default ""RandomPartitioner"". I should use OrderPreservingPartitioner.

After I changed to OrderPreservingPartitioner, everything works fine.

Sorry about this.
;;;","31/Oct/11 15:56;jbellis;Actually we do have a problem, since we're supposed to check the partitioner setting in system table and raise an error there.  But it looks like we run into this problem first...

If we only built the interval tree for LCS then that would take care of it for the size-tiered case.  Any better ideas?;;;","31/Oct/11 15:57;jbellis;What if we stored the partitioner in the metadata/statistics component so we don't depend on that not changing to be able to read the data files?;;;","31/Oct/11 16:02;jbellis;That approach would also make our partitioner-change detection more robust against manually copying data files into a cluster with a different partitioner.;;;","08/Nov/11 02:55;yukim;Patch attached for 1.0 branch.
It writes partitioner class name to sstable metadata and when opening sstable, checks recorded partitioner against node's.

Since opening sstable is done in parallel, C* just skips failed ones regardless of whether its system keyspace or not.

And note that I did not modify the descriptor version(""h"") here. If updating version is desirable, I'll update the patch.;;;","08/Nov/11 03:21;jbellis;bq. I did not modify the descriptor version(""h"") here

We probably should, but see CASSANDRA-3470.;;;","11/Nov/11 01:52;yukim;Rebased and modified original patch to use minor versioning of sstable metadata.;;;","16/Nov/11 22:09;jbellis;Committed w/ update to throw RuntimeException instead of using assert, and remove of the old SystemTable-based partitioner checking.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Row cache provider reported wrong in cassandra-cli,CASSANDRA-3405,12528834,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,marcuse,marcuse,26/Oct/11 08:15,16/Apr/19 09:32,14/Jul/23 05:52,30/Oct/11 18:56,0.8.8,,,,,,0,,,,"When doing ""show schema;"" in the CLI, the row_cache_provider is reported as ConcurrentLinkedHashCacheProvider while it really is SerializingCacheProvider

Same goes for ""describe keyspace"" (after CASSANDRA-3384) on the 0.8 branch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/11 08:18;marcuse;3405.patch;https://issues.apache.org/jira/secure/attachment/12500838/3405.patch",,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,214694,,,Mon Oct 31 22:09:30 UTC 2011,,,,,,,,,,"0|i0gjhj:",94587,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"26/Oct/11 08:18;marcuse;Trivial patch that adds the class name to the cfm thrift message;;;","30/Oct/11 18:56;xedin;Committed, thanks!;;;","30/Oct/11 21:06;hudson;Integrated in Cassandra-0.8 #390 (See [https://builds.apache.org/job/Cassandra-0.8/390/])
    CFMetaData.convertToThrift method to set RowCacheProvider
patch by Marcus Eriksson; reviewed by Pavel Yaskevich for CASSANDRA-3405

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1195218
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
;;;","31/Oct/11 22:09;jbellis;(Also looks to not be a problem in the 1.0 branch.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] fix logging contexts,CASSANDRA-3404,12528815,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,dbrosius@apache.org,dbrosius@apache.org,26/Oct/11 03:40,16/Apr/19 09:32,14/Jul/23 05:52,26/Oct/11 08:26,,,,,,,0,,,,"a couple of places the logging context doesn't match the class, probably due to copy/paste bug.
fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/11 03:40;dbrosius@apache.org;log_context.diff;https://issues.apache.org/jira/secure/attachment/12500813/log_context.diff",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,214675,,,Wed Oct 26 08:26:12 UTC 2011,,,,,,,,,,"0|i0gjh3:",94585,,,,,Low,,,,,,,,,,,,,,,,,"26/Oct/11 08:26;slebresne;Commmitted in r1189073, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe_ring topology information is wrong/incomplete,CASSANDRA-3403,12528813,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,patricioe,brandon.williams,brandon.williams,26/Oct/11 02:46,16/Apr/19 09:32,14/Jul/23 05:52,26/Oct/11 19:48,1.0.1,,,,,,0,,,,"In CASSANDRA-2882, topology information was added to describe_ring, however it asks the gossiper for the DC information, and the gossiper can only have this with a gossip-enabled snitch, which currently means the Ec2Snitch.  Instead, it should be asking the snitch for the DC for each endpoint.

Also, the port information should just be removed: whatever port the client has connected to in order to call describe_ring is the right port to use for all endpoints.",,patricioe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/11 17:42;patricioe;trunk-3403-v1.diff;https://issues.apache.org/jira/secure/attachment/12500918/trunk-3403-v1.diff",,,,,,,,,,,,,,1.0,patricioe,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,214673,,,Wed Oct 26 19:48:27 UTC 2011,,,,,,,,,,"0|i0gjgn:",94583,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"26/Oct/11 17:44;patricioe;Patch tested with SimpleSnitch and PropertyFileSnitch for one and two datacenter.

Also tested using HectorAutoDiscoveryService.;;;","26/Oct/11 19:48;brandon.williams;Commited, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException during nodetool repair,CASSANDRA-3400,12528521,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,scottfines,scottfines,24/Oct/11 16:41,16/Apr/19 09:32,14/Jul/23 05:52,25/Oct/11 15:52,0.8.8,1.0.1,,,,,0,,,,"When running a nodetool repair, the following exception can be thrown:


ERROR [AntiEntropySessions:12] 2011-10-24 11:17:52,154 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[AntiEntropySessions:12,5,RMI Runtime]
java.lang.RuntimeException: java.util.ConcurrentModificationException
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
at java.util.concurrent.FutureTask.run(FutureTask.java:138)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.ConcurrentModificationException
at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
at java.util.HashMap$KeyIterator.next(HashMap.java:828)
at org.apache.cassandra.service.AntiEntropyService$RepairSession$RepairJob.sendTreeRequests(AntiEntropyService.java:784)
at org.apache.cassandra.service.AntiEntropyService$RepairSession.runMayThrow(AntiEntropyService.java:680)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
... 6 more
",Suse Enterprise linux 11.4,scottfines,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/11 08:47;slebresne;3400.patch;https://issues.apache.org/jira/secure/attachment/12500632/3400.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,214381,,,Tue Oct 25 18:07:07 UTC 2011,,,,,,,,,,"0|i0gjfj:",94578,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"24/Oct/11 16:48;scottfines;A few notes as I discover them:

This error occurs every time the nodetool runs repair--repair effectively does not work on this node now.

;;;","25/Oct/11 08:47;slebresne;There is a race between when we send requests and when start handling their result, i.e if a response comes before all the requests have been sent. It's fairly unlikely to happen imho but given than one request is local, if there is no and barely any data to build the merkle tree from, I suppose it's possible the local response comes quickly enough.

Patch attached to resolve the issue. ;;;","25/Oct/11 14:05;jbellis;+1

nit: we never use thread.interrupt() so custom is to throw AssertionError for InterruptedException;;;","25/Oct/11 15:52;slebresne;Committed (with the AssertionError change);;;","25/Oct/11 18:07;hudson;Integrated in Cassandra-0.8 #388 (See [https://builds.apache.org/job/Cassandra-0.8/388/])
    Fix race in AntiEntropyService
patch by slebresne; reviewed by jbellis for CASSANDRA-3400

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1188740
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AntiEntropyService.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Truncate disregards running compactions when deleting sstables,CASSANDRA-3399,12528489,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,slebresne,slebresne,24/Oct/11 14:24,16/Apr/19 09:32,14/Jul/23 05:52,31/Oct/11 22:10,0.8.8,1.0.2,,,,,0,,,,"All truncation do is `cfs.markCompacted(truncatedSSTables)` without holding any lock or anything. Which have the effect of actually deleting sstables that may be compacting. More precisely there is three problems:
# It removes those compacting sstables from the current set of active sstables for the cfs. But when they are done compacting, DataTracker.replaceCompactedSSTables() will be called and it assumes that the compacted sstable are parts of the current set of active sstables. In other words, we'll get an exception looking like the one of CASSANDRA-3306.
# The result of the compaction will be added as a new active sstable (actually no, because the code will throw an exception before because of the preceding point, but that's something we should probably deal with).
# Currently, compaction don't 'acquire references' on SSTR. That's because the code assumes we won't compact twice the same sstable and that compaction is the only mean to delete an sstable. With these two assumption, acquiring references is not necessary, but truncate break that first assumption.

As for solution, I see two possibilities:
# make the compaction lock be per-cf instead of global (which I think is easy and a good idea anyway) and grab the write lock to do the markCompacted call. The big downside is that truncation will potentially take much longer.
# had two phases: mark the sstable that are not compacting as compacted and set the dataTracker as 'truncated at', and let it deal with the other sstable when their compaction is done. A bit like what is proposed for CASSANDRA-3116 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/11 16:48;jbellis;3399.txt;https://issues.apache.org/jira/secure/attachment/12501633/3399.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,214349,,,Tue Nov 01 00:24:00 UTC 2011,,,,,,,,,,"0|i0gjf3:",94576,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"26/Oct/11 17:42;jbellis;I think we should:

- grab the Big Lock for this ticket
- create another ticket to take a 3116 approach in 1.1
- create another ticket to break the Big Lock apart to a per-CF lock in 1.0.2;;;","31/Oct/11 16:32;jbellis;created CASSANDRA-3429 and CASSANDRA-3430.;;;","31/Oct/11 16:41;jbellis;big lock patch attached.;;;","31/Oct/11 16:42;jbellis;(patch is against 1.0, will also commit to 0.8);;;","31/Oct/11 16:48;jbellis;new patch w/ less crack smoking;;;","31/Oct/11 16:52;slebresne;+1;;;","31/Oct/11 22:10;jbellis;committed;;;","01/Nov/11 00:24;hudson;Integrated in Cassandra-0.8 #391 (See [https://builds.apache.org/job/Cassandra-0.8/391/])
    acquire compactionlock during truncate
patch by jbellis; reviewed by slebresne for CASSANDRA-3399

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1195695
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problem markers don't show up in Eclipse,CASSANDRA-3397,12528394,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dallsopp,dallsopp,dallsopp,22/Oct/11 21:25,16/Apr/19 09:32,14/Jul/23 05:52,23/Dec/11 22:12,1.0.7,,,Packaging,,,0,ant,eclipse,ide,"The generated Eclipse files install an Ant Builder to build Cassandra within Eclipse. This appears to mean that the default Java Builder is not present. This means that no problem markers show up in the Problem view or the Package Explorer etc when there are compiler errors or warnings  - you have to study the console output, then navigate manually to the sources of the problems, which is very tedious.

It seems to be possible to re-install the default Java Builder in parallel with the Ant Builder, getting the best of both worlds. I have documented this on the wiki at http://wiki.apache.org/cassandra/RunningCassandraInEclipse

I was wondering a) whether this can be done automatically by the generate-eclipse-files Ant target, and b) whether using both Builders will be problem if one is working on any of the generated code (Thrift, CQL etc). The Java Builder can be temporarily disabled if so by unticking it under Properties->Builders...

See also https://issues.apache.org/jira/browse/CASSANDRA-2854",Eclipse,dallsopp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/11 09:39;dallsopp;Cassandra-3397.patch;https://issues.apache.org/jira/secure/attachment/12502740/Cassandra-3397.patch",,,,,,,,,,,,,,1.0,dallsopp,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,214254,,,Fri Dec 23 22:12:12 UTC 2011,,,,,,,,,,"0|i0gjdz:",94571,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"23/Oct/11 13:45;dallsopp;Looking at build.xml (generate-eclipse-files target), the .project file is generated from a verbatim listing, so adding a section for the Java Builder is trivial if that's the way we want to go...;;;","23/Oct/11 17:27;urandom;This has annoyed me as well, so if there are no landmines associated with enabling both builders, I'd be in favor of it.;;;","07/Nov/11 09:39;dallsopp;Small patch attached to add the Java Builder into the .project file when the generate-eclipse-files Ant target is run.;;;","07/Nov/11 09:39;dallsopp;See attached Cassandra-3397.patch;;;","22/Dec/11 22:12;urandom;I wasn't aware of this issue when I submitted (and later committed) CASSANDRA-3632.  Have you had the chance to try this again since?

CASSANDRA-3632 restored the default Java builder (so problem marks do show now), but I left the Ant Builder out entirely.  I personally found it too heavy to be running, for example, on every file save (I mash CTRL+S compulsively).  Do you find the Ant Builder useful or were you primarily interested in restoring the error markers?;;;","23/Dec/11 20:56;dallsopp;I was just interested in the error markers, thanks - I too found the Ant Builder too heavy!;;;","23/Dec/11 22:12;urandom;Great!  Closing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Quorum returns incorrect results during hinted handoff,CASSANDRA-3395,12528316,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,21/Oct/11 20:03,16/Apr/19 09:32,14/Jul/23 05:52,04/Nov/11 08:38,1.0.2,,,,,,0,,,,"In a 3 node cluster with RF=3 and using a single coordinator, if monotonically increasing columns are inserted into a row and the latest one sliced (both at QUORUM) during HH replay occasionally this column will not be seen.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/11 20:05;brandon.williams;logs.tar.bz2;https://issues.apache.org/jira/secure/attachment/12500235/logs.tar.bz2","01/Nov/11 16:21;brandon.williams;ttest.py;https://issues.apache.org/jira/secure/attachment/12501791/ttest.py","21/Oct/11 20:05;brandon.williams;ttestraw.py;https://issues.apache.org/jira/secure/attachment/12500236/ttestraw.py",,,,,,,,,,,,3.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,112709,,,Fri Nov 04 08:38:53 UTC 2011,,,,,,,,,,"0|i0gjd3:",94567,,,,,Normal,,,,,,,,,,,,,,,,,"21/Oct/11 20:05;brandon.williams;Logs from a failure where 20172 was expected but 20171 was returned and the pycassa script to reproduce.  Procedure is start the script and kill a node, wait, bring it up back up and wait for HH.  Takes a few tries.;;;","27/Oct/11 21:38;jbellis;This sure looks like a case of ""if timestamps are equal, all bets are off"" to me.  Suggest testing w/ explicitly increasing timestamps.;;;","01/Nov/11 16:13;jbellis;Sorry, mis-read the script.  Brandon points out that there are no column overwrites, only appends.;;;","01/Nov/11 16:21;brandon.williams;Equivalent script using telephus that does not repro.  Slightly hackish in that it won't create the ks/cf, but the pycassa script can do that and be killed before it inserts.;;;","01/Nov/11 17:23;jbellis;This is tricky, but we figured out what's happening.

First, hinted handoff isn't important to reproducing, but bouncing nodes is.  You need a node to miss an update to get this.

If you do that, then you can get this situation, as seen in Brandon's log:

all 3 nodes reply: (not strictly necessary, all we need is two nodes that disagree with each other)
{noformat}
DEBUG [ReadRepairStage:8] 2011-10-21 19:35:23,105 RowDigestResolver.java (line 62) resolving 3 responses
{noformat}

The responses don't match:
{noformat}
DEBUG [pool-2-thread-6] 2011-10-21 19:35:23,105 StorageProxy.java (line 615) Digest mismatch: org.apache.cassandra.service.DigestMismatchException: Mismatch for key DecoratedKey(91747740688180627279175449712403223124, 747465737472617732) (6705a2ef7042fd98f2c30c5450d33e17 vs bf8c16eb98f3209d3abb723ee8c33185)
{noformat}

The coordinator requests the actual data from each replica and merges the responses:
{noformat}
DEBUG [pool-2-thread-6] 2011-10-21 19:35:23,107 SliceQueryFilter.java (line 123) collecting 0 of 2147483647: 00004ecb:false:4@1319226304173446
DEBUG [pool-2-thread-6] 2011-10-21 19:35:23,108 SliceQueryFilter.java (line 123) collecting 1 of 2147483647: 00004ecc:false:4@1319226304178304
{noformat}

Note that 00004ecb=20171, and 00004ecc=20172.  So both columns are present, and the coordinator now has a slice of [20171, 20172].  It repairs the missing data, then reverses the order as requested in the query and returns [20172, 20171] to the client.

So, the bug on the Cassandra side is that we don't re-restrict the resultset to the requested count after a digest mismatch, before sending it to the client.

Then, the client calls popitem() on the result, which means you get back the *last* item in the resultset, i.e., 20171.

In short: Cassandra needs to fix sending back more results than requested when there are different versions on different nodes that need to be resolved.  We'll address this as part of the related CASSANDRA-3303.  Unfortunately, this code path is one that we know from experience is easy to introduce regressions to, so I don't think we can safely do this in 0.8; the fix will be in 1.0.2+.

However, a simple workaround exists, which is for clients to consume results from the front of the row, instead of the back.  This is what the telphus script did by accident, which is why Brandon couldn't reproduce with that version.;;;","04/Nov/11 08:38;slebresne;CASSANDRA-3303 has been committed with the fix for this. As said by Jonathan above, the fix is 1.0 only.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in PrecompactedRow.write via CommutativeRowIndexer during bootstrap,CASSANDRA-3394,12528300,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,thepaul,thepaul,21/Oct/11 17:09,16/Apr/19 09:32,14/Jul/23 05:52,21/Oct/11 19:43,0.8.8,1.0.1,,,,,0,,,,"{noformat}
ERROR [CompactionExecutor:5] 2011-10-21 15:48:16,138 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[CompactionExecutor:5,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.PrecompactedRow.write(PrecompactedRow.java:107)
        at org.apache.cassandra.io.sstable.SSTableWriter$CommutativeRowIndexer.doIndexing(SSTableWriter.java:514)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:359)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:314)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1118)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1109)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

{{<sylvain> The bug is that the compacted row was gcing *every* tombstones instead of none.}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/11 17:10;slebresne;0001-Don-t-expire-tombstones-in-CommutativeRowIndexer.patch;https://issues.apache.org/jira/secure/attachment/12500210/0001-Don-t-expire-tombstones-in-CommutativeRowIndexer.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,97927,,,Fri Oct 21 20:00:32 UTC 2011,,,,,,,,,,"0|i0gjcv:",94566,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"21/Oct/11 17:12;slebresne;Note: attached patch is for 0.8. The fix for 1.0.1 is the same but that line is now in IncomingStreamReader. I'll handle that during merge once this is reviewed.;;;","21/Oct/11 17:15;jbellis;+1;;;","21/Oct/11 19:43;slebresne;Committed;;;","21/Oct/11 20:00;hudson;Integrated in Cassandra-0.8 #385 (See [https://builds.apache.org/job/Cassandra-0.8/385/])
    Don't expire counter tombstones after streaming
patch by slebresne; reviewed by jbellis for CASSANDRA-3394

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1187477
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFM.toAvro() incorrectly serialises key_validation_class defn,CASSANDRA-3391,12528043,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,20/Oct/11 22:26,16/Apr/19 09:32,14/Jul/23 05:52,21/Oct/11 13:43,0.8.8,1.0.1,,,,,0,,,,"see http://www.mail-archive.com/user@cassandra.apache.org/msg18132.html

Repo with 

{code}
create keyspace Stats with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options={replication_factor:1};

use Stats;

create column family Sample_Stats with default_validation_class=CounterColumnType
    and key_validation_class='CompositeType(UTF8Type,UTF8Type)'
    and comparator='CompositeType(UTF8Type, UTF8Type)'
    and replicate_on_write=true;

[default@Stats] describe cluster;
Cluster Information:
   Snitch: org.apache.cassandra.locator.SimpleSnitch
   Partitioner: org.apache.cassandra.dht.RandomPartitioner
   Schema versions: 
	1d39bbf0-fb60-11e0-0000-242d50cf1ffd: [127.0.0.1]
{code}

Stop and restart the node

{code:java}
ERROR 10:12:22,729 Exception encountered during startup
java.lang.RuntimeException: Could not inflate CFMetaData for {""keyspace"": ""Stats"", ""name"": ""Sample_Stats"", ""column_type"": ""Standard"", ""comparator_type"": ""org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)"", ""subcomparator_type"": null, ""comment"": """", ""row_cache_size"": 0.0, ""key_cache_size"": 200000.0, ""read_repair_chance"": 1.0, ""replicate_on_write"": true, ""gc_grace_seconds"": 864000, ""default_validation_class"": ""org.apache.cassandra.db.marshal.CounterColumnType"", ""key_validation_class"": ""org.apache.cassandra.db.marshal.CompositeType"", ""min_compaction_threshold"": 4, ""max_compaction_threshold"": 32, ""row_cache_save_period_in_seconds"": 0, ""key_cache_save_period_in_seconds"": 14400, ""row_cache_keys_to_save"": 2147483647, ""merge_shards_chance"": 0.1, ""id"": 1000, ""column_metadata"": [], ""row_cache_provider"": ""org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider"", ""key_alias"": null, ""compaction_strategy"": ""org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy"", ""compaction_strategy_options"": {}, ""compression_options"": {}}
	at org.apache.cassandra.config.CFMetaData.fromAvro(CFMetaData.java:362)
	at org.apache.cassandra.config.KSMetaData.fromAvro(KSMetaData.java:193)
	at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:99)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:502)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:161)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Caused by: org.apache.cassandra.config.ConfigurationException: Invalid definition for comparator org.apache.cassandra.db.marshal.CompositeType.
	at org.apache.cassandra.db.marshal.TypeParser.getRawAbstractType(TypeParser.java:319)
	at org.apache.cassandra.db.marshal.TypeParser.getAbstractType(TypeParser.java:247)
	at org.apache.cassandra.db.marshal.TypeParser.parse(TypeParser.java:83)
	at org.apache.cassandra.db.marshal.TypeParser.parse(TypeParser.java:92)
	at org.apache.cassandra.config.CFMetaData.fromAvro(CFMetaData.java:358)
	... 6 more
Caused by: org.apache.cassandra.config.ConfigurationException: Nonsensical empty parameter list for CompositeType
	at org.apache.cassandra.db.marshal.CompositeType.getInstance(CompositeType.java:67)
	at org.apache.cassandra.db.marshal.CompositeType.getInstance(CompositeType.java:61)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.cassandra.db.marshal.TypeParser.getRawAbstractType(TypeParser.java:307)
	... 10 more
{code}

Will post the patch in a minute. ",,jplock,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/11 22:34;amorton;0001-3391-fix.patch;https://issues.apache.org/jira/secure/attachment/12499967/0001-3391-fix.patch",,,,,,,,,,,,,,1.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,96256,,,Fri Oct 21 16:00:27 UTC 2011,,,,,,,,,,"0|i0gjbr:",94561,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"20/Oct/11 22:34;amorton;Use AbstractType.toString() when serialising key_validator_class to Avro.

Checked other places to CFMD.toAvro() and CFMD.fromAvro() they seemed ok. ;;;","21/Oct/11 13:34;slebresne;+1;;;","21/Oct/11 13:43;slebresne;Committed, thanks;;;","21/Oct/11 16:00;hudson;Integrated in Cassandra-0.8 #384 (See [https://builds.apache.org/job/Cassandra-0.8/384/])
    Correctly serialize key_validation_class for avro
patch by amorton; reviewed by slebresne for CASSANDRA-3391

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1187339
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadResponseSerializer.serializedSize() calculation is wrong,CASSANDRA-3390,12528014,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,yangyangyyy,yangyangyyy,20/Oct/11 19:44,16/Apr/19 09:32,14/Jul/23 05:52,24/Oct/11 20:24,0.8.8,1.0.1,,,,,0,,,,"in ReadResponse.java


the following code

    public long serializedSize(ReadResponse response, int version)
    {
        int size = DBConstants.intSize;
        size += (response.isDigestQuery() ? response.digest() : ByteBufferUtil.EMPTY_BYTE_BUFFER).remaining();
        size += DBConstants.boolSize;
        if (response.isDigestQuery())
            size += response.digest().remaining();
        else
            size += Row.serializer().serializedSize(response.row(), version);
        return size;
    }


adds the digest size 2 times

this triggers assertion error in at least ReadVerbHandler


",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/11 19:45;yangyangyyy;3390.patch;https://issues.apache.org/jira/secure/attachment/12499916/3390.patch","23/Oct/11 14:18;jbellis;3390.txt;https://issues.apache.org/jira/secure/attachment/12500365/3390.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,95644,,,Mon Oct 24 23:10:09 UTC 2011,,,,,,,,,,"0|i0gjbb:",94559,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"20/Oct/11 19:45;yangyangyyy;should add the digest size only once;;;","20/Oct/11 19:57;yangyangyyy;seems there are still other places where the count is wrong

ERROR [ReadStage:129] 2011-10-20 15:54:50,932 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadStage:1
29,5,main]
java.lang.AssertionError: Final buffer length 1778 to accomodate data size of 969 (predicted 888)
        at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:56)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
;;;","20/Oct/11 20:08;yangyangyyy;it seems that with the remaining error

buffer.getData().length = size*2 + 1

 buffer.getLength() = size + 81

this may be of some help in debugging
;;;","20/Oct/11 20:09;yangyangyyy;the part i fixed is on the isDigest() path,

I'm suspecting that the remaining error is on the non-digest path, since the sizes are pretty large;;;","20/Oct/11 20:18;jbellis;The first one was fixed in CASSANDRA-3373.  You should probably retest on latest trunk to make sure the other isn't fixed too.;;;","20/Oct/11 20:49;yangyangyyy;tested with latest on 1.0 branch (I checked that it does have the 3373 fix)


still same:


        at java.lang.Thread.run(Thread.java:662)
ERROR [ReadStage:57] 2011-10-20 16:48:50,117 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadStage:57,5,main]
java.lang.AssertionError: Final buffer length 560 to accomodate data size of 360 (predicted 279)
        at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:56)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [ReadStage:45] 2011-10-20 16:48:50,200 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadStage:45,5,main]
java.lang.AssertionError: Final buffer length 1002 to accomodate data size of 574 (predicted 500)
        at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:682)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:56)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
;;;","20/Oct/11 20:50;yangyangyyy;btw how can I find the github commit corresponding to the 1.0.0 official release?
thanks
Yang;;;","20/Oct/11 21:36;brandon.williams;This is the wrong place to ask, but just switch to the 1.0.0 branch: https://github.com/apache/cassandra/tree/cassandra-1.0.0;;;","21/Oct/11 09:34;slebresne;Do you have an easy way to reproduce that remaining error ? Do you use something specific: super columns ? counters ? expiring columns ?;;;","21/Oct/11 18:27;yangyangyyy;standard columns
not counter,
yes they are expiring columns


I found this seems to have to do with thread issues, since it works fine with single client ;;;","21/Oct/11 20:00;yangyangyyy;I went through the serialize() and serializedSize() code and they seem to agree, 

now the most likely cause is thread issue,  would the serialize() ever be called from multiple threads?


the buffer.data().length  should not be in the assert check, that discrepancy comes from the result of expand(), really that is an internal thing, should not be checked.


;;;","21/Oct/11 20:47;jbellis;Sure it should be, because that's what tells us if serializedSize has a bug.;;;","21/Oct/11 20:53;yangyangyyy;I mean  

buffer.getData().length  returns the underlying byte[] length, which is always doubled on expansion, so this will not be the same as the actual bytes written. actually this agrees with what we are seeing, getData().length() == (size + 1 ) * 2, because the buffer is doubled on the last write of one byte.

buffer.getLength()  returns the ""count"" var, yes, this should be the same as the serializedSize() calculation

;;;","21/Oct/11 21:05;jbellis;But we pre-allocate the buffer size:

{code}
        int size = (int) serializer.serializedSize(object, version);
        DataOutputBuffer buffer = new DataOutputBuffer(size);
{code}

The point is to avoid copies during buffer re-allocations, so again, this is a good check to have.;;;","21/Oct/11 21:08;jbellis;I added the object being serialized to the assertion failure message in r1187539.  Does that give us anything useful?;;;","21/Oct/11 21:13;yangyangyyy;I did this too myself

doesn't seem particularly helpful, except for telling us that it's a normal column, with TTL


I'm intrigued by why the difference is always 81 or 77 bytes


ERROR [ReadStage:176] 2011-10-21 14:45:04,099 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadStage:1
76,5,main]
java.lang.AssertionError:  row:Row(key=DecoratedKey(58613415544222752604144955634601649391, 3164323233656134), cf=ColumnFamily(mea
suredSession [0000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656135:false:4@1319222
703553!600,0000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656137:false:4@1319222703
556!600,0000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656139:false:4@1319222703562
!600,0000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656162:false:4@1319222703567!60
0,0000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656164:false:4@1319222703571!600,0
000013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656166:false:4@1319222703584!600,0000
013327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656231:false:4@1319222703589!600,0000013
327cc2ad7303030303030303030303030303030303030303030303030303030303030303030303164323233656233:false:4@1319222703592!600,]))Final b
uffer length 1152 to accomodate data size of 652 (predicted 575)
        at org.apache.cassandra.utils.FBUtilities.serialize(FBUtilities.java:683)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:56)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
;;;","21/Oct/11 21:31;jbellis;I think you're right about the threading being the cause.  Are you using row cache?;;;","21/Oct/11 21:35;yangyangyyy;yes, let me disable rowcache and see if it goes away...;;;","21/Oct/11 22:17;yangyangyyy;yeah, disabled row cache, with multiple threads running, no assertion error so far;;;","23/Oct/11 06:57;yangyangyyy;Jonathan: I see what you mean:

the returned CF from cachedRow, in CFS.java:1169 :



    /**
     *  Filter a cached row, which will not be modified by the filter, but may be modified by throwing out
     *  tombstones that are no longer relevant.
     *  The returned column family won't be thread safe.
     */
    ColumnFamily filterColumnFamily(ColumnFamily cached, QueryFilter filter, int gcBefore)
    {

it seems that for some CFs, the buffer.length() was calculated, then another thread did the 
removeDeletedColumns() for expiring columns in filterColumnFamily(), then serializedSize() was calculated again, something like that.

if that is the case, would it actually cause correctness problems, or is it just an annoying discrepancy between 2 reports on the size (which reflect the size at 2 times in history)?



;;;","23/Oct/11 07:39;yangyangyyy;well, expiring column would lead to the size to be bigger than buffer.length(), 
but what we saw is the reverse, so it could be caused because new columns are added to the cached CF.;;;","23/Oct/11 14:18;jbellis;It looks like the culprit is this part:

{code}
                    if (sliceFilter.count >= cached.getColumnCount())
{code}

... then we don't clone the cached CF.  But this is broken as-written; it's possible for new columns to be inserted, increasing the cached's row size after the check.

I think the right thing to do here is jut get rid of this special case.  We could try to preserve it by only skipping thie copy when the requested count == MAXINT, but that's contrary to best-practices (we strongly discourage using MAXINT as the limit).;;;","23/Oct/11 15:51;yangyangyyy;tried the latest patch, seems to work fine. but understandably, the performance is a bit slower due to the extra copy (even with arraybackedSortedColumns );;;","24/Oct/11 08:14;slebresne;bq.  We could try to preserve it by only skipping thie copy when the requested count == MAXINT

I don't think that would work either because If I understand correctly the problem, it's that when we serialize, we may add a column between when we compute the serialized size and when we do the actual serialization. The request count shouldn't make any difference here (besides, I agree with the contrary to best-practices argument).

I'll note that I think this optimization has always been wrong, because the cf can also change between when the column count is written in the serialization form and the actual write of the columns. If columns are added, we're kind of fine, we'll serialize more columns that advertised but the code will be ok. But it's possible to have a race where we actually remove columns, because a tombstone could be gced by another thread, in which case we could have a EOFException or something like that during deserialization (it's sufficiently unlikely that either nobody ran into it, or got it only once and never again and so didn't report it or something).

Anyway +1 on the patch, but I believe it would be correct to commit to 0.8 too for the above reason.;;;","24/Oct/11 14:26;jbellis;bq. we may add a column between when we compute the serialized size and when we do the actual serialization

Right, I meant that if we were willing to drop the assert and allow the buffer to expand in the rare case of a race here.

I'll commit to 0.8 and 1.0.;;;","24/Oct/11 20:24;jbellis;committed;;;","24/Oct/11 23:10;hudson;Integrated in Cassandra-0.8 #387 (See [https://builds.apache.org/job/Cassandra-0.8/387/])
    remove incorrect optimization from slice read path
patch by jbellis; reviewed by slebresne for CASSANDRA-3390

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1188353
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageService.setMode() is used inconsistently,CASSANDRA-3388,12527840,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,thobbs,thobbs,19/Oct/11 22:45,16/Apr/19 09:32,14/Jul/23 05:52,28/Oct/11 17:23,1.0.2,,,,,,0,,,,"{{StorageService.setMode()}}, which ends up setting the OperationMode attribute of the related mbean, is used inconsistently.  In most places, it's used like ""{{setMode(""MODE: details"")}}, but in a few places, it's used more like a normal log message.

To make this attribute more usable through JMX, {{setMode()}} should have a signature like {{setMode(mode, details)}}, where the mode parameter could be an enum (or even just a string, the main thing is just being consistent).  The OperationMode JMX attribute should definitely remain a string, though.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/11 16:17;slebresne;3388.patch;https://issues.apache.org/jira/secure/attachment/12500707/3388.patch","25/Oct/11 17:27;slebresne;3388_v2.patch;https://issues.apache.org/jira/secure/attachment/12500725/3388_v2.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,92099,,,Fri Oct 28 17:23:57 UTC 2011,,,,,,,,,,"0|i0gjaf:",94555,,thobbs,,thobbs,Low,,,,,,,,,,,,,,,,,"19/Oct/11 22:50;jbellis;We're talking about this, right?

{code}
    private void setMode(String m, boolean log)
{code}

Why would we even expose this over JMX?  It's not intended to be public.;;;","19/Oct/11 23:26;thobbs;Yes, that's the method.  Sorry, I forgot that there was also a ""log"" parameter.

It's a fairly reliable way to figure out if a node is joining, moving, decommissioning, or has finished decommissioning.  I'm open to alternative ways of discovering this reliably.  I assumed that's why this was exposed through JMX in the first place.;;;","20/Oct/11 01:43;jbellis;I don't see setMode or getMode in StorageServiceMBean.  I'm lost, how is it exposed through JMX?;;;","20/Oct/11 04:08;thobbs;It's named getOperationMode().;;;","20/Oct/11 04:25;jbellis;Ah, okay.

setMode isn't exposed though, which is as it should be since it's informational only.  Setting it wouldn't actually change any internal state to match.;;;","20/Oct/11 04:33;thobbs;Right, I wasn't suggesting that setMode() be exposed externally at all.  I was merely saying that the way it's used _internally_ is inconsistent.  (And since the attribute is readable through JMX, being consistent matters.);;;","20/Oct/11 16:31;jbellis;Ah, I get it now.  I was confused by the description of ""making this attribute more usable through JMX."";;;","25/Oct/11 16:41;thobbs;A couple of comments on v1 of the patch:
* I think you accidentally lower-cased 'Leaving' on one line
* Everything is done consistently now, but I'm a little bit concerned about this being accidentally broken in the future.  Would you mind either splitting the mode (currently just the string prefix) into a separate parameter or at least documenting the setMode() method with a note about how it should be used?

Thanks, Sylvain.;;;","25/Oct/11 17:27;slebresne;Yeah right. I was going with the 'smallest diff' approach but I agree we can make that cleaner. v2 does so by adding a Mode enumeration. A few details (that I think are ok but are worth mentioning):
* the patch changes the semantic of the getOperationMode JMX attribute to only return the ""mode"", not the message part. It can be changed back but I'm not sure the message is really useful outside of the log itself and it feels slightly cleaner that way.
* the modes are in caps. Again, feels more simple, natural but that can change.
* I've changed the last ""mode"" from Draining to Drained. Feels more useful/right that way.
* ;;;","25/Oct/11 18:52;thobbs;Minor note: {{setMode(Mode.DECOMMISSIONED, null, true);}} could omit the {{null}} parameter.

Other than that, it looks perfect to me, and I agree with your thoughts.  Thanks!

+1;;;","28/Oct/11 17:23;slebresne;Committed (with suggested minor fix);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unnecessary locking in hint path due to InetAddress.getLocalhost(),CASSANDRA-3387,12527830,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yangyangyyy,yangyangyyy,yangyangyyy,19/Oct/11 20:59,16/Apr/19 09:32,14/Jul/23 05:52,20/Oct/11 16:36,1.0.1,,,,,,0,,,,"in tests we found locking contention also due to the following




Stacks at 01:49:19 PM (uptime 10:54)


MutationStage:62 [BLOCKED] CPU time: 0:06
java.net.InetAddress.getLocalHost()
org.apache.cassandra.utils.UUIDGen.getClockSeqAndNode()
org.apache.cassandra.utils.UUIDGen.createTimeUUIDBytes(long)
org.apache.cassandra.utils.UUIDGen.getTimeUUIDBytes()
org.apache.cassandra.db.RowMutation.hintFor(RowMutation, ByteBuffer)
org.apache.cassandra.service.StorageProxy$4.run()
java.lang.Thread.run()





we can easily change every getLocalHost() call to use a cached value",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/11 21:03;yangyangyyy;3387.diff;https://issues.apache.org/jira/secure/attachment/12499757/3387.diff","19/Oct/11 21:25;yangyangyyy;3387_v2.diff;https://issues.apache.org/jira/secure/attachment/12499764/3387_v2.diff",,,,,,,,,,,,,2.0,yangyangyyy,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,92085,,,Thu Oct 20 16:36:50 UTC 2011,,,,,,,,,,"0|i0gj9z:",94553,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"19/Oct/11 21:03;yangyangyyy;use cached value , do not call getLocalhost() every time when we try to construct a hint
;;;","19/Oct/11 21:12;jbellis;Why not just use FBUtilities.getLocalAddress?;;;","19/Oct/11 21:24;yangyangyyy;oh, I didn't know that one.

yes, FBUtilities.getLocalAddress() is better;;;","19/Oct/11 21:25;yangyangyyy;version 2 , using fb utils;;;","20/Oct/11 16:36;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in hinted handoff,CASSANDRA-3385,12527792,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,yangyangyyy,yangyangyyy,19/Oct/11 17:44,16/Apr/19 09:32,14/Jul/23 05:52,27/Oct/11 16:43,1.0.1,,,,,,1,hintedhandoff,,,"I'm using the current HEAD of 1.0.0 github branch, and I'm still seeing this error, not sure if it's  this bug or another one.



 INFO [HintedHandoff:1] 2011-10-19 12:43:17,674 HintedHandOffManager.java (line 263) Started hinted handoff for token: 11342745564
0312821154458202477256070484 with IP: /10.39.85.140
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,885 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHan
doff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,886 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more


this could possibly be related to #3291
",,danbarker85,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/11 12:58;nicktelford;0001-Changed-assertions-in-delivery-of-hints-to-a-check-t.patch;https://issues.apache.org/jira/secure/attachment/12500876/0001-Changed-assertions-in-delivery-of-hints-to-a-check-t.patch","26/Oct/11 14:31;jbellis;3385.txt;https://issues.apache.org/jira/secure/attachment/12500888/3385.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,89290,,,Thu Oct 27 16:43:20 UTC 2011,,,,,,,,,,"0|i0gj93:",94549,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"19/Oct/11 17:49;yangyangyyy;enabled assertion



 INFO [HintedHandoff:1] 2011-10-19 13:44:08,346 HintedHandOffManager.java (line 263) Started hinted handoff for token: 0 with IP: 
/10.196.37.187
ERROR [HintedHandoff:1] 2011-10-19 13:44:08,513 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHan
doff:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:285)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)



line 285 is:
                assert versionColumn != null;
well the other following fields could also be null too,
                assert versionColumn != null;
                assert tableColumn != null;
                assert keyColumn != null;
                assert mutationColumn != null;

;;;","19/Oct/11 19:48;yangyangyyy;I just found that this contributes to another symptom I'm seeing: for RF=3, and a ring of 3 nodes, if I bring down 1 box, the remaining 2 still work fine for Quorum access, but the latency is 20x high.

I can see from debugging that a lot of time is spent on storing hints into local system table on the coordinator. but this Table.apply is slow because a lot of time is spent on the lock, while it really should not happen since the lock is sharded into 4096 ones. it turns out that all the keys used in the hints writing are the same key, at least in the examples I looked at in the debugger, if I'm correct in this observation, this is a serious bug;;;","19/Oct/11 19:57;yangyangyyy;I see, the key in hinted table is the ID of the dead box. given that this leads to lock contention, would it be better to change the storage layout of hints? ---- I have never tried hinted handoff before, not sure if lock contention was a problem before
;;;","19/Oct/11 19:57;jbellis;So either new-style hints are being written without versionColumn by RowMutation.hintFor, or old style hints did not get cleaned out properly by SystemTable.purgeIncompatibleHints.  But both of those look fine to me.;;;","19/Oct/11 20:03;yangyangyyy;
the hints code was from:

https://github.com/apache/cassandra/commit/3893f24098c3d82dc31571f0b6841e2d5821ea74#diff-12

#CASSANDRA-2034

maybe it should be better to NOT let the writer wait for hints to finish? right now the local hints write make the entire write slower in probably 2 ways : 1) the main write has to wait for hint write to finish, which is slow due to lock 2) hints writes are slow, which create a lot of jobs on MUTATION stage, so even if main write does not wait for them, the MUTATION stage could possibly be bogged down with hints writes, and not able to handle normal writes fast enough


;;;","19/Oct/11 20:07;jbellis;IMO the right fix to the lock contention is to simply not synchronize when there are no indexes present.  But that's unrelated to the assertion failure here.;;;","19/Oct/11 20:18;yangyangyyy;that works for me too. but I guess you will finally have to handle cases where indexes are needed. in those cases, probably we need to change the hints format away from using IP as key;;;","19/Oct/11 20:23;jbellis;If you're not going to use IPs as keys how are you going to replay hints efficiently?  You need to consider the read path as well as the write when modeling something. :);;;","19/Oct/11 20:32;jbellis;Created CASSANDRA-3386 for the contention problem.;;;","25/Oct/11 15:21;danbarker85;I am using the RPM provided by datastax, I upgraded from 0.8.7 to 1.0.0, then performed a scrub and repair on each node as described in the documentation.
This worked fine for all nodes except one, which now throws exceptions whenever I start it up, and goes ""Down"" occassionly.  All other nodes have been fine.

This is from the log, I think it could be related, but I'm not sure:

INFO [HintedHandoff:4] 2011-10-25 15:30:08,867 HintedHandOffManager.java (line 263) Started hinted handoff for token: 0 with IP: /xx.xx.xx.xx
 INFO [HintedHandoff:4] 2011-10-25 15:30:08,868 HintedHandOffManager.java (line 318) Finished hinted handoff of 0 rows to endpoint /xx.xx.xx.xx
 INFO [HintedHandoff:4] 2011-10-25 15:30:09,998 HintedHandOffManager.java (line 263) Started hinted handoff for token: 148873535527910577765226390751398592512 with IP: /xx.xx.xx.xx
ERROR [HintedHandoff:4] 2011-10-25 15:30:09,999 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:4,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:285)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
 INFO [HintedHandoff:5] 2011-10-25 15:30:59,893 HintedHandOffManager.java (line 263) Started hinted handoff for token: 106338239662793269832304564822427566080 with IP: /xx.xx.xx.xx
ERROR [HintedHandoff:5] 2011-10-25 15:30:59,894 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:5,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:285)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
 INFO [HintedHandoff:6] 2011-10-25 15:31:41,194 HintedHandOffManager.java (line 263) Started hinted handoff for token: 42535295865117307932921825928971026432 with IP: /xx.xx.xx
 INFO [HintedHandoff:6] 2011-10-25 15:31:41,194 HintedHandOffManager.java (line 318) Finished hinted handoff of 0 rows to endpoint /xx.xx.xx.xx;;;","26/Oct/11 12:58;nicktelford;As Jonathan said, it looks like either old hints that haven't been cleaned or some corruption to the hints that are stored.

While it's probably important to find the source of this problem, I think it's really bad that we use assertions to check for it. Hinted Handoff is an optimization, so for invalid hints shouldn't break things.

The attached patch removes the assertions and replaces them with a check to ensure the hint is valid. If it's not, a warning will be emitted (so you know something's not quite right) and everything will continue as normal.;;;","26/Oct/11 13:48;jbellis;I'd say that reasoning makes this the perfect case for assertions -- it doesn't affect anything but hints for the assert to fail.;;;","26/Oct/11 13:52;nicktelford;Wouldn't this disrupt the delivery of hints that aren't corrupt?;;;","26/Oct/11 14:08;jbellis;One possible avenue for this is that if startup takes long enough (due to CL replay + sstable index sampling, probably) that compactions start before the upgrade hint purge, compaction can generation ""new"" 0.8 hints after we try to delete them.  Patch to switch to truncate to avoid this, although CASSANDRA-3399 is open to make truncate more bulletproof in that situation itself.  In the meantime, removing the hints columnfamily manually before restarting should fix the problem.

Also added some debug logging to the hint purge in r1189221.;;;","26/Oct/11 14:10;jbellis;bq. Wouldn't this disrupt the delivery of hints that aren't corrupt?

Sure.  Point is, that's not a Big Deal.  So it's worth having the big neon sign of an exception telling people ""this ain't right."";;;","26/Oct/11 14:31;jbellis;new version of patch adds a second check for 0.8 hints during hint delivery;;;","26/Oct/11 15:31;nicktelford;I guess I can live with ignoring the known types of corruption (old hints) and leaving the assertions to flag up unknown forms of corruption.;;;","26/Oct/11 21:50;brandon.williams;+1;;;","27/Oct/11 16:43;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"it would be nice if ""describe keyspace"" in cli shows ""Cache Provider""",CASSANDRA-3384,12527782,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,vijay2win@yahoo.com,vijay2win@yahoo.com,19/Oct/11 16:55,16/Apr/19 09:32,14/Jul/23 05:52,19/Oct/11 19:45,0.8.8,1.0.1,,Legacy/Tools,,,0,lhf,,,Describe keyspace in the cli doesn't show the cache provider it would be nice to show it to verify the settings.,JVM on Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/11 19:38;xedin;CASSANDRA-3384.patch;https://issues.apache.org/jira/secure/attachment/12499741/CASSANDRA-3384.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,89279,,,Wed Oct 19 23:35:45 UTC 2011,,,,,,,,,,"0|i0gj8n:",94547,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"19/Oct/11 17:17;xedin;In the latest 0.8 branch I see ""row_cache_provider"" showing, can you clarify what else do you want to see there?;;;","19/Oct/11 17:54;brandon.williams;I too see this in 0.8.;;;","19/Oct/11 18:13;vijay2win@yahoo.com;I dont see it in 0.8.6 may be in the later version?


[default@unknown] use Keyspace2;
Authenticated to keyspace: Keyspace2
[default@Keyspace2] update column family Standard2 with rows_cached=1000 and row_cache_provider=SerializingCacheProvider;
bf5faf90-fa7d-11e0-0000-90ba79db4aff
Waiting for schema agreement...
[default@Keyspace2] describe keyspace Keyspace2;                                WARNING: Could not connect to the JMX on 0.0.0.0:7199, information won't be shown.

Keyspace: Keyspace2:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [us-east:2]
  Column Families:
    ColumnFamily: Standard2
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 1000.0/0
      Key cache size / save period in seconds: 1.0/14400
      Memtable thresholds: 3.5390625/120/128 (millions of ops/minutes/MB)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
[default@Keyspace2] ;;;","19/Oct/11 18:14;xedin;cassandra-0.8 branch has it.;;;","19/Oct/11 19:30;vijay2win@yahoo.com;Pavel, I dont see it... are you seeing it by doing ""show keyspaces/describe keyspace <name>""? i dont see in 1.0 or the latest 0.8 build.


[default@vj] update column family Standard1 with rows_cached=1000 and row_cache_provider=SerializingCacheProvider;
7ed92090-fa88-11e0-0000-242d50cf1ffe
Waiting for schema agreement...
... schemas agree across the cluster
[default@vj] describe keyspace vj;
Keyspace: vj:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [datacenter1:1]
  Column Families:
    ColumnFamily: Standard1
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 1000.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.29062499999999997/1440/62 (millions of ops/minutes/MB)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
[default@vj] 
;;;","19/Oct/11 19:35;xedin;Take a look at the title, it states that command is ""show schema;"" and open CliClient.java:1673 under cassandra-0.8, let me check if ""describe keyspace"" shows it.;;;","19/Oct/11 19:38;xedin;Here is one-line patch to show it in ""describe keyspace <keyspace>;"" and ""show keyspaces"";;;","19/Oct/11 19:40;brandon.williams;+1;;;","19/Oct/11 19:45;xedin;Committed.;;;","19/Oct/11 19:48;vijay2win@yahoo.com;Sorry for the confusion... Thanks +1;;;","19/Oct/11 23:35;hudson;Integrated in Cassandra-0.8 #382 (See [https://builds.apache.org/job/Cassandra-0.8/382/])
    CLI `describe keyspace <ks>` to show ""Row Cache Provider""
patch by Pavel Yaskevich; reviewed by Brandon Williams for CASSANDRA-3384

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1186429
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy does not log correctly when schema is not in agreement,CASSANDRA-3381,12527672,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tommysdk,cywjackson,cywjackson,18/Oct/11 22:52,16/Apr/19 09:32,14/Jul/23 05:52,24/Oct/11 23:10,1.0.1,,,,,,0,,,,"""logger.debug(""%s disagrees (%s)"", host, entry.getKey());""

that would literally log: 

DEBUG [pool-2-thread-359] 2011-10-18 10:34:45,376 StorageProxy.java (line 821) %s disagrees (%s)


simple fix: replace with %s with {} ... may want to consider logging better comment?",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/11 18:02;tommysdk;CASSANDRA-3381.patch;https://issues.apache.org/jira/secure/attachment/12500504/CASSANDRA-3381.patch",,,,,,,,,,,,,,1.0,tommysdk,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,88932,,,Mon Oct 24 23:10:09 UTC 2011,,,,,,,,,,"0|i0gj73:",94540,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"24/Oct/11 18:02;tommysdk;Fixed the erroneous debug logging statement by replacing %s with {}, as supported by SLF4J. Also made use of the {}-notation on some of the other debug logging statements in the class.;;;","24/Oct/11 23:10;jbellis;committed.  Thanks, Jackson and Tommy!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
correct dropped messages logging,CASSANDRA-3377,12527620,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,18/Oct/11 17:02,16/Apr/19 09:32,14/Jul/23 05:52,19/Oct/11 17:39,0.8.8,1.0.1,,,,,0,logging,,,"CASSANDRA-3004 switched MessagingService back to logging only ""recent"" dropped messages instead of server lifetime totals, but the log message was not updated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/11 17:05;jbellis;3377.txt;https://issues.apache.org/jira/secure/attachment/12499562/3377.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,88871,,,Wed Oct 19 14:02:02 UTC 2011,,,,,,,,,,"0|i0gj5b:",94532,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"18/Oct/11 18:31;brandon.williams;+1;;;","19/Oct/11 14:02;hudson;Integrated in Cassandra-0.8 #381 (See [https://builds.apache.org/job/Cassandra-0.8/381/])
    correct dropped messages logging
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3377

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1186204
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid clock drift on some Windows machines,CASSANDRA-3375,12527612,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,flavio,flavio,18/Oct/11 16:21,16/Apr/19 09:32,14/Jul/23 05:52,18/Oct/11 17:05,1.0.1,,,,,,0,windows,,,"Performing Thread.sleep() with non-rounded values increases the frequency of interrupts on Windows machines; this can cause performance problems, and on some machines even clock drift problems for the duration of the sleep.
Fixing the issue is trivial: lower the degree of randomness by allowing only ""rounded"" sleep periods.
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6464007
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6435126",Windows,,,,,,,,,,,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,"18/Oct/11 16:23;flavio;trunk-3375.txt;https://issues.apache.org/jira/secure/attachment/12499558/trunk-3375.txt",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,88863,,,Tue Oct 18 17:05:12 UTC 2011,,,,,,,,,,"0|i0gj4f:",94528,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"18/Oct/11 16:23;flavio;Simple fix as proposed in description. I forgot to mention that the problem is in HintedHandOffManager.;;;","18/Oct/11 17:05;brandon.williams;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL can't create column with compression or that use leveled compaction,CASSANDRA-3374,12527593,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,slebresne,slebresne,18/Oct/11 13:48,16/Apr/19 09:32,14/Jul/23 05:52,30/Jan/12 16:22,1.0.8,,,Legacy/CQL,,,0,cql,,,"Looking at CreateColumnFamilyStatement.java, it doesn't seem CQL can create compressed column families, nor define a compaction strategy.",,jplock,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/12 21:06;xedin;CASSANDRA-3374-compaction_strategy_class.patch;https://issues.apache.org/jira/secure/attachment/12512230/CASSANDRA-3374-compaction_strategy_class.patch","22/Dec/11 21:51;xedin;CASSANDRA-3374.patch;https://issues.apache.org/jira/secure/attachment/12508452/CASSANDRA-3374.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,88837,,,Mon Jan 30 16:22:04 UTC 2012,,,,,,,,,,"0|i0gj3z:",94526,,thepaul,,thepaul,Low,,,,,,,,,,,,,,,,,"20/Oct/11 20:51;tjake;There is also no way to specify a index_type;;;","03/Nov/11 13:27;slebresne;I think we want to support the strategy options and compaction options generically. This would make maintenance easier as we won't have to update this each time we add a new option, but more importantly if it's not generic, it means one cannot use a custom compressor or compaction strategy having options. I understand this complicate matters because the parser probably needs to be updated accordingly, but I believe we'd better bite the bullet now.

It would also be nice to add index_type support and to update the CQL doc.;;;","03/Nov/11 13:31;xedin;If Jonathan says that we can support compound options like 'compression:<something>' in CREATE TABLE I'm not against it.;;;","03/Nov/11 17:18;jbellis;I thought we'd use the syntax we use for replication strategy options.;;;","03/Nov/11 17:22;xedin;ok, I will modify patch to support that then.;;;","28/Nov/11 16:24;xedin;CQLsh fails with ""Unmatched named substitution: 'sstable_compressor' not given for CREATE COLUMNFAMILY cf ( KEY text PRIMARY KEY ) WITH compression_parameters:sstable_compressor = 'SnappyCompressor' and compression_parameters:chunk_size_in_kb = 16 and comparator = ascii;""... Tested with raw thrift call - works as excepted. 

Jonathan do you have any idea how to fix that cqlsh problem?;;;","02/Dec/11 05:38;jbellis;No idea. Assigning to Paul to have a look at that part.;;;","12/Dec/11 18:42;thepaul;This is due to a bug in the python driver: http://code.google.com/a/apache-extras.org/p/cassandra-dbapi2/issues/detail?id=11

Also, changes like this should probably include the relevant changes to the CQL doc at doc/cql/CQL.textile.

I'll look at making those changes if I have a few moments.;;;","22/Dec/11 21:18;jbellis;The python driver bug is fixed, are we good to go for this patch?;;;","22/Dec/11 21:39;xedin;rebased patch. everything works as expected with cql fixed!;;;","22/Dec/11 21:51;xedin;patch adds update to CQL.doc;;;","23/Dec/11 23:20;thepaul;+1;;;","24/Dec/11 18:37;xedin;Committed.;;;","27/Jan/12 20:40;thepaul;Was this meant to expose 'compaction_strategy_class' to CQL, as well as compaction_strategy_options? It's added here as a valid option, but it doesn't actually do anything, and it appears there is no other possible way to set CFMetaData.compactionStrategyClass from CQL.;;;","27/Jan/12 20:47;jbellis;bq. Was this meant to expose 'compaction_strategy_class' to CQL

Yes;;;","27/Jan/12 21:06;xedin;it had that option but never actually bothered to set it :);;;","30/Jan/12 16:06;thepaul;+1;;;","30/Jan/12 16:22;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadResponseSerializer doesn't compute serialized size correctly,CASSANDRA-3373,12527592,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,18/Oct/11 13:42,16/Apr/19 09:32,14/Jul/23 05:52,18/Oct/11 14:10,1.0.1,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/11 13:43;slebresne;3373.patch;https://issues.apache.org/jira/secure/attachment/12499542/3373.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,88836,,,Tue Oct 18 14:10:45 UTC 2011,,,,,,,,,,"0|i0gj3j:",94524,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"18/Oct/11 13:43;slebresne;The digest bytes are added twice when it's a digest query. Patch attached.;;;","18/Oct/11 13:48;jbellis;+1;;;","18/Oct/11 14:10;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra inferred schema and actual data don't match,CASSANDRA-3371,12527421,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,petewarden,petewarden,17/Oct/11 10:25,16/Apr/19 09:32,14/Jul/23 05:52,13/Feb/12 23:45,1.0.8,,,,,,2,,,,"It's looking like there may be a mismatch between the schema that's being reported by the latest CassandraStorage.java, and the data that's actually returned. Here's an example:

rows = LOAD 'cassandra://Frap/PhotoVotes' USING CassandraStorage();
DESCRIBE rows;
rows: {key: chararray,columns: {(name: chararray,value: bytearray,photo_owner: chararray,value_photo_owner: bytearray,pid: chararray,value_pid: bytearray,matched_string: chararray,value_matched_string: bytearray,src_big: chararray,value_src_big: bytearray,time: chararray,value_time: bytearray,vote_type: chararray,value_vote_type: bytearray,voter: chararray,value_voter: bytearray)}}
DUMP rows;
(691831038_1317937188.48955,{(photo_owner,1596090180),(pid,6855155124568798560),(matched_string,),(src_big,),(time,Thu Oct 06 14:39:48 -0700 2011),(vote_type,album_dislike),(voter,691831038)})

getSchema() is reporting the columns as an inner bag of tuples, each of which contains 16 values. In fact, getNext() seems to return an inner bag containing 7 tuples, each of which contains two values. 

It appears that things got out of sync with this change:
http://svn.apache.org/viewvc/cassandra/branches/cassandra-0.8/contrib/pig/src/java/org/apache/cassandra/hadoop/pig/CassandraStorage.java?r1=1177083&r2=1177082&pathrev=1177083

See more discussion at:
http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/pig-cassandra-problem-quot-Incompatible-field-schema-quot-error-tc6882703.html
",,howech,jeromatron,sdolgy,,,,,,,,,,,,,,,,,,,,,,,,,,,PIG-2485,,,,CASSANDRA-3745,,,,,,,,,"08/Feb/12 14:01;brandon.williams;0001-Rework-pig-schema.txt;https://issues.apache.org/jira/secure/attachment/12513811/0001-Rework-pig-schema.txt","08/Feb/12 14:01;brandon.williams;0002-Output-support-to-match-input.txt;https://issues.apache.org/jira/secure/attachment/12513812/0002-Output-support-to-match-input.txt","19/Oct/11 20:46;brandon.williams;3371-v2.txt;https://issues.apache.org/jira/secure/attachment/12499755/3371-v2.txt","20/Oct/11 12:03;brandon.williams;3371-v3.txt;https://issues.apache.org/jira/secure/attachment/12499851/3371-v3.txt","19/Jan/12 19:34;brandon.williams;3371-v4.txt;https://issues.apache.org/jira/secure/attachment/12511156/3371-v4.txt","31/Jan/12 22:18;brandon.williams;3371-v5-rebased.txt;https://issues.apache.org/jira/secure/attachment/12512674/3371-v5-rebased.txt","31/Jan/12 20:59;brandon.williams;3371-v5.txt;https://issues.apache.org/jira/secure/attachment/12512626/3371-v5.txt","13/Feb/12 22:58;xedin;3371-v6-cleanup.patch;https://issues.apache.org/jira/secure/attachment/12514413/3371-v6-cleanup.patch","13/Feb/12 21:37;brandon.williams;3371-v6.txt;https://issues.apache.org/jira/secure/attachment/12514398/3371-v6.txt","17/Oct/11 20:49;petewarden;pig.diff;https://issues.apache.org/jira/secure/attachment/12499429/pig.diff","13/Feb/12 21:37;brandon.williams;smoke_test.txt;https://issues.apache.org/jira/secure/attachment/12514399/smoke_test.txt",,,,11.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,88124,,,Mon Feb 13 23:45:18 UTC 2012,,,,,,,,,,"0|i0gj2n:",94520,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"17/Oct/11 16:45;brandon.williams;Pete, can you attach this patch as a file?  It's difficult to read in the description.;;;","17/Oct/11 20:49;petewarden;Messy patch to implement matching schema and data return in CassandraLoader.;;;","17/Oct/11 20:55;petewarden;I've attached the patch as a file (couldn't see how when creating the ticket). It's definitely not ready for prime-time, but hopefully should get across the idea of what I'm going for. Some notes:

- I think the Hex handling has changed between top-of-tree and the 0.8.7 version I'm on, so the diff includes some reversions so I can compile it against my older version. This shouldn't be in any final version of the patch.

- It has been a long time since I last wrote Java code, so my changes are probably very non-idiomatic.

- I don't attempt to handle super-columns in getNext(). Looking at the original getSchema() code, it looks like that case might be unhandled anyway? I see 'if (cfDef.column_type.equals(""Super"")) return null;' near the top.
;;;","17/Oct/11 20:57;petewarden;And for completeness, here's the definition of the column family in the examples above:


create column family PhotoVotes with
  comparator = UTF8Type and
  column_metadata =
  [
    {column_name: voter, validation_class: UTF8Type, index_type: KEYS},
    {column_name: vote_type, validation_class: UTF8Type},
    {column_name: photo_owner, validation_class: UTF8Type, index_type: KEYS},
    {column_name: src_big, validation_class: UTF8Type},
    {column_name: pid, validation_class: UTF8Type, index_type: KEYS},
    {column_name: matched_string, validation_class: UTF8Type},
    {column_name: time, validation_class: UTF8Type},
  ];
;;;","19/Oct/11 20:46;brandon.williams;This approach has many problems, such as column name and key collisions.  Instead, what we need to is wrap the column/value pairs in their own tuple and then insert these into an outer tuple that goes in the bag, causing the schema the match our actual output.  v2 does this.;;;","19/Oct/11 20:54;petewarden;Is there a reason the columns can't at least go into a map? As things stand, it's painfully hard to do the natural row.column.value lookup in a script. Or to put it as a concrete example, I can currently do this:

all_votes = LOAD 'cassandra://Frap/PhotoVotes' USING CassandraStorage();
album_votes = FILTER all_votes BY ((vote_type EQ 'album_like') OR (vote_type EQ 'album_dislike'));

What does this example look like with your approach?;;;","20/Oct/11 12:03;brandon.williams;I had what I thought would be a good idea to accomodate this: I'd alias the tuples themselves after the index names, allowing you to do something like this:

{noformat}
album_votes = FILTER all_votes BY (columns.vote_type.value EQ 'album_like') OR (columns.vote_type.value EQ 'album_dislike');
{noformat}

It's not that easy, however.  When you dereference a bag, it automatically dereferences the tuple inside it (programmatically, a bag can only have one tuple, but that tuple can contain anything, and thus the automatic deref to mask this from the user.)  Unfortunately, it appears to also deref any other tuples inside as well, so you end up in a situation where 'columns.$0' returns the column name, and weird aliasing issues like 'columns.<first index name>' also returns the column name and the second index returns the value, with the rest being null.

To get around this, I thought I'd nest each tuple inside another bag.  This of course results in a crazy looking schema:
{noformat}
votes: {key: bytearray,columns: {(matched_string: {(name: chararray,value: chararray)},photo_owner: {(name: chararray,value: chararray)},pid: {(name: chararray,value: chararray)},src_big: {(name: chararray,value: chararray)},time: {(name: chararray,value: chararray)},vote_type: {(name: chararray,value: chararray)},voter: {(name: chararray,value: chararray)})}}
{noformat}
but if it still derefs as elegantly, it won't be too bad.  The problem here is that it still can't deref correctly, you end up with nonsensical parsing errors such as:
{noformat}
ERROR 1200: Pig script failed to parse: 
<file photo.pig, line 5, column 8> pig script failed to validate: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1128: Cannot find field name in matched_string:bag{:tuple(name:chararray,value:chararray)
{noformat}
even though the 'name' alias is clearly there and 'matched_string' was already being deref'd.

So I decided to just get rid of the all the bags, and instead did a tuple of tuples.  This worked!  But the problem here is that a tuple has to fit into memory, where a bag can spill to disk if it is very large.  Currently this won't be an issue since we have to fit the entire row into memory via thrift anyway, but in the future when we have large row support the contract may have to change again.  Maybe this is the best option though since right now it Just Works and nothing else is viable.

While going through all this I noticed two other problems we currently have.  We need to put the indexed columns at the beginning so they can match the schema (since any amount of non-indexed columns may fall between them in sorting order) and the default validator is always being used due to a bad lookup.  These are trivial to solve, however.  v3 fixes these issues and takes the 'tuple of tuples' approach, for better or worse.;;;","20/Oct/11 12:09;brandon.williams;bq. Is there a reason the columns can't at least go into a map? 

Maps only support string keys as far as I know (and also have the unspillable problem.);;;","20/Oct/11 22:02;petewarden;Thanks Brandon, sounds very promising, excited to try it out.;;;","21/Oct/11 05:28;petewarden;Works like a charm, thanks again Brandon.
;;;","09/Nov/11 20:58;jeromatron;Brandon, Jacob Perkins, and Jeremy (me) had a long discussion about how to address this as well as other issues with CassandraStorage.  We came down to a list of 3-4 things that if implemented would resolve the problem with pig 0.9 as well as make CassandraStorage much more usable.

1. Fix schema so that this ticket's problem is resolved - this goes along with #2.

2. have the default return value from CassandraStorage be (key, column, value) as is thought of for transposing wide rows.  If in the constructor, something like pygmalion's FromCassandraBag is specified, then return that.  See pygmalion's doc for that.

3. Inspect what is passed in for the output and if it conforms to pygmalion's ToCassandraBag - namely a key and a bunch of columns, it will introspect the pig schema and use those pig variable names for the column names.

4. Optionally handle the uniqueness case with some kind of context/random identifier so that multiple CassandraStorage instances writing out don't get confused with the schema.;;;","11/Nov/11 21:31;petewarden;Thanks for digging into this. To help me understand better, do you have a concrete example of how the example script I mentioned would look? With Brandon's current solution it's like this:

all_votes = LOAD 'cassandra://Frap/PhotoVotes' USING CassandraStorage();
album_votes = FILTER all_votes BY ((columns.vote_type.value EQ 'album_like') OR (columns.vote_type.value EQ 'album_dislike'));

This is very usable, and my only concern would be that alternative solutions wouldn't make this common use case as easy.;;;","15/Nov/11 18:16;jeromatron;I think with the pygmalion style of specification, it might require a bit more on the load, but less on the filter.  The columns var wouldn't be needed on the filter, for example.  It would just look like the pygmalion examples except you wouldn't need FromCassandraBag, you would specify that in ""USING CassandraStorage('vote_type')""

So I think it would just be:
all_votes = LOAD 'cassandra://Frap/PhotoVotes' USING CassandraStorage('vote_type'); -- you can add any additional columns
album_votes = FILTER all_votes BY (vote_type EQ 'album_like') OR (vote_type EQ 'album_dislike');

You can add other columns as you need them in the CassandraStorage section.;;;","16/Nov/11 20:14;petewarden;That makes sense, and looks very approachable, thanks. I'll keep an eye on the comments here so I can give it a try as soon as a prototype is available. Thanks again Brandon, Jacob and Jeremy for your hard work on this one.;;;","01/Dec/11 22:28;verbal;I've been following this thread and am actively trying to use CassandraStorage. Before I found this thread, I tried a bunch of stuff on my own with varying degrees of success. My most pressing issue right now is regarding rows with different columns. I do not believe the Pig schema that gets generated supports that. Please advise. Thanks!;;;","19/Jan/12 19:44;brandon.williams;bq. 1. Fix schema so that this ticket's problem is resolved

v4 does this, however it's not quite all of what we want.

bq. 2. have the default return value from CassandraStorage be (key, column, value) as is thought of for transposing wide rows

After thinking about this more, that's the wrong way to approach that, because if you DO want to work within the row, now you have to do an expensive group to get back what we had before -- a nest structure -- where breaking that structure up into (k, c, v) is extremely cheap if that's what you want.  So ultimately, we need to stick with a bag for spillage, and thus keep the existing schema.  v4 does this.

v4 also names the *values* of indexed/validated columns after their name, which is more pygmalion-style, since you'll always want to filter the value, not the name.

The problem, however, is strange parsing problems again:

{noformat}
ERROR 1200: Pig script failed to parse: 
<file foo.pig, line 3, column 7> pig script failed to validate: org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1128: Cannot find field owner_id in :tuple(name:chararray,owner_id:chararray)
{noformat}

The seems related to the fact that schema-wise, a bag can only contain a single tuple - but that tuple can then contain any number of items.  Apparently this is only a hard requirement in 0.9 or later, but I tested it up to trunk so it doesn't look like it's going anywhere.

In practice, however, getNext doesn't actually return this 'container' tuple.  If you do you get casting errors.

I'm not really sure how we can fix this, and finding other examples of LoadMetadata implemented with bags are hard to come by.

;;;","31/Jan/12 20:58;brandon.williams;I resolved PIG-2485 as invalid.  You can read the explanation there, but I'll go ahead and summarize: a bag's schema can only contain one tuple because it is assumed that all tuples in the bag have the same schema.  Obviously this won't be true in Cassandra since we allow any column to have any schema that you like.  However, after talking with Dmitriy Ryaboy, I have a plan.  We got good results out of tuple-of-tuples, but this won't work with wide rows.  Another thing it won't work with is small rows where some columns have metadata, and some do not, because when you define a tuple-of-tuples that is a hard constraint; you can't define 4 and then return 20.  So what I propose is that we change the output format to be a tuple-of-tuples for all columns that have metadata, and then a bag with the rest of the columns with a single schema (the default comparator/validator.)  This will work for both static and wide rows, unless you manage to define metadata on so many columns in a wide row that they themselves qualify as wide.

To give an example, let's continue with what Pete started with a slight modification:
{noformat}
create column family PhotoVotes with
comparator = UTF8Type and
column_metadata =
[
{column_name: voter, validation_class: UTF8Type, index_type: KEYS},
{column_name: vote_type, validation_class: UTF8Type},
{column_name: photo_owner, validation_class: UTF8Type, index_type: KEYS},
{column_name: src_big, validation_class: UTF8Type},
{column_name: pid, validation_class: UTF8Type, index_type: KEYS},
{column_name: matched_string, validation_class: UTF8Type},
{column_name: time, validation_class: LongType},
];
{noformat}

Loading this from pig produces a schema like:
(key: bytearray,matched_string: (name: chararray,value: chararray),photo_owner: (name: chararray,value: chararray),pid: (name: chararray,value: chararray),src_big: (name: chararray,value: chararray),time: (name: chararray,value: long),vote_type: (name: chararray,value: chararray),voter: (name: chararray,value: chararray),columns: {(name: chararray,value: bytearray)})

This should allow you do things like:

FILTER rows by vote_type.value eq 'album_like'

Note that the *tuple* is named after the index, and inside the tuple we still have 'name' and 'value'.  This is because if we don't have the name accessible, this is going to be hard to store later (and schema introspection is a bit more magic than I'd care to use.);;;","31/Jan/12 20:59;brandon.williams;v5 implements this schema.  No work on the putNext side of things yet since I haven't quite decided how to handle that.;;;","31/Jan/12 22:18;brandon.williams;Rebased v5 after CASSANDRA-3251;;;","08/Feb/12 10:35;jalkanen;This issue also affects 1.0.7. Banged my head against the wall for an hour or so before I found this issue. The patch does not apply cleanly on 1.0.7 either :-/;;;","08/Feb/12 14:01;brandon.williams;New patches build upon v5 and adds output storage to match.  The catch, however, is that either the tuples (indexed/validated columns) or the bag (unknown columns) are optional, so if you are dealing with narrow output, you can just make a tuple like (key, (name,value), (name,value)) and that will work.;;;","13/Feb/12 21:37;brandon.williams;v6 is rebased and contains minor cleanups, smoke_test contains a file to be replayed by the cli and a pig script to exercise loading/storing every cassandra type.;;;","13/Feb/12 22:58;xedin;+1 on the v6 with cleanup patch attached - replaced ArrayList, HashMap with interfaces, added generic description to reader/writer so no more blind casts, changed thrift {Super}Column to user setX(...) methods and removed whitespaces. ;;;","13/Feb/12 23:45;brandon.williams;Committed w/cleanup patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deflate Compression corrupts SSTables,CASSANDRA-3370,12527375,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,christianmovi,christianmovi,16/Oct/11 23:04,16/Apr/19 09:32,14/Jul/23 05:52,20/Oct/11 13:35,1.0.1,,,,,,0,compression,,,"Hi,

it seems that the Deflate Compressor corrupts the SSTables. 3 out of 3 Installations were corrupt. Snappy works fine.

Here is what I did:

1. Start a single cassandra node (I was using ByteOrderedPartitioner)
2. Write data into cf that uses deflate compression - I think it has to be enough data so that the data folder contains some files.
3. When I now try to read (I did a range scan) from my application, it fails and the logs show corruptions:

Caused by: org.apache.cassandra.io.compress.CorruptedBlockException: (/home/cspriegel/Development/cassandra1/data/Test/Response-h-2-Data.db): corruption detected, chunk at 0 of length 65536.

regards,
Christian","Ubuntu Linux, amd64, Cassandra 1.0.0-rc2",stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/11 10:57;slebresne;3370.patch;https://issues.apache.org/jira/secure/attachment/12499845/3370.patch","19/Oct/11 21:33;christianmovi;Test.zip;https://issues.apache.org/jira/secure/attachment/12499765/Test.zip","16/Oct/11 23:06;christianmovi;system.log;https://issues.apache.org/jira/secure/attachment/12499227/system.log",,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,87538,,,Thu Oct 20 13:35:22 UTC 2011,,,,,,,,,,"0|i0gj27:",94518,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"16/Oct/11 23:06;christianmovi;Attached system.log of broken installation;;;","18/Oct/11 15:15;slebresne;The exception is raised because there is a digest mismatch for the initial block of one of the sstable.

Haven't been able to reproduce so far using stress with deflate with the default 1M keys (which create a bunch of sstables, at least on my machine) using row slices and range scans (using both Random and ByteOrdered partitioners).

Would you be able to 1) try with 1.0.0 and 2) try with the stress tool that comes with Cassandra (it's in tools/stress of the source distribution, and you'll want to insert values with 'stress -I DeflateCompressor' and read with 'stress -I DeflateCompressor -o RANGE_SLICE' ) and see if you can reproduce? Another question is, did you used openJDK or Sun JDK?;;;","18/Oct/11 15:21;christianmovi;My Java version is: 
java version ""1.6.0_26""
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)

Cassandra report during startup:
INFO 17:20:39,113 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_26

I will test tonight ...;;;","18/Oct/11 20:33;christianmovi;I tested again with 1.0.0. Unfortunetaly the problem still exists.

But I think I was able to narrow it down: It seems that the problem only occurs when I insert large byte-arrays.

It seems to work fine with 10kb arrays, no problem there. I was able to repeatedly insert and read.

With 100kb or 200kb arrays it crashes after about 1000-2000 insertions. (Insertions work, but range scan afterwards crashes)




;;;","18/Oct/11 20:42;christianmovi;btw: I just tested the DeflaterOutputStream/DeflaterInputStream classes in a small testcase and there it works fine. I thought maybe Deflate in my jvm is broken.;;;","19/Oct/11 12:48;slebresne;Still cannot reproduce. I've tried multiple times inserting 5000 keys using the stress tool using values of 10KB, 20KB, 100KB and 200KB (using 'stress -I DeflateCompressor -S 200000 -n 5000'). I then try to reading both with 'stress -o RANGE_SLICE -n 5000' and by simply fetching the 100 first keys using the CLI (with a simple 'list Standard1;') and got no exceptions (the actual listing in the CLI took a while to be printed on screen because the columns are big but outside of that, no errors).

Would you mind trying the same experiment (with the same tools) or providing the test script you're using so we can check if it has to do with the specific insertions or with something in your environment.
;;;","19/Oct/11 15:18;christianmovi;Ok, I will try the stress tool.

Just to be sure: You are talking about stress.py and not the java-based stress? Because I was trying the java stress and it did not accept the -I parameter.
;;;","19/Oct/11 15:23;slebresne;No, I'm talking of the java one. The python one is old and won't support compression for instance. The java one in the 1.0.0 source does support compression through the -I parameter. Are you sure you're looking at the right version ? ;;;","19/Oct/11 15:33;christianmovi;I see! I got the wrong version. Sorry, I did not know that it was included in the cassandra source. I thought I had to download it some place else.

I will try with that and let you know about the results...;;;","19/Oct/11 20:27;christianmovi;I tried using stress from 1.0.0 and I got the same results as you. Stress for some reason works fine. 

One thing is strange about stress:
I let stress run for quite some time, but there is only 12 MB in the datafolder.
I let my tool run for 10 seconds, but there are 97MB in the data folder.

Is stress maybe not generating random data, so that it compresses really well? Might that be the difference?

Can I maybe share my application with you? Its a single source file with a pom.xml. 

If you have any idea what I can do, please let me know.

;;;","19/Oct/11 21:33;christianmovi;Attached my client that causes the crash.;;;","20/Oct/11 10:57;slebresne;You test did help. Turns out that's because you're inserting random and thus basically uncompressible data, and the compressed data was bigger than the uncompressed one. The code is supposed to handle that but there is a bug in that part.

Patch attached to fix.;;;","20/Oct/11 11:02;christianmovi;Great! This would also apply if some app would insert already compressed data.;;;","20/Oct/11 13:20;xedin;+1;;;","20/Oct/11 13:35;slebresne;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"AssertionError when adding a node and doing repair, repair hangs",CASSANDRA-3369,12527360,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,christianmovi,christianmovi,16/Oct/11 16:35,16/Apr/19 09:32,14/Jul/23 05:52,21/Oct/11 13:33,0.8.8,1.0.1,,,,,0,,,,"Hi again,

I was playing aroung with Cassandra 1.0.0-rc2 and got an AssertionError. The cluster I set up was two cassandra nodes on one laptop using different 127.0.0.* loopback devices. Both nodes have separate folders on the harddisk.


Here is what I did:


1. Started node1 and inserted some data into it using a simple singlethreaded testprogram (uses hector 0.8.0-2):
127.0.0.1       datacenter1 rack1       Up     Normal  583.55 MB       100.00% Token(bytes[63e5b6995466cd3221cba16646ae19ed])

2. I started another node, node 2 = 127.0.0.2:
127.0.0.2       datacenter1 rack1       Up     Normal  147.57 KB       50.00%  Token(bytes[4d6ccfeaa8bb59551751a2816fde9343])
127.0.0.1       datacenter1 rack1       Up     Normal  583.55 MB       50.00%  Token(bytes[63e5b6995466cd3221cba16646ae19ed])

3. I triggered a ""nodetool -h 127.0.0.1  repair"" on the first node that had the data from my test.


This repair does not seem to ever end. The nodetool is hanging now but my computer is idle. I get an AssertionError on the first node:
java.lang.AssertionError
	at org.apache.cassandra.service.AntiEntropyService$Validator.prepare(AntiEntropyService.java:283)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:825)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:63)
	at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:432)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)


Update: I dont know if it is important but here is the schema of my test:
create keyspace Test with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = {replication_factor:2};
CREATE COLUMN FAMILY Response WITH key_validation_class=BytesType AND compression_options={sstable_compression:DeflateCompressor};


kind regards,
Christian","Ubuntu Linux amd64, 1.0.0-rc2",stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/11 10:23;slebresne;3369.patch;https://issues.apache.org/jira/secure/attachment/12499528/3369.patch","16/Oct/11 16:38;christianmovi;NODE1_cassandra.yaml;https://issues.apache.org/jira/secure/attachment/12499204/NODE1_cassandra.yaml","16/Oct/11 16:38;christianmovi;NODE1_system.log;https://issues.apache.org/jira/secure/attachment/12499205/NODE1_system.log","16/Oct/11 16:38;christianmovi;NODE2_cassandra.yaml;https://issues.apache.org/jira/secure/attachment/12499206/NODE2_cassandra.yaml","16/Oct/11 16:38;christianmovi;NODE2_system.log;https://issues.apache.org/jira/secure/attachment/12499207/NODE2_system.log",,,,,,,,,,5.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,87508,,,Fri Oct 21 16:00:27 UTC 2011,,,,,,,,,,"0|i0gj1r:",94516,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"16/Oct/11 16:38;christianmovi;Added config and log files from both cassandra nodes.;;;","16/Oct/11 23:09;christianmovi;Update: I also had problems because I was using Defalte compression, which seems to be broken. The AssertionError during repair still occurs with Snappy Compression.;;;","18/Oct/11 10:23;slebresne;This is due to the code not handling correctly a case of having no sample keys for one of it's token range.

Let's not that:
  * this is not specific to 1.0.0, 0.8 is affected too.
  * this only affect order preserving partitioner (not because the order matter but because this code path is not taken with RandomPartitioner since a few releases now, at least as far as repair is concerned).
  * this would be unlikely to show in any serious production setting, since having no sample for a given range means that either you have almost no rows in the cluster or that the cluster is extremely badly balanced.

Patch attached to fix (against 0.8).;;;","20/Oct/11 21:54;jbellis;+1;;;","21/Oct/11 13:33;slebresne;Committed;;;","21/Oct/11 16:00;hudson;Integrated in Cassandra-0.8 #384 (See [https://builds.apache.org/job/Cassandra-0.8/384/])
    fix assertion error during repair with ordered partitioners
patch by slebresne; reviewed by jbellis for CASSANDRA-3369

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1187333
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AntiEntropyService.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'show schema' in cli does not show compression_options,CASSANDRA-3368,12527358,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,christianmovi,christianmovi,16/Oct/11 16:09,16/Apr/19 09:32,14/Jul/23 05:52,16/Oct/11 22:43,1.0.1,,,Legacy/Tools,,,0,,,,"Hi,

using the cassandra-cli command line tool, I realized that a 'show schema' does not print out the compression_options I specified when creating them.
Both, the server and the cli tool, where version 1.0.0-rc2.


Example:

[default@Test] CREATE COLUMN FAMILY Response2 WITH key_validation_class=BytesType AND compression_options {sstable_compression:DeflateCompressor};


[default@Test] show schema;
create keyspace Test
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = [{replication_factor : 2}];
use Test;
create column family Response2
  with column_type = 'Standard'
  and comparator = 'BytesType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'BytesType'
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and keys_cached = 200000.0
  and key_cache_save_period = 14400
  and read_repair_chance = 1.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'ConcurrentLinkedHashCacheProvider';


Not really critical, but might be confusing for some people = me ;-)

kind regards,
Christian",Ubuntu Linux amd64,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/11 21:35;xedin;CASSANDRA-3368.patch;https://issues.apache.org/jira/secure/attachment/12499224/CASSANDRA-3368.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,87501,,,Sun Oct 16 22:43:59 UTC 2011,,,,,,,,,,"0|i0gj1b:",94514,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"16/Oct/11 21:36;xedin;rebased with the latest cassandra-1.0 branch (last commit 757e1273c9ea7fd836b7a5cac03f1e7e8128f68b);;;","16/Oct/11 22:39;brandon.williams;+1;;;","16/Oct/11 22:43;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[patch] use long math, if you want long results",CASSANDRA-3364,12527307,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,dbrosius@apache.org,dbrosius@apache.org,15/Oct/11 17:31,16/Apr/19 09:32,14/Jul/23 05:52,17/Oct/11 17:19,1.0.1,,,,,,0,,,,"Code calculates long values, using integer intermediate input, which can cause truncation errors, safer just to use long input.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/11 17:31;dbrosius@apache.org;use_long_math.diff;https://issues.apache.org/jira/secure/attachment/12499153/use_long_math.diff",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,86714,,,Mon Oct 17 17:19:26 UTC 2011,,,,,,,,,,"0|i0gizb:",94505,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"17/Oct/11 17:19;brandon.williams;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2GB row size limit in ColumnIndex offset calculation,CASSANDRA-3358,12527029,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tho,tho,tho,13/Oct/11 14:36,16/Apr/19 09:32,14/Jul/23 05:52,13/Oct/11 17:34,0.7.10,0.8.8,1.0.1,,,,0,,,,"Index offset is calculated using int instead of long resulting in overflow at 2GB row size. As a result affected columns can not be retrieved. 

Fix: use long instead of int",,tho,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,tho,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,85391,,,Thu Oct 13 17:40:42 UTC 2011,,,,,,,,,,"0|i0giwv:",94494,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"13/Oct/11 15:46;jbellis;Weird -- you're absolutely right, but I could have sworn we fixed this already. :);;;","13/Oct/11 17:34;jbellis;committed the proposed fix, thanks!;;;","13/Oct/11 17:40;hudson;Integrated in Cassandra-0.7 #554 (See [https://builds.apache.org/job/Cassandra-0.7/554/])
    fix ColumnIndexer to use long offsets
patch by Thomas Richter; reviewed by jbellis for CASSANDRA-3358

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1183000
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/ColumnIndexer.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableImport/Export don't handle tombstone well if value validation != BytesType,CASSANDRA-3357,12527002,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,13/Oct/11 10:46,16/Apr/19 09:32,14/Jul/23 05:52,13/Oct/11 18:50,0.8.8,1.0.1,,Legacy/Tools,,,0,,,,"SSTableImport/Export use the value validator even on tomstone, but for those the value is the local deletion time, so this don't necessarily validate (with UTF8Type for instance)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/11 10:47;slebresne;3357.patch;https://issues.apache.org/jira/secure/attachment/12498866/3357.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,85332,,,Thu Oct 13 18:50:28 UTC 2011,,,,,,,,,,"0|i0giwf:",94492,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"13/Oct/11 15:39;jbellis;+1;;;","13/Oct/11 17:22;hudson;Integrated in Cassandra-0.8 #369 (See [https://builds.apache.org/job/Cassandra-0.8/369/])
    Fix handling of tombstone by SSTableExport/Import when validation != BytesType
patch by slebresne; reviewed by jbellis for CASSANDRA-3357

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1182950
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/SSTableExport.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/SSTableImport.java
;;;","13/Oct/11 18:50;slebresne;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add more data type checks when storing data from Pig to Cassandra,CASSANDRA-3356,12526977,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,jeromatron,jeromatron,13/Oct/11 05:21,16/Apr/19 09:32,14/Jul/23 05:52,22/Feb/12 01:21,,,,,,,0,pig,,,"Pre-CASSANDRA-2810, the decompose method was tried before assuming the data was of type DataByteArray.  It needs to have both - first a check to see if it can be decomposed.  If that doesn't work (exception) it can try the cast.  If that doesn't work, then we can't store the object - or as Brandon says, we need to put another special case in ObjToBB.  This is all based on a conversation in IRC with Brandon Williams, Jacob Perkins, and Jeremy Hanna about a problem arising when trying to store an object of type long to Cassandra.",,brandon.williams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,85290,,,Wed Feb 22 01:21:40 UTC 2012,,,,,,,,,,"0|i0givz:",94490,,,,,Low,,,,,,,,,,,,,,,,,"22/Feb/12 01:21;brandon.williams;Think I got this one in CASSANDRA-3886.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
misc CQL doc fixes,CASSANDRA-3353,12526923,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,12/Oct/11 20:16,16/Apr/19 09:32,14/Jul/23 05:52,13/Oct/11 22:09,1.0.1,,,Legacy/Documentation and Website,,,0,cql,,,See patch to follow for some minor documentation fixes (mostly formatting nits).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/11 20:17;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3353-minor-formatting-fixes.txt;https://issues.apache.org/jira/secure/attachment/12498811/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3353-minor-formatting-fixes.txt","12/Oct/11 20:17;urandom;ASF.LICENSE.NOT.GRANTED--v1-0002-bring-term-info-current-w-recent-changes.txt;https://issues.apache.org/jira/secure/attachment/12498812/ASF.LICENSE.NOT.GRANTED--v1-0002-bring-term-info-current-w-recent-changes.txt",,,,,,,,,,,,,2.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,76186,,,Thu Oct 13 23:21:54 UTC 2011,,,,,,,,,,"0|i0giuv:",94485,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"12/Oct/11 20:20;jbellis;+1;;;","13/Oct/11 22:09;urandom;committed.;;;","13/Oct/11 23:21;hudson;Integrated in Cassandra #1157 (See [https://builds.apache.org/job/Cassandra/1157/])
    bring term info current w/ recent changes

Patch by eevans; reviewed by jbellis for CASSANDRA-3353
minor formatting fixes to doc

Patch by eevans; reviewed by jbellis for CASSANDRA-3353

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1183135
Files : 
* /cassandra/trunk/doc/cql/CQL.textile

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1183134
Files : 
* /cassandra/trunk/doc/cql/CQL.textile
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
If node fails to join a ring it will stay in joining state indefinately,CASSANDRA-3351,12526780,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,tuke,tuke,12/Oct/11 00:34,16/Apr/19 09:32,14/Jul/23 05:52,21/Oct/11 22:26,0.8.8,1.0.1,,,,,0,gossip,,,"While attempting to add a new node to my ring something went wrong and I had to terminate the node on ec2. After this the node keeps appearing in the ring command in ""joining"" state and never goes away. Per driftx on the Cassandra channel if I do a whole cluster restart it should go away, but since this is a production system this is not really possible. Additionally if I could join a node with same IP again this should go away, but being on ec2 this is not always easy. So not sure if this truly qualifies as a bug or more like a feature request, but I feel there should be a way to remove a node in any state if I wish without joining a node with same ip or doing a whole cluster restart.","xlarge ec2, Ubuntu 11.04 Natty, JNA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/11 17:41;brandon.williams;3351-trunk.txt;https://issues.apache.org/jira/secure/attachment/12499899/3351-trunk.txt","20/Oct/11 17:37;brandon.williams;3351.txt;https://issues.apache.org/jira/secure/attachment/12499898/3351.txt","12/Oct/11 00:37;tuke;cassandra.log.gz;https://issues.apache.org/jira/secure/attachment/12498689/cassandra.log.gz",,,,,,,,,,,,3.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,72542,,,Sat Oct 22 01:00:20 UTC 2011,,,,,,,,,,"0|i0gitz:",94481,,thepaul,,thepaul,Low,,,,,,,,,,,,,,,,,"12/Oct/11 00:37;tuke;Also per driftx I ran gms trace and grabbed about 1 minute worth of log entries that I have attached here. Problem node ip is 10.82.211.8;;;","20/Oct/11 17:37;brandon.williams;Two problems introduced in CASSANDRA-2496: the fatclient logic was changed a bit too much, and the isDeadState check was assuming fat clients had dead state, thus calling setHasToken to effectively not mark  them as fat clients.;;;","21/Oct/11 22:13;thepaul;+1;;;","21/Oct/11 22:26;brandon.williams;Committed.;;;","22/Oct/11 01:00;hudson;Integrated in Cassandra-0.8 #386 (See [https://builds.apache.org/job/Cassandra-0.8/386/])
    Prevent nodes that failed to join from being stuck in the joining state
indefinitely.
Patch by brandonwilliams, reviewed by Paul Cannon for CASSANDRA-3351

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1187578
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't USE numeric keyspace names in CQL,CASSANDRA-3350,12526768,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,thepaul,thepaul,11/Oct/11 22:10,16/Apr/19 09:32,14/Jul/23 05:52,12/Oct/11 14:54,0.8.8,1.0.1,,,,,0,lhf,,,"Cassandra allows keyspace names to start with a digit or an underscore (see o.a.c.db.migration.Migration.isLegalName), but CQL's {{USE}} statement only accepts a CQL identifier, which must start with a letter. So there's no way to use a keyspace named ""142"" or ""\_hi\_"" in CQL, for example.

The {{USE}} statement should accept string literals and integers as well as identifiers, and CQL identifiers ({{IDENT}}) should probably allow starting with the underscore.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/11 10:35;xedin;CASSANDRA-3350.patch;https://issues.apache.org/jira/secure/attachment/12498728/CASSANDRA-3350.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,72419,,,Wed Oct 12 15:22:45 UTC 2011,,,,,,,,,,"0|i0gitj:",94479,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"12/Oct/11 13:09;jbellis;+1;;;","12/Oct/11 14:54;jbellis;committed;;;","12/Oct/11 15:22;hudson;Integrated in Cassandra-0.8 #367 (See [https://builds.apache.org/job/Cassandra-0.8/367/])
    allow numeric keyspace names in USE statement
patch by pyaskevich; reviewed by jbellis for CASSANDRA-3350

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1182413
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/Cql.g
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE on malformed CQL,CASSANDRA-3349,12526757,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,thepaul,thepaul,11/Oct/11 21:17,16/Apr/19 09:32,14/Jul/23 05:52,12/Oct/11 14:53,0.8.8,1.0.1,,,,,0,lhf,,,"It's not clear why, but the CQL grammar specification in Cql.g allows for an empty WHERE clause on DELETE, i.e.:

{noformat}
DELETE FROM someCF WHERE;
{noformat}

When this is used, with or without a column list, it causes an NPE on the node processing the CQL. Traceback on a recent 1.0.0 build:

{noformat}
ERROR [pool-2-thread-1] 2011-10-11 15:45:25,655 Cassandra.java (line 4082) Internal error processing execute_cql_query
java.lang.NullPointerException
        at org.apache.cassandra.cql.CqlParser.deleteStatement(CqlParser.java:1994)
        at org.apache.cassandra.cql.CqlParser.query(CqlParser.java:292)
        at org.apache.cassandra.cql.QueryProcessor.getStatement(QueryProcessor.java:984)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:500)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1268)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{noformat}

The CQL client gets an error with the message, ""Internal application error"".

It might be better to allow leaving off the ""WHERE"" as well as the condition, to match SQL semantics, although fixing that probably won't solve this problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/11 10:24;xedin;CASSANDRA-3349.patch;https://issues.apache.org/jira/secure/attachment/12498727/CASSANDRA-3349.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,59409,,,Wed Oct 12 15:22:45 UTC 2011,,,,,,,,,,"0|i0gitb:",94478,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"12/Oct/11 01:50;jbellis;We don't want to allow leaving it off, because we don't support scan-and-delete server-side. (And truncate is far more performant.)

I'd rather fix by requiring the rest of the WHERE.;;;","12/Oct/11 10:25;xedin;rebased with the latest 0.8 branch (last commit 9b9c4d32973ea4e586031775b3322180169135cd);;;","12/Oct/11 13:07;jbellis;+1;;;","12/Oct/11 14:53;jbellis;committed;;;","12/Oct/11 15:22;hudson;Integrated in Cassandra-0.8 #367 (See [https://builds.apache.org/job/Cassandra-0.8/367/])
    update CQL grammar to require key clause in delete statement
patch by pyaskevich; reviewed by jbellis for CASSANDRA-3349

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1182411
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/Cql.g
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HsHa broken at startup,CASSANDRA-3346,12526665,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,11/Oct/11 14:12,16/Apr/19 09:32,14/Jul/23 05:52,11/Oct/11 14:51,1.0.0,,,,,,0,,,,"{noformat}

ERROR 09:10:21,781 Exception encountered during startup
java.lang.IllegalArgumentException
        at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:589)
        at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:514)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.<init>(DebuggableThreadPoolExecutor.java:90)
        at org.apache.cassandra.concurrent.JMXEnabledThreadPoolExecutor.<init>(JMXEnabledThreadPoolExecutor.java:76)
        at org.apache.cassandra.thrift.CassandraDaemon$ThriftServer.<init>(CassandraDaemon.java:192)
        at org.apache.cassandra.thrift.CassandraDaemon.startServer(CassandraDaemon.java:75)
        at org.apache.cassandra.service.AbstractCassandraDaemon.startRPCServer(AbstractCassandraDaemon.java:281)
        at org.apache.cassandra.service.AbstractCassandraDaemon.start(AbstractCassandraDaemon.java:253)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:350)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,59135,,,Tue Oct 11 14:51:41 UTC 2011,,,,,,,,,,"0|i0gis7:",94473,,,,,Normal,,,,,,,,,,,,,,,,,"11/Oct/11 14:51;brandon.williams;Stupid mistake fixed in r1181816;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair still streams unnecessary sstables,CASSANDRA-3345,12526654,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,11/Oct/11 13:07,16/Apr/19 09:32,14/Jul/23 05:52,11/Oct/11 15:12,1.0.0,,,,,,0,repair,,,"Through rebases, CASSANDRA-2610 unfortunately got committed with the use of the wrong streaming method, the one that stream all the sstables of the keyspace.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/11 13:08;slebresne;3345.patch;https://issues.apache.org/jira/secure/attachment/12498590/3345.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,59123,,,Tue Oct 11 15:12:04 UTC 2011,,,,,,,,,,"0|i0girr:",94471,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"11/Oct/11 13:08;slebresne;That sucks, sorry. Patch attached.;;;","11/Oct/11 14:56;brandon.williams;+1;;;","11/Oct/11 15:12;slebresne;Committed for the 1.0.0 reroll;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction throttling can be too slow,CASSANDRA-3344,12526640,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,frousseau,frousseau,11/Oct/11 10:29,16/Apr/19 09:32,14/Jul/23 05:52,14/Oct/11 08:55,0.8.8,1.0.1,,,,,0,,,,"Compaction throttling needs to know how many active compactions are running (to divide bandwith for each active compaction).

The way active compaction is counted can be broken because it counts the number of active threads in the executor BUT the thread starts by acquiring a lock.
If the lock can't be acquired immediately : the thread is seen as ""active"" but does not participate in IO operations.
The case can happen when major compaction are triggered (major compaction acquire a write lock, while minor compactions acquire a read lock).

Having compaction througput to 16Mb/s, we observed is the following  (two times) :
 - only 1 active compaction (a long one for a few hours) starting at 16Mb/s, then after some time running at 2Mb/s, thus taking a very long time to complete
 - many pending compactions

Using JMX and monitoring the stack trace of the compaction threads showed that :
 - 1 thread was effectively compacting
 - 1 thread was waiting to acquire the write lock (due to a major compaction)
 - 6 threads were waiting to acquire the read lock (probably due to the thread above trying to acquire the write lock)

Attached is a proposed patch (very simple, not yet tested) which counts only active compactions.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/11 10:31;frousseau;001-CASSANDRA-3344.patch;https://issues.apache.org/jira/secure/attachment/12498568/001-CASSANDRA-3344.patch","11/Oct/11 12:09;slebresne;3344.patch;https://issues.apache.org/jira/secure/attachment/12498586/3344.patch","11/Oct/11 12:27;slebresne;3344_v2.patch;https://issues.apache.org/jira/secure/attachment/12498587/3344_v2.patch",,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,59005,,,Fri Oct 14 09:21:48 UTC 2011,,,,,,,,,,"0|i0girb:",94469,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"11/Oct/11 11:50;slebresne;Good diagnostic, I think you are right that this can happen. However the patch seems to replace the body of getActiveCompactions() by a recursive call to itself.;;;","11/Oct/11 12:09;slebresne;Attaching a simple patch to add a counter that we only increment once we really do compaction.;;;","11/Oct/11 12:19;frousseau;Thanks for catching that.
The intent was to return the size of the CompactionExecutor.compactions list
;;;","11/Oct/11 12:27;slebresne;Right, that's indeed even simpler. Attaching v2 to do that instead.;;;","14/Oct/11 02:18;jbellis;+1 v2;;;","14/Oct/11 08:55;slebresne;Committed, thanks;;;","14/Oct/11 09:21;hudson;Integrated in Cassandra-0.8 #372 (See [https://builds.apache.org/job/Cassandra-0.8/372/])
    Only count compaction as active (for throttling) once the compaction lock has been acquired.
patch by Fabien Rousseau and slebresne; reviewed by jbellis for CASSANDRA-3344

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1183241
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool printing classpath,CASSANDRA-3343,12526553,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,cdaw,cdaw,11/Oct/11 03:31,16/Apr/19 09:32,14/Jul/23 05:52,11/Oct/11 12:13,1.0.0,,,,,,0,,,,"* Get file from: [https://repository.apache.org/content/repositories/orgapachecassandra-046/org/apache/cassandra/apache-cassandra/1.0.0/apache-cassandra-1.0.0-bin.tar.gz]
* Install C* and start server
* Run: nodetool -h localhost ring

{code}
Cathy-Daws-MacBook-Pro:bin cathy$ ./nodetool -h localhost ring

./../conf:./../build/classes/main:./../build/classes/thrift:./../lib/antlr-3.2.jar:./../lib/apache-cassandra-1.0.0.jar:./../lib/apache-cassandra-clientutil-1.0.0.jar:./../lib/apache-cassandra-thrift-1.0.0.jar:./../lib/avro-1.4.0-fixes.jar:./../lib/avro-1.4.0-sources-fixes.jar:./../lib/commons-cli-1.1.jar:./../lib/commons-codec-1.2.jar:./../lib/commons-lang-2.4.jar:./../lib/compress-lzf-0.8.4.jar:./../lib/concurrentlinkedhashmap-lru-1.2.jar:./../lib/guava-r08.jar:./../lib/high-scale-lib-1.1.2.jar:./../lib/jackson-core-asl-1.4.0.jar:./../lib/jackson-mapper-asl-1.4.0.jar:./../lib/jamm-0.2.5.jar:./../lib/jline-0.9.94.jar:./../lib/json-simple-1.1.jar:./../lib/libthrift-0.6.jar:./../lib/log4j-1.2.16.jar:./../lib/servlet-api-2.5-20081211.jar:./../lib/slf4j-api-1.6.1.jar:./../lib/slf4j-log4j12-1.6.1.jar:./../lib/snakeyaml-1.6.jar:./../lib/snappy-java-1.0.3.jar
Address         DC          Rack        Status State   Load            Owns    Token                                       
127.0.0.1       datacenter1 rack1       Up     Normal  8.91 KB         100.00% 10597065753338857570408052040129979696      
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,57031,,,Tue Oct 11 12:13:31 UTC 2011,,,,,,,,,,"0|i0giqv:",94467,,,,,Low,,,,,,,,,,,,,,,,,"11/Oct/11 04:05;brandon.williams;Looks like this snuck into cassandra.in.sh through CASSANDRA-3311 (accidental?) which is the wrong place to do this if we do want to print the classpath.;;;","11/Oct/11 12:13;slebresne;That's clearly a mistake. Corrected as r1181741.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli allows setting min_compaction_threshold to 1,CASSANDRA-3342,12526542,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,amnorvend,amnorvend,11/Oct/11 01:01,16/Apr/19 09:32,14/Jul/23 05:52,14/Oct/11 21:38,0.8.8,1.0.1,,Legacy/Tools,,,0,,,,"{{
[root@Apture] update column family MagicLinks with min_compaction_threshold=1 and max_compaction_threshold=20;
b98e3b80-f3a3-11e0-0000-76abb4a6dbbf
Waiting for schema agreement...
... schemas agree across the cluster
}}

I'm told that a min_compaction_threshold of 1 is nonsensical.  I had a spell where my servers stopped doing compactions.  Once I upped the min_compaction_threshold, they started compacting again.  I'm unable to confirm for sure that this was the case.","Linux 2.6.32-131.6.1.el6.x86_64 #1 SMP Mon Jun 20 14:15:38 EDT 2011 x86_64 x86_64 x86_64 GNU/Linux (RHEL 6)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/11 21:23;xedin;CASSANDRA-3342.patch;https://issues.apache.org/jira/secure/attachment/12499100/CASSANDRA-3342.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,57014,,,Fri Oct 14 22:15:27 UTC 2011,,,,,,,,,,"0|i0giqf:",94465,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"11/Oct/11 01:05;brandon.williams;ISTM the easiest thing to do here is actually use 2 when a user specifies 1.;;;","14/Oct/11 02:22;jbellis;I'd rather raise an error server-side, than silently not do what was requested;;;","14/Oct/11 21:26;brandon.williams;+1 with minor typo fixes: s/then/than/ and s/greather/greater/;;;","14/Oct/11 21:38;xedin;Committed.;;;","14/Oct/11 22:15;hudson;Integrated in Cassandra-0.8 #374 (See [https://builds.apache.org/job/Cassandra-0.8/374/])
    ColumnFamily min_compaction_threshold should be >= 2
patch by Pavel Yaskevich; reviewed by Brandon Williams for CASSANDRA-3342

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1183510
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix for Build Error in contrib/pig to accomodate refactoring of hexToBytes,CASSANDRA-3341,12526465,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,boneill,boneill,10/Oct/11 16:25,16/Apr/19 09:32,14/Jul/23 05:52,12/Oct/11 16:21,,,,,,,0,,,,"hexToBytes moved to Hex from FBUtilities.
Need to update contrib/pig to accomodate that move.",linux using contrib/pig on trunk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/11 16:27;boneill;trunk-3341.txt;https://issues.apache.org/jira/secure/attachment/12498435/trunk-3341.txt",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,56852,,,Wed Oct 12 16:21:34 UTC 2011,,,,,,,,,,"0|i0gipz:",94463,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"10/Oct/11 16:27;boneill;patch for contrib/pig;;;","12/Oct/11 16:21;brandon.williams;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Uncompressed sizes are used to estimate space for compaction of compressed sstables,CASSANDRA-3338,12526414,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,10/Oct/11 08:30,16/Apr/19 09:32,14/Jul/23 05:52,10/Oct/11 14:00,1.0.0,,,,,,0,compression,,,We are using the uncompressed data size when estimating if we have enough to compact sstables. This means we can easily refuse compaction when there is clearly enough room to compact.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/11 13:35;slebresne;3338-v2.patch;https://issues.apache.org/jira/secure/attachment/12498419/3338-v2.patch","10/Oct/11 08:42;slebresne;3338.patch;https://issues.apache.org/jira/secure/attachment/12498400/3338.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,55861,,,Mon Oct 10 14:00:31 UTC 2011,,,,,,,,,,"0|i0gion:",94457,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"10/Oct/11 12:59;jbellis;maybe we should rename .length() to uncompressedLength() as well?

otherwise +1;;;","10/Oct/11 13:35;slebresne;Actually renaming length() to uncompressedLength() proved to be a good idea, as I had missed quite a bunch of places where the onDiskLength should be used. Attached v2.;;;","10/Oct/11 13:46;jbellis;+1;;;","10/Oct/11 14:00;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ThreadPoolExecutor creates threads as non-daemon and will block on shutdown by default,CASSANDRA-3335,12526280,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,brandon.williams,brandon.williams,07/Oct/11 21:40,16/Apr/19 09:32,14/Jul/23 05:52,20/Dec/11 20:41,1.0.7,,,,,,0,,,,"This is most obviously visible in OptionalTasks which should not block shutdown, but often does.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/11 22:31;jbellis;3335-v2.txt;https://issues.apache.org/jira/secure/attachment/12499105/3335-v2.txt","10/Dec/11 19:29;jbellis;3335-v3.txt;https://issues.apache.org/jira/secure/attachment/12506864/3335-v3.txt","16/Dec/11 04:47;jbellis;3335-v4.txt;https://issues.apache.org/jira/secure/attachment/12507648/3335-v4.txt","07/Oct/11 22:00;jbellis;3335.txt;https://issues.apache.org/jira/secure/attachment/12498247/3335.txt","12/Dec/11 19:41;brandon.williams;3335v3_jstack.txt;https://issues.apache.org/jira/secure/attachment/12507042/3335v3_jstack.txt",,,,,,,,,,5.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,50529,,,Tue Dec 20 20:41:22 UTC 2011,,,,,,,,,,"0|i0ginb:",94451,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"07/Oct/11 22:00;jbellis;Patch that switches DTPE to daemon threads, and Memtable's jamm TPE to DTPE. The only other direct use of TPE is the thrift handler for sync mode in ACD, which doesn't seem to have a problem killing things off when we ask it to so I left it alone.;;;","10/Oct/11 18:33;brandon.williams;+1 on this patch for being technically correct, but it actually does not help.  What I see is OptionalTasks jump to 100% on control-c, and then eventually it spins down and a few minutes later java finally exits.  During the last bit, I can see jamm still working.  I'm not sure why java is taking so long to exit, but it is responsive to nodetool the entire time, though tpstats shows nothing active.;;;","14/Oct/11 22:31;jbellis;v2 adds a log line before running any scheduled task. Maybe that will help pinpoint the culprit.;;;","14/Oct/11 22:32;jbellis;(all the task logging is done w/ the StorageService logger so it can be enabled separately from the reset of the package involved);;;","01/Dec/11 22:07;brandon.williams;Tracked down to the shutdown hook:

{noformat}
   java.lang.Thread.State: TIMED_WAITING (sleeping)
    at java.lang.Thread.sleep(Native Method)
    at org.apache.cassandra.utils.ExpiringMap.shutdown(ExpiringMap.java:103)
    at org.apache.cassandra.net.MessagingService.shutdown(MessagingService.java:495)
    at org.apache.cassandra.service.StorageService$2.runMayThrow(StorageService.java:426)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    at java.lang.Thread.run(Thread.java:662)
{noformat}

Appears to be the ExpiringMap added in CASSANDRA-2034.;;;","10/Dec/11 19:28;jbellis;Patch to stop adding new messages to the callback map when MessagingService is shutdown.;;;","12/Dec/11 19:41;brandon.williams;Doesn't seem to help.  Thread dump attached.;;;","16/Dec/11 04:47;jbellis;Okay, take four here.  The problem with v3 is that we were only blocking sendOneWay during shutdown, not addCallback, which is the source of the ExpiringMap entries we were waiting for.

As I commented,

{noformat}
    /**
     * There isn't a good way to shut down the MessagingService. One problem (but not the only one)
     * is that StorageProxy has no way to communicate back to clients, ""I'm nominally alive, but I can't
     * send that request to the nodes with your data.""  Neither TimedOut nor Unavailable is appropriate
     * to return in that situation.
     *
     * So instead of shutting down MS and letting StorageProxy/clients cope somehow, we shut down
     * the Thrift service and then wait for all the outstanding requests to finish or timeout.
     */
{noformat}

That part was straightforward.  I also had to make the Thrift shutdown actually work -- we were calling setSoTimeout to attempt to make accept() nonblocking, but ""0"" means ""wait indefinitely"" not ""don't wait at all"".  Then we needed to handle the timeout in the accept loop.

Finally, I did a bunch of cleanup to ExpiringMap and added trace-level logging in case we need to go at this *again*.


;;;","20/Dec/11 18:56;brandon.williams;+1;;;","20/Dec/11 20:41;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dropping index causes some inflight mutations to fail,CASSANDRA-3334,12526275,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,hsn,hsn,07/Oct/11 21:20,16/Apr/19 09:32,14/Jul/23 05:52,20/Oct/11 21:13,1.0.1,,,,,,0,indexing,,,"dropping index causes some inflight mutations to fail. hector on client side didnt throw any exception

 INFO [MigrationStage:1] 2011-10-07 23:11:53,742 Migration.java (line 119) Applying migration fb1a8540-f128-11e0-0000-23b38323f4da Update column family to org.apache.cassandra.config.CFMetaData@786669[cfId=1000,ksName=test,cfName=sipdb,cfType=Standard,comparator=org.apache.cassandra.db.marshal.AsciiType,subcolumncomparator=<null>,comment=phone calls routing information,rowCacheSize=0.0,keyCacheSize=0.0,readRepairChance=0.0,replicateOnWrite=false,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.Int32Type,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@8bb33c,mergeShardsChance=0.1,keyAlias=java.nio.HeapByteBuffer[pos=461 lim=464 cap=466],column_metadata={java.nio.HeapByteBuffer[pos=0 lim=3 cap=3]=ColumnDefinition{name=6b616d, validator=org.apache.cassandra.db.marshal.AsciiType, index_type=null, index_name='null'}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO [MigrationStage:1] 2011-10-07 23:11:53,805 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Migrations@27537043(7860/9825 serialized/live bytes, 1 ops)
 INFO [MigrationStage:1] 2011-10-07 23:11:53,820 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Schema@8340427(3320/4150 serialized/live bytes, 3 ops)
 INFO [FlushWriter:3] 2011-10-07 23:11:53,820 Memtable.java (line 237) Writing Memtable-Migrations@27537043(7860/9825 serialized/live bytes, 1 ops)
 INFO [FlushWriter:3] 2011-10-07 23:11:55,008 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Migrations-h-14-Data.db (7924 bytes)
 INFO [FlushWriter:3] 2011-10-07 23:11:55,008 Memtable.java (line 237) Writing Memtable-Schema@8340427(3320/4150 serialized/live bytes, 3 ops)
 INFO [CompactionExecutor:4] 2011-10-07 23:11:55,008 CompactionTask.java (line 119) Compacting [SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-13-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-14-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-11-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-12-Data.db')]
 INFO [FlushWriter:3] 2011-10-07 23:11:56,430 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Schema-h-14-Data.db (3470 bytes)
 INFO [CompactionExecutor:3] 2011-10-07 23:11:56,446 CompactionTask.java (line 119) Compacting [SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-13-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-14-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-12-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-11-Data.db')]
ERROR [MutationStage:56] 2011-10-07 23:11:56,508 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:56,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:51] 2011-10-07 23:11:56,539 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:51,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:38] 2011-10-07 23:11:56,633 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:38,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:57] 2011-10-07 23:11:56,664 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:57,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
",windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/11 03:16;jbellis;0001-cleanup.patch;https://issues.apache.org/jira/secure/attachment/12499122/0001-cleanup.patch","15/Oct/11 03:16;jbellis;0002-fix.patch;https://issues.apache.org/jira/secure/attachment/12499123/0002-fix.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,50524,,,Thu Oct 20 21:03:56 UTC 2011,,,,,,,,,,"0|i0gimv:",94449,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"15/Oct/11 03:16;jbellis;Patch to fix attached, as well as patch to clean up the SecondaryIndexManager a bit.;;;","19/Oct/11 16:50;slebresne;+1;;;","20/Oct/11 21:03;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache Daemon missing from the binary tarball,CASSANDRA-3331,12526225,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bcoverston,bcoverston,bcoverston,07/Oct/11 15:25,16/Apr/19 09:32,14/Jul/23 05:52,27/Oct/11 17:27,1.0.1,,,,,,0,windows,,,"Apparently the tools used to run the binary release are missing from the binary tarball.

I will verify that they are in the 1.0 branch, then update the ticket so we can ensure that they are included.

Ben",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/11 18:04;bcoverston;0001-add-files-to-src-and-bin-add-uninstall.patch;https://issues.apache.org/jira/secure/attachment/12498202/0001-add-files-to-src-and-bin-add-uninstall.patch","07/Oct/11 18:16;bcoverston;0002-adding-daemon-directory.patch;https://issues.apache.org/jira/secure/attachment/12498208/0002-adding-daemon-directory.patch",,,,,,,,,,,,,2.0,bcoverston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,50311,,,Thu Oct 27 16:54:51 UTC 2011,,,,,,,,,,"0|i0gilj:",94443,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"07/Oct/11 18:04;bcoverston;Added INSTALL option to the batch file. Also included the bin/daemon directory along with the files and licenses.;;;","07/Oct/11 19:02;jbellis;committed with some minor fixes;;;","13/Oct/11 22:18;urandom;I'd like to respectfully suggest that we _don't_ do this.

I know that we're aiming to run out-of-the-box wherever possible, but there has to be a line between software we supply, and what is expected to already be there.  I think this crosses that line, in the same way that jsvc (for unix hosts), or even the JVM itself, would.;;;","13/Oct/11 22:33;jbellis;I'm fine with reverting this.  (IIRC though there were some quoting fixes to the .bat file, as well as the UNINSTALL option, so it should be a little more surgical than just doing a svn reverse-merge.);;;","14/Oct/11 07:41;slebresne;I'm also fine with reverting this, but I'd add a few remarks:
  # if we do revert, it would be nice to ""fix"" the .bat file so that it don't look for it in our specific repository that we would not ship (i.e, do a little bit more than reverting because the .bat before this patch was clearly suggesting we intended to ship it but did not). Ideally we would just expect it to be installed in whatever is supposed to be standard for windows and offer an helpful message if it is not found (since we can't do what we do for the debian package, i.e make it a dependency of the package)
  # it's been committed to 1.0.0 and I don't really want to re-roll the vote for that. That would mean we ship procrun for 1.0.0 but then never after that. A little weird imho.;;;","14/Oct/11 12:50;jbellis;bq. Ideally we would just expect it to be installed in whatever is supposed to be standard for windows 

Is there a standard?  It just comes as a .zip, not a proper installer.

bq. That would mean we ship procrun for 1.0.0 but then never after that

The alternative is to keep it for all of 1.0 which will set the expectation that we'll keep doing it...  I'd rather bite the bullet early.  Everyone waits for 1.0.1 anyway right? :);;;","27/Oct/11 16:54;jbellis;removed and updated README;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support TimeUUID in CassandraStorage,CASSANDRA-3327,12526104,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,140am,140am,06/Oct/11 19:41,16/Apr/19 09:32,14/Jul/23 05:52,19/Dec/11 20:27,0.8.10,1.0.7,,,,,1,pig,,,"Cassandra CLI:

{code}
grunt> raw = LOAD 'cassandra://TEST/CF'
>>     USING CassandraStorage()
>>     AS (
>>         key:chararray,
>>         columns:bag {
>>             column:tuple(
>>                 name,
>>                 value
>>             )
>>         });

grunt> describe raw;
raw: {key: chararray,columns: {(name: bytearray,value: bytearray)}}

log_test =
    FOREACH raw
    GENERATE
        (CHARARRAY) key,
        flatten(columns);

grunt> DUMP log_test;
{code}

Returns:

{code}
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias log_test. Backend error : Unexpected data type java.util.UUID found in stream. Note only standard Pig type is supported when you output from UDF/LoadFunc
        at org.apache.pig.PigServer.openIterator(PigServer.java:890)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:655)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:303)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:67)
        at org.apache.pig.Main.run(Main.java:487)
        at org.apache.pig.Main.main(Main.java:108)
Caused by: java.lang.RuntimeException: Unexpected data type java.util.UUID found in stream. Note only standard Pig type is supported when you output from UDF/LoadFunc
        at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:478)
        at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:542)
        at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:357)
        at org.apache.pig.impl.io.InterRecordWriter.write(InterRecordWriter.java:73)
        at org.apache.pig.impl.io.InterStorage.putNext(InterStorage.java:87)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:138)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:97)
        at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:498)
        at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.collect(PigMapOnly.java:48)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:263)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:256)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:58)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
{code}

According to driftx on IRC the setTupleValue function in CassandraStorage needs to handle the uuid case and cast it to a DataByteArray.",Cassandra 0.8.6 Build #348 (CASSANDRA-2777 + CASSANDRA-2810),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/11 18:28;brandon.williams;3327-v2.txt;https://issues.apache.org/jira/secure/attachment/12499727/3327-v2.txt","14/Oct/11 22:50;brandon.williams;3327.txt;https://issues.apache.org/jira/secure/attachment/12499107/3327.txt",,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,49685,,,Mon Dec 19 21:49:52 UTC 2011,,,,,,,,,,"0|i0gik7:",94437,,rbranson,,rbranson,Normal,,,,,,,,,,,,,,,,,"14/Oct/11 22:50;brandon.williams;Patch to do what that driftx guy suggested.;;;","19/Oct/11 18:28;brandon.williams;v2 uses UUIDGen to decompose to a byte array.;;;","19/Dec/11 20:08;rbranson;Reviewed this, I am +1;;;","19/Dec/11 20:27;brandon.williams;Committed.;;;","19/Dec/11 21:49;hudson;Integrated in Cassandra-0.8 #420 (See [https://builds.apache.org/job/Cassandra-0.8/420/])
    TimeUUID support in CassandraStorage.
Patch by brandonwilliams, reviewed by Rick Branson for CASSANDRA-3327

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1220926
Files : 
* /cassandra/branches/cassandra-0.8/contrib/pig/src/java/org/apache/cassandra/hadoop/pig/CassandraStorage.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction degrades key cache stats,CASSANDRA-3325,12526023,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,frousseau,frousseau,frousseau,06/Oct/11 10:29,16/Apr/19 09:32,14/Jul/23 05:52,06/Oct/11 16:05,1.0.0,,,,,,0,compaction,,,"When ""compaction_preheat_key_cache"" is set to true, then during compaction, it keep tracks of cached keys to to re-cache their new position.
It does this by calling  the following method on every key of the compacted sstable :
sstable.getCachedPosition(row.key)
which also update cache stats, thus lowering hit rate

Below is an attached patch allowing to know if the key is cached, but without updating the stats.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/11 10:33;frousseau;001-CASSANDRA-3325.patch;https://issues.apache.org/jira/secure/attachment/12497972/001-CASSANDRA-3325.patch",,,,,,,,,,,,,,1.0,frousseau,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,49580,,,Thu Oct 06 16:05:04 UTC 2011,,,,,,,,,,"0|i0gijb:",94433,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"06/Oct/11 16:05;jbellis;committed to 1.0.0, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pig_cassandra script errors when running against pig 0.9.1 tar ball because there are multiple jars.,CASSANDRA-3320,12525954,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,boneill,boneill,boneill,05/Oct/11 20:07,16/Apr/19 09:32,14/Jul/23 05:52,12/Oct/11 16:12,0.8.8,,,,,,0,,,,"The pig_cassandra script in contrib/pig/bin assumes there is only one pig jar file in $PIG_HOME.  However, the latest release of pig 0.9.1 has two jar files: one for hadoop and one without hadoop.  See below:

bone@zen:~/tools/pig-0.9.1-> ls -al *.jar
-rw-r--r--  1 bone  staff   5130595 Sep 29 18:55 pig-0.9.1-withouthadoop.jar
-rw-r--r--  1 bone  staff  12430153 Sep 29 18:55 pig-0.9.1.jar


This breaks the shell script with:
bin/pig_cassandra: line 42: [: /Users/bone/tools/pig/pig-0.9.1-withouthadoop.jar: binary operator expected
Unrecognized option: -x

Attached is a patch for the shell script that takes the last jar file listed in the directory. This fixes the problem.  I also add an ""echo"" to notify the user which jar file they are using. 
",Running on mac os x.  PIG_HOME set to a fresh download of pig 0.9.1.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/11 20:09;boneill;trunk-3320.txt;https://issues.apache.org/jira/secure/attachment/12497880/trunk-3320.txt",,,,,,,,,,,,,,1.0,boneill,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,46578,,,Wed Oct 12 17:19:43 UTC 2011,,,,,,,,,,"0|i0gihb:",94424,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"05/Oct/11 20:09;boneill;Patch;;;","10/Oct/11 16:58;boneill;Also, I just noticed when running on my linux under bash I get the following error:
[boneill@boneill-lin] pig $ bin/pig_cassandra -x local example-script.pig 
Unrecognized option: -x
Could not create the Java virtual machine.

When I echo the $PIG_JAR variable, I get:
$PIG_JAR = /home/boneill/tools/pig/pig*.jar

Thus, the jar file resolution isn't working.  This same patch fixes this on linux/bash as well.
;;;","12/Oct/11 16:12;brandon.williams;Committed, thanks.;;;","12/Oct/11 17:19;hudson;Integrated in Cassandra-0.8 #368 (See [https://builds.apache.org/job/Cassandra-0.8/368/])
    Fix pig script to only pick up a single pig jar.
Patch by Brian Oneill, reviewed by brandonwilliams for CASSANDRA-3320

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1182456
Files : 
* /cassandra/branches/cassandra-0.8/contrib/pig/bin/pig_cassandra
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to delete after running scrub,CASSANDRA-3318,12525837,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,hsn,hsn,05/Oct/11 14:21,16/Apr/19 09:32,14/Jul/23 05:52,06/Oct/11 15:22,1.0.0,,,,,,0,,,,"Another problem with sstable deletions on 1.0. Running scrub produces lot of unable to delete messages on windows.

ERROR 16:16:37,562 Unable to delete \var\lib\cassandra\data\test\sipdb-h-711-Dat
a.db (it will be removed on server restart; we'll also retry after GC)
 INFO 16:16:37,577 Scrub of SSTableReader(path='\var\lib\cassandra\data\test\sip
db-h-711-Data.db') complete: 48396 rows in new sstable and 0 empty (tombstoned)
rows dropped",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/11 21:50;jbellis;3318.txt;https://issues.apache.org/jira/secure/attachment/12497905/3318.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,46461,,,Thu Oct 06 15:22:05 UTC 2011,,,,,,,,,,"0|i0gigf:",94420,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"05/Oct/11 18:41;jbellis;Is 711 a pre-scrub or post-scrub sstable?;;;","05/Oct/11 18:49;jbellis;Suspect this also affects 0.7 and 0.8.;;;","05/Oct/11 21:08;hsn; INFO 23:05:39,845 Scrub of SSTableReader(path='\var\lib\cassandra\data\system\S
chema-h-10-Data.db') complete: 8 rows in new sstable and 0 empty (tombstoned) ro
ws dropped
ERROR 23:05:39,845 Unable to delete \var\lib\cassandra\data\system\Schema-h-10-D
ata.db (it will be removed on server restart; we'll also retry after GC)

its table after scrub is finished.;;;","05/Oct/11 21:50;jbellis;that means it's the sstable from before scrub.

patch attached to close the readers before replacing the old sstable with the new one.

(the diff is intimidating but it's mostly indentation changes from combining two try/finally blocks.  recommend using a ""smart"" diff tool like intellij's, post-patch application.);;;","05/Oct/11 21:51;jbellis;(patch is against trunk);;;","06/Oct/11 07:24;slebresne;+1;;;","06/Oct/11 15:20;jbellis;I'm only going to commit this to 1.0 after all, because before that our PhantomReference-based deleting is very unlikely to run into this problem, so I'm going to let the stable releases stay stable.;;;","06/Oct/11 15:22;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to delete -Index files if index is currently building,CASSANDRA-3314,12525770,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,hsn,hsn,05/Oct/11 07:31,16/Apr/19 09:32,14/Jul/23 05:52,19/Oct/11 13:33,1.0.1,,,,,,0,compaction,indexing,,"If there is index building in progress, following errors are thrown if cassandra is trying to delete *-Index.db files. There is no problem with deleting -Data or -Filter.. files. CF is using leveled compaction but it is probably not related.

ERROR [NonPeriodicTasks:1] 2011-10-05 09:13:03,702 AbstractCassandraDaemon.java
(line 133) Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
java.lang.RuntimeException: java.io.IOException: Failed to delete C:\var\lib\cas
sandra\data\test\sipdb-h-772-Index.db
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:3
4)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:44
1)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.
access$301(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.
run(ScheduledThreadPoolExecutor.java:206)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:662)",,hsn,satishbabu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/11 11:34;jbellis;0001-cleanup.patch;https://issues.apache.org/jira/secure/attachment/12499533/0001-cleanup.patch","18/Oct/11 11:34;jbellis;0002-acquire-references.patch;https://issues.apache.org/jira/secure/attachment/12499534/0002-acquire-references.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,46319,,,Wed Oct 19 13:33:49 UTC 2011,,,,,,,,,,"0|i0gien:",94412,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"05/Oct/11 18:52;jbellis;Not sure how this happens, because submitIndexBuild does grab the compaction lock, so schema changes should be blocked until it's done.;;;","06/Oct/11 15:12;xedin;Can you please provider bigger exception or cassandra log and tell what disk access mode do you use for primary index: mmap or standard? 

My guess if that SSTableDeletingTask can't delete -Index.db component on Windows because it has open file descriptor somewhere...;;;","06/Oct/11 16:23;hsn;access mode standard. i dont have that log, but it should be easy to reproduce.;;;","06/Oct/11 19:18;xedin;Can you try with disk_access_mode: mmap to check if this is really a problem with descriptors leaking?;;;","07/Oct/11 13:18;hsn;with mmap mode, i dont see errors;;;","07/Oct/11 17:49;xedin;Interesting thing that I can't reproduce this behavior and don't see in the code what can cause it, can you please provide steps to reproduce?;;;","07/Oct/11 18:03;jbellis;Also, would be good to know if you can reproduce in latest 0.8, as I would expect.;;;","07/Oct/11 18:07;xedin;Tried on both latest 0.8 and 1.0.0 with disk_access_mode: standard and with/without secondary indexes (used stress.java to populate system with data and nodetool to run different operations) and couldn't reproduce... ;;;","07/Oct/11 21:22;hsn;load table with data ( 2 m rows used). start index build, it will take a while. During index building run read/write test which will write enough data to trigger table compaction. i didnt tested it on 0.8;;;","07/Oct/11 21:33;hsn;Here is larger segment of log, since index building started. I can reproduce it any time. Ignore bug _Nothing to compact in sipdb_, its not related. Its other leveldb compaction bug.

 INFO [MigrationStage:1] 2011-10-07 23:24:35,293 Migration.java (line 119) Applying migration c12c20d0-f12a-11e0-0000-23b38323f4da Update column family to org.apache.cassandra.config.CFMetaData@7c401b[cfId=1000,ksName=test,cfName=sipdb,cfType=Standard,comparator=org.apache.cassandra.db.marshal.AsciiType,subcolumncomparator=<null>,comment=phone calls routing information,rowCacheSize=0.0,keyCacheSize=0.0,readRepairChance=0.0,replicateOnWrite=false,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.Int32Type,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@154c995,mergeShardsChance=0.1,keyAlias=java.nio.HeapByteBuffer[pos=478 lim=481 cap=483],column_metadata={java.nio.HeapByteBuffer[pos=0 lim=3 cap=3]=ColumnDefinition{name=6b616d, validator=org.apache.cassandra.db.marshal.AsciiType, index_type=KEYS, index_name='kam'}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO [MigrationStage:1] 2011-10-07 23:24:35,309 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Migrations@25069868(7870/9837 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2011-10-07 23:24:35,309 Memtable.java (line 237) Writing Memtable-Migrations@25069868(7870/9837 serialized/live bytes, 1 ops)
 INFO [MigrationStage:1] 2011-10-07 23:24:35,309 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Schema@3067216(3325/4156 serialized/live bytes, 3 ops)
 INFO [FlushWriter:4] 2011-10-07 23:24:35,356 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Migrations-h-17-Data.db (7934 bytes)
 INFO [FlushWriter:4] 2011-10-07 23:24:35,371 Memtable.java (line 237) Writing Memtable-Schema@3067216(3325/4156 serialized/live bytes, 3 ops)
 INFO [FlushWriter:4] 2011-10-07 23:24:35,418 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Schema-h-17-Data.db (3475 bytes)
 INFO [MigrationStage:1] 2011-10-07 23:24:35,418 SecondaryIndexManager.java (line 181) Creating new index : ColumnDefinition{name=6b616d, validator=org.apache.cassandra.db.marshal.AsciiType, index_type=KEYS, index_name='kam'}
 INFO [Creating index: sipdb.kam] 2011-10-07 23:24:35,449 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-sipdb@30231056(2284899/30490286 serialized/live bytes, 42052 ops)
 INFO [FlushWriter:4] 2011-10-07 23:24:35,449 Memtable.java (line 237) Writing Memtable-sipdb@30231056(2284899/30490286 serialized/live bytes, 42052 ops)
 INFO [FlushWriter:4] 2011-10-07 23:24:36,262 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\test\sipdb-h-1049-Data.db (4506422 bytes)
 INFO [CompactionExecutor:3] 2011-10-07 23:24:36,262 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [Creating index: sipdb.kam] 2011-10-07 23:24:36,262 SecondaryIndex.java (line 145) Submitting index build of sipdb.kam for data in SSTableReader(path='\var\lib\cassandra\data\test\sipdb-h-1047-Data.db'), SSTableReader(path='\var\lib\cassandra\data\test\sipdb-h-1049-Data.db')
 INFO [CompactionExecutor:3] 2011-10-07 23:24:36,262 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [pool-1-thread-1] 2011-10-07 23:24:36,403 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.2837528604119 (just-counted was 36.2837528604119).  calculation took 16ms for 67 columns
 INFO [pool-1-thread-1] 2011-10-07 23:24:36,496 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.2837528604119 (just-counted was 35.62471395881007).  calculation took 15ms for 132 columns
 INFO [pool-1-thread-1] 2011-10-07 23:24:36,668 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.2837528604119 (just-counted was 35.51029748283753).  calculation took 31ms for 263 columns
 INFO [pool-1-thread-1] 2011-10-07 23:24:37,043 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.286453839516824 (just-counted was 36.286453839516824).  calculation took 62ms for 535 columns
 INFO [pool-1-thread-1] 2011-10-07 23:24:37,746 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.286453839516824 (just-counted was 36.11138755980861).  calculation took 109ms for 1065 columns
 INFO [pool-1-thread-1] 2011-10-07 23:24:39,137 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.286453839516824 (just-counted was 36.1418534947848).  calculation took 219ms for 2127 columns
 INFO [CompactionExecutor:3] 2011-10-07 23:24:39,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:24:39,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [pool-1-thread-1] 2011-10-07 23:24:42,012 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.286453839516824 (just-counted was 35.84278599835191).  calculation took 437ms for 4235 columns
 INFO [CompactionExecutor:3] 2011-10-07 23:24:44,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:24:44,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [pool-1-thread-1] 2011-10-07 23:24:47,403 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.286453839516824 (just-counted was 36.06701179177452).  calculation took 875ms for 8500 columns
 INFO [CompactionExecutor:3] 2011-10-07 23:24:49,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:24:49,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:24:54,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:24:54,340 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [pool-1-thread-1] 2011-10-07 23:24:58,512 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 36.286453839516824 (just-counted was 36.10818295381003).  calculation took 1750ms for 16940 columns
 INFO [CompactionExecutor:3] 2011-10-07 23:24:59,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:24:59,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:04,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:04,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:09,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:09,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:14,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:14,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:19,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:19,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:24,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:24,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:29,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:29,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [pool-1-thread-1] 2011-10-07 23:25:33,279 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 37.129730574542535 (just-counted was 37.129730574542535).  calculation took 3656ms for 32878 columns
 INFO [CompactionExecutor:3] 2011-10-07 23:25:34,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:34,341 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:39,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:39,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:44,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:44,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:49,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:49,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:54,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:54,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:59,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:25:59,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:04,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:04,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:09,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:09,342 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:14,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:14,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:19,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:19,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:24,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:24,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [OptionalTasks:1] 2011-10-07 23:26:25,483 MeteredFlusher.java (line 62) flushing high-traffic column family ColumnFamilyStore(table='test', columnFamily='sipdb') (estimated 66493149 bytes)
 INFO [OptionalTasks:1] 2011-10-07 23:26:25,483 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-sipdb.kam@22387768(1290429/59891601 serialized/live bytes, 65999 ops)
 INFO [OptionalTasks:1] 2011-10-07 23:26:25,483 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-sipdb@14468404(495177/6607770 serialized/live bytes, 9112 ops)
 INFO [FlushWriter:5] 2011-10-07 23:26:25,499 Memtable.java (line 237) Writing Memtable-sipdb.kam@22387768(1290429/59891601 serialized/live bytes, 65999 ops)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:29,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:29,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [FlushWriter:5] 2011-10-07 23:26:34,234 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\test\sipdb.kam-h-1-Data.db (6963969 bytes)
 INFO [FlushWriter:5] 2011-10-07 23:26:34,234 Memtable.java (line 237) Writing Memtable-sipdb@14468404(495177/6607770 serialized/live bytes, 9112 ops)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:34,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:34,343 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [FlushWriter:5] 2011-10-07 23:26:35,765 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\test\sipdb-h-1050-Data.db (984842 bytes)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:35,765 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:35,781 CompactionTask.java (line 119) Compacting [SSTableReader(path='\var\lib\cassandra\data\test\sipdb-h-1049-Data.db'), SSTableReader(path='\var\lib\cassandra\data\test\sipdb-h-1050-Data.db')]
 INFO [pool-1-thread-1] 2011-10-07 23:26:37,374 Memtable.java (line 177) ColumnFamilyStore(table='test', columnFamily='sipdb.kam') liveRatio is 37.129730574542535 (just-counted was 36.844857020417244).  calculation took 12578ms for 65753 columns
 INFO [CompactionExecutor:3] 2011-10-07 23:26:42,078 CompactionTask.java (line 222) Compacted to [\var\lib\cassandra\data\test\sipdb-h-1051-Data.db,].  5 491 264 to 5 468 836 (~99% of original) bytes for 50 480 keys at 0,828250MBPS.  Time: 6 297ms.
 INFO [CompactionExecutor:3] 2011-10-07 23:26:42,078 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:42,078 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:42,078 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO [CompactionExecutor:3] 2011-10-07 23:26:42,093 CompactionTask.java (line 80) Nothing to compact in sipdb.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
ERROR [NonPeriodicTasks:1] 2011-10-07 23:26:42,140 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
java.lang.RuntimeException: java.io.IOException: Failed to delete C:\var\lib\cassandra\data\test\sipdb-h-1049-Index.db
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Failed to delete C:\var\lib\cassandra\data\test\sipdb-h-1049-Index.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:54)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:44)
	at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:138)
	at org.apache.cassandra.io.sstable.SSTableDeletingTask.runMayThrow(SSTableDeletingTask.java:81)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 8 more
;;;","17/Oct/11 17:49;jbellis;01 does some cleanup around index building, including adding docstrings to make clear when references need to be acquired on sstables involved.

02 adds the missing reference acquisitions for new index creation and loadNewSSTables.;;;","18/Oct/11 08:14;slebresne;There seems to be a mistake with the attached patch, or at least they don't correspond to the description. First patch don't really add any doscstrings and the second patch doesn't add any reference acquisitions.;;;","18/Oct/11 11:34;jbellis;correct patches attached.;;;","19/Oct/11 10:32;slebresne;The first patch removes the import of Lock in CompactionManager which prevents compilation. But otherwise +1.;;;","19/Oct/11 13:33;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cancelling index build throws assert error,CASSANDRA-3313,12525766,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,hsn,hsn,05/Oct/11 06:58,16/Apr/19 09:32,14/Jul/23 05:52,20/Oct/11 21:14,1.0.1,,,,,,0,indexing,,,"Canceling index build throws this, but checking log there was no compaction running in background.

INFO 08:46:41,343 Writing Memtable-IndexInfo@9480253(34/42 serialized/live byte
s, 1 ops)
ERROR 08:46:41,343 Fatal exception in thread Thread[CompactionExecutor:3,5,main]

java.lang.AssertionError
        at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates
(SecondaryIndexManager.java:397)
        at org.apache.cassandra.db.Table.indexRow(Table.java:534)
        at org.apache.cassandra.db.index.SecondaryIndexBuilder.build(SecondaryIn
dexBuilder.java:64)
        at org.apache.cassandra.db.compaction.CompactionManager$7.run(Compaction
Manager.java:856)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:44
1)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 08:46:41,531 Completed flushing \var\lib\cassandra\data\system\IndexInfo-h
-1-Data.db (88 bytes)",,hsn,satishbabu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/11 17:58;jbellis;3313.txt;https://issues.apache.org/jira/secure/attachment/12499408/3313.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,46313,,,Thu Oct 20 21:14:14 UTC 2011,,,,,,,,,,"0|i0gie7:",94410,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"05/Oct/11 07:22;jbellis;I'm not aware of a 'cancel index build' command so I'm curious what you mean by that.;;;","05/Oct/11 11:57;hsn;redefine CF to column_metadata without index.;;;","17/Oct/11 17:58;jbellis;patch to accommodate index disappearing mid-update.  applies on top of CASSANDRA-3314.;;;","19/Oct/11 16:44;slebresne;+1

nitpick: to be uber pedantic, we could use the row key validator to print the row key in the log messages.;;;","20/Oct/11 21:09;jbellis;same as CASSANDRA-3334;;;","20/Oct/11 21:14;jbellis;actually it's not quite the same, got confused by merge conflicts;;;","20/Oct/11 21:14;jbellis;rebased + committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
need initClause when catch Exception and throw new Exception in cli,CASSANDRA-3312,12525758,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,satishbabu,cywjackson,cywjackson,05/Oct/11 02:22,16/Apr/19 09:32,14/Jul/23 05:52,05/Oct/11 11:14,0.8.7,,,,,,0,,,,"through CASSANDRA-2746 , we added initCause to the Cli such that we could see more meaningful exception stacktrace when certain exception is thrown.

However, there are still some other area, eg:

executeGetWithConditions(Tree)
executeSet(Tree)
executeIncr(Tree, long)
etc etc...

basically any time you do a
{code}
            {
                throw new RuntimeException(e.getMessage());
            }
{code}

the real exception is lost. The right approach should be:

{code}
        catch (Exception e)
        {
            throw new RuntimeException(e.getMessage(), e);
        }
{code}

eg: i was getting this:
null
java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:310)
        at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:217)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:345)
Caused by: java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeGetWithConditions(CliClient.java:815)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:208)
        ... 2 more


but i have no idea what the problem is with just the above stack trace.

with the fix, this would tell us more:
null
java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:310)
        at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:217)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:345)
Caused by: java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeGetWithConditions(CliClient.java:815)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:208)
        ... 2 more
Caused by: UnavailableException()
        at org.apache.cassandra.thrift.Cassandra$get_indexed_slices_result.read(Cassandra.java:14065)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_indexed_slices(Cassandra.java:810)
        at org.apache.cassandra.thrift.Cassandra$Client.get_indexed_slices(Cassandra.java:782)
        at org.apache.cassandra.cli.CliClient.executeGetWithConditions(CliClient.java:806)
        ... 3 more",,satishbabu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/11 07:01;satishbabu;3312.patch;https://issues.apache.org/jira/secure/attachment/12497764/3312.patch",,,,,,,,,,,,,,1.0,satishbabu,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,46284,,,Wed Oct 05 12:22:54 UTC 2011,,,,,,,,,,"0|i0gidj:",94407,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"05/Oct/11 02:35;jbellis;I think throw new RTE(e) would be fine, btw.;;;","05/Oct/11 07:01;satishbabu;Added patch to support more exceptional details;;;","05/Oct/11 07:23;jbellis;Tagging 0.8.7 because I *think* it's present there too.;;;","05/Oct/11 11:14;xedin;Committed, thanks!;;;","05/Oct/11 12:22;hudson;Integrated in Cassandra-0.8 #362 (See [https://builds.apache.org/job/Cassandra-0.8/362/])
    Improved CLI exceptions
patch by satishbabu; reviewed by xedin for CASSANDRA-3312

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1179164
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cqlsh: Error running ""select *"" vs ""select all columns""",CASSANDRA-3311,12525734,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,cdaw,cdaw,04/Oct/11 21:08,16/Apr/19 09:32,14/Jul/23 05:52,05/Oct/11 00:36,1.0.0,,,Legacy/CQL,,,0,,,,"* Install cql 1.0.5 from [http://code.google.com/a/apache-extras.org/p/cassandra-dbapi2/downloads/detail?name=cql-1.0.5.tar.gz&can=2&q=]

* Query using ""select *""
{code}
cqlsh> select * from users;
Exception: 'utf8' codec can't decode byte 0xb4 in position 7: unexpected code byte
{code}

* Query selecting all columns
{code}
 select KEY, password, gender, session_token, state, birth_year from users;
   KEY |  password | gender | session_token | state | birth_year |
 user1 | ch@ngem3a |      f |          None |    TX |       1968 |
{code}

* Test Data
{code}
CREATE KEYSPACE ks1 with 
  strategy_class =  
    'org.apache.cassandra.locator.SimpleStrategy' 
  and strategy_options:replication_factor=1;
  
use ks1;

CREATE COLUMNFAMILY users (
  KEY varchar PRIMARY KEY, password varchar, gender varchar,
  session_token varchar, state varchar, birth_year bigint);
  
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3a', 'f', 'TX', '1968');    
{code}","Server: BDP/main
CQLSH: 1.0.5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/11 21:50;xedin;CASSANDRA-3311.patch;https://issues.apache.org/jira/secure/attachment/12497713/CASSANDRA-3311.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,45384,,,Wed Oct 05 00:36:28 UTC 2011,,,,,,,,,,"0|i0gidb:",94406,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"04/Oct/11 21:16;jbellis;may be a bug in the schema part of the resultset.  can you take a look, Pavel?;;;","05/Oct/11 00:36;jbellis;inlined declaration of cD with assignment + committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update lib/jamm-0.2.5 to the version deployed to Maven central,CASSANDRA-3310,12525733,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,stephenc,stephenc,04/Oct/11 21:08,16/Apr/19 09:32,14/Jul/23 05:52,05/Oct/11 01:32,1.0.0,,,,,,0,,,,"https://oss.sonatype.org/content/groups/public/com/github/stephenc/jamm/0.2.5/jamm-0.2.5.jar is the version that is being synced to Maven central

MD5: 79f6bf54227abd15d4277ff0fe7038cb",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,45383,,,Wed Oct 05 01:32:52 UTC 2011,,,,,,,,,,"0|i0gicn:",94403,,,,,Low,,,,,,,,,,,,,,,,,"05/Oct/11 01:32;tjake;Thx, verified its working.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool Doesnt close the open JMX connection causing it to leak Threads,CASSANDRA-3309,12525728,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,04/Oct/11 20:22,16/Apr/19 09:32,14/Jul/23 05:52,04/Oct/11 21:21,0.8.7,1.0.0,,,,,0,,,,"When nodetool is used intensively we will see 1000's of ""JMX server connection timeout""

Fix is to close the connections when no longer needed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/11 20:55;vijay2win@yahoo.com;0001-fixing-jmx-thread-leak.patch;https://issues.apache.org/jira/secure/attachment/12497705/0001-fixing-jmx-thread-leak.patch",,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,45238,,,Tue Oct 04 22:15:03 UTC 2011,,,,,,,,,,"0|i0gicf:",94402,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"04/Oct/11 20:55;vijay2win@yahoo.com;Simple fix which just fixes nodetool to close the connection which it creates. tested and works fine. ;;;","04/Oct/11 21:21;brandon.williams;Committed;;;","04/Oct/11 22:15;hudson;Integrated in Cassandra-0.8 #360 (See [https://builds.apache.org/job/Cassandra-0.8/360/])
    Nodetool closes JMX connections to avoid leaking timer threads.
Patch by vijay, reviewed by brandonwilliams for CASSANDRA-3309

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1178957
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/NodeCmd.java
* /cassandra/branches/cassandra-1.0.0/CHANGES.txt
* /cassandra/branches/cassandra-1.0.0/src/java/org/apache/cassandra/tools/NodeCmd.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed streaming may cause duplicate SSTable reference,CASSANDRA-3306,12525677,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,hsn,hsn,04/Oct/11 14:39,16/Apr/19 09:32,14/Jul/23 05:52,31/Oct/12 16:13,1.1.7,1.2.0 beta 2,,,,,1,,,,"during stress testing, i always get this error making leveledcompaction strategy unusable. Should be easy to reproduce - just write fast.

ERROR [CompactionExecutor:6] 2011-10-04 15:48:52,179 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[CompactionExecutor:6,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.DataTracker$View.newSSTables(DataTracker.java:580)
	at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:546)
	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:268)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:232)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:960)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:199)
	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:47)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:131)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:114)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

and this is in json data for table:

{
  ""generations"" : [ {
    ""generation"" : 0,
    ""members"" : [ 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484 ]
  }, {
    ""generation"" : 1,
    ""members"" : [ ]
  }, {
    ""generation"" : 2,
    ""members"" : [ ]
  }, {
    ""generation"" : 3,
    ""members"" : [ ]
  }, {
    ""generation"" : 4,
    ""members"" : [ ]
  }, {
    ""generation"" : 5,
    ""members"" : [ ]
  }, {
    ""generation"" : 6,
    ""members"" : [ ]
  }, {
    ""generation"" : 7,
    ""members"" : [ ]
  } ]
}",,cburroughs,dashv,hsn,jjordan,joelastpass,jonma,omid,scode,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/12 22:45;yukim;0001-CASSANDRA-3306-test.patch;https://issues.apache.org/jira/secure/attachment/12550547/0001-CASSANDRA-3306-test.patch","30/Oct/12 22:34;yukim;0001-change-DataTracker.View-s-sstables-from-List-to-Set.patch;https://issues.apache.org/jira/secure/attachment/12551435/0001-change-DataTracker.View-s-sstables-from-List-to-Set.patch","30/Oct/12 22:34;yukim;0002-fail-stream-session-for-invalid-request.patch;https://issues.apache.org/jira/secure/attachment/12551436/0002-fail-stream-session-for-invalid-request.patch",,,,,,,,,,,,3.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,44375,,,Wed Mar 11 20:32:13 UTC 2015,,,,,,,,,,"0|i0aym7:",61881,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"04/Oct/11 15:18;hsn;another problem. why not store data in some system CF? would be probably safer choice.

ERROR [CompactionExecutor:5] 2011-10-04 17:13:13,922 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[CompactionExecutor:5,5,main]
java.io.IOError: java.io.IOException: Failed to rename \var\lib\cassandra\data\test\sipdb.json to \var\lib\cassandra\data\test\sipdb-old.json
	at org.apache.cassandra.db.compaction.LeveledManifest.serialize(LeveledManifest.java:382)
	at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:182)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:152)
	at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:466)
	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:275)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:232)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:960)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:199)
	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:47)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:131)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:114)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Failed to rename \var\lib\cassandra\data\test\sipdb.json to \var\lib\cassandra\data\test\sipdb-old.json
	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:64)
	at org.apache.cassandra.db.compaction.LeveledManifest.serialize(LeveledManifest.java:375)
	... 15 more
;;;","04/Oct/11 15:27;jbellis;Because then you get into hairy cyclical situations where you can't read the manifest until you replay the commitlog, but replaying the commitlog requires writing new sstables and thus knowing the manifest;;;","04/Oct/11 15:52;hsn;as i understand new flushed tables are placed at level 0. Just replay commitlog and put all new stuff in lvl 0. after comitlog is done, it can do voodoo shuffles.

but why not to rename tables like table-h-333-l1-Data.db?

idea to have stables with non overlapping key ranges is interesting, but read performance is kinda slow (about 50% of normal) here. Its cassandra core modified to get advantage of leveled tables? i.e. search one sstable at level1, one at lvl2 using bloom filters for key?;;;","04/Oct/11 16:01;jbellis;This isn't really a great place to rehash http://leveldb.googlecode.com/svn/trunk/doc/impl.html and CASSANDRA-1608.;;;","04/Oct/11 16:04;brandon.williams;bq. why not store data in some system CF? would be probably safer choice.

This has historically been a bad idea, see CASSANDRA-1155, then CASSANDRA-1318 and finally CASSANDRA-1430.;;;","24/Oct/11 14:25;slebresne;I don't suppose you were using column family truncation in your tests, where you? ;;;","24/Oct/11 16:42;hsn;no truncation, no supercolumns.;;;","24/Oct/11 16:45;slebresne;Are you still able to reproduce reliably? Because we aren't and being able to would help considerably, so if you are and could share whatever script you're using to reproduce, that would be awesome.;;;","25/Oct/11 06:33;hsn;i tested it on 1.0 final and it worked without error for 1 test run. i will give it another test without index.;;;","26/Oct/11 16:14;slebresne;I'll note that Ramesh Natarajan reported on the mailing list what clearly appears to be the same bug (http://www.mail-archive.com/user@cassandra.apache.org/msg18146.html), but while not using leveled compaction. I also think he was using the 1.0.0 final.;;;","04/Nov/11 18:32;slebresne;I'll note that more info have been added to the messages thrown by the exception here in 1.0.1. So if someone can reproduce this issue on 1.0.1, it would be useful to get the stacktrace (the full system.log would actually be even better).;;;","24/Nov/11 02:16;jonma;This　AssertionError happened always in cassandra1.0.0 ,not just only in LeveledCompactionStrategy;;;","24/Nov/11 02:24;jonma;I suppose it's a bug in DataTracker .;;;","24/Nov/11 08:10;slebresne;As I already said, if you are able to reproduce this, please try reproducing with 1.0.3. And if you are still able to, please attach you system.log with the exception here because it will have more info on the error that should help. And if you're not able to reproduce with 1.0.3, then I guess it means we've fixed it without knowing.;;;","01/Dec/11 19:42;joelastpass;Running under 1.0.4, can easily reproduce this by just kicking of a repair of any LeveledCompactionStrategy CF.  

The 'zero' on the assert indicates the value (added that to the code to see what the value was): 

java.lang.AssertionError: 0
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:178)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:481)
        at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:275)
        at org.apache.cassandra.db.DataTracker.addSSTables(DataTracker.java:237)
        at org.apache.cassandra.db.DataTracker.addStreamedSSTable(DataTracker.java:242)
        at org.apache.cassandra.db.ColumnFamilyStore.addSSTable(ColumnFamilyStore.java:920)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:141)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:103)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)


Relevant lines from system.log leading up the it: 

 INFO [FlushWriter:794] 2011-12-01 14:23:22,966 Memtable.java (line 275) Completed flushing /var/lib/cassandra/data/sso/Sessions-hc-12524-Data.db (1119784 bytes)
 INFO [CompactionExecutor:2379] 2011-12-01 14:23:22,969 CompactionTask.java (line 112) Compacting [SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12501-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12517-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12513-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12512-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12502-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12507-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12519-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12500-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12508-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12504-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12510-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12515-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12509-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12524-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12514-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12518-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12505-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12516-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12511-Data.db'), SSTableReader(path='/var/lib/cassandra/data/sso/Sessions-hc-12506-Data.db')]
 INFO [AntiEntropyStage:1] 2011-12-01 14:25:06,321 AntiEntropyService.java (line 186) [repair #ea080b70-1c51-11e1-0000-692e0c239dfd] Received merkle tree for Sessions from /xxxxxxxx
ERROR [Thread-177] 2011-12-01 14:25:17,863 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Thread-177,5,main]
java.lang.AssertionError: 0  [see above]

If you want more let me know I can reproduce instantly.
;;;","02/Dec/11 05:36;jbellis;Joe, your assertion is the one in CASSANDRA-3536 (where I've attached a patch fixing it).  Closing this other one as cantrepro.;;;","23/Oct/12 22:44;yukim;This error actually happens on 1.1. And I can easily reproduce with unit test(Test code attached).

{code}
    [junit] ERROR 17:34:46,696 Fatal exception in thread Thread[CompactionExecutor:3,1,main]
    [junit] java.lang.AssertionError: Expecting new size of 2, got 1 while replacing [SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-1-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-5-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-4-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-2-Data.db')] by [SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-6-Data.db')] in View(pending_count=0, sstables=[SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-1-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-2-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-4-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-4-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-5-Data.db')], compacting=[SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-1-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-5-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-4-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hf-2-Data.db')])
    [junit] 	at org.apache.cassandra.db.DataTracker$View.newSSTables(DataTracker.java:651)
    [junit] 	at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:616)
    [junit] 	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
    [junit] 	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:253)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:994)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:200)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$1.runMayThrow(CompactionManager.java:154)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:680)
{code}

The cause is actually in streaming. StreamInSession can add duplicate reference to SSTable to DataTracker when it is left even after stream session finishes. This typically happens when source node is marked as dead by FailureDetector during streaming session(GC storm is the one I saw) and keep sending file in same session after the node comes back.;;;","23/Oct/12 22:45;yukim;Test code attached. Compaction strategy is not related.;;;","24/Oct/12 12:11;slebresne;Good analysis Yuki. I'm not really sure what is the right fix though. Given that this should very rarely happen (repair uses a much higher failure detection threshold than the normal one, though maybe we can increase it even more to make this even less likely) and that I don't seen any obvious way to avoid that kind of situation, maybe making DataTracker handle duplicate addition of a SSTableReader is the simplest thing to do. The obvious way to do that would be to change the View sstables List to a Set, which leads me to the current commentary in the code:
{noformat}
        // We can't use a SortedSet here because ""the ordering maintained by a sorted set (whether or not an
        // explicit comparator is provided) must be <i>consistent with equals</i>.""  In particular,
        // ImmutableSortedSet will ignore any objects that compare equally with an existing Set member.
        // Obviously, dropping sstables whose max column timestamp happens to be equal to another's
        // is not acceptable for us.  So, we use a List instead.
{noformat}
I think that comment is obsolete. Namely, it was added with CASSANDRA-2498 and at the time the list of sstable was kept in max timestamp order at all time. But since then, we've moved the sorting in max timestamp in CollationController directly (which is less fragile), so the order inside DataTracker doesn't matter anymore.;;;","24/Oct/12 20:46;jbellis;bq. This typically happens when source node is marked as dead by FailureDetector during streaming session(GC storm is the one I saw) and keep sending file in same session after the node comes back

But we close the session on convict, so shouldn't it start a new one?;;;","24/Oct/12 22:00;yukim;bq. But we close the session on convict, so shouldn't it start a new one?

Yes, StreamInSession gets closed and removed on convict _once_. But if GC pause happens in the middle of streaming session, the node resumes streaming in the same session after GC. Since resumed stream carries session ID that is once closed on receiver side, StreamInSession is created again with the same old session ID and this time just 1 file to receive.
This continues again and again until source node's StreamingOutSession sends all files.
You can see this in receiver's log file like below:

{code}
INFO [Thread-50] 2012-10-20 13:13:26,574 StreamInSession.java (line 214) Finished streaming session 10 from /10.xx.xx.xx
INFO [Thread-51] 2012-10-20 13:13:29,691 StreamInSession.java (line 214) Finished streaming session 10 from /10.xx.xx.xx
INFO [Thread-52] 2012-10-20 13:13:32,957 StreamInSession.java (line 214) Finished streaming session 10 from /10.xx.xx.xx
{code}

Duplication happens during this partially broken streaming session. Because StreamInSession is removed after sending SESSION_FINISHED reply, and StreamOutSession keeps sending files, sometimes the same StreamInSession instance receives more than 1 file and calls closeIfFinished every time it received the file.
(Sorry, this is hard to explain in words.
https://github.com/apache/cassandra/blob/cassandra-1.1.6/src/java/org/apache/cassandra/streaming/StreamInSession.java#L181 this part is executed multiple times with _readers_ growing by received new file.)

So as Sylvain stated above, changing DataTracker.View's sstable to Set is one way to eliminate duplicate reference and we should do it. In addition, I'm thinking not to create duplicate StreamInSession by checking StreamHeader.pendingFiles because this field is only filled when initiating streaming.;;;","25/Oct/12 07:44;slebresne;That code is a mess so let me give a shot at describing what happens for the record. Say node1 wants to stream files A, B and C to node2. If everything goes well what happens is:
# node1 sends the first file A with a StreamHeader that says that A, B and C are pending files and A is the currently sent file. On node2, a new StreamInSession is created with those information.
# Once A is finished, node2 remove A from the pending file in the StreamInSession send an acknowledgement to node1, and then node1 sends B with a StreamHeader with no pending files (basically the list of pending files is only sent the first time so that the StreamInSession on node2 knows when everything is finished) and B as current file. When node2 received that StreamHeader, it retrieve the StreamInSession, setting B as the current files.
# Once B is finished, node2 removes it from pending files, acks to node1 and node1 sends C with a StreamHeader with no pending file and C as current file.  Node2 retrieven the StreamInSession and modify it accordingly.
# At last, once C is finished, node2 removes it from the pending files. Then it realizes the pending files are empty and so that the streaming is finished and at that point it adds all the SSTableReader created so far to the cfs (and acks to node1 the end of the streaming).

Now, the problem is if say node1 is marked dead by mistake by node2 during say the streaming of A. I that happens, the only thing we do on node2 is to close the session and remove the streamInSession from the global sessions map.  However we don't shutdown the stream or anything, so if node1 is in fact still alive, what will happen is:
# A will finish his transfer correctly. Once that's done, node2 will still send an acknowledgement (probably the first mistake, we could check that the session has been closed and send an error instead).
# Node1 getting it's acknowledgement will send B with a StreamHeader that has B as current file and no pending files as usual. On reception, node2 will not find any StreamInSession (it has been removed during the close), and so it will create a new one as if that was the beginning of a transfer. And that session will have no pending file (second mistake: if we have to create a new StreamInSession but there is no pending file at all something wrong has happened).
# Once B is fully streamed, node2 will acknowledge it to node1 and remove it from it's streamInSession. But that session the new one we just created with no pending file. So the streamInSession will consider the streaming is finished, and it will thus add the SSTableReader for B to the cfs.
# Because B has been acknowledged, node1 will start sending C (again, with no pending file in the StreamHeader). This will be done as soon as B was finished, and so concurrently with the streamInSession on node2 closing itself.
# So when node2 receives the StreamHeader with C, it will try to retrieve the session and will find the previous session. And will happily add C as the current file for that session (third and fourth mistake: StreamInSession should not add a file as current unless it is a pending file for this session, and a session could detect that it's being reused even though it has just detected itself as finished).
# Now when C transfer finishes, the seesion will be notify and since it still has no pending files, it will once again consider the streaming as complete.  But since it's still the same session, it still has the SSTableReader for B in its list of created reader (as well as the one for C now). And that's when it adds B for a second time to the DataTracker.

I also not that we end up without having ever add the SSTableReader for A to the cfs since the very first StreamInSession was never finished. This is not a big deal in that the stream itself has been indicated as failed to the client anyway, but just to say that it's not just a problem of duplicating a SSTableReader preference.

Anyway, let me back on what I said earlier. We should definitively fix some if not all of the ""mistake"" above (and send a SESSION_FAILURE to node1 as soon as we detect something is wrong).

But that being said, my comment on the comment in DataTracker being obsolete still stand, and replacing the list by a set in there would have at least the advantage of slightly simplifying the code of DataTracker.View.newSSTables(), as well as being more resilient if a SSTableReader is added twice. Not a big deal though.
;;;","29/Oct/12 20:28;yukim;Attaching first attempt.
I changed DataTracker.View's sstables to Set, and made stream fail when file arrives after StreamInSession failed.

Changing List to Set for sstables sometimes makes CollationControllerTest fail. It was introduced in CASSANDRA-4116, and I think the test and CollationController#collectAllData expect sstables to be ordered by timestamp. I'm not sure if the test is obsolete or we really need sstables to be sorted all the time.
0002 patch alone will fix the issue, so we can apply that for now.;;;","30/Oct/12 08:15;slebresne;For patch 0002, we shouldn't check the FailureDetector otherwise we don't really fix the issue. The only way we know this bug can happen is wher the FailureDetector *had* marked a node down while it shouldn't have (besides, we just got something from a node so it's fair to assume it is alive).

bq. and I think the test and CollationController#collectAllData expect sstables to be ordered by timestamp

It doesn't seem to me that collectAllData needs sstable ordered. In fact, I think that it does a second pass over the sstables iterators just because it doesn't assume sstables are ordered by max timestamp. Moreover, I'm pretty sure it would be a bug to assume that. If you look at DataTracker.View.newSSTables, it ends by {{Iterables.addAll(newSSTables, replacements)}} which clearly won't maintain any specific ordering of sstables.

bq. I'm not sure if the test is obsolete.

I don't think the test is obsolete but I think we have a minor bug in CollationController. The test want to test that we correctly exclude sstable whose maxTimestamp is less than the most recent row tombstone we have. But that test checks controller.getSstablesIterated(), and for collectAllData, it will count every sstable it include in the first iteration of collectAllData but don't remove those that are remove by the second pass. In other words, I think the correct fix is to decrement stablesIterated in CollationController when in the second pass we remove a sstable (or more simply to set it to iterators.size() just before we collate everything).;;;","30/Oct/12 22:34;yukim;bq. we shouldn't check the FailureDetector otherwise we don't really fix the issue.

Ok. I've fixed this and reattached 0002.

bq. The test want to test that we correctly exclude sstable whose maxTimestamp is less than the most recent row tombstone we have.

Right. But I think the test assumes that SSTables are added to List in order of flush, and that's true as long as we use List. So what I suggest is to remove that part from the test since we no longer use List.
And sstablesIterated counter in collectAllData is doing fine because we actually read the data from sstable when we go over
{code}
IColumnIterator iter = filter.getSSTableColumnIterator(sstable);
{code}
before incrementing counter.

So I removed that test from CollationControllerTest in 0001-change-DataTracker.View-s-sstables-from-List-to-Set.patch.;;;","31/Oct/12 09:48;slebresne;bq. Ok. I've fixed this and reattached 0002.

Alright, +1 on 0002. Let's commit that for now to 1.1/1.2 as this fix this ticket.

{quote}
the test assumes that SSTables are added to List in order of flush
And sstablesIterated counter in collectAllData is doing fine because we actually read the data
{quote}

Right. I guess what I meant is that what is tested right now is not really sensible. Relying on the order of flush is only valid for a small, controlled test, but in reality as soon as compaction kicks in, the order of sstable in DataTracker will be meaningless even with a List instead of a Set. Basically the guarantee collectAll gives us today is that it will eliminate sstables whose maxTimestamp < mostRecentTombstone with just having read the sstable row header, not the full data. But that's not what sstablesIterated counts so it's broken.

That being said, I think we can improve collectAll in the way described in CASSANDRA-4883. If we do so, the test will pass again without relying on any assumption of the order of sstables in DataTracker. So overall I suggest moving all of this to CASSANDRA-4883.;;;","31/Oct/12 16:13;yukim;Committed 0002 to 1.1 and trunk.;;;","11/Mar/15 19:44;dashv;I understand that this has been fixed in newer versions of Cassandra.

But I'm currently seeing this exact issue on a production 1.1.1 node in my cluster.

What should be my next step?

Do I simply restart it?

Run cleanup? Scrub? Repair?

Sounds like repair would just fail with the same problem.

Any advice would be appreciated.;;;","11/Mar/15 19:56;yukim;Yes, restarting the node will help.
No need to clean up/scrub.

Please use user@cassandra.apache.org mailing list for these type of questions.;;;","11/Mar/15 20:32;dashv;Thanks for the swift reply and I will use the mailing list in the future.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing fields in show schema output,CASSANDRA-3304,12525659,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,hsn,hsn,04/Oct/11 12:52,16/Apr/19 09:32,14/Jul/23 05:52,04/Oct/11 15:58,1.0.0,,,Legacy/Tools,,,0,,,,"if you compare output of these 2 commands:

*show keyspaces*
Keyspace: test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: sipdb
    ""phone calls routing information""
      Key Validation Class: org.apache.cassandra.db.marshal.IntegerType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.AsciiType
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Key cache size / save period in seconds: 0.0/0
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name: kam
          Validation Class: org.apache.cassandra.db.marshal.AsciiType
      *Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompacti*

*show schema*
create column family sipdb
  with column_type = 'Standard'
  and comparator = 'AsciiType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'IntegerType'
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and keys_cached = 0.0
  and key_cache_save_period = 0
  and read_repair_chance = 0.0
  and gc_grace = 0
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = false
  and row_cache_provider = 'ConcurrentLinkedHashCacheProvider'
  and comment = 'phone calls routing information'
  and column_metadata = [
    {column_name : 'kam',
    validation_class : AsciiType}];

You will discover that show schema is missing: 1. compaction strategy. 2. how many keys to save",,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/11 15:32;xedin;CASSANDRA-3304.patch;https://issues.apache.org/jira/secure/attachment/12497650/CASSANDRA-3304.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,44289,,,Wed Oct 05 20:38:09 UTC 2011,,,,,,,,,,"0|i0gia7:",94392,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"04/Oct/11 13:22;jbellis;also durable writes;;;","04/Oct/11 15:43;jbellis;+1;;;","04/Oct/11 15:58;xedin;Committed.;;;","05/Oct/11 19:13;jbellis;Pavel, can you commit the relevant parts to 0.8 as well?  (My fault for not tagging this 0.8.7 originally.);;;","05/Oct/11 19:28;xedin;Done, that was only ""durable_writes"".;;;","05/Oct/11 20:38;hudson;Integrated in Cassandra-0.8 #365 (See [https://builds.apache.org/job/Cassandra-0.8/365/])
    Fix missing fields in CLI `show schema` output
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3304

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1179395
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Short reads protection results in returning more columns than asked for,CASSANDRA-3303,12525636,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,byronclark,slebresne,slebresne,04/Oct/11 09:04,16/Apr/19 09:32,14/Jul/23 05:52,04/Nov/11 08:37,1.0.2,,,,,,0,,,,"When we detect a short read (in SP.fetchRows), we retry a new command created by:
{noformat}
logger.debug(""detected short read: expected {} columns, but only resolved {} columns"", sliceCommand.count, liveColumnsInRow);
int retryCount = sliceCommand.count + sliceCommand.count - liveColumnsInRow;
SliceFromReadCommand retryCommand = new SliceFromReadCommand(command.table,
                                                             command.key,
                                                             command.queryPath,
                                                             sliceCommand.start,
                                                             sliceCommand.finish,
                                                             sliceCommand.reversed,
                                                             retryCount);
{noformat}
That is, in that new command, the count is greater than what asked in the initial command. But we never cut back the result of that new retried query.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/11 02:46;byronclark;ASF.LICENSE.NOT.GRANTED--long_read.sh;https://issues.apache.org/jira/secure/attachment/12499627/ASF.LICENSE.NOT.GRANTED--long_read.sh","19/Oct/11 04:20;byronclark;cassandra-3303-1.patch;https://issues.apache.org/jira/secure/attachment/12499632/cassandra-3303-1.patch","19/Oct/11 04:24;byronclark;cassandra-3303-2.patch;https://issues.apache.org/jira/secure/attachment/12499634/cassandra-3303-2.patch","30/Oct/11 02:07;byronclark;cassandra-3303-3.patch;https://issues.apache.org/jira/secure/attachment/12501488/cassandra-3303-3.patch","01/Nov/11 19:48;byronclark;cassandra-3303-4.patch;https://issues.apache.org/jira/secure/attachment/12501816/cassandra-3303-4.patch","03/Nov/11 20:18;byronclark;cassandra-3303-5.patch;https://issues.apache.org/jira/secure/attachment/12502214/cassandra-3303-5.patch",,,,,,,,,6.0,byronclark,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,44229,,,Fri Nov 04 08:37:54 UTC 2011,,,,,,,,,,"0|i0gi9r:",94390,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"05/Oct/11 19:28;jbellis;Tagging this 1.0.1 since short reads being broken is nothing new (I would say returning too much data, is better than returning not enough);;;","19/Oct/11 02:46;byronclark;[^long_read.sh] is the script I'm using with ccm to reproduce the issue. It's the same script used for CASSANDRA-2643.;;;","19/Oct/11 04:20;byronclark;[^cassandra-3303-1.patch] is a first attempt at fixing this.  I'm not sure it handles the situation correctly for a slice requested in reverse order.;;;","19/Oct/11 04:24;byronclark;[^cassandra-3303-2.patch] doesn't reuse a counter variable on an inner loop.;;;","20/Oct/11 14:37;slebresne;This doesn't always work, because we cannot be sure that on the retry phase, we will have a digest mismatch again (some read repair could have kicked in). I.e, we should also trim also in the initial for loop, not only in the 'deal with repair response' one.

Also (and I'm nitpicking a bit) it would be nice to push the trim code into RetriedSliceFromReadCommand (and to make it handle reversed queries). For instance we could add a filter(ColumFamily) method or something like that. Same thing for the original count, rather than using instanceof, I'd rather add a originalCount() method to SliceFromReadCommand (that would return count) that the Retried variant would override.;;;","30/Oct/11 02:07;byronclark;[^cassandra-3303-3.patch] should address all the concerns mentioned. 

I'm going to go ahead and pull some more of the logic out of StorageProxy and get rid of all the instanceof checks.;;;","31/Oct/11 21:59;jbellis;So, we should wait before reviewing?;;;","01/Nov/11 14:58;byronclark;Yes, please. I'll be attaching the improved patch later today.;;;","01/Nov/11 17:20;jbellis;Belatedly and probably redundantly, I note that any time we hit RowRepairResolver we can have this count problem, not just w/ short read resolution.  (E.g., CASSANDRA-3395);;;","01/Nov/11 19:48;byronclark;[^cassandra-3303-4.patch] removes all of the ugly logic to handle this case (and the short read case) into the ReadCommands.

Jonathan - Does this already address CASSANDRA-3395 or is more work needed for that one?;;;","02/Nov/11 14:51;slebresne;On the patch, a few minor remarks:
* There is a weird UnImplNode on top of ReadCommand.java (that prevents compilation)
* I think we could replace isLongRead and trimLongRead by a maybeTrim() method that would be a noop by default. Basically trimLongRead requires that isLongRead() has been called (otherwise it throws an unsupported exception), so in those case I think it's easier to use if it's just one method. Same for the pair (isShortRead, generateShortRetry) actually.
* nit: There is some imports rewrite (java.io.* -> multiple imports) in SFRC. It's a big deal but that kind of thing makes diffs bigger than necessary so it's nice to take the habit to no do that.

Now I don't think this patch addresses CASSANDRA-3395 as is. For that, we would basically need to move the trimLongRead to SliceFromReadCommand directly (the bug of CASSANDRA-3395 is that normal SliceFromReadCommand can have more that requested columns because of reconciliation of the result of different nodes). The easiest way here is probably to move the trim code to a SliceFromReadCommand.maybeTrim() and to have that method uses some getRequestedCount() method. For SFRC, the latter method would return count, for RSFRC, it would return originalCount.
;;;","03/Nov/11 20:18;byronclark;[^cassandra-3303-5.patch] addresses all the comments in the most recent review. Additionally, it checks for a long read on SliceFromReadCommand instead of just RetriedSliceFromReadCommand.;;;","04/Nov/11 08:37;slebresne;+1, committed
Thanks Byron.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stop Cassandra result in hang,CASSANDRA-3302,12525624,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,cywjackson,cywjackson,04/Oct/11 05:08,16/Apr/19 09:32,14/Jul/23 05:52,27/Oct/11 13:16,1.0.1,,,,,,0,,,,"testing this under trunk via a hacked package (replacing jars from 0.8.6 deb installation)

When calling service cassandra stop, the Cassandra process hang:

http://aep.appspot.com/display/i6aIUCkt4kz0HG5l2VszMM7QvLo/

The following logs is observed in the C* log:

 INFO [main] 2011-10-03 23:20:46,434 AbstractCassandraDaemon.java (line 270) Cassandra shutting down...
 INFO [main] 2011-10-03 23:20:46,434 CassandraDaemon.java (line 218) Stop listening to thrift clients

Re-run this using 1.0.0 branch, (following the same ""hack"" procedure), C* stop properly, and the following is observed in the log:

 INFO [main] 2011-10-04 05:02:08,048 AbstractCassandraDaemon.java (line 270) Cassandra shutting down...
 INFO [main] 2011-10-04 05:02:08,049 CassandraDaemon.java (line 218) Stop listening to thrift clients
 INFO [Thread-2] 2011-10-04 05:02:08,318 MessagingService.java (line 482) Shutting down MessageService...
 INFO [Thread-2] 2011-10-04 05:02:08,319 MessagingService.java (line 497) Waiting for in-progress requests to complete
 INFO [ACCEPT-/10.83.77.171] 2011-10-04 05:02:08,319 MessagingService.java (line 637) MessagingService shutting down server thread.


could this be related to CASSANDRA-3261 ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/11 11:03;slebresne;3302.patch;https://issues.apache.org/jira/secure/attachment/12501074/3302.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,44007,,,Thu Oct 27 13:16:33 UTC 2011,,,,,,,,,,"0|i0gi9b:",94388,,tjake,,tjake,Normal,,,,,,,,,,,,,,,,,"13/Oct/11 18:59;jbellis;Jackson, if I remember correctly, this is broken in trunk but not 1.0.0?  Hence the suspicion of CASSANDRA-3261.;;;","13/Oct/11 19:47;cywjackson;indeed i was testing on trunk but not 1.0.0

but i have not tried to test using 1.0.0 to see if it fails there. i could give it a quick try;;;","13/Oct/11 22:26;cywjackson;smoke test quickly repeating the same step except swaps with 1.0.0 jars cannot reproduce the problem;;;","24/Oct/11 14:30;tjake;Jackson is this still happening? Or is it ok to close?;;;","25/Oct/11 00:17;cywjackson;still happen in 1.0 branch, so NOT OK to close.

stack trace looks the same as in the earlier link;;;","27/Oct/11 11:03;slebresne;This is indeed a bug from CASSANDRA-3261 that forgot to override the interrupt method.;;;","27/Oct/11 12:55;tjake;+1;;;","27/Oct/11 13:16;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java Stress Tool:  COUNTER_GET reads from CounterSuper1 instead of SuperCounter1,CASSANDRA-3301,12525608,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,cdaw,cdaw,04/Oct/11 00:39,16/Apr/19 09:32,14/Jul/23 05:52,04/Oct/11 14:40,0.8.7,1.0.0,,,,,0,,,,"Output from stress tool - COUNTER_ADD works fine bug COUNTER_GET does not
{code}
./stress --operation=COUNTER_ADD --family-type=Super --num-keys=1 --consistency-level=TWO --replication-factor=3 --nodes=cathy1
Unable to create stress keyspace: Keyspace already exists.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
1,0,0,0.0060,0
END


./stress --operation=COUNTER_GET --family-type=Super --num-keys=1 --consistency-level=QUORUM --nodes=cathy1
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
Operation [0] retried 10 times - error reading counter key 0 ((InvalidRequestException): unconfigured columnfamily CounterSuper1)

0,0,0,NaN,0
END
{code}

The CF created is called *SuperCounter1* and not *CounterSuper1*
{code}
 INFO 00:34:21,344 ColumnFamilyStore(table='Keyspace1', columnFamily='SuperCounter1') liveRatio is 9.167798032786886 (just-counted was 9.167798032786886).  calculation took 1281ms for 9883 columns
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/11 07:07;slebresne;3301.patch;https://issues.apache.org/jira/secure/attachment/12497594/3301.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,43978,,,Tue Oct 04 14:40:08 UTC 2011,,,,,,,,,,"0|i0gi8v:",94386,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"04/Oct/11 01:05;cdaw;Workaround is to edit: tools/stress/src/org/apache/cassandra/stress/operations/CounterGetter.java:
ColumnParent parent = new ColumnParent(""CounterSuper1"").setSuper_column(superColumn.getBytes());
;;;","04/Oct/11 07:07;slebresne;Oops, thanks Cathy. Patch attached.;;;","04/Oct/11 13:19;jbellis;+1;;;","04/Oct/11 14:31;hudson;Integrated in Cassandra-0.8 #359 (See [https://builds.apache.org/job/Cassandra-0.8/359/])
    Fix stress COUNTER_GET options
patch by slebresne; reviewed by jbellis for CASSANDRA-3301

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1178785
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/tools/stress/src/org/apache/cassandra/stress/operations/CounterGetter.java
;;;","04/Oct/11 14:40;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clientutil depends on FBUtilities (bad),CASSANDRA-3299,12525590,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,03/Oct/11 21:33,16/Apr/19 09:32,14/Jul/23 05:52,10/Oct/11 15:22,1.0.1,,,Legacy/CQL,,,0,cql,,,"clientutils' (indirect )dependency on FBUtilities (needed for tests) would result in huge numbers of classes being pulled in transitively.

The attached patch moves the {{bytesToHex}} and {{hexToBytes}} methods into a new class ({{o.a.c.utils.Hex}}), which has no external dependencies.

This should be pretty safe, but I've marked it fixfor-1.0.1 since we're so close to release, and because the JDBC driver can embed a snapshot jar in the meantime.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/11 21:35;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3299-move-FBUtils-methods-for-bytes-hex-to-s.txt;https://issues.apache.org/jira/secure/attachment/12497546/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3299-move-FBUtils-methods-for-bytes-hex-to-s.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,43944,,,Mon Oct 10 16:51:57 UTC 2011,,,,,,,,,,"0|i0gi7z:",94382,,,,,Normal,,,,,,,,,,,,,,,,,"03/Oct/11 21:59;jbellis;Nit: can we just name the methods Hex.toBytes and Hex.toString?

+1 otherwise;;;","03/Oct/11 23:39;urandom;I'm OK with that, but are those names going to be too generic if someone decides to do a static import?;;;","07/Oct/11 17:52;jbellis;True enough. Not a big deal either way, let's go with what you have.;;;","10/Oct/11 15:22;urandom;committed;;;","10/Oct/11 15:26;jbellis;I saw this committed to trunk -- if it's intended for 1.0.1 it should also go in the 1.0 branch;;;","10/Oct/11 15:39;urandom;you're right; my bad, committed to cassandra-1.0 as well;;;","10/Oct/11 16:51;hudson;Integrated in Cassandra #1150 (See [https://builds.apache.org/job/Cassandra/1150/])
    move FBUtils methods for bytes<->hex to separate class

Patch by eevans; reviewed by jbellis for CASSANDRA-3299

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1181018
Files : 
* /cassandra/trunk/build.xml
* /cassandra/trunk/drivers/java/test/org/apache/cassandra/cql/JdbcDriverTest.java
* /cassandra/trunk/drivers/java/test/org/apache/cassandra/cql/jdbc/PreparedStatementTest.java
* /cassandra/trunk/examples/simple_authentication/src/org/apache/cassandra/auth/SimpleAuthenticator.java
* /cassandra/trunk/src/java/org/apache/cassandra/auth/Resources.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/marshal/BytesType.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/AbstractByteOrderedPartitioner.java
* /cassandra/trunk/src/java/org/apache/cassandra/dht/BytesToken.java
* /cassandra/trunk/src/java/org/apache/cassandra/hadoop/ConfigHelper.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/ByteBufferUtil.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/Hex.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/MerkleTree.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/marshal/RoundTripTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/utils/FBUtilitiesTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/utils/HexTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/utils/MerkleTreeTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompressedRandomAccessReaderTest fails on Windows,CASSANDRA-3298,12525564,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,03/Oct/11 18:44,16/Apr/19 09:32,14/Jul/23 05:52,04/Oct/11 17:17,1.0.0,,,,,,0,,,,"    [junit] Testcase: testResetAndTruncate(org.apache.cassandra.io.compress.CompressedRandomAccessReaderTest):      FAILED
    [junit] expected:<43> but was:<49>
    [junit] junit.framework.AssertionFailedError: expected:<43> but was:<49>
    [junit]     at org.apache.cassandra.io.compress.CompressedRandomAccessReaderTest.testResetAndTruncate(CompressedRandomAccessReaderTest.java:81)
    [junit]     at org.apache.cassandra.io.compress.CompressedRandomAccessReaderTest.testResetAndTruncate(CompressedRandomAccessReaderTest.java:39)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/11 23:03;xedin;CASSANDRA-3298.patch;https://issues.apache.org/jira/secure/attachment/12497564/CASSANDRA-3298.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,43863,,,Tue Oct 04 17:17:14 UTC 2011,,,,,,,,,,"0|i0gi7r:",94381,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"04/Oct/11 16:34;jbellis;So it's a bug in the test?;;;","04/Oct/11 16:35;xedin;yes;;;","04/Oct/11 17:07;jbellis;looks like the fix is to make it use different files for compressed and uncompressed tests.

+1;;;","04/Oct/11 17:17;xedin;Yes and also put them into temp directory.

Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
truncate can still result in data being replayed after a restart,CASSANDRA-3297,12525557,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,03/Oct/11 18:02,16/Apr/19 09:32,14/Jul/23 05:52,05/Oct/11 18:35,0.8.8,1.0.0,,,,,0,commitlog,,,Our first stab at fixing this was CASSANDRA-2950.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/11 18:37;jbellis;3297.txt;https://issues.apache.org/jira/secure/attachment/12497509/3297.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,43845,,,Wed Oct 05 19:23:19 UTC 2011,,,,,,,,,,"0|i0gi73:",94378,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"03/Oct/11 18:37;jbellis;The primary fix here is this:

{noformat}
+        // flush the CF being truncated before forcing the new segment
+        forceBlockingFlush();
{noformat}

Without this, forcing a new segment doesn't help us if the CF-to-truncate was dirty, since its last memtable will be in the new, non-deletable (since it is the last) segment.

The rest of the patch does three things:

- removes redundant code from RMTruncateTest
- fixes CL.resetUnsafe for windows by making it close the segments it's clearing
- adds debug logging;;;","03/Oct/11 18:42;jbellis;(patch is against trunk);;;","05/Oct/11 16:56;slebresne;nitpick: the assert that segments is not empty in CL.createNewSegment() could be moved one line up since sync() already assume this.

But otherwise, patch lgtm. +1.;;;","05/Oct/11 18:35;jbellis;moved assert + committed;;;","05/Oct/11 19:23;hudson;Integrated in Cassandra-0.8 #364 (See [https://builds.apache.org/job/Cassandra-0.8/364/])
    fix truncate allowing data to be replayed post-restart
patch by jbellis; reviewed by slebresne for CASSANDRA-3297

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1179359
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/Truncation.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLogSegment.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CsDef instead of CfDef in system_add_keyspace() function,CASSANDRA-3296,12525512,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,alexiswilke,alexiswilke,03/Oct/11 06:06,16/Apr/19 09:32,14/Jul/23 05:52,03/Oct/11 08:07,0.8.7,,,,,,0,,,,"throw new InvalidRequestException(""CsDef ("" + cf.getName() +"") had a keyspace definition that did not match KsDef"");

The string starts with ""CsDef ("" when it should be ""CfDef ("".","In this file:
apache-cassandra-0.8.6-src/src/java/org/apache/cassandra/thrift/CassandraServer.java
Around line #893",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,43526,,,Mon Oct 03 08:07:43 UTC 2011,,,,,,,,,,"0|i0gi6n:",94376,,,,,Low,,,,,,,,,,,,,,,,,"03/Oct/11 08:07;slebresne;fixed in r1178325, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metered Flusher log message confusing,CASSANDRA-3293,12525436,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,hsn,hsn,01/Oct/11 15:41,16/Apr/19 09:32,14/Jul/23 05:52,03/Oct/11 04:52,0.8.7,1.0.0,,,,,0,,,," INFO [NonPeriodicTasks:1] 2011-10-01 17:34:32,652 MeteredFlusher.java (line 62) flushing high-traffic column family ColumnFamilyStore(+table='rapidshare',+ columnFamily='resultcache')

instead of table it should be *keyspace=*",,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,42930,,,Mon Oct 03 04:52:52 UTC 2011,,,,,,,,,,"0|i0gi5b:",94370,,,,,Low,,,,,,,,,,,,,,,,,"03/Oct/11 04:52;jbellis;fixed in r1178297.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
creating column family sets durable_writes to true,CASSANDRA-3292,12525396,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,hsn,hsn,01/Oct/11 07:00,16/Apr/19 09:32,14/Jul/23 05:52,19/Oct/11 13:22,0.8.8,1.0.1,,,,,0,schema,,,"[default@rapidshare] describe keyspace rapidshare;
Keyspace: rapidshare:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: *false*
    Options: [datacenter1:1]
  Column Families:
[default@rapidshare] create column family t1;
1ba19300-ebfa-11e0-0000-34912694d0bf
Waiting for schema agreement...
... schemas agree across the cluster
[default@rapidshare] describe keyspace rapidshare;
Keyspace: rapidshare:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: *true*
    Options: [datacenter1:1]
  Column Families:
    ColumnFamily: t1
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.028124999999999997/1440/6 (millions of ops/minutes/MB)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []",,hsn,satishbabu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/11 04:17;jbellis;0001-r-m-rename-migrations.patch;https://issues.apache.org/jira/secure/attachment/12499500/0001-r-m-rename-migrations.patch","18/Oct/11 04:17;jbellis;0002-clean-up-KSM.durableWrites.patch;https://issues.apache.org/jira/secure/attachment/12499501/0002-clean-up-KSM.durableWrites.patch","18/Oct/11 04:17;jbellis;0003-cloneWith.patch;https://issues.apache.org/jira/secure/attachment/12499502/0003-cloneWith.patch","18/Oct/11 04:17;jbellis;0004-update-tests-to-use-KSM.testMetadata.patch;https://issues.apache.org/jira/secure/attachment/12499503/0004-update-tests-to-use-KSM.testMetadata.patch",,,,,,,,,,,4.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,41774,,,Wed Oct 19 13:22:50 UTC 2011,,,,,,,,,,"0|i0gi4v:",94368,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"01/Oct/11 07:52;satishbabu;I can re-pro in the following version too
 INFO 00:32:55,237 Cassandra version: 1.0.0-rc1-SNAPSHOT
 INFO 00:32:55,237 Thrift API version: 19.18.0
Satish;;;","03/Oct/11 19:23;jbellis;01: removes the unused rename CF/KS code

02: adds KSM.cloneWith for migrations to use that will be futureproof against new constructors being added;;;","03/Oct/11 19:25;jbellis;(patch is against trunk);;;","17/Oct/11 16:51;jbellis;rebased 02;;;","17/Oct/11 20:28;xedin;you forgot to remove Table.renameCf method and some of the tests are failing
{noformat}
    [junit] Testsuite: org.apache.cassandra.db.ReadMessageTest
    [junit] Tests run: 3, Failures: 1, Errors: 0, Time elapsed: 0,599 sec
    [junit] 
    [junit] Testcase: testNoCommitLog(org.apache.cassandra.db.ReadMessageTest):	FAILED
    [junit] 
    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.cassandra.db.ReadMessageTest.testNoCommitLog(ReadMessageTest.java:135)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.ReadMessageTest FAILED
    [junit] Testsuite: org.apache.cassandra.db.RecoveryManager2Test
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0,669 sec
    [junit] 
    [junit] Testcase: testWithFlush(org.apache.cassandra.db.RecoveryManager2Test):	FAILED
    [junit] Expecting only 1 replayed mutation, got 0
    [junit] junit.framework.AssertionFailedError: Expecting only 1 replayed mutation, got 0
    [junit] 	at org.apache.cassandra.db.RecoveryManager2Test.testWithFlush(RecoveryManager2Test.java:68)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.RecoveryManager2Test FAILED
    [junit] Testsuite: org.apache.cassandra.db.RecoveryManager3Test
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0,621 sec
    [junit] 
    [junit] Testcase: testMissingHeader(org.apache.cassandra.db.RecoveryManager3Test):	FAILED
    [junit] Columns [(as string: )])] is not expected [col1]
    [junit] junit.framework.AssertionFailedError: Columns [(as string: )])] is not expected [col1]
    [junit] 	at org.apache.cassandra.db.TableTest.assertColumns(TableTest.java:553)
    [junit] 	at org.apache.cassandra.db.TableTest.assertColumns(TableTest.java:514)
    [junit] 	at org.apache.cassandra.db.RecoveryManager3Test.testMissingHeader(RecoveryManager3Test.java:76)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.RecoveryManager3Test FAILED
    [junit] Testsuite: org.apache.cassandra.db.RecoveryManagerTest
    [junit] Tests run: 3, Failures: 1, Errors: 1, Time elapsed: 0,611 sec
    [junit] 
    [junit] Testcase: testOne(org.apache.cassandra.db.RecoveryManagerTest):	FAILED
    [junit] Columns [(as string: )])] is not expected [col1]
    [junit] junit.framework.AssertionFailedError: Columns [(as string: )])] is not expected [col1]
    [junit] 	at org.apache.cassandra.db.TableTest.assertColumns(TableTest.java:553)
    [junit] 	at org.apache.cassandra.db.TableTest.assertColumns(TableTest.java:514)
    [junit] 	at org.apache.cassandra.db.RecoveryManagerTest.testOne(RecoveryManagerTest.java:70)
    [junit] 
    [junit] 
    [junit] Testcase: testRecoverCounter(org.apache.cassandra.db.RecoveryManagerTest):	Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.cassandra.db.RecoveryManagerTest.testRecoverCounter(RecoveryManagerTest.java:99)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.RecoveryManagerTest FAILED
{noformat}

Otherwise it seems to be the way to go :);;;","18/Oct/11 04:17;jbellis;updated w/ above fixes;;;","18/Oct/11 16:49;xedin;Please don't forget to remove Table.renameCf(...) method when you will be committing, +1.;;;","18/Oct/11 17:17;jbellis;Yes, that's part of the new 01.;;;","18/Oct/11 17:24;xedin;Sorry, seems like it didn't override 0001 for me, see it now, +1.;;;","18/Oct/11 18:22;hudson;Integrated in Cassandra-0.8 #377 (See [https://builds.apache.org/job/Cassandra-0.8/377/])
    r/m obsolete CF/KS rename code
patch by jbellis; reviewed by pyaskevich for CASSANDRA-3292

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1185761
Files : 
* /cassandra/branches/cassandra-0.8/src/avro/internode.genavro
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/Table.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/RenameColumnFamily.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/RenameKeyspace.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/DefsTest.java
;;;","19/Oct/11 07:01;hudson;Integrated in Cassandra-0.8 #380 (See [https://builds.apache.org/job/Cassandra-0.8/380/])
    finish fixing changing durable_writes keyspace option during CF creation
patch by jbellis; reviewed by pyaskevich for CASSANDRA-3292
add KSM.systemKeyspace and cloneWith methods
patch by jbellis; reviewed by pyaskevich for CASSANDRA-3292

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1185963
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/KSMetaData.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/SchemaLoader.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/config/DatabaseDescriptorTest.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/DefsTest.java

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1185961
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/KSMetaData.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/AddColumnFamily.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/DropColumnFamily.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/UpdateKeyspace.java
;;;","19/Oct/11 13:22;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1.0 needs to clean out old-style hints,CASSANDRA-3291,12525391,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,01/Oct/11 02:28,16/Apr/19 09:32,14/Jul/23 05:52,01/Oct/11 05:41,1.0.0,,,,,,0,,,,(Only marking this Minor because the manual workaround of deleting hint files is trivial.),,patricioe,yangyangyyy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/11 02:35;jbellis;3291.txt;https://issues.apache.org/jira/secure/attachment/12497264/3291.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,41760,,,Wed Oct 19 17:45:34 UTC 2011,,,,,,,,,,"0|i0gi4f:",94366,,patricioe,,patricioe,Low,,,,,,,,,,,,,,,,,"01/Oct/11 02:35;jbellis;Patch to update hint purging to check for a 1.0 marker instead of 0.7.

(Note that we do not need to check for both separately, since 1.0 doesn't care about 0.6-style hints any more than 0.7 did.);;;","01/Oct/11 03:17;patricioe;The patch looks good to me.

Just for the records, is there a need to check for 0.8 hints? (Don't remember if there was a major change in the structures between 0.8 and 1.0);;;","01/Oct/11 04:05;jbellis;Yes, the change was from 0.8 to 1.0.  So 1.0 just throws everything pre-1.0 away (after snapshot, just in case).;;;","01/Oct/11 05:41;jbellis;committed;;;","19/Oct/11 16:45;yangyangyyy;I'm using the current HEAD of 1.0.0 github branch, and I'm still seeing this error, not sure if it's  this bug or another one.



 INFO [HintedHandoff:1] 2011-10-19 12:43:17,674 HintedHandOffManager.java (line 263) Started hinted handoff for token: 11342745564
0312821154458202477256070484 with IP: /10.39.85.140
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,885 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHan
doff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,886 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
;;;","19/Oct/11 17:12;slebresne;Note sure what is the current HEAD of 1.0.0 given that branch as been removed in favor of the 1.0 one. But if it still points to the line
{noformat}
assert mutationColumn != null;
{noformat}
then I'm willing to bet it's another bug, since I believe the preceding assertions should have fired before this one if this was old-style hints. In which case, do you mind opening a new ticket?;;;","19/Oct/11 17:45;yangyangyyy;created #3385, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assert err on ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:126),CASSANDRA-3289,12525378,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,cywjackson,cywjackson,30/Sep/11 23:06,16/Apr/19 09:32,14/Jul/23 05:52,01/Oct/11 15:04,1.0.0,,,,,,0,,,,"I have the following in trunk:

RowKey: b
=> (column=a, value=38383838383838383838, timestamp=1317421952793000)
=> (column=d, value=617364646661736466, timestamp=1317420968944000)
=> (column=e, value=38383838383838383838, timestamp=1317421096152000)
=> (column=f, value=33343334333433343334, timestamp=1317422838818000)
=> (column=g, value=33343334333433343334, timestamp=1317422565130000)
=> (column=i, value=33343334333433343334, timestamp=1317422879258000)
=> (column=j, value=33343334333433343334, timestamp=1317422814873000)
=> (column=o, value=33343334333433343334, timestamp=1317422867106000)
=> (column=x, value=33343334333433343334, timestamp=1317422394097000)
=> (column=z, value=38383838383838383838, timestamp=1317421982057000)

Keyspace: testks:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [168:1]
  Column Families:
    ColumnFamily: testcf
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Key cache size / save period in seconds: 200000.0/14400
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy

every thing is flushed to the sstables, but not in the same sstables, and the columns are in some what 'random' form:

/var/lib/cassandra/data/testks/testcf-h-10-Data.db
{
""61"": [[""0c"",""76"",1317405903119000], [""0d"",""76"",1317405977002000], [""7a"",""38383838383838383838"",1317422276322000]],
""62"": [[""61"",""38383838383838383838"",1317421952793000], [""63"",""4e864303"",1317421827329000,""d""], [""64"",""617364646661736466"",1317420968944000], [""65"",""38383838383838383838"",1317421096152000], [""67"",""33343334333433343334"",1317422565130000], [""78"",""33343334333433343334"",1317422394097000], [""7a"",""38383838383838383838"",1317421982057000]]
}
/var/lib/cassandra/data/testks/testcf-h-12-Data.db
{
""62"": [[""6a"",""33343334333433343334"",1317422814873000]]
}
/var/lib/cassandra/data/testks/testcf-h-13-Data.db
{
""62"": [[""66"",""33343334333433343334"",1317422838818000]]
}
/var/lib/cassandra/data/testks/testcf-h-14-Data.db
{
""62"": [[""6f"",""33343334333433343334"",1317422867106000]]
}
/var/lib/cassandra/data/testks/testcf-h-15-Data.db
{
""62"": [[""69"",""33343334333433343334"",1317422879258000]]
}


then i basically make a call to get key=b with all the column names (yes included column names that didn't exist to save time):

ColumnFamilyResult<String, String> queryColumns = template.queryColumns(""b"", Arrays.asList(""a"",""b"",""c"",""d"",""e"",""f"",""g"",""h"",""i"",""j"",""k"",""l"",""m"",""n"",""o"",""p"",""q"",""r"",""s"",""t"",""u"",""v"",""w"",""x"",""y"",""z""));

(let me know if it would be easier to just upload the sstables to the ticket)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/11 14:16;jbellis;3289-v2.txt;https://issues.apache.org/jira/secure/attachment/12497361/3289-v2.txt","01/Oct/11 02:14;jbellis;3289.txt;https://issues.apache.org/jira/secure/attachment/12497262/3289.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,41740,,,Sat Oct 01 15:04:25 UTC 2011,,,,,,,,,,"0|i0gi3j:",94362,,slebresne,,slebresne,Critical,,,,,,,,,,,,,,,,,"30/Sep/11 23:08;cywjackson;should mention a workaround is to do major compaction to stack everything back to a single sstable;;;","01/Oct/11 02:01;jbellis;The full trace:
{noformat}
java.lang.AssertionError: Added column does not sort as the last column
        at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:126)
        at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:123)
        at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:118)
        at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:116)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:61)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1297)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1147)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1114)
        at org.apache.cassandra.db.Table.getRow(Table.java:388)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:58)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:795)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat};;;","01/Oct/11 02:14;jbellis;collectTimeOrderedData has to use a column store that allows random inserts.  Patch attached to use ThreadSafeSortedColumns there, and ArrayBacked for collectAllData.;;;","01/Oct/11 13:33;slebresne;Unfortunately this doesn't work in the case of reading to put in the cache, because in that case, we need to always use ThreadSafeSortedColumns (in CFS, line 1128). I see a few solutions:
  * make ABSC work even if the input doesn't arrive in sorted order (i.e, remove the assertion since the code is already there). We had this debate already, I still think that it is a solution that make sense and note that in that case it would likely still be the more efficient solution. But I can understand that feeling is not shared.
  * make CollactionController only use non thread-safe CF (a little bit like the attached patch, but using TreeMapSortedColumns in the collectTimeOrderedData) *and* add a  cloneMe towards TSSC in the 'read for cache' case. But that involves a copy of the CF.
  * instead of passing the Factory around, pass a boolean indicating if the returned CF should be thread safe or not. In that way, if the boolean is true, both collectTimeOrderedData and collectAddData would use TSSC, otherwise, cTOD would use TMSC and cAD would use ABSC.

Again, I think the first one has my preference, but if we're afraid that by doing so we'll start using ABSC in places where it's clearly not the most efficient solution, then the last solution is probably the best one.

But I would agree that this passing of Factory around is not very easy to use correctly and we should probably come up with a cleaner solution.
;;;","01/Oct/11 14:16;jbellis;v2 attached.  you will perhaps not be surprised that I prefer the boolean approach. :);;;","01/Oct/11 14:52;slebresne;bq. v2 attached

+1

bq. you will perhaps not be surprised that I prefer the boolean approach

nothing ventured, nothing gained :);;;","01/Oct/11 15:04;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CfDef can default to an invalid id and fail during system_add_column_family,CASSANDRA-3288,12525372,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,zznate,zznate,30/Sep/11 22:12,16/Apr/19 09:32,14/Jul/23 05:52,01/Oct/11 05:37,1.0.0,,,Legacy/CQL,,,0,,,,"The line from this commit:
https://github.com/apache/cassandra/commit/38e3e85b121ba6308ba3ceb26312d12ed0d609ec#L1R683

Introduced an issue in that some clients, particularly Hector, will send a CfDef with an ID having been set to 0. Done via the CfDef#setId, the isSetId bit is flipped to true, causing error if schemaId of 0 already exists, which given the use case, is likely. 

Since we know the context of a system_create_column_family, this can be sidestepped by just stepping on whatever ID is there (irrelevant on a create anyway) with the value returned from: Schema.instance.nextCFId()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/11 22:16;jbellis;3288.txt;https://issues.apache.org/jira/secure/attachment/12497231/3288.txt","30/Sep/11 23:18;zznate;3822v2.txt;https://issues.apache.org/jira/secure/attachment/12497243/3822v2.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,41673,,,Sat Oct 01 05:37:42 UTC 2011,,,,,,,,,,"0|i0gi33:",94360,,zznate,,zznate,Critical,,,,,,,,,,,,,,,,,"30/Sep/11 22:16;jbellis;patch to explicitly ignore client-set ids on create;;;","30/Sep/11 22:20;jbellis;first reported on http://www.mail-archive.com/user@cassandra.apache.org/msg17649.html;;;","30/Sep/11 23:18;zznate;v2 adds same ignore for system_add_keyspace;;;","01/Oct/11 05:37;jbellis;committed v2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrap is broken in 1.0.0-rc1,CASSANDRA-3285,12525291,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,slebresne,slebresne,30/Sep/11 10:53,16/Apr/19 09:32,14/Jul/23 05:52,30/Sep/11 15:58,1.0.0,,,,,,0,bootstrap,,,"The commit of #3219 introduced two bugs: the condition to bootstrap is that there *are* non-system tables instead, a _not_ is missing, and the setToken() was wrongly push up into the ""I'm not bootstrapping"" block so a boostrapping node was left in the joining state.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/11 10:53;slebresne;fix-boostrap.patch;https://issues.apache.org/jira/secure/attachment/12497138/fix-boostrap.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,40997,,,Fri Sep 30 15:58:18 UTC 2011,,,,,,,,,,"0|i0gi1j:",94353,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"30/Sep/11 15:17;jbellis;+1;;;","30/Sep/11 15:58;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
help create column family refers to outdated fields,CASSANDRA-3284,12525288,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,hsn,hsn,hsn,30/Sep/11 10:25,16/Apr/19 09:32,14/Jul/23 05:52,30/Sep/11 20:46,1.0.0,,,Legacy/Tools,,,0,,,,"help create column family in cassandra-cli refers to old, unsupported options.

- memtable_operations: Number of operations in millions before the memtable
  is flushed. Default is memtable_throughput / 64 * 0.3

- memtable_throughput: Maximum size in MB to let a memtable get to before
  it is flushed. Default is to use 1/16 the JVM heap size.",,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,hsn,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,40994,,,Fri Sep 30 20:46:09 UTC 2011,,,,,,,,,,"0|i0gi13:",94351,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"30/Sep/11 20:46;jbellis;fixed in r1177825, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI does not support removing compression options from a ColumnFamily,CASSANDRA-3282,12525272,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,zznate,zznate,30/Sep/11 06:08,16/Apr/19 09:32,14/Jul/23 05:52,30/Sep/11 20:20,1.0.0,,,Legacy/Tools,,,0,,,,This may be an issue with ThriftValidator as well - not accepting a null or empty compression properties map as a disable flag.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/11 20:10;xedin;CASSANDRA-3282-doc.patch;https://issues.apache.org/jira/secure/attachment/12497210/CASSANDRA-3282-doc.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,40960,,,Fri Sep 30 20:20:40 UTC 2011,,,,,,,,,,"0|i0gi0f:",94348,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"30/Sep/11 15:46;xedin;Works for me on the latest cassandra-1.0.0 branch:

{noformat}
[git:cassandra-1.0.0] (~/work/java/cassandra-trunk) → ./bin/cassandra-cli --host localhost
Connected to: ""Test Cluster"" on localhost/9160
Welcome to the Cassandra CLI.

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@unknown] create keyspace ks;
972385e0-eb7a-11e0-0000-242d50cf1fdd
Waiting for schema agreement...
... schemas agree across the cluster
[default@ks] create column family cf with comparator=UTF8Type and key_validation_class=UTF8Type and 
...	compression_options={sstable_compression:SnappyCompressor, chunk_length_kb:16};        
11870140-eb7b-11e0-0000-242d50cf1fdd
Waiting for schema agreement...
... schemas agree across the cluster
[default@ks] describe cf;                                                                           
    ColumnFamily: cf
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Key cache size / save period in seconds: 200000.0/14400
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
      Compression Options:
        chunk_length_kb: 16
        sstable_compression: org.apache.cassandra.io.compress.SnappyCompressor
[default@ks] update column family cf with comparator=UTF8Type and key_validation_class=UTF8Type and 
...	compression_options=null;                                                              
18177490-eb7b-11e0-0000-242d50cf1fdd
Waiting for schema agreement...
... schemas agree across the cluster
[default@ks] describe cf;                                                                           
    ColumnFamily: cf
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Key cache size / save period in seconds: 200000.0/14400
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy
[default@ks] quit;
{noformat}

Can you please provide at least your error message?;;;","30/Sep/11 16:18;zznate;Thanks Pavel - I just verified this on the rc1 binary as well. Can we keep this open in the context of adding the above to the CLI help?;;;","30/Sep/11 16:22;xedin;I will attach help update as a patch tonight.;;;","30/Sep/11 20:14;brandon.williams;+1;;;","30/Sep/11 20:20;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSLFactory should not enable cipher suites that aren't supported,CASSANDRA-3278,12525224,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,gcristea,gcristea,29/Sep/11 18:21,16/Apr/19 09:32,14/Jul/23 05:52,29/Nov/11 16:42,1.0.5,,,,,,0,,,,"The socket creation (server or otherwise) in SSLFactory.java calls [setEnabledCipherSuites|http://download.oracle.com/javase/6/docs/api/javax/net/ssl/SSLServerSocket.html#setEnabledCipherSuites(java.lang.String\[\])] with the values specified in EncryptionOptions.java:

{code}
public String[] cipherSuites = {
    ""TLS_RSA_WITH_AES_128_CBC_SHA"", 
    ""TLS_RSA_WITH_AES_256_CBC_SHA""
};
{code}

The call to [setEnabledCipherSuites|http://download.oracle.com/javase/6/docs/api/javax/net/ssl/SSLServerSocket.html#setEnabledCipherSuites(java.lang.String\[\])] fails on systems that don't have [Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files 6|http://www.oracle.com/technetwork/java/javase/downloads/jce-6-download-429243.html] because AES256 is not supported.

To avoid installing the unlimited strength policy file the code in SSLFactory.java should call [getSupportedCipherSuites|http://download.oracle.com/javase/6/docs/api/javax/net/ssl/SSLServerSocket.html#getSupportedCipherSuites()] to find out which of the suites specified are supported.

Thanks,
George",OpenJDK on debian squeeze,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/11 04:30;vijay2win@yahoo.com;0001-commiting-changes-to-make-the-ks-ts-more-flexible-v2.patch;https://issues.apache.org/jira/secure/attachment/12505446/0001-commiting-changes-to-make-the-ks-ts-more-flexible-v2.patch","06/Oct/11 23:46;vijay2win@yahoo.com;0001-commiting-filter-for-supported-suits.patch;https://issues.apache.org/jira/secure/attachment/12498096/0001-commiting-filter-for-supported-suits.patch","06/Oct/11 23:46;vijay2win@yahoo.com;0002-commiting-changes-to-make-the-ks-ts-more-flexible.patch;https://issues.apache.org/jira/secure/attachment/12498095/0002-commiting-changes-to-make-the-ks-ts-more-flexible.patch","29/Nov/11 04:30;vijay2win@yahoo.com;0002-commiting-filter-for-supported-suits-v2.patch;https://issues.apache.org/jira/secure/attachment/12505447/0002-commiting-filter-for-supported-suits-v2.patch","29/Nov/11 04:30;vijay2win@yahoo.com;0003-expose-the-available-options-in-yaml-v2.patch;https://issues.apache.org/jira/secure/attachment/12505448/0003-expose-the-available-options-in-yaml-v2.patch","06/Oct/11 23:46;vijay2win@yahoo.com;0003-expose-the-available-options-in-yaml.patch;https://issues.apache.org/jira/secure/attachment/12498094/0003-expose-the-available-options-in-yaml.patch","04/Oct/11 00:24;gcristea;cassandra-3278-cache.txt;https://issues.apache.org/jira/secure/attachment/12497575/cassandra-3278-cache.txt","04/Oct/11 00:24;gcristea;cassandra-3278-nocache.txt;https://issues.apache.org/jira/secure/attachment/12497574/cassandra-3278-nocache.txt",,,,,,,8.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,40339,,,Tue Nov 29 16:42:05 UTC 2011,,,,,,,,,,"0|i0ghyn:",94340,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"29/Sep/11 18:44;jbellis;Thanks for the bug report, George. It sounds like you have a good handle on this, can you submit a patch?;;;","30/Sep/11 00:34;gcristea;Sure, I'll try to get the patch to you by EOD tomorrow. 

Once question: do you see a problem with calling getSupportedCipherSuites and doing the filtering for every socket creation? 

Thanks,
George;;;","04/Oct/11 00:24;gcristea;I wasn't happy with reading the keystore/trusstore files and doing the cipher suites' filtering for each socket creation so I ended up creating two patches:
* cassandra-3278-nocache.txt: Does the filtering, the down side that the filtering is done for each socket that's created.
* cassandra-3278-cache.txt: Caches the SSLContext along with the supported cipher suites for server and non-server sockets. The down side is that changing the keystore/truststore requires a restart of the node.

I don't have enough information to decide which version is preferable, I leave that to you.

Thanks,
George;;;","05/Oct/11 20:43;vijay2win@yahoo.com;George,

Thanks for the patch,

The problem with the cached is that we need to restart the whole cluster when we change the KS/TS, instead we will have the flexibility if the new connections will just pick it up. We persist the connections untill disconnect hence the performance shouldn't be a concern. Also there can be variety of ssl client (example fat clients) which may have different sets of supported suits (caching one might not help).

1) cassandra-3278-nocache isn't a patch by itself (Can you rebase it?)
2) in the non cached one, If we can log a info on the filtered suit it will be great,

Just a side note... I would use Sets.intersection to reduce the amount of code :)
;;;","06/Oct/11 23:46;vijay2win@yahoo.com;We can also expose some of the available options so the users can choose.;;;","28/Oct/11 16:26;jbellis;Since Cassandra sockets are long-lived I'm fine with having filter-each-socket being the only option.;;;","28/Nov/11 21:22;brandon.williams;This looks good, but one minor nit: instead of cipherSuites in the yaml can we use the more idiomatic 'cipher_suites'?;;;","29/Nov/11 04:30;vijay2win@yahoo.com;Done!, the only change between the v1 is change from cipherSuites to cipher_suites. Thanks!;;;","29/Nov/11 16:42;brandon.williams;Thanks, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Cassandra compile under JDK 7,CASSANDRA-3275,12525211,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,satishbabu,xedin,xedin,29/Sep/11 16:59,16/Apr/19 09:32,14/Jul/23 05:52,30/Sep/11 15:36,1.0.1,,,,,,0,jdk7,,,"Currently system won't compile under JDK 7 because of errors in CQL JDBC component.

{noformat}
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CResultSet.java:39: error: CResultSet is not abstract and does not override abstract method <T>getObject(String,Class<T>) in ResultSet
    [javac] class CResultSet extends AbstractResultSet implements CassandraResultSet
    [javac] ^
    [javac]   where T is a type-variable:
    [javac]     T extends Object declared in method <T>getObject(String,Class<T>)
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraConnection.java:81: error: CassandraConnection is not abstract and does not override abstract method getNetworkTimeout() in Connection
    [javac] class CassandraConnection extends AbstractCassandraConnection implements Connection
    [javac] ^
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraDataSource.java:24: error: CassandraDataSource is not abstract and does not override abstract method getParentLogger() in CommonDataSource
    [javac] public class CassandraDataSource implements DataSource
    [javac]        ^
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraDatabaseMetaData.java:32: error: CassandraDatabaseMetaData is not abstract and does not override abstract method generatedKeyAlwaysReturned() in DatabaseMetaData
    [javac] class CassandraDatabaseMetaData implements DatabaseMetaData
    [javac] ^
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraDriver.java:40: error: CassandraDriver is not abstract and does not override abstract method getParentLogger() in Driver
    [javac] public class CassandraDriver implements Driver
    [javac]        ^
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraStatement.java:50: error: CassandraStatement is not abstract and does not override abstract method isCloseOnCompletion() in Statement
    [javac] class CassandraStatement extends AbstractStatement implements Statement
    [javac] ^
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraPreparedStatement.java:61: error: CassandraPreparedStatement is not abstract and does not override abstract method isCloseOnCompletion() in Statement
    [javac] class CassandraPreparedStatement extends CassandraStatement implements PreparedStatement
    [javac] ^
    [javac] Note: /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraPreparedStatement.java uses or overrides a deprecated API.
{noformat}",,satishbabu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/11 07:01;satishbabu;3275-patch.diff;https://issues.apache.org/jira/secure/attachment/12497122/3275-patch.diff",,,,,,,,,,,,,,1.0,satishbabu,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,40144,,,Fri Sep 30 16:41:44 UTC 2011,,,,,,,,,,"0|i0ghxr:",94336,,ardot,,ardot,Normal,,,,,,,,,,,,,,,,,"29/Sep/11 17:55;jbellis;Is that because JDK7 added some new ResultSet methods, or are we violating some generics rule that wasn't enforced before?;;;","30/Sep/11 05:45;satishbabu;In JDK7 several new methods are added to support JDBC 4.1, Need to update drivers to fix these errors

Area: API: JDBC
Synopsis: New JDBC Methods, Including new Methods in Interfaces
Description: For the Java SE 7 release, there are new methods to support JDBC 4.1. This includes methods added to the java.sql.Connection, java.sql.Driver, javax.sql.CommonDatasource, and java.sql.Statement interfaces. Because all methods of an interface must be implemented, previous code that uses these interfaces will not compile on Java SE 7 unless you add the new methods. See the JDBC documentation for more information.
Nature of Incompatibility: source

http://www.oracle.com/technetwork/java/javase/compatibility-417013.html
;;;","30/Sep/11 06:59;satishbabu;Added default implementation to newly added java.sql package;;;","30/Sep/11 13:39;ardot;+1 LGTM.;;;","30/Sep/11 13:54;xedin;Can you make this patch compatible with cassandra-1.0.0 branch? it seems like you use old directory structure - drivers/java...;;;","30/Sep/11 14:20;xedin;but I can confirm that this compiles so +1, I will check if we can move this to 1.1 and commit in that case.;;;","30/Sep/11 15:14;jbellis;IMO we should put it in 1.0.1.;;;","30/Sep/11 15:36;xedin;Committed.;;;","30/Sep/11 16:41;hudson;Integrated in Cassandra #1133 (See [https://builds.apache.org/job/Cassandra/1133/])
    fix JDBC driver to compile under JDK 7
patch by satishbabu; reviewed by xedin for CASSANDRA-3275

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177701
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/drivers/java/src/org/apache/cassandra/cql/jdbc/AbstractCassandraConnection.java
* /cassandra/trunk/drivers/java/src/org/apache/cassandra/cql/jdbc/AbstractResultSet.java
* /cassandra/trunk/drivers/java/src/org/apache/cassandra/cql/jdbc/AbstractStatement.java
* /cassandra/trunk/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraDataSource.java
* /cassandra/trunk/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraDatabaseMetaData.java
* /cassandra/trunk/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraDriver.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FailureDetector can take a very long time to mark a host down,CASSANDRA-3273,12525134,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,29/Sep/11 04:21,16/Apr/19 09:32,14/Jul/23 05:52,03/Oct/11 20:37,0.8.7,1.0.0,,,,,0,,,,"There are two ways to trigger this:

* Bring a node up very briefly in a mixed-version cluster and then terminate it
* Bring a node up, terminate it for a very long time, then bring it back up and take it down again

In the first case, what can happen is a very short interval arrival time is recorded by the versioning logic which requires reconnecting and can happen very quickly. This can easily be solved by rejecting any intervals within a reasonable bound, for instance the gossiper interval.

The second instance is harder to solve, because what is happening is that an extremely large interval is recorded, which is the time the node was left dead the first time.  This throws off the mean of the intervals and causes it to take a much longer time than it should to mark it down the second time.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/11 21:04;brandon.williams;3273.txt;https://issues.apache.org/jira/secure/attachment/12497059/3273.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,38928,,,Mon Oct 03 21:20:41 UTC 2011,,,,,,,,,,"0|i0ghwv:",94332,,thepaul,,thepaul,Normal,,,,,,,,,,,,,,,,,"29/Sep/11 04:36;jbellis;bq. a very short interval arrival time is recorded by the versioning logic which requires reconnecting and can happen very quickly

I thought only gossip heartbeats generate interval measurements, is that incorrect?

bq. an extremely large interval is recorded, which is the time the node was left dead the first time

What if we reset the intervals when we get a node back-from-the-dead?;;;","29/Sep/11 15:53;slebresne;As a side note, this is a big pain in the ass for (distributed) testing, where you start/stop node quickly.;;;","29/Sep/11 18:12;brandon.williams;bq. I thought only gossip heartbeats generate interval measurements, is that incorrect?

Heartbeats and generation changes.  I take back what I said though, it's not the versioning reconnection, and it's not a problem with regard to making the FD take a long time to mark a host down.

It is, however, possible to receive two intervals in a short amount of time, just due to timer skew between the two hosts, but it can only happen once since after that they will be in sync from the FD's perspective.

The net effect of this in the pathological case would be that the FD causes a host to be marked down if the host suddenly becomes silent for a period of 4-5s after the FD receives the initial (500ms) interval and then the short (1ms) one only.  

;;;","29/Sep/11 18:39;jbellis;So it sounds like only case 2 is worth bothering with?;;;","29/Sep/11 21:04;brandon.williams;bq. What if we reset the intervals when we get a node back-from-the-dead?

That makes sense if we're observing a generation change, the node either rebooted or was taken over by a new machine, so relearning the network characteristics is a good idea.

In the case that there was only a heartbeat change, that indicates there was something bad (most likely in the network) and we should remember that for next time to avoid flapping.  However, in the case of a long partition where the generation won't change, we don't want to record the partition time as an interval since if the partition reoccurs soon, it will take us a very long time to mark the host down again.

This patch clears the intervals on a generation change, and handles the long partition case by defining a reasonable maximum to record, in this case the rpc timeout, since adapting beyond this rather than failing quickly doesn't make much sense that I can think of, but I'll entertain a higher hard set default if anyone disagrees.;;;","30/Sep/11 00:20;cywjackson;smoke test only: so far so good

i have a node (6-node cluster) that was down for a LONG time (700 PHI), then start that node for about 30 sec before stopping it

ring shows that node is down in about 20-30secs, gives or takes

{noformat}
TRACE [GossipTasks:1] 2011-09-30 00:14:58,727 FailureDetector.java (line 156) PHI for /10.40.22.186 : 703.9568334429565
TRACE [GossipTasks:1] 2011-09-30 00:14:58,727 FailureDetector.java (line 160) notifying listeners that /10.40.22.186 is down
TRACE [GossipTasks:1] 2011-09-30 00:14:58,727 FailureDetector.java (line 161) intervals: 1027.0 1904.0 2153.0 951.0 215.0 1788.0 1002.0 1002.0 895.0 1133.0 1869.0 mean: 1267.1818181818182
DEBUG [GossipStage:1] 2011-09-30 00:14:58,728 Gossiper.java (line 661) Clearing interval times for /10.40.22.186 due to generation change
DEBUG [GossipStage:1] 2011-09-30 00:14:58,728 FailureDetector.java (line 242) Ignoring interval time of 2054002.0
TRACE [GossipTasks:1] 2011-09-30 00:14:59,729 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.0
TRACE [GossipTasks:1] 2011-09-30 00:15:00,730 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.43429448190325176
TRACE [GossipTasks:1] 2011-09-30 00:15:01,732 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.8690228244277856
TRACE [GossipTasks:1] 2011-09-30 00:15:02,733 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.2890479080886867
TRACE [GossipTasks:1] 2011-09-30 00:15:03,734 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.19662520906271305
TRACE [GossipTasks:1] 2011-09-30 00:15:04,735 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.20189636121957935
TRACE [GossipTasks:1] 2011-09-30 00:15:05,737 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.5977870734348798
TRACE [GossipTasks:1] 2011-09-30 00:15:06,738 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.20802340729819624
TRACE [GossipTasks:1] 2011-09-30 00:15:07,739 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.6139326289463335
TRACE [GossipTasks:1] 2011-09-30 00:15:08,740 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.21152308625862737
TRACE [GossipTasks:1] 2011-09-30 00:15:09,741 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.21261773854488178
TRACE [GossipTasks:1] 2011-09-30 00:15:10,743 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.6270982327510521
TRACE [GossipTasks:1] 2011-09-30 00:15:11,744 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.1968065146773795
TRACE [GossipTasks:1] 2011-09-30 00:15:12,745 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.579337235438655
TRACE [GossipTasks:1] 2011-09-30 00:15:13,746 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.37217274142982526
TRACE [GossipTasks:1] 2011-09-30 00:15:14,747 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.7443454828596505
TRACE [GossipTasks:1] 2011-09-30 00:15:15,757 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.3555955505071756
TRACE [GossipTasks:1] 2011-09-30 00:15:16,758 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.7083717111193488
TRACE [GossipTasks:1] 2011-09-30 00:15:17,759 FailureDetector.java (line 156) PHI for /10.40.22.186 : 1.061147871731522
TRACE [GossipTasks:1] 2011-09-30 00:15:18,760 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.3194684082936909
TRACE [GossipTasks:1] 2011-09-30 00:15:19,762 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.6395757534039692
TRACE [GossipTasks:1] 2011-09-30 00:15:20,763 FailureDetector.java (line 156) PHI for /10.40.22.186 : 0.9593636301059537
TRACE [GossipTasks:1] 2011-09-30 00:15:21,764 FailureDetector.java (line 156) PHI for /10.40.22.186 : 1.2791515068079384
TRACE [GossipTasks:1] 2011-09-30 00:15:22,765 FailureDetector.java (line 156) PHI for /10.40.22.186 : 1.598939383509923
TRACE [GossipTasks:1] 2011-09-30 00:15:23,767 FailureDetector.java (line 156) PHI for /10.40.22.186 : 1.919046728620201
TRACE [GossipTasks:1] 2011-09-30 00:15:24,768 FailureDetector.java (line 156) PHI for /10.40.22.186 : 2.238834605322186
TRACE [GossipTasks:1] 2011-09-30 00:15:25,769 FailureDetector.java (line 156) PHI for /10.40.22.186 : 2.5586224820241705
TRACE [GossipTasks:1] 2011-09-30 00:15:26,771 FailureDetector.java (line 156) PHI for /10.40.22.186 : 2.8787298271344484
TRACE [GossipTasks:1] 2011-09-30 00:15:27,772 FailureDetector.java (line 156) PHI for /10.40.22.186 : 3.198517703836433
TRACE [GossipTasks:1] 2011-09-30 00:15:28,773 FailureDetector.java (line 156) PHI for /10.40.22.186 : 3.518305580538418
TRACE [GossipTasks:1] 2011-09-30 00:15:29,774 FailureDetector.java (line 156) PHI for /10.40.22.186 : 3.838093457240402
TRACE [GossipTasks:1] 2011-09-30 00:15:30,776 FailureDetector.java (line 156) PHI for /10.40.22.186 : 4.158200802350681
TRACE [GossipTasks:1] 2011-09-30 00:15:31,777 FailureDetector.java (line 156) PHI for /10.40.22.186 : 4.4779886790526655
TRACE [GossipTasks:1] 2011-09-30 00:15:32,778 FailureDetector.java (line 156) PHI for /10.40.22.186 : 4.79777655575465
TRACE [GossipTasks:1] 2011-09-30 00:15:33,779 FailureDetector.java (line 156) PHI for /10.40.22.186 : 5.117564432456635
TRACE [GossipTasks:1] 2011-09-30 00:15:34,781 FailureDetector.java (line 156) PHI for /10.40.22.186 : 5.437671777566913
TRACE [GossipTasks:1] 2011-09-30 00:15:35,782 FailureDetector.java (line 156) PHI for /10.40.22.186 : 5.757459654268897
TRACE [GossipTasks:1] 2011-09-30 00:15:36,783 FailureDetector.java (line 156) PHI for /10.40.22.186 : 6.077247530970882
TRACE [GossipTasks:1] 2011-09-30 00:15:37,784 FailureDetector.java (line 156) PHI for /10.40.22.186 : 6.397035407672866
TRACE [GossipTasks:1] 2011-09-30 00:15:38,785 FailureDetector.java (line 156) PHI for /10.40.22.186 : 6.7168232843748505
TRACE [GossipTasks:1] 2011-09-30 00:15:39,786 FailureDetector.java (line 156) PHI for /10.40.22.186 : 7.036611161076836
TRACE [GossipTasks:1] 2011-09-30 00:15:40,788 FailureDetector.java (line 156) PHI for /10.40.22.186 : 7.356718506187114
TRACE [GossipTasks:1] 2011-09-30 00:15:41,789 FailureDetector.java (line 156) PHI for /10.40.22.186 : 7.676506382889099
TRACE [GossipTasks:1] 2011-09-30 00:15:42,790 FailureDetector.java (line 156) PHI for /10.40.22.186 : 7.996294259591083
TRACE [GossipTasks:1] 2011-09-30 00:15:43,791 FailureDetector.java (line 156) PHI for /10.40.22.186 : 8.316082136293067
TRACE [GossipTasks:1] 2011-09-30 00:15:43,792 FailureDetector.java (line 160) notifying listeners that /10.40.22.186 is down
TRACE [GossipTasks:1] 2011-09-30 00:15:43,792 FailureDetector.java (line 161) intervals: 1001.0 2004.0 1011.0 481.0 999.0 1514.0 487.0 1551.0 450.0 1001.0 2002.0 1516.0 2003.0 3012.0 mean: 1359.4285714285713
{noformat};;;","03/Oct/11 20:23;thepaul;+1;;;","03/Oct/11 20:37;brandon.williams;Committed.;;;","03/Oct/11 21:20;hudson;Integrated in Cassandra-0.8 #357 (See [https://builds.apache.org/job/Cassandra-0.8/357/])
    Fix bug where the FailureDetector can take a very long time to mark a
host down.
Patch by brandonwilliams, reviewed by Paul Cannon for CASSANDRA-3273

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1178563
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/FailureDetector.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/IFailureDetector.java
* /cassandra/branches/cassandra-1.0.0/CHANGES.txt
* /cassandra/branches/cassandra-1.0.0/src/java/org/apache/cassandra/gms/FailureDetector.java
* /cassandra/branches/cassandra-1.0.0/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-1.0.0/src/java/org/apache/cassandra/gms/IFailureDetector.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
READ Operation with CL=EACH_QUORUM succeed when a DC is down (RF=3),CASSANDRA-3272,12525118,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,cywjackson,cywjackson,28/Sep/11 23:25,16/Apr/19 09:32,14/Jul/23 05:52,18/Oct/11 14:06,1.1.0,,,,,,0,,,,"""READ EACH_QUORUM: 	Returns the record with the most recent timestamp once a quorum of replicas in each data center of the cluster has responded.""

In other words, if a DC is down and the QUORUM could not be reached on that DC, read should fail.

test case:
- Cassandra version 0.8.6:
INFO [main] 2011-09-28 22:26:24,297 StorageService.java (line 371) Cassandra version: 0.8.6

- 6-node cluster with 2 DC and 3 node each. RF=3 in each DC:
[default@Keyspace3] describe keyspace;
Keyspace: Keyspace3:
Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
Durable Writes: true
Options: [DC2:3, DC1:3]
Column Families:
ColumnFamily: test
Key Validation Class: org.apache.cassandra.db.marshal.BytesType
Default column value validator: org.apache.cassandra.db.marshal.BytesType
Columns sorted by: org.apache.cassandra.db.marshal.BytesType
Row cache size / save period in seconds: 0.0/0
Key cache size / save period in seconds: 200000.0/14400
Memtable thresholds: 1.0875/1440/232 (millions of ops/minutes/MB)
GC grace seconds: 864000
Compaction min/max thresholds: 4/32
Read repair chance: 1.0
Replicate on write: true
Built indexes: []

all nodes are up, insert a row:

$ nodetool -h localhost ring
Address DC Rack Status State Load Owns Token
141784319550391026443072753096570088106
10.34.79.179 DC1 RAC1 Up Normal 11.13 KB 16.67% 0
10.34.70.163 DC2 RAC1 Up Normal 11.14 KB 16.67% 28356863910078205288614550619314017621
10.35.81.147 DC1 RAC1 Up Normal 11.14 KB 16.67% 56713727820156410577229101238628035242
10.84.233.170 DC2 RAC1 Up Normal 11.14 KB 16.67% 85070591730234615865843651857942052864
10.195.201.236 DC1 RAC1 Up Normal 11.14 KB 16.67% 113427455640312821154458202477256070485
10.118.147.73 DC2 RAC1 Up Normal 11.14 KB 16.67% 141784319550391026443072753096570088106

- insert a value 

[default@Keyspace3] set test[utf8('test-key-1')][utf8('test-col')]=utf8('test-value');
Value inserted.

sanity check (cli connects to a node in DC1) :
[default@Keyspace3] consistencylevel as EACH_QUORUM;                                  
Consistency level is set to 'EACH_QUORUM'.
[default@Keyspace3] get test[utf8('test-key-1')];   
=> (column=746573742d636f6c, value=test-value, timestamp=1317249361722000)
Returned 1 results

shut down DC2:
$ nodetool -h localhost ring
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               141784319550391026443072753096570088106     
10.34.79.179    DC1         RAC1        Up     Normal  51.86 KB        16.67%  0                                           
10.34.70.163    DC2         RAC1        Down   Normal  51.88 KB        16.67%  28356863910078205288614550619314017621      
10.35.81.147    DC1         RAC1        Up     Normal  47.5 KB         16.67%  56713727820156410577229101238628035242      
10.84.233.170   DC2         RAC1        Down   Normal  51.88 KB        16.67%  85070591730234615865843651857942052864      
10.195.201.236  DC1         RAC1        Up     Normal  47.5 KB         16.67%  113427455640312821154458202477256070485     
10.118.147.73   DC2         RAC1        Down   Normal  51.88 KB        16.67%  141784319550391026443072753096570088106  

[default@Keyspace3] get test[utf8('test-key-1')];   
=> (column=746573742d636f6c, value=746573742d76616c7565, timestamp=1317249361722000)
Returned 1 results.

tried with pycassaShell:
>>> col_fam.get('test-key-1',read_consistency_level=pycassa.ConsistencyLevel.EACH_QUORUM)
OrderedDict([('test-col', 'test-value')])
",,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/11 22:02;jbellis;3272-v2.txt;https://issues.apache.org/jira/secure/attachment/12499103/3272-v2.txt","13/Oct/11 19:12;jbellis;3272.txt;https://issues.apache.org/jira/secure/attachment/12498911/3272.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,37574,,,Tue Oct 18 15:26:21 UTC 2011,,,,,,,,,,"0|i0ghwf:",94330,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"29/Sep/11 00:39;jbellis;Why are you using EACH_QUORUM?;;;","29/Sep/11 00:40;jbellis;(I ask because I've never seen it be the ""right"" CL yet.);;;","29/Sep/11 00:52;brandon.williams;bq. In other words, if a DC is down and the QUORUM could not be reached on that DC, read should fail.

I can't think of any reason someone would actually want to do EACH_QUORUM on a read, when LOCAL_QUORUM would be sufficient if the write was done at EACH_QUORUM.  ;;;","29/Sep/11 01:41;cywjackson;""Why are you using EACH_QUORUM?""
 -- no particular reason other than testing.

""I can't think of any reason someone would actually want to do EACH_QUORUM on a read, when LOCAL_QUORUM would be sufficient if the write was done at EACH_QUORUM. ""
 -- would it make sense to make such consistency level as an ""invalid to use"" for READ then?

What if write (update, say add a new column/value to the key) is done via LOCAL_QUORUM to DC1 but DC2 were down. 
anti-entropy repair wasn't done
stop DC1 (for some reason this step is important)
now bring DC2 back up (assume HH is off)
read LQ from DC2.. 
you get an outdated result. 

at least i think with EACH_QUORUM on a read, you should get an consistent result if 2 nodes replicas agreed on each DC (in the example where RF=3 in each DC)

(and you should get an UAE in the above example);;;","13/Oct/11 19:12;jbellis;currently EACH_QUORUM silently does LOCAL_QUORUM instead. Patch attached to raise an error saying EACH_QUORUM is only supported for writes.;;;","14/Oct/11 08:28;slebresne;Shouldn't we move the validation up. We already have a validateConsistencyLevel in ThriftValidation for instance, we could just add a flag to if it's a write or read operation and throw the right exception there. If only for consistency, but I think it's nice to keep validating the queries upfront so we don't have to later anyway.

Which btw make me see that this validateConsistencyLevel is not called by CQL (which thus don't do the right check this function already does I suppose, or duplicate it). It would be worth checking if that's the only place where CQL is more loose than thrift in validating the query (and maybe we could refactor a bit the validation code so it's easier to apply it to both thrift and CQL).;;;","14/Oct/11 22:02;jbellis;v2 attached;;;","18/Oct/11 10:34;slebresne;+1 with the only nitpick that validateStrategyForCL could probably be more accurately (though less concisely) named validateStrategyForMultiDataCenterCL since it don't really use the consistency level excepted for the exception message.;;;","18/Oct/11 14:06;jbellis;renamed to requireNetworkTopologyStrategy and committed;;;","18/Oct/11 15:26;hudson;Integrated in Cassandra #1160 (See [https://builds.apache.org/job/Cassandra/1160/])
    EACH_QUORUM is only supported for writes
patch by jbellis; reviewed by slebresne for CASSANDRA-3272

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1185669
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/NEWS.txt
* /cassandra/trunk/src/java/org/apache/cassandra/cql/QueryProcessor.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/CassandraServer.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/RequestType.java
* /cassandra/trunk/src/java/org/apache/cassandra/thrift/ThriftValidation.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error during multi-threaded compaction in 0.8,CASSANDRA-3270,12525107,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,kmueller,kmueller,28/Sep/11 21:40,16/Apr/19 09:32,14/Jul/23 05:52,29/Sep/11 22:09,1.0.0,,,,,,0,compaction,,,"I'm running 0.8.6 plus the multi-threaded compaction patch in issue 2901.  I'm getting an error compacting last night:


Error occured during compaction
java.util.concurrent.ExecutionException: java.lang.ClassCastException: java.util.concurrent.ThreadPoolExecutor cannot be cast to org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.compaction.CompactionManager.performMajor(CompactionManager.java:278)
        at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1856)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1447)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: java.util.concurrent.ThreadPoolExecutor cannot be cast to org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:53)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:92)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Reducer.getCompactedRow(ParallelCompactionIterable.java:211)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Reducer.getReduced(ParallelCompactionIterable.java:185)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Reducer.getReduced(ParallelCompactionIterable.java:146)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:105)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:92)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.compaction.CompactionManager.doCompactionWithoutSizeEstimation(CompactionManager.java:573)
        at org.apache.cassandra.db.compaction.CompactionManager.doCompaction(CompactionManager.java:507)
        at org.apache.cassandra.db.compaction.CompactionManager$4.call(CompactionManager.java:320)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/11 17:03;jbellis;2901-0.8-v2.txt;https://issues.apache.org/jira/secure/attachment/12497187/2901-0.8-v2.txt","29/Sep/11 22:09;jbellis;2901-0.8-v2.txt;https://issues.apache.org/jira/secure/attachment/12497070/2901-0.8-v2.txt","28/Sep/11 22:04;jbellis;3270.txt;https://issues.apache.org/jira/secure/attachment/12496934/3270.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,37561,,,Fri Sep 30 21:28:43 UTC 2011,,,,,,,,,,"0|i0ghvj:",94326,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"28/Sep/11 22:04;jbellis;Patch (against trunk) to fix the problem.  Should also apply to 0.8 + the old 2901 patch with minimal effort.;;;","28/Sep/11 22:47;kmueller;Applying the patch by hand to 0.8 gives me:


    [javac] src/java/org/apache/cassandra/db/compaction/ParallelCompactionIterable.java:89: cannot find symbol
    [javac] symbol  : constructor Reducer(org.apache.commons.collections.iterators.CollatingIterator)
    [javac] location: class org.apache.cassandra.db.compaction.ParallelCompactionIterable.Reducer
    [javac]         return new Unwrapper(new Reducer(iter), controller);
    [javac]                              ^
    [javac] src/java/org/apache/cassandra/db/compaction/ParallelCompactionIterable.java:146: cannot find symbol
    [javac] symbol  : constructor ReducingIterator()
    [javac] location: class org.apache.cassandra.utils.ReducingIterator<org.apache.cassandra.db.compaction.ParallelCompactionIterable.RowContainer,org.apache.cassandra.db.compaction.ParallelCompactionIterable.CompactedRowContainer>
    [javac]     private class Reducer extends ReducingIterator<RowContainer, CompactedRowContainer> implements CloseableIterator<CompactedRowContainer>
    [javac]             ^

This is probably something trivial, but I don't know the rest to make it work in 0.8?;;;","29/Sep/11 08:56;slebresne;lgtm, +1

@Karl: that error seems completely unrelated to the patch on this ticket, but seems more like you applied the wrong patch from 2901 (the one for 1.0.0).;;;","29/Sep/11 22:09;jbellis;Patch attached with both 2901 and 3270 rebased and combined for 0.8.;;;","29/Sep/11 22:09;jbellis;committed to trunk;;;","29/Sep/11 23:23;kmueller;Thanks Jonathan.  

I will apply this patch to 0.8 trunk and test MT compaction performance as we discussed in email.;;;","30/Sep/11 07:27;kmueller;Getting this issue applying 2901-v8-2.txt against 0.8 trunk: http://pastebin.com/81WQxkYV;;;","30/Sep/11 17:03;jbellis;updated v2 to include new files;;;","30/Sep/11 19:57;kmueller;Sorry to be a pain in the butt, but it's not starting up.  I get the same error with 0.8.6 based source or 0.8 trunk.  This is with the updated v2 patch. 


Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/CassandraDaemon
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.CassandraDaemon
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
Could not find the main class: org.apache.cassandra.thrift.CassandraDaemon.  Program will exit.
;;;","30/Sep/11 20:35;jbellis;Usually that just means ant got confused.  Try ""ant realclean"" first.;;;","30/Sep/11 21:28;kmueller;That worked.  Going to run some tests over the weekend for this and 2901 as discussed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
possible early deletion of commit logs,CASSANDRA-3269,12525098,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,yangyangyyy,yangyangyyy,28/Sep/11 20:27,16/Apr/19 09:32,14/Jul/23 05:52,04/Oct/11 16:59,1.0.0,,,,,,0,,,,"I ran my cluster for about 2 days. the cluster has 2 nodes. I restarted one box several times, and the other one was always running. the one always running ended up accumulating 100GB of commit logs.


this is 1.0.0 code from about Sept 15 in github. I kept the original setting for 
#commitlog_total_space_in_mb: 4096
i.e. commented out


here is some sample of the output:

-rw-r--r-- 1 yyang yyang 134217857 2011-09-28 03:51 CommitLog-1317181834810.log
-rw-r--r-- 1 yyang yyang 134217869 2011-09-28 03:50 CommitLog-1317181764105.log
-rw-r--r-- 1 yyang yyang 134217783 2011-09-28 03:49 CommitLog-1317181694633.log
-rw-r--r-- 1 yyang yyang 134217750 2011-09-28 02:39 CommitLog-1317176955102.log
yyang@ip-10-71-21-46:/mnt/cass/log/cassandra$ ls -lt /mnt/cass/lib//cassandra/commitlog/|wc -l
727
yyang@ip-10-71-21-46:/mnt/cass/log/cassandra$ du -s /mnt/cass/lib/cassandra/commitlog/ 
95095316        /mnt/cass/lib/cassandra/commitlog/
",,yangyangyyy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/11 15:14;jbellis;3269-v2.txt;https://issues.apache.org/jira/secure/attachment/12497474/3269-v2.txt","03/Oct/11 15:23;jbellis;3269-v3.txt;https://issues.apache.org/jira/secure/attachment/12497475/3269-v3.txt","03/Oct/11 11:33;slebresne;3269.patch;https://issues.apache.org/jira/secure/attachment/12497443/3269.patch",,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,37551,,,Tue Oct 04 16:58:45 UTC 2011,,,,,,,,,,"0|i0ghv3:",94324,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"28/Sep/11 20:46;jbellis;Hmm.  Would be useful to have a debug log -- it would say this

{noformat}
                logger.debug(""Not safe to delete commit log "" + segment + ""; dirty is "" + segment.dirtyString() + ""; hasNext: "" + iter.hasNext());
{noformat}

whenever a CF was flushed but it can't remove a segment.

For bonus points, you could add a log message in CommitLog before it creates the forceFlush runnable in createNewSegment when the size cap is exceeded.;;;","28/Sep/11 22:07;yangyangyyy;Thanks Jonathan. 
unfortunately I removed the logs. I'll keep the load testing, if this appears again, I'll update this bug;;;","29/Sep/11 17:01;yangyangyyy;I ran the code for a while, it created a lot of commit logs (not yet over the 4G limit ), I shut it down, ran with the code with extra debugging. 

then it showed the following messages, saying that not safe to remove. but in the end, the old commit logs were indeed removed, and I have only a new one left. (see the ls output at the end)



 INFO [FlushWriter:2] 2011-09-29 16:50:28,410 Memtable.java (line 236) Writing Memtable-multi_click_filter@1810361645(106184307/1327303837 serialized/live bytes, 1538903 ops)
DEBUG [COMMIT-LOG-WRITER] 2011-09-29 16:50:28,410 CommitLog.java (line 501) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317314880183.log); dirty is ; hasNext: false
 INFO [FlushWriter:2] 2011-09-29 16:50:31,610 Memtable.java (line 272) Completed flushing /mnt/cass/lib/cassandra/data/testBudget_items/multi_click_filter-h-33-Data.db (118127883 bytes)
DEBUG [COMMIT-LOG-WRITER] 2011-09-29 16:50:31,611 CommitLog.java (line 461) discard completed log segments for ReplayPosition(segmentId=1317314880183, position=0), column family 1010.
 INFO [FlushWriter:2] 2011-09-29 16:50:31,611 Memtable.java (line 236) Writing Memtable-IpFilter@1199856819(109065609/1328000737 serialized/live bytes, 1580661 ops)
DEBUG [COMMIT-LOG-WRITER] 2011-09-29 16:50:31,612 CommitLog.java (line 501) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317314880183.log); dirty is ; hasNext: false
 INFO [FlushWriter:2] 2011-09-29 16:50:34,038 Memtable.java (line 272) Completed flushing /mnt/cass/lib/cassandra/data/testBudget_items/IpFilter-h-33-Data.db (107340235 bytes)
DEBUG [COMMIT-LOG-WRITER] 2011-09-29 16:50:34,040 CommitLog.java (line 461) discard completed log segments for ReplayPosition(segmentId=1317314880183, position=0), column family 1013.
 INFO [FlushWriter:2] 2011-09-29 16:50:34,040 Memtable.java (line 236) Writing Memtable-opsMetrics@1355039451(1297090/77250 serialized/live bytes, 15090 ops)
DEBUG [COMMIT-LOG-WRITER] 2011-09-29 16:50:34,040 CommitLog.java (line 501) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317314880183.log); dirty is ; hasNext: false
 INFO [FlushWriter:2] 2011-09-29 16:50:34,129 Memtable.java (line 272) Completed flushing /mnt/cass/lib/cassandra/data/testBudget_counters/opsMetrics-h-19-Data.db (6282 bytes)
DEBUG [COMMIT-LOG-WRITER] 2011-09-29 16:50:34,130 CommitLog.java (line 461) discard completed log segments for ReplayPosition(segmentId=1317314880183, position=0), column family 1001.
DEBUG [COMMIT-LOG-WRITER] 2011-09-29 16:50:34,130 CommitLog.java (line 501) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317314880183.log); dirty is ; hasNext: false






yyang@ip-10-71-21-46:/mnt/cass/log/cassandra$ ls -lt /mnt/cass/lib/cassandra/commitlog/
total 2474280
-rw-r--r-- 1 yyang yyang         0 2011-09-29 16:48 CommitLog-1317314880183.log
-rw-r--r-- 1 yyang yyang         0 2011-09-29 16:43 CommitLog-1317314635458.log
-rw-r--r-- 1 yyang yyang         0 2011-09-29 16:40 CommitLog-1317314406536.log
-rw-r--r-- 1 yyang yyang         0 2011-09-29 16:36 CommitLog-1317314186503.log
-rw-r--r-- 1 yyang yyang         0 2011-09-29 16:21 CommitLog-1317313281629.log
-rw-r--r-- 1 yyang yyang 115115322 2011-09-29 16:02 CommitLog-1317312032419.log
-rw-r--r-- 1 yyang yyang 134217870 2011-09-29 16:00 CommitLog-1317311854994.log
-rw-r--r-- 1 yyang yyang 134217761 2011-09-29 15:57 CommitLog-1317311685452.log
-rw-r--r-- 1 yyang yyang 134217843 2011-09-29 15:54 CommitLog-1317311520485.log
-rw-r--r-- 1 yyang yyang 134217795 2011-09-29 15:52 CommitLog-1317311347744.log
-rw-r--r-- 1 yyang yyang 134217777 2011-09-29 15:49 CommitLog-1317311185371.log
-rw-r--r-- 1 yyang yyang 134217819 2011-09-29 15:46 CommitLog-1317311021414.log
-rw-r--r-- 1 yyang yyang 134217766 2011-09-29 15:43 CommitLog-1317310849097.log
-rw-r--r-- 1 yyang yyang 134217800 2011-09-29 15:40 CommitLog-1317310682639.log
-rw-r--r-- 1 yyang yyang 134217867 2011-09-29 15:38 CommitLog-1317310520489.log
-rw-r--r-- 1 yyang yyang 134217882 2011-09-29 15:35 CommitLog-1317310339007.log
-rw-r--r-- 1 yyang yyang 134217857 2011-09-29 15:32 CommitLog-1317310162460.log
-rw-r--r-- 1 yyang yyang 134217876 2011-09-29 15:29 CommitLog-1317310000804.log
-rw-r--r-- 1 yyang yyang 134217883 2011-09-29 15:26 CommitLog-1317309831513.log
-rw-r--r-- 1 yyang yyang 134217886 2011-09-29 15:23 CommitLog-1317309658187.log
-rw-r--r-- 1 yyang yyang 134217846 2011-09-29 15:20 CommitLog-1317309490763.log
-rw-r--r-- 1 yyang yyang 134217741 2011-09-29 15:18 CommitLog-1317309323730.log
-rw-r--r-- 1 yyang yyang 134217828 2011-09-29 15:15 CommitLog-1317309154140.log
-rw-r--r-- 1 yyang yyang 134217738 2011-09-29 15:12 CommitLog-1317308989678.log
yyang@ip-10-71-21-46:/mnt/cass/log/cassandra$ ls -lt /mnt/cass/lib/cassandra/commitlog/
total 4
-rw-r--r-- 1 yyang yyang 270 2011-09-29 16:50 CommitLog-1317314880183.log
;;;","29/Sep/11 17:58;jbellis;Are you simply writing faster than it can flush?;;;","29/Sep/11 18:13;yangyangyyy;I don't think so, I have set the memtable_total_space to be very high
due to my larger memory available, and rely on the
commit_log_total_size to trigger flushing  (also I changed the
getLiveSize() ... as I said in the other JIRA I commented , since my
traffic contains a large portion of counter adds). so essentially the
only trigger for flushing should come from commit_log_total_size
threshold,  but I don't see these flushing requests coming out, it's
not that the flushing started and can't finish.


I'm still running it, let me set it to a smaller threshold and
hopefully get to the point soon.

On Thu, Sep 29, 2011 at 10:59 AM, Jonathan Ellis (Commented) (JIRA)
<jira@apache.org> wrote:
;;;","02/Oct/11 23:40;yangyangyyy;the following shows that createNewSegment() sees that for  the oldest segment file xxxxx63739.log , it's forcing out a flush of LocationInfo CF. it does this multiple times (at least 1000 times ... ), but the actualy CFS.forceFlush() always says that the CF is clean. so no flushing happens.



DEBUG [COMMIT-LOG-WRITER] 2011-10-02 12:29:22,217 CommitLog.java (line 572) create new segment
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 12:29:22,217 CommitLog.java (line 523)  commit log total size now:23488116331
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 12:29:22,217 CommitLog.java (line 578) forcing a flush on segments : CommitLog-1317524063739.log out of 176
 WARN [COMMIT-LOG-WRITER] 2011-10-02 12:29:22,217 ColumnFamilyStore.java (line 718) forceFlush requested but everything is clean
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 12:29:22,218 CommitLog.java (line 584) forcing out CF:LocationInfo
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 12:30:15,357 CommitLog.java (line 572) create new segment
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 12:30:15,357 CommitLog.java (line 523)  commit log total size now:23622334131


but when flushing (triggered by other cf) happens, the discardSegment() says that the oldest one is still dirty on LocationInfo.



DEBUG [COMMIT-LOG-WRITER] 2011-10-02 05:11:14,182 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is opsMetrics (1001), LocationInfo (0), Budget (1000), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:21:26,481 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), Budget (1000), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:21:26,868 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:32:15,341 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:32:20,905 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:32:39,725 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:32:54,023 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:33:01,934 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 06:33:11,167 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 08:16:28,250 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 08:16:36,208 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 08:16:56,037 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 08:17:18,003 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: true



since the isClean() of CFS is determined by columnFamilies.size() and isClean() of segment is determined by lastCFWrite(), it seems that these 2 data structures somehow got out of sync.


also an easy suspect is that the lastCFWrite is not concurrent, but I can't show an exact scenario of how this would lead to the symptom I see



;;;","03/Oct/11 06:51;yangyangyyy;this piece of logging is particularly interesting.  the LocationInfo CF is flushed, right after that, it's trying to discard oldest segment, but the discard fails saying that locationInfo bit is dirty. but supposedly the bit should have been turned off




 INFO [main] 2011-10-02 04:22:55,033 CommitLog.java (line 174) Log replay complete, 15600762 replayed mutations
 INFO [main] 2011-10-02 04:22:58,560 ColumnFamilyStore.java (line 651) flush position is ReplayPosition(segmentId=1317524063739,
 position=0)

 INFO [main] 2011-10-02 04:22:59,244 ColumnFamilyStore.java (line 665) Enqueuing flush of Memtable-LocationInfo@2115195389(29/36
 serialized/live bytes, 1 ops)
 INFO [FlushWriter:2] 2011-10-02 04:22:59,244 Memtable.java (line 283) flushing memtable LocationInfo
 INFO [FlushWriter:2] 2011-10-02 04:22:59,244 Memtable.java (line 287) entered lock flushing memtable LocationInfo
 INFO [FlushWriter:2] 2011-10-02 04:22:59,244 Memtable.java (line 290) really start flushing memtable LocationInfo
 INFO [FlushWriter:2] 2011-10-02 04:22:59,245 Memtable.java (line 236) Writing Memtable-LocationInfo@2115195389(29/36 serialized
/live bytes, 1 ops)
 INFO [FlushWriter:2] 2011-10-02 04:22:59,396 Memtable.java (line 272) Completed flushing /mnt/cass/lib/cassandra/data/system/Lo
cationInfo-h-4-Data.db (80 bytes)
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 04:22:59,397 CommitLog.java (line 461) discard completed log segments for ReplayPosition(segmentId=1317524063739, position=0), column family 0.
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 04:22:59,397 CommitLog.java (line 473) trying to discard segment CommitLog-1317524063739.log
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 04:22:59,398 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: false
 INFO [main] 2011-10-02 04:22:59,447 ColumnFamilyStore.java (line 651) flush position is ReplayPosition(segmentId=1317524063739, position=174)
 INFO [main] 2011-10-02 04:22:59,447 ColumnFamilyStore.java (line 665) Enqueuing flush of Memtable-LocationInfo@2010157886(53/66 serialized/live bytes, 2 ops)
 INFO [FlushWriter:2] 2011-10-02 04:22:59,448 Memtable.java (line 283) flushing memtable LocationInfo
 INFO [FlushWriter:2] 2011-10-02 04:22:59,448 Memtable.java (line 287) entered lock flushing memtable LocationInfo
 INFO [FlushWriter:2] 2011-10-02 04:22:59,448 Memtable.java (line 290) really start flushing memtable LocationInfo
 INFO [FlushWriter:2] 2011-10-02 04:22:59,448 Memtable.java (line 236) Writing Memtable-LocationInfo@2010157886(53/66 serialized/live bytes, 2 ops)
 INFO [FlushWriter:2] 2011-10-02 04:22:59,572 Memtable.java (line 272) Completed flushing /mnt/cass/lib/cassandra/data/system/LocationInfo-h-6-Data.db (163 bytes)
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 04:22:59,573 CommitLog.java (line 461) discard completed log segments for ReplayPosition(segmentId=1317524063739, position=174), column family 0.
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 04:22:59,573 CommitLog.java (line 473) trying to discard segment CommitLog-1317524063739.log
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 04:22:59,573 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: false
 WARN [MutationStage:23] 2011-10-02 04:23:21,015 Memtable.java (line 136) MemoryMeter uninitialized (jamm not specified as java :
;;;","03/Oct/11 07:19;yangyangyyy;I think I found why:



CommitLogSegment.turnOffIfNotWritten() :
    void turnOffIfNotWritten(Integer cfId, Integer flushPosition)
    {
        Integer lastWritten = cfLastWrite.get(cfId);
        if (lastWritten == null || lastWritten < flushPosition)
            cfLastWrite.remove(cfId);
    }

the comparison ""<"" probably should be ""<=""

let's say we flush a Memtable, and we have only 1 commitlogsegment in system now. after the flush, no write has come in, now the post-flush job tries to discard segments, 
the context passed in points to the current pointer, which is the same as cfLastWrite, 
then the comparison fails here, and the bit in this logsegment is not removed.

later, either the cf is never flushed, or when it is being forcefully flushed, we find that there is no new writes to the CF, so the CF isclean(), so the flush is never submitted, and the post-flush can never be run, so the dirty bit can never be cleared.....

sounds right?;;;","03/Oct/11 09:31;slebresne;Actually no. The cfLastWrite position is the last position we started writing at for the given column family, not the last position we finished writing to. More precisely, we populate cfLastWrite in CLS.turnOn() which itself is called in CLS.write(RowMutation) just before the actual write with the position prior to the write. In other words, if lastWritten == flushPosition, it can only mean that the flush grabbed the commit log position *before* the last write (and thus we don't want to turn the segment off).;;;","03/Oct/11 11:33;slebresne;But there is something very wrong in the commit log code. The commit r1171248 broke it (we ended up always setting the position of the LastWrite at -1). That commit is from September 15th, so that could match with what you're seeing, even though it should have resulted in commit log segment being deleted sooner than is safe rather than later. Attaching patch to fix anyway.;;;","03/Oct/11 15:14;jbellis;not to bikeshed this too much, but v2 moves context and currentPosition out of the try/catch to make it clear that cP will always be valid in the catch section.  also merging declaration and assignment of cP prevents bogus refactors like the original regression.;;;","03/Oct/11 15:23;jbellis;v3 inlines cP entirely.;;;","03/Oct/11 15:25;slebresne;+1 on v3;;;","03/Oct/11 15:29;jbellis;committed v3;;;","03/Oct/11 16:22;slebresne;To be clear, the committed patch is probably not the source of the problem Yang is seeing, so let's keep that open for now.

Yang, I don't see anything yet from the logs you've pasted that seem wrong per se. Can you try to lower the commit_log_total_size
to something small (like 300MB) and see if you can reproduce easily ? (you should hopefully never have more than 3 commit logs (2 even) since when the third one is created, we should be over the limit and the older one should be deleted).;;;","03/Oct/11 16:31;yangyangyyy;Sylvain:


I did try to move down the threshold to 300MB, but in that case, it does not seem to occur easily (I haven't seen the issue appear after running under a 300MB threshold for 10 hours).


but if you look at  comment https://issues.apache.org/jira/browse/CASSANDRA-3269?focusedCommentId=13119163&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13119163


you see that LocationInfo being flushed already, twice, once at position 0, once at 174, then right after that, it tries to discard the segment, but complains that it's dirty, and the only dirty bit is locationInfo itself. that is the part that doesn't sound right.

;;;","03/Oct/11 17:05;yangyangyyy;I'm a bit unclear about the logic in cfLastWrites vs position, it seems that cflastWrites take value from the logwriter position, which always increases, so cfLastWrite can be only smaller or equal than the position, which is the value obtained through maybeSwitchMemtable()---->getContext() .


but in log I see


 WARN [COMMIT-LOG-WRITER] 2011-10-03 15:48:38,038 CommitLogSegment.java (line 205) turnOffIfNotWritten:ad_ip_agent 50020888/3959
3935


the above logging is produced in 



    void turnOffIfNotWritten(Integer cfId, Integer flushPosition)
    {   
        Integer lastWritten = cfLastWrite.get(cfId);
                String keypace = Schema.instance.getCF(cfId).left;

                final ColumnFamilyStore cfs = Table.open(keypace).getColumnFamilyStore(cfId);
                logger.warn(""turnOffIfNotWritten:"" + cfs.columnFamily + "" "" + lastWritten + ""/"" + flushPosition);

        if (lastWritten == null || lastWritten < flushPosition)
            cfLastWrite.remove(cfId);
    }
;;;","03/Oct/11 17:34;yangyangyyy;I changed the commitlog rotation size to 5MB and total to 200MB, see if the symptoms come out more easily;;;","03/Oct/11 21:37;yangyangyyy;I added a warn() to CLS.turnOffIfNotWritten() :

    void turnOffIfNotWritten(Integer cfId, Integer flushPosition)
    {  
        Integer lastWritten = cfLastWrite.get(cfId);
                String keypace = Schema.instance.getCF(cfId).left;

                final ColumnFamilyStore cfs = Table.open(keypace).getColumnFamilyStore(cfId);
                logger.warn(""turnOffIfNotWritten:"" + cfs.columnFamily + "" "" + lastWritten + ""/"" + flushPosition);

        if (lastWritten == null || lastWritten < flushPosition)
            cfLastWrite.remove(cfId);
    }



I saw many cases where lastWritten is > than the flushPosition, this does not make sense. 
I think this is definitely going to lead to the un-purged commit logs symptoms, IF no more writes are added onto the dirty CFs in the oldest segment. so the analysis I gave before still holds, but we just need to find out why  lastWritten > flushPosition.

 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:19,552 CommitLogSegment.java (line 205) turnOffIfNotWritten:ad_impression_session 96030/46355
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:22,688 CommitLogSegment.java (line 205) turnOffIfNotWritten:ad_ip_agent 3070922/46355
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:24,397 CommitLogSegment.java (line 205) turnOffIfNotWritten:multi_click_filter 4996295/46522
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:25,576 CommitLogSegment.java (line 205) turnOffIfNotWritten:session_limit_filter 5242578/59906
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:27,013 CommitLogSegment.java (line 205) turnOffIfNotWritten:measuredSession 5241697/80685
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:28,740 CommitLogSegment.java (line 205) turnOffIfNotWritten:IpFilter 5242052/96877
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:28,919 CommitLogSegment.java (line 205) turnOffIfNotWritten:session_limit_filter 5242475/870755
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:29,187 CommitLogSegment.java (line 205) turnOffIfNotWritten:measuredSession 5242807/890241
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:29,471 CommitLogSegment.java (line 205) turnOffIfNotWritten:IpFilter 5242120/890241
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:43:29,690 CommitLogSegment.java (line 205) turnOffIfNotWritten:IpFilter 3422607/768691
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:45:36,674 CommitLogSegment.java (line 205) turnOffIfNotWritten:ad_impression_session 3552787/165636
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:45:39,484 CommitLogSegment.java (line 205) turnOffIfNotWritten:ad_ip_agent 5242441/165636
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:45:40,832 CommitLogSegment.java (line 205) turnOffIfNotWritten:multi_click_filter 5241748/165636
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:45:41,670 CommitLogSegment.java (line 205) turnOffIfNotWritten:ad_ip_agent 5242691/1387810
 WARN [COMMIT-LOG-WRITER] 2011-10-03 19:45:41,863 CommitLogSegment.java (line 205) turnOffIfNotWritten:multi_click_filter 5242353/1409533


;;;","03/Oct/11 21:52;yangyangyyy;ok, this looks to be what could lead to the symptoms:


let's say the commitlog position is 0 now.

Table.apply() :

  switchlock.readlock.lock();
          commitlog.instance.add(mutation) ====> executor.add(new LogRecordAdder())
                             // but the Adder is not really executed yet, just submitted.
         
         /// add to memtable
  switchlock.readlock.unlock();



then we have a flush.
  CFS.maybeSwitchMemtable()
 
  switchlock.writelock.lock();
    ctx = Commitlog.instance.getContext();   // returns 0 

  

/// now the logRecordAdder is executed , and advances the position to 199;
    
/// blahblah
   postflusher.executes(
      discardCompletedSegments
                 
             turnOffIfNotWritten() ====> check fails, so the CF written by the last Adder
                                         is not cleaned
  )
  unlock


as  a result, IF the CF is never written again, it will forever remain dirty in the segment.



then we need to maintain the order between the adder and the getContext() call in maybeSwitchMemtable()


;;;","03/Oct/11 22:38;yangyangyyy;per  https://issues.apache.org/jira/browse/CASSANDRA-3269?focusedCommentId=13119382&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13119382


;;;","04/Oct/11 07:37;slebresne;{noformat}
switchlock.writelock.lock();
ctx = Commitlog.instance.getContext(); // returns 0

/// now the logRecordAdder is executed , and advances the position to 199;
{noformat}

That could (should) not happen. The commit log is mono-threaded by mean of its executor. And every action on the commit log happens in a task on that executor. In particular Commitlog.instance.getContext() push a task on the commit log executor. This means that a logRecordAdder that has been pushed before the switchlock is grabbed for the flush cannot return a position that is after the position return by getContext() at the beginning of the flush (i.e, it either return a greater position in the same segment or a position in a newer segment). So thanks to the switchlock (that stops writes momentarily), we know that that the ctx position for the flush is after every write that has been done in the memtable we are flushing.
Now we call discardCompletedSegments later, so what can happen is that there has been writes to the commit log between the time we had grabbed that flush position and the time discardCompletedSegments is called. That is the goal of CL.lastWrite. If when we discard segments, there has been no write on this segment after the flush position we are considering, then the segment can be turnOff, otherwise there is still ""active"" write for the column family so we don't turn it off. But if that happens (i.e, if lastWrite >= flushPosition), it means that the writes have been done in a newer memtable than the one we just flushed. So the segment will be turnOff when the flush for that newer memtable happens.

{quote}
you see that LocationInfo being flushed already, twice, once at position 0, once at 174, then right after that, it tries to discard the segment, but complains that it's dirty, and the only dirty bit is locationInfo itself. that is the part that doesn't sound right.
{quote}

That log is not fully conclusive, because it is entirely possible that there has been some write to that commit log after the second flush position. In which case it's ok to not unmark LocationInfo.

In particular, the last line of the log:
{noformat}
DEBUG [COMMIT-LOG-WRITER] 2011-10-02 04:22:59,573 CommitLog.java (line 502) Not safe to delete commit log CommitLogSegment(/mnt/cass/lib/cassandra/commitlog/CommitLog-1317524063739.log); dirty is LocationInfo (0), ; hasNext: false
{noformat}
shows that the segment is the active one (hasNext == false), so it's perfectly reasonable to think there has been some write since position 174. The log is missing the value of lastWrite in the message to be able to say if it's the case.;;;","04/Oct/11 16:12;yangyangyyy;bq. That could (should) not happen. The commit log is mono-threaded by mean of its executor.



this is no longer the case since it was changed (particularly getContext() ) in https://issues.apache.org/jira/browse/CASSANDRA-3253 due to possibility of deadlock;;;","04/Oct/11 16:18;slebresne;Hum, #3253 moved a flush to an unrelated executor, so that a task on the commit log executor don't wait for another task on the commit log executor *because* that commit log executor is mono-threaded and so this would deadlock. But the commit log executor is still mono-thread (all hell would break loose if it wasn't) and CL.getContext() does push a task on that executor.;;;","04/Oct/11 16:42;slebresne;{quote}
there is hard evidence in that link that cfWrite has appeared to be > flushPosition.
this would indeed lead to the turnOffIfNotWritten() not turning off the dirty bit, this point is right, no???
{quote}

Yes, it can and will happen that cfWrite > flushPosition. And then the dirty bit will not be turning off. That is right. However, if that happen, it means that there is a write for the column family in a memtable that is not flushed yet. And thus the flush of this memtable would clean the dirty bit off. The invariant that the code enforces  (unless there is a bug of course) is that 'for the flush of a given memtable, every write in that memtable will have been written to the commit log at a position that is < the flushPosition for *that* flush'. If follows that if cfWrite > flushPosition, there a write in a unflushed memtable.;;;","04/Oct/11 16:58;yangyangyyy;thanks Sylvain.

now I see that cfWrite> flushPosition is  fine .
after I updated to the git HEAD  , the run in the last day has not produced excessive commitlogs. let me keep running and see if it's gone now


Yang;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot read counter value from jdbc cql,CASSANDRA-3268,12525073,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,coreyhulen,coreyhulen,coreyhulen,28/Sep/11 16:02,16/Apr/19 09:32,14/Jul/23 05:52,29/Sep/11 01:02,1.0.0,,,Legacy/CQL,,,0,cql,,,"it appears on line #36 in src/java/org/apache/cassandra/cql/jdbc/TypesMap.java  (notice it's in the portion of code that sits in the main src dir not the drivers)

map.put(""org.apache.cassandra.db.marshal.ColumnCounterType"", JdbcCounterColumn.instance);

should be 

map.put(""org.apache.cassandra.db.marshal.CounterColumnType"", JdbcCounterColumn.instance);

Notice CounterColumnType is reversed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,coreyhulen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,36757,,,Thu Sep 29 01:02:35 UTC 2011,,,,,,,,,,"0|i0ghun:",94322,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"29/Sep/11 01:02;jbellis;fixed in r1177137, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
missed CQL term rename,CASSANDRA-3266,12524968,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,27/Sep/11 21:02,16/Apr/19 09:32,14/Jul/23 05:52,27/Sep/11 22:10,1.0.0,,,Legacy/CQL,,,0,cql,,,The CQL grammar was missed in the rename of {{bytea}} to {{blob}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/11 21:03;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3266-update-for-missed-term-rename-s-bytea-b.txt;https://issues.apache.org/jira/secure/attachment/12496799/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3266-update-for-missed-term-rename-s-bytea-b.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,33632,,,Tue Sep 27 22:10:47 UTC 2011,,,,,,,,,,"0|i0ghtr:",94318,,,,,Normal,,,,,,,,,,,,,,,,,"27/Sep/11 22:00;jbellis;+1;;;","27/Sep/11 22:10;urandom;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Whitespace in SimpleSeedProvider string makes seed ignored,CASSANDRA-3263,12524864,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,marcuse,marcuse,27/Sep/11 13:49,16/Apr/19 09:32,14/Jul/23 05:52,03/Oct/11 19:35,1.0.0,,,,,,0,,,,"If a seeds given to SimpleSeedProvider contains whitespace, the seed will be ignored

for example ""1.2.3.4, 5.6.7.8"" will only make 5.6.7.8 a seed.

patch simply trim()s the host.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/11 13:50;marcuse;3263.txt;https://issues.apache.org/jira/secure/attachment/12496684/3263.txt",,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19204,,,Mon Oct 03 19:35:59 UTC 2011,,,,,,,,,,"0|i0ghsf:",94312,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Sep/11 13:50;marcuse;trim() host;;;","27/Sep/11 16:25;marcuse;of course i meant that only 1.2.3.4 will became a seed in the example in the description..;;;","03/Oct/11 19:35;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleSnitch.compareEndpoints doesn't respect the intent of the snitch,CASSANDRA-3262,12524827,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,27/Sep/11 09:48,16/Apr/19 09:32,14/Jul/23 05:52,28/Sep/11 04:17,0.8.7,1.0.0,,,,,0,,,,"SimpleSnitch is supposed to not sort the input addresses, thus respecting the order of the partitioner. However, it's compareEndpoints instead uses IP addresses comparison. Note that this matter when the dynamicSnitch fall back to the wrapped snitch since it uses the compareEndpoint method then.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/11 10:10;slebresne;3262.patch;https://issues.apache.org/jira/secure/attachment/12496657/3262.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,17916,,,Wed Sep 28 05:15:25 UTC 2011,,,,,,,,,,"0|i0ghrz:",94310,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Sep/11 10:10;slebresne;Note that I refactor a bit the code because it felt weird to have compareEndpoint being concrete in AbstractEndpointSnitch but sortByProximity being abstract. It seems more natural to have compareEndpoint being the abstract method and sortByProximity being defined from it in AES.;;;","27/Sep/11 12:55;jbellis;The reason for this behavior (CASSANDRA-1314) is because for best performance, normal reads want exactly the opposite behavior of counters: we want to direct reads to the same replica so that the cache stays hot.  Put another way, if each replica is serving a distinct range then you get (replica count) times as much cache memory [with CL.ONE and RR off) than if each is getting the full range of requests from different coordinators.;;;","27/Sep/11 13:56;slebresne;I think there is a misunderstanding. I agree with the pinning of replicas. The problem is that the current implementation of AbstractEndpointSnitch.compareEndpoints is:
{noformat}
public int compareEndpoints(InetAddress target, InetAddress a1, InetAddress a2)
{
    return a1.getHostAddress().compareTo(a2.getHostAddress());
}
{noformat}
If you sort a list of hosts using that, you will always return the host that have the ""smallest"" IP. In other words, in a 3 nodes cluster with RF=3, all and every read will hit the exact same node.

What makes it kind of work today is that this compareEndpoints() method is barely used. It's used only in the case where the dynamic snitch have no scores for the endpoints. Otherwise, it's sortByProximity that is used (which doesn't rely on compareEndpoints -- this is confusing and my patch corrects it). And sortByProximity does *the right thing*, i.e, it doesn't sort the input list since it is supposed to be in token order (which effectively pin one range to every replica).

So the patch here proposes two things:
  * If fixes the compareEndpoints method: comparing IP addresses is not a good idea.
  * It refactors the code to make sortByProximity use compareEndpoint, to having getting in that situation again.;;;","28/Sep/11 04:17;jbellis;lgtm.  made some minor updates to comments and committed.;;;","28/Sep/11 05:15;hudson;Integrated in Cassandra-0.8 #346 (See [https://builds.apache.org/job/Cassandra-0.8/346/])
    Keep SimpleSnitch proximity ordering unchanged from what the Strategy generates, as intended
patch by slebresne; reviewed by jbellis for CASSANDRA-3262

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1176712
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/AbstractEndpointSnitch.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/AbstractNetworkTopologySnitch.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/DynamicEndpointSnitch.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/SimpleSnitch.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MergeIterator assertion on sources != empty can be thrown,CASSANDRA-3260,12524658,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,26/Sep/11 16:11,16/Apr/19 09:32,14/Jul/23 05:52,27/Sep/11 13:16,1.0.0,,,,,,0,,,,"MergeIterator.get assert that it don't get an empty list of sources. This seems to at least not be the case in the unit test for some of tests (this don't make any test fail however, but there is a few stack trace thrown). I think it's pretty unnatural to ""fail"" on an empty list of sources and would force every caller to first take the empty case into account, so I propose to just remove that assertion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/11 16:12;slebresne;3260.patch;https://issues.apache.org/jira/secure/attachment/12496494/3260.patch","27/Sep/11 07:34;slebresne;3260_v2.patch;https://issues.apache.org/jira/secure/attachment/12496641/3260_v2.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,3230,,,Tue Sep 27 13:16:31 UTC 2011,,,,,,,,,,"0|i0ghr3:",94306,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"26/Sep/11 16:50;jbellis;Should also then change the next line from

        if (sources.size() == 1)

to

        if (sources.size() <= 1)

+1 w/ that;;;","27/Sep/11 07:34;slebresne;Hum, I don't think that works because both OneToOne and TrivialOneToOne constructors do sources.get(0), so that should stay a == 1.

The initial idea was to use the manyToOne that works fine will an empty list of sources, but attaching an alternative v2 that adds a simple EmptyIterator for the empty case.;;;","27/Sep/11 12:49;jbellis;+1;;;","27/Sep/11 12:50;jbellis;TBH worrying about the empty case is probably premature optimization, so I'm fine w/ committing either one.;;;","27/Sep/11 13:16;slebresne;I agree this is premature optimization, I attached the second one as an alternative to consider. Committed the first one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace token leaves the old node state in tact causing problems in cli,CASSANDRA-3259,12524654,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,26/Sep/11 15:51,16/Apr/19 09:32,14/Jul/23 05:52,30/Sep/11 21:48,0.8.7,1.0.0,,,,,0,,,,"in the replace token patch we dont evict the node from the Gossip which will leave the node lingering around and causes issues in cli (UNReachable nodes)

As a part of the replace token if the token is replaced with another token we should remove the old nodes Gossip states.",JVM on CentOS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/11 21:23;vijay2win@yahoo.com;0001-evict-replaced-node-immediately-v2.patch;https://issues.apache.org/jira/secure/attachment/12497221/0001-evict-replaced-node-immediately-v2.patch","30/Sep/11 05:39;vijay2win@yahoo.com;0001-evict-replaced-node-immediately.patch;https://issues.apache.org/jira/secure/attachment/12497115/0001-evict-replaced-node-immediately.patch","01/Oct/11 00:54;vijay2win@yahoo.com;0002-evict-during-replace-token-for-1.0.patch;https://issues.apache.org/jira/secure/attachment/12497254/0002-evict-during-replace-token-for-1.0.patch",,,,,,,,,,,,3.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,1832,,,Tue Oct 04 21:02:35 UTC 2011,,,,,,,,,,"0|i0ghqn:",94304,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"30/Sep/11 05:39;vijay2win@yahoo.com;This patch fixes this issue. We can immediately delete the Application states of the token which we just replaced so that we dont gossip about it.;;;","30/Sep/11 19:35;brandon.williams;If this is causing problems in the cli, the cli is doing something wrong, because removeEndpoint already takes it out of the Gossiper's unreachable nodes.  What problem is this solving?

We do need to expire remove this endpoint but I don't think exposing evictFromMembership is the best way.;;;","30/Sep/11 19:40;vijay2win@yahoo.com;The problem is that, when we do the replace token we dont change the status of the old node (to removed ip or something)... hence the Node will get the State back as it is still gossiping about the replaced node. Even if removeToken() the states are not removed and hence if someone is talking about this node after (Async evict by doStatus()) this node will be added back.... hence the node will never go out of the Application State. This patch will instead will evict immediately when it sees a irrelevant host. This is simpler solution than adding another status like ""remove ip"". This works as expected in my tests.;;;","30/Sep/11 21:23;vijay2win@yahoo.com;v2 adds abstraction to evict based on brandon's comments in IRC.;;;","30/Sep/11 21:48;brandon.williams;Committed, thanks;;;","30/Sep/11 22:14;hudson;Integrated in Cassandra-0.8 #352 (See [https://builds.apache.org/job/Cassandra-0.8/352/])
    Evict gossip state immediately when a token is taken over.
Patch by vijay, reviewed by brandonwilliams for CASSANDRA-3259

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177847
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
;;;","01/Oct/11 00:52;vijay2win@yahoo.com;Hi Brandon, Really sorry about extra work... i missed one change for 1.0 i was separating these 2 patches and some how i missed it.... ;;;","03/Oct/11 08:59;slebresne;This hasn't been merged up to 1.0 yet so it should probably be (with the last changes from Vijay?), and it could probably use an entry in the changelog when doing so.;;;","04/Oct/11 21:02;brandon.williams;Done and done.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
*.bat files fails when CASSANDRA_HOME contains a white space.,CASSANDRA-3258,12524632,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,talmdal,talmdal,talmdal,26/Sep/11 13:46,16/Apr/19 09:32,14/Jul/23 05:52,29/Sep/11 01:44,0.8.7,,,Packaging,,,0,,,,"Issues 2952 and 2237 fixed the issue for cassandra.bat. But the following bat files need the same fix:
json2sstable.bat
nodetool.bat
sstable2json.bat
sstablekeys.bat
",Windows 7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,talmdal,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,1833,,,Thu Sep 29 02:22:10 UTC 2011,,,,,,,,,,"0|i0ghq7:",94302,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"26/Sep/11 13:47;talmdal;This also applies to 1.0.0;;;","26/Sep/11 23:07;jbellis;Can you submit a patch?;;;","27/Sep/11 02:52;talmdal;Sure, I'll try getting that done tomorrow;;;","28/Sep/11 14:20;talmdal;Pull request: https://github.com/apache/cassandra/pull/4
Change: https://github.com/talmdal/cassandra/commit/2ffdc581aa758c8bdb2a58d7e43f1e07feca7141;;;","28/Sep/11 15:30;jbellis;please post a patch here, the ASF does not accept patches via github yet;;;","28/Sep/11 15:47;talmdal;diff --git a/bin/json2sstable.bat b/bin/json2sstable.bat
index 158210c..ceb7987 100644
--- a/bin/json2sstable.bat
+++ b/bin/json2sstable.bat
@@ -18,7 +18,7 @@
 if ""%OS%"" == ""Windows_NT"" setlocal
 
 if NOT DEFINED CASSANDRA_HOME set CASSANDRA_HOME=%~dp0..
-if NOT DEFINED CASSANDRA_CONF set CASSANDRA_CONF=%CASSANDRA_HOME%\conf
+if NOT DEFINED CASSANDRA_CONF set CASSANDRA_CONF=""%CASSANDRA_HOME%\conf""
 if NOT DEFINED CASSANDRA_MAIN set CASSANDRA_MAIN=org.apache.cassandra.tools.SSTableImport
 if NOT DEFINED JAVA_HOME goto err
 
@@ -29,10 +29,10 @@ set JAVA_OPTS=^
 REM ***** CLASSPATH library setting *****
 
 REM Ensure that any user defined CLASSPATH variables are not used on startup
-set CLASSPATH=%CASSANDRA_HOME%\conf
+set CLASSPATH=""%CASSANDRA_HOME%\conf""
 
 REM For each jar in the CASSANDRA_HOME lib directory call append to build the CLASSPATH variable.
-for %%i in (%CASSANDRA_HOME%\lib\*.jar) do call :append %%~fi
+for %%i in (""%CASSANDRA_HOME%\lib\*.jar"") do call :append %%~fi
 goto okClasspath
 
 :append
@@ -41,7 +41,7 @@ goto :eof
 
 :okClasspath
 REM Include the build\classes\main directory so it works in development
-set CASSANDRA_CLASSPATH=%CLASSPATH%;%CASSANDRA_HOME%\build\classes\main;%CASSANDRA_CONF%;%CASSANDRA_HOME%\build\classes\thrift
+set CASSANDRA_CLASSPATH=%CLASSPATH%;""%CASSANDRA_HOME%\build\classes\main"";%CASSANDRA_CONF%;""%CASSANDRA_HOME%\build\classes\thrift""
 
 set CASSANDRA_PARAMS=
 set TOOLS_PARAMS=
diff --git a/bin/nodetool.bat b/bin/nodetool.bat
index ea86317..a78c1c9 100644
--- a/bin/nodetool.bat
+++ b/bin/nodetool.bat
@@ -21,7 +21,7 @@ if NOT DEFINED CASSANDRA_HOME set CASSANDRA_HOME=%~dp0..
 if NOT DEFINED JAVA_HOME goto err
 
 REM Ensure that any user defined CLASSPATH variables are not used on startup
-set CLASSPATH=%CASSANDRA_HOME%\conf
+set CLASSPATH=""%CASSANDRA_HOME%\conf""
 
 REM For each jar in the CASSANDRA_HOME lib directory call append to build the CLASSPATH variable.
 rem for %%i in (%CASSANDRA_HOME%\lib*.jar) do call :append %%~fi
@@ -34,7 +34,7 @@ goto :eof
 
 :okClasspath
 REM Include the build\classes\main directory so it works in development
-set CASSANDRA_CLASSPATH=%CLASSPATH%;%CASSANDRA_HOME%\build\classes\main;%CASSANDRA_HOME%\build\classes\thrift
+set CASSANDRA_CLASSPATH=%CLASSPATH%;""%CASSANDRA_HOME%\build\classes\main"";""%CASSANDRA_HOME%\build\classes\thrift""
 goto runNodeTool
 
 :runNodeTool
diff --git a/bin/sstable2json.bat b/bin/sstable2json.bat
index 6ca91e4..cc4cbce 100644
--- a/bin/sstable2json.bat
+++ b/bin/sstable2json.bat
@@ -18,7 +18,7 @@
 if ""%OS%"" == ""Windows_NT"" setlocal
 
 if NOT DEFINED CASSANDRA_HOME set CASSANDRA_HOME=%~dp0..
-if NOT DEFINED CASSANDRA_CONF set CASSANDRA_CONF=%CASSANDRA_HOME%\conf
+if NOT DEFINED CASSANDRA_CONF set CASSANDRA_CONF=""%CASSANDRA_HOME%\conf""
 if NOT DEFINED CASSANDRA_MAIN set CASSANDRA_MAIN=org.apache.cassandra.tools.SSTableExport
 if NOT DEFINED JAVA_HOME goto err
 
@@ -29,10 +29,10 @@ set JAVA_OPTS=^
 REM ***** CLASSPATH library setting *****
 
 REM Ensure that any user defined CLASSPATH variables are not used on startup
-set CLASSPATH=%CASSANDRA_HOME%\conf
+set CLASSPATH=""%CASSANDRA_HOME%\conf""
 
 REM For each jar in the CASSANDRA_HOME lib directory call append to build the CLASSPATH variable.
-for %%i in (%CASSANDRA_HOME%\lib\*.jar) do call :append %%~fi
+for %%i in (""%CASSANDRA_HOME%\lib\*.jar"") do call :append %%~fi
 goto okClasspath
 
 :append
@@ -41,7 +41,7 @@ goto :eof
 
 :okClasspath
 REM Include the build\classes\main directory so it works in development
-set CASSANDRA_CLASSPATH=%CLASSPATH%;%CASSANDRA_HOME%\build\classes\main;%CASSANDRA_CONF%;%CASSANDRA_HOME%\build\classes\thrift
+set CASSANDRA_CLASSPATH=%CLASSPATH%;""%CASSANDRA_HOME%\build\classes\main"";%CASSANDRA_CONF%;""%CASSANDRA_HOME%\build\classes\thrift""
 
 set CASSANDRA_PARAMS=
 set TOOLS_PARAMS=
diff --git a/bin/sstablekeys.bat b/bin/sstablekeys.bat
index a0d7641..7d6446a 100644
--- a/bin/sstablekeys.bat
+++ b/bin/sstablekeys.bat
@@ -18,7 +18,7 @@
 if ""%OS%"" == ""Windows_NT"" setlocal
 
 if NOT DEFINED CASSANDRA_HOME set CASSANDRA_HOME=%~dp0..
-if NOT DEFINED CASSANDRA_CONF set CASSANDRA_CONF=%CASSANDRA_HOME%\conf
+if NOT DEFINED CASSANDRA_CONF set CASSANDRA_CONF=""%CASSANDRA_HOME%\conf""
 if NOT DEFINED CASSANDRA_MAIN set CASSANDRA_MAIN=org.apache.cassandra.tools.SSTableExport
 if NOT DEFINED JAVA_HOME goto err
 
@@ -29,10 +29,10 @@ set JAVA_OPTS=^
 REM ***** CLASSPATH library setting *****
 
 REM Ensure that any user defined CLASSPATH variables are not used on startup
-set CLASSPATH=%CASSANDRA_HOME%\conf
+set CLASSPATH=""%CASSANDRA_HOME%\conf""
 
 REM For each jar in the CASSANDRA_HOME lib directory call append to build the CLASSPATH variable.
-for %%i in (%CASSANDRA_HOME%\lib\*.jar) do call :append %%~fi
+for %%i in (""%CASSANDRA_HOME%\lib\*.jar"") do call :append %%~fi
 goto okClasspath
 
 :append
@@ -41,7 +41,7 @@ goto :eof
 
 :okClasspath
 REM Include the build\classes\main directory so it works in development
-set CASSANDRA_CLASSPATH=%CLASSPATH%;%CASSANDRA_HOME%\build\classes\main;%CASSANDRA_CONF%;%CASSANDRA_HOME%\build\classes\thrift
+set CASSANDRA_CLASSPATH=%CLASSPATH%;""%CASSANDRA_HOME%\build\classes\main"";%CASSANDRA_CONF%;""%CASSANDRA_HOME%\build\classes\thrift""
 
 set CASSANDRA_PARAMS=
 set TOOLS_PARAMS=
;;;","28/Sep/11 15:49;talmdal;Hmm, hope this is the right way to submit the patch.

Given the back and forth, it would have just been easier for you to add the quotes :-/;;;","29/Sep/11 01:44;jbellis;Committed, thanks!

(For future reference, attaching a file with the patch is easiest to work with.);;;","29/Sep/11 02:22;hudson;Integrated in Cassandra-0.8 #349 (See [https://builds.apache.org/job/Cassandra-0.8/349/])
    Fix tool .bat files when CASSANDRA_HOME contains spaces
patch by Tim Almdal; reviewed by jbellis for CASSANDRA-3258

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177149
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/bin/json2sstable.bat
* /cassandra/branches/cassandra-0.8/bin/nodetool.bat
* /cassandra/branches/cassandra-0.8/bin/sstable2json.bat
* /cassandra/branches/cassandra-0.8/bin/sstablekeys.bat
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enabling SSL on a fairly light cluster leaks Open files.,CASSANDRA-3257,12524571,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,25/Sep/11 17:25,16/Apr/19 09:32,14/Jul/23 05:52,26/Sep/11 17:14,0.8.7,1.0.0,,,,,0,,,,"To reproduce:

Enable SSL encryption and let the server be idle for a day or so you will see the below....

[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ /usr/sbin/lsof |grep -i cassandra-app.jks |wc -l ;date
16333
Sun Sep 25 17:23:29 UTC 2011
[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ java -jar cmdline-jmxclient-0.10.3.jar - localhost:7501 java.lang:type=Memory gc
[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ /usr/sbin/lsof |grep -i cassandra-app.jks |wc -l ;date
64
Sun Sep 25 17:23:53 UTC 2011
[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ 

After running GC manually the issue goes away.",JVM on CentOS,satishbabu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/11 16:54;vijay2win@yahoo.com;0001-ssl-open-file-issue.patch;https://issues.apache.org/jira/secure/attachment/12496503/0001-ssl-open-file-issue.patch",,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,3220,,,Tue Sep 27 06:26:04 UTC 2011,,,,,,,,,,"0|i0ghpr:",94300,,,,,Low,,,,,,,,,,,,,,,,,"26/Sep/11 16:08;vijay2win@yahoo.com;This issue elevates when a node is down.;;;","26/Sep/11 16:16;vijay2win@yahoo.com;Seems like the issue may be because we are opening the FIS and not closing it.... in SSLFactory.createSSLContext, testing it... patch soon.;;;","26/Sep/11 16:54;vijay2win@yahoo.com;Closing the FIS fixes the issue (in SSLFactory).;;;","26/Sep/11 17:14;brandon.williams;Committed, thanks!;;;","26/Sep/11 23:08;jbellis;should use FileUtils.closeQuietly so if one close throws, the other still happens;;;","27/Sep/11 05:33;jbellis;done in r1176204.  also cleaned up formatting.;;;","27/Sep/11 06:26;vijay2win@yahoo.com;Thanks Jonathan!, was just about to attach it...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError when repairing a node,CASSANDRA-3256,12524504,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,alienth,alienth,24/Sep/11 06:10,16/Apr/19 09:32,14/Jul/23 05:52,26/Sep/11 14:32,0.8.7,,,,,,0,repair,,,"When repairing a node, the following exception was thrown two times:

{code}
ERROR [AntiEntropyStage:2] 2011-09-23 23:00:24,016 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[AntiEntropyStage:2,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.AntiEntropyService.rendezvous(AntiEntropyService.java:170)
        at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:90)
        at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.doVerb(AntiEntropyService.java:518)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

No other errors occurred on the node. From peeking at the code, this assertion appears to simply check if an existing repair session could be found. Interestingly, the repair did continue to run after this as evidenced by several other AntiEntropyService entires in the log.

8 node ring with an RF of 3, if that matters at all. No other nodes in the ring threw exceptions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/11 12:53;slebresne;3256.patch;https://issues.apache.org/jira/secure/attachment/12496471/3256.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,3269,,,Mon Sep 26 15:17:40 UTC 2011,,,,,,,,,,"0|i0ghpb:",94298,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"24/Sep/11 06:13;alienth;I should note, this same node has been repaired several times in the past with no issues.;;;","25/Sep/11 02:18;alienth;The repair which triggered this assertion error did eventually return with no further errors.;;;","26/Sep/11 12:53;slebresne;My guess is that this is probably harmless. Basically the node received a merkle tree for a session that it doesn't know about. This means that the said repair session has been interrupted. _A priori_, I see only two things that can cause this:
  * The repair thread on the host has been interrupted. But 1) in that case you should have found a exception earlier on saying ""Interrupted while waiting for repair: repair will continue in the background."" and 2) I don't see what could interrupt that thread.
  * The node restarted and only now receives a response for request made before the restart. This is imho the more likely scenario. And if so, the previous repair (the one started before the restart) won't succeed, but there is no more consequence than that.

That being throwing an assertion error is probably such a great idea given that it's a scenario that can happen. Attaching a patch that simply log an hopefully more explicit message.;;;","26/Sep/11 13:09;jbellis;+1;;;","26/Sep/11 14:32;slebresne;Committed. Thanks.;;;","26/Sep/11 15:17;hudson;Integrated in Cassandra-0.8 #341 (See [https://builds.apache.org/job/Cassandra-0.8/341/])
    Log a miningfull warning when a node receive a message for a repair session that don't exist anymore
patch by slebresne; reviewed by jbellis for CASSANDRA-3256

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1175880
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AntiEntropyService.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sstable scrub status persists in compactionstats after scrub is complete,CASSANDRA-3255,12524501,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,alienth,alienth,24/Sep/11 04:37,16/Apr/19 09:32,14/Jul/23 05:52,28/Sep/11 15:52,0.8.7,,,,,,0,compaction,,,"When scrubbing the sstables on a node, the 'Scrub' info persists in the 'compactionstats' nodetool utility, even after the scrub is complete.",,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/11 12:00;xedin;CASSANDRA-3255.patch;https://issues.apache.org/jira/secure/attachment/12496872/CASSANDRA-3255.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,1834,,,Wed Sep 28 16:16:32 UTC 2011,,,,,,,,,,"0|i0ghov:",94296,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"25/Sep/11 02:06;jeromatron;Is this the same as CASSANDRA-3236?;;;","25/Sep/11 02:16;alienth;Yep, didn't see that. I'm fine with closing this as a duplicate.;;;","25/Sep/11 04:54;alienth;Actually, the other issue claims that the scrub is hanging. The issue I've listed here is that the information just persists even after completion.

This is very easy to test. Just create a CF, add some data, flush, and then run scrub a few times in a row. The scrubs will all complete, but all of the info from the scrubs will stick around in 'compactionstats'.;;;","28/Sep/11 15:26;jbellis;+1 on Pavel's patch;;;","28/Sep/11 15:52;xedin;Committed.;;;","28/Sep/11 16:16;hudson;Integrated in Cassandra-0.8 #347 (See [https://builds.apache.org/job/Cassandra-0.8/347/])
    Fix Scrub compaction finishing
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for (CASSANDRA-3255)

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1176926
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inherent deadlock situation in commitLog flush?,CASSANDRA-3253,12524448,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,yangyangyyy,yangyangyyy,23/Sep/11 17:53,16/Apr/19 09:32,14/Jul/23 05:52,26/Sep/11 09:07,1.0.0,,,,,,0,commitlog,,,"after my system ran for a while, it consitently goes into frozen state where all the mutations stage threads are waiting
on the switchlock,

the reason is that the switchlock is held by commit log, as shown by the following thread dump:



""COMMIT-LOG-WRITER"" prio=10 tid=0x00000000010df000 nid=0x32d3 waiting on condition [0x00007f2d81557000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f3579eec060> (a java.util.concurrent.FutureTask$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:248)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog.getContext(CommitLog.java:386)
        at org.apache.cassandra.db.ColumnFamilyStore.maybeSwitchMemtable(ColumnFamilyStore.java:650)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:722)
        at org.apache.cassandra.db.commitlog.CommitLog.createNewSegment(CommitLog.java:573)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:81)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:596)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Thread.java:679)


we can clearly see that the COMMIT-LOG-WRITER thread is running the regular appender , but the appender itself calls getContext(), which again submits a new Callable to be executed, and waits on the Callable. but the new Callable is never going to be executed since the executor has only *one* thread.


I believe this is a deterministic bug.



",,awinter,yangyangyyy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/11 01:24;jbellis;3253.txt;https://issues.apache.org/jira/secure/attachment/12496376/3253.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,3343,,,Mon Sep 26 09:07:35 UTC 2011,,,,,,,,,,"0|i0ghnz:",94292,,slebresne,,slebresne,Critical,,,,,,,,,,,,,,,,,"23/Sep/11 18:05;yangyangyyy;it seems to be added in the recent feature to flush earliest segment with dirty CFs : 


https://github.com/apache/cassandra/commit/f599559221ad074d9af0a99d7ffdd482c2b6b10c#diff-3

CFS.forceFlush() was added to the commit log writing path;;;","23/Sep/11 18:50;yangyangyyy;looks to be related to https://issues.apache.org/jira/browse/CASSANDRA-1991;;;","25/Sep/11 01:24;jbellis;excellent diagnosis of the problem, Yang.

patch attached to push the flush calls off of the CL executor.;;;","26/Sep/11 09:07;slebresne;+1
Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage uses comparator for both super column names and sub column names.,CASSANDRA-3251,12524432,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,danapsimer,danapsimer,23/Sep/11 16:18,16/Apr/19 09:32,14/Jul/23 05:52,31/Jan/12 21:42,0.8.10,1.0.8,,,,,0,cassandra,hadoop,pig,"The CassandraStorage class uses the same comparator for super and sub column names.

This is because it calls columnsToTuple recursively without any indication that the subsequent call is for sub columns.  Also, the getDefaultMarshallers method does not return the sub column name comparator.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/12 19:34;brandon.williams;3251-v2.txt;https://issues.apache.org/jira/secure/attachment/12510086/3251-v2.txt","31/Jan/12 20:25;xedin;CASSANDRA-3251-v3.patch;https://issues.apache.org/jira/secure/attachment/12512623/CASSANDRA-3251-v3.patch","23/Sep/11 16:31;danapsimer;CASSANDRA-3251.patch;https://issues.apache.org/jira/secure/attachment/12496280/CASSANDRA-3251.patch",,,,,,,,,,,,3.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,3525,,,Tue Jan 31 21:42:10 UTC 2012,,,,,,,,,,"0|i0ghn3:",94288,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"23/Sep/11 16:29;danapsimer;This is a small fix for the super column issues.;;;","19/Oct/11 18:22;brandon.williams;Dana, can you rebase?  I can't get this to apply.;;;","10/Jan/12 19:34;brandon.williams;v2 with a slightly different approach.;;;","31/Jan/12 19:42;xedin;isSub = 2 lead to key_validator, it should be changed to 3. Wouldn't it be a good idea to set 'AbstractType comparator' instead of 'boolean isSub' in columnToTuple(...)?;;;","31/Jan/12 20:25;xedin;v3 with implements proposed parameter change to AbstractType comparator and removes unnecessary 'ByteBuffer name' parameter.;;;","31/Jan/12 21:32;brandon.williams;+1;;;","31/Jan/12 21:42;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fsync the directory after new sstable or commit log segment are created,CASSANDRA-3250,12524426,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,hanzhu,hanzhu,23/Sep/11 14:39,16/Apr/19 09:32,14/Jul/23 05:52,19/Dec/11 23:50,1.1.0,,,,,,0,,,,"The mannual of fsync said:
bq.   Calling  fsync()  does  not  necessarily  ensure  that  the entry in the directory containing the file has also reached disk.  For that an explicit fsync() on a file descriptor for the directory is also needed.

At least on ext4, syncing the directory is a must to have step, as described by [1]. Otherwise, the new sstables or commit logs could be missed after crash even if itself is synced. 

Unfortunately, JVM does not provide an approach to sync the directory...

[1] http://www.linuxfoundation.org/news-media/blogs/browse/2009/03/don%E2%80%99t-fear-fsync
",,cburroughs,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HDFS-5042,,,,,,"17/Dec/11 14:36;xedin;CASSANDRA-3250-v2.patch;https://issues.apache.org/jira/secure/attachment/12507780/CASSANDRA-3250-v2.patch","18/Dec/11 16:27;xedin;CASSANDRA-3250-v3.patch;https://issues.apache.org/jira/secure/attachment/12507839/CASSANDRA-3250-v3.patch","12/Dec/11 11:38;xedin;CASSANDRA-3250.patch;https://issues.apache.org/jira/secure/attachment/12506992/CASSANDRA-3250.patch",,,,,,,,,,,,3.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,1835,,,Mon Dec 19 23:50:49 UTC 2011,,,,,,,,,,"0|i0ghmn:",94286,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"23/Sep/11 14:42;hanzhu;There is a relative discussion on PostgreSQL.

http://postgresql.1045698.n5.nabble.com/fsync-reliability-td4330289.html;;;","26/Sep/11 05:32;hanzhu;On XFS, this is not a problem per the developer's response[1].

[1] http://www.spinics.net/lists/xfs/msg07229.html;;;","15/Dec/11 22:43;jbellis;- shouldn't the directory cache be per-Writer, not static?
- why would the FD be null?  if it shouldn't be, let's use an assert there;;;","15/Dec/11 22:54;xedin;bq. shouldn't the directory cache be per-Writer, not static?

It's useful to make it static instead of instantiation per-Writer because we need every writer to be able to see already open directories.

bq. why would the FD be null? if it shouldn't be, let's use an assert there

Directory FD could be null when it's the first time we see directory e.g. first time for commitlog, different data directories, also I made it that way because of CASSANDRA-2749.;;;","17/Dec/11 05:19;jbellis;bq. we need every writer to be able to see already open directories

While it doesn't really matter with the current directory-per-keyspace, syncing every directory open globally when any sstable is written could be problematic with CASSANDRA-2749 (especially with leveled compaction, which does lots of small sstable writes).  What's the problem with just having writers sync directories for files they're writing?;;;","17/Dec/11 13:25;xedin;No problem with that, here is the version where SequentialWriter syncs only directory for file it writes.;;;","17/Dec/11 14:36;xedin;re-attached v2 which does only one directory sync per file (after first file sync).;;;","18/Dec/11 06:28;jbellis;v2 looks a lot simpler, I like it.

Is there a reason to limit this to Linux-only?  If open + fsync + close exist, it should work.  (And if they don't, we already have the link error to tell us that.);;;","18/Dec/11 12:56;xedin;The reason is O_DIRECTORY is a Linux-specific flag.;;;","18/Dec/11 16:00;jbellis;Is O_DIRECTORY necessary for the sync to work?  Or is that just a ""make sure this path is actually a directory"" flag?  If the latter, then we can leave it out, since we know the path is a directory by construction.;;;","18/Dec/11 16:27;xedin;Ok, v3 O_DIRECTORY changed to O_RDONLY and checks for linux are removed from CLibrary.

O_DIRECTORY just causes open to fail if given path is not a directory, so it seems we can live without it :);;;","19/Dec/11 04:54;jbellis;+1;;;","19/Dec/11 09:23;xedin;Committed.;;;","19/Dec/11 23:25;jbellis;... actually, can you back this out of 1.0 and put it in 1.1 instead?  It *should* be fine but let's not take chances with regressions.;;;","19/Dec/11 23:50;xedin;Changed to 1.1 only.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader ignores option doesn't work correctly,CASSANDRA-3247,12524396,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,23/Sep/11 09:07,16/Apr/19 09:32,14/Jul/23 05:52,23/Sep/11 12:35,0.8.7,1.0.0,,,,,0,bulkloader,,,The --ignores option is supposed to take an argument but it doesn't.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/11 09:09;slebresne;3247.patch;https://issues.apache.org/jira/secure/attachment/12496243/3247.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,3526,,,Fri Sep 23 13:21:27 UTC 2011,,,,,,,,,,"0|i0ghlj:",94281,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"23/Sep/11 12:20;jbellis;+1

(nit: ""Unknow"" should be ""Unknown"");;;","23/Sep/11 12:35;slebresne;Committed (with spelling fix), thanks;;;","23/Sep/11 13:21;hudson;Integrated in Cassandra-0.8 #338 (See [https://builds.apache.org/job/Cassandra-0.8/338/])
    fix sstableloader --ignores option
patch by slebresne; reviewed by jbellis for CASSANDRA-3247

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1174701
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/BulkLoader.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memtable_total_space_in_mb does not accept the value 0 in Cassandra 1.0,CASSANDRA-3246,12524380,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,thobbs,thobbs,23/Sep/11 05:48,16/Apr/19 09:32,14/Jul/23 05:52,23/Sep/11 07:28,1.0.0,,,,,,0,,,,"This affects 1.0 beta1.

From the key explanation in cassandra.yaml it looks like it should accept the value ""0""

# Total memory to use for memtables. Cassandra will flush the largest
# memtable when this much memory is used.
# If omitted, Cassandra will set it to 1/3 of the heap.
# If set to 0, only the old flush thresholds are used.
memtable_total_space_in_mb: 0

However in the code I could see the following:

if (conf.memtable_total_space_in_mb <= 0)
throw new ConfigurationException(""memtable_total_space_in_mb must be positive"");
logger.info(""Global memtable threshold is enabled at {}MB"", conf.memtable_total_space_in_mb);",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,3527,,,Fri Sep 23 07:28:05 UTC 2011,,,,,,,,,,"0|i0ghl3:",94279,,,,,Low,,,,,,,,,,,,,,,,,"23/Sep/11 07:28;jbellis;The old thresholds were made no-ops in CASSANDRA-2449.  I removed the line ""If set to 0, only the old flush thresholds are used"" just now in r1174563.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Don't fail when numactl is installed, but NUMA policies are not supported",CASSANDRA-3245,12524371,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,scode,thepaul,thepaul,23/Sep/11 02:56,16/Apr/19 09:32,14/Jul/23 05:52,26/Sep/11 20:47,1.0.0,,,Packaging,,,0,,,,"When numactl is installed but NUMA policies are not supported, trying to run cassandra gives only:

{noformat}
numactl: This system does not support NUMA policy
{noformat}

..and the startup script fails there.

We should probably fail a little more gracefully. Possibly the best way to tell if numactl will work is by using:

{noformat}
numactl --hardware
{noformat}

but I don't have ready access to a machine with proper NUMA support at the moment so I can't check how easy it is to tell the difference in the output.

It looks just as reliable (if possibly a bit more brittle) to check for the existence of the directory {{/sys/devices/system/node}}. If that directory doesn't exist, we shouldn't even try to use or run numactl.","Any Linux system where a 'numactl' executable is available, but no NUMA policies are actually supported. EC2 nodes are easy examples of environments with no NUMA policy support.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/11 19:39;scode;3245.txt;https://issues.apache.org/jira/secure/attachment/12496408/3245.txt",,,,,,,,,,,,,,1.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,776,,,Mon Sep 26 20:47:27 UTC 2011,,,,,,,,,,"0|i0ghkn:",94277,,thepaul,,thepaul,Low,,,,,,,,,,,,,,,,,"23/Sep/11 08:52;scode;Or we could do something seemingly ugly but probably very effective and safe: Try running ""ls -d / > /dev/null"" with numactl and if that fails, assume it is because numactl isn't working. That should hopefully work in pretty much any environment.

I'll submit a patch soonish.
;;;","25/Sep/11 19:39;scode;Attaching trivial patch that does the ls test.

@paul Can you test whether numactl exits with a proper non-0 exit status on the system where it is not supported? I've tested the script as such, but I don't have a system available without the necessary support to test on.
;;;","26/Sep/11 15:46;thepaul;bq. @paul Can you test whether numactl exits with a proper non-0 exit status on the system where it is not supported?

Indeed it does; using --interleave=all when there is no NUMA policy support causes an exit value of 1.

+1;;;","26/Sep/11 20:47;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node which was decommissioned and shut-down reappears on a single node,CASSANDRA-3243,12524242,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,alienth,alienth,22/Sep/11 07:19,16/Apr/19 09:32,14/Jul/23 05:52,30/Sep/11 20:15,0.8.7,,,,,,0,,,,"I decommissioned a node several days ago. It was no longer in the ring list on any node in the ring. However, it was in the dead gossip list.

In an attempt to clean it out of the dead gossip list so I could truncate, I shut down the entire ring and bought it back up. Once the ring came back up, one node showed the decommissioned node as still in the ring in a state of 'Down'. No other node in the ring shows this info.

I successfully ran removetoken on the node to get that phantom node out. However, it is back in the dead gossip list, preventing me from truncating.

Where might the info on this decommissioned node be being stored? Is HH possibly trying to deliver to the removed node, thus putting it back in the ring on one node?

I find it extremely curious that none of the other nodes in the ring showed the phantom node. Shouldn't gossip have propagated the node everywhere, even if it was down?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/11 23:16;brandon.williams;3243.txt;https://issues.apache.org/jira/secure/attachment/12496337/3243.txt","23/Sep/11 06:00;alienth;locationinfo_0919.tgz;https://issues.apache.org/jira/secure/attachment/12496228/locationinfo_0919.tgz","23/Sep/11 06:00;alienth;locationinfo_0922.tgz;https://issues.apache.org/jira/secure/attachment/12496229/locationinfo_0922.tgz",,,,,,,,,,,,3.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,3528,,,Fri Sep 30 21:49:04 UTC 2011,,,,,,,,,,"0|i0ghjr:",94273,,bcoverston,,bcoverston,Low,,,,,,,,,,,,,,,,,"22/Sep/11 07:27;alienth;Looking through the logs, the node which saw the decommissioned node didn't print anything about discovering it via gossip. The very first log line I have regarding the phantom node is when I forced a removetoken.;;;","22/Sep/11 10:32;brandon.williams;Can you explain what you mean by ""dead gossip list"" and how this prevents truncate?

bq. Where might the info on this decommissioned node be being stored?

After CASSANDRA-2496, we store dead gossip states for 3 days, so that any other nodes that were down at the time of removal can know later not to repopulate the ring with the removed node, but this isn't persisted anywhere, so since you did a full ring restart, the only candidate left is the persisted endpoints, though all nodes should have removed it from there after the decommission/removetoken.

bq. Is HH possibly trying to deliver to the removed node, thus putting it back in the ring on one node?

No, HH will only attempt delivery on an onAlive event, and it doesn't inject any gossip states.;;;","22/Sep/11 19:39;alienth;bq. Can you explain what you mean by ""dead gossip list"" and how this prevents truncate?

The decommissioned node is showing up in the 'UNREACHABLE' list when calling 'describe cluster'. When I attempt to run truncate, the command returns that truncate cannot occur due to a node being down.

bq. After CASSANDRA-2496, we store dead gossip states for 3 days, so that any other nodes that were down at the time of removal can know later not to repopulate the ring with the removed node, but this isn't persisted anywhere, so since you did a full ring restart, the only candidate left is the persisted endpoints, though all nodes should have removed it from there after the decommission/removetoken.

Is there a way I can get a list of endpoints to see how this node showed back up?


Also, any thoughts on why this node only re-appeared on a single node?

Thanks!
Jason;;;","22/Sep/11 22:29;brandon.williams;bq. Is there a way I can get a list of endpoints to see how this node showed back up?

It must be a saved endpoint, if you can attached the latest LocationInfo sstable from that machine (and tell me the IP and/or token) I can take a look.

bq. Also, any thoughts on why this node only re-appeared on a single node?

I'm not sure, let's see if it was still persisted first.;;;","23/Sep/11 06:00;alienth;LocationInfo from the node which re-added the dead node back to the ring.

This LocationInfo is from *after* the decommission of the phantom node, but before the restart which resulted in the node re-adding the phantom node.

The phantom node's token was 120000000000000000000000000000000000000.;;;","23/Sep/11 06:00;alienth;LocationInfo from the node which re-added the dead node back to the ring.

This LocationInfo is from *after* the restart which resulted in the phantom node re-appearing. It is also after the forced token removal of the phantom node.

The phantom node's token was 120000000000000000000000000000000000000.;;;","23/Sep/11 20:02;brandon.williams;0919 is missing the LOCATION_KEY (the node's own token) which is odd, because cassandra will refuse to startup with this table since it should not exist without this key. It does show itself in the saved endpoints, but no other nodes.

0922 is complete in that it contains LOCATION_KEY and cassandra starts right up with it, and I can see the removed token in the saved endpoints with an ip address of 10.34.22.201.  However the strange thing is the timestamp on that column is approximately 2 days _older_ than the one for the local node itself, which should be impossible.  Is there any chance this node's clock was way off or changed?;;;","23/Sep/11 22:28;alienth;Clock on these boxes seems fine. We keep ntpd running at all times. I've also verified via logging that it has been consistent.

How do I go about getting that endpoint *out* of the LocationInfo?;;;","23/Sep/11 23:03;alienth;For outside reference:

Brandon recommended I delete the 'Ring' key from the LocationInfo on this node and then restart to resolve the weirdness.;;;","23/Sep/11 23:16;brandon.williams;The best explanation I have for how you could get here is that SystemTable only forces a flush on the updateToken(Token token) signature.  removeToken and updateToken(InetAddress ep, Token token) do not, so if the machine is restarted before the commitlog is synced, the update/removal can be lost.  Patch to address this.;;;","30/Sep/11 20:05;bcoverston;+1 on the patch;;;","30/Sep/11 20:15;brandon.williams;Committed.  Please reopen if you see this again after 0.8.7.;;;","30/Sep/11 21:49;hudson;Integrated in Cassandra-0.8 #351 (See [https://builds.apache.org/job/Cassandra-0.8/351/])
    Flush system table after updating or removing tokens.
Patch by brandonwilliams, reviewed by Ben Coverston for CASSANDRA-3243

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177810
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"can't use RackInferringSnitch and CQL JDBC's ""CREATE KEYSPACE"" with NetworkTopologyStrategy",CASSANDRA-3239,12524230,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,machenmusik@comcast.net,machenmusik@comcast.net,22/Sep/11 03:24,16/Apr/19 09:32,14/Jul/23 05:52,27/Sep/11 22:15,0.8.7,,,Legacy/CQL,,,0,,,,"If using the CQL JDBC driver, there's a problem with using RackInferringSnitch

1. With RackInferringSnitch, the datacenter names are numeric
2. With CQL and NetworkTopologyStrategy, the data center replicas are specified as strategy_options:<dc-name>=<#-of-replicas>
3. Using a number for <dc-name> fails
4. Using a quoted number for <dc-name> fails
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/11 16:28;xedin;CASSANDRA-3239.patch;https://issues.apache.org/jira/secure/attachment/12496761/CASSANDRA-3239.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,3529,,,Tue Sep 27 22:15:43 UTC 2011,,,,,,,,,,"0|i0ghhz:",94265,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"22/Sep/11 03:54;jbellis;Note: if the fix is complicated let's target it for 1.0.0;;;","22/Sep/11 17:57;jbellis;What is ""compident?"" I'm pretty sure we don't want to allow int-as-ident in most places.;;;","22/Sep/11 18:41;xedin;If I understand everything correctly it means ""composite identifier"" and it is only used by ""CREATE SCHEMA"" statement.;;;","27/Sep/11 16:28;xedin;rebased with the latest cassandra-0.8 branch and COMPIDENT changed to support integers only at the second part.;;;","27/Sep/11 20:01;urandom;+1;;;","27/Sep/11 22:15;urandom;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutboundTcpConnection throws RuntimeException,CASSANDRA-3235,12523869,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,21/Sep/11 14:22,16/Apr/19 09:32,14/Jul/23 05:52,21/Sep/11 15:10,1.0.0,,,,,,0,,,,"Regression introduced in CASSANDRA-1788, as reported by liangfeng on the user list.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/11 14:24;jbellis;3235.txt;https://issues.apache.org/jira/secure/attachment/12495369/3235.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,3530,,,Wed Sep 21 15:10:45 UTC 2011,,,,,,,,,,"0|i0ghg7:",94257,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"21/Sep/11 14:36;slebresne;+1;;;","21/Sep/11 15:10;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeveledCompaction has several performance problems,CASSANDRA-3234,12523796,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,20/Sep/11 23:47,16/Apr/19 09:32,14/Jul/23 05:52,23/Sep/11 13:56,1.0.0,,,,,,0,lcs,,,"Two main problems:

- BF size calculation doesn't take into account LCS breaking the output apart into ""bite sized"" sstables, so memory use is much higher than predicted
- ManyToMany merging is slow.  At least part of this is from running the full reducer machinery against single input sources, which can be optimized away.",,segy,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/11 17:45;slebresne;0005-use-Array-and-Tree-backed-columns-in-compaction-v2.patch;https://issues.apache.org/jira/secure/attachment/12495411/0005-use-Array-and-Tree-backed-columns-in-compaction-v2.patch","21/Sep/11 22:43;jbellis;0005-use-Array-and-Tree-backed-columns-in-compaction-v3.txt;https://issues.apache.org/jira/secure/attachment/12496044/0005-use-Array-and-Tree-backed-columns-in-compaction-v3.txt","22/Sep/11 07:41;slebresne;0005-use-Array-and-Tree-backed-columns-in-compaction-v4.patch;https://issues.apache.org/jira/secure/attachment/12496077/0005-use-Array-and-Tree-backed-columns-in-compaction-v4.patch","23/Sep/11 08:37;jbellis;0006-avoid-echoedRow-when-checking-shouldPurge-is-more-ex.patch;https://issues.apache.org/jira/secure/attachment/12496239/0006-avoid-echoedRow-when-checking-shouldPurge-is-more-ex.patch","23/Sep/11 08:06;jbellis;0006-avoid-echoedRow-when-checking-shouldPurge-is-more-ex.patch;https://issues.apache.org/jira/secure/attachment/12496233/0006-avoid-echoedRow-when-checking-shouldPurge-is-more-ex.patch","22/Sep/11 22:59;jbellis;0006-avoid-echoedRow-when-checking-shouldPurge-is-more-ex.patch;https://issues.apache.org/jira/secure/attachment/12496194/0006-avoid-echoedRow-when-checking-shouldPurge-is-more-ex.patch","21/Sep/11 15:49;jbellis;ASF.LICENSE.NOT.GRANTED--0001-optimize-single-source-case-for-MergeIterator.txt;https://issues.apache.org/jira/secure/attachment/12495388/ASF.LICENSE.NOT.GRANTED--0001-optimize-single-source-case-for-MergeIterator.txt","21/Sep/11 15:49;jbellis;ASF.LICENSE.NOT.GRANTED--0002-add-TrivialOneToOne-optimization.txt;https://issues.apache.org/jira/secure/attachment/12495389/ASF.LICENSE.NOT.GRANTED--0002-add-TrivialOneToOne-optimization.txt","21/Sep/11 15:49;jbellis;ASF.LICENSE.NOT.GRANTED--0003-fix-leveled-BF-size-calculation.txt;https://issues.apache.org/jira/secure/attachment/12495390/ASF.LICENSE.NOT.GRANTED--0003-fix-leveled-BF-size-calculation.txt","21/Sep/11 15:49;jbellis;ASF.LICENSE.NOT.GRANTED--0004-avoid-calling-shouldPurge-unless-necessary.txt;https://issues.apache.org/jira/secure/attachment/12495391/ASF.LICENSE.NOT.GRANTED--0004-avoid-calling-shouldPurge-unless-necessary.txt","21/Sep/11 15:49;jbellis;ASF.LICENSE.NOT.GRANTED--0005-use-Array-and-Tree-backed-columns-in-compaction.txt;https://issues.apache.org/jira/secure/attachment/12495392/ASF.LICENSE.NOT.GRANTED--0005-use-Array-and-Tree-backed-columns-in-compaction.txt",,,,11.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,3531,,,Fri Sep 23 13:56:48 UTC 2011,,,,,,,,,,"0|i0ghfr:",94255,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"21/Sep/11 03:54;jbellis;split OneToOne classes into separate patches and fixed bug in estimated tables calculation;;;","21/Sep/11 17:45;slebresne;I haven't looked at the 3 first patches, but on patch 4 and 5.

+1 on patch 4 (though I agree with the comment in there that it's not the more beautiful refactor ever :))

On patch 5, it cloneMeShallow the first read column family and basically skip all the columns, so that's wrong. Attaching a v2 that makes SSTII directly use the right ISortedColumn factory (to avoid full cloning). Problem is this doesn't translate to ParallelCompactionIterable too well since the actual read is deep into the code. For it, I think we have 2 easy solutions:
  * Just use ArraySortedColumns all the way. This is actually ok because addAll works whatever the input is, it does a merge.
  * Do a full clone to a TreeMapBacked CF on the first cf read
  * Use TreeMapBack CFs all the way.

I went with the first solution in the patch attached (more because it requires the less changes than anything else), though that's probably not optimal for LeveledCompaction (but I'm not sure ParallelCompaction is useful for LeveledCompaction). ;;;","21/Sep/11 17:48;jbellis;bq. that's probably not optimal for LeveledCompaction

Can you elaborate?;;;","21/Sep/11 17:54;jbellis;{code}
+                thisCF = row.getColumnFamilyWithColumns(cf == null ? TreeMapBackedSortedColumns.factory() : ArrayBackedSortedColumns.factory());
{code}

I don't understand why not just ABSC here.
;;;","21/Sep/11 18:09;slebresne;ABSC.addAll does a merge. So if you do cf1.addAll(cf2) and cf1 and cf2 are comparable in size, that's likely as good as it gets. However, if cf2 has much less columns than cf1, then it's likely that TMBSC.addAll (that adds the columns of cf2 one by one) will be faster.

So I guess it all depends on how many sstables we are merging. If we have a lot of them, then adding to TMBSC will be a win (my claim on LeveledCompaction was misdirected, I was thinking of it as lots of sstables, but that doesn't mean we'll merge lots of sstables (unless L0 is behind)). Anyway, it can very well be that ABSC is better in the vast majority of cases so I'm fine with using just that. ;;;","21/Sep/11 22:43;jbellis;simple 05 v3 attached using just ABSC;;;","22/Sep/11 07:41;slebresne;The problem is that now CFS.removeDeleted throws a ConcurrentModificationException. Attaching v4 that replaces the
{noformat}
for (IColumn c : cf)
    cf.remove(c.name())
{noformat}
pattern by
{noformat}
Iterator<IColumn> iter = cf.iterator()
while (iter.hasNext())
{
    IColumn c = iter.next();
    iter.remove();
}
{noformat}
;;;","22/Sep/11 18:03;brandon.williams;+1;;;","22/Sep/11 20:47;jbellis;06 attached as well.;;;","22/Sep/11 22:59;jbellis;updated 06;;;","23/Sep/11 08:06;jbellis;v2 improves leveled expensiveness equation to

L0.size() + count(levels) - ignore.size();;;","23/Sep/11 08:37;jbellis;updated to ""Sets.difference(L0, sstablesToIgnore).size() + manifest.getLevelCount()"";;;","23/Sep/11 08:52;slebresne;The patch as is doesn't compile because it removes the import of StringUtils in LeveledManifest.java (it also add the import of DataTracker in AbstractCompactionStrategy which I think is not useful). But other than +1 on 06 v3.;;;","23/Sep/11 13:56;jbellis;fixed + committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL does not throw an error when invalid hex is supplied,CASSANDRA-3231,12523646,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,20/Sep/11 01:06,16/Apr/19 09:32,14/Jul/23 05:52,27/Sep/11 20:14,0.8.7,,,,,,0,,,,"As reported on irc, if you try to create an index on a CF with a default comparator of BytesType, but you supply invalid hex, weird things happen.  Namely if you try to create one on 'category' you instead get one on '\xca\xfe\xff\xff', which is 4 bytes that appears to coincide with attempting to interpret 'ca', 'te', 'go', 'ry' as hex.",,satishbabu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/11 15:22;jbellis;3231-v2.txt;https://issues.apache.org/jira/secure/attachment/12496700/3231-v2.txt","27/Sep/11 14:16;xedin;CASSANDRA-3231.patch;https://issues.apache.org/jira/secure/attachment/12496686/CASSANDRA-3231.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,1846,,,Tue Sep 27 20:14:56 UTC 2011,,,,,,,,,,"0|i0ghen:",94250,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Sep/11 13:12;xedin;This is not a CQL/CLI problem but a problem in FBUtilities bytesToHex and hexToBytes methods. New issue would be opened to fix that (if needed).;;;","27/Sep/11 13:20;jbellis;I don't think we need a new issue.;;;","27/Sep/11 13:22;xedin;There is the question then - do we really need to change behavior of bytesToHex/hexToBytes methods of FBUtilities class?;;;","27/Sep/11 13:28;jbellis;we need to validate index targets one way or the other.

shouldn't this be done by calling AbstractType.validate?;;;","27/Sep/11 13:32;xedin;This is not a question of validation because {from/get}String methods of any type should return valid data rather the question of BytesType string representation, I'm going to let Brandon comment on the latter.;;;","27/Sep/11 13:46;jbellis;""public ByteBuffer fromString(String source) throws MarshalException""

if the source is not a valid encoding of the type (as ""category"" is not for BytesType) then this should throw;;;","27/Sep/11 13:53;xedin;Exactly, but FBUtilities.hexToBytes does not validate the string so we need to way to validate was is string a correct hex or not;;;","27/Sep/11 14:23;jbellis;what is this existing part of fromString checking for?

{code}
        catch (NumberFormatException e)
        {
            throw new MarshalException(String.format(""cannot parse '%s' as hex bytes"", source), e);
        }
{code};;;","27/Sep/11 14:26;xedin;I think that was expecting hexToBytes to validate string and throw a NumberFormatException but it never did before this patch.;;;","27/Sep/11 15:15;jbellis;looks like that's from the old hexToBytes implementation, where the inner loop used to be

{code}
            bytes[i] = (byte)Integer.parseInt(str.substring(i*2, i*2+2), 16);
{code}

So I think it's safe to remove now.;;;","27/Sep/11 15:19;xedin;do you want to throw MarshalException directly from FBUtilities.hexToBytes to remove NumberFormatException from BytesType.fromString(String)?;;;","27/Sep/11 15:22;jbellis;v2 attached that doesn't need an extra regexp pass.  (since we use bytestype for cql blob data it can be performance sensitive.);;;","27/Sep/11 15:32;xedin;+1 just please add the test from my patch so we don't face such regressions in the future.;;;","27/Sep/11 20:14;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clientutil jar missing from artifacts,CASSANDRA-3230,12523621,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,19/Sep/11 21:13,16/Apr/19 09:32,14/Jul/23 05:52,20/Sep/11 11:39,1.0.0,,,Packaging,,,0,cql,,,The new clientutil jar is not being included in binary release artifacts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/11 21:14;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3230-copy-clientutil-jar-to-binary-release-a.txt;https://issues.apache.org/jira/secure/attachment/12495149/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3230-copy-clientutil-jar-to-binary-release-a.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4006,,,Tue Sep 20 11:39:45 UTC 2011,,,,,,,,,,"0|i0ghe7:",94248,,,,,Normal,,,,,,,,,,,,,,,,,"19/Sep/11 21:25;jbellis;+1;;;","20/Sep/11 11:39;urandom;committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cassandra-cli use micro second timestamp, but CQL use milli second",CASSANDRA-3227,12523482,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,sabro,sabro,18/Sep/11 15:55,16/Apr/19 09:32,14/Jul/23 05:52,22/Sep/11 03:56,1.0.0,,,Legacy/CQL,,,1,cql,,,"cassandra-cli set micro second timestamp by FBUtilities.timestampMicros. But CQL insert or update operation set milli second timestamp by AbstractModification.getTimestamp.

If you register data by cassandra-cli, you can't update data by CQL. Because CQL timestamp is judged as past time.",,sabro,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/11 02:18;jbellis;3227.txt;https://issues.apache.org/jira/secure/attachment/12495008/3227.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4008,,,Thu Sep 22 03:56:13 UTC 2011,,,,,,,,,,"0|i0ghcv:",94242,,sabro,,sabro,Normal,,,,,,,,,,,,,,,,,"19/Sep/11 02:18;jbellis;{noformat}
+    - CQL inserts/updates now generate microsecond resolution timestamps
+      by default, instead of millisecond.
{noformat}

Adds ClientState.getTimestamp to go the extra mile:
{noformat}
+    /**
+     * This clock guarantees that updates from a given client will be ordered in the sequence seen,
+     * even if multiple updates happen in the same millisecond.  This can be useful when a client
+     * wants to perform multiple updates to a single column.
+     */
{noformat}
;;;","19/Sep/11 08:45;slebresne;I think this breaks rolling upgrade for anyone using CQL, since during the time where this is a mix a node pre and post this patch, columns will get either micro or milli seconds timestamps depending on the coordinator. ;;;","19/Sep/11 13:56;jbellis;I'm happy to note this in NEWS.  If this is a problem for your workload, you can do a non-rolling restart.;;;","19/Sep/11 15:25;slebresne;Works for me but more because ""we either do it now or never and leaving it to milliseconds would really sucks"" than out of sheer enthusiasm towards the idea.

As for the patch, I'm not too sure about this clientState.getTimestamp(). A clientState is a potentially a long leaving object, it lasts as long as the connection to a given client last. Since the patch only initialize to the current time but then update by +1 on every call disregarding of the actual time, it seems the timestamp will get completely desynchronized quickly. Or am I misunderstanding this ?;;;","19/Sep/11 15:35;jbellis;{code}
+    public long getTimestamp()
+    {
+        long current = System.currentTimeMillis() * 1000;
+        clock = clock >= current ? clock + 1 : current;
+        return clock;
+    }
{code}

I may be missing something obvious, but my intention was to only use the +1 path if it needs to to avoid a conflict.  (And as soon as wall-clock time exceeds the +1 path, it switches back.);;;","19/Sep/11 15:47;slebresne;bq. I may be missing something obvious,

Nah, but I probably need to see a doctor for that dyslexia.

+1 with the upgrade warning in NEWS;;;","21/Sep/11 21:40;sabro;Thank you. The patch fixed the problem.
""aa-bb"" is registered in unpatched cassandra.
""cc-dd"" is registered in patched cassandra.

{noformat}
[default@QuestWorld] list Globalization;
Using default limit of 100
-------------------
RowKey: 537570706f72746564
=> (column=aa-bb, value=cc, timestamp=1316640431837)
=> (column=cc-dd, value=ee, timestamp=1316640612801000)
=> (column=en-UK, value=en, timestamp=1316356279874416)
=> (column=en-US, value=en, timestamp=1312220986047000)
=> (column=fr-FR, value=fl, timestamp=1316356768334)
=> (column=ja-JP, value=ja, timestamp=1312220978873000)
=> (column=ko-kr, value=no, timestamp=1316356622692000)
=> (column=mm-nn, value=ss, timestamp=1316640580750399)
=> (column=yy-zz, value=xx, timestamp=1316640399778399)
-------------------
RowKey: ffffffffed
=> (column=it-CH, value=it, timestamp=1316356541245000)

2 Rows Returned.
{noformat};;;","22/Sep/11 03:56;jbellis;(committed);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeveledCompactionStrategy is too complacent,CASSANDRA-3224,12523442,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,17/Sep/11 18:10,16/Apr/19 09:32,14/Jul/23 05:52,20/Sep/11 13:13,1.0.0,,,,,,0,compaction,,,"As the title says, it barely does anything.  I inserted 50G worth of data with 1G heap and 99% overwrite ratio, and it only compacted twice:

{noformat}
 INFO [CompactionExecutor:1] 2011-09-16 22:29:54,572 CompactionTask.java (line 118) Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-1-Data.db')]
 INFO [CompactionExecutor:1] 2011-09-16 22:29:58,606 CompactionTask.java (line 220) Compacted to [/var/lib/cassandra/data/Keyspace1/Standard1-h-2-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-4-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-5-Data.db,].  12,595,811 to 12,595,811 (~100% of original) bytes for 40,501 keys at 3.058122MBPS.  Time: 3,928ms.
 INFO [CompactionExecutor:1] 2011-09-16 22:29:58,607 CompactionTask.java (line 222) CF Total Bytes Compacted: 12,595,811
 INFO [CompactionExecutor:3] 2011-09-16 22:29:58,889 CompactionTask.java (line 118) Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-4-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-5-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-3-Data.db')]
 INFO [CompactionExecutor:3] 2011-09-16 22:30:06,900 CompactionTask.java (line 220) Compacted to [/var/lib/cassandra/data/Keyspace1/Standard1-h-7-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-9-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-11-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-12-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-14-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-15-Data.db,].  28,374,396 to 28,374,396 (~100% of original) bytes for 91,236 keys at 3.380379MBPS.  Time: 8,005ms.
 INFO [CompactionExecutor:3] 2011-09-16 22:30:06,901 CompactionTask.java (line 222) CF Total Bytes Compacted: 40,970,207
{noformat}

Resulting in the following levels:

{noformat}
L0: 4965
L1: 6
L2: 0
L3: 0
L4: 0
L5: 0
L6: 0
L7: 0
{noformat}

This is obviously going to result in extremely poor read performance.",,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/11 18:02;jbellis;3224-v2.txt;https://issues.apache.org/jira/secure/attachment/12495117/3224-v2.txt","19/Sep/11 19:34;jbellis;3224-v3.txt;https://issues.apache.org/jira/secure/attachment/12495131/3224-v3.txt","19/Sep/11 20:25;jbellis;3224-v4.txt;https://issues.apache.org/jira/secure/attachment/12495137/3224-v4.txt","19/Sep/11 23:07;jbellis;3224-v5.txt;https://issues.apache.org/jira/secure/attachment/12495159/3224-v5.txt","20/Sep/11 02:43;jbellis;3224-v6.txt;https://issues.apache.org/jira/secure/attachment/12495186/3224-v6.txt","18/Sep/11 03:08;jbellis;3224.txt;https://issues.apache.org/jira/secure/attachment/12494955/3224.txt","19/Sep/11 19:23;brandon.williams;system.log.bz2;https://issues.apache.org/jira/secure/attachment/12495127/system.log.bz2",,,,,,,,7.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4009,,,Tue Sep 20 13:13:11 UTC 2011,,,,,,,,,,"0|i0ghbj:",94236,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"18/Sep/11 03:08;jbellis;This is a regression caused by CASSANDRA-3181.

The problem is that when CompactionTask calls submitBackground, the original Task is not yet technically done, so LeveledCompactionStrategy returns ""nothing to do"" to prevent multiple tasks running in parallel, which is not supported for LCS.  So after the first compaction runs five minutes in, that's all she wrote.

Fixing the Task/Executor mess is out of scope for 1.0 (created CASSANDRA-3225 to address in 1.1) s;;;","19/Sep/11 17:43;brandon.williams;Better, but it looks like LCS starts miscounting at some point, and this causes compactions to stop:

{noformat}

DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,359 LeveledManifest.java (line 218) Level 0 contains 85 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,360 LeveledManifest.java (line 218) Level 1 contains 0 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,360 LeveledManifest.java (line 218) Level 2 contains 14 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,361 LeveledManifest.java (line 218) Level 3 contains 0 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,361 LeveledManifest.java (line 218) Level 4 contains 0 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,361 LeveledManifest.java (line 218) Level 5 contains 0 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,361 LeveledManifest.java (line 218) Level 6 contains 0 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,361 LeveledManifest.java (line 218) Level 7 contains 0 SSTables
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,361 LeveledManifest.java (line 197) Compaction score for level 0 is 0.0
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,362 LeveledManifest.java (line 197) Compaction score for level 2 is 0.0
DEBUG [CompactionExecutor:4] 2011-09-19 17:40:31,362 LeveledCompactionStrategy.java (line 112) CompactionManager candidates are 
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,362 LeveledManifest.java (line 218) Level 0 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,362 LeveledManifest.java (line 218) Level 1 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,362 LeveledManifest.java (line 218) Level 2 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,362 LeveledManifest.java (line 218) Level 3 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledManifest.java (line 218) Level 4 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledManifest.java (line 218) Level 5 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledManifest.java (line 218) Level 6 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledManifest.java (line 218) Level 7 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledCompactionStrategy.java (line 112) CompactionManager candidates are 
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledManifest.java (line 218) Level 0 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledManifest.java (line 218) Level 1 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,363 LeveledManifest.java (line 218) Level 2 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,364 LeveledManifest.java (line 218) Level 3 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,364 LeveledManifest.java (line 218) Level 4 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,364 LeveledManifest.java (line 218) Level 5 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,364 LeveledManifest.java (line 218) Level 6 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,364 LeveledManifest.java (line 218) Level 7 contains 0 SSTables
DEBUG [CompactionExecutor:1] 2011-09-19 17:40:31,364 LeveledCompactionStrategy.java (line 112) CompactionManager candidates are 
{noformat}

Actual levels are (according to json manifest):
{noformat}
L0: 85
L1: 0
L2: 14
L3: 0
L4: 0
L5: 0
L6: 0
L7: 0
{noformat}

Attempting to force a compaction via nodetool:
{noformat}

 INFO 17:42:28,933 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-2-Data.db'), SSTableReader(path='/var/lib
/cassandra/data/system/LocationInfo-h-1-Data.db')]
DEBUG 17:42:28,933 Expected bloom filter size : 512
 INFO 17:42:28,956 Compacted to [/var/lib/cassandra/data/system/LocationInfo-h-3-Data.db,].  498 to 447 (~89% of original) bytes for 3 key
s at 0.019377MBPS.  Time: 22ms.
 INFO 17:42:28,956 CF Total Bytes Compacted: 225,632,757
 INFO 17:42:28,956 Nothing to compact in Migrations.Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
 INFO 17:42:28,957 Nothing to compact in Schema.Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)
{noformat};;;","19/Sep/11 18:02;jbellis;v2 attached w/ one-line fix to score computation;;;","19/Sep/11 19:23;brandon.williams;Somehow, I ended up in the following situation, even though my workload is 100% unique and some compactions did succeed:

{noformat}
L0: 974
L1: 0
L2: 0
L3: 0
L4: 0
L5: 0
L6: 0
L7: 0
{noformat}

The box then died from OOM while trying to write the BF.  System log with db.compaction at DEBUG attached.;;;","19/Sep/11 19:34;jbellis;v3 improves some logging and adds an assert;;;","19/Sep/11 20:25;jbellis;v4 adds more debug logging, which makes clear that the empty manifests being logged were from other CFS.

caps L0 compaction candidates at 32 to avoid OOMing from pessimisting BF sizing.

picks L0 initial victim based on age of data rather than randomly.;;;","19/Sep/11 23:07;jbellis;v5 attached.  changes max-from-L0 to 10, and fixes manifest loading on restart.;;;","20/Sep/11 02:43;jbellis;v6 changes scoring of levels to prioritize higher levels, reducing the duplicate work that gets done when lower levels are done in turn.

also logs what level each compacted sstable comes from.;;;","20/Sep/11 12:43;brandon.williams;+1;;;","20/Sep/11 13:13;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
probably don't need to do full copy to row cache after un-mmap() change,CASSANDRA-3223,12523412,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yangyangyyy,yangyangyyy,yangyangyyy,17/Sep/11 00:31,16/Apr/19 09:32,14/Jul/23 05:52,18/Sep/11 04:51,1.0.0,,,,,,0,,,,"3179  changes from directly using the bytebuffer from mmap(), to copying that buffer,

CFS.cacheRow() https://github.com/apache/cassandra/blob/cassandra-1.0.0/src/java/org/apache/cassandra/db/ColumnFamilyStore.java   line 1126
says it makes a deep copy exactly to prevent issues from unmmap().

maybe this deep copy is not needed now given 3179


if so, maybe slightly better performance in both speed and memory",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/11 04:19;yangyangyyy;0002-do-not-need-to-deep-copy-column-value-bytebuffer-now.patch;https://issues.apache.org/jira/secure/attachment/12494959/0002-do-not-need-to-deep-copy-column-value-bytebuffer-now.patch",,,,,,,,,,,,,,1.0,yangyangyyy,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4010,,,Sun Sep 18 04:51:07 UTC 2011,,,,,,,,,,"0|i0ghb3:",94234,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"17/Sep/11 02:04;jbellis;I think you're right.  Care to submit a patch to remove the extra copy?;;;","17/Sep/11 04:50;yangyangyyy;sure

;;;","18/Sep/11 04:19;yangyangyyy;do not need to deep copy column value bytebuffer into row cache, now that we already do this copy in JIRA 3179;;;","18/Sep/11 04:51;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cfhistograms is transposed/wrong again,CASSANDRA-3222,12523391,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,16/Sep/11 20:58,16/Apr/19 09:32,14/Jul/23 05:52,17/Sep/11 17:45,0.7.10,0.8.7,,,,,0,,,,"Read/write latencies are transposed, row size is always equal the column count.  I think we've fixed this at least twice before, but here it is again.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/11 22:37;brandon.williams;3222.txt;https://issues.apache.org/jira/secure/attachment/12494880/3222.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4011,,,Sat Sep 17 17:55:21 UTC 2011,,,,,,,,,,"0|i0ghaf:",94231,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"16/Sep/11 22:37;brandon.williams;Turns out that CASSANDRA-2123 never fixed the transposition.  CASSANDRA-2284 broke the row size/column count.;;;","17/Sep/11 02:05;jbellis;+1;;;","17/Sep/11 17:45;brandon.williams;Committed.;;;","17/Sep/11 17:55;hudson;Integrated in Cassandra-0.7 #553 (See [https://builds.apache.org/job/Cassandra-0.7/553/])
    Fix cfhistograms read/write latency transposition.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-3222

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1172024
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/tools/NodeCmd.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodes started at the same time end up with the same token,CASSANDRA-3219,12523364,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,tjake,tjake,16/Sep/11 17:39,16/Apr/19 09:32,14/Jul/23 05:52,19/Sep/11 16:59,1.0.0,,,,,,0,bootstrap,,,"Since autoboostrap is defaulted to on when you start a cluster at once (http://screenr.com/5G6) you can end up with nodes being assigned the same token.

{code}
INFO 17:34:55,688 Node /67.23.43.14 is now part of the cluster
 INFO 17:34:55,698 InetAddress /67.23.43.14 is now UP
 INFO 17:34:55,698 Nodes /67.23.43.14 and tjake2/67.23.43.15 have the same token 8823900603000512634329811229926543166.  Ignoring /67.23.43.14
 INFO 17:34:55,698 Node /98.129.220.182 is now part of the cluster
 INFO 17:34:55,698 InetAddress /98.129.220.182 is now UP
 INFO 17:34:55,698 Nodes /98.129.220.182 and tjake2/67.23.43.15 have the same token 8823900603000512634329811229926543166.  Ignoring /98.129.220.182
{code}",,slebresne,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/11 19:50;jbellis;3219.txt;https://issues.apache.org/jira/secure/attachment/12494854/3219.txt","19/Sep/11 11:46;slebresne;3219_v2.patch;https://issues.apache.org/jira/secure/attachment/12495049/3219_v2.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4013,,,Mon Sep 19 17:22:04 UTC 2011,,,,,,,,,,"0|i0gh93:",94225,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"16/Sep/11 18:45;jbellis;We should add special values ""auto"" and ""random"" to initial_token, so you can have random with bootstrap and auto-selected w/o.

Of course both of those are not recommended vs picking your own.;;;","16/Sep/11 18:55;vijay2win@yahoo.com;Can we also have some thing like ""equal split"" which will try to split the token ranges into perfect halfs? this will work well for bootstrapping ring of sizes = 2^n;;;","16/Sep/11 19:06;jbellis;That is what ""auto"" does, with the caveat that nodes need to be started 2 minutes apart so they don't race as in Jake's example here.;;;","16/Sep/11 19:50;jbellis;auto and random initial_token modes added.;;;","19/Sep/11 11:46;slebresne;bq. auto and random initial_token modes added.

I hate it and I'm -1 on that idea.

Basically, I think that it's more complicated to explain/understand how to choose between those two options that it was to explain the ""old"" auto-bootstrap option while it's essentially the same option. The default to random would also make it more likely for people to leave it at that when bootstrapping new nodes, while random is really the worst possible algorithm you can use expect maybe for the 2-3 initial nodes of a cluster (and even then it's really only admissible because the balanced token algorithm don't work in that case and picking random token is the only simple choice we have).

I'd rather add back the auto-bootstrap option than setting the initial_token to random.

As for alternatives, I can propose one of:
  * Decide whether we are really bootstrapping (if we are, balanced token is the ""right"" automatic choice, otherwise we have no other choice than to fall back to random tokens) or not based on whether there is a keyspace defined already. That is the same test we use to decide whether we actually do some bootstrap streaming or not so this doesn't seem too far fetched (attaching a v2 patch for that to make it clear what I mean here).
  * Stop pretending we know how to pick up token automatically and just force user to set a token. We can default that token to 0 so that you can start a single node cluster with 0 configuration and we can ship a new small tiny script that compute the tokens for a n node initial cluster if we want people to be able to do without a calculator.
;;;","19/Sep/11 13:53;jbellis;bq. I think that it's more complicated to explain/understand how to choose between those two options

Huh?  This is a HUGE simplification because initial_token behavior depends only on initial_token.  The old behavior (where initial_token=empty behavior does one thing with auto_bootstrap=true, and another with a_b=false) was ENORMOUSLY confusing: EVERY training class I taught was baffled by this.

Further, the old behavior doesn't let you specify bootstrap/random or nobootstrap/auto, both of which are valid things to do.

bq. based on whether there is a keyspace defined already

I don't see how this helps the situation Jake describes.

bq. just force user to set a token

This is a non-starter.;;;","19/Sep/11 15:00;slebresne;
bq. Further, the old behavior doesn't let you specify bootstrap/random

That would depend on what is definition of valid. In my opinion, picking random token is *always* stupid, it will always result in crappy distribution (it just happens to be less stupid than ""auto"" in the noboostrap case, which imho is and should remain the only reason we ever generate random token). If you bootstrap, it means you have an existing cluster with data in it (I'm not saying you *have to*, I'm saying this is why bootstrap is for and so should be the case if you don't do something wrong). In such situation, I don't see why you would want to pick a random token. If some people like to live on the edge, they can write a random token generator and use that, but that we would want to expose an option, hence suggesting that this could be something useful ...

bq. or nobootstrap/auto, both of which are valid things to do

Again, really depends on the definition of ""valid"". First, if you start two nodes at the same time with that, you end up in this ticket situation. Sure the patch adds a ""don't do it"" comment but it doesn't really fix it more than that. Second, noboostrap (when token selection is involved, i.e, not a replace_token) is mainly useful to set up an initial cluster, that is when nodes don't have data at all (otherwise you want to bootstrap the node). In that situation, auto will likely don't do anything useful (it's a completely degenerated case for the algorithm). That the nobootstrap/auto pair doesn't work correctly is actually the only reason I can come up for us picking a random token in <= 0.8 versions.

Besides, when was the last time we had a user requesting to do one of boostrap/random or nobootstrap/auto, or us recommanding anything else than 'pick your token yourself'?

bq. I don't see how this helps the situation Jake describes

I'm willing to bet that when Jake encountered that problem he was trying to set up an initial cluster *before* having set up schemas and inserted some data. In that case, the second patch would pick a random token so there wouldn't be problem.

The thing is, there is not too many way to create a Cassandra cluster. First you create a cluster with n initial machines. For that you want to be in mode noboostrap/random (noboostrap/auto doesn't really work too well with no data; and by noboostrap I don't really speak of the auto-boostrap=false option, but more of not doing data streaming). Once you have data in the cluster, you want to bootstrap and then auto is always less stupid than random (IMHO). Hence the rational for the v2 patch.

bq. This is a non-starter

I find it weird to consider that a non-starter so rapidly when we all know that the very first advise we give is to hand pick token and that it's unreasonable to use auto (let's not talk about random) token in any real life situation (even the config file basically says it). But I'm willing to consider that it's not the right time to discuss that and to discard that solution, at least for now.
;;;","19/Sep/11 15:11;jbellis;bq. when Jake encountered that problem he was trying to set up an initial cluster before having set up schemas and inserted some data

Maybe, but it sounded to me like the situation was ""I was adding new nodes to an existing cluster, and they picked the same token.""  Which as I pointed out in chat is NOT a new problem, but it's one we should address.

Another way of looking at my patch is, it's okay for defaults to give you something suboptimal (random tokens) but it's not okay for it to give you something broken (two nodes w/ same token).  If you want auto token picking and its potential downsides, you need to opt in.  (And hopefully read the comments and go with manual token assignment instead.)

bq. I find it weird to consider that a non-starter so rapidly

Because demo-ability matters.;;;","19/Sep/11 15:14;vijay2win@yahoo.com;IMO... Instead of random we should actually try and balance the tokens when no keyspace is defined... By which I mean moving the nodes around as there is no data to stream and at that time it will be more predictive... This will give a better distribution...


;;;","19/Sep/11 15:30;jbellis;Sure, in magic fairy land I'd love that too, but the question here is what can we improve for 1.0.;;;","19/Sep/11 16:14;slebresne;bq. Which as I pointed out in chat is NOT a new problem, but it's one we should address.

Agreed, but I suspect this is due (in the not a new problem case) to races in Boostrapper.getBootstrapSource() detection of already bootstrapping node. We should fix that if possible, which the patch don't really since if you will still potentially have those race with auto. But note that this problem is present in 0.8 and I think is not a top priority because it's relatively rare to actually start bootstrapping 2 nodes at the same time in real life. Again, I'm not saying we shouldn't fix, but it's ok to say that as long as it's not worth than in 0.8, it can wait post 1.0.0 to get fixed.

Now there is a actual new problem with 1.0.0. That problem is that when you start an initial cluster, i.e, when in 0.8 you would start node with auto-boostrap=false, you do often end up starting nodes simultaneously. That is why older version were using random token when auto-bootstrap was false. This problem does need to be fix for 1.0.0 because that is a serious regression. However, my argument is that even though we now default to auto-boostrap=true, that doesn't mean that there is no difference between setting up the initial nodes of a cluster and the latter bootstrapping of nodes to add capacity to an existing cluster. Indeed, in 1.0.0 we decided to draw this line based on whether a schema had been created or not (we call the bootstrap() method based on that). Imho, this means that we have no boostrap option and the ""I have no schema"" is the old auto-boostrap=false. So we should use random token in that case and balanced one otherwise the same way we are doing it in 0.8.

And I'm saying that I would prefer we do that and report the fixing of Boostrapper.getBootstrapSource() rather than exposing (and making the default) the random choice of tokens, which is my opinion is a bad idea.;;;","19/Sep/11 16:59;jbellis;I'm still not convinced this is intuitive behavior, but it's no more broken than what we've lived with for the last couple years, so I'll run with that on the grounds of ""we're already in freeze, we shouldn't mess with things when we can help it.""

rebased v2 + committed;;;","19/Sep/11 17:22;jbellis;v2 actually breaks things because getNewToken doesn't repeat the test for seed-ness.  I reverted things and went with a simpler change to accomplish the same goal in r1172717.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Failure reading a erroneous/spurious AutoSavingCache file can result in a failed application of a migration, which can prevent a node from reaching schema agreement.",CASSANDRA-3218,12523356,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,eldondev,eldondev,eldondev,16/Sep/11 16:58,16/Apr/19 09:32,14/Jul/23 05:52,22/Sep/11 21:09,0.8.7,1.0.0,,,,,0,,,,"Failure reading a erroneous/spurious AutoSavingCache file can result in a failed application of a migration, which can prevent a node from reaching schema agreement. This is distinctly possible when a machine loses it's data partition, and attempts to recover the schema upon restart, and so has to apply all the migrations. The initial stack traces look like this:

Add column family: org.apache.cassandra.config.CFMetaData@38bcfee6[cfId=1000,ksName=someks,cfName=somecf,cfType=Standard,comparat
or=org.apache.cassandra.db.marshal.UTF8Type,subcolumncomparator=<null>, ...

Followed by:

ERROR 00:56:47,974 Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.RuntimeException: java.lang.NegativeArraySizeException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NegativeArraySizeException
        at org.apache.cassandra.cache.AutoSavingCache.readSaved(AutoSavingCache.java:130)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:273)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:465)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:435)
        at org.apache.cassandra.db.Table.initCf(Table.java:369)
        at org.apache.cassandra.db.migration.AddColumnFamily.applyModels(AddColumnFamily.java:93)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:153)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:73)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more

Ultimately, attempted changes to this keyspace/cf will fail like this:

ERROR 13:07:51,006 Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown CF 1000
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException: Unknown CF 1000
        at org.apache.cassandra.db.Table.getColumnFamilyStore(Table.java:155)
        at org.apache.cassandra.db.Table.getColumnFamilyStore(Table.java:148)
        at org.apache.cassandra.db.migration.DropKeyspace.applyModels(DropKeyspace.java:63)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:153)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:73)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/11 17:49;eldondev;CASSANDRA-3218-autosave_exception_handling-v2.patch;https://issues.apache.org/jira/secure/attachment/12495115/CASSANDRA-3218-autosave_exception_handling-v2.patch","16/Sep/11 17:23;eldondev;CASSANDRA-3218-autosave_exception_handling.patch;https://issues.apache.org/jira/secure/attachment/12494825/CASSANDRA-3218-autosave_exception_handling.patch",,,,,,,,,,,,,2.0,eldondev,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4014,,,Thu Sep 22 22:22:44 UTC 2011,,,,,,,,,,"0|i0gh8n:",94223,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Sep/11 17:23;eldondev;First pass at a patch that fixes this;;;","16/Sep/11 19:02;jbellis;Maybe we should just extend the existing IOException to Exception.  Basically our attitude should be ""I don't care what went wrong loading the cache, it's optional, let me continue."";;;","19/Sep/11 17:48;eldondev;Sounds good to me.;;;","22/Sep/11 21:09;jbellis;committed, thanks!;;;","22/Sep/11 22:22;hudson;Integrated in Cassandra-0.8 #337 (See [https://builds.apache.org/job/Cassandra-0.8/337/])
    Don't allow any cache loading exceptions to halt startup
patch by Eldon Stegall; reviewed by jbellis for CASSANDRA-3218

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1174392
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cache/AutoSavingCache.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A streamOutSession keeps sstables references forever if the remote end dies,CASSANDRA-3216,12523332,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,16/Sep/11 13:36,16/Apr/19 09:32,14/Jul/23 05:52,19/Sep/11 15:32,1.0.0,,,,,,0,streaming,,,"A streamOutSession acquire a reference on the sstable it will stream and release them as soon as each sstable has been fully streamed. However, since a stream session has currently no means to know when it failed, we'll keep references indefinitely (meaning until next restart) if their is a failure. One way a stream session could very easily fail is if the remote end dies. We must make sure we correctly release sstable references when that happens.

Note that it won't be bulletproof, there is probably other means by which a streaming could fail: a bug in the code throwing an exception, no space left on the receiving end, etc... But those are unlikely enough that I propose to care only for the case of a node dying for now and leave the bullet-proofing to CASSANDRA-3112. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/11 13:37;slebresne;3216.patch;https://issues.apache.org/jira/secure/attachment/12494794/3216.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4015,,,Mon Sep 19 15:32:20 UTC 2011,,,,,,,,,,"0|i0gh7r:",94219,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Sep/11 19:10;jbellis;Couldn't this cause false positives if a GC pause marks a node as ""down"" but really the stream would finish normally afterwards?;;;","19/Sep/11 06:52;slebresne;It would, it's the same problem than with CASSANDRA-2433. But the patch the technique we've use there to wait to reach twice the normal phi convict threshold to hopefully make sure this won't happen. But we can increase the threshold even higher if we think that doubling doesn't put us enough on the safe side.;;;","19/Sep/11 13:57;jbellis;+1;;;","19/Sep/11 15:32;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The word count example demonstrating hadoop integration fails in trunk,CASSANDRA-3215,12523306,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,mccloud35,mccloud35,16/Sep/11 09:47,16/Apr/19 09:32,14/Jul/23 05:52,19/Sep/11 08:47,1.0.0,,,,,,0,hadoop,,,"The following stack traces after running, bin/hadoop in the trunk (0.8.2-dev-SNAPSHOT):

./bin/word_count
11/09/15 12:28:28 INFO WordCount: output reducer type: cassandra
11/09/15 12:28:29 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
11/09/15 12:28:30 INFO mapred.JobClient: Running job: job_local_0001
11/09/15 12:28:30 INFO mapred.MapTask: io.sort.mb = 100
11/09/15 12:28:30 INFO mapred.MapTask: data buffer = 79691776/99614720
11/09/15 12:28:30 INFO mapred.MapTask: record buffer = 262144/327680
11/09/15 12:28:30 WARN mapred.LocalJobRunner: job_local_0001
java.lang.RuntimeException: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:132)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:620)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getLocation(ColumnFamilyRecordReader.java:176)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:113)
	... 4 more
11/09/15 12:28:31 INFO mapred.JobClient:  map 0% reduce 0%
11/09/15 12:28:31 INFO mapred.JobClient: Job complete: job_local_0001
11/09/15 12:28:31 INFO mapred.JobClient: Counters: 0
11/09/15 12:28:31 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
11/09/15 12:28:32 INFO mapred.JobClient: Running job: job_local_0002
11/09/15 12:28:32 INFO mapred.MapTask: io.sort.mb = 100
11/09/15 12:28:32 INFO mapred.MapTask: data buffer = 79691776/99614720
11/09/15 12:28:32 INFO mapred.MapTask: record buffer = 262144/327680
11/09/15 12:28:32 WARN mapred.LocalJobRunner: job_local_0002
java.lang.RuntimeException: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:132)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:620)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getLocation(ColumnFamilyRecordReader.java:176)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:113)
	... 4 more
11/09/15 12:28:33 INFO mapred.JobClient:  map 0% reduce 0%
11/09/15 12:28:33 INFO mapred.JobClient: Job complete: job_local_0002
11/09/15 12:28:33 INFO mapred.JobClient: Counters: 0
11/09/15 12:28:33 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
11/09/15 12:28:34 INFO mapred.JobClient: Running job: job_local_0003
11/09/15 12:28:34 INFO mapred.MapTask: io.sort.mb = 100
11/09/15 12:28:34 INFO mapred.MapTask: data buffer = 79691776/99614720
11/09/15 12:28:34 INFO mapred.MapTask: record buffer = 262144/327680
11/09/15 12:28:34 WARN mapred.LocalJobRunner: job_local_0003
java.lang.RuntimeException: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:132)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:620)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getLocation(ColumnFamilyRecordReader.java:176)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:113)
	... 4 more
11/09/15 12:28:35 INFO mapred.JobClient:  map 0% reduce 0%
11/09/15 12:28:35 INFO mapred.JobClient: Job complete: job_local_0003
11/09/15 12:28:35 INFO mapred.JobClient: Counters: 0
11/09/15 12:28:35 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
11/09/15 12:28:36 INFO mapred.JobClient: Running job: job_local_0004
11/09/15 12:28:36 INFO mapred.MapTask: io.sort.mb = 100
11/09/15 12:28:37 INFO mapred.MapTask: data buffer = 79691776/99614720
11/09/15 12:28:37 INFO mapred.MapTask: record buffer = 262144/327680
11/09/15 12:28:37 WARN mapred.LocalJobRunner: job_local_0004
java.lang.RuntimeException: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:132)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:620)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getLocation(ColumnFamilyRecordReader.java:176)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:113)
	... 4 more
11/09/15 12:28:37 INFO mapred.JobClient:  map 0% reduce 0%
11/09/15 12:28:37 INFO mapred.JobClient: Job complete: job_local_0004
11/09/15 12:28:37 INFO mapred.JobClient: Counters: 0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4016,,,Mon Sep 19 08:47:54 UTC 2011,,,,,,,,,,"0|i0gh7b:",94217,,,,,Low,,,,,,,,,,,,,,,,,"16/Sep/11 21:15;brandon.williams;This shouldn't happen with the reverted CASSANDRA-2388, can you update to the latest trunk and try again?;;;","19/Sep/11 08:47;mccloud35;This is fixed after updating trunk and building. As per Brandon's comment reverting CASSANDRA-2388 was the fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Accomodate missing encryption_options in IncomingTcpConnection.stream,CASSANDRA-3212,12523189,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,15/Sep/11 13:09,16/Apr/19 09:32,14/Jul/23 05:52,15/Sep/11 13:57,0.8.6,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3599,,,,,,,,"15/Sep/11 13:19;jbellis;3212.txt;https://issues.apache.org/jira/secure/attachment/12494613/3212.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4017,,,Thu Sep 15 14:19:27 UTC 2011,,,,,,,,,,"0|i0gh5r:",94210,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"15/Sep/11 13:19;jbellis;one-line fix.;;;","15/Sep/11 13:23;slebresne;+1;;;","15/Sep/11 13:57;jbellis;committed;;;","15/Sep/11 14:19;hudson;Integrated in Cassandra-0.8 #329 (See [https://builds.apache.org/job/Cassandra-0.8/329/])
    Accomodate missing encryption_options in IncomingTcpConnection.stream
patch by jbellis; reviewed by slebresne for CASSANDRA-3212

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1171091
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/IncomingTcpConnection.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memtables do not need to be flushed on the Table.apply() path anymore after 2449,CASSANDRA-3210,12523138,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yangyangyyy,yangyangyyy,yangyangyyy,15/Sep/11 01:24,16/Apr/19 09:32,14/Jul/23 05:52,15/Sep/11 03:39,1.0.0,,,,,,0,,,,"2449 removes auto-flush from Table.apply(), but the data structure is still there, no harm, but better remove it:

in
https://github.com/apache/cassandra/blob/c7cdc317c9a14e29699f9842424388aee77d0e1a/src/java/org/apache/cassandra/db/Table.java

line 399 and 470",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/11 01:24;yangyangyyy;0001-memtables-do-not-need-to-be-flushed-on-the-Table.app.patch;https://issues.apache.org/jira/secure/attachment/12494550/0001-memtables-do-not-need-to-be-flushed-on-the-Table.app.patch",,,,,,,,,,,,,,1.0,yangyangyyy,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4018,,,Thu Sep 15 03:39:05 UTC 2011,,,,,,,,,,"0|i0gh4v:",94206,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"15/Sep/11 03:39;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
USE <keyspace> doesn't work for numeric keyspaces,CASSANDRA-3208,12523104,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,liqweed,liqweed,14/Sep/11 21:03,16/Apr/19 09:32,14/Jul/23 05:52,16/Sep/11 21:22,0.8.7,,,,,,0,cli,,,"In the CLI, {code}USE <keyspace>;{code} doesn't work for keyspaces' names that contain only digits.
The error I'm getting is:
{{Syntax error at position 4: mismatched input '20110914' expecting Identifier}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/11 20:43;xedin;CASSANDRA-3208.patch;https://issues.apache.org/jira/secure/attachment/12494860/CASSANDRA-3208.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4019,,,Fri Sep 16 22:22:54 UTC 2011,,,,,,,,,,"0|i0gh47:",94203,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"14/Sep/11 22:48;jbellis;Did you try quoting the KS name?;;;","16/Sep/11 08:21;liqweed;Quoting (neither single nor double quotes) didn't help.;;;","16/Sep/11 20:43;xedin;Numberic keyspace names do work for me on the latest cassandra-0.8 branch, the only thing what patch does is allows to use quotes in USE command.

Here is the CLI session

{noformat}
[git:cassandra-0.8] (~/work/java/cassandra) → ./bin/cassandra-cli --host localhost
Connected to: ""Test Cluster"" on localhost/9160
Welcome to the Cassandra CLI.

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@unknown] create keyspace 20110914;
afcddc30-e0a3-11e0-0000-242d50cf1ff5
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] use 20110914;
Authenticated to keyspace: 20110914
[default@20110914] use '20110914';
Authenticated to keyspace: 20110914
[default@20110914]
{noformat};;;","16/Sep/11 21:11;jbellis;+1;;;","16/Sep/11 22:22;hudson;Integrated in Cassandra-0.8 #333 (See [https://builds.apache.org/job/Cassandra-0.8/333/])
    Allow using quotes in ""USE <keyspace>;"" CLI command
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3208

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1171792
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log message at INFO when a global or keyspace level repair operation completes,CASSANDRA-3207,12523081,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,bcoverston,bcoverston,14/Sep/11 18:05,16/Apr/19 09:32,14/Jul/23 05:52,19/Sep/11 12:46,0.8.7,,,,,,0,logging,repair,,"If JMX times out it's difficult to tell when repair completes.Right now we log at DEBUG for each column family but we need a way to tell when the repair operation completes as a whole.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/11 15:30;slebresne;3207.patch;https://issues.apache.org/jira/secure/attachment/12494805/3207.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4020,,,Mon Sep 19 13:20:03 UTC 2011,,,,,,,,,,"0|i0gh3r:",94201,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Sep/11 15:30;slebresne;Attached patch adds two new log messages:
  * at the completion of a repair session. This part is already in 1.0.0 but not in 0.8.
  * at the beginning and total completion of a repair ""command"". A command being basically a 'nodetool repair' call, that usually consist of multiple session (one by token range).;;;","16/Sep/11 19:08;jbellis;+1;;;","19/Sep/11 12:46;slebresne;Committed, thanks;;;","19/Sep/11 13:20;hudson;Integrated in Cassandra-0.8 #335 (See [https://builds.apache.org/job/Cassandra-0.8/335/])
    Log message at INFO when a global or keyspace level repair operation completes
patch by slebresne; reviewed by jbellis for CASSANDRA-3207

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1172591
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AntiEntropyService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"increase file descriptor limit in deb, rpm packages",CASSANDRA-3206,12523066,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,jbellis,jbellis,14/Sep/11 16:04,16/Apr/19 09:32,14/Jul/23 05:52,23/Sep/11 21:23,0.8.7,,,Packaging,,,0,,,,"We can use a lot of file descriptors (one per socket, 5? per sstable).  People hit this regularly on the user list and it will get worse with Leveled compaction, which limits sstable size to a relatively low size (currently 5MB).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/11 21:13;thepaul;3206.patch.txt;https://issues.apache.org/jira/secure/attachment/12496328/3206.patch.txt",,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4021,,,Fri Sep 23 22:47:32 UTC 2011,,,,,,,,,,"0|i0gh3b:",94199,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"14/Sep/11 16:06;jbellis;I'd suggest sizing this based on the number of sstables we need to support a ""sane"" amount of storage.  1TB?  2?;;;","14/Sep/11 16:32;jbellis;bq. 5? per sstable

This is incorrect -- when using mmap'd I/O, we don't consume FD-per-sstable (we use one temporarily when setting up the mmap, then release it).  And even in buffered I/O mode we use one FD per sstable being read, per reading thread.  (Which will be much less than one FD per sstable in most cases, although range scans under LevelDB do not yet do a very good job of cutting back the number of SSTables consulted.)

So it sounds like we mostly need to make sure we have it high enough that we tolerate high numbers of unpooled connections (common in PHP environments, I'm told).;;;","14/Sep/11 16:42;jbellis;Jeremy pointed out that we can also use a large number of FDs during compaction: either major compaction for tiered strategy, or a L0 compaction under Leveled, can open an effectively arbitrary number of sstables if compaction is behind.;;;","14/Sep/11 16:43;jbellis;So...  64K?;;;","14/Sep/11 21:19;scode;My two cents: Just go wild. I really don't see the need to be conservative. On any modern system that you run Cassandra on, the resources consumed by file descriptors is going to be irrelevant and I don't see when you'd ever actually want Cassandra to hit the limit, unless it's *completely* run-away and buggy in which case the limit need not be low. Better a very high number so people don't run into it, than try to shave off.

64k seems reasonable, I'd be fine with 250k ;)
;;;","14/Sep/11 22:49;jbellis;SGTM.;;;","15/Sep/11 02:59;thepaul;I concur. I'll go with 100k unless someone has a good argument for having it be higher.;;;","23/Sep/11 21:23;brandon.williams;Committed.;;;","23/Sep/11 22:47;hudson;Integrated in Cassandra-0.8 #339 (See [https://builds.apache.org/job/Cassandra-0.8/339/])
    Increase FD limit to 100k in packaging.
Patch by Paul Cannon, reviewed by brandonwilliams for CASSANDRA-3206

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1175027
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/debian/cassandra.conf
* /cassandra/branches/cassandra-0.8/debian/init
* /cassandra/branches/cassandra-0.8/redhat/cassandra.conf
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamily.cloneMeShallow doesn't respect the insertionOrdered flag (for ArrayBackedSortedColumns),CASSANDRA-3205,12523056,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,14/Sep/11 14:18,16/Apr/19 09:32,14/Jul/23 05:52,14/Sep/11 14:32,1.0.0,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/11 14:21;slebresne;3205.patch;https://issues.apache.org/jira/secure/attachment/12494447/3205.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4022,,,Wed Sep 14 14:32:31 UTC 2011,,,,,,,,,,"0|i0gh2v:",94197,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"14/Sep/11 14:24;jbellis;+1;;;","14/Sep/11 14:32;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress cannot set the compaction strategy,CASSANDRA-3204,12522993,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,brandon.williams,brandon.williams,14/Sep/11 04:24,16/Apr/19 09:32,14/Jul/23 05:52,14/Sep/11 16:02,1.0.0,,,Legacy/Tools,,,0,,,,"stress can't set the compaction strategy, so testing leveldb-style compaction is more difficult than it should be, especially with lots of cluster setup/teardown.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/11 12:22;xedin;CASSANDRA-3204.patch;https://issues.apache.org/jira/secure/attachment/12494435/CASSANDRA-3204.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4023,,,Wed Sep 14 16:02:36 UTC 2011,,,,,,,,,,"0|i0gh2f:",94195,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"14/Sep/11 12:22;xedin;added -Z (--compaction-strategy=<strategy>) option.

rebased with latest cassandra-1.0.0 (last commit 9867a49581b88f2b53efe925290de81b6afc06f0);;;","14/Sep/11 15:50;brandon.williams;+1;;;","14/Sep/11 16:02;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Odd flush behavior,CASSANDRA-3203,12522964,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,brandon.williams,brandon.williams,13/Sep/11 21:49,16/Apr/19 09:32,14/Jul/23 05:52,14/Sep/11 23:09,1.0.0,,,,,,0,,,,"Given the same workload against 0.8, trunk is creating more than twice the amount of sstables.  Even though a uniform stress workload is being generated, flush size degrades quickly:

{noformat}
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:22,878 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@2058235391(7741
035/110172631 serialized/live bytes, 151785 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:24,888 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1520390052(3887
220/72403158 serialized/live bytes, 76220 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:26,890 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1868496516(4097
085/76255481 serialized/live bytes, 80335 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:28,893 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@498232521(43513
20/80922269 serialized/live bytes, 85320 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:29,895 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1592308290(2310
810/44514839 serialized/live bytes, 45310 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:30,897 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@775439677(22684
80/64984390 serialized/live bytes, 44480 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:31,899 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@928217914(26741
85/76231422 serialized/live bytes, 52435 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:32,901 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@158103119(27511
95/77317732 serialized/live bytes, 53945 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:33,903 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@2035169258(3132
420/88934701 serialized/live bytes, 61420 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:34,905 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1097314626(2979
675/83651699 serialized/live bytes, 58425 ops)
{noformat}

The serialized to live size ratio appears completely out of whack.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/11 04:32;jbellis;3203-prelim.txt;https://issues.apache.org/jira/secure/attachment/12494378/3203-prelim.txt","14/Sep/11 20:34;jbellis;3203-v2.txt;https://issues.apache.org/jira/secure/attachment/12494507/3203-v2.txt","15/Sep/11 01:21;brandon.williams;3203.png;https://issues.apache.org/jira/secure/attachment/12494549/3203.png",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4024,,,Fri Sep 16 05:09:20 UTC 2011,,,,,,,,,,"0|i0gh1z:",94193,,brandon.williams,,brandon.williams,Critical,,,,,,,,,,,,,,,,,"14/Sep/11 00:56;brandon.williams;Here is what I've noticed:

Things proceed normally for a while:
{noformat}
 INFO 00:50:16,801 Enqueuing flush of Memtable-Standard1@304913075(36443580/45554475 serialized/live bytes, 714580 ops)
{noformat}

Eventually, something bad happens:
{noformat}
 INFO 00:51:00,001 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='Counter1') (estimated 0 bytes)
 INFO 00:51:00,002 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='Super1') (estimated 0 bytes)
 INFO 00:51:00,002 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='SuperCounter1') (estimated 0 bytes)
 INFO 00:51:00,002 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='Standard1') (estimated 88291581 bytes)
 INFO 00:51:00,004 Enqueuing flush of Memtable-Standard1@741697653(2646900/88291581 serialized/live bytes, 51900 ops)
 INFO 00:51:00,004 flushing high-traffic column family ColumnFamilyStore(table='system', columnFamily='NodeIdInfo') (estimated 0 bytes)
 INFO 00:51:00,004 flushing high-traffic column family ColumnFamilyStore(table='system', columnFamily='IndexInfo') (estimated 0 bytes)
 INFO 00:51:00,004 flushing high-traffic column family ColumnFamilyStore(table='system', columnFamily='LocationInfo') (estimated 87 bytes)
 INFO 00:51:00,005 Enqueuing flush of Memtable-LocationInfo@1706933590(70/87 serialized/live bytes, 2 ops)
 INFO 00:51:00,005 flushing high-traffic column family ColumnFamilyStore(table='system', columnFamily='Migrations') (estimated 0 bytes)
 INFO 00:51:00,006 flushing high-traffic column family ColumnFamilyStore(table='system', columnFamily='HintsColumnFamily') (estimated 0 bytes)
 INFO 00:51:00,006 flushing high-traffic column family ColumnFamilyStore(table='system', columnFamily='Schema') (estimated 0 bytes)
 INFO 00:51:00,006 estimated 0 bytes used by all memtables pre-flush
 INFO 00:51:00,008 flushing ColumnFamilyStore(table='Keyspace1', columnFamily='Standard1') to free up 459320 bytes
 INFO 00:51:00,009 Enqueuing flush of Memtable-Standard1@456936648(14280/476332 serialized/live bytes, 280 ops)
 INFO 00:51:00,010 flushing ColumnFamilyStore(table='system', columnFamily='Schema') to free up 0 bytes
 INFO 00:51:00,010 flushing ColumnFamilyStore(table='system', columnFamily='HintsColumnFamily') to free up 0 bytes
 INFO 00:51:00,011 flushing ColumnFamilyStore(table='system', columnFamily='Migrations') to free up 0 bytes
 INFO 00:51:00,011 flushing ColumnFamilyStore(table='system', columnFamily='LocationInfo') to free up 0 bytes
 INFO 00:51:00,012 flushing ColumnFamilyStore(table='system', columnFamily='IndexInfo') to free up 0 bytes
 INFO 00:51:00,013 flushing ColumnFamilyStore(table='system', columnFamily='NodeIdInfo') to free up 0 bytes
 INFO 00:51:00,013 flushing ColumnFamilyStore(table='Keyspace1', columnFamily='SuperCounter1') to free up 0 bytes
 INFO 00:51:00,014 flushing ColumnFamilyStore(table='Keyspace1', columnFamily='Super1') to free up 0 bytes
 INFO 00:51:00,014 flushing ColumnFamilyStore(table='Keyspace1', columnFamily='Counter1') to free up 0 bytes
 INFO 00:51:01,016 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='Counter1') (estimated 0 bytes)
 INFO 00:51:01,016 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='Super1') (estimated 0 bytes)
 INFO 00:51:01,016 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='SuperCounter1') (estimated 0 bytes)
 INFO 00:51:01,016 flushing high-traffic column family ColumnFamilyStore(table='Keyspace1', columnFamily='Standard1') (estimated 83808954 bytes)
 INFO 00:51:01,017 Enqueuing flush of Memtable-Standard1@911886300(2512770/83817460 serialized/live bytes, 49270 ops)
{noformat}

After this point, it stays bad:

{noformat}
 INFO 00:51:01,879 Enqueuing flush of Memtable-Standard1@937412989(2670870/96531021 serialized/live bytes, 52370 ops)
 INFO 00:51:02,029 Enqueuing flush of Memtable-Standard1@282282499(2668065/88997573 serialized/live bytes, 52315 ops)
{noformat};;;","14/Sep/11 04:32;jbellis;patch that fixes two problems:

- removes double-counting of ByteBuffer overhead in jamm 0.2.4
- removes 25% fudge factor that is no longer useful with Slab allocation

That still leaves us with at least one other problem, though.;;;","14/Sep/11 05:19;brandon.williams;Something strange is going on here, as this patch actually exacerbated the problem slightly.;;;","14/Sep/11 20:30;jbellis;v2 attached:

- fixed-better version of jamm 0.2.5 (now just includes buffer.remaining() + shallow overhead)
- pre-adds the CFMetadata object to the ""seen"" set for MemoryMeter so we don't re-count it for each row [this is the big one]

I'm getting liveRatio of 4-6 now which makes me wonder if we need to add a higher fudge factor to make it not OOM again. :);;;","14/Sep/11 21:17;brandon.williams;bq. I'm getting liveRatio of 4-6 now which makes me wonder if we need to add a higher fudge factor to make it not OOM again. 

I couldn't OOM it, and I get much more consistent flush behavior now, even better than before CASSANDRA-1610 which inadvertently caused the re-counting.

+1;;;","14/Sep/11 23:09;jbellis;committed;;;","15/Sep/11 01:21;brandon.williams;Here's what jconsole looks like with with a 1G heap receiving 100M inserts from stress.  Analysis of the heap indicates the memory decrease is due to bloom filters and index sampling, so I think we're safe without a fudge factor, especially since the fudge factor predated the SlabAllocator.  The points where the heap did hit ~75%, the GCI pressure valve did a good job in combination with CMS of dropping the usage back down before there was any danger.;;;","16/Sep/11 05:09;jbellis;If MeteredFlusher + getLiveSize were really doing their job right though, the pressure valve wouldn't be getting involved.  (However, I'm not sure it's entirely fair to tell it ""you can have 1/3 of the heap"" and then fill up more than 2/3 w/ BF + index.)

I think I'd like to hold off on further tweaking until after 1.0; the current fudge factor means we're erring on the side of extra safety, and I'm okay with that (especially since, as you noted, leveled compaction isn't very sensitive to initial flush size).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFMetata.getMergeShardChance returns readRepairChance,CASSANDRA-3202,12522911,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,slebresne,slebresne,13/Sep/11 15:39,16/Apr/19 09:32,14/Jul/23 05:52,13/Sep/11 17:00,0.8.6,,,,,,0,counters,,,"The summary says it all. Note that CASSANDRA-3178 will hopefully make that option useless. But in the meantime, this breaks the tests in 1.0.0 since read repair chance was set to 0.1 by default (while the test assumes merge_shard_chance is 1), so let's fix that quickly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/11 15:40;slebresne;3202.patch;https://issues.apache.org/jira/secure/attachment/12494250/3202.patch",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4025,,,Tue Sep 13 17:22:08 UTC 2011,,,,,,,,,,"0|i0gh1j:",94191,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"13/Sep/11 16:16;jbellis;+1;;;","13/Sep/11 17:00;slebresne;Committed, thanks;;;","13/Sep/11 17:22;hudson;Integrated in Cassandra-0.8 #326 (See [https://builds.apache.org/job/Cassandra-0.8/326/])
    Fix CFMetata.getMergeShardChance returning readRepairChance
patch by slebresne; reviewed by jbellis for CASSANDRA-3202

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1170236
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
debian packaging installation problem when installing for the first time,CASSANDRA-3198,12522884,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jsevellec,jsevellec,jsevellec,13/Sep/11 11:59,16/Apr/19 09:32,14/Jul/23 05:52,03/Oct/11 21:53,0.8.7,,,Packaging,,,0,,,,"when installing cassandra through the debian packaging for the first time, there is permission problem when starting Cassandra.

Normally, the postinst script change owner of /var/log/cassandra and /var/lib/cassandra from root to cassandra user.

there is a problem with the test which verify if threre is a need to change the owner of these directory or not.

On a new install, the $2 parameter is not set and the the test is false and the owner is not changed.

(simply, i think replace ""&&"" with ""||"" might work)
",,shyamal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/11 17:15;slebresne;3198.patch;https://issues.apache.org/jira/secure/attachment/12496899/3198.patch","13/Sep/11 23:35;shyamal;debian-postinst-fixperms.patch;https://issues.apache.org/jira/secure/attachment/12494343/debian-postinst-fixperms.patch","13/Sep/11 14:15;jsevellec;trunk-3198-v1.patch;https://issues.apache.org/jira/secure/attachment/12494233/trunk-3198-v1.patch",,,,,,,,,,,,3.0,jsevellec,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20971,,,Mon Oct 03 22:57:15 UTC 2011,,,,,,,,,,"0|i0ggzz:",94184,,urandom,,urandom,Normal,,,,,,,,,,,,,,,,,"13/Sep/11 14:15;jsevellec;here is the patch;;;","13/Sep/11 23:35;shyamal;I believe this is the correct patch. It sets permissions on first install, or failing that, when upgrading from earlier than 0.6.4-2 per CASSANDRA-1004;;;","14/Sep/11 13:46;slebresne;That second patch looks good, but maybe we should change that 0.6.4-2 to 0.8.6. Eric, since you initially added that test, an opinion ?;;;","14/Sep/11 15:43;urandom;The comparison to 0.6.4-2 can be safely removed now, so it should look something like:

{noformat}
if [ -z ""$2"" ]; then
    chown -R cassandra: /var/lib/cassandra
    chown -R cassandra: /var/log/cassandra
fi
{noformat};;;","14/Sep/11 16:02;slebresne;bq. The comparison to 0.6.4-2 can be safely removed now

Hum, the goal of this ticket is that anyone that have started using the deb package after 0.6.4-2 have /var/{lib,log}/cassandra owned by root.root, which was not intended. So we can decide that we want to leave all those people with those ""wrong"" permissions, but it's not safer to remove the comparison now that it was when the patch was initially committed.;;;","14/Sep/11 19:02;urandom;{quote}
Hum, the goal of this ticket is that anyone that have started using the deb package after 0.6.4-2 have /var/{lib,log}/cassandra owned by root.root, which was not intended. So we can decide that we want to leave all those people with those ""wrong"" permissions, but it's not safer to remove the comparison now that it was when the patch was initially committed.
{quote}

I think that statement was always bugged.  What it _should_ do is to ensure that /var/{lib,log}/cassandra are owned by user cassandra on new installs only, not upgrades (allowing the user to override that ownership).

At this point, I'm not sure what the comparison to 0.6.4-2 was supposed to accomplish, I assume it was a (misguided )attempt at applying this policy to versions greater than or equal to 0.6.4-2 (we didn't always create the cassandra user and chown in postinst).  Either way, the current packaging will never be used to upgrade from an 0.6.x release directly, so there is no danger in removing it.

So, looking at that (untested )snippet again:

{noformat}
if [ -z ""$2"" ]; then
    chown -R cassandra: /var/lib/cassandra
    chown -R cassandra: /var/log/cassandra
fi
{noformat}

would only apply ownership changes if argument #2 is empty, that is to say, there is no previously configured version (i.e. not an upgrade).;;;","14/Sep/11 19:07;jsevellec;I agree with that too;;;","15/Sep/11 00:59;shyamal;BTW, I tested the snippet in Eric's comment with the additional pre-0.6.4-2 check (my patch) pretty extensively: install, upgrade to package with new version, remove and reinstall etc. It certainly looks right, and I am sure it works because I tried it, several ways. I did not test the upgrade from 0.6.4-2 since I did not completely understand why it was important, but I left it in once I traced it down to CASSANDRA-1004. As I suspected  there's people out there that know better :-)

(BTW, in testing I found one minor irritation that /var/log/cassandra/output.log gets created owned by root after installation. I suspect this is because it is created before jsvc switches the user to after allowing privileged operations to complete...once I'm sure the problem is real I'll file it separately....but it was a good test case for the patch!)

;;;","15/Sep/11 10:48;slebresne;bq. we didn't always create the cassandra user and chown in postinst

Yes, and to make it clear what my point is: while we do create the cassandra user since 0.6.4-2, we *never* chown in the postinst _excepted_ for people that upgraded *all the way* since a pre 0.6.4 release. That is, any current new installation (say of 0.8.5) *does not* chown in the postint and /var/lib/cassandra and /var/log/cassandra (the directories) are owned by root, not cassandra.

Now I'm perfectly fine saying that there is no point in changing things during upgrades and to only fix the chown in the postinst for new installs, but just wanted to make it clear that this has nothing to do with upgrades from some 0.6.x versions.


;;;","28/Sep/11 15:53;shyamal;Any chance this can be fixed for the upcoming 1.0.0 release?;;;","28/Sep/11 17:15;slebresne;Yes, there is probably no reason for not fixing this. Attaching the simple patch that only chown the directories on new installs.

I still don't really understand how previous install have worked so far with /var/lib/cassandra being own by root but the cassandra process being run as the cassandra user, but it costs nothing to be conservative: let's fix it for new install and do nothing for upgrade for now.

We can always keep debating on the best way to ""fix"" old installs, but let's have clean new install right now. ;;;","28/Sep/11 21:43;shyamal;Agree completely, Thanks!;;;","03/Oct/11 21:53;urandom;committed;;;","03/Oct/11 22:57;hudson;Integrated in Cassandra-0.8 #358 (See [https://builds.apache.org/job/Cassandra-0.8/358/])
    set ownership on new installs

Patch by Sylvain Lebresne; reviewed by eevans for CASSANDRA-3198

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1178592
Files : 
* /cassandra/branches/cassandra-0.8/debian/cassandra.postinst
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cassandra-CLI does not allow ""Config"" as column family name",CASSANDRA-3195,12522876,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,kalmatar,kalmatar,13/Sep/11 10:13,16/Apr/19 09:32,14/Jul/23 05:52,14/Sep/11 12:06,0.8.6,,,Legacy/Tools,,,0,,,,"""create column family Config"" does not work, ""create column family Configg"" does.

I suppose the intent is that column families can be named freely, that they have a namespace completely of their own, and separate from, say, Cassandra-CLI commands.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/11 11:26;xedin;CASSANDRA-3195.patch;https://issues.apache.org/jira/secure/attachment/12494429/CASSANDRA-3195.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4027,,,Wed Sep 14 15:15:48 UTC 2011,,,,,,,,,,"0|i0ggyn:",94178,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"13/Sep/11 12:43;jbellis;Did you try quoting the desired name?;;;","13/Sep/11 13:01;kalmatar;create column family 'Config';

gives ""Invalid column family name: 'Config'"",

create column family ""Config"";

gives ""Syntax error at position 21: unexpected """""" for `create column family ""Config"";`.""

or did I miss another way of quoting?

;;;","14/Sep/11 11:26;xedin;CLI supports only single-quotes. Patch to support quoting of the ColumnFamily name is attached (`update column family` statement already supports that) and test for that.;;;","14/Sep/11 11:56;jbellis;+1;;;","14/Sep/11 12:06;xedin;Committed.;;;","14/Sep/11 15:15;hudson;Integrated in Cassandra-0.8 #327 (See [https://builds.apache.org/job/Cassandra-0.8/327/])
    Allow quoting of the ColumnFamily name in CLI `create column family` statement
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3195

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1170555
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/cli/CliTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repair streaming forwarding loop,CASSANDRA-3194,12522858,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,awinter,awinter,13/Sep/11 08:02,16/Apr/19 09:32,14/Jul/23 05:52,13/Sep/11 17:37,1.0.0,,,,,,0,,,,"I am able to reproduce what appears to be a streaming forwarding loop when running repairs.  This affect only nodes using broadcast_address (ec2 external ip) & listen_address of 0.0.0.0. (Configuration is using property file snitch in a multi DC NTS where some DC's are EC2 and others are not).  The hosts in the other dc's not using broadcast_address do not experience this symptom.

on ec2 host dc1host1:
INFO [AntiEntropyStage:1] 2011-09-13 06:34:01,673 StreamingRepairTask.java (line 211) [streaming task #ce793c30-ddd1-11e0-0000-071a4b76fefb] Received task from /0.0.0.0 to stream 12259 ranges to /external.ec2.ip.dc1host3
 INFO [AntiEntropyStage:1] 2011-09-13 06:34:01,673 StreamingRepairTask.java (line 136) [streaming task #ce793c30-ddd1-11e0-0000-071a4b76fefb] Forwarding streaming repair of 12259 ranges to /external.ec2.ip.of.dc1host1 (to be streamed with /external.ip.of.host3)

The above appears to trigger another streaming task and results in saturating the network interfaces dc1host1.  The above log entries are repeated until cassandra is killed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/11 12:53;slebresne;3194.patch;https://issues.apache.org/jira/secure/attachment/12494224/3194.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4028,,,Tue Sep 13 17:37:28 UTC 2011,,,,,,,,,,"0|i0ggy7:",94176,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"13/Sep/11 12:53;slebresne;Oups, forgot to switch to getBroadcastAddress  in StreamingRepairTask. Patch attached.;;;","13/Sep/11 17:17;brandon.williams;+1;;;","13/Sep/11 17:37;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossip teardown causes test failures,CASSANDRA-3193,12522843,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,13/Sep/11 04:28,16/Apr/19 09:32,14/Jul/23 05:52,13/Sep/11 21:38,1.0.0,,,,,,0,,,,Just look at any recent jenkins or buildbot attempt for the semi-nonsensical NBHM.remove NPE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4029,,,Tue Sep 13 21:38:51 UTC 2011,,,,,,,,,,"0|i0ggxr:",94174,,,,,Normal,,,,,,,,,,,,,,,,,"13/Sep/11 21:38;jbellis;fixed in r1170360;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in RowRepairResolver,CASSANDRA-3192,12522833,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,cdaw,cdaw,13/Sep/11 01:07,16/Apr/19 09:32,14/Jul/23 05:52,13/Sep/11 12:57,1.0.0,,,,,,0,,,,"On a 3 node brisk cluster (running against C* 1.0 branch), I was running the java stress tool and the terasort concurrently in two sessions.  Eventually both jobs failed with TimedOutException.
  
From this point forward most additional activity will fail with a TimedOutException. 
* Java Stress Tool - 5 rows / 10 columns - Operation [0] retried 10 times - error inserting key 0 ((TimedOutException))
* Hive - show tables: FAILED: Error in metadata: com.datastax.bdp.hadoop.hive.metastore.CassandraHiveMetaStoreException: There was a problem with the Cassandra Hive MetaStore: Could not connect to Cassandra. Reason: Error connecting to node localhost

However, the Cassandra CLI appears to be happy
* Cassandra CLI: you can successfully insert and read using consistencylevel as ONE or ALL

The seed node has the following error repeatedly occurring in the logs.  The other two nodes have no errors.

{code}
ERROR [ReadRepairStage:15] 2011-09-13 00:44:25,971 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadRepairStage:15,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:82)
	at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{code}",,awinter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/11 03:48;jbellis;3192.txt;https://issues.apache.org/jira/secure/attachment/12494171/3192.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4030,,,Tue Sep 13 12:57:57 UTC 2011,,,,,,,,,,"0|i0ggxb:",94172,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"13/Sep/11 01:45;cdaw;I noticed that after starting up the server, this error message was in the log file of the second node (non-seed) after startup.

*node2*
{code}
ERROR 01:30:05,522 Fatal exception in thread Thread[HintedHandoff:1,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:282)
	at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
	at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:333)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

;;;","13/Sep/11 03:42;brandon.williams;Solved in CASSANDRA-3156;;;","13/Sep/11 03:48;jbellis;patch to fix regression caused by CASSANDRA-2643;;;","13/Sep/11 03:49;jbellis;(There's actually still a bug where we don't handle null CFs correctly in the resolve.  See patch.);;;","13/Sep/11 12:16;slebresne;+1;;;","13/Sep/11 12:57;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unable to start single node cluster when listen_address is not localhost,CASSANDRA-3191,12522827,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,tjake,tjake,13/Sep/11 00:10,16/Apr/19 09:32,14/Jul/23 05:52,13/Sep/11 17:30,1.0.0,,,,,,0,,,,"Due to bootstrap default == true

{code}
INFO 20:07:51,839 Joining: waiting for ring and schema information
 INFO 20:08:21,839 Joining: getting bootstrap token
ERROR 20:08:21,843 Exception encountered during startup.
java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap
	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:168)
	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:150)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapToken(BootStrapper.java:145)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:528)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:450)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:372)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:213)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:335)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Exception encountered during startup.
{code}",,satishbabu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/11 17:07;jbellis;3191.txt;https://issues.apache.org/jira/secure/attachment/12494271/3191.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4031,,,Tue Sep 13 17:30:08 UTC 2011,,,,,,,,,,"0|i0ggwv:",94170,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"13/Sep/11 00:13;brandon.williams;You can workaround this by explicitly disabling it in the config:

bq. auto_bootstrap: false

But imo, when a token is specified we should automatically assume there is no need to bootstrap.;;;","13/Sep/11 00:27;tjake;The fact the option is missing from the config will make it hard for users to fix without pulling some hair out.;;;","13/Sep/11 03:50;jbellis;Is your single node not a seed?;;;","13/Sep/11 13:13;tjake;I left the listen address blank.  and the seed is 127.0.0.1;;;","13/Sep/11 16:27;slebresne;I could be wrong, but I think that was is happening here is that leaving it blank will resolve to the machine address (i.e, not the loopback ip), and thus the node doesn't recognize itself as a seed. In a way, it's a configuration error, listen_address should be 127.0.0.1. Not sure we can really improve that error message, but maybe we can improve the comments in the yaml.;;;","13/Sep/11 17:07;jbellis;Patch to explain the problem and how to solve it in the exception message.
 ;;;","13/Sep/11 17:14;slebresne;+1;;;","13/Sep/11 17:30;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Return both listen_address and rpc_address through describe_ring,CASSANDRA-3187,12522800,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,nickmbailey,nickmbailey,nickmbailey,12/Sep/11 20:22,16/Apr/19 09:32,14/Jul/23 05:52,13/Sep/11 18:30,0.8.6,,,,,,0,,,,"CASSANDRA-1777 changed describe_ring to return the rpc address associated with a node instead of the listen_address. This allows using different interfaces for listen_address and rpc_address, but breaks when rpc_address is set to something like 0.0.0.0.

I think the describe_ring should just return both interfaces. We can add an optional field to the TokenRange struct that is 'listen_endpoints' or something similar and populate that with the listen addresses of nodes.",,stuhood,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/11 22:45;nickmbailey;0001-0.8-Return-both-rpc-address-and-listen-address-with-the-.patch;https://issues.apache.org/jira/secure/attachment/12494143/0001-0.8-Return-both-rpc-address-and-listen-address-with-the-.patch","13/Sep/11 03:15;nickmbailey;0001-0.8-Return-both-rpc-address-and-listen-address-with-v2.patch;https://issues.apache.org/jira/secure/attachment/12494167/0001-0.8-Return-both-rpc-address-and-listen-address-with-v2.patch","12/Sep/11 22:45;nickmbailey;0001-1.0-Return-both-rpc-address-and-listen-address-with-the-.patch;https://issues.apache.org/jira/secure/attachment/12494144/0001-1.0-Return-both-rpc-address-and-listen-address-with-the-.patch","13/Sep/11 03:15;nickmbailey;0001-1.0-Return-both-rpc-address-and-listen-address-with-v2.patch;https://issues.apache.org/jira/secure/attachment/12494168/0001-1.0-Return-both-rpc-address-and-listen-address-with-v2.patch",,,,,,,,,,,4.0,nickmbailey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,656,,,Wed Sep 14 15:15:48 UTC 2011,,,,,,,,,,"0|i0ggv3:",94162,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"12/Sep/11 21:36;nickmbailey;Applies to 1.0.0 branch and 0.8 branch;;;","12/Sep/11 22:45;nickmbailey;Better version of the patch, separate patches for 0.8 and 1.0.;;;","12/Sep/11 22:47;jbellis;The question in my mind is, do we want to ""unbreak"" 0.8.4/0.8.5, and make ""endpoints"" the same as it was in earlier versions (i.e. listen_address) and add a new field for rpc_address?;;;","12/Sep/11 22:49;brandon.williams;bq. The question in my mind is, do we want to ""unbreak"" 0.8.4/0.8.5, and make ""endpoints"" the same as it was in earlier versions (i.e. listen_address) and add a new field for rpc_address?

If I could do it all over again, I wouldn't have put CASSANDRA-1777 in 0.8, but I think at this point changing it back in a minor is bad and will end up causing a lot of confusion.;;;","12/Sep/11 22:53;jbellis;Does make it simpler to write a client that works against both latest-0.7 and latest-0.8, though (for 0.8 >= 0.8.6).

Especially if we push 0.8.6 out quickly.  For whatever reason it seems like many 0.8 deployments haven't moved off of 0.8.1 yet.;;;","12/Sep/11 22:56;brandon.williams;bq. Especially if we push 0.8.6 out quickly. For whatever reason it seems like many 0.8 deployments haven't moved off of 0.8.1 yet.

True, and now that I think about I've heard nothing but complaints about CASSANDRA-1777, and no one clamored too hard for it to begin with.;;;","13/Sep/11 03:15;nickmbailey;Rebased to make the required parameter listen_address again and add an optional rpc_address.;;;","13/Sep/11 18:30;brandon.williams;Committed;;;","14/Sep/11 15:15;hudson;Integrated in Cassandra-0.8 #327 (See [https://builds.apache.org/job/Cassandra-0.8/327/])
    Return both listen_address and rpc_address through describe_ring.
Patch by Nick Bailey, reviewed by brandonwilliams for CASSANDRA-3187

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1170284
Files : 
* /cassandra/branches/cassandra-0.8/NEWS.txt
* /cassandra/branches/cassandra-0.8/interface/cassandra.thrift
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/AuthenticationException.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/AuthenticationRequest.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/AuthorizationException.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/CfDef.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Column.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/ColumnDef.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/ColumnOrSuperColumn.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/ColumnParent.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/ColumnPath.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/CounterColumn.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/CounterSuperColumn.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/CqlResult.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/CqlRow.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Deletion.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/IndexClause.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/IndexExpression.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/InvalidRequestException.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/KeyCount.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/KeyRange.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/KeySlice.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/KsDef.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Mutation.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/NotFoundException.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SchemaDisagreementException.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SlicePredicate.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SliceRange.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SuperColumn.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/TimedOutException.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/TokenRange.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/UnavailableException.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool should not NPE when rack/dc info is not yet available,CASSANDRA-3186,12522798,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,12/Sep/11 20:15,16/Apr/19 09:32,14/Jul/23 05:52,16/Nov/11 21:46,0.8.8,,,,,,1,lhf,,,"As the title says.  What happens is the persisted ring is loaded, but if those nodes are down and you're using a snitch like ec2 that gets rack/dc info from gossip, nodetool NPEs instead of showing that the node is down.",,sdolgy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3175,CASSANDRA-3114,CASSANDRA-3114,,,,,,"16/Nov/11 21:38;alexaraujo;cassandra-0.8-3186.txt;https://issues.apache.org/jira/secure/attachment/12503955/cassandra-0.8-3186.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,1857,,,Thu Nov 17 00:01:54 UTC 2011,,,,,,,,,,"0|i0ggun:",94160,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"12/Sep/11 20:19;jbellis;Is this related to CASSANDRA-3175?;;;","12/Sep/11 20:25;brandon.williams;Yes.;;;","16/Nov/11 21:38;alexaraujo;Merges 1.0's Ec2Snitch which does the right thing;;;","16/Nov/11 21:46;brandon.williams;Committed.  Note that I think this is the preferable way to solve this, since it allows you to change snitches for CASSANDRA-3114, though your consistency guarantees will likely be violated and require repair afterwards.;;;","17/Nov/11 00:01;hudson;Integrated in Cassandra-0.8 #399 (See [https://builds.apache.org/job/Cassandra-0.8/399/])
    Set default rack/dc in ec2snitch to avoid NPEs.
Patch by Alex Araujo, reviewed by brandonwilliams for CASSANDRA-3186.

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1202892
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/Ec2Snitch.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the versions that are referenced in the generated POMs so that they match the versions in svn's lib folder,CASSANDRA-3184,12522777,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stephenc,stephenc,stephenc,12/Sep/11 18:37,16/Apr/19 09:32,14/Jul/23 05:52,13/Sep/11 12:31,1.0.0,,,Packaging,,,0,,,,Update the versions before the release so that the release uses the same dependencies for Maven downloaded dependencies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/11 11:38;stephenc;CASSANDRA-3184.patch;https://issues.apache.org/jira/secure/attachment/12494218/CASSANDRA-3184.patch",,,,,,,,,,,,,,1.0,stephenc,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4033,,,Tue Sep 13 12:31:18 UTC 2011,,,,,,,,,,"0|i0ggtj:",94155,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"12/Sep/11 18:46;stephenc;specifically the dependencies for snappy-java ( http://search.maven.org/#artifactdetails%7Corg.xerial.snappy%7Csnappy-java%7C1.0.3.3%7Cbundle ) and compress-lzf ( http://search.maven.org/#artifactdetails%7Ccom.ning%7Ccompress-lzf%7C0.8.4%7Cbundle ) are missing;;;","13/Sep/11 11:38;stephenc;Patch also fixes the version and scm paths in build.xml so that the generated POMs are correct;;;","13/Sep/11 11:39;stephenc;Submitted patch will fix the poms;;;","13/Sep/11 11:40;stephenc;I've made the patch, up to a committer to take and apply;;;","13/Sep/11 12:31;slebresne;+1, committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove special-cased maximum on sstables-to-compact for leveled strategy,CASSANDRA-3182,12522760,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,12/Sep/11 16:01,16/Apr/19 09:32,14/Jul/23 05:52,13/Sep/11 04:46,1.0.0,,,,,,0,compaction,lcs,,With CASSANDRA-3171 fixed we don't need to be scared of large numbers of compaction candidates anymore.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/11 16:05;jbellis;3182.txt;https://issues.apache.org/jira/secure/attachment/12494052/3182.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4035,,,Tue Sep 13 04:46:48 UTC 2011,,,,,,,,,,"0|i0ggsv:",94152,,bcoverston,,bcoverston,Low,,,,,,,,,,,,,,,,,"12/Sep/11 16:05;jbellis;as described.;;;","13/Sep/11 04:21;bcoverston;+1;;;","13/Sep/11 04:46;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction fails to occur,CASSANDRA-3181,12522759,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,12/Sep/11 15:51,16/Apr/19 09:32,14/Jul/23 05:52,14/Sep/11 01:27,1.0.0,,,,,,0,compaction,,,"Compaction just stops running at some point.  To repro, insert like 20M rows with a 1G heap and you'll get around 1k sstables.  Restarting doesn't help, you have to invoke a major to get anything to happen.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/11 13:14;jbellis;3181-2.txt;https://issues.apache.org/jira/secure/attachment/12494442/3181-2.txt","13/Sep/11 22:03;jbellis;3181.txt;https://issues.apache.org/jira/secure/attachment/12494333/3181.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4036,,,Wed Sep 14 13:32:33 UTC 2011,,,,,,,,,,"0|i0ggsf:",94150,,bcoverston,,bcoverston,Normal,,,,,,,,,,,,,,,,,"12/Sep/11 15:51;brandon.williams;Note that this is with the default compaction strategy.;;;","13/Sep/11 04:30;bcoverston;Tested this a bit and I was able to reproduce the problem.

Basically what I think happened was:

# There were compactions in-flight and scheduled. 
# Something happened to halt the compactions (probably restarted the server)
# https://issues.apache.org/jira/browse/CASSANDRA-2444 got in the way

I propose one of the following: 

* add option in as originally proposed
OR
* put compactions on a timer and ask periodically `does any compaction need to be done`. Also stagger the start of the compactions by some variable amount (5-10 minutes) similar to what we did with Hinted Handoff in 0.7.

 ;;;","13/Sep/11 17:43;slebresne;What is unclear is what did stop the compactions from happening ? Because Brandon said that restarting didn't helped, but I get from that that compaction stopped before the restart. Basically, if the only thing that happened is ""after a restart, if you don't do any inserts, no compaction happens"", then ok, that's not a big deal. But if it is indeed that ""Compaction just stops running at some point"", then there is something that we need to fix.;;;","13/Sep/11 17:50;bcoverston;From his repro I was able to get compactions going again with a simple write/flush.

From Brandon, he's not sure how compactions stopped. There are only two scenarios where I see compactions stopping like this with the tiered compaction strategy:

# the server was restarted
# a compaction failed unexpectedly

The fact that there were no errors in the log makes #2 unlikely. I'm pretty sure that we're just looking at a problem where a server restarts and there is no more activity triggering a flush.;;;","13/Sep/11 17:50;brandon.williams;What started all this is my attempt at running repair.  So I think repair was blocking minors, but then I ran into countless other issues (like CASSANDRA-3179) and was forced to restart.;;;","13/Sep/11 20:09;slebresne;bq. . So I think repair was blocking minors

repair should be on its own executor now, so it shouldn't block minors.

bq. a compaction failed unexpectedly

hum, a compaction failing shouldn't stop other compactions. Otherwise this is worth fixing.

bq. I'm pretty sure that we're just looking at a problem where a server restarts and there is no more activity triggering a flush

If that's the case, then is there really much we want to do ? And even if we want, we should move that to 1.0.1. Just want to make sure this doesn't hide a real, unknown,  problem.;;;","13/Sep/11 20:15;brandon.williams;bq. repair should be on its own executor now, so it shouldn't block minors.

Ok, then maybe I just hit one of the OOM bugs, and compaction had never fully completed.  After restarting I never did any more writes, and we know compaction won't happen at startup.

bq. If that's the case, then is there really much we want to do ? And even if we want, we should move that to 1.0.1. Just want to make sure this doesn't hide a real, unknown, problem.

I think CASSANDRA-2444 was wrong.  It should be an option, and one that is off by default.  Starting a server with 1k sstables and having nothing happen is a bit of a shock, and having no way out of it besides hacks like forcing a flush or a major isn't great.;;;","13/Sep/11 20:16;jbellis;bq. CASSANDRA-2444 got in the way

I'm not sure what the right solution is here.  I buy the premise of 2444 that you don't necessarily want to get hammered by compaction when you're first starting up (warming up caches).  So I don't think ""check for compactions ever N seconds"" is a great policy.  But, I'm not sure ""check every N seconds, starting M minutes after startup"" is great either because it's not something a user will just guess when he's wondering ""why aren't compactions happening yet?""

Any other ideas?;;;","13/Sep/11 22:03;jbellis;Attached patch does a couple things:

- Schedules a compaction submission for each CFS 5 minutes after startup.  My reasoning is, five minutes is (a) enough time for most caches to warm up under load and (b) when it is not, at least it is enough time to reduce the compaction i/o limit.
- removes the permanent check-for-compactions-every-3s task from leveled compaction; I don't like spinning that up for no reason, when we already kick off a check on each flush and end-of-compaction, which should be adequate.  (Every 3s for 1 CFS = every 0.0003s for 10K CFS.)
- makes Leveled getMaximal return a ""normal, leveling"" compaction, if any needs to be done, allowing users of leveldb compaction to kick things off earlier than 5m via ""nodetool compact,"" if desired;;;","13/Sep/11 22:48;bcoverston;Was a little concerned about removing the scheduled compaction from leveldb, but the mechanics are really no different from the tiered compaction in terms of ""it will stop when it's finished"" assuming that nothing goes wrong with the running compactions.

To be (probably overly) pedantic another advantage this has is that you are essentially kicking off only a single compaction where when the server was brought down there were probably multiple compactions in flight.

+1

;;;","14/Sep/11 01:27;jbellis;committed;;;","14/Sep/11 13:14;jbellis;patch to keep shutdown from waiting for the compaction kickoff;;;","14/Sep/11 13:18;slebresne;+1;;;","14/Sep/11 13:32;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JVM segfaults,CASSANDRA-3179,12522755,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,12/Sep/11 15:23,16/Apr/19 09:32,14/Jul/23 05:52,13/Sep/11 21:02,1.0.0,,,,,,0,,,,Both with and without compressed OOPs enabled.  Seems to mostly happen during compaction+reads.  I'll attach some hs_err files shortly.,"java version ""1.6.0_26""
Java(TM) SE Runtime Environment (build 1.6.0_26-b03)
Java HotSpot(TM) 64-Bit Server VM (build 20.1-b02, mixed mode)
",awinter,scode,yangyangyyy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/11 17:46;jbellis;3179-performance-test.txt;https://issues.apache.org/jira/secure/attachment/12494280/3179-performance-test.txt","12/Sep/11 16:58;jbellis;3179-v2.txt;https://issues.apache.org/jira/secure/attachment/12494065/3179-v2.txt","12/Sep/11 16:50;jbellis;3179.txt;https://issues.apache.org/jira/secure/attachment/12494061/3179.txt","12/Sep/11 15:26;brandon.williams;hs_err_pid12074.log;https://issues.apache.org/jira/secure/attachment/12494044/hs_err_pid12074.log","12/Sep/11 15:26;brandon.williams;hs_err_pid28971.log;https://issues.apache.org/jira/secure/attachment/12494045/hs_err_pid28971.log","13/Sep/11 22:07;yangyangyyy;hs_err_pid6461.log;https://issues.apache.org/jira/secure/attachment/12494335/hs_err_pid6461.log","12/Sep/11 15:26;brandon.williams;hs_err_pid7031.log;https://issues.apache.org/jira/secure/attachment/12494046/hs_err_pid7031.log",,,,,,,,7.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4037,,,Wed Sep 14 16:34:15 UTC 2011,,,,,,,,,,"0|i0ggrj:",94146,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"12/Sep/11 15:48;brandon.williams;I am unable to repro without compaction running.;;;","12/Sep/11 16:28;jbellis;The problem is that our whole ""zero copy"" mmap'd read path is broken by ""unmap sstables as soon as they are no longer referenced.""  Specifically, we release references during reads in CollationController or getRangeSlice as soon as we have generated CF objects from the sstables, but we will continue to reference the buffer contents later on when we send the results back to the user or the coordinator (or, as in the 7031 log, just use the results internally).;;;","12/Sep/11 16:48;jbellis;patch to copy buffers read from an mmap'd source.;;;","12/Sep/11 16:55;jbellis;v2 adds missing flip();;;","12/Sep/11 23:24;brandon.williams;+1;;;","13/Sep/11 04:16;jbellis;Also a patch that removes refcounting (and omits the copy above) to check what the performance hit is over pre-CASSANDRA-2521 sstable deletions.

Note that this is NOT intended as a ""real"" patch; it does not actually add back in GC-based sstable deletion.  No sstables will be removed w/o a server restart, but that should be adequate for testing read performance.;;;","13/Sep/11 07:25;yangyangyyy;btw, as I was trying to going through the code to understand the problem, I found it seems that the MMapedSegmentedFile.cleanup() code could be skipped in some sequences of mixed read and compact:

read increases the refcount, then DataTracker.replaceCompactedSSTables()---->....---> SSTableReader.releaseReference() could see a refcount of 2, and not call the dfile.cleanup()


it's not a big problem ( code seems to run fine with the cleanup() lines commented out), but that kind of thwarts the purpose of adding them in ;;;","13/Sep/11 17:34;jbellis;rebased performance-test;;;","13/Sep/11 20:23;brandon.williams;I'm unable to discern any read performance difference between the performance test and v2 patches.;;;","13/Sep/11 21:02;jbellis;committed v2;;;","13/Sep/11 21:05;jbellis;bq. SSTableReader.releaseReference() could see a refcount of 2, and not call the dfile.cleanup()

That means a read thread has a reference still, so cleanup will run when THAT thread releases it.;;;","13/Sep/11 22:06;yangyangyyy;thanks.



unfortunately I got the SEGV again, after updating to the latest version. I tried to confirm so ran it again, so far seen 2 times.




 INFO 14:49:57,806 Compacting Minor: [SSTableReader(path='/usr/scratch/yyang/cass/lib/cassandra/data/testBudget_items/IpFilter-h
-76-Data.db'), SSTableReader(path='/usr/scratch/yyang/cass/lib/cassandra/data/testBudget_items/IpFilter-h-77-Data.db'), SSTableR
eader(path='/usr/scratch/yyang/cass/lib/cassandra/data/testBudget_items/IpFilter-h-79-Data.db'), SSTableReader(path='/usr/scratc
h/yyang/cass/lib/cassandra/data/testBudget_items/IpFilter-h-78-Data.db')]
9135.963: [GC 9135.963: [ParNew: 2550601K->133528K(2764800K), 1.2821340 secs] 5163440K->2756799K(5836800K) icms_dc=0 , 1.2824040
 secs] [Times: user=1.06 sys=0.04, real=1.28 secs]
 INFO 14:50:07,017 Completed flushing /usr/scratch/yyang/cass/lib/cassandra/data/testBudget_items/measuredSession-h-88-Data.db (
21435483 bytes)
 INFO 14:50:07,018 Writing Memtable-session_limit_filter@839347940(21205810/281849841 serialized/live bytes, 286565 ops)
 INFO 14:50:15,987 Completed flushing /usr/scratch/yyang/cass/lib/cassandra/data/testBudget_items/session_limit_filter-h-88-Data
.db (23326502 bytes)
 INFO 14:50:15,999 Writing Memtable-ad_impression_session@100750437(21205662/296528055 serialized/live bytes, 286563 ops)
9154.456: [GC 9154.456: [ParNew: 2591128K->125067K(2764800K), 0.2846910 secs] 5214399K->2756607K(5836800K) icms_dc=0 , 0.2849470 secs] [Times: user=0.71 sys=0.01, real=0.29 secs]
 INFO 14:50:26,220 Completed flushing /usr/scratch/yyang/cass/lib/cassandra/data/testBudget_items/ad_impression_session-h-89-Data.db (52154466 bytes)
 INFO 14:50:26,221 Writing Memtable-ad_ip_agent@1095480823(21205662/286042295 serialized/live bytes, 286563 ops)
 INFO 14:50:26,463 Compacted to [/usr/scratch/yyang/cass/lib/cassandra/data/testBudget_items/IpFilter-h-80-Data.db,].  94,782,508 to 83,577,108 (~88% of original) bytes for 6,368 keys at 2.783688MBPS.  Time: 28,633ms.
 INFO 14:50:26,464 CF Total Bytes Compacted: 3,494,202,400
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00002aaaab7b9c88, pid=6461, tid=1253022016
#
# JRE version: 7.0-b147
# Java VM: Java HotSpot(TM) 64-Bit Server VM (21.0-b17 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# J  org.apache.cassandra.io.util.AbstractDataInput.readByte()B
#
# Core dump written. Default location: /usr/scratch/yyang/whisky/core or core.6461



attaching the err file separately;;;","13/Sep/11 22:07;yangyangyyy;failure log after using updated code;;;","13/Sep/11 22:46;brandon.williams;Can you reproduce w/o compressed oops?;;;","13/Sep/11 23:02;yangyangyyy;let me try ...

On Tue, Sep 13, 2011 at 3:47 PM, Brandon Williams (JIRA)
;;;","14/Sep/11 00:34;yangyangyyy;confirmed with the -XX:-UseCompressedOops arg,

has generated 1 SEGV so far


btw, it would be really helpful to add a simple dumb stress test testcase,
some bugs are not easily exposed with the unit tests.;;;","14/Sep/11 00:40;brandon.williams;What process are you following to reproduce?  I can't do it with compaction+reads, but I'm also not using JRE 7.;;;","14/Sep/11 00:52;yangyangyyy;I'm just writing + reading on the cassandra server, and doing this on the side:


while : ;do nodetool flush ; sleep 20;done


(disclaimer: the cassandra server is a modified one, where I have a
thread that takes in client request through a custom AVRO server, and
then call the storageProxy.batch_mutate() and StorageProxy.get()
directly. it should not be materially different from the pure
cassandra server as far as this bug is concerned, since it's basically
just swapping out the thrift server
)

On Tue, Sep 13, 2011 at 5:42 PM, Brandon Williams (JIRA)
;;;","14/Sep/11 01:22;jbellis;You're sure you're on 1.0.0 >= r1170342?;;;","14/Sep/11 01:22;yangyangyyy;very interesting, I switched from mmap mode to ""standard"", and go the following errors on compaction,
I believe they follow the same path as the mmap route, but since it gave an exception here, instead of siliently SEGV, this could provide a useful hint to what caused the SEGV




Caused by: java.nio.channels.ClosedChannelException
        at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:268)
        at java.io.RandomAccessFile.readByte(RandomAccessFile.java:640)
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:356)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:367)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:87)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:82)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:72)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:36)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:79)
        ... 21 more
ERROR 01:19:22,415 Fatal exception in thread Thread[ReadStage:246,5,main]
java.lang.RuntimeException: java.lang.RuntimeException: error reading 1 of 1
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1165)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.RuntimeException: error reading 1 of 1
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:83)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:40)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:107)
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:194)
        at org.apache.cassandra.utils.MergeIterator.<init>(MergeIterator.java:47)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.<init>(MergeIterator.java:142)
        at org.apache.cassandra.utils.MergeIterator.get(MergeIterator.java:66)
        at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:96)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:249)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:61)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1276)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1171)

        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1127)
        at org.apache.cassandra.db.Table.getRow(Table.java:388)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:61)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:694)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1161)
        ... 3 more
Caused by: java.nio.channels.ClosedChannelException
        at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:268)
        at java.io.RandomAccessFile.readByte(RandomAccessFile.java:640)
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:356)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:367)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:87)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:82)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:72)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:36)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:79)
        ... 21 more
 INFO 01:19:23,738 Compacted to [/mnt/cass/lib/cassandra/data/testBudget_items/measuredSession-h-10-Data.db,].  16,840,264 to 16,840,036 (~99% of original) bytes for 22,531 keys at 3.247707MBPS.  Time: 4,945ms.
 INFO 01:19:23,738 CF Total Bytes Compacted: 157,135,410
;;;","14/Sep/11 01:26;yangyangyyy;sorry for the misunderstanding. I applied the v2 patch on
4dbda612f49c97fd5e3f66e7875a20ec9a0dc829   (Aug 26 version)

I could test the latest one if you think that's going to be different.



On Tue, Sep 13, 2011 at 6:22 PM, Jonathan Ellis (JIRA) <jira@apache.org> wrote:
;;;","14/Sep/11 01:30;jbellis;Please test 1.0.0 branch to make sure we are doing the same thing.;;;","14/Sep/11 07:56;yangyangyyy;I know now, it must be due to
https://issues.apache.org/jira/browse/CASSANDRA-3110

let me verify with the latest code


;;;","14/Sep/11 16:34;yangyangyyy;with 1.0.0 HEAD, no more SEGV after a night of stress tests.

thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counter shard merging is not thread safe,CASSANDRA-3178,12522749,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,12/Sep/11 14:58,16/Apr/19 09:32,14/Jul/23 05:52,07/Nov/11 15:38,0.8.8,1.0.3,,,,,1,counters,,,"The first part of the counter shard merging process is done during counter replication. This was done there because it requires that all replica are made aware of the merging (we could only rely on nodetool repair for that but that seems much too fragile, it's better as just a safety net). However this part isn't thread safe as multiple threads can do the merging for the same shard at the same time (which shouldn't really ""corrupt"" the counter value per se, but result in an incorrect context).

Synchronizing that part of the code would be very costly in term of performance, so instance I propose to move the part of the shard merging done during replication to compaction. It's a better place anyway. The only downside is that it means compaction will sometime send mutations to other node as a side effect, which doesn't feel very clean but is probably not a big deal either.",,feestend,ghinkle,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3641,,,,,,,,"03/Nov/11 11:49;slebresne;0001-Move-shard-merging-completely-to-compaction-1.0.patch;https://issues.apache.org/jira/secure/attachment/12502133/0001-Move-shard-merging-completely-to-compaction-1.0.patch","15/Sep/11 09:37;slebresne;0001-Move-shard-merging-completely-to-compaction-v2.patch;https://issues.apache.org/jira/secure/attachment/12494593/0001-Move-shard-merging-completely-to-compaction-v2.patch","12/Sep/11 15:04;slebresne;0001-Move-shard-merging-completely-to-compaction.patch;https://issues.apache.org/jira/secure/attachment/12494039/0001-Move-shard-merging-completely-to-compaction.patch","03/Nov/11 11:49;slebresne;0002-Simplify-improve-shard-merging-code-1.0.patch;https://issues.apache.org/jira/secure/attachment/12502134/0002-Simplify-improve-shard-merging-code-1.0.patch","15/Sep/11 09:37;slebresne;0002-Simplify-improve-shard-merging-code-v2.patch;https://issues.apache.org/jira/secure/attachment/12494594/0002-Simplify-improve-shard-merging-code-v2.patch","12/Sep/11 15:04;slebresne;0002-Simplify-improve-shard-merging-code.patch;https://issues.apache.org/jira/secure/attachment/12494040/0002-Simplify-improve-shard-merging-code.patch",,,,,,,,,6.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,14977,,,Mon Nov 07 16:02:58 UTC 2011,,,,,,,,,,"0|i0ggr3:",94144,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,"12/Sep/11 15:04;slebresne;First patch move the code from counter replication to compaction (as described above). The second patch adapt the code to work correctly with the move to compaction but also simply and improve that code.;;;","13/Sep/11 15:51;slebresne;Note that the removal of flush_after_mins in 1.0 is a problem for this patch. The reason is that we want to remove a shard corresponding to a NodeId for which we know no increment has been made after time t. For that removal to be safe, we must make sure that compaction includes everything that has been issued before time t. For that, current patch check that the compaction has started after time t + 2 * flush_after_mins. I'll update the patch to use the memtables creationTime instead.  ;;;","15/Sep/11 09:37;slebresne;Attaching v2, rebased and that remove the use of flush_after_mins.;;;","28/Oct/11 17:08;jbellis;Yuki, can you review?;;;","02/Nov/11 16:08;yukim;LGTM on 0.8 branch. I think it's safe to apply this on 1.0, but before that I want to make sure it works. Could you modify the patch so that I can test on 1.0?;;;","03/Nov/11 11:49;slebresne;Attaching patches rebased against 1.0;;;","04/Nov/11 09:14;yukim;+!. 1.0 patch also works fine.;;;","07/Nov/11 15:38;slebresne;Committed, thanks;;;","07/Nov/11 16:02;hudson;Integrated in Cassandra-0.8 #394 (See [https://builds.apache.org/job/Cassandra-0.8/394/])
    patch by slebresne; reviewed by yukim for CASSANDRA-3178

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1198725
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/CounterColumn.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/CounterMutation.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/Memtable.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionController.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/LazilyCompactedRow.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/PrecompactedRow.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/context/CounterContext.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/CounterMutationTest.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/context/CounterContextTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disabling hinted handoff counterintuitively continues to log handoff messages,CASSANDRA-3176,12522664,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jeromatron,jeromatron,10/Sep/11 23:20,16/Apr/19 09:32,14/Jul/23 05:52,01/Oct/11 15:17,1.0.0,,,,,,0,HintedHandoff,,,"In order to test a theory, we tried to disable hinted handoff on our cluster.  We updated all of the yaml files and then restarted all the nodes in our cluster.  However we continue to get messages such as this in the logs after restarting:
{quote}
INFO [HintedHandoff:1] 2011-09-10 22:41:40,813 HintedHandOffManager.java (line 323) Started hinted handoff for endpoint /10.1.2.3
INFO [HintedHandoff:1] 2011-09-10 22:41:40,813 HintedHandOffManager.java (line 379) Finished hinted handoff of 0 rows to endpoint /10.1.2.3
INFO [HintedHandoff:1] 2011-09-10 22:41:45,025 HintedHandOffManager.java (line 323) Started hinted handoff for endpoint /10.2.3.4
INFO [HintedHandoff:1] 2011-09-10 22:41:45,026 HintedHandOffManager.java (line 379) Finished hinted handoff of 0 rows to endpoint /10.2.3.4
INFO [HintedHandoff:1] 2011-09-10 22:42:10,017 HintedHandOffManager.java (line 323) Started hinted handoff for endpoint /10.3.4.5
INFO [HintedHandoff:1] 2011-09-10 22:42:10,017 HintedHandOffManager.java (line 379) Finished hinted handoff of 0 rows to endpoint /10.3.4.5
{quote}

Also looking at the System.HintsColumnFamily in jmx there is activity there such as pending tasks that come and go.",,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5408,,,,,,,,"01/Oct/11 05:55;jbellis;3176.txt;https://issues.apache.org/jira/secure/attachment/12497268/3176.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4039,,,Sat Oct 01 15:17:40 UTC 2011,,,,,,,,,,"0|i0ggq7:",94140,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"10/Sep/11 23:28;brandon.williams;Disabling HH means hints won't be written, not that existing hints won't be delivered.;;;","10/Sep/11 23:53;jeromatron;Sounds like
1) the logging happens even when it's disabled.
2) I double checked in the StorageProxy.HintedHandoffEnabled and it is set to false
3) there were probably some residual hints that were never delivered before disabling HH.;;;","01/Oct/11 05:55;jbellis;This isn't the first time the continued handoff messages post-disable have confused someone.  Patch to avoid these is attached.;;;","01/Oct/11 15:17;slebresne;+1, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractCompactionIterable uses a 1MB buffer for every sstable,CASSANDRA-3171,12522467,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,09/Sep/11 19:47,16/Apr/19 09:32,14/Jul/23 05:52,09/Sep/11 21:20,1.0.0,,,,,,0,,,,"This causes an OOM for any compaction task that needs to open all the sstables when you have a lot of them (repair, major, etc)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/11 20:59;jbellis;3171-v2.txt;https://issues.apache.org/jira/secure/attachment/12493855/3171-v2.txt","09/Sep/11 19:57;brandon.williams;3171.txt;https://issues.apache.org/jira/secure/attachment/12493847/3171.txt",,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4044,,,Fri Sep 09 21:20:01 UTC 2011,,,,,,,,,,"0|i0ggo7:",94131,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"09/Sep/11 19:57;brandon.williams;Patch to drop buffer size to 64k, as is the RandomAccessReader default.;;;","09/Sep/11 20:03;jbellis;Is there anywhere we actually need non-default buffer sizes?  I think we should probably rip out that constructor parameter (on both RAR and SW) to force everyone to be sane whether they like it or not.

See also: the 10MB buffer in streaming and the 8MB one in IndexWriter.;;;","09/Sep/11 20:59;jbellis;v2 removes unnecessary bufferSize parameters across the call heirarchy.  Main exception is CRAR, to which I added

{code}
// TODO refactor this to separate concept of ""buffer to avoid lots of read() syscalls"" and ""compression buffer""
{code};;;","09/Sep/11 21:14;brandon.williams;+1;;;","09/Sep/11 21:20;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
read repair: NEWS does not match actual behavior,CASSANDRA-3169,12522441,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,09/Sep/11 15:42,16/Apr/19 09:32,14/Jul/23 05:52,12/Sep/11 15:43,1.0.0,,,,,,0,,,,"NEWS says:

{noformat}
    - Hinted Handoff is substantially more robust, with the result that
      when HH is enabled, repair only needs to be run if a node crashes.
    - Because of this, read repair is disabled now by default on newly
      created ColumnFamilies.
{noformat}

But default RR value is still 1.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/11 15:44;jbellis;3169.txt;https://issues.apache.org/jira/secure/attachment/12493791/3169.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4045,,,Mon Sep 12 15:43:04 UTC 2011,,,,,,,,,,"0|i0ggnb:",94127,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"09/Sep/11 15:45;jbellis;Patch reduces default RR to 0.1 and updates NEWS to match -- seems premature to default it to all the way off.;;;","09/Sep/11 15:46;slebresne;+1;;;","12/Sep/11 15:43;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Arena allocation causes excessive flushing on small heaps,CASSANDRA-3168,12522427,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,09/Sep/11 13:52,16/Apr/19 09:32,14/Jul/23 05:52,12/Sep/11 15:50,1.0.0,,,,,,0,,,,"adding allocator.size() to Memtable.getLiveSize has two problems:

1) it double-counts allocated parts of regions
2) it makes the size of an empty memtable the size of a single region

(2) is a particular problem because flushing a nearly-empty memtable will not actually free up much memory -- we just trade one almost-empty region, for another.  In testing, I even saw this happening to the low-traffic system tables like LocationInfo.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/11 18:33;stuhood;0002-3168.txt;https://issues.apache.org/jira/secure/attachment/12493837/0002-3168.txt","09/Sep/11 14:04;jbellis;3168.txt;https://issues.apache.org/jira/secure/attachment/12493777/3168.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4046,,,Mon Sep 12 15:50:32 UTC 2011,,,,,,,,,,"0|i0ggmv:",94125,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"09/Sep/11 14:04;jbellis;patch to remove allocator.size() from getLiveSize calculation.  also adds note about arena allocation to NEWS.;;;","09/Sep/11 18:33;stuhood;Using both currentThroughput and allocator.size is definitely wrong... don't know what I was thinking there.

Since this was the only code using SlabAllocator.size(), here's a patch to remove it.;;;","09/Sep/11 18:35;stuhood;+1.;;;","12/Sep/11 15:50;jbellis;added debug logging to SlabAllocator for region count, and committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rolling upgrades from 0.7 to 0.8 not possible,CASSANDRA-3166,12522418,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,marcuse,marcuse,09/Sep/11 12:45,16/Apr/19 09:32,14/Jul/23 05:52,12/Sep/11 16:50,0.8.6,,,,,,0,,,,"We are in the progress of upgrading to 0.8 and we need to do a rolling upgrade, this fails miserably and it is reproducible;

1. set up a 3 node cluster with 0.7.9 and rf=3, read and write, QUORUM
2. upgrade one of the nodes (i upped a seednode, not sure if that is important)
3. continue reading/writing
4. see logs on the 0.7 node fill up with: INFO 12:36:08,240 Received connection from newer protocol version. Ignorning message.


it does work if i start the 0.7.9 nodes *after* the 0.8.4 node which makes me think that it matters if it is the 0.8 node connecting to the 0.7 nodes or the other way round.

Debug logging on the 0.8 node shows:
/var/log/cassandra/system.log.9:DEBUG [pool-2-thread-82] 2011-09-09 11:55:06,067 StorageProxy.java (line 178) Write timeout java.util.concurrent.TimeoutException for one (or more) of: 
/var/log/cassandra/system.log.9:DEBUG [pool-2-thread-76] 2011-09-09 11:55:06,067 StorageProxy.java (line 584) Read timeout: java.util.concurrent.TimeoutException: Operation timed out - received only 1 responses from /193.182.3.92,  .

nothing except for the ""newer protocol version..."" in the 0.7-logs

i will continue to look at this issue but if anyone has a quick patch, let me know

",,jborgstrom,kzadorozhny,mihasya,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2860,CASSANDRA-7734,,,,,,,"11/Sep/11 09:55;marcuse;3166.txt;https://issues.apache.org/jira/secure/attachment/12493952/3166.txt",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4048,,,Mon Sep 12 17:48:01 UTC 2011,,,,,,,,,,"0|i0gglz:",94121,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"09/Sep/11 12:48;marcuse;oh, note that it fails all the way to the client as well, timeouts in hector;;;","09/Sep/11 13:42;scode;Are you seeing any ""Version for $IP is $VERSION"" output when running in debug mode? IncomingTcpConnection.run() should be logging that.;;;","09/Sep/11 13:59;scode;Currently suspecting 6eb154ba5616e0df3ce4f11c88dbb1c92d317465 which adds the version reset on disconnection.
;;;","09/Sep/11 14:09;scode;And that commit was due to CASSANDRA-2818 and CASSANDRA-2860.;;;","09/Sep/11 15:04;scode;Removing the resetVersion() did not help. I added some logging to IncomingTcpConnection and it seems that when the 0.8 node goes up first, the 0.7 node never tries to make an outgoing connection to it.

If my understanding is correct, from reading CASSANDRA-2818 and looking at the code, I think the intent is that we discover the version of the other guy whenever that guy connects to *us*; we can never find out that the other side has a mis-matched version based on activity on the outbound connection.

So, incoming connections would be a necessity in order for the 0.8 node to ever adjust it's lingo.;;;","09/Sep/11 15:10;jbellis;That's right.  This is why when we do get a message from a newer-version host we make sure to add it to gossiper so we connect back to it.

Not sure if that fix got applied to 0.7 -- if not, making the 0.8 node a seed should work around it.;;;","09/Sep/11 15:46;scode;Sorry, epic fail on my part. Removing the version reset does have an effect, and the 0.7 node is in fact connecting. I made a boo-boo involving running from the wrong working copy...
;;;","09/Sep/11 16:51;scode;I'm having difficulty coming up with a clean yet simple fix here. Reverting CASSANDRA-2860 certainly fixes this problem, but re-introduces CASSANDRA-2860 instead.

I could imagine an environment variable/config option to disable the support for pretending you are older than you are, which could be used in a second round of rolling restarts after upgrading all nodes of a cluster to 0.8. A JMX tweakable setting would be nice, but upon changing it you'd want to tear down all the TCP connections to re-initiate versioning negotiation so maybe it's okay to leave it with an extra round of restarts required.

Alternatively, I think (not tested) things will tend to sort itself out incrementally every time you restart a 0.8 node since it will tend to initiate connections to other nodes immediately, but documenting for users that they need to restart nodes all over the place until everyone seems to have gotten it seems like a poor solution.

Adding some new kind of message that says ""i really am this other version"" or similar isn't clean.

Am I missing a much simpler and cleaner fix here?
;;;","11/Sep/11 09:54;marcuse;Minimal patch attached

Clear version in IncomingTcpConnection instead since that is the one setting it;
before we could end up in a state where the outgoing connections got closed, but the incoming one was still up, meaning the version was reset and it was never possible to get the version set again.

Now it is the IncomingTcpConnections responsibility to keep track of versions, if that one is closed, we are bound to get a new incoming connection and therefor set the version correctly

;;;","11/Sep/11 09:55;marcuse;Make IncomingTcpConnection responsible for version handling;;;","11/Sep/11 10:33;scode;+1 on my end. That's a very simple solution that I wasn't seeing. Can't figure out a way it will break anything.

* 0.7 <-> 0.7: No version mismatch ever, no reset ever happens. All is well.
* 0.8 <-> 0.8: Same.
* 0.7 <-> 0.8: 0.8 -> 0.7 will be killed (streaming) or retained but messages ignored (messaging). 0.7 -> 0.8 will work, and 0.8 will know the version of 0.7. Future outgoing will use correct version, and the pre-existing messaging connection starts sending messages at a version that isn't ignored.
* 0.7 node restarted and upgraded to 0.8 talking to 0.8: Both incoming/outgoing go down, so version reset, then equivalent of 0.8 <-> 0.8.
* 0.7 node restarted and upgraded to 0.8 talking to 0.7: Both incoming/outgoing go down, so version reset, then equivalent of 0.7 <-> 0.8.

;;;","12/Sep/11 16:50;brandon.williams;Committed, with minor changes: logs pushed to debug, if logic around logging removed since interpolation of {} automatically does that.;;;","12/Sep/11 17:48;hudson;Integrated in Cassandra-0.8 #322 (See [https://builds.apache.org/job/Cassandra-0.8/322/])
    Make IncomingTcpConnection responsible for version handling.
Patch by Marcus Erikkson, reviewed by Peter Schuller and brandonwilliams
for CASSANDRA-3166

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1169823
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/IncomingTcpConnection.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/OutboundTcpConnection.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GCInspector still not avoiding divide by zero,CASSANDRA-3164,12522322,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,08/Sep/11 20:59,16/Apr/19 09:32,14/Jul/23 05:52,13/Sep/11 20:17,0.7.10,0.8.6,,,,,0,thistimeforsure,,,"This is because Long objects need to be compared with .equals, not ==.

CASSANDRA-3076 is the original issue but we should use a new ticket for this since 0.7.9 and 0.8.5 are both released already.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/11 18:17;jbellis;3164.txt;https://issues.apache.org/jira/secure/attachment/12493833/3164.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4050,,,Wed Sep 14 14:49:04 UTC 2011,,,,,,,,,,"0|i0ggkv:",94116,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"08/Sep/11 23:38;tjake;curse you autoboxing!;;;","09/Sep/11 18:17;jbellis;obvious patch is obvious;;;","13/Sep/11 18:01;slebresne;+1;;;","13/Sep/11 20:17;jbellis;committed;;;","14/Sep/11 14:49;hudson;Integrated in Cassandra-0.7 #552 (See [https://builds.apache.org/job/Cassandra-0.7/552/])
    Fix divide by zero error in GCInspector
patch by jbellis; reviewed by slebresne for CASSANDRA-3164

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1170308
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/GCInspector.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SlabAllocator OOMs much faster than HeapAllocator,CASSANDRA-3163,12522316,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,08/Sep/11 20:43,16/Apr/19 09:32,14/Jul/23 05:52,09/Sep/11 18:13,1.0.0,,,,,,0,,,,"SlabAllocator will OOM with stress at around 5.5M rows, which in this configuration is roughly 3.3M rows per node.  Reverting to the HeapAllocator allowed all 10M rows to finish.  Examining the SA heap dump in MAT just shows ~98% of the heap is in 'remainder'","3 nodes, 1G heap, RF=2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/11 14:17;jbellis;3163-v2.txt;https://issues.apache.org/jira/secure/attachment/12493778/3163-v2.txt","09/Sep/11 17:21;jbellis;3163-v3.txt;https://issues.apache.org/jira/secure/attachment/12493822/3163-v3.txt","09/Sep/11 18:01;jbellis;3163-v4.txt;https://issues.apache.org/jira/secure/attachment/12493828/3163-v4.txt","09/Sep/11 13:26;jbellis;3163.txt;https://issues.apache.org/jira/secure/attachment/12493775/3163.txt","08/Sep/11 21:22;brandon.williams;system.log.gz;https://issues.apache.org/jira/secure/attachment/12493670/system.log.gz",,,,,,,,,,5.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4051,,,Fri Sep 09 18:13:43 UTC 2011,,,,,,,,,,"0|i0ggkf:",94114,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"08/Sep/11 20:46;jbellis;what liveRatio was it logging on the OOM run?;;;","08/Sep/11 21:22;brandon.williams;Here's a system.log with your extra debug statements and Memtable set to DEBUG.  It OOM'd many times, but never actually dumped.;;;","09/Sep/11 13:26;jbellis;What is happening is, the slab-allocated row keys are being put into the IndexSummary of the new sstable on flush.  So the regions can only be GC's post-compaction.;;;","09/Sep/11 14:17;jbellis;v2 re-clones into indexposition instead of trying to avoid cloning at all.  this should be a small penalty since we're only re-cloning sampled keys, not all of them.;;;","09/Sep/11 17:18;jbellis;v3 also applies getMinimalKey to SSTR.first, last fields.;;;","09/Sep/11 18:01;jbellis;v4 fixes NPE on sstable load;;;","09/Sep/11 18:09;brandon.williams;+1;;;","09/Sep/11 18:13;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PIG_OPTS bash variable interpolation doesn't work correctly when PIG_OPTS is set in the environment.,CASSANDRA-3160,12522273,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,eldondev,eldondev,08/Sep/11 16:01,16/Apr/19 09:32,14/Jul/23 05:52,08/Sep/11 17:12,0.8.6,1.0.0,,,,,0,,,,PIG_OPTS bash variable interpolation doesn't work correctly when PIG_OPTS is set in the environment due to variable preceding quotes.,bash,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/11 16:08;eldondev;CASSANDRA-3160.patch;https://issues.apache.org/jira/secure/attachment/12493608/CASSANDRA-3160.patch",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4054,,,Thu Sep 08 17:33:00 UTC 2011,,,,,,,,,,"0|i0ggj3:",94108,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"08/Sep/11 17:12;brandon.williams;Committed, thanks.;;;","08/Sep/11 17:33;hudson;Integrated in Cassandra-0.8 #321 (See [https://builds.apache.org/job/Cassandra-0.8/321/])
    Fix interpolation of PIG_OPTS.
Patch by Eldon Stegall, reviewed by brandonwilliams for CASSANDRA-3160

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166809
Files : 
* /cassandra/branches/cassandra-0.8/contrib/pig/bin/pig_cassandra
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple classpath entries in the cassandra-all.jar,CASSANDRA-3159,12522269,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,tjake,tjake,08/Sep/11 14:59,16/Apr/19 09:32,14/Jul/23 05:52,08/Sep/11 17:22,1.0.0,,,,,,0,,,,"from CASSANDRA-2936

{code}
Sep 8, 2011 8:47:45 AM java.util.jar.Attributes read
WARNING: Duplicate name in Manifest: Class-Path.
Ensure that the manifest does not have duplicate entries, and
that blank lines separate individual sections in both your
manifest and in the META-INF/MANIFEST.MF entry in the jar file.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/11 15:24;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3159-remove-extra-cp-entry.txt;https://issues.apache.org/jira/secure/attachment/12493603/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3159-remove-extra-cp-entry.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4055,,,Thu Sep 08 18:23:09 UTC 2011,,,,,,,,,,"0|i0ggiv:",94107,,,,,Low,,,,,,,,,,,,,,,,,"08/Sep/11 15:26;urandom;the attached patch merges the two classpath entries, and also renames the cookie jar to clientutil (a previous change somehow lost).;;;","08/Sep/11 15:41;tjake;+1;;;","08/Sep/11 17:22;urandom;committed.;;;","08/Sep/11 18:23;hudson;Integrated in Cassandra #1091 (See [https://builds.apache.org/job/Cassandra/1091/])
    remove extra cp entry

Patch by eevans; reviewed by tjake for CASSANDRA-3159

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166812
Files : 
* /cassandra/trunk/build.xml
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve caching of same-version Messages on digest and repair paths,CASSANDRA-3158,12522263,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,08/Sep/11 14:03,16/Apr/19 09:32,14/Jul/23 05:52,08/Sep/11 16:04,0.8.6,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/11 14:48;jbellis;3158-v2.txt;https://issues.apache.org/jira/secure/attachment/12493597/3158-v2.txt","08/Sep/11 14:03;jbellis;3158.txt;https://issues.apache.org/jira/secure/attachment/12493591/3158.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4056,,,Thu Sep 08 17:33:01 UTC 2011,,,,,,,,,,"0|i0ggif:",94105,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"08/Sep/11 14:48;jbellis;v2 also lazy-initializes the caching producer for the initial digest reads, and updates comments;;;","08/Sep/11 14:57;slebresne;For the initial digest reads, it would be nice to use the CachingMessageProducer for the local digest read too.;;;","08/Sep/11 15:04;jbellis;no Messages are created for local reads.;;;","08/Sep/11 15:07;slebresne;Oh right, my bad. Anyways, lgtm, +1.;;;","08/Sep/11 16:04;jbellis;committed;;;","08/Sep/11 16:36;jbellis;Note that on merge to trunk I also added a readCallbacks.clear() statement at the beginning of the short-read do/while loop.;;;","08/Sep/11 17:33;hudson;Integrated in Cassandra-0.8 #321 (See [https://builds.apache.org/job/Cassandra-0.8/321/])
    Improve caching of same-version Messages on digest and repair paths
patch by jbellis; reviewed by slebresne for CASSANDRA-3158

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166774
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After a ""short read"", the wrong read command may be used",CASSANDRA-3157,12522259,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,08/Sep/11 13:47,16/Apr/19 09:32,14/Jul/23 05:52,08/Sep/11 14:58,1.0.0,,,,,,0,,,,"In fetchRows, there is this code:
{noformat}
    for (int i = 0; i < commandsToSend.size(); i++)
    {
        ReadCallback<Row> handler = readCallbacks.get(i);
        ReadCommand command = commands.get(i);
{noformat}
On the first iteration of fetchRows, commands == commandsToSend so this is ok, but on a short read, commandsToSend will only contain the command to retry so we'll pick up the wrong command on the last line.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/11 13:48;slebresne;3157.patch;https://issues.apache.org/jira/secure/attachment/12493584/3157.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4057,,,Thu Sep 08 15:17:19 UTC 2011,,,,,,,,,,"0|i0gghr:",94102,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"08/Sep/11 14:09;jbellis;+1 on the fix.  (is while-on-newline really an improvement?);;;","08/Sep/11 14:16;slebresne;bq. is while-on-newline really an improvement?

Felt more coherent with the coding style to have the bracket on its own line. But I don't have to commit it.;;;","08/Sep/11 14:58;slebresne;Committed, thanks;;;","08/Sep/11 15:17;hudson;Integrated in Cassandra #1089 (See [https://builds.apache.org/job/Cassandra/1089/])
    After a ""short read"", the wrong read command may be used
patch by slebresne; reviewed by jbellis for CASSANDRA-3157

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166716
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assertion error in RowRepairResolver,CASSANDRA-3156,12521845,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,brandon.williams,brandon.williams,08/Sep/11 03:18,16/Apr/19 09:32,14/Jul/23 05:52,08/Sep/11 19:26,1.0.0,,,,,,0,,,,"Only seems to happen on a coordinator who does not have a copy of the data:

DEBUG 03:15:59,866 Processing response on a callback from 3840@/10.179.64.227
DEBUG 03:15:59,866 Preprocessed data response
DEBUG 03:15:59,866 Processing response on a callback from 3841@/10.179.111.137
DEBUG 03:15:59,866 Preprocessed digest response
DEBUG 03:15:59,865 Processing response on a callback from 3837@/10.179.111.137
DEBUG 03:15:59,865 Preprocessed data response
DEBUG 03:15:59,865 Preprocessed data response
DEBUG 03:15:59,867 Preprocessed digest response
DEBUG 03:15:59,867 resolving 2 responses
ERROR 03:15:59,866 Fatal exception in thread Thread[ReadRepairStage:526,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:77)
        at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR 03:15:59,866 Fatal exception in thread Thread[ReadRepairStage:525,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:77)
        at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR 03:15:59,867 Fatal exception in thread Thread[ReadRepairStage:528,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:77)
        at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
DEBUG 03:15:59,867 resolving 2 responses
DEBUG 03:15:59,867 resolving 2 responses
DEBUG 03:15:59,867 resolving 2 responses",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/11 19:18;jbellis;3156.txt;https://issues.apache.org/jira/secure/attachment/12493647/3156.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4058,,,Thu Sep 08 20:22:17 UTC 2011,,,,,,,,,,"0|i0gghb:",94100,,brandon.williams,,brandon.williams,Critical,,,,,,,,,,,,,,,,,"08/Sep/11 03:22;brandon.williams;Also some spurious digest mismatches mixed in, even though I have no reason to suspect there is actually a mismatch in my dev env (3 nodes, rf=2):


DEBUG 03:15:59,823 Digest mismatch:
org.apache.cassandra.service.DigestMismatchException: Mismatch for key DecoratedKey(20580074455139572311737153648595094740, 30363933) (fb3f10b793298382b554737490bc78b5 vs db8d74ec919be7c1a1dda15c85754eb0)
        at org.apache.cassandra.service.RowDigestResolver.resolve(RowDigestResolver.java:105)
        at org.apache.cassandra.service.RowDigestResolver.resolve(RowDigestResolver.java:30)
        at org.apache.cassandra.service.ReadCallback$AsyncRepairRunner.runMayThrow(ReadCallback.java:229)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

But this seems to happen when the coordinator _does_ have a copy of the data.
;;;","08/Sep/11 03:51;jbellis;So, how RR is supposed to work is like this:

Optimistic phase:
Coordinator sends data read to closest replica, digest to others

If there is a mismatch (optimism fail), we go to the repair phase:
Coordinator sends data reads to all replicas to merge + repair

The failing assert is saying ""I got a digest reply, during the repair phase--i.e. we sent a data request but got a digest back.""  

No idea how this is happening.;;;","08/Sep/11 18:20;brandon.williams;Appears to be caused by http://svn.apache.org/viewvc?revision=1151304&view=revision;;;","08/Sep/11 19:18;jbellis;The new threadlocal code was omitting to reset the buffer before each use.  Patch attached.;;;","08/Sep/11 19:23;brandon.williams;+1;;;","08/Sep/11 19:26;jbellis;committed;;;","08/Sep/11 20:22;hudson;Integrated in Cassandra #1092 (See [https://builds.apache.org/job/Cassandra/1092/])
    reset ReadVerbHandler buffer between uses
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3156

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166865
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/db/ReadVerbHandler.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary index should report it's memory consumption,CASSANDRA-3155,12521842,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tjake,jasonrutherglen,jasonrutherglen,08/Sep/11 02:12,16/Apr/19 09:32,14/Jul/23 05:52,23/Dec/11 19:11,1.0.7,,,Feature/2i Index,,,0,,,,Non-CFS backed secondary indexes will consume RAM which should be reported back to Cassandra to be factored into it's flush by RAM amount.,,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/11 22:59;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3155-report-all-live-index-memory.txt;https://issues.apache.org/jira/secure/attachment/12508461/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3155-report-all-live-index-memory.txt",,,,,,,,,,,,,,1.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,1859,,,Fri Dec 23 19:11:13 UTC 2011,,,,,,,,,,"0|i0gggv:",94098,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"22/Dec/11 21:38;jbellis;This feels clunky to me.  Wouldn't it be better to have both CFS and non-CFS indexes provide the same API so we can do it polymorphically w/ a single loop?;;;","22/Dec/11 23:04;tjake;attached a different approach.  The polymorphic approach would still be clunky because it includes self, so you need to get self + indexes.  latest patch seems more readable.;;;","23/Dec/11 04:13;jbellis;+1;;;","23/Dec/11 19:11;tjake;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad equality check in ColumnFamilyStore.isCompleteSSTables(),CASSANDRA-3154,12521834,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tupshin,tupshin,tupshin,07/Sep/11 23:21,16/Apr/19 09:32,14/Jul/23 05:52,08/Sep/11 17:36,1.0.0,,,,,,0,,,,The equality check in isCompleteSSTables() always fails because it tries to call equals() with a Set and a List. This might result in failure to purge tombstones in some cases.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/11 01:31;jbellis;3154.txt;https://issues.apache.org/jira/secure/attachment/12493534/3154.txt","07/Sep/11 23:23;tupshin;CASSANDRA-3154.diff;https://issues.apache.org/jira/secure/attachment/12493527/CASSANDRA-3154.diff",,,,,,,,,,,,,2.0,tupshin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4059,,,Fri Sep 09 11:19:45 UTC 2011,,,,,,,,,,"0|i0gggf:",94096,,bcoverston,,bcoverston,Low,,,,,,,,,,,,,,,,,"07/Sep/11 23:30;bcoverston;The cardinality restriction of .isEqualCollection is probably more restrictive than we need, but this does indeed fix the existing shallow equality problem.

+1;;;","08/Sep/11 01:31;jbellis;I'd rather get rid of that code.  It's not useful because

- for leveled compactions, you are effectively guaranteed that once you have more than a couple sstables, you'll never compact all sstables at once
- for non-leveled compactions, you have a small enough number of sstables that isKeyInRemainingSSTables is fine without adding additional optimization for the ""major"" case

This patch gets rid of isMajor, and additionally renames CompactionType to OperationType to better reflect the ""compaction"" stage's role as generic background IO manager.;;;","08/Sep/11 03:11;tupshin;+1 to getting rid of the code instead.;;;","08/Sep/11 17:22;bcoverston;Patch is good.
+1 Getting rid of isMajor makes the logic behind the determination of the compaction operation much cleaner.;;;","08/Sep/11 17:36;jbellis;committed;;;","08/Sep/11 18:23;hudson;Integrated in Cassandra #1091 (See [https://builds.apache.org/job/Cassandra/1091/])
    remove isMajor compaction designation
patch by jbellis; reviewed by Tupshin Harper and Ben Coverston for CASSANDRA-3154

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166822
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/cache/AutoSavingCache.java
* /cassandra/trunk/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/AbstractCompactionIterable.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionController.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionInfo.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionIterable.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionTask.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionType.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/OperationType.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/ParallelCompactionIterable.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/SecondaryIndexBuilder.java
* /cassandra/trunk/test/unit/org/apache/cassandra/io/LazilyCompactedRowTest.java
;;;","09/Sep/11 11:19;hudson;Integrated in Cassandra #1093 (See [https://builds.apache.org/job/Cassandra/1093/])
    Remove wrongly added import (by CASSANDRA-3154)

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1167078
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionTask.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logic of AbstractNetworkTopologySnitch.compareEndpoints is wrong,CASSANDRA-3152,12521828,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,07/Sep/11 22:22,16/Apr/19 09:32,14/Jul/23 05:52,08/Sep/11 00:54,0.8.6,,,,,,0,,,,"Current logic in ANTS.cE is to compare the rack and then compare the DC's, the problem is when we have the same rack name but the racks are in a diffrent DC's this logic breaks...

Example: 
""us-east,1a"", InetAddress.getByName(""127.0.0.1"")
""us-east,1b"", InetAddress.getByName(""127.0.0.2"")
""us-east,1c"", InetAddress.getByName(""127.0.0.3"")
""us-west,1a"", InetAddress.getByName(""127.0.0.4"")
""us-west,1b"", InetAddress.getByName(""127.0.0.5"")
""us-west,1c"", InetAddress.getByName(""127.0.0.6"")

Expected:
/127.0.0.1,/127.0.0.3,/127.0.0.2,/127.0.0.4,/127.0.0.5,/127.0.0.6

Current:
/127.0.0.1,/127.0.0.4,/127.0.0.3,/127.0.0.2,/127.0.0.5,/127.0.0.6",JVM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/11 23:41;vijay2win@yahoo.com;0001-fix-dc-rack-sorting-on-ANTS.patch;https://issues.apache.org/jira/secure/attachment/12493529/0001-fix-dc-rack-sorting-on-ANTS.patch",,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4061,,,Thu Sep 08 01:20:19 UTC 2011,,,,,,,,,,"0|i0ggfj:",94092,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"07/Sep/11 23:41;vijay2win@yahoo.com;Tested and passed basically moved the DC comparison logic up in ANTS;;;","08/Sep/11 00:54;jbellis;committed;;;","08/Sep/11 01:20;hudson;Integrated in Cassandra-0.8 #320 (See [https://builds.apache.org/job/Cassandra-0.8/320/])
    allow topology sort to work with non-unique rack names between datacenters
patch by Vijay; reviewed by jbellis for CASSANDRA-3152

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166484
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/AbstractNetworkTopologySnitch.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IntervalTree could miscalculate its max,CASSANDRA-3145,12521505,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,thepaul,thepaul,06/Sep/11 22:02,16/Apr/19 09:32,14/Jul/23 05:52,07/Sep/11 18:28,1.0.0,,,,,,0,,,,"The implementation of IntervalTree in trunk expects an ordered list of Interval objects as the argument to its constructor. It uses the ordering (only) to determine its minimum and maximum endpoints out of all Intervals stored in it. However, no ordering should be able to guarantee the first element has the set-wide minimum and that the last element has the set-wide maximum; you have to order by minima or maxima or some combination.

I propose that the requirement for ordered input to the IntervalTree constructor be dropped, seeing as how the elements will be sorted as necessary inside the IntervalNode object anyway. The set-wide minimum and maximum could be more straightforwardly calculated inside IntervalNode, and just exposed via IntervalTree.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/11 22:13;thepaul;3145.patch.txt;https://issues.apache.org/jira/secure/attachment/12493224/3145.patch.txt",,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4066,,,Wed Sep 07 19:02:11 UTC 2011,,,,,,,,,,"0|i0ggcf:",94078,,bcoverston,,bcoverston,Low,,,,,,,,,,,,,,,,,"06/Sep/11 22:13;thepaul;fixes IntervalTree.max calculation.

also eliminates a small amount of overhead for the use of a ConcurrentSkipListSet, since there isn't a need for the efficient contains/remove operations or the concurrency support.;;;","07/Sep/11 18:21;bcoverston;+1 the patch looks good;;;","07/Sep/11 18:28;jbellis;committed;;;","07/Sep/11 19:02;hudson;Integrated in Cassandra #1084 (See [https://builds.apache.org/job/Cassandra/1084/])
    fix IntervalTree max calculation
patch by Paul Cannon; reviewed by Ben Coverston for CASSANDRA-3145

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166302
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/DataTracker.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/IntervalTree/IntervalNode.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/IntervalTree/IntervalTree.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"trunk is unable to participate with an 0.8 ring, again",CASSANDRA-3144,12521471,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,brandon.williams,brandon.williams,brandon.williams,06/Sep/11 17:04,16/Apr/19 09:32,14/Jul/23 05:52,07/Sep/11 01:57,1.0.0,,,,,,0,,,,"Title pretty much says it all, looks like a rehash of CASSANDRA-2818 to some degree.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/11 22:59;jbellis;3144.txt;https://issues.apache.org/jira/secure/attachment/12493243/3144.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4067,,,Wed Sep 07 02:15:39 UTC 2011,,,,,,,,,,"0|i0ggbz:",94076,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"06/Sep/11 22:59;jbellis;As Brandon determined, this is a regression caused by CASSANDRA-1788.;;;","06/Sep/11 23:09;brandon.williams;+1.  I hope this ticket dies in a fire.;;;","07/Sep/11 01:57;jbellis;committed;;;","07/Sep/11 02:15;hudson;Integrated in Cassandra #1081 (See [https://builds.apache.org/job/Cassandra/1081/])
    use message.version in outbound header instead of MS.version
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3144

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1165957
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/net/OutboundTcpConnection.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CustomTThreadPoolServer should log TTransportException at DEBUG level,CASSANDRA-3142,12521408,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jancona,jancona,jancona,06/Sep/11 02:10,16/Apr/19 09:32,14/Jul/23 05:52,07/Sep/11 13:42,0.8.6,,,,,,0,,,,"Currently CustomTThreadPoolServer, like the Thrift TThreadPoolServer, silently ignores TTransportException in its run() method. This is appropriate in most cases because TTransportException occurs fairly often when client connections die. However TTransportException is also thrown when TFramedTransport encounters a frame that is larger than thrift_framed_transport_size_in_mb. In that case, silently exiting the run loop leads to a SocketException on the client side which can be both difficult to diagnose, in part because nothing is logged by Cassandra, and high-impact, because the client may respond by marking the server node down and retrying the too-large request on another node, where it also fails. This process repeated leads to the entire cluster being marked down (see https://github.com/rantav/hector/issues/212). I've filed two Thrift issues (https://issues.apache.org/jira/browse/THRIFT-1323 and https://issues.apache.org/jira/browse/THRIFT-1324), but in the meantime, I suggest that CustomTThreadPoolServer log the exception at DEBUG level in order to support easier troubleshooting.
",,jancona,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/11 17:43;jancona;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3142-Add-debug-level-logging-of-TTransportEx.txt;https://issues.apache.org/jira/secure/attachment/12493184/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3142-Add-debug-level-logging-of-TTransportEx.txt",,,,,,,,,,,,,,1.0,jancona,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4068,,,Wed Sep 07 14:22:33 UTC 2011,,,,,,,,,,"0|i0ggb3:",94072,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"07/Sep/11 13:42;xedin;Committed.;;;","07/Sep/11 14:22;hudson;Integrated in Cassandra-0.8 #318 (See [https://builds.apache.org/job/Cassandra-0.8/318/])
    CustomTThreadPoolServer to log TTransportException at DEBUG level
patch by Jim Ancona; reviewed by Pavel Yaskevich for CASSANDRA-3142

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166173
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CustomTThreadPoolServer.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent users from creating keyspaces with LocalStrategy replication,CASSANDRA-3139,12521380,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,05/Sep/11 19:03,16/Apr/19 09:32,14/Jul/23 05:52,05/Sep/11 22:21,0.8.6,,,Legacy/CQL,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/11 21:36;xedin;CASSANDRA-3139.patch;https://issues.apache.org/jira/secure/attachment/12493069/CASSANDRA-3139.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4069,,,Mon Sep 05 23:19:59 UTC 2011,,,,,,,,,,"0|i0gg9j:",94065,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"05/Sep/11 21:32;jbellis;LocalStrategy isn't deprecated; it's just reserved for internal use.;;;","05/Sep/11 21:36;xedin;error message is fixed.;;;","05/Sep/11 21:48;jbellis;+1;;;","05/Sep/11 22:21;xedin;Committed.;;;","05/Sep/11 23:19;hudson;Integrated in Cassandra-0.8 #315 (See [https://builds.apache.org/job/Cassandra-0.8/315/])
    Prevent users from creating keyspaces with LocalStrategy replication
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3139

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1165438
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/cli/CliTest.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/thrift/ThriftValidationTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PropertyFileSnitch's ResourceWatcher fails because it uses FBUtilities.resourceToFile(..) while PropertyFileSnitch uses classloader.getResourceAsStream(..),CASSANDRA-3138,12521372,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mck,mck,mck,05/Sep/11 16:41,16/Apr/19 09:32,14/Jul/23 05:52,05/Sep/11 19:59,0.8.6,,,,,,0,,,,"Resource files are not necessarily plain files. They could be inside a jar file. See CASSANDRA-2036

This will cause {noformat}RROR 24:15,806 ResourceWatcher$WatchedResource: Timed run of class org.apache.cassandra.locator.PropertyFileSnitch$1 failed.
org.apache.cassandra.config.ConfigurationException: unable to locate cassandra-topology.properties
	at org.apache.cassandra.utils.FBUtilities.resourceToFile(FBUtilities.java:467)
	at org.apache.cassandra.utils.ResourceWatcher$WatchedResource.run(ResourceWatcher.java:57)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662){noformat}",,sumit.thakur@rancoretech.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/11 16:52;mck;CASSANDRA-3138.patch;https://issues.apache.org/jira/secure/attachment/12493046/CASSANDRA-3138.patch",,,,,,,,,,,,,,1.0,mck,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4070,,,Fri Sep 13 08:01:51 UTC 2013,,,,,,,,,,"0|i0gg93:",94063,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"05/Sep/11 16:52;mck;Simple solution that prevents the ResourceWatcher for PropertyFileSnitch from being submitted if the property file is not a plain file on disk.;;;","05/Sep/11 19:59;jbellis;committed (w/ logged message at debug), thanks!;;;","05/Sep/11 20:14;hudson;Integrated in Cassandra-0.8 #314 (See [https://builds.apache.org/job/Cassandra-0.8/314/])
    avoid trying to watch cassandra-topology.properties when loaded from jar
patch by Mck SembWever; reviewed by jbellis for CASSANDRA-3138

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1165405
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/PropertyFileSnitch.java
;;;","13/Sep/13 08:01;sumit.thakur@rancoretech.com;Hello All,

Same issue in apache cassandra 1.1.5

ERROR [ScheduledTasks:1] 2013-09-12 17:34:55,268 ResourceWatcher.java (line 67) Timed run of class org.apache.cassandra.locator.PropertyFileSnitch$1 failed.
org.apache.cassandra.config.ConfigurationException: unable to locate cassandra-topology.properties
	at org.apache.cassandra.utils.FBUtilities.resourceToFile(FBUtilities.java:327)
	at org.apache.cassandra.utils.ResourceWatcher$WatchedResource.run(ResourceWatcher.java:57)
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:79)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh doesn't work on windows (no readline),CASSANDRA-3131,12521228,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,urandom,urandom,02/Sep/11 20:40,16/Apr/19 09:32,14/Jul/23 05:52,11/Nov/11 22:32,1.0.3,,,,,,0,cql,,,"Saulius Menkevicius reports in CASSANDRA-3010 that {{cqlsh}} doesn't start on Windows because the readline module is not present.

{{cqlsh}} should be fixed to only use readline if it is present.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/11 22:24;thepaul;3131.patch.txt;https://issues.apache.org/jira/secure/attachment/12503432/3131.patch.txt",,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,1865,,,Fri Nov 11 22:32:28 UTC 2011,,,,,,,,,,"0|i0gg5z:",94049,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"08/Nov/11 17:11;thepaul;Addressed in CASSANDRA-3188.;;;","11/Nov/11 22:24;thepaul;Although cqlsh works on windows as is (both with cygwin and under the Command Prompt application), it mistakenly tries to use ANSI escape codes for color under Command Prompt, which show up as messy {{];34;40m}}-ish garbage.

The improvements here make cqlsh smarter about choosing whether to use color by default.;;;","11/Nov/11 22:32;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""show schema"" in CLI outputs invalid text structure that cannot be replayed (easily tweakable though)",CASSANDRA-3129,12521215,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,udontknow,udontknow,02/Sep/11 18:23,16/Apr/19 09:32,14/Jul/23 05:52,06/Sep/11 16:50,0.8.6,,,Legacy/Tools,,,0,,,,"Log explaining the problem. Trouble happens at the ""and replication_factor = 1"" string

[default@unknown] connect cassandra2/9160;                                       
Connected to: ""Lab1"" on cassandra2/9160

* Create a keyspace with a pretty simple definition
[default@unknown] create keyspace foo
...	  with placement_strategy = 'SimpleStrategy'
...	  and strategy_options = [{replication_factor : 1}];
f9e13340-d58f-11e0-0000-e3f60146f2df
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] use foo;
Authenticated to keyspace: foo

* Copy the schema so we can paste it later
[default@foo] show schema; 
create keyspace foo
  and replication_factor = 1
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = [{replication_factor : 1}];

use foo;


* Remove the keyspace, so we can paste the exact same text above
[default@foo] drop keyspace foo;
07c93a70-d590-11e0-0000-e3f60146f2df
Waiting for schema agreement...
... schemas agree across the cluster

* Paste the schema shown above as result of the 'show schema' command
[default@unknown] create keyspace foo
...	  and replication_factor = 1
...	  with placement_strategy = 'SimpleStrategy'
...	  and strategy_options = [{replication_factor : 1}];
No enum constant org.apache.cassandra.cli.CliClient.AddKeyspaceArgument.REPLICATION_FACTOR
* Presented an error that should not occur if show schema generated valid text
","CentOS 6.0 
Linux cassandra2.local 2.6.32-71.29.1.el6.x86_64 #1 SMP Mon Jun 27 19:49:27 BST 2011 x86_64 x86_64 x86_64 GNU/Linux
java version ""1.7.0""
Java(TM) SE Runtime Environment (build 1.7.0-b147)
Java HotSpot(TM) 64-Bit Server VM (build 21.0-b17, mixed mode)
Cassandra 0.8.4 from the official tarball, also reproducible on the datastax 0.8.4-1 rpm.",udontknow,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/11 16:05;xedin;CASSANDRA-3129.patch;https://issues.apache.org/jira/secure/attachment/12493170/CASSANDRA-3129.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4075,,,Tue Sep 06 17:29:16 UTC 2011,,,,,,,,,,"0|i0gg53:",94045,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"02/Sep/11 19:45;xedin;Evaldo, please confirm that this fixed the problem!;;;","06/Sep/11 15:27;jbellis;Does this handle an upgraded 0.7 schema that didn't have replication_factor set in strategy_options?;;;","06/Sep/11 15:47;xedin;makes sure to include ""replication_factor"" if set as an attribute to the strategy_options.;;;","06/Sep/11 15:57;jbellis;The existing backwards compatibility code in KSMetadata doesn't cover this?

(No, I don't know the answer to these questions.);;;","06/Sep/11 16:05;xedin;Oh, I overlooked that - it actually does add replication_factor to the strategy_options.;;;","06/Sep/11 16:23;jbellis;+1

I also note that the original report shows ""... and ... with ... and ..."" in the create statement.  Is there a second bug?  The first option should always be ""with."";;;","06/Sep/11 16:40;xedin;bq. I also note that the original report shows ""... and ... with ... and ..."" in the create statement. Is there a second bug? The first option should always be ""with.""

It was fixed with removal of replication_strategy - writeAttr(...) call there had a wrong second argument.;;;","06/Sep/11 16:50;xedin;Committed.;;;","06/Sep/11 17:29;hudson;Integrated in Cassandra-0.8 #316 (See [https://builds.apache.org/job/Cassandra-0.8/316/])
    Fix CLI `show schema;` to output correct keyspace definition statement
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3129

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1165758
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unable to remove column metadata via CLI,CASSANDRA-3126,12521207,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,hsn,hsn,02/Sep/11 16:39,16/Apr/19 09:32,14/Jul/23 05:52,15/Oct/11 20:17,0.8.8,1.0.1,,Legacy/Tools,,,0,cassandra-cli,,,"I cant find way how to remove all columns definitions without CF import/export.

[default@int4] update column family sipdb with column_metadata = [];
Syntax error at position 51: required (...)+ loop did not match anything at input ']'

[default@int4] update column family sipdb with column_metadata = [{}];
Command not found: `update column family sipdb with column_metadata = [{}];`. Type 'help;' or '?' for help.
[default@int4]
",,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/11 15:51;xedin;CASSANDRA-3126.patch;https://issues.apache.org/jira/secure/attachment/12499147/CASSANDRA-3126.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,1868,,,Sat Oct 15 21:22:05 UTC 2011,,,,,,,,,,"0|i0gg3j:",94038,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"15/Oct/11 15:51;xedin;column_metadata = []; should be functional now.;;;","15/Oct/11 16:06;jbellis;+1;;;","15/Oct/11 20:17;xedin;Committed.;;;","15/Oct/11 21:22;hudson;Integrated in Cassandra-0.8 #375 (See [https://builds.apache.org/job/Cassandra-0.8/375/])
    Fix completely removing column metadata using CLI
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3126

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1183681
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/Cli.g
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/cli/CliTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't try to build secondary indexes when there is none,CASSANDRA-3123,12521174,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,02/Sep/11 12:01,16/Apr/19 09:32,14/Jul/23 05:52,02/Sep/11 17:20,0.8.5,,,Feature/2i Index,,,0,,,,"buildSecondaryIndexes() is sometimes called without checking the cfs has secondary indexes. Has a result, it prints a useless message and will trigger a bunch of useless action (among which, a full scan of the indexed column family). This is not a huge problem in 0.8 because only the fairly new loadNewSSTables() call does this (which doesn't mean we should fix it). But in trunk, it does this after every streamIn session. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/11 17:03;slebresne;3123-v2.patch;https://issues.apache.org/jira/secure/attachment/12492774/3123-v2.patch","02/Sep/11 12:04;slebresne;3123.patch;https://issues.apache.org/jira/secure/attachment/12492735/3123.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4077,,,Fri Sep 02 18:15:32 UTC 2011,,,,,,,,,,"0|i0gg2f:",94033,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"02/Sep/11 12:04;slebresne;Attaching patch against 0.8. It makes buildSecondaryIndexes just return if there is no indexed columns.;;;","02/Sep/11 13:24;jbellis;If we're going to change to relying on the build method to recognize it was asked to perform a no-op and bail early, we should call it ""maybeBuild..."" or similar and add javadoc so it's clear that it might be a no-op.;;;","02/Sep/11 17:03;slebresne;Updated patch with proposed changes;;;","02/Sep/11 17:08;jbellis;+11111111;;;","02/Sep/11 17:20;slebresne;Committed, thanks;;;","02/Sep/11 18:15;hudson;Integrated in Cassandra-0.8 #312 (See [https://builds.apache.org/job/Cassandra-0.8/312/])
    Don't try to build secondary indexes when there is none
patch by slebresne; reviewed by jbellis for CASSANDRA-3123

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1164634
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/StreamInSession.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cli syntax for creating keyspace is inconsistent in 1.0,CASSANDRA-3119,12521061,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tjake,slebresne,slebresne,01/Sep/11 13:46,16/Apr/19 09:32,14/Jul/23 05:52,07/Sep/11 20:09,1.0.0,,,,,,0,cli,,,"In 0.8, to create a keyspace you could do:
{noformat}
create keyspace test with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = [{replication_factor:3}]
{noformat}

In current trunk, if you try that, you get back ""null"". Turns out this is because the syntax for strategy_options has changed and you should not use the brackets, i.e:
{noformat}
strategy_options = {replication_factor:3}
{noformat}
(and note that reversely, this syntax doesn't work in 0.8).

I'm not sure what motivated that change but this is very user unfriendly. The help does correctly mention the new syntax, but it is the kind of changes that takes you 5 minutes to notice. It will also break people scripts for no good reason that I can see.

We should either:
# revert to the old syntax
# support both the new and old syntax
# at least print a meaningful error message when the old syntax is used

Imho, the last solution is by far the worst solution.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/11 19:18;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3119-warn-on-old-cli-syntax.txt;https://issues.apache.org/jira/secure/attachment/12493364/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3119-warn-on-old-cli-syntax.txt",,,,,,,,,,,,,,1.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4080,,,Wed Sep 07 21:23:30 UTC 2011,,,,,,,,,,"0|i0gg0v:",94026,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"01/Sep/11 13:54;jbellis;I'm in favor of 3., because the old syntax was broken -- we never actually supported a list there, which is what it implies.;;;","01/Sep/11 13:55;jbellis;As mentioned on the other ticket, this should also be described in NEWS.;;;","01/Sep/11 14:22;xedin;it seems like the best move here will be to support both [{}] and {} in there because it's imposible to say what caused that ""null"" because it fails somewhere deep in ANTLR (in most cases when recognition was failed ANTLR throws standard exceptions which we handle correctly as ""Command not found"" or ""Syntax error"").;;;","01/Sep/11 16:57;urandom;bq. ... the old syntax was broken – we never actually supported a list there, which is what it implies.

I agree completely, but a user whose script has broken is still going to be irritated.

These things are like paper cuts, they aren't a huge deal when looked at in isolation, but they add up to a negative experience pretty quickly.;;;","07/Sep/11 19:19;tjake;The patch accepts both new and old syntax, also warns the user when they use [{}]


;;;","07/Sep/11 20:09;xedin;Committed.;;;","07/Sep/11 21:23;hudson;Integrated in Cassandra #1085 (See [https://builds.apache.org/job/Cassandra/1085/])
    Fix inconsistency of the CLI syntax when {} should be used instead of [{}]
patch by Jake Luciani; reviewed by Pavel Yaskevich for CASSANDRA-3119

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1166367
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/trunk/test/unit/org/apache/cassandra/cli/CliTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageServiceMBean is missing a getCompactionThroughputMbPerSec() method,CASSANDRA-3117,12521008,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,01/Sep/11 02:47,16/Apr/19 09:32,14/Jul/23 05:52,01/Sep/11 16:25,0.8.5,,,Legacy/Tools,,,0,,,,"Without a getter, you can assign a new value but not query the existing one (which is strange).",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/11 15:26;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3117-add-missing-mbean-method.txt;https://issues.apache.org/jira/secure/attachment/12492612/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3117-add-missing-mbean-method.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4082,,,Fri Sep 02 07:22:59 UTC 2011,,,,,,,,,,"0|i0gfzr:",94021,,,,,Low,,,,,,,,,,,,,,,,,"01/Sep/11 15:29;jbellis;+1;;;","01/Sep/11 16:25;urandom;committed.;;;","02/Sep/11 07:22;hudson;Integrated in Cassandra-0.8 #310 (See [https://builds.apache.org/job/Cassandra-0.8/310/])
    add missing mbean method

Patch by eevans; reviewed by jbellis for CASSANDRA-3117

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1164127
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageServiceMBean.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compactions can (seriously) delay schema migrations,CASSANDRA-3116,12521007,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,urandom,urandom,01/Sep/11 02:42,16/Apr/19 09:32,14/Jul/23 05:52,31/Oct/11 13:32,1.1.0,,,,,,0,compaction,,,A compaction lock is acquired when dropping keyspaces or column families which will cause the schema migration to block if a compaction is in progress.,,cherro,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3225,,,,,,,,,,,"26/Oct/11 17:37;jbellis;3116-v2.txt;https://issues.apache.org/jira/secure/attachment/12500916/3116-v2.txt","28/Oct/11 19:30;jbellis;3116-v3.txt;https://issues.apache.org/jira/secure/attachment/12501348/3116-v3.txt","17/Oct/11 17:11;jbellis;3116.txt;https://issues.apache.org/jira/secure/attachment/12499402/3116.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,1872,,,Mon Oct 31 17:16:49 UTC 2011,,,,,,,,,,"0|i0gfzb:",94019,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"01/Sep/11 02:51;jbellis;agreed that this sucks.  would like to switch to some kind of test-and-set safety for compaction vs migration instead.  not immediately obvious to me how to do this.;;;","01/Sep/11 17:00;tom.wilkie;Its worse than this too; a migration tried to get the write lock, which gets blocked on a big compaction (holding the read lock).  This migration waiting on the write lock then blocks all other compactions waiting on the read lock.  So you only get one compaction going on and thousands backing up.

A really hacky temporary fix would be to use a tryLock(timeout) and short sleep in a loop in the migration.  This would at least not starve the merges, but would starve the migrations quite badly.;;;","02/Sep/11 08:29;slebresne;I think the _raison d'être_ of the lock is because we need to mark all sstables compacted for them to be removed when dropping, but that cannot be done correctly if some sstable are being compacted. But couldn't we just ""delay"" the compacted marking ? For instance, we could have a 'isDropped' switch in DataTracker such that when that switch is on the replace() method just remove the 'replacements' sstables. So the drop keyspace/cf would set the isDropped flag first, then grab any sstable files that is not being compacted and mark those right away. It may be a bit tricky to do that atomically but _à priori_ it sounds doable. We'll probably want to add a call to some checkForDropped() method in case a compaction fails to be sure we don't leave sstables behind in that case.

Another option may be to just stop the running compactions (CASSANDRA-1740) so that we can mark everything compacted at once. I may be harder to make that thread safe though, not sure, and CASSANDRA-1740 is not in yet. ;;;","17/Oct/11 17:11;jbellis;Patch to replace locking in migrations + valid checks in CompactionManager with isValid checks in DataTracker.

compactionLock is still used but only for major compaction.  should we get rid of that too and say ""if you want to be absolutely sure you're compacting everything, disable minor compactions before invoking major?"";;;","17/Oct/11 17:11;jbellis;Note: applies after the Rename migration removal in CASSANDRA-3292.;;;","19/Oct/11 15:19;slebresne;Not sure this work correctly. I believe we first have a problem with DT.removeAllSSTables(), because this is during the drop and really do remove *all* sstables, including the ones that are being compacted (and thus it will unreference those while they are being compacted, which is bad). So we should first change that to only remove the one that are not compacting. Then we must make sure that anything that was not removed by that gets removed later. Which involves removing any flushed memtable (though that doesn't really matter since a dropped CF is flushed before being invalidated) and we must make sure that compacted sstable do are marked compacted but also that replacements are directly marked as compacted too (which mainly involve that we call removeOldSSTablesSize() on them). And I suppose we could make sure no new compaction is automatically triggered on an invalidated CF so we don't have a race or something.

bq. compactionLock is still used but only for major compaction. should we get rid of that too and say ""if you want to be absolutely sure you're compacting everything, disable minor compactions before invoking major?""

I think there is really no much cost to keeping the lock in there if the write lock is only acquired by events triggered by a user and I would prefer having major compaction do what it pretend by default rather that having a ""complicated"" procedure. That being said, I would be for replacing the global compactionLock by one lock per CF (which should be easy).
;;;","26/Oct/11 17:37;jbellis;bq. only remove the one that are not compacting

done.  (renamed removeAllSSTables to unreferenceSSTables, which could still stand improvement...)

bq. removing any flushed memtable ... also that replacements are directly marked as compacted too 

done (both by ultimately funneling through the replace method)

bq. we could make sure no new compaction is automatically triggered on an invalidated CF 

this shouldn't be a problem, if it happens.  I'd rather not go to extra effort to prevent something harmless.

v2 also gets rid of CFS.flushlock (we already flush for the drop snapshot) and removes CFS.isDropped in favor of isValid.;;;","28/Oct/11 17:20;slebresne;The patch already needs rebase, but based on the diff a few comments:
* In unmarkCompacting, calling twice markCompacted is not so harmless has it will trigger an assertion. What we could do is checked the sstableReader isCompacted flag.
* attemptUpdate breaks atomicity for unreferenceSSTable. We should make sure the compareAndSet is done on the view we used to compute notCompacting, otherwise we could have bug in View.replace (like in CASSANDRA-3306). It's probably simpler to move back the compareAndSet in both unreferenceSSTable and replace, and call a 'finalizeReplace' for the addNewSSTableSize and following methods.
* We could rename the unregisterMBean method in KeysIndex to invalidate.
* We may still want to check for isValid before doing a validation compaction because it doesn't call markCompacting, so it could still run after it's invalidated on some sstable that have not yet be removed because are being compacted.
;;;","28/Oct/11 19:30;jbellis;bq. What we could do is checked the sstableReader isCompacted flag

I think I like moving the check into removeOldSSTables instead, since it's clearly threadsafe (even though there are no existing thread safety issues, why take chances with future complications).  The new version also allows markCompacted to ""work"" when called multiple times with asserts turned off; before, the file create IOException would blow up anyway.

bq. attemptUpdate breaks atomicity for unreferenceSSTable

Damn it, you're right.  So many CAS loops feels like we're making this too fragile.  But you're right, that's better than passing View references around.

bq. rename the unregisterMBean method in KeysIndex to invalidate

done.

bq. We may still want to check for isValid before doing a validation compaction 

Done, although I suspect my comment attempting to explain the reason for the check may cause more confusion than it's worth. :)

v3 attached.  CASSANDRA-3409 throws a bit of a wrench into things since I don't see a good way to avoid the lock; still, that's not a frequently used migration.;;;","31/Oct/11 13:11;slebresne;+1 on v3. Nit: for the assert in DT.postReplace(), I believe the msg should include the sstable, not 'this'.;;;","31/Oct/11 13:32;jbellis;fixed + committed;;;","31/Oct/11 17:16;hudson;Integrated in Cassandra #1177 (See [https://builds.apache.org/job/Cassandra/1177/])
    replace compactionlock use in schema migration by checking CFS.isInvalidD
patch by jbellis; reviewed by slebresne for CASSANDRA-3116

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1195542
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/DataTracker.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/Table.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/SecondaryIndex.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/SecondaryIndexManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/index/keys/KeysIndex.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/DropColumnFamily.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/migration/DropKeyspace.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/KeyCacheTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/compaction/CompactionsTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/streaming/StreamingTransferTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LongCompactionSpeedTest running longer starting with builds on Aug31,CASSANDRA-3115,12520975,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,cdaw,cdaw,31/Aug/11 20:57,16/Apr/19 09:32,14/Jul/23 05:52,01/Sep/11 19:10,,,,,,,0,,,,"The Long tests started consistently timing out as this build of cassandra: [https://jenkins.qa.datastax.com/job/CassandraLong/131/console]

The regression server shows pretty consistent run times for this test, and then consistent timeouts from this point forward.
{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Tests run: 6, Failures: 0, Errors: 0, Time elapsed: 111.379 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 1637 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: 6144 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 2379 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=500000: 15690 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=500000 colsper=1: 20953 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=1000 colsper=5: 5672 ms
{code}

After increasing the timeout, the run time shows are now:
{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Tests run: 6, Failures: 0, Errors: 0, Time elapsed: 409.486 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 2465 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: 29407 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 2456 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=500000: 14588 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=500000 colsper=1: 100794 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=1000 colsper=5: 19266 ms
{code}


*Single node local run:  Build 1056 / on Aug 30 / Macbook Pro w/ 8 GB ram (all apps shutdown)*
{panel}
+Run 1: Fresh install with no log or lib dir+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 850 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *3004 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 767 ms
    
+Run 2: Invoke test without restarting the server+
	[junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 826 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *3030 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 776 ms

+Run 3: Invoke test without restarting the server+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 830 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *2964 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 635 ms

+Run 4: Invoke test without restarting the server+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 931 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *2987 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 910 ms
{panel}

*Singled node local run: Build 1062 / on Aug 31 / Macbook pro w/ 8GB ram (all apps shutdown)*
{panel}
+Run 1: Fresh restart with no log or lib dir+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 802 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *17649 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 713 ms

+Run 2: Invoke test without restarting the server+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 832 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *16875 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 868 ms

+Run 3: Invoke test without restarting the server+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 809 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *16818 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 807 ms

+Run 4: Invoke test without restarting the server+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 834 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *16997 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 873 ms
{panel}


*Cassandra Build at time of test failure*
[https://jenkins.qa.datastax.com/job/Cassandra/168/changes]
{panel}
Revision: 1163394

* make Range and Bounds objects client-safe patch by Mck SembWever and jbellis for CASSANDRA-3108
* Catch invalid key_validation_class before instantiating UpdateColumnFamily patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3102
* Add validation that Keyspace names are case-insensitively unique patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3066
* merge from 0.7
* merge from 0.7
* merge from 0.7
* update CHANGES for #3023 and #3044

Revision 1163394 by jbellis: 
make Range and Bounds objects client-safe
patch by Mck SembWever and jbellis for CASSANDRA-3108
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/dht/Range.java
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/dht/Bounds.java

Revision 1163291 by xedin: 
Catch invalid key_validation_class before instantiating UpdateColumnFamily
patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3102
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java

Revision 1163289 by xedin: 
Add validation that Keyspace names are case-insensitively unique
patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3066
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/ClientState.java
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java

Revision 1163281 by jbellis: 
merge from 0.7
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/InvalidRequestException.java
	/cassandra/branches/cassandra-0.8/conf/cassandra.yaml
	/cassandra/branches/cassandra-0.8/contrib
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SuperColumn.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/NotFoundException.java
	/cassandra/branches/cassandra-0.8
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Column.java

Revision 1163268 by jbellis: 
merge from 0.7
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/InvalidRequestException.java
	/cassandra/branches/cassandra-0.8/conf/cassandra.yaml
	/cassandra/branches/cassandra-0.8/contrib
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SuperColumn.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/NotFoundException.java
	/cassandra/branches/cassandra-0.8
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Column.java

Revision 1163235 by jake: 
merge from 0.7
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/InvalidRequestException.java
	/cassandra/branches/cassandra-0.8/contrib
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SuperColumn.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/NotFoundException.java
	/cassandra/branches/cassandra-0.8
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/GCInspector.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Column.java

Revision 1163205 by jbellis: 
update CHANGES for #3023 and #3044
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/utils/BloomFilter.java
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/utils/BloomCalculations.java
{panel}

","Cassandra-0.8 branch, nightly builds.
MacOS and Debian",stuhood,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4083,,,Fri Sep 02 07:22:58 UTC 2011,,,,,,,,,,"0|i0gfyv:",94017,,,,,Low,,,,,,,,,,,,,,,,,"01/Sep/11 19:04;brandon.williams;git bisect blames http://svn.apache.org/viewvc?view=rev&revision=1163205

The only obvious candidate there is the added debug statement, and indeed removing it allows the test to pass again.  Since the test passes in under 2 minutes but the timeout is 5, this is likely the per-row BF logging causing it.  Jonathan suggested that we move to the async appender: http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/AsyncAppender.html;;;","01/Sep/11 19:10;brandon.williams;For now we decided the simplest thing to do is push the log statement to TRACE.  Done in r1164213.;;;","02/Sep/11 07:22;hudson;Integrated in Cassandra-0.8 #310 (See [https://builds.apache.org/job/Cassandra-0.8/310/])
    Push BF debug logging to trace to fix LongCompactionSpeedTest timeouts.
Patch by brandonwilliams for CASSANDRA-3115

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1164213
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/utils/BloomFilter.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
After Choosing EC2Snitch you can't migrate off w/o a full cluster restart,CASSANDRA-3114,12520973,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,bcoverston,bcoverston,31/Aug/11 20:40,16/Apr/19 09:32,14/Jul/23 05:52,16/Nov/11 21:47,,,,,,,1,,,,"Once you choose the Ec2Snitch the gossip messages will trigger this exception if you try to move (for example) to the property file snitch:

ERROR [pool-2-thread-11] 2011-08-30 16:38:06,935 Cassandra.java (line 3041) Internal error processing get_slice 
java.lang.NullPointerException 
at org.apache.cassandra.locator.Ec2Snitch.getDatacenter(Ec2Snitch.java:84) 
at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:122) 
at org.apache.cassandra.service.DatacenterReadCallback.assureSufficientLiveNodes(DatacenterReadCallback.java:77) 
at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:516) 
at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:480) 
at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:109) 
at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:263) 
at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:345) 
at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:306) 
at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:3033) 
at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889) 
at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
at java.lang.Thread.run(Thread.java:662)",,brandon.williams,cywjackson,sdolgy,tnine,tvachon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3186,,CASSANDRA-3186,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,1873,,,Wed Nov 16 21:47:54 UTC 2011,,,,,,,,,,"0|i0gfyf:",94015,,,,,Normal,,,,,,,,,,,,,,,,,"31/Aug/11 21:16;brandon.williams;I'm not sure there's a good solution here.  We could make PFEPS inject the local nodes dc/rack info into gossip similar to what I suggested in CASSANDRA-1974, but you'd still have to name things with the ec2snitch conventions for things to not break, and it would be very PFEPS-specific; other snitches are out of the question.

Ultimately I'm inclined to say you need to choose your snitch like you choose your partitioner: very carefully.;;;","07/Sep/11 19:01;cywjackson;What if do this in the Abstract?

{code:title=AbstractEndpointSnitch.java}
    public void gossiperStarting()
    {
    	String dc = getDatacenter(FBUtilities.getBroadcastAddress());
    	String rack = getRack(FBUtilities.getBroadcastAddress());
    	logger.info(this.getClass().getSimpleName() +"" adding ApplicationState DC="" + dc + "" Rack="" + rack);
        Gossiper.instance.addLocalApplicationState(ApplicationState.DC, StorageService.instance.valueFactory.datacenter(dc));
        Gossiper.instance.addLocalApplicationState(ApplicationState.RACK, StorageService.instance.valueFactory.rack(rack));
    }
{code};;;","07/Sep/11 19:04;brandon.williams;I don't see how making your dc/rack names your external IP address is going to solve anything.;;;","07/Sep/11 19:04;cywjackson;""but you'd still have to name things with the ec2snitch conventions for things to not break"" still hold true with the above.;;;","15/Sep/11 23:32;cywjackson;""I don't see how making your dc/rack names your external IP address is going to solve anything.""

well the NPE was on 
{code}
return Gossiper.instance.getEndpointStateForEndpoint(endpoint).getApplicationState(ApplicationState.DC).value;
{code}

the given endpoint is not the local address; its the address from ""other"" nodes. For those ""other"" nodes, if they are not using the Ec2Snitch, which would have populated the ""ApplicationState.DC"" and ""ApplicationState.RACK"" with the values, getApplicationState(ApplicationState.DC) (and getApplicationState(ApplicationState.RACK) for that matter) is going to be return null. Hence you got a NPE from that line on .value.

Defaulting the AbstractEndpointSnitch's gossiperStarting by populating the ApplicationState.DC,ApplicationState.RACK wll help then any snitch relying the gossip info to getDC and getRack.
;;;","16/Sep/11 02:04;brandon.williams;bq. Defaulting the AbstractEndpointSnitch's gossiperStarting by populating the ApplicationState.DC,ApplicationState.RACK wll help then any snitch relying the gossip info to getDC and getRack.

Yes, but setting DC to 'foo' and rack to 'bar' just creates a new DC and rack and breaks the replication policy and consistency guarantees.;;;","16/Nov/11 21:47;brandon.williams;Closing, see CASSANDRA-3186;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in ReversedType comparator,CASSANDRA-3111,12520907,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,frousseau,frousseau,31/Aug/11 10:35,16/Apr/19 09:32,14/Jul/23 05:52,31/Aug/11 16:57,0.8.5,,,,,,0,,,,"Scenario :
 * create a cf with a reversed comparator
 * insert a few columns in a row
 * try to read data with : SliceRange(start='', finish='', reversed=true)
 ** no data is returned, but some columns are expected
 * try to read data with : SliceRange(start='', finish='', reversed=false)
 ** if not flushed on disk : no data is returned


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/11 10:40;frousseau;001-CASSANDRA-3111.patch;https://issues.apache.org/jira/secure/attachment/12492441/001-CASSANDRA-3111.patch",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4085,,,Wed Aug 31 17:16:02 UTC 2011,,,,,,,,,,"0|i0gfxj:",94011,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"31/Aug/11 16:57;slebresne;+1

Committed with an added unit test. Thanks;;;","31/Aug/11 17:16;hudson;Integrated in Cassandra-0.8 #306 (See [https://builds.apache.org/job/Cassandra-0.8/306/])
    Fix handling of the empty byte buffer by ReversedType
patch by frousseau; reviewed by slebresne for CASSANDRA-3111

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163695
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/marshal/ReversedType.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTables iterators are closed and before being used,CASSANDRA-3110,12520904,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,31/Aug/11 10:22,16/Apr/19 09:32,14/Jul/23 05:52,31/Aug/11 14:57,1.0.0,,,,,,0,,,,"Seems there is misplaced finally blocks in CollationController: we close the sstable iterators and release the sstable references *before* having actually used said iterators/sstables.
Note: this cause a lot of tests to fail in `ant test-compression` in particular.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/11 10:22;slebresne;3110.patch;https://issues.apache.org/jira/secure/attachment/12492440/3110.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4086,,,Wed Aug 31 15:19:21 UTC 2011,,,,,,,,,,"0|i0gfx3:",94009,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"31/Aug/11 13:17;jbellis;+1;;;","31/Aug/11 14:57;jbellis;(committed);;;","31/Aug/11 15:19;hudson;Integrated in Cassandra #1060 (See [https://builds.apache.org/job/Cassandra/1060/])
    Fix closing sstable iterators before using them
patch by slebresne; reviewed by jbellis for CASSANDRA-3110

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163652
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/CollationController.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Range and Bounds objects client-safe,CASSANDRA-3108,12520830,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mck,jbellis,jbellis,30/Aug/11 20:43,16/Apr/19 09:32,14/Jul/23 05:52,30/Aug/11 21:16,0.8.5,,,,,,0,hadoop,,,"From Mck's comment on CASSANDRA-1125:

Something broke here in production once we went out with 0.8.2. It may have been some poor testing, i'm not entirely sure and a little surprised.

CFIF:135 breaks because inside dhtRange.intersects(jobRange) there's a call to new Range(token, token) which calls StorageService.getPartitioner() and StorageService is null as we're not inside the server.

A quick fix is to change Range:148 from new Range(token, token) to new Range(token, token, partitioner) making the presumption that the partitioner for the new Range will be the same as this Range.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/11 20:45;jbellis;3108.txt;https://issues.apache.org/jira/secure/attachment/12492363/3108.txt",,,,,,,,,,,,,,1.0,mck,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4088,,,Wed Sep 07 09:37:04 UTC 2011,,,,,,,,,,"0|i0gfw7:",94005,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"30/Aug/11 20:45;jbellis;patch to make all internal Range and Bounds construction use the 3-arg constructor.
;;;","30/Aug/11 21:11;mck;Tested in production. 
+1;;;","30/Aug/11 21:16;jbellis;committed, thanks!;;;","30/Aug/11 22:25;hudson;Integrated in Cassandra-0.8 #305 (See [https://builds.apache.org/job/Cassandra-0.8/305/])
    make Range and Bounds objects client-safe
patch by Mck SembWever and jbellis for CASSANDRA-3108

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163394
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/dht/Bounds.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/dht/Range.java
;;;","05/Sep/11 11:33;mck;Didn't see it until now but your patch Jonathan removes the limitation that ConfigHelper's InputKeyRange cannot wrap.
I've entered CASSANDRA-3137 to allow wrapping intersections in {{ColumnFamilyInputFormat}}.;;;","05/Sep/11 19:19;jbellis;That was unintentional -- how did I do that?;;;","05/Sep/11 19:40;mck;You drastically removed the usage of the {{Range(left, right)}} constructor so that even the usage of {{intersectionBothWrapping(..)}} and {{intersectionOneWrapping(..)}} avoids any server-side calls.

In CFIF there AFAIK doesn't seem any other limitation to wrapping ranges...;;;","06/Sep/11 21:28;jbellis;bq. even the usage of intersectionBothWrapping(..) and intersectionOneWrapping(..) avoids any server-side calls

Right.  What I don't follow is how this changes behavior of allowing wrapping ranges?;;;","07/Sep/11 09:37;mck;No, in itself it doesn't implement wrapping ranges. But by making intersectionBothWrapping(..) and intersectionOneWrapping(..) client safe it makes it possible to implement.
I think the best explanation to your question Jonathan is to look at the patch available in CASSANDRA-3137;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getRangeToEndpointMap() method removed,CASSANDRA-3106,12520793,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,nickmbailey,nickmbailey,nickmbailey,30/Aug/11 19:31,16/Apr/19 09:32,14/Jul/23 05:52,31/Aug/11 19:57,1.0.0,,,Legacy/Tools,,,0,,,,"When getRangeToRPCAddress was added, getRangeToEndpointMap was removed, however, both are useful. We should add it back.",,brandon.williams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/11 19:32;nickmbailey;0001-Add-getRangeToEndpointMap-method-back.patch;https://issues.apache.org/jira/secure/attachment/12492287/0001-Add-getRangeToEndpointMap-method-back.patch",,,,,,,,,,,,,,1.0,nickmbailey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,4090,,,Wed Aug 31 20:54:32 UTC 2011,,,,,,,,,,"0|i0gfs7:",93987,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"30/Aug/11 19:32;nickmbailey;Adds back getRangeToEndpointMap method.

Incidentally fixed trailing whitespace in both files since my vim setup does that automatically.;;;","31/Aug/11 14:58;jbellis;Why isn't rpcaddress sufficient?;;;","31/Aug/11 15:38;nickmbailey;The rpc address of a node doesn't really give any guarantee of uniqueness.;;;","31/Aug/11 19:57;brandon.williams;Committed.;;;","31/Aug/11 20:54;hudson;Integrated in Cassandra #1063 (See [https://builds.apache.org/job/Cassandra/1063/])
    Restore getRangeToEndpointMap.
Patch by Nick Bailey, reviewed by brandonwilliams for CASSANDRA-3106

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163772
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageServiceMBean.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
catch invalid key_validation_class before instantiating UpdateColumnFamily,CASSANDRA-3102,12520679,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,30/Aug/11 03:05,16/Apr/19 09:32,14/Jul/23 05:52,30/Aug/11 16:58,0.8.5,,,Legacy/CQL,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/11 03:06;jbellis;3102.txt;https://issues.apache.org/jira/secure/attachment/12492189/3102.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20970,,,Tue Aug 30 17:21:58 UTC 2011,,,,,,,,,,"0|i0gfqf:",93979,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"30/Aug/11 03:06;jbellis;one-line fix attached;;;","30/Aug/11 16:58;xedin;Committed.;;;","30/Aug/11 17:21;hudson;Integrated in Cassandra-0.8 #304 (See [https://builds.apache.org/job/Cassandra-0.8/304/])
    Catch invalid key_validation_class before instantiating UpdateColumnFamily
patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3102

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163291
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should check for errors when calling /bin/ln,CASSANDRA-3101,12520615,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,thepaul,thepaul,29/Aug/11 18:39,16/Apr/19 09:32,14/Jul/23 05:52,14/Dec/11 21:44,1.0.7,,,,,,0,lhf,,,"It looks like cassandra.utils.CLibrary.createHardLinkWithExec() does not check for any errors in the execution of the hard-link-making utility. This could be bad if, for example, the user has put the snapshot directory on a different filesystem from the data directory. The hard linking would fail and the sstable snapshots would not exist, but no error would be reported.

It does look like errors with the more direct JNA link() call are handled correctly- an exception is thrown. The WithExec version should probably do the same thing.

Definitely it would be enough to check the process exit value from /bin/ln for nonzero in the *nix case, but I don't know whether 'fsutil hardlink create' or 'cmd /c mklink /H' return nonzero on failure.

For bonus points, use any output from the Process's error stream in the text of the exception, to aid in debugging problems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/11 18:54;vijay2win@yahoo.com;0001-0001-throw-IOE-while-calling-bin-ln-v2.patch;https://issues.apache.org/jira/secure/attachment/12507031/0001-0001-throw-IOE-while-calling-bin-ln-v2.patch","14/Dec/11 17:11;vijay2win@yahoo.com;0001-3101-throw-IOE-while-calling-bin-ln-v3.patch;https://issues.apache.org/jira/secure/attachment/12507379/0001-3101-throw-IOE-while-calling-bin-ln-v3.patch","31/Oct/11 21:50;vijay2win@yahoo.com;0001-3101-throw-IOE-while-calling-bin-ln.patch;https://issues.apache.org/jira/secure/attachment/12501687/0001-3101-throw-IOE-while-calling-bin-ln.patch",,,,,,,,,,,,3.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,1876,,,Thu Jan 12 22:30:36 UTC 2012,,,,,,,,,,"0|i0gfpz:",93977,,thepaul,,thepaul,Low,,,,,,,,,,,,,,,,,"31/Oct/11 21:50;vijay2win@yahoo.com;To test: 
link snapshots to a different device
remove jni lib
run nt snapshot;;;","09/Dec/11 23:24;thepaul;The following imports appear to be unused, and should probably be removed:

{code}
import java.io.BufferedInputStream;
import java.io.DataInputStream;
import java.io.InputStream;
import sun.misc.IOUtils;
{code}

Also, why add the logger.error() call in createHardLink, but not in createHardLinkWithExec? seems like if we want to know about the error in the C* log in the first case, we'd also want to know about it in the second case.

As a less important point, I wonder if it isn't worth modifying the existing hard-linking code a bit: ""cmd /c mklink /H"" seems a much more clunky way to make a hard link, and I think the logic is backwards: ""fsutil hardlink create"" is supported in all Windows versions since Windows Server 2000, except XP. Maybe the osversion comparison was meant to use cmd /c mklink /H in the ""Windows XP or less"" case, not ""Windows Vista or later"".

Tested on Windows 7, though, after mounting a USB drive at the ""snapshots"" dir and trying to snapshot that keyspace. Error showed up correctly with output to nodetool, although no errors showed in the c* log.;;;","11/Dec/11 17:06;vijay2win@yahoo.com;Hi Paul, i dont have windows machine where i can test the command on.... Do you mind giving me the command so i can just add it? I will fix the remaining...;;;","12/Dec/11 16:41;thepaul;Vijay- I meant that the ""fsutil hardlink create"" command (which is already in the code) looks like the right one to use in all cases except Windows XP, but since we don't have a good way to verify, we might as well leave that part alone.

Let's just get rid of the unused imports and add the extra logger.error() call.;;;","12/Dec/11 18:54;vijay2win@yahoo.com;Hi Paul, attached has the fix. Thanks!;;;","13/Dec/11 18:50;thepaul;This works, except you've taken out a logger.error() call instead of adding another one. I think it's worth logging an error for the cassandra log in both cases.;;;","14/Dec/11 17:11;vijay2win@yahoo.com;I miss read the previous message hence removed it sorry, plz find the updated. thanks!;;;","14/Dec/11 18:25;thepaul;awesome.

+1;;;","14/Dec/11 21:44;xedin;Committed.;;;","12/Jan/12 22:19;jbellis;bq. ""cmd /c mklink /H"" seems a much more clunky way to make a hard link, and I think the logic is backwards: ""fsutil hardlink create"" is supported in all Windows versions since Windows Server 2000, except XP

mklink does not require admin privileges, so that's the preferred method, but it was introduced in Vista.

fsutil is present on XP+ so we use it as a fallback: http://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/fsutil.mspx?mfr=true

;;;","12/Jan/12 22:30;thepaul;aha, thanks for clarification.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counters are not always hinted,CASSANDRA-3099,12520576,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,29/Aug/11 14:29,16/Apr/19 09:32,14/Jul/23 05:52,29/Aug/11 15:08,0.8.5,,,,,,0,,,,"CASSANDRA-2892 mistakenly removed some hints for counters, namely the hints that were supposed to be stored on the local node (that is, instead of removing from the hintedEndpoints multimap only the local write (since it has been already applied), we were removing everything having the local node as destination).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/11 14:29;slebresne;0001-hint-counters.patch;https://issues.apache.org/jira/secure/attachment/12492076/0001-hint-counters.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20969,,,Mon Aug 29 16:23:13 UTC 2011,,,,,,,,,,"0|i0gfov:",93972,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"29/Aug/11 14:38;jbellis;+1;;;","29/Aug/11 15:08;slebresne;Committed, thanks;;;","29/Aug/11 16:23;hudson;Integrated in Cassandra-0.8 #298 (See [https://builds.apache.org/job/Cassandra-0.8/298/])
    Always hint counters
patch by slebresne; reviewed by jbellis for CASSANDRA-3099

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1162844
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageProxy.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test RoundRobinScheduler timeouts,CASSANDRA-3096,12520496,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,28/Aug/11 22:56,16/Apr/19 09:32,14/Jul/23 05:52,01/Sep/11 03:14,1.0.0,,,Legacy/CQL,,,0,,,,"CASSANDRA-3079 was very hasty, and introduced two bugs that would: 1) cause the scheduler to busywait after a timeout, 2) never actually throw timeouts. This calls for a test.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3079,,,,,,"28/Aug/11 23:11;stuhood;0001-Properly-throw-timeouts-decrement-the-count-of-waiters.txt;https://issues.apache.org/jira/secure/attachment/12492017/0001-Properly-throw-timeouts-decrement-the-count-of-waiters.txt",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20967,,,Thu Sep 01 07:27:27 UTC 2011,,,,,,,,,,"0|i0gfnj:",93966,,kingryan,,kingryan,Normal,,,,,,,,,,,,,,,,,"28/Aug/11 22:58;stuhood;0001 - Properly throw timeouts from WeightedQueue, decrement the count of waiters on timeout, fix off-by-one in taskCount, and test all of it.;;;","31/Aug/11 17:59;kingryan;looks good, +1;;;","01/Sep/11 03:14;jbellis;committed;;;","01/Sep/11 07:27;hudson;Integrated in Cassandra #1064 (See [https://builds.apache.org/job/Cassandra/1064/])
    Properly throw timeouts, decrement the count of waiters on timeout, fix off-by-one in taskCount
patch by Stu Hood; reviewed by Ryan King for CASSANDRA-3096

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163898
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/scheduler/RoundRobinScheduler.java
* /cassandra/trunk/src/java/org/apache/cassandra/scheduler/WeightedQueue.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leveled compaction allows multiple simultaneous compaction Tasks,CASSANDRA-3087,12520318,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,26/Aug/11 22:54,16/Apr/19 09:32,14/Jul/23 05:52,31/Aug/11 16:32,1.0.0,,,,,,0,lcs,,,"CASSANDRA-1608 attempts to restrict itself to one compaction task per CF (see discussion there for why this is necessary) by synchronizing LCS.getBackgroundTasks but this is not sufficient.  Consider this sequence of events:

1. getBackgroundTasks returns a Task for compacting some L0 sstables.  this Task is scheduled.
2. Another SSTable for this CF is flushed, so CompactionManager.submitBackground is called.  getBT is not currently in-progress so the synchronization does not stop another Task from being returned and scheduled.",,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/11 15:56;bcoverston;3087-v2.txt;https://issues.apache.org/jira/secure/attachment/12492469/3087-v2.txt","27/Aug/11 03:03;jbellis;3087.txt;https://issues.apache.org/jira/secure/attachment/12491870/3087.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20964,,,Wed Aug 31 17:16:48 UTC 2011,,,,,,,,,,"0|i0gfin:",93944,,bcoverston,,bcoverston,Normal,,,,,,,,,,,,,,,,,"27/Aug/11 03:03;jbellis;Fix makes getBT wait for the current Task in that CF, if any, before returning a new one.;;;","31/Aug/11 15:56;bcoverston;Cleaned up a few of the imports.

+1;;;","31/Aug/11 16:32;jbellis;committed;;;","31/Aug/11 17:16;hudson;Integrated in Cassandra #1061 (See [https://builds.apache.org/job/Cassandra/1061/])
    fix race that allowed multiple simultaneous leveled compaction tasks
patch by jbellis; reviewed by Ben Coverston for CASSANDRA-3087

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163688
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/LeveledCompactionTask.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in sstable reference counting,CASSANDRA-3085,12520293,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,jbellis,jbellis,26/Aug/11 19:22,16/Apr/19 09:32,14/Jul/23 05:52,29/Aug/11 20:29,1.0.0,,,,,,0,,,,"DataTracker gives us an atomic View of memtable/sstables, but acquiring references is not atomic.  So it is possible to acquire references to an SSTableReader object that is no longer valid, as in this example:

View V contains sstables {A, B}.  We attempt a read in thread T using this View.
Meanwhile, A and B are compacted to {C}, yielding View W.  No references exist to A or B so they are cleaned up.
Back in thread T we acquire references to A and B.  This does not cause an error, but it will when we attempt to read from them next.",,scode,yangyangyyy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/11 21:58;jbellis;3085-v2.txt;https://issues.apache.org/jira/secure/attachment/12492015/3085-v2.txt","26/Aug/11 20:59;jbellis;3085.txt;https://issues.apache.org/jira/secure/attachment/12491836/3085.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20962,,,Sat Sep 17 00:26:54 UTC 2011,,,,,,,,,,"0|i0gfhr:",93940,,slebresne,,slebresne,Critical,,,,,,,,,,,,,,,,,"26/Aug/11 20:59;jbellis;Turns out this is only a problem for where we're operating on sub-View granularity post-CASSANDRA-1608: CFS.markCurrentViewReferenced was correctly retrying until it got a consistent set of references.;;;","28/Aug/11 21:56;jbellis;v2 encapsulates the lockless atomic acquisition in CFS.markReferenced(Interval).

Not 100% sure how important the changes to the getRangeSlice tokens were, that I took out. :)

If we need those, we might need to make getRangeSlice loop manually w/o the encapsulation, since we need the view to compute the Interval, but we need the Interval to search for sstables.;;;","29/Aug/11 15:55;slebresne;* The try of the try...finally block should start just after the markReferenced to be safe in CollationControler.
* In CFS.getRangeSlice, I am not confortable using the same try...finally block for both the view and the iterator. RowIteratorFactory.getIterator() does make seeks and could throw an error that would leave a bunch of sstable referenced.
* Nit: Maybe we could use a plain old DataTracker.View instead of introducing ViewFragment, since the View constructor is accessible to CFS anyway ?;;;","29/Aug/11 16:02;jbellis;It seems weird to me to overload the purpose of a full View for this: it's not meant to be updated, nor does it contain the full set of sstables.  I think I'd rather use a different class so that View's purpose remains clear.;;;","29/Aug/11 16:16;slebresne;I would argue that even now a View is not meant to be updated either (we pick new Views but a View itself is fixed), nor does it really ensure that it contains the full set of sstables in a way since that set is a moving target. But I suppose this is just a matter of perspective on what View is, and I'm fine with ViewFragment. ;;;","29/Aug/11 20:29;jbellis;committed with requested fixes, fix for interval computation involving stopAt.isEmpty(), and javadoc for CFS.markReferenced;;;","29/Aug/11 21:17;hudson;Integrated in Cassandra #1055 (See [https://builds.apache.org/job/Cassandra/1055/])
    fix race condition in sstable reference counting
patch by jbellis; reviewed by slebresne for CASSANDRA-3085

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1162988
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/CollationController.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/RowIteratorFactory.java
;;;","14/Sep/11 23:17;yangyangyyy;bq. but it will when we attempt to read from them next.

Jonathan: since the sstable deletion code is wrapped inside SSTableReader.releaseReference(), I thought as long as anyone is holding a reference to the SSTableReader, the file would not be deleted? could you please explain a bit?

Thanks
Yang;;;","14/Sep/11 23:58;jbellis;Right.  A sstable starts with one reference.  When we do a read, we acquire a reference, then release when we're done.  When we compact it, we release a reference.  

So we can do arbitrary numbers of reads w/o reference count getting to zero, but once we compact, either the compact release or a read release will drop it to zero. This last release will unmap and delete it, whether from compaction or a read.;;;","15/Sep/11 08:37;yangyangyyy;Thanks Jonathan.

but I still can't see why the old code would cause errors, could you please see if the following reasoning makes sense?



if you look at the operations of +1 and -1 by read paths and the compaction path, either the read path or the compaction can be seen as the following sequence

+1

//access the SSTableReader

-1


where for the compaction the ""+1"" happens at creation of the SSTableReader; for read paths the ""+1"" happens at acquireReference()

since every path (either compaction or reader) does one +1 and one -1, by the time a path finishes, the ref count will be equal to the number of live code paths

if the file is removed, the ref count must be 0, hence live paths count at that moment must be 0. if there are no future paths to run, it's all good. if there are , the path would access a file already removed and we have a problem. but this is impossible because: if  the +1 comes after compaction.release(), because *compaction.release() comes after the view change in DataTracker.replace()*, then reader path +1 comes after DataTracker.replace(),  but this is impossible because the read path can not see that SSTableReader in its view.

;;;","15/Sep/11 19:59;jbellis;bq. since every path (either compaction or reader) does one +1 and one -1

Not so -- you're missing the ""this sstable is compacted and is obsolete"" release done by DataTracker.;;;","16/Sep/11 07:35;yangyangyyy;
bq.  Not so – you're missing the ""this sstable is compacted and is obsolete"" release done by DataTracker.


do you mean this part in DataTracker ? (line 262)

    private void replace(Collection<SSTableReader> oldSSTables, Iterable<SSTableReader> replacements)
    {
        View currentView, newView;
        do
        {
            currentView = view.get();
            newView = currentView.replace(oldSSTables, replacements);
        }
        while (!view.compareAndSet(currentView, newView));

        addNewSSTablesSize(replacements);
        removeOldSSTablesSize(oldSSTables); //<==== this calls releaseReference() 

        notifySSTablesChanged(replacements, oldSSTables);
        cfstore.updateCacheSizes();
    }

the SSTableReaders which do a releaseReference() are all in the ""oldSSTables"" set, the newView takes out all the oldSSTables in the newView computation. Since the view change is done atomically in view.compareAndSet(), each SSTableReader can invoke the releaseReference() only once in this path, since once it's called releaseReference(), it must have been removed from the view.

so, for this path, each SSTableReader can invoke the releaseReference() once in its life time. that's what I   mean: you can ""view"" the compaction path as  doing  a ""+"" at the construction of SSTableReader, and doing a corresponding ""-"" at the code above.


also 
StreamingOutSession.close() calls releaseReference() without acquireReference(), would that cause a problem ? similar calls are StreamingOut.createPendingFiles() , StreamingOutSession.startNext()


Thanks a lot for your patience, please pardon my numerous questions, I just want to make sure that I thoroughly understand it and there are no hidden issues.
;;;","16/Sep/11 08:39;slebresne;bq. but I still can't see why the old code would cause errors

I don't think it was, the report of this issue is not totally correct, but the code was fairly ugly: badly encapsulated and potentially more inefficient that it needs to.

bq. StreamingOutSession.close() calls releaseReference() without acquireReference(), would that cause a problem ? similar calls are StreamingOut.createPendingFiles() , StreamingOutSession.startNext()

It's not a problem because those releaseReference calls do are paired with acquireReference calls (or rather they are supposed to be and last time I checked all seemed ok). The only thing is that in that case, the acquireReference calls are unfortunately not near (from a code point of view) to their paired releaseReference. But we have no choice with how the code of streaming is structured right now. Typically for a streamOutSession, references are acquired in either StreamingRepairTask or StreamOut.transferRanges and released as soon as the sstable is not needed, that is either in createPendingFiles() if it happens the sstable has none of the ranges we want to stream or in next()/close() when the sstable has been fully streamed (and thus won't been accessed by this streamOutSession).

The only small annoying thing with Streaming is that since acquire/release is not enclosed within a try ... finally block, we could leave a sstable marked forever if an error occurs during streaming. CASSANDRA-3112 includes code that would basically allow to deal with this. 
;;;","17/Sep/11 00:26;yangyangyyy;thanks guys, I'm clear now;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
o.a.c.dht.Range.differenceToFetch() doesn't handle all cases correctly,CASSANDRA-3084,12520286,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,thobbs,thobbs,26/Aug/11 19:06,16/Apr/19 09:32,14/Jul/23 05:52,30/Aug/11 05:50,0.8.5,,,,,,0,,,,"It's possible that differenceToFetch is making implicit assumptions about the relationship between the two ranges, but the following cases are not handled correctly (the old range is (A, B], the new is (C, D]:

{noformat}
--C--A-----B--D--
{noformat}

Here, the result will be (C, A] and (D, B], instead of (C, A] and (B, D].

{noformat}
--C--A-----D--B--
{noformat}

The result will be (C, D] instead of just (C, A].

{noformat}
--A--C-----D--B--
{noformat}

The result will be (B, D] when nothing needs to be transfered.

If there is some kind of implicit assumption that these cases won't arise, it either needs to be explicit (assertions, exceptions) or the cases need to be handled.  It should be easy to cover this with unit tests.",,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/11 17:33;thobbs;3084-unit-test.txt;https://issues.apache.org/jira/secure/attachment/12492101/3084-unit-test.txt","29/Aug/11 19:17;thobbs;3084-v2.txt;https://issues.apache.org/jira/secure/attachment/12492114/3084-v2.txt","29/Aug/11 17:33;thobbs;3084.txt;https://issues.apache.org/jira/secure/attachment/12492102/3084.txt",,,,,,,,,,,,3.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20961,,,Tue Aug 30 06:15:16 UTC 2011,,,,,,,,,,"0|i0gfhb:",93938,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"29/Aug/11 17:33;thobbs;3084.txt covers all cases of possible range differences.

3084-unit-test.txt provides unit tests that fail before the patch.

The logic is definitely a bit hairy here, so any thoughts on how to simplify it are welcome.;;;","29/Aug/11 18:15;stuhood;Couldn't a bunch of difference cases be eliminated by taking advantage of the intersection implementation? The difference between ranges x and y would be {{z = x.intersect\(y\); y.subtract(z)}}, and I think a subtract method with a ""y must contain z"" precondition would be much easier to implement.;;;","29/Aug/11 19:17;thobbs;Good idea!

3084-v2.txt simplifies the logic by adding a subtractContained() method.;;;","30/Aug/11 05:16;stuhood;+1 Awesome.
_nitpick: brackets should go on new lines in differenceToFetch_;;;","30/Aug/11 05:50;jbellis;committed with braces fixed.  thanks Tyler and Stu!;;;","30/Aug/11 06:15;hudson;Integrated in Cassandra-0.8 #300 (See [https://builds.apache.org/job/Cassandra-0.8/300/])
    fix corner cases in Range.differenceToFetch
patch by Tyler Hobbs; reviewed by Stu Hood for CASSANDRA-3084

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163090
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/dht/Range.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/dht/RangeTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operation with CL=EACH_QUORUM doesn't succeed when a replica is down (RF=3),CASSANDRA-3082,12520268,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,patricioe,patricioe,patricioe,26/Aug/11 16:17,16/Apr/19 09:32,14/Jul/23 05:52,26/Aug/11 21:06,0.7.9,0.8.5,,,,,0,consistency,,,"{code}  DatacenterSyncWriteResponseHandler#assureSufficientLiveNodes()
     ...
     ...
        // Throw exception if any of the DC doesn't have livenodes to accept write.
        for (String dc: strategy.getDatacenters())
        {
        	if (dcEndpoints.get(dc).get() != responses.get(dc).get())
                throw new UnavailableException();
        }
{code}

should be:
 
{code}
      if (dcEndpoints.get(dc).get() < responses.get(dc).get())
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/11 19:17;patricioe;CASSANDRA-0.7-3084.txt;https://issues.apache.org/jira/secure/attachment/12491823/CASSANDRA-0.7-3084.txt",,,,,,,,,,,,,,1.0,patricioe,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20960,,,Fri Aug 26 21:21:23 UTC 2011,,,,,,,,,,"0|i0gfgf:",93934,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"26/Aug/11 19:18;patricioe;It passed a manual test using CCM with 6 nodes.

DC1:3
DC2:3

Originally, with all replicas up and running, the insert with CL=EACH_QUORUM failed.

The patch addresses that issue.;;;","26/Aug/11 21:06;jbellis;committed;;;","26/Aug/11 21:21;hudson;Integrated in Cassandra-0.7 #545 (See [https://builds.apache.org/job/Cassandra-0.7/545/])
    fix UnavailableException with writes at CL.EACH_QUORM
patch by Patricio Echague; reviewed by jbellis for CASSANDRA-3082

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1162255
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/DatacenterSyncWriteResponseHandler.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in new GCInspector log,CASSANDRA-3076,12520117,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,25/Aug/11 14:26,16/Apr/19 09:32,14/Jul/23 05:52,31/Aug/11 20:05,0.7.9,0.8.5,,,,,0,,,,Small regression from CASSANDRA-2868,Lion OSX,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/11 14:29;tjake;3076.txt;https://issues.apache.org/jira/secure/attachment/12491637/3076.txt","31/Aug/11 19:51;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3076-avoid-div-by-zero.txt;https://issues.apache.org/jira/secure/attachment/12492514/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3076-avoid-div-by-zero.txt",,,,,,,,,,,,,2.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20959,,,Wed Aug 31 20:05:02 UTC 2011,,,,,,,,,,"0|i0gfdr:",93922,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"25/Aug/11 14:27;tjake;{code}
    [junit] ERROR 10:20:16,755 Fatal exception in thread Thread[ScheduledTasks:1,5,main]
    [junit] java.lang.AssertionError
    [junit] 	at org.apache.cassandra.service.GCInspector.logGCResults(GCInspector.java:110)
    [junit] 	at org.apache.cassandra.service.GCInspector.access$000(GCInspector.java:41)
    [junit] 	at org.apache.cassandra.service.GCInspector$1.run(GCInspector.java:85)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
    [junit] 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:680)
    [junit] ------------- ---------------- ---------------
{code};;;","25/Aug/11 14:38;jbellis;The assert is basically saying ""if total gc time has increased, count should have increased as well.""

If that's valid, then the ""if (previousTotal.equals(total)) continue"" check should handle this.  If it's not, we should probably remove the assert entirely.
;;;","25/Aug/11 14:50;tjake;Right, I think it's likely a OSX lion thing.  Removing the assert works for me.;;;","25/Aug/11 15:38;jbellis;ok, done in 0.7.  (Brandon already did that in 0.8 in r1161167.);;;","26/Aug/11 13:09;tjake;New error due to the count being zero. The original patch fixes this.


{code}
  [junit] ERROR 09:06:26,417 Fatal exception in thread Thread[ScheduledTasks:1,5,main]
    [junit] java.lang.ArithmeticException: / by zero
    [junit] 	at org.apache.cassandra.service.GCInspector.logGCResults(GCInspector.java:117)
    [junit] 	at org.apache.cassandra.service.GCInspector.access$000(GCInspector.java:41)
    [junit] 	at org.apache.cassandra.service.GCInspector$1.run(GCInspector.java:85)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
    [junit] 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:680)
{code};;;","26/Aug/11 16:35;jbellis;+1 if you fix the spacing then;;;","30/Aug/11 14:35;tjake;committed;;;","30/Aug/11 14:57;hudson;Integrated in Cassandra-0.7 #546 (See [https://builds.apache.org/job/Cassandra-0.7/546/])
    Fix div by zero error
patch by tjake; reviewed by jbellis for CASSANDRA-3076

jake : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163234
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/GCInspector.java
;;;","31/Aug/11 19:28;jbellis;Still seeing this post-patch in trunk:

{noformat}
    [junit] ERROR 14:23:17,606 Fatal exception in thread Thread[ScheduledTasks:1,5,main]
    [junit] java.lang.ArithmeticException: / by zero
    [junit] 	at org.apache.cassandra.service.GCInspector.logGCResults(GCInspector.java:119)
    [junit] 	at org.apache.cassandra.service.GCInspector.access$000(GCInspector.java:41)
    [junit] 	at org.apache.cassandra.service.GCInspector$1.run(GCInspector.java:85)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
    [junit] 	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:680)
{noformat};;;","31/Aug/11 19:54;jbellis;+1 new patch;;;","31/Aug/11 20:05;tjake;committed ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cassandra CLI unable to use list command with INTEGER column names, resulting in syntax error",CASSANDRA-3075,12520071,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,renatodasil,renatodasil,25/Aug/11 08:05,16/Apr/19 09:32,14/Jul/23 05:52,28/Aug/11 11:26,0.8.5,,,Legacy/Tools,,,0,features,newbie,,"I have a Column Family named 1105115.

I have inserted the CF with Hector, and it did not
throw any exception concerning the name of the
column.

If I am issuing the command

list 1105115;

I incur the following error:

[default@unknown] list 1105115;
Syntax error at position 5: mismatched input '1105115' expecting Identifier

I presume we are not to name CFs as integers?

 Or is there something I am missing from
the bellow help content:

[default@unknown] help list;
list <cf>;
list <cf>[<startKey>:];
list <cf>[<startKey>:<endKey>];
list <cf>[<startKey>:<endKey>] limit <limit>;

List a range of rows, and all of their columns, in the specified column
family.

The order of rows returned is dependant on the Partitioner in use.

Required Parameters:
- cf: Name of the column family to list rows from.

Optional Parameters:
- endKey: Key to end the range at. The end key will be included
in the result. Defaults to an empty byte array.

- limit: Number of rows to return. Default is 100.

- startKey: Key start the range from. The start key will be
included in the result. Defaults to an empty byte array.

Examples:
list Standard1;
list Super1[j:];
list Standard1[j:k] limit 40;

================================================

Column Family Info:

    ColumnFamily: 1105115
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.AsciiType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.5203125/111/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
","64 Bit Ubuntu 11.04(full update), AMD64 + 8GB RAM + 500GB Hdd, Java 1.6.0_26, Cassandra 0.8.0 + 4GB heap, Cassandra CLI",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3054,,,,,,,,"27/Aug/11 23:15;xedin;CASSANDRA-3075.patch;https://issues.apache.org/jira/secure/attachment/12491979/CASSANDRA-3075.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20958,,,Sun Aug 28 12:16:15 UTC 2011,,,,,,,,,,"0|i0a4f3:",56989,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"25/Aug/11 08:11;renatodasil;Column Family name edit on the extra info... sorry;;;","25/Aug/11 08:23;renatodasil;Issue with Column Families containing INTIGER names, where integer named column families cannot be listed due to Syntax error, withing Cassandra-cli, Cassandra ver 0.8.0.;;;","25/Aug/11 14:18;jbellis;One possible solution would be to allow quoting CF names.;;;","27/Aug/11 23:15;xedin;Made column family (and keyspace) support integer representation in all statements (create/update/set/get/list/drop/drop index/truncate/assume), only one limitation in here - when you use ""drop index on <cf>.<column>"" command if <cf> is numeric it should be put in quotes. Tests are updated to check if everything works correctly.

Rebased with cassandra-0.8 branch (last commit d9091c5aa88927ee2daa31ec69d865946b975fd9);;;","28/Aug/11 04:35;jbellis;+1;;;","28/Aug/11 11:26;xedin;Committed.;;;","28/Aug/11 12:16;hudson;Integrated in Cassandra-0.8 #297 (See [https://builds.apache.org/job/Cassandra-0.8/297/])
    Fix parsing of the Keyspace and ColumnFamily names in numeric and string representations in CLI
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3075

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1162495
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/Cli.g
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliCompiler.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/cli/CliTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
comments and documentation for index_interval are misleading,CASSANDRA-3074,12520009,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mdennis,mdennis,mdennis,24/Aug/11 19:24,16/Apr/19 09:32,14/Jul/23 05:52,25/Aug/11 19:06,0.8.5,,,,,,1,,,,"The comments and documentation for index_interval are misleading.  They state the larger the *sampling* the more effective the index as at the cost of space.  This is true, but in the context of the configuration variable it implies the larger the *setting* is the larger the index is while in fact it's the opposite of that.",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/11 18:46;mdennis;3074-cassandra-0.8.patch;https://issues.apache.org/jira/secure/attachment/12491675/3074-cassandra-0.8.patch",,,,,,,,,,,,,,1.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20957,,,Thu Aug 25 20:21:47 UTC 2011,,,,,,,,,,"0|i0gfd3:",93919,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"25/Aug/11 14:30;jbellis;the proposed changes conflate the index entries themselves (always one per key) and the sampling rate (which is what index_interval affects).;;;","25/Aug/11 18:45;mdennis;poor choice of words on my part.  new version attached.;;;","25/Aug/11 19:06;jbellis;committed, thanks!;;;","25/Aug/11 20:21;hudson;Integrated in Cassandra-0.8 #295 (See [https://builds.apache.org/job/Cassandra-0.8/295/])
    clarify index_interval explanation
patch by mdennis for CASSANDRA-3074

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1161701
Files : 
* /cassandra/branches/cassandra-0.8/conf/cassandra.yaml
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossip state is not removed after a new IP takes over a token,CASSANDRA-3071,12519848,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,23/Aug/11 16:59,16/Apr/19 09:32,14/Jul/23 05:52,23/Aug/11 18:11,0.7.9,0.8.5,,,,,0,,,,"When a new node takes over a token, the endpoint state in the gossiper is never removed for the old node.  ",,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/11 17:38;brandon.williams;3071-v2.txt;https://issues.apache.org/jira/secure/attachment/12491369/3071-v2.txt","23/Aug/11 17:00;brandon.williams;3071.txt;https://issues.apache.org/jira/secure/attachment/12491359/3071.txt",,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20956,,,Tue Aug 23 18:29:18 UTC 2011,,,,,,,,,,"0|i0gfbz:",93914,,,,,Low,,,,,,,,,,,,,,,,,"23/Aug/11 17:00;brandon.williams;This was originally part of a patch in CASSANDRA-957, but looks worthy enough to break out and get committed in older versions.;;;","23/Aug/11 17:10;jbellis;Is it possible for this to remove an endpoint that it shouldn't?

E.g., X has token T

X moves to token U but node N was down

N comes back up, thinks T and U are both owned by X

node Y takes token T

we remove X from gossip;;;","23/Aug/11 17:17;brandon.williams;bq. N comes back up, thinks T and U are both owned by X

I don't think this can happen.  When N starts up, it will load the persisted tokens, BUT they won't be associated with IPs.  It can only learn that U is owned by X via gossip, and T will be down until it learns about Y.;;;","23/Aug/11 17:21;jbellis;what behavior does this fix?  just gossip trying to reach the old node?;;;","23/Aug/11 17:29;brandon.williams;I think it solves this: http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Completely-removing-a-node-from-the-cluster-td6705079.html  Jeremy also reported a problem with a large amount of hints that I think this solves since SP.shouldHint is directly impacted by this.;;;","23/Aug/11 17:38;brandon.williams;v2 adds more protection around shouldHint by checking that the endpoint is a member.  ;;;","23/Aug/11 17:45;jbellis;If the node doesn't have a token, it doesn't matter if it's a gossip member, it won't be part of getWriteEndpoints and won't be hinted anyway.;;;","23/Aug/11 17:51;jbellis;+1 on v1, -1 on conflating gossip membership w/ token ownership as in v2;;;","23/Aug/11 18:11;brandon.williams;bq. If the node doesn't have a token, it doesn't matter if it's a gossip member, it won't be part of getWriteEndpoints and won't be hinted anyway.

I can't see a way for that either, but I'm still suspicious of the link to shouldHint.

bq. +1 on v1, -1 on conflating gossip membership w/ token ownership as in v2

Fair enough, committed v1.;;;","23/Aug/11 18:29;hudson;Integrated in Cassandra-0.7 #541 (See [https://builds.apache.org/job/Cassandra-0.7/541/])
    Remove gossip state when a new IP takes over a token.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-3071

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1160825
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/StorageService.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating a keyspace SYSTEM cause issue,CASSANDRA-3066,12519538,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,wajam,wajam,21/Aug/11 16:33,16/Apr/19 09:32,14/Jul/23 05:52,30/Aug/11 16:54,0.8.5,,,,,,0,,,,"It's possible to create a keyspace SYSTEM but impossible to do anything with it after.

I know naming a keyspace SYSTEM is probably not a good idea but I was testing something on a test cluster and found this bug. Step to reproduce:

connect localhost/9160;
create keyspace SYSTEM;
use SYSTEM;
create column family test
with comparator = UTF8Type and subcomparator = UTF8Type
and default_validation_class = UTF8Type
and column_metadata = [{column_name: title, validation_class: UTF8Type},
    {column_name: publisher, validation_class: UTF8Type}];

And you get:

system keyspace is not user-modifiable

Although SYSTEM keyspace have been created and is a different keyspace as system.",Windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/11 17:43;jbellis;3066-v2.txt;https://issues.apache.org/jira/secure/attachment/12491232/3066-v2.txt","21/Aug/11 20:45;wajam;CASSANDRA-3066-0.8-v1.patch;https://issues.apache.org/jira/secure/attachment/12491110/CASSANDRA-3066-0.8-v1.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20955,,,Tue Aug 30 17:21:58 UTC 2011,,,,,,,,,,"0|i0gf9z:",93905,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"21/Aug/11 20:46;wajam;If what we want is that the keyspace name is case-sensitive and system <> SYSTEM, patch attached! Otherwise, I have no clue :);;;","22/Aug/11 17:40;jbellis;The real problem is allowing the create in the first place.  v2 attached.  From the comments: 

{code}
        // keyspace names must be unique case-insensitively because the keyspace name beomes the directory
        // where we store CF sstables.  Names that differ only in case would thus cause problems on
        // case-insensitive filesystems (NTFS, most installations of HFS+).
{code}

(Also renamed the ListAccess methods to SchemaAccess.);;;","30/Aug/11 16:54;xedin;Committed.;;;","30/Aug/11 17:21;hudson;Integrated in Cassandra-0.8 #304 (See [https://builds.apache.org/job/Cassandra-0.8/304/])
    Add validation that Keyspace names are case-insensitively unique
patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3066

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163289
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/ClientState.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstable2json on an index sstable failed with NPE,CASSANDRA-3059,12519354,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,cywjackson,cywjackson,19/Aug/11 00:15,16/Apr/19 09:32,14/Jul/23 05:52,24/Aug/11 14:54,0.8.5,,,Legacy/Tools,,,0,,,,"$ ./bin/sstable2json /var/lib/cassandra-trunk/data/Keyspace1/Standard1.Idx1-h-1-Data.db 
{
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:74)
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:69)
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:64)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:147)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:87)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:71)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:177)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:142)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:134)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:304)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:335)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:348)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:406)


cfm is null for Index CF?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/11 02:37;jbellis;3059.txt;https://issues.apache.org/jira/secure/attachment/12490909/3059.txt","24/Aug/11 13:06;tjake;3059_v2.txt;https://issues.apache.org/jira/secure/attachment/12491477/3059_v2.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19337,,,Wed Aug 24 14:54:53 UTC 2011,,,,,,,,,,"0|i0ei3z:",82696,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"19/Aug/11 02:37;jbellis;patch against 0.8;;;","24/Aug/11 13:06;tjake;v2 fixes off by one bug in substring. otherwise +1;;;","24/Aug/11 14:54;jbellis;committed v2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
secondary index on a column that has a value of size > 64k will fail on flush,CASSANDRA-3057,12519349,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,cywjackson,cywjackson,18/Aug/11 22:02,16/Apr/19 09:32,14/Jul/23 05:52,19/Aug/11 01:54,0.8.5,,,Feature/2i Index,,,0,,,,"exception seen on flush when an indexed column contain size > 64k:

granted that having a value > 64k possibly mean something that shouldn't be indexed as it most likely would have a high cardinality, but i think there would still be some valid use case for it.

test case:
simply run the stress test with 
-n 1 -u 0 -c 2  -y Standard  -o INSERT  -S 65536 -x KEYS

then call a flush

exception:
 INFO [FlushWriter:8] 2011-08-18 21:49:33,214 Memtable.java (line 218) Writing Memtable-Standard1.Idx1@1652462853(16/20 serialized/live bytes, 1 ops)
Standard1@980087547(196659/245823 serialized/live bytes, 3 ops)
ERROR [FlushWriter:8] 2011-08-18 21:49:33,230 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[FlushWriter:8,5,RMI Runtime]
java.lang.AssertionError: 65536
        at org.apache.cassandra.utils.ByteBufferUtil.writeWithShortLength(ByteBufferUtil.java:330)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:245)
        at org.apache.cassandra.db.Memtable.access$400(Memtable.java:49)
        at org.apache.cassandra.db.Memtable$3.runMayThrow(Memtable.java:270)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/11 01:17;jbellis;3057-v3.txt;https://issues.apache.org/jira/secure/attachment/12490904/3057-v3.txt","19/Aug/11 01:03;xedin;CASSANDRA-3057-v2.patch;https://issues.apache.org/jira/secure/attachment/12490903/CASSANDRA-3057-v2.patch","18/Aug/11 23:07;xedin;CASSANDRA-3057.patch;https://issues.apache.org/jira/secure/attachment/12490889/CASSANDRA-3057.patch",,,,,,,,,,,,3.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20953,,,Fri Aug 19 02:25:19 UTC 2011,,,,,,,,,,"0|i0gf6n:",93890,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Aug/11 22:05;jbellis;Let's add a ThriftValidation check that values inserted into an indexed column are less than the ""short"" length.;;;","18/Aug/11 22:12;mdennis;+1 on validation approach (rejecting indexed values > max column name size) + useful error message ""can't index column value of size A for index B in CF C of KS D"" instead of ""assertion spew""

for durability of previous conversations: the problem is the value in a column ends up as the name in a the index column family.  So, if you try to index a value that is greater than the max column name size it is not handled well.;;;","18/Aug/11 22:15;jbellis;(technically, the value becomes a row key, which happens to have the same size limit as column names.);;;","19/Aug/11 00:53;jbellis;We're already looping through ColumnDefinitions for getValueValidator, let's extract a getColumnDefinition from that so we only do the search once.;;;","19/Aug/11 01:03;xedin;Sorry, I missed metadata.getColumnDefinition(ByteBuffer) method that is why I did a loop, fixed now.;;;","19/Aug/11 01:17;jbellis;v3 attached to only do the definition lookup once.;;;","19/Aug/11 01:24;xedin;Misunderstood what you meant... +1;;;","19/Aug/11 01:54;jbellis;committed;;;","19/Aug/11 02:25;hudson;Integrated in Cassandra-0.8 #285 (See [https://builds.apache.org/job/Cassandra-0.8/285/])
    return an InvalidRequestException if an indexed column is assigned a value larger than 64K
patch by pyaskevich and jbellis for CASSANDRA-3057

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1159473
Files : 
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/thrift/ThriftValidationTest.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CLI ""drop index"" doesn't handle numeric-only hex column identifiers properly",CASSANDRA-3054,12519305,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,dkuebric,dkuebric,18/Aug/11 15:39,16/Apr/19 09:32,14/Jul/23 05:52,19/Aug/11 15:03,0.8.5,,,Legacy/Tools,,,0,,,,"CLI drop index doesn't accept requests to drop columns whose hex names include only numeric characters.  The 617070 column name below is col2.

[default@Host] use MyKeyspace;
Authenticated to keyspace: MyKeyspace
[default@Host] drop index on MyCF.617070; 
Syntax error at position 22: mismatched input '617070' expecting Identifier

While drop index seems to parse correctly with alpha chars included:

[default@Host] drop index on MyCF.617070x;
Column '617070x' definition was not found in ColumnFamily 'MyCF'.
[default@Host] drop index on MyCF.col2;
Column 'col2' definition was not found in ColumnFamily 'MyCF'.

cassandra-user thread: http://mail-archives.apache.org/mod_mbox/cassandra-user/201108.mbox/%3CB2D1533B-C69E-467A-9653-1D086E33227C@thelastpickle.com%3E",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3075,,,,,,"19/Aug/11 12:45;xedin;CASSANDRA-3054.patch;https://issues.apache.org/jira/secure/attachment/12490950/CASSANDRA-3054.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20952,,,Fri Aug 19 15:20:07 UTC 2011,,,,,,,,,,"0|i0gf5b:",93884,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"19/Aug/11 13:40;jbellis;does allowing those different terms for columnName cause problems elsewhere?  I'm fine requiring quoting for non-Identifier.;;;","19/Aug/11 13:45;xedin;it's used only by drop index statement (because other commands should support function calls too) so it would not affect other statements.;;;","19/Aug/11 14:42;jbellis;+1;;;","19/Aug/11 15:03;xedin;Committed.;;;","19/Aug/11 15:20;hudson;Integrated in Cassandra-0.8 #286 (See [https://builds.apache.org/job/Cassandra-0.8/286/])
    Fix of numeric-only and string column names handling in CLI ""drop index""
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3054

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1159657
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/cli/CliTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/Cli.g
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[patch] EstimatedHIstogram doesn't overide equals correctly,CASSANDRA-3053,12519235,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,18/Aug/11 04:28,16/Apr/19 09:32,14/Jul/23 05:52,18/Aug/11 19:41,1.0.0,,,,,,0,,,,"EstimatedHistogram declares equals with an EstimatedHistogram parameter, instead of Object, thus only working correctly for statically typed EstimatedHistogram references. Fixed to take Object parm.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/11 04:29;dbrosius@apache.org;override_equals.diff;https://issues.apache.org/jira/secure/attachment/12490746/override_equals.diff",,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20951,,,Thu Aug 18 20:21:00 UTC 2011,,,,,,,,,,"0|i0gf4v:",93882,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Aug/11 19:41;jbellis;committed, thanks!;;;","18/Aug/11 20:21;hudson;Integrated in Cassandra #1034 (See [https://builds.apache.org/job/Cassandra/1034/])
    update EH.equals to work with any Object
patch by Dave Brosius; reviewed by jbellis for CASSANDRA-3053

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1159374
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/utils/EstimatedHistogram.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: ResultSet.next() gives NPE when run after an INSERT or CREATE statement,CASSANDRA-3052,12519225,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ardot,cdaw,cdaw,18/Aug/11 01:32,16/Apr/19 09:32,14/Jul/23 05:52,22/Aug/11 17:04,0.8.5,,,,,,0,cql,JDBC,,"This test script used to work until I upgraded the jdbc driver to 1.0.4.

*CQL 1.0.4*: apache-cassandra-cql-1.0.4-SNAPSHOT.jar build at revision 1158979

*Repro Script*: 
* drop in test directory, change package declaration and run:  ant test -Dtest.name=resultSetNPE
* The script gives you a NullPointerException when you uncomment out the following lines after a CREATE or INSERT statement.
{code}
colCount = res.getMetaData().getColumnCount();

res.next();
{code}
* Please note that there is no need to comment out those lines if a SELECT statement was run prior.


{code}
package com.datastax.bugs;

import java.sql.DriverManager;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;

import org.junit.Test;

public class resultSetNPE {
    
    @Test
    public void createKS() throws Exception {   
        Connection initConn = null;
        Connection connection = null;

        ResultSet res;
        Statement stmt;
        int colCount = 0;
        
        Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
        
        // Check create keyspace
        initConn = DriverManager.getConnection(""jdbc:cassandra://127.0.0.1:9160/default"");     
        stmt = initConn.createStatement();

        try {
          System.out.println(""Running DROP KS Statement"");  
          res = stmt.executeQuery(""DROP KEYSPACE ks1"");  
          // res.next();
          
        } catch (SQLException e) {
            if (e.getMessage().startsWith(""Keyspace does not exist"")) 
            {
                // Do nothing - this just means you tried to drop something that was not there.
                // res = stmt.executeQuery(""CREATE KEYSPACE ks1 with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy' and strategy_options:replication_factor=1"");  
            } 
        }   
          
        System.out.println(""Running CREATE KS Statement"");
        res = stmt.executeQuery(""CREATE KEYSPACE ks1 with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy' and strategy_options:replication_factor=1"");  
        // res.next();

        initConn.close();    
    }  
 
    @Test
    public void createCF() throws Exception 
    {   

        Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
        int colCount = 0;

        Connection connection = DriverManager.getConnection(""jdbc:cassandra://127.0.0.1:9160/ks1"");     
        Statement stmt = connection.createStatement();

        System.out.print(""Running CREATE CF Statement"");
        ResultSet res = stmt.executeQuery(""CREATE COLUMNFAMILY users (KEY varchar PRIMARY KEY, password varchar, gender varchar, session_token varchar, state varchar, birth_year bigint)"");    
        
        //colCount = res.getMetaData().getColumnCount();
        System.out.println("" -- Column Count: "" + colCount); 
        //res.next();
        
        connection.close();               
    }  
    
    @Test
    public void simpleSelect() throws Exception 
    {   
        Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
        int colCount = 0;

        Connection connection = DriverManager.getConnection(""jdbc:cassandra://127.0.0.1:9160/ks1"");     
        Statement stmt = connection.createStatement();
        
        System.out.print(""Running INSERT Statement"");
        ResultSet res = stmt.executeQuery(""INSERT INTO users (KEY, password) VALUES ('user1', 'ch@nge')"");  
        //colCount = res.getMetaData().getColumnCount();
        System.out.println("" -- Column Count: "" + colCount); 
        //res.next();
        
        System.out.print(""Running SELECT Statement"");
        res = stmt.executeQuery(""SELECT KEY, gender, state FROM users"");  
        colCount = res.getMetaData().getColumnCount();
        System.out.println("" -- Column Count: "" + colCount); 
        res.getRow();
        res.next();
            
        connection.close(); 
    }  
}
{code}
",,ardot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/11 17:09;ardot;statement-improper-result.txt;https://issues.apache.org/jira/secure/attachment/12491102/statement-improper-result.txt",,,,,,,,,,,,,,1.0,ardot,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20950,,,Mon Aug 22 17:04:59 UTC 2011,,,,,,,,,,"0|i0gf4f:",93880,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"18/Aug/11 02:28;ardot;You are using {{executeQuery()}} on an {{INSERT}} (or {{UPDATE}}) statement which is not intended to return a {{ResultSet}}. You should be using an {{executeUpdate()}} which returns a count of the operations (Which CQL always returns as zero).

However NPE is a bad way of reporting the problem. I'll look into, and it provide an appropriate {{SQLException}} failure outcome.;;;","18/Aug/11 16:59;cdaw;Hi Rick,

I am using a test harness which runs cql from a flat file, and up until this point all queries ran fine through executeQuery().

Thanks,
Cathy;;;","18/Aug/11 17:06;jbellis;Unfortunate that improving our spec-compliance broke the test harness, but Rick is correct: the JDBC spec says that executeQuery may only be used for SQL returning a resultset, otherwise you must use executeUpdate.

Since CQL is relatively simple I don't think you need to get much more sophisticated than ""if statement.startswith(""select"") executeQuery, else executeUpdate."";;;","18/Aug/11 18:44;cdaw;I will make the updates to the test harness as recommended above.;;;","19/Aug/11 20:09;cdaw;* DROP/CREATE KEYSPACE does not work with Statement.executeUpdate()
* DROP/CREATE KEYSPACE do however work with Statement.execute()
{code}
    [junit] DROP KEYSPACE cqldb
    [junit] java.sql.SQLException: Not an update statement.

    [junit] CREATE KEYSPACE cqldb with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy'  and strategy_options:replication_factor=1
    [junit] java.sql.SQLException: Not an update statement.
{code}

I assumed this would work since since these are DDL statements
{panel}
Executes the given SQL statement, which may be an INSERT, UPDATE, or DELETE statement or an SQL statement that returns nothing, such as an SQL DDL statement.
{panel}


Do you want a new bug for this? Or is this as expected?;;;","19/Aug/11 20:14;jbellis;That looks like a bug to me.  We can address it in this issue.;;;","19/Aug/11 21:37;ardot;The spec implies that {{executeUpdate()}} is the correct method for CQL/SQL commands like {{CREATE, INSERT,UPDATE,DELETE}}.

It also implies that the check is to be done on the returned result. In the case of {{executeQuery()}} an {{SQLException}} should be thrown if a {{ResultSet}} is not returned by the server. In the case of {{executeUpdate()}} return an error if a {{int}} update count is not returned.

We have an inconsistent solution in place and I'm currently working on a patch.
;;;","21/Aug/11 17:09;ardot;Patch checks to make sure the intended result is returned from the various {{execute}} method variants. ;;;","22/Aug/11 17:04;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
On Disk Compression breaks SSL Encryption,CASSANDRA-3051,12519195,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,bcoverston,bcoverston,17/Aug/11 22:06,16/Apr/19 09:32,14/Jul/23 05:52,30/Aug/11 13:50,1.0.0,,,,,,0,,,,"Encryption depends on FileStreamTask.write [1] protected member to be called because the SSLFileStreamTask.write overrides this to write back to the server.

When enabled, compression circumvents the call and the client does not communicate using an SSL socket back to the server.

[1]
protected long write(FileChannel fc, Pair<Long, Long> section, long length, long bytesTransferred) throws IOException

",Trunk,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/11 09:17;xedin;CASSANDRA-3051-v2.patch;https://issues.apache.org/jira/secure/attachment/12492216/CASSANDRA-3051-v2.patch","29/Aug/11 22:32;xedin;CASSANDRA-3051.patch;https://issues.apache.org/jira/secure/attachment/12492154/CASSANDRA-3051.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20949,,,Tue Aug 30 14:24:23 UTC 2011,,,,,,,,,,"0|i0gf3z:",93878,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"29/Aug/11 15:05;xedin;Removed SSLFileStreamTask and added EncryptionOptions to the constructor of the FileStreamTask.

Rebased with latest trunk (last commit 0a4b1667bee674f7c0a22057cbdab97e368a20d1);;;","29/Aug/11 15:27;jbellis;lgtm, +1 if it actually works :);;;","29/Aug/11 15:34;xedin;I don't see anh reason why it won't :) but can't write a test because SSL is treacky with it's stores...;;;","29/Aug/11 15:43;gdusbabek;wait, you tested it locally first, right?  It's not difficult to set up a streaming environment locally.;;;","29/Aug/11 15:48;jbellis;do we have a ""ssl howto"" somewhere?  I was hoping it would be in cassandra.yaml by encryption_options but no.  Or at least, not sufficiently ""for dummies"" for me.;;;","29/Aug/11 15:55;xedin;No, unfortunately I haven't found any info about how to do that so you are welcome to test if you can...;;;","29/Aug/11 15:58;gdusbabek;Not that I know of.  If someone wants to write one it would flesh out these basic steps:
# follow the steps for generating a keystore and a trust store here: http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore
# plug those files into encryption_options in cassandra.yaml
# make sure encryption_options.internode_encryption = all in the yaml.

I was mostly raising a voice of caution against committing code backed up by statements like ""I don't see anh reason why it won't...""  This is usually a prelude to something like ""we need to quickly release a new version because XYZ broke streaming.""  Just sayin'.;;;","29/Aug/11 16:20;xedin;Following your logic - person who was working on ssl should've done that at first place, there is no guarantee that it works in the current state. I'm not pushing things forward just wondering why testing wasn't done before.;;;","29/Aug/11 16:25;gdusbabek;Pavel, I tested SSL prior to committing the feature.

I was under the impression that this ticket exists because compression, when enabled, breaks SSL.  The implication is that it was working prior, else the ticket would be something more like ""SSL is broken."";;;","29/Aug/11 16:48;xedin;You misunderstood that, it is not breaking SSL it was just special cased in FileStreamTask so it wasn't using ssl socket. This patch removes special casing for SSL streaming by creating ssl socket directly in FileStreamTask if encryption options were set.;;;","29/Aug/11 17:51;jbellis;Pavel, can you try to set up local SSL w/ a ccm cluster based on Gary's instructions to verify?;;;","29/Aug/11 18:09;xedin;Sure;;;","29/Aug/11 21:38;xedin;I figured out the problem - SSLSocket always returns null on getChannel even on current code, I will refactor FileStreamingTask to support DataOutputStream instead of SocketChannel to unify normal and SSL transfers.;;;","29/Aug/11 22:32;xedin;CompressedRandomAccessReader.transfer method was removed with special casing for compressed files, SocketChannel based transfer changed to DataOuputStream based to unify SSL and normal modes. SSLFileStreamTask removed as unused.;;;","30/Aug/11 02:12;jbellis;- feels like we lose more than we gain by making writeHeader/write separate methods.  they aren't really self-contained so you have to keep the context they were called in, around mentally.  and if they were in-line, it would be obvious that you don't need to re-seek for each call to write().
- comments in write() don't really add much to what the code says, imo
- is flushing with each chunk necessary?  seems like that would harm performance
;;;","30/Aug/11 09:17;xedin;removed writeHeader method, seek and flush are done once per section.;;;","30/Aug/11 13:31;jbellis;+1;;;","30/Aug/11 13:50;xedin;Committed.;;;","30/Aug/11 14:24;hudson;Integrated in Cassandra #1056 (See [https://builds.apache.org/job/Cassandra/1056/])
    Fix streaming over SSL when compressed SSTable involved
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3051

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163207
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java
* /cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java
* /cassandra/trunk/src/java/org/apache/cassandra/security/streaming/SSLFileStreamTask.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/FileStreamTask.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hector NodeAutoDiscoverService fails to resolve hosts due to / being part of the IP address,CASSANDRA-3044,12519055,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,synfinatic,synfinatic,16/Aug/11 23:03,16/Apr/19 09:32,14/Jul/23 05:52,17/Aug/11 22:19,0.8.5,,,,,,0,,,,"Didn't get this problem with Cassandra 0.8.2- started happening under 0.8.4.  Temporary work around was to disable node auto discovery.  Seems to be related to:

http://svn.apache.org/viewvc?view=rev&revision=1155157
http://issues.apache.org/jira/browse/CASSANDRA-1777

LOG:

240514 [pool-2-thread-1] INFO me.prettyprint.cassandra.connection.NodeAutoDiscoverService - using existing hosts [10.255.255.176(10.255.255.176):9160, 10.255.255.175(10.255.255.175):9160]
240553 [pool-2-thread-1] ERROR me.prettyprint.cassandra.service.CassandraHost - Unable to resolve host /10.255.255.176
240553 [pool-2-thread-1] INFO me.prettyprint.cassandra.connection.NodeAutoDiscoverService - Found a node we don't know about /10.255.255.176(/10.255.255.176):9160 for TokenRange TokenRange(start_token:33370589793653380361461751202224080323, end_token:93518639523624865529944734322199113946, endpoints:[/10.255.255.176])
240553 [pool-2-thread-1] INFO me.prettyprint.cassandra.connection.NodeAutoDiscoverService - Found 1 new host(s) in Ring
240553 [pool-2-thread-1] INFO me.prettyprint.cassandra.connection.NodeAutoDiscoverService - Addding found host /10.255.255.176(/10.255.255.176):9160 to pool
240554 [pool-2-thread-1] ERROR me.prettyprint.cassandra.connection.HConnectionManager - General exception host to HConnectionManager: /10.255.255.176(/10.255.255.176):9160
java.lang.IllegalArgumentException: protocol = socket host = null
        at sun.net.spi.DefaultProxySelector.select(DefaultProxySelector.java:151)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:358)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.thrift.transport.TSocket.open(TSocket.java:178)
        at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
        at me.prettyprint.cassandra.connection.HThriftClient.open(HThriftClient.java:123)
        at me.prettyprint.cassandra.connection.ConcurrentHClientPool.<init>(ConcurrentHClientPool.java:43)
        at me.prettyprint.cassandra.connection.RoundRobinBalancingPolicy.createConnection(RoundRobinBalancingPolicy.java:68)
        at me.prettyprint.cassandra.connection.HConnectionManager.addCassandraHost(HConnectionManager.java:103)
        at me.prettyprint.cassandra.connection.NodeAutoDiscoverService.doAddNodes(NodeAutoDiscoverService.java:68)
        at me.prettyprint.cassandra.connection.NodeAutoDiscoverService$QueryRing.run(NodeAutoDiscoverService.java:53)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)","Cassandra 0.8.4, hector 0.8.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20948,,,Thu Aug 18 10:38:47 UTC 2011,,,,,,,,,,"0|i0gf13:",93865,,,,,Normal,,,,,,,,,,,,,,,,,"16/Aug/11 23:12;synfinatic;driftx says on IRC that this bug was already fixed so closing it myself.;;;","17/Aug/11 20:36;brandon.williams;Reopening because http://svn.apache.org/viewvc?view=rev&revision=1155157 should be in 0.8.4, so something else may be going on here.;;;","17/Aug/11 22:19;brandon.williams;We also need to apply the http://svn.apache.org/viewvc?view=rev&revision=1155157 fix in the other direction.  Fixed in r1158940.;;;","18/Aug/11 10:38;t0mas;Issue also affects the Pelops client from Scale7 (https://github.com/s7/scale7-pelops).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableImportTest fails on Windows because of malformed file path,CASSANDRA-3043,12519054,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vloncar,vloncar,vloncar,16/Aug/11 22:34,16/Apr/19 09:32,14/Jul/23 05:52,17/Aug/11 13:22,0.8.5,,,Legacy/Testing,,,0,windows,,,SSTableImportTest uses URL.getPath() to create path to JSON files. This fails on Windows in many cases (for example if there are spaces in path which get encoded as %20 which Windows doesn't like). Trick is to create URI from URL which satisfies all platforms.,Windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/11 02:31;jbellis;3043-v2.txt;https://issues.apache.org/jira/secure/attachment/12490593/3043-v2.txt","16/Aug/11 22:37;vloncar;windows-sstable-import-fix.patch;https://issues.apache.org/jira/secure/attachment/12490581/windows-sstable-import-fix.patch",,,,,,,,,,,,,2.0,vloncar,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20947,,,Wed Aug 17 14:22:07 UTC 2011,,,,,,,,,,"0|i0gf0v:",93864,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Aug/11 22:36;vloncar;Patch to fix SSTableImportTest;;;","17/Aug/11 02:31;jbellis;v2 moves the logic to a method and adds comments.  Does that look good to you?;;;","17/Aug/11 07:18;vloncar;LGTM.

There is a minor typo in the comment, says ""The rick is..."".;;;","17/Aug/11 13:22;jbellis;committed, thanks!;;;","17/Aug/11 14:22;hudson;Integrated in Cassandra-0.8 #283 (See [https://builds.apache.org/job/Cassandra-0.8/283/])
    fix SSTIT on Windows
patch by Vladimir Loncar; reviewed by jbellis for CASSANDRA-3043

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1158693
Files : 
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/tools/SSTableImportTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError on nodetool cleanup,CASSANDRA-3039,12518911,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,rays,rays,15/Aug/11 19:18,16/Apr/19 09:32,14/Jul/23 05:52,16/Aug/11 19:23,0.8.5,,,,,,0,exception,nodetool,,"While doing a cleanup I got the following AssertionError, I have tried a scrub and a major compaction before the cleanup which has not helped.

ST:

 INFO 18:49:58,540 Scrubbing SSTableReader(path='/vol/cassandra/data/system/LocationInfo-g-93-Data.db')
 INFO 18:49:58,834 Scrub of SSTableReader(path='/vol/cassandra/data/system/LocationInfo-g-93-Data.db') complete: 4 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:49:58,913 Scrubbing SSTableReader(path='/vol/cassandra/data/system/Migrations-g-56-Data.db')
 INFO 18:49:59,218 Scrub of SSTableReader(path='/vol/cassandra/data/system/Migrations-g-56-Data.db') complete: 1 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:49:59,256 Scrubbing SSTableReader(path='/vol/cassandra/data/system/Schema-g-58-Data.db')
 INFO 18:49:59,323 Scrub of SSTableReader(path='/vol/cassandra/data/system/Schema-g-58-Data.db') complete: 34 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:49:59,416 Scrubbing SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5074-Data.db')
 INFO 18:50:50,137 Scrub of SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5074-Data.db') complete: 91735 rows in new sstable and 32 empty (tombstoned) rows dropped
 INFO 18:50:50,137 Scrubbing SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5075-Data.db')
 INFO 18:50:53,075 Scrub of SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5075-Data.db') complete: 27940 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:50:53,089 Scrubbing SSTableReader(path='/vol/cassandra/data/SpiderServices/Content-g-238-Data.db')

 INFO 18:51:10,302 Scrub of SSTableReader(path='/vol/cassandra/data/SpiderServices/Content-g-238-Data.db') complete: 70815 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:53:05,420 Cleaning up SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5078-Data.db')
 INFO 18:53:13,266 Cleaned up to /vol/cassandra/data/SpiderServices/Content2-tmp-g-5079-Data.db.  198,705,176 to 198,705,176 (~100% of original) bytes for 27,940 keys.  Time: 7,846ms.
 INFO 18:53:13,267 Cleaning up SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5077-Data.db')
ERROR 18:53:33,913 Fatal exception in thread Thread[CompactionExecutor:21,1,RMI Runtime]
java.lang.AssertionError
	at org.apache.cassandra.db.compaction.PrecompactedRow.write(PrecompactedRow.java:107)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:132)
	at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:866)
	at org.apache.cassandra.db.compaction.CompactionManager.access$500(CompactionManager.java:65)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:204)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)","Distributor ID:	Ubuntu
Description:	Ubuntu 10.10
Release:	10.10
Codename:	maverick

AWS: m2.xlarge instance

6 Node Cluster",scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/11 19:42;jbellis;3039.txt;https://issues.apache.org/jira/secure/attachment/12490464/3039.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20946,,,Tue Aug 16 21:22:28 UTC 2011,,,,,,,,,,"0|i0gezz:",93860,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"15/Aug/11 19:42;jbellis;Patch to make cleanup and normal compaction able to skip empty rows (rows containing nothing but expired tombstones).

Scrub can already handle these, so you can workaround by scrubbing frequently, but if your workload results in a lot of these rows you probably want to apply this patch sooner than later.;;;","16/Aug/11 19:23;xedin;committed.;;;","16/Aug/11 21:22;hudson;Integrated in Cassandra-0.8 #281 (See [https://builds.apache.org/job/Cassandra-0.8/281/])
    Make cleanup and normal compaction able to skip empty rows (rows containing nothing but expired tombstones).
patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3039

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1158425
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool snapshot does not handle keyspace arguments correctly,CASSANDRA-3038,12518904,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,zznate,zznate,zznate,15/Aug/11 17:40,16/Apr/19 09:32,14/Jul/23 05:52,22/Aug/11 15:43,0.8.5,,,Tool/nodetool,,,0,,,,"Given the following invocation:
./bin/nodetool snapshot Keyspace1 -t keyspace1_snapshot -h localhost

Snapshots are made for all keyspaces

Given a multi-keyspace invocation:
./bin/nodetool snapshot Keyspace1 Keyspace2 Keyspac3 -t keyspace1_snapshot -h localhost

Snapshots will be made for Keyspace2 and Keyspace3 but not Keyspace1. 

It appears there is just some antiquated command argument noodling in NodeCmd#handleSnapshots",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/11 17:42;zznate;3038.txt;https://issues.apache.org/jira/secure/attachment/12490452/3038.txt",,,,,,,,,,,,,,1.0,zznate,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20945,,,Mon Aug 22 16:20:22 UTC 2011,,,,,,,,,,"0|i0gezr:",93859,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"15/Aug/11 17:42;zznate;Change to just make a copy of the cmdArgs array without modifying position. ;;;","15/Aug/11 17:44;zznate;Works with single, multi or no keyspace arguments. Added additional print statements to reflect what keyspaces were requested.;;;","22/Aug/11 15:43;brandon.williams;Committed.;;;","22/Aug/11 16:20;hudson;Integrated in Cassandra-0.8 #288 (See [https://builds.apache.org/job/Cassandra-0.8/288/])
    Handle snapshot arguments correctly.
Patch by Nate McCall, reviewed by brandonwilliams for CASSANDRA-3038

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1160311
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/NodeCmd.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vague primary key references in CQL,CASSANDRA-3036,12518838,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,kreynolds,kreynolds,14/Aug/11 19:24,16/Apr/19 09:32,14/Jul/23 05:52,17/Aug/11 22:18,0.8.5,,,Legacy/CQL,,,0,core,cql,,"create columnfamily wonk (id 'utf8' primary key, id int)
update wonk set id=1 where id='test'
create index wonk_id on wonk (id)

This does what you would expect but then the results are unclear when using 'id' in a where clause.

""select * from wonk where id=1"" returns nothing and ""select * from wonk where id='test'"" works fine.

Perhaps secondary indexes should not be allowed on columns that have the same name as the key_alias? At least a warning or something should be thrown to indicate you've just made a useless index.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/11 21:08;xedin;CASSANDRA-3036-v2.patch;https://issues.apache.org/jira/secure/attachment/12490699/CASSANDRA-3036-v2.patch","17/Aug/11 17:36;xedin;CASSANDRA-3036.patch;https://issues.apache.org/jira/secure/attachment/12490674/CASSANDRA-3036.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20943,,,Wed Aug 17 23:20:53 UTC 2011,,,,,,,,,,"0|i0gezb:",93857,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Aug/11 16:19;jbellis;We should validate that column_metadata does not get created with the key_alias, or column insertions where the column name is the key_alias.  (For both CQL and ""old"" paths.);;;","17/Aug/11 12:42;xedin;That is impossible to validate when comparator is not AsciiType because we force key_alias to be ascii;;;","17/Aug/11 13:34;jbellis;Well, there's two ways we can compare them anyway:

- compare the native bytes 
- compare the human-readable strings

I think I'd prefer #1, because 99.9% of the time these are going to give the same answer for names/aliases that do not conflict, and String creation is relatively expensive so I'd rather not do that.;;;","17/Aug/11 14:05;xedin;Sorry this is my bad because it thought that AsciiType.decompose and UTF8Type.decompose won't produce the same bytes, tested now. So yes, we can just check for default/column comparator and if it is Ascii/UTF8/Bytes we can just compare native bytes.;;;","17/Aug/11 14:26;jbellis;My point is we can *always* just compare native bytes.;;;","17/Aug/11 14:28;xedin;ok;;;","17/Aug/11 20:43;jbellis;- let's name the ColumnDef variable in validateCfDef something other than ""column"" (which usually means an IColumn or similar)
- when we throw an invalid column name error, we should use the CF's comparator rather than AsciiType (which will error out on many binary column names)
- CCFS calls getByteBuffer which catches MarshallException and turns it into IRE.  Suggest doing the validation ones from CCFS.validate, that way we don't have to do it again in CCFS.getColumns.  (Can just call AbstractType.fromString directly)
- go ahead and commit the println removal separately :);;;","17/Aug/11 21:08;xedin;bq. let's name the ColumnDef variable in validateCfDef something other than ""column"" (which usually means an IColumn or similar)

Renamed to columnDef

bq. when we throw an invalid column name error, we should use the CF's comparator rather than AsciiType (which will error out on many binary column names)

I use cf.key_alias in there which we force to be AsciiType so it should be fine

bq. CCFS calls getByteBuffer which catches MarshallException and turns it into IRE. Suggest doing the validation ones from CCFS.validate, that way we don't have to do it again in CCFS.getColumns. (Can just call AbstractType.fromString directly)

changed CCFS.getColumns to use comparator.fromString and changed validate method as you mentioned.;;;","17/Aug/11 21:50;jbellis;+1;;;","17/Aug/11 22:18;xedin;Committed.;;;","17/Aug/11 23:20;hudson;Integrated in Cassandra-0.8 #284 (See [https://builds.apache.org/job/Cassandra-0.8/284/])
    Validate that column names in column_metadata does not equal to key_alias on create/update of the ColumnFamily and CQL 'ALTER' statement.
patch by Pavel Yaskevich; reviewed by Jonathan Ellis for CASSANDRA-3036

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1158939
Files : 
* /cassandra/branches/cassandra-0.8/test/system/test_cql.py
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/thrift/ThriftValidationTest.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/CreateColumnFamilyStatement.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/AlterTableStatement.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"[patch] BufferedInputStream.skip only skips bytes that are in the buffer, so keep skipping until done",CASSANDRA-3034,12518824,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius@apache.org,dbrosius@apache.org,14/Aug/11 15:43,16/Apr/19 09:32,14/Jul/23 05:52,15/Aug/11 04:03,1.0.0,,,,,,0,,,,"code calls skip(remaining) without checking result. Skip isn't guaranteed to skip what you requested, especially BufferedInputStream, so keep skipping until the remaining bytes is 0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/11 15:44;dbrosius@apache.org;skip_fully.diff;https://issues.apache.org/jira/secure/attachment/12490385/skip_fully.diff",,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20942,,,Mon Aug 15 04:03:45 UTC 2011,,,,,,,,,,"0|i0geyv:",93855,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"15/Aug/11 04:03;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in describe_ring,CASSANDRA-3023,12518752,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,efalcao,efalcao,12/Aug/11 22:47,16/Apr/19 09:32,14/Jul/23 05:52,24/Aug/11 16:11,0.8.5,,,,,,0,,,,"Not sure how much of the following is relevant besides the stack trace, but here I go:

I have a 2 DC, 2 node per DC cluster. DC1 had it's seed replaced but I hadn't restarted. I upgraded to 0.8.4 in the following fashion:

-edited seeds
-stopped both DC1 nodes
-upgraded jars
-started both nodes at the same time

The non-seed node came up first and showed the following error. Then when the seed node came up, the error went away on the non-seed node but started occurring on the seed node:

ERROR [pool-2-thread-15] 2011-08-12 22:32:27,438 Cassandra.java (line 3668) Internal error processing describe_ring
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.getRangeToRpcaddressMap(StorageService.java:623)
	at org.apache.cassandra.thrift.CassandraServer.describe_ring(CassandraServer.java:731)
	at org.apache.cassandra.thrift.Cassandra$Processor$describe_ring.process(Cassandra.java:3664)
	at org.apache.cassandra.thrift.Brisk$Processor.process(Brisk.java:464)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/11 16:04;brandon.williams;3023.txt;https://issues.apache.org/jira/secure/attachment/12491501/3023.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,617,,,Wed Aug 24 21:10:12 UTC 2011,,,,,,,,,,"0|i0gewf:",93844,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"15/Aug/11 20:32;brandon.williams;This is going to happen anytime a) the tokens are known from being persisted in the system table and (so the coordinator knows about the nodes) but b) it has not actually talked to them, so it has no state information yet.  There's no good way to fix this, because either we leave out tokens that we know about but do not have any other information for, or we just leave the NPE.  On the bright side, the window to trigger this should be very short, probably just a handful of seconds at most.;;;","15/Aug/11 21:19;efalcao;FWIW, and I should have mentioned originally, the exception is thrown repeatedly every few seconds and doesn't seem to stop on the seed node;;;","15/Aug/11 21:46;brandon.williams;Was the cluster still mixed between 0.8.3 and 0.8.4 at that point?  That would do it.  The easiest thing to do is not describe the ring until all machines are on the same version.;;;","15/Aug/11 21:56;efalcao;i was upgrading one DC first and that's when i noticed it (and never upgraded the 2nd DC). Not sure if that matters. If so, I'll try upgrading both DC's and see if the error persists after fully upgraded.;;;","15/Aug/11 22:03;brandon.williams;This will happen in a mixed cluster because nodes prior to CASSANDRA-1777 will not have advertised the information that describe_ring now relies on, causing the NPE when the nodes that do have 1777 try to access it.

There's not a lot we can do here.  We can basically return half-broken data for nodes that haven't advertised this info yet, but that puts us back to before CASSANDRA-1777, so it's not much of a win.  I can't think of a reason why clients would need to call describe_ring while there's an upgrade in process; topology isn't changing.;;;","24/Aug/11 02:08;jbellis;I think ""return half-broken data"" is the right solution, since pre-1777 clients aren't going to be looking for the new data anyway.;;;","24/Aug/11 11:08;wmeler;Describe_ring is needed even if topology isn't changing. Smart clients need to describe ring all the time - especially at startup to know which nodes they should contact. I have such implementation in C based client app.
Also Hadoop is describing ring prior to job execution in org.apache.cassandra.hadoop.ColumnFamilyInputFormat.

Will problem disappear after full upgrade to 0.8.4? Now I have work-around - call describe_ring only on my old 0.8.1 nodes, but after full upgrade if it won't work I will have a big problem...;;;","24/Aug/11 15:46;brandon.williams;bq. Will problem disappear after full upgrade to 0.8.4?

Yes.;;;","24/Aug/11 16:04;brandon.williams;bq. I think ""return half-broken data"" is the right solution, since pre-1777 clients aren't going to be looking for the new data anyway.

You're right, but it sure bothers my OCD :)

Trivial patch attached.;;;","24/Aug/11 16:07;jbellis;+1;;;","24/Aug/11 16:11;brandon.williams;Committed.;;;","24/Aug/11 20:53;efalcao;Just another quick note: by the comments here, I thought I'd be able to do a rolling restart to 0.8.4

By the time I got to my last node and stopped it, my clients were under the impression that the whole cluster was down (it wasn't) but checking cassandra logs I saw the stacktrace above spewing out 5-10 times per second. When the last node was started, the stacktraces were still flowing rapidly. I downgraded my entire cluster to 0.8.3 to stop the error and bring my site back online

Any insight?;;;","24/Aug/11 21:10;efalcao;I think my clients may have rapidly been calling describe_ring.........hrm......which should have stopped NPE'ing after the full (but rolling) upgrade.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failures in cassandra long test: LongCompactionSpeedTest,CASSANDRA-3022,12518732,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,cdaw,cdaw,12/Aug/11 19:44,16/Apr/19 09:32,14/Jul/23 05:52,13/Aug/11 06:06,0.8.5,,,,,,0,,,,"*The failing test case*
{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
{code}


*The following error is repeated in the console output*
{code}
    [junit] ERROR 04:02:20,654 Error in ThreadPoolExecutor
    [junit] java.util.MissingFormatArgumentException: Format specifier 's'
    [junit] 	at java.util.Formatter.format(Formatter.java:2432)
    [junit] 	at java.util.Formatter.format(Formatter.java:2367)
    [junit] 	at java.lang.String.format(String.java:2769)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:136)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionIterator.getReduced(CompactionIterator.java:123)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionIterator.getReduced(CompactionIterator.java:43)
    [junit] 	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
    [junit] 	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
    [junit] 	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
    [junit] 	at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
    [junit] 	at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager.doCompactionWithoutSizeEstimation(CompactionManager.java:559)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager.doCompaction(CompactionManager.java:506)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:141)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:107)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
{code}

*Cassandra Revision List at time of failure
{code}
Summary
* log ks and cf of large rows being compacted patch by Ryan King; reviewed by jbellis for CASSANDRA-3019
* revert r1156772
* cache invalidate removes saved cache files patch by Ed Capriolo; reviewed by jbellis for CASSANDRA-2325
* make sure truncate clears out the commitlog patch by jbellis; reviewed by slebresne for CASSANDRA-2950
* include column name in validation failure exceptions patch by jbellis; reviewed by David Allsopp for CASSANDRA-2849
* fix NPE when encryption_options is unspecified patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3007
* update CHANGES
* update CHANGES

Revision 1156830 by jbellis: 
log ks and cf of large rows being compacted
patch by Ryan King; reviewed by jbellis for CASSANDRA-3019
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionController.java

Revision 1156791 by jbellis: 
revert r1156772
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java

Revision 1156772 by jbellis: 
cache invalidate removes saved cache files
patch by Ed Capriolo; reviewed by jbellis for CASSANDRA-2325
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java

Revision 1156763 by jbellis: 
make sure truncate clears out the commitlog
patch by jbellis; reviewed by slebresne for CASSANDRA-2950
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java
	/cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java

Revision 1156753 by jbellis: 
include column name in validation failure exceptions
patch by jbellis; reviewed by David Allsopp for CASSANDRA-2849
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java

Revision 1156749 by jbellis: 
fix NPE when encryption_options is unspecified
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3007
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java

Revision 1156695 by jbellis: 
update CHANGES
	/cassandra/branches/cassandra-0.8/CHANGES.txt

Revision 1156694 by jbellis: 
update CHANGES
	/cassandra/branches/cassandra-0.8/CHANGES.txt
{code}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/11 19:53;jbellis;3022.txt;https://issues.apache.org/jira/secure/attachment/12490282/3022.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20940,,,Sat Aug 13 22:20:46 UTC 2011,,,,,,,,,,"0|i0gew7:",93843,,cdaw,,cdaw,Normal,,,,,,,,,,,,,,,,,"12/Aug/11 19:53;jbellis;patch to fix, and provide human-readable keys where possible;;;","12/Aug/11 23:27;cdaw;The patch fixed the problem.
{code}

long-test:
     [echo] running long tests
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/Users/cathy/dev/cassandra-0.8/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
    [junit] Testsuite: org.apache.cassandra.db.LongTableTest
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 15.548 sec
    [junit] 
    [junit] Testsuite: org.apache.cassandra.db.MeteredFlusherTest
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 25.154 sec
    [junit] 
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Tests run: 6, Failures: 0, Errors: 0, Time elapsed: 82.062 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 988 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: 2751 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 1281 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=500000: 10034 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=500000 colsper=1: 13489 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=1000 colsper=5: 9982 ms
    [junit] ------------- ---------------- ---------------
    [junit] Testsuite: org.apache.cassandra.utils.LongBloomFilterTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 65.727 sec
    [junit] 
    [junit] Testsuite: org.apache.cassandra.utils.LongLegacyBloomFilterTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 42.892 sec
    [junit] 

BUILD SUCCESSFUL
{code};;;","13/Aug/11 06:06;jbellis;committed;;;","13/Aug/11 22:20;hudson;Integrated in Cassandra-0.8 #277 (See [https://builds.apache.org/job/Cassandra-0.8/277/])
    fix string formatting bug, and provide human-readable keys where possible
patch by jbellis; tested by Cathy Daw for CASSANDRA-3022

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1157333
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionController.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null pointer dereference of m in org.apache.cassandra.db.commitlog.CommitLogSegment.dirtyString(),CASSANDRA-3021,12518719,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,fantayeneh,fantayeneh,fantayeneh,12/Aug/11 16:24,16/Apr/19 09:32,14/Jul/23 05:52,12/Aug/11 19:33,0.8.5,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/11 16:27;fantayeneh;trunk-fixed-nulldereference.patch;https://issues.apache.org/jira/secure/attachment/12490268/trunk-fixed-nulldereference.patch",,,,,,,,,,,,,,1.0,fantayeneh,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20939,,,Fri Aug 12 20:21:47 UTC 2011,,,,,,,,,,"0|i0gevz:",93842,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"12/Aug/11 16:27;fantayeneh;Fixes the NP Dereference;;;","12/Aug/11 19:33;jbellis;committed to 0.8.5 and trunk, thanks!;;;","12/Aug/11 20:21;hudson;Integrated in Cassandra-0.8 #275 (See [https://builds.apache.org/job/Cassandra-0.8/275/])
    fix NPE when debug logging is enabled and dropped CF is present
patch by fantayeneh gizaw; reviewed by jbellis for CASSANDRA-3021

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1157225
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLogSegment.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL counter support is undocumented,CASSANDRA-3013,12518477,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,10/Aug/11 16:42,16/Apr/19 09:32,14/Jul/23 05:52,10/Aug/11 17:25,0.8.4,,,Legacy/CQL,Legacy/Documentation and Website,,0,cql,,,"When counter support was added to CQL, the documentation wasn't updated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/11 16:43;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3013-add-missing-CQL-counter-documentation.txt;https://issues.apache.org/jira/secure/attachment/12490004/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-3013-add-missing-CQL-counter-documentation.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20937,,,Wed Aug 10 18:13:58 UTC 2011,,,,,,,,,,"0|i0geuf:",93835,,,,,Low,,,,,,,,,,,,,,,,,"10/Aug/11 16:56;jbellis;+1;;;","10/Aug/11 17:25;urandom;committed;;;","10/Aug/11 18:13;hudson;Integrated in Cassandra-0.8 #269 (See [https://builds.apache.org/job/Cassandra-0.8/269/])
    add missing CQL counter documentation

Patch by eevans; reviewed by jbellis for CASSANDRA-3013

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156268
Files : 
* /cassandra/branches/cassandra-0.8/doc/cql/CQL.textile
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error upgrading when replication_factor is stored in strategy_options,CASSANDRA-3011,12518462,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,10/Aug/11 14:54,16/Apr/19 09:32,14/Jul/23 05:52,10/Aug/11 19:21,0.8.4,,,,,,0,,,,"[from the ML]

{noformat}
When I upgraded from 0.8.2 to 0.8.3 I encountered a exception during startup:
...
Caused by: org.apache.cassandra.config.ConfigurationException:
replication_factor is an option for SimpleStrategy, not
NetworkTopologyStrategy
       at org.apache.cassandra.locator.NetworkTopologyStrategy.<init>(NetworkTopologyStrategy.java:70)
...
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/11 14:55;jbellis;3011.txt;https://issues.apache.org/jira/secure/attachment/12489985/3011.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20936,,,Wed Aug 10 17:20:04 UTC 2011,,,,,,,,,,"0|i0getz:",93833,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"10/Aug/11 15:58;brandon.williams;+1;;;","10/Aug/11 17:20;hudson;Integrated in Cassandra-0.8 #268 (See [https://builds.apache.org/job/Cassandra-0.8/268/])
    ignore saved replication_factor strategy_option for NTS
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3011

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156244
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/KSMetaData.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in MessagingService.java:420,CASSANDRA-3007,12518303,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,vilda,vilda,09/Aug/11 10:22,16/Apr/19 09:32,14/Jul/23 05:52,11/Aug/11 19:16,0.8.5,,,,,,0,nullpointerexception,streaming,,"I'm getting large quantity of exceptions during streaming. It is always in MessagingService.java:420. The streaming appears to be blocked.

 INFO 10:11:14,734 Streaming to /10.235.77.27
ERROR 10:11:14,734 Fatal exception in thread Thread[StreamStage:2,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.MessagingService.stream(MessagingService.java:420)
        at org.apache.cassandra.streaming.StreamOutSession.begin(StreamOutSession.java:176)
        at org.apache.cassandra.streaming.StreamOut.transferRangesForRequest(StreamOut.java:148)
        at org.apache.cassandra.streaming.StreamRequestVerbHandler.doVerb(StreamRequestVerbHandler.java:54)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
","Linux w0 2.6.35-24-virtual #42-Ubuntu SMP Thu Dec 2 05:15:26 UTC 2010 x86_64 GNU/Linux
java version ""1.6.0_18""
OpenJDK Runtime Environment (IcedTea6 1.8.7) (6b18-1.8.7-2~squeeze1)
OpenJDK 64-Bit Server VM (build 14.0-b16, mixed mode)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/11 13:03;jbellis;3007.txt;https://issues.apache.org/jira/secure/attachment/12489840/3007.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19191,,,Thu Aug 11 20:21:40 UTC 2011,,,,,,,,,,"0|i0get3:",93829,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"09/Aug/11 13:00;jbellis;What kind of streaming are you attempting?  ;;;","09/Aug/11 13:03;jbellis;Never mind, not relevant.  Looks like you upgraded from 0.7 without updating your configuration file?

Fix for missing encryption_options attached.;;;","09/Aug/11 15:02;vilda;It's removetoken command.

Yes, I updated the node and forgot to specify encryption_options - thanks!;;;","11/Aug/11 18:18;brandon.williams;+1;;;","11/Aug/11 19:16;jbellis;committed;;;","11/Aug/11 20:21;hudson;Integrated in Cassandra-0.8 #272 (See [https://builds.apache.org/job/Cassandra-0.8/272/])
    fix NPE when encryption_options is unspecified
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3007

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156749
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enormous counter,CASSANDRA-3006,12518299,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,yulinyen,yulinyen,09/Aug/11 09:41,16/Apr/19 09:32,14/Jul/23 05:52,10/Aug/11 15:31,0.8.4,,,,,,0,,,,"I have two-node cluster with the following keyspace and column family settings.

Cluster Information:
   Snitch: org.apache.cassandra.locator.SimpleSnitch
   Partitioner: org.apache.cassandra.dht.RandomPartitioner
   Schema versions: 
	63fda700-c243-11e0-0000-2d03dcafebdf: [172.17.19.151, 172.17.19.152]

Keyspace: test:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [datacenter1:2]
  Column Families:
    ColumnFamily: testCounter (Super)
    ""APP status information.""
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.CounterColumnType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType/org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 1.1578125/1440/247 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []

Then, I use a test program based on hector to add a counter column (testCounter[sc][column]) 1000 times. In the middle the adding process, I intentional shut down the node 172.17.19.152. In addition to that, the test program is smart enough to switch the consistency level from Quorum to One, so that the following adding actions would not fail. 

After all the adding actions are done, I start the cassandra on 172.17.19.152, and I use cassandra-cli to check if the counter is correct on both nodes, and I got a result 1001 which should be reasonable because hector will retry once. However, when I shut down 172.17.19.151 and after 172.17.19.152 is aware of 172.17.19.151 is down, I try to start the cassandra on 172.17.19.151 again. Then, I check the counter again, this time I got a result 481387 which is so wrong.

I use 0.8.3 to reproduce this bug, but I think this also happens on 0.8.2 or before also. ",ubuntu 10.04,colinkuo,dehora,stuhood,yangyangyyy,yulinyen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/11 13:24;slebresne;3006.patch;https://issues.apache.org/jira/secure/attachment/12489971/3006.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20933,,,Fri Aug 12 06:40:29 UTC 2011,,,,,,,,,,"0|i0gesv:",93828,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"09/Aug/11 09:43;yulinyen;I forgot the mention that the counter is out of sync between these two nodes, one shows 481387 and the other one shows 20706.;;;","09/Aug/11 14:28;slebresne;I've haven't had luck with reproducing so far. I've tried to stick with the description above but did not used hector (not saying it is hector fault though, maybe it is the way it does retry that I don't emulate well). If you are able to share a minimal hector script with which you reproduce this easily, that would be very helpful.;;;","10/Aug/11 01:57;yulinyen;Here is the test program I am using now. the hector version is 0.8.0-2.
Hope this will be helpful.
------------------------------------------------

import java.util.Arrays;

import me.prettyprint.cassandra.model.AllOneConsistencyLevelPolicy;
import me.prettyprint.cassandra.serializers.StringSerializer;
import me.prettyprint.cassandra.service.CassandraHostConfigurator;
import me.prettyprint.cassandra.service.ThriftCluster;
import me.prettyprint.hector.api.Keyspace;
import me.prettyprint.hector.api.beans.HCounterColumn;
import me.prettyprint.hector.api.factory.HFactory;
import me.prettyprint.hector.api.mutation.Mutator;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


public class CounterTest {
	private Logger logger = LoggerFactory.getLogger(CounterTest.class) ;
	private static final Integer COUNTER_NUM = 1000 ;
	private static final StringSerializer ss = StringSerializer.get();
	private static final String HOST = ""172.17.19.151:9160"" ;
	private ThriftCluster cluster ;
	
	/**
	 * @param args
	 */
	public static void main(String[] args) {
		CounterTest tc = new CounterTest() ;

		try {
			tc.testAlarmCounter() ;
		} catch (InterruptedException e) {
			
		}
	}
	
	public CounterTest(){
		CassandraHostConfigurator chc = new CassandraHostConfigurator(HOST) ;
		chc.setMaxActive(100) ;
		chc.setMaxIdle(10) ;
		chc.setCassandraThriftSocketTimeout(60000) ;
		
		cluster = new ThriftCluster(""Test Cluster"", chc) ;
	}

	public void testAlarmCounter() throws InterruptedException{
		int successCounter = 0 ;
		int cl = 0;
		
		for(int i=0; i<COUNTER_NUM; i++){
			try{
				logger.info(""count: ""+i) ;
				
				Mutator<String> mutator = HFactory.createMutator(getKeyspace(cl), StringSerializer.get());
				
				HCounterColumn<String> column = HFactory.createCounterColumn(""testSC"", 1L) ;
				mutator.addCounter(""sc"", ""testCounter"", HFactory.createCounterSuperColumn(""testC"", Arrays.asList(column), ss, ss));
				mutator.execute() ;
				
				successCounter++ ;
			} catch(Exception e){
				logger.info(""Error! Change consistency level to 1."", e) ;
				cl=1 ;
			}
			
			Thread.sleep(50) ;
		}
		
		logger.info(""\nsuccess counter: ""+successCounter) ;
	}
	
	private Keyspace getKeyspace(int cl){
		if(cl == 1)
			return HFactory.createKeyspace(""test"", cluster, new AllOneConsistencyLevelPolicy()) ;
		else
			return HFactory.createKeyspace(""test"", cluster) ; // default consistency level is Quorum
	}
};;;","10/Aug/11 03:32;yulinyen;In order to make it easier to reproduce this issue, I document how I recreate this issue step by step.

1. clean any thing that is inside /var/lib/cassandra on node 172.17.19.151

2. start cassandra on node 172.17.19.151.

3. clean any thing that is inside /var/lib/cassnadra on node 172.17.19.152

4. modify the cassandra.yaml of 172.17.19.152 and add 172.17.19.151 as a seed.

5. start cassandra on node 172.17.19.152, I could see two node has formed a cluster, I also double check that using nodetool.

6. on node 172.17.19.151, I use cassandra-cli: to connect 172.17.19.151/9160, and execute commands -> 

create keyspace test
with placement_strategy = 'org.apache.cassandra.locator.NetworkTopologyStrategy'
and strategy_options = [{datacenter1:2}];

create column family testCounter
    with column_type = Super
    and default_validation_class = CounterColumnType
    and replicate_on_write = true
    and comparator = BytesType
    and subcomparator = BytesType
    and comment = 'APP status information.';

7. use the test program to add the counter 1000 times. between each adding action the program will pause 50 millisecond.

8. in the middle of the adding process, shut down the cassandra on node 172.17.19.152, (let's say I shut down node 172.17.19.152 when count is 200.). Because the test program changes the consistency level to One when it encounters an exception (timeout exception to be exact), the following adding actions will still be success.

9. wait for the overall adding process to complete. I saw ""success counter: 999"" due to one exception. 

10. use the cassandra-cli to connect to 172.17.19.151 and 172.17.19.152 and check the counter value, the value is 1001 on both nodes. It shows 1001 because hector will retry when it encounters the timeout exception. 

11. shutdown the cassandra on 172.17.19.151, wait for a few seconds, I saw ""InetAddress /172.17.19.151 is now dead"" on node 172.17.19.152.

12. after seeing ""InetAddress /172.17.19.151 is now dead"", restart the cassandra on node 172.17.19.151.

13. check the counter again with cassandra-cli on both nodes, this time the counter should no longer be 1001, it should be other weird number.

Hope someone else could recreate it by these steps.;;;","10/Aug/11 13:24;slebresne;Thanks, that helps a lot.

The problem is due to the RowMutation optimization that keeps the serialized data received on the wire to use in the commit log. This is wrong for counters because we use the deserialization of the RowMutation to clean the delta on the counter columns.

At least in the script to reproduce, this was a hint problem. The first node was creating a hint for the second one and was storing it himself (and the value of this hints was not cleared because of the bug above).

Patch attached to fix this. This basically disable the optimization for RowMutation that contains counter. I don't pretend this is particularly clean but I don't see any other simple solution.;;;","10/Aug/11 15:02;jbellis;+1

(nit: we can break from the loop once we find a counter);;;","10/Aug/11 15:31;slebresne;Committed with breaking early change.;;;","10/Aug/11 16:21;hudson;Integrated in Cassandra-0.8 #267 (See [https://builds.apache.org/job/Cassandra-0.8/267/])
    Force deserialization of RowMutation for counters
patch by slebresne; reviewed by jbellis for CASSANDRA-3006

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156221
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/RowMutation.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
;;;","12/Aug/11 06:40;yulinyen;Just tried 0.8.4. It seems much better now, but I still got value that is not what I expected.

Follow the same steps above, after step 11, I check the counter on .152, the counter values changes from 1001 to 200. And then I follow the steps 12 and 13. The counter value of .151 is still 1001, but the counter value of .152 is still 200. After  changing the consistencylevel to quorum and do the read again, this time the counter value of .152 became 1001.

I am kind of confused, because at step 10, I can get 1001 on both nodes, how come the value of .152 changes to 200 when .151 is down. Is this the right behavior of current design? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trunk single-pass streaming doesn't handle large row correctly,CASSANDRA-3003,12518240,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,yukim,slebresne,slebresne,08/Aug/11 19:52,16/Apr/19 09:32,14/Jul/23 05:52,05/Sep/11 14:57,1.0.0,,,,,,0,streaming,,,"For normal column family, trunk streaming always buffer the whole row into memory. In uses
{noformat}
  ColumnFamily.serializer().deserializeColumns(in, cf, true, true);
{noformat}
on the input bytes.
We must avoid this for rows that don't fit in the inMemoryLimit.

Note that for regular column families, for a given row, there is actually no need to even recreate the bloom filter of column index, nor to deserialize the columns. It is enough to filter the key and row size to feed the index writer, but then simply dump the rest on disk directly. This would make streaming more efficient, avoid a lot of object creation and avoid the pitfall of big rows.

Counters column family are unfortunately trickier, because each column needs to be deserialized (to mark them as 'fromRemote'). However, we don't need to do the double pass of LazilyCompactedRow for that. We can simply use a SSTableIdentityIterator and deserialize/reserialize input as it comes.",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/11 15:42;yukim;3003-v3.txt;https://issues.apache.org/jira/secure/attachment/12492255/3003-v3.txt","04/Sep/11 15:25;yukim;3003-v5.txt;https://issues.apache.org/jira/secure/attachment/12492974/3003-v5.txt","21/Aug/11 13:05;yukim;ASF.LICENSE.NOT.GRANTED--3003-v1.txt;https://issues.apache.org/jira/secure/attachment/12491091/ASF.LICENSE.NOT.GRANTED--3003-v1.txt","26/Aug/11 15:52;yukim;ASF.LICENSE.NOT.GRANTED--3003-v2.txt;https://issues.apache.org/jira/secure/attachment/12491798/ASF.LICENSE.NOT.GRANTED--3003-v2.txt","31/Aug/11 16:45;yukim;v3003-v4.txt;https://issues.apache.org/jira/secure/attachment/12492476/v3003-v4.txt",,,,,,,,,,5.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20932,,,Mon Sep 05 15:23:41 UTC 2011,,,,,,,,,,"0|i0ges7:",93825,,slebresne,,slebresne,Critical,,,,,,,,,,,,,,,,,"08/Aug/11 19:55;slebresne;Marking critical, because at least for counter column family, when the row is larger than the inMemoryLimit, the code will actually crash because it will use lazilyCompactedRow which will try to do it's 2 passes.;;;","10/Aug/11 07:06;stuhood;Oof... I don't know how I missed this one in review: very, very sorry Yuki/Sylvain.

Perhaps we can use this as an opportunity to switch to using only PrecompactedRow (for narrow rows which might go to cache) or EchoedRow (for wide rows, which go directly to disk)?

In order to use EchoedRow, we'd have to move where we do CounterContext cleanup: I've suggested in the past that it could be done at read time if we added ""fromRemote"" as a field in the metadata of an SSTable. Every SSTable*Iterator would be affected, because they'd need to respect the fromRemote field.

Alternatively, we could revert 2920 and 2677 (which I would hate: this has been a huge cleanup).

EDIT: Oops, apparently I didn't review this one. Anyway!;;;","10/Aug/11 09:28;slebresne;bq. In order to use EchoedRow, we'd have to move where we do CounterContext cleanup

I really think it is not very hard to do 'inline'. We really just want to deserialize, cleanup, reserialize. It should be super easy to add some ""CounterCleanedRow"" that does that.

bq. at it could be done at read time if we added ""fromRemote"" as a field in the metadata of an SSTable

Yes, but it does sound a bit complicated to me compared to doing the cleanup right away during streaming. It would also be less efficient, because until we have compacted the streamed sstable, each read will have to call the cleanup over and over, while we really only care to have it done twice (unless we completely change where we do cleanup).

bq. Perhaps we can use this as an opportunity to switch to using only PrecompactedRow (for narrow rows which might go to cache) or EchoedRow (for wide rows, which go directly to disk)?

I agree in that there is no point in doing manual deserialization there. About the PrecompactedRow for narrow rows which might go to cache, I'll just precise that it is worth using PrecompactedRow only if 1) we are doing AES streaming and 2) the row is in cache in the first place (which we can know since we always at least deserialize the row key).;;;","11/Aug/11 00:16;stuhood;bq. I really think it is not very hard to do 'inline'. We really just want to deserialize, cleanup, reserialize. It should be super easy to add some ""CounterCleanedRow"" that does that.
I'm probably missing something, but isn't the problem that this can't be done without two passes for rows that are too large to fit in memory? And you can't perform two passes without buffering data somewhere? I suggested removing the cleanup step out of streaming because then the row could be echoed to disk without modification.

bq. It would also be less efficient, because until we have compacted the streamed sstable, each read will have to call the cleanup over and over
This is true, but compaction is fairly likely to trigger soon after a big batch of streamed files arrives, since they will trigger compaction thresholds.;;;","11/Aug/11 01:51;yukim;Stu, Sylvan,

Let me try to fix this by using EchoedRow to serialize directly to disk, and creating new ""CounterCleanedRow"" suggested by Sylvain above.;;;","11/Aug/11 07:41;slebresne;bq. I'm probably missing something, but isn't the problem that this can't be done without two passes for rows that are too large to fit in memory?

Hum true. What we need to do is deserialize each row with the 'fromRemote' flag on so that the delta are cleaned up, and them reserialize the result. But that will potentially reduce the column serialized size (and thus modify the row total size and the column index). Now we could imagine to remember the offset of the beginning of the row, to load the column index in memory and update it during the first pass (it would likely be ok to simply update the index offsets without changing the index structure itself), and to seek back at the end to write the updated data size and column index. However, this unfortunately won't be doable with the current SequentialWriter (and CompressedSequentialWriter) since we cannot seek back (without truncating). Retrospectively, it would have been nicer to have the cleaning of a counter context not change its size :(

So yeah, it sucks. I'm still mildly fan of moving the cleanup because it ""feels wrong"" somehow. It feels it would be better to have that delta cleaning done sooner than latter. But this may end up being the simplest/more efficient solution.;;;","11/Aug/11 13:33;jbellis;bq. it would have been nicer to have the cleaning of a counter context not change its size

Can we pad it somehow?;;;","11/Aug/11 14:06;slebresne;bq. Can we pad it somehow?

It's doable. Basically a context is an array of shards, with a header that is a (variable) list of which of those shards are a delta. When we cleanup the delta we remove the header basically. We could have a specific cleanup for streaming that just set all the header to -1. But we probably want to do that only for the cleanup during streaming, and have compaction clean those afterwards, otherwise it is ugly. I don't know how much easier it is than cleaning during reads, though it avoids having to add a new info for sstable metadata.;;;","19/Aug/11 20:24;jbellis;How is this looking, Yuki?;;;","21/Aug/11 13:05;yukim;Instead of creating CounterCleanedRow, I added appendFromStream method to SSTW, which handles both normal and counter column.

I still need to work on SSTII because attached patch causes problem when iterating over cleaned up CounterColumns with 0-padding added during streaming.
That also causes StreamingTransferTest fail.

Will post update version soon.;;;","26/Aug/11 15:52;yukim;V2 attached and ready for the review.
For Counter columns, instead of padding in place of removed delta, v2 just ""mark"" the counter column to clear delta later, by multiplying #elt by -1 in order to keep the header size for later removal. Marking only occur when deserialize ""fromRemote"", and actual removal of delta is done when reading again from disk after the streaming.;;;","26/Aug/11 16:52;jbellis;Does CounterColumn.create work for both ""normal,"" non-streamed counter updates, as well as streaming?  Or do we need two distinct paths there?;;;","27/Aug/11 01:19;yukim;Looks like we need distinct paths. Counter reads from remote in the read path also get marked (have negative #elt) and may cause problem. I'll take a look.;;;","30/Aug/11 15:42;yukim;v3 attached. It marks counter column to delete delta after deserializing it from stream without clearing all delta. In this way, marking does not affect regular counter update.;;;","30/Aug/11 23:04;yukim;In v3, I forgot to handle the case where counter columns inside super column. I'll update soon.;;;","31/Aug/11 16:45;yukim;Added handling of counters inside SuperColumn.;;;","31/Aug/11 18:26;slebresne;I think this is a little bit sad to deserialize all the columns in the non-counter case. We do need to do it right now because of the computation of the max timestamp, but maybe we could have the other side send use the max timestamp as part of the stream header (but I agree, it's a bit more complicated).

For the record, the handling of counter columns amounts to the initial proposition of Stu of moving the cleanup to the reads (though the solution is slightly different). So the ""we'll cleanup on each read before the sstable is compacted"" remark does hold here, but I don't see a better solution right now and the ""those sstables will likely be compacted quickly"" argument probably make this ok anyway.

Other comments:
* we need to use Integer.MIN_VALUE as the value for expireBefore when deserializing the columns, otherwise the expired columns will be converted to DeletedColumns, which will change there serialized size (and thus screw up the data size and column index)
* for markDeltaAsDeleted, we must check if the length is already negative and leave it so if it is, otherwise if a streamed sstable get re-streamed to another node before it was compacted, we could end up not cleaning the delta correctly.
* it would be nice in SSTW.appendFromStream() to assert the sanity of our little deserialize-reserialize dance and assert what we did write the number of bytes that we wrote in the header.
* the patch change a clearAllDelta to a markDeltaAsDeleted in CounterColumnTest which is bogus (and the test does fail with that change).
* I would markDeltaAsDeleted to markForClearingDelta as this describe what the function does better
* nitpick: there is a few space at end of lines in some comments (I know I know, I'm picky).
;;;","04/Sep/11 15:37;yukim;Sylvain,

Thank you for the review.
For now, I leave the max timestamp calculation part as it is done during streaming.

bq. we need to use Integer.MIN_VALUE as the value for expireBefore when deserializing the columns, otherwise the expired columns will be converted to DeletedColumns, which will change there serialized size (and thus screw up the data size and column index)

Fixed.

bq. for markDeltaAsDeleted, we must check if the length is already negative and leave it so if it is, otherwise if a streamed sstable get re-streamed to another node before it was compacted, we could end up not cleaning the delta correctly.

bq. it would be nice in SSTW.appendFromStream() to assert the sanity of our little deserialize-reserialize dance and assert what we did write the number of bytes that we wrote in the header.

Nice point. I added the same assertion as other append() does.

bq. the patch change a clearAllDelta to a markDeltaAsDeleted in CounterColumnTest which is bogus (and the test does fail with that change).

I forgot to revert this one. I should have run test before submitting...

bq. I would markDeltaAsDeleted to markForClearingDelta as this describe what the function does better

Fixed.

bq. nitpick: there is a few space at end of lines in some comments (I know I know, I'm picky).

Fixed this one too, I guess.;;;","05/Sep/11 14:57;slebresne;lgtm, +1.

Committed with a tiny change to use a cheaper array backed column family in appendToStream, since we deserialize in order (and in a single thread).;;;","05/Sep/11 15:23;hudson;Integrated in Cassandra #1074 (See [https://builds.apache.org/job/Cassandra/1074/])
    Handle large rows with single-pass streaming
patch by yukim; reviewed by slebresne for CASSANDRA-3003

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1165306
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/CounterColumn.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/context/CounterContext.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/IncomingStreamReader.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ec2MultiRegionSnitch throws AssertionError on EC2,CASSANDRA-3000,12518136,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,initcron,initcron,07/Aug/11 14:47,16/Apr/19 09:32,14/Jul/23 05:52,10/Aug/11 16:28,,,,,,,0,ec2multiregionsnitch,ec2snitch,snitch,"I found Ec2MultiRegionSnitch patch at https://issues.apache.org/jira/browse/CASSANDRA-2452 

However, I could not find any documentation on how to get it working, which address to use as seed, listen and thrift addresses. I used the following, 

seed_address     = Public DNS of the seed node 
listen_address   = Public DNS of the cluster node
rpc_address      = 0.0.0.0
endpoint_snitch: org.apache.cassandra.locator.Ec2MultiRegionSnitch


When I try to start cassandra, I get the following error: 

 INFO 14:44:19,822 Ec2Snitch adding ApplicationState ec2region=eu-west ec2zone=1c
 INFO 14:44:19,831 Starting Messaging Service on ec2-46-137-139-124.eu-west-1.compute.amazonaws.com/10.227.143.202:7000
 INFO 14:44:19,851 Using saved token 162732122844140653649170199706439942449
 INFO 14:44:19,852 Enqueuing flush of Memtable-LocationInfo@550579946(53/66 serialized/live bytes, 2 ops)
 INFO 14:44:19,852 Writing Memtable-LocationInfo@550579946(53/66 serialized/live bytes, 2 ops)
 INFO 14:44:19,908 Completed flushing /var/lib/cassandra/data/system/LocationInfo-h-20-Data.db (163 bytes)
 INFO 14:44:19,913 Compacting Major: [SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-20-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-19-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-17-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-18-Data.db')]
ERROR 14:44:19,922 Exception encountered during startup.
java.lang.AssertionError
        at org.apache.cassandra.gms.Gossiper.compareEndpointStartup(Gossiper.java:620)
        at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:803)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:706)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:839)
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:986)
        at org.apache.cassandra.service.StorageService.setToken(StorageService.java:219)
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:520)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:434)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:213)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:335)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:91)
Exception encountered during startup.
java.lang.AssertionError
        at org.apache.cassandra.gms.Gossiper.compareEndpointStartup(Gossiper.java:620)
        at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:803)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:706)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:839)
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:986)
        at org.apache.cassandra.service.StorageService.setToken(StorageService.java:219)
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:520)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:434)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:213)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:335)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:91)

","build version: apache-cassandra-2011-08-07_02-23-42

OS: Ubuntu 10.04.2 LTS \n \l

# uname -a
Linux ip-10-227-143-202 2.6.32-312-ec2 #24-Ubuntu SMP Fri Jan 7 18:30:50 UTC 2011 x86_64 GNU/Linux
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/11 16:16;vijay2win@yahoo.com;0001-3000-fixing-localaddress-calls-insted-of-broadcastAd.patch;https://issues.apache.org/jira/secure/attachment/12489994/0001-3000-fixing-localaddress-calls-insted-of-broadcastAd.patch",,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20931,,,Wed Aug 10 17:14:23 UTC 2011,,,,,,,,,,"0|i0gerj:",93822,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"08/Aug/11 05:29;vijay2win@yahoo.com;""listen_address = Public DNS of the cluster node""
the idea is to use broadcast address as the AWS public ip and Listen address to use AWS private IP. it is done automatically.... I will add additional checks to look for user error like this...;;;","08/Aug/11 05:55;initcron;>""listen_address = Public DNS of the cluster node""
>the idea is to use broadcast address as the AWS public ip and Listen address to use AWS private IP. it is done >automatically.... I will add additional checks to look for user error like this...

Vijay, I still dont get this. What do you mean when you say ""it is done automatically"" ?  Does it mean that I do not need to configure listen address at all in cassandra.yaml and it will automatically be set?

Also, the reason I set listen_address to public dns on the cluster node is because it will automatically get resolved to private ip for the instances within the same region. Only the tnstances in other ec2 regions will route to its public IP. So it should do the right thing while setting up the listen address. 

;;;","08/Aug/11 15:46;vijay2win@yahoo.com;Does it mean that I do not need to configure listen address at all in cassandra.yaml and it will automatically be set?

Yes. And the private and public ip's are quried @ amazon. broadcast ip will be set to public ip and the private ip will be used for inter node communication within the region. (https://issues.apache.org/jira/browse/CASSANDRA-2452)

I couldn't reproduce this issue, do you have an existing cluster which you are trying to update the snitch?;;;","08/Aug/11 16:07;initcron;Vijay, 

Here is the cassandra.yaml config I am using  http://pastebin.com/7rBH45S8

I am not updating snitch on a existing cluster, but building one from scratch. 

Could you have a look at the config and see if I am doing anything wrong? ;;;","10/Aug/11 16:16;vijay2win@yahoo.com;Issue seems to be caused by LocalAddress to broadcast address changes... but shows up when using broad cast address like in EC2MRS... Attached patch adds some comments and fixes.;;;","10/Aug/11 16:28;brandon.williams;Committed.;;;","10/Aug/11 17:14;hudson;Integrated in Cassandra #1014 (See [https://builds.apache.org/job/Cassandra/1014/])
    Use broadcastAddress instead of localAddress.
Patch by Vijay, reviewed by brandonwilliams for CASSANDRA-3000

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156254
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/trunk/src/java/org/apache/cassandra/net/MessagingService.java
* /cassandra/trunk/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java
* /cassandra/trunk/src/java/org/apache/cassandra/net/OutboundTcpConnection.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix NPE in getRangeToRpcaddressMap,CASSANDRA-2996,12518032,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,05/Aug/11 12:49,16/Apr/19 09:32,14/Jul/23 05:52,09/Aug/11 16:56,0.8.4,,,,,,0,,,,"DatabaseDescriptor.getRpcAddress() can be null, which getRangeToRpcaddressMap doesn't take into account",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/11 13:17;slebresne;2996-v2.patch;https://issues.apache.org/jira/secure/attachment/12489472/2996-v2.patch","05/Aug/11 12:49;slebresne;2996.patch;https://issues.apache.org/jira/secure/attachment/12489469/2996.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20930,,,Fri Aug 05 16:33:16 UTC 2011,,,,,,,,,,"0|i0geqn:",93818,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"05/Aug/11 12:58;jbellis;Should we fix by making getRA return localAddress instead of null?;;;","05/Aug/11 13:17;slebresne;Yeah, I admit I went for lazy no that one, thinking that maybe there was a reason for having getRA returning null. But apparently there is none. Attaching v2 that sets rpcAddress to the right value upfront.;;;","05/Aug/11 13:45;jbellis;+1;;;","05/Aug/11 16:33;hudson;Integrated in Cassandra-0.8 #257 (See [https://builds.apache.org/job/Cassandra-0.8/257/])
    Fix NPE in getRangeToRpcaddressMap
patch by slebresne; reviewed by jbellis for CASSANDRA-2996

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1154219
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AbstractCassandraDaemon.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutOfBounds in CompressedSequentialWriter.flushData,CASSANDRA-2994,12517968,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,stuhood,stuhood,04/Aug/11 22:07,16/Apr/19 09:32,14/Jul/23 05:52,08/Aug/11 14:16,1.0.0,,,,,,0,,,,"Near the beginning of a wide row test with CASSANDRA-47 compression enabled on a counter column family, I see the following exception:

{code:java} WARN [CompactionExecutor:5] 2011-08-04 21:50:14,558 FileUtils.java (line 95) Failed closing org.apache.cassandra.io.compress.CompressedSequentialWriter@28f01347
java.lang.IndexOutOfBoundsException
	at java.io.RandomAccessFile.writeBytes(Native Method)
	at java.io.RandomAccessFile.write(RandomAccessFile.java:466)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.flushData(CompressedSequentialWriter.java:88)
	at org.apache.cassandra.io.util.SequentialWriter.flushInternal(SequentialWriter.java:174)
	at org.apache.cassandra.io.util.SequentialWriter.syncInternal(SequentialWriter.java:150)
	at org.apache.cassandra.io.util.SequentialWriter.close(SequentialWriter.java:283)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.close(CompressedSequentialWriter.java:159)
	at org.apache.cassandra.io.util.FileUtils.closeQuietly(FileUtils.java:91)
	at org.apache.cassandra.io.sstable.SSTableWriter.cleanupIfNecessary(SSTableWriter.java:201)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:176)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:120)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:103)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [CompactionExecutor:5] 2011-08-04 21:50:14,561 AbstractCassandraDaemon.java (line 146) Fatal exception in thread Thread[CompactionExecutor:5,1,main]
java.lang.IndexOutOfBoundsException
	at java.io.RandomAccessFile.writeBytes(Native Method)
	at java.io.RandomAccessFile.write(RandomAccessFile.java:466)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.flushData(CompressedSequentialWriter.java:88)
	at org.apache.cassandra.io.util.SequentialWriter.flushInternal(SequentialWriter.java:174)
	at org.apache.cassandra.io.util.SequentialWriter.reBuffer(SequentialWriter.java:226)
	at org.apache.cassandra.io.util.SequentialWriter.writeAtMost(SequentialWriter.java:117)
	at org.apache.cassandra.io.util.SequentialWriter.write(SequentialWriter.java:101)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.cassandra.db.compaction.PrecompactedRow.write(PrecompactedRow.java:105)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:150)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:153)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:120)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:103)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}",,cburroughs,stuhood,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/11 12:54;slebresne;2994-v2.patch;https://issues.apache.org/jira/secure/attachment/12489471/2994-v2.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20929,,,Mon Aug 08 15:23:22 UTC 2011,,,,,,,,,,"0|i0geq7:",93816,,,,,Normal,,,,,,,,,,,,,,,,,"04/Aug/11 22:17;stuhood;Before anyone pursues this, lemme confirm that it isn't due to a bad merge... sorry, should have done that before.

EDIT: Confirmed in trunk.;;;","04/Aug/11 22:51;stuhood;The workload was a wide row usecase, but based on the stack trace, the row hadn't reached a width to trigger LazilyCompactedRow.;;;","05/Aug/11 08:57;slebresne;A probable cause is that snappy doesn't seem to guarantee that the compressed output will shorter or equal to the input (indeed, Snappy.maxCompressedLength(65536) == 76490).

Attaching fix for that. I don't see what else it can be actually, but I don't know how to reproduce, so Stu, if you can reproduce and can check if this fixes it, that'd be cool.;;;","05/Aug/11 11:57;xedin;I guess you will also need to replace 

compressed = new byte[metadata.chunkLength]; 

with 

compressed = new byte[Snappy.maxCompressedLength(metadata.chunkLength)];

in the CompressedRandomAccessReader constructor.
;;;","05/Aug/11 12:54;slebresne;Oups, you're right, patch updated.;;;","05/Aug/11 18:49;stuhood;+1
Looks like that was it: thanks!;;;","08/Aug/11 14:16;slebresne;Committed, thanks
;;;","08/Aug/11 15:23;hudson;Integrated in Cassandra #1009 (See [https://builds.apache.org/job/Cassandra/1009/])
    Fix OutOfBounds with compression
patch by slebresne; reviewed by stuhood for CASSANDRA-2994

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1154969
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/compress/CompressedSequentialWriter.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issues with parameters being escaped correctly in Python CQL,CASSANDRA-2993,12517966,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,bcvisin,bcvisin,04/Aug/11 21:54,16/Apr/19 09:32,14/Jul/23 05:52,10/Aug/11 14:37,0.8.4,,,,,,0,CQL,parameter,python,"When using parameterised queries in Python CQL strings are not being escaped correctly.


Query and Parameters:
{code}
'UPDATE sites SET :col = :val WHERE KEY = :site_id'

{'col': 'feed_stats:1312493736688033024',
 'site_id': '899d15e8-bd4a-11e0-bc8c-001fe14cba06',
 'val': ""(dp0\nS'1'\np1\n(lp2\nI1\naI2\naI3\naI4\nasS'0'\np3\n(lp4\nI1\naI2\naI3\naI4\nasS'3'\np5\n(lp6\nI1\naI2\naI3\naI4\nasS'2'\np7\n(lp8\nI1\naI2\naI3\naI4\nas.""}
{code}

Query trying to be executed after processing parameters
{code}     
""UPDATE sites SET 'feed_stats:1312493736688033024' = '(dp0\nS''1''\np1\n(lp2\nI1\naI2\naI3\naI4\nasS''0''\np3\n(lp4\nI1\naI2\naI3\naI4\nasS''3''\np5\n(lp6\nI1\naI2\naI3\naI4\nasS''2''\np7\n(lp8\nI1\naI2\naI3\naI4\nas.' WHERE KEY = '899d15e8-bd4a-11e0-bc8c-001fe14cba06'""

{code}",Python CQL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/11 19:49;thobbs;2993-cql-grammar.txt;https://issues.apache.org/jira/secure/attachment/12489734/2993-cql-grammar.txt","08/Aug/11 19:49;thobbs;2993-pycql.txt;https://issues.apache.org/jira/secure/attachment/12489733/2993-pycql.txt","08/Aug/11 19:49;thobbs;2993-system-test.txt;https://issues.apache.org/jira/secure/attachment/12489735/2993-system-test.txt",,,,,,,,,,,,3.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20928,,,Wed Aug 10 15:24:53 UTC 2011,,,,,,,,,,"0|i0gepz:",93815,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"04/Aug/11 21:55;bcvisin;This is using pickle to serialise objects in python.;;;","08/Aug/11 19:49;thobbs;I think this is primarily an issue with multiline string literals, both in the CQL grammar and in the python driver.

2993-pycql.txt updates the python cql driver regex to handle multiline SELECT clauses.

2993-cql-grammar.txt makes the STRING_LITERAL token include '\r' and '\n'.

2993-system-test.txt adds a system test for multiline keys, names, and values.;;;","08/Aug/11 22:52;bcvisin;I believe Tyler is correct in his diagnosis and I think that the patches should fix the issues I am having.;;;","08/Aug/11 23:17;bcvisin;Try:

{code}
cql = 'UPDATE sites SET :col = :val WHERE KEY = :site_id'

vals = {'col': 'feed_stats:1312493736688033024',
 'site_id': '29ffb9d2-c205-11e0-a2a2-001fe14cba06',
 'val': ""(dp0\nS'1'\np1\n(lp2\nI1\naI2\naI3\naI4\nasS'0'\np3\n(lp4\nI1\naI2\naI3\naI4\nasS'3'\np5\n(lp6\nI1\naI2\naI3\naI4\nasS'2'\np7\n(lp8\nI1\naI2\naI3\naI4\nas.""}

cursor = cql.connect('cf')
cursor.execute(cql, vals)
{code}

My Result (after patching with Tyler's pycql.txt, but not with cql-grammar.txt)
I am printing out the prepared query
{code}
USE totalporn;
UPDATE sites SET 'feed_stats:1312493736688033024' = '(dp0
S''1''
p1
(lp2
I1
aI2
aI3
aI4
asS''0''
p3
(lp4
I1
aI2
aI3
aI4
asS''3''
p5
(lp6
I1
aI2
aI3
aI4
asS''2''
p7
(lp8
I1
aI2
aI3
aI4
as.' WHERE KEY = '29ffb9d2-c205-11e0-a2a2-001fe14cba06'
Traceback (most recent call last):
  File ""/home/blake/test.py"", line 10, in <module>
    cursor.execute(sql, vals)
  File ""/usr/local/lib/python2.7/dist-packages/cql/cursor.py"", line 135, in execute
    raise cql.ProgrammingError(""Bad Request: %s"" % ire.why)
cql.ProgrammingError: Bad Request: line 30:3 mismatched character ''' expecting '.'
{code}


The string comes from a pickle'd dict contaning strings as keys and a list as values:

{code}

import pickle

vals = {'col': 'feed_stats:1312493736688033024',
 'site_id': '29ffb9d2-c205-11e0-a2a2-001fe14cba06',
 'val': ""(dp0\nS'1'\np1\n(lp2\nI1\naI2\naI3\naI4\nasS'0'\np3\n(lp4\nI1\naI2\naI3\naI4\nasS'3'\np5\n(lp6\nI1\naI2\naI3\naI4\nasS'2'\np7\n(lp8\nI1\naI2\naI3\naI4\nas.""}

print pickle.loads((vals['val']))

{code}
returns
{code}
{'1': [1, 2, 3, 4], '0': [1, 2, 3, 4], '3': [1, 2, 3, 4], '2': [1, 2, 3, 4]}
{code}


;;;","09/Aug/11 03:08;thobbs;Blake: after applying the cql-grammar patch (and doing ""ant clean"" before ""ant""), this test case works for me.;;;","09/Aug/11 19:57;bcvisin;Works for me too.  Thanks Tyler!;;;","10/Aug/11 14:37;xedin;committed, thanks!;;;","10/Aug/11 15:24;hudson;Integrated in Cassandra-0.8 #266 (See [https://builds.apache.org/job/Cassandra-0.8/266/])
    Fixes issues with parameters being escaped incorrectly in Python CQL
patch by Tyler Hobbs; reviewed by Pavel Yaskevich for CASSANDRA-2993

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156198
Files : 
* /cassandra/branches/cassandra-0.8/test/system/test_cql.py
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/drivers/py/cql/cursor.py
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/Cql.g
* /cassandra/drivers/py/test/test_regex.py
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra doesn't start on Red Hat Linux due to hardcoded JAVA_HOME,CASSANDRA-2992,12517928,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,tarasp,tarasp,04/Aug/11 14:26,16/Apr/19 09:32,14/Jul/23 05:52,08/Aug/11 13:11,0.8.4,,,Packaging,,,0,,,,"On CentOS /etc/init.d/cassandra has
bq. export JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0/

While there is no such a directory on our server it was ok for 0.8.2, because /usr/sbin/cassandra checked the executable
{quote}
if [ -x $JAVA_HOME/bin/java ]; then
    JAVA=$JAVA_HOME/bin/java
else
    JAVA=`which java`
fi
{quote}

But 0.8.3 builds replaced the above code with one that doesn't check if JAVA_HOME is set correctly.
{quote}
if [ -n ""$JAVA_HOME"" ]; then
    JAVA=""$JAVA_HOME/bin/java""
else
    JAVA=java
fi
{quote}

That's why cassandra doesn't start anymore.


The correct fix would be to remove ""export JAVA_HOME"" from /etc/init.d/cassandra or set it only to correct path and only if it hasn't already been set.

It would also be nice to revert to ""[ -x $JAVA_HOME/bin/java ]"" in /usr/sbin/cassandra
",CentOS release 5.6,tarasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/11 22:53;thepaul;0001-don-t-hardcode-JAVA_HOME-in-redhat-initscript.patch.txt;https://issues.apache.org/jira/secure/attachment/12489600/0001-don-t-hardcode-JAVA_HOME-in-redhat-initscript.patch.txt",,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20927,,,Mon Aug 08 14:14:42 UTC 2011,,,,,,,,,,"0|i0gepr:",93814,,tarasp,,tarasp,Low,,,,,,,,,,,,,,,,,"04/Aug/11 14:37;jbellis;Is this fixed by CASSANDRA-2785 ?;;;","04/Aug/11 14:51;tarasp;No, CASSANDRA-2785 actually caused this bug, since now invalid JAVA_HOME is not ignored:
http://svn.apache.org/viewvc/cassandra/branches/cassandra-0.8/bin/cassandra?r1=1126728&r2=1152241&pathrev=1152241

But the real bug is setting JAVA_HOME to a fixed value in redhat specific scripts.;;;","06/Aug/11 20:26;thepaul;I don't want to get into the business of second-guessing the admin when a configuration parameter is set, so I vote we fix the hard-coded JAVA_HOME in redhat/cassandra. Probably by borrowing the jvm search stuff from debian/init and adjusting the directory list appropriately.;;;","06/Aug/11 21:02;jbellis;so, use existing JAVA_HOME if set, else search for it?  sounds good to me.;;;","06/Aug/11 22:53;thepaul;0001: add search for appropriate JAVA_HOME in redhat initscript, now that cassandra-env.sh expects the right $JAVA to be preconfigured (see #2785).;;;","06/Aug/11 22:57;thepaul;Taras, would you mind verifying that the list of JVM search dirs I included matches what you expect to find? I'm not as familiar with the range of java deployment practices on RH-based OSes.;;;","08/Aug/11 12:29;tarasp;Our system administrators say the patch is perfect.;;;","08/Aug/11 13:11;jbellis;Committed.

;;;","08/Aug/11 13:11;jbellis;Note that CASSANDRA-2785 was reverted for the second 0.8.3 vote, so I'm updating both affects and fix-for versions to 0.8.4.;;;","08/Aug/11 14:14;hudson;Integrated in Cassandra-0.8 #260 (See [https://builds.apache.org/job/Cassandra-0.8/260/])
    don't hardcode JAVA_HOME in RH init script
patch by Paul Cannon; reviewed by Taras Puchko for CASSANDRA-2992

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1154950
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/redhat/cassandra
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We should refuse query for counters at CL.ANY,CASSANDRA-2990,12517821,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,03/Aug/11 18:10,16/Apr/19 09:32,14/Jul/23 05:52,09/Aug/11 20:27,0.8.4,,,,,,0,counters,,,"We currently do not reject writes for counters at CL.ANY, even though this is not supported (and rightly so).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/11 17:40;slebresne;2990.patch;https://issues.apache.org/jira/secure/attachment/12489861/2990.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20926,,,Tue Aug 09 21:36:14 UTC 2011,,,,,,,,,,"0|i0gepb:",93812,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"09/Aug/11 18:40;jbellis;A few days ago, you said, ""A counter mutation only live enough so that it is applied to the first replica. Once this is done, a *row* mutation is generated for the other replica. That second mutation can be hinted. But that is a row mutation, so there should be no special casing at all for that.""

Why can't we hint the first replica?;;;","09/Aug/11 19:14;slebresne;bq. Why can't we hint the first replica?

Well, actually I think we could. Or at least if we cannot I forgot why. We would need to be sure we never replay an hint twice though, which I'm not sure is a guarantee right now. Also, we can only make this if what we store as a hint is the serialized mutation (in this case, the serialized CounterMutation): we can't apply the CounterMutation on a non-replica (partly because that would potentially increase the counter context too much, partly because counter remove suck, which would probably be a problem at some point).

So it should be doable, but it's a bit of work.;;;","09/Aug/11 19:32;jbellis;Okay, +1 on making the validation match what is actually currently supported (no ANY for counters), although I'd change ""not supported"" to ""not yet supported.""

We can deal w/ adding ANY support if and when someone actually needs it.;;;","09/Aug/11 20:27;slebresne;Committed with message tweak;;;","09/Aug/11 21:36;hudson;Integrated in Cassandra-0.8 #265 (See [https://builds.apache.org/job/Cassandra-0.8/265/])
    Refuse counter write at CL.ANY
patch by slebresne; reviewed by jbellis for CASSANDRA-2990

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1155548
Files : 
* /cassandra/branches/cassandra-0.8/test/system/test_cql.py
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/test/system/test_thrift_server.py
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/UpdateStatement.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CliClient print memtable threshold in incorrect order,CASSANDRA-2983,12516133,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,cywjackson,cywjackson,01/Aug/11 23:15,16/Apr/19 09:32,14/Jul/23 05:52,02/Aug/11 00:01,0.8.3,,,,,,0,,,,"as a continuation from #2839 looks like it was incorrectly merged into 0.8 as well, hence affecting > 0.8.2.

for trunk, this is also changed (time is taken out). So I guess format wise, we would stick with the fixed format in 0.7.7 per #2839 , which is:

{code}
sessionState.out.printf(""      Memtable thresholds: %s/%s/%s (millions of ops/minutes/MB)%n"",
    cf_def.memtable_operations_in_millions, cf_def.memtable_flush_after_mins, cf_def.memtable_throughput_in_mb);
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20923,,,Tue Aug 02 00:01:46 UTC 2011,,,,,,,,,,"0|i0genr:",93805,,,,,Low,,,,,,,,,,,,,,,,,"02/Aug/11 00:01;brandon.williams;Fixed in r1152967;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool netstats progress does not update on receiving side,CASSANDRA-2972,12515826,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,wmeler,wmeler,30/Jul/11 03:42,16/Apr/19 09:32,14/Jul/23 05:52,03/Aug/11 21:14,0.8.4,,,,,,0,,,,"when you add/remove node to cluster, nodetool netstats show correct results only on sending side - on receiving side you can see only 0% progress",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/11 17:45;yukim;ASF.LICENSE.NOT.GRANTED--cassandra-0.8-2972-v2.txt;https://issues.apache.org/jira/secure/attachment/12488424/ASF.LICENSE.NOT.GRANTED--cassandra-0.8-2972-v2.txt","01/Aug/11 17:26;yukim;ASF.LICENSE.NOT.GRANTED--cassandra-0.8-2972.txt;https://issues.apache.org/jira/secure/attachment/12488423/ASF.LICENSE.NOT.GRANTED--cassandra-0.8-2972.txt",,,,,,,,,,,,,2.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20920,,,Wed Aug 03 22:19:59 UTC 2011,,,,,,,,,,"0|i0gelb:",93794,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"30/Jul/11 04:14;jbellis;What do you think, Yuki?;;;","01/Aug/11 17:26;yukim;Patch attached for 0.8 branch.
StreamInSession holds references to all pending files(files attribute) and current processing file(current attribute). The former include the latter but those are different objects after deserialized, and only the latter gets update on progress.
When returning incoming streaming status via JMX, only the former pending files described above are returned due to the nature of HashSet#add.;;;","01/Aug/11 17:45;yukim;Slightly modified version.;;;","03/Aug/11 17:23;jbellis;I think we also need to update remoteFile.progress in IncomingStreamReader, at least in the 0.8 branch;;;","03/Aug/11 21:03;yukim;IncomingStreamReader#readnwrite (or SSLIncomingStreamReader#readwrite) in the 0.8 branch does update remoteFile.progress, so the issue is just a matter of dispaying through JMX.;;;","03/Aug/11 21:08;jbellis;ah, you're right.  i was thrown off by the whacky indentation there for ISR.;;;","03/Aug/11 21:14;jbellis;committed v2;;;","03/Aug/11 22:19;hudson;Integrated in Cassandra-0.8 #254 (See [https://builds.apache.org/job/Cassandra-0.8/254/])
    include files-to-be-streamed in StreamInSession.getSources
patch by Yuki Morishita; reviewed by jbellis for CASSANDRA-2972

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1153668
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/StreamInSession.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/IncomingStreamReader.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError during compaction of CF with counter columns,CASSANDRA-2968,12515766,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,tarasp,tarasp,29/Jul/11 12:00,16/Apr/19 09:32,14/Jul/23 05:52,02/Aug/11 15:05,0.8.3,,,,,,2,,,,"Having upgraded from 0.8.0 to 0.8.2 we ran nodetool compact and got

Error occured during compaction
java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.compaction.CompactionManager.performMajor(CompactionManager.java:277)
        at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1762)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1358)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.AssertionError                                                                                                                                                                                                          
        at org.apache.cassandra.db.context.CounterContext.removeOldShards(CounterContext.java:593)                                                                                                                                           
        at org.apache.cassandra.db.CounterColumn.removeOldShards(CounterColumn.java:237)                                                                                                                                                     
        at org.apache.cassandra.db.CounterColumn.removeOldShards(CounterColumn.java:256)                                                                                                                                                     
        at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:88)                                                                                                                                                
        at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:140)                                                                                                                            
        at org.apache.cassandra.db.compaction.CompactionIterator.getReduced(CompactionIterator.java:123)                                                                                                                                     
        at org.apache.cassandra.db.compaction.CompactionIterator.getReduced(CompactionIterator.java:43)                                                                                                                                      
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)                                                                                                                                                 
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)                                                                                                                                            
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)                                                                                                                                                     
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)                                                                                                                                    
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)                                                                                                                                           
        at org.apache.cassandra.db.compaction.CompactionManager.doCompactionWithoutSizeEstimation(CompactionManager.java:569)                                                                                                                
        at org.apache.cassandra.db.compaction.CompactionManager.doCompaction(CompactionManager.java:506)                                                                                                                                     
        at org.apache.cassandra.db.compaction.CompactionManager$4.call(CompactionManager.java:319)                                                                                                                                           
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)                                                                                                                                                                
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)                                                                                                                                                                          
        ... 3 more",CentOS release 5.6,ambroff,andrden,tarasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/11 17:22;slebresne;2968.patch;https://issues.apache.org/jira/secure/attachment/12488422/2968.patch","31/Jul/11 12:43;andrden;AffiliateActivity-g-147-Data.db;https://issues.apache.org/jira/secure/attachment/12488352/AffiliateActivity-g-147-Data.db","31/Jul/11 12:43;andrden;AffiliateActivity-g-147-Index.db;https://issues.apache.org/jira/secure/attachment/12488353/AffiliateActivity-g-147-Index.db","30/Jul/11 13:18;andrden;AffiliateActivity-g-195-Data.db;https://issues.apache.org/jira/secure/attachment/12488300/AffiliateActivity-g-195-Data.db","30/Jul/11 13:18;andrden;AffiliateActivity-g-195-Index.db;https://issues.apache.org/jira/secure/attachment/12488301/AffiliateActivity-g-195-Index.db",,,,,,,,,,5.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20918,,,Tue Aug 02 16:36:05 UTC 2011,,,,,,,,,,"0|i0gekf:",93790,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"30/Jul/11 13:18;andrden;Reproducible test case for this bug:

create column family AffiliateActivity with default_validation_class=CounterColumnType
   and key_validation_class=LongType and comparator=AsciiType
   and memtable_throughput=30
   and memtable_operations=0.5
   and replicate_on_write=true;

put AffiliateActivity-g-195-Data.db and AffiliateActivity-g-195-Index.db (attached) into cassandra 0.8.2 data directory for some keyspace, then run cassandra server to open the files and run nodetool scrub

Those AffiliateActivity-g-195 data files were originally created with cassandra 0.8

java.io.IOError: java.lang.AssertionError
	at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:775)
	at org.apache.cassandra.db.compaction.CompactionManager.doScrub(CompactionManager.java:631)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:65)
	at org.apache.cassandra.db.compaction.CompactionManager$3.call(CompactionManager.java:251)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.AssertionError
	at org.apache.cassandra.db.context.CounterContext.removeOldShards(CounterContext.java:593)
	at org.apache.cassandra.db.CounterColumn.removeOldShards(CounterColumn.java:237)
	at org.apache.cassandra.db.CounterColumn.removeOldShards(CounterColumn.java:256)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:88)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:140)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:146)
	at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:719)
	... 8 more


;;;","30/Jul/11 14:41;andrden;Debugger shows at the exception breakpoint in the test case described that
""state.getCount()""	-9132236803706623116	
""state.getClock()""	-9132236803706623116	

and context.array() (exactly 100 bytes in size) is the following so the duplication is really there, 
and this record in sstable is likely corrupted - but how come!

000100012335e4a0b2c911e000006bb62b336af7 8143c702fe516b74 8143c702fe516b74
23d44780b2c911e00000f03fd2ec87ff000000000000b3c
0000000000000b3c025188750b2c911e0000058b7809d76fff082e59147f68e9cf082e59147f68e9c
;;;","30/Jul/11 16:59;andrden;yes, sstable likely corrupted but by which code? 0.8.0 or 0.8.2?

sstable2json AffiliateActivity-g-195-Data.db
(first row is parsed ok, second has state.getCount()==state.getClock())

....
""00000000019d499f"": [[""clicks"",""000100012335e4a0b2c911e000006bb62b336af7000000000000285c000000000000285c23d44780b2c911e00000f03fd2ec87ff0000000000000002000000000000000225188750b
2c911e0000058b7809d76ff00000000000000690000000000000069"",1311664683623,""c"",-9223372036854775808]],
""0000000000748b45"": [[""clicks"",""000100012335e4a0b2c911e000006bb62b336af78143c702fe516b748143c702fe516b7423d44780b2c911e00000f03fd2ec87ff000000000000b3c0000000000000b3c025188750b
2c911e0000058b7809d76fff082e59147f68e9cf082e59147f68e9c"",1311665756252,""c"",-9223372036854775808]],
....

cassandra-cli:
get AffiliateActivity[27085215];          
=> (counter=clicks, value=10439) - reasonable value

get AffiliateActivity[7637829]; 
=> (counter=clicks, value=8198429924508872144) - nonsense value

;;;","31/Jul/11 12:43;andrden;actually 0.8.2 continues to write non-compactable sstables :-(

AffiliateActivity-g-147-Data.db is a recent file written 3 days after upgrade from 0.8.0 to 0.8.2 and it cannot be even scrubbed

The pattern I can see is that we have only every other sstable file left in data directory - AffiliateActivity-g-133, 135, 137, 139, 141, 143, 145, 147. Not quite sure - but it seems 1 compaction is done to new files - otherwise where are the even numbered ones? - but those files which resulted from first compaction are broken?

Anyway that's purely 0.8.2 problem, not any upgrade issue;;;","31/Jul/11 17:09;ambroff;For what it's worth I've seen this same bug in 0.8.1.;;;","01/Aug/11 12:38;andrden;no, I was wrong, compaction is not the culprit

I ran nodetool flush and produced 3 new small sstables - strangely they are all numbered with odd numbers only 153 155 157 - but the point is a freshly made sstable with only 8 rows is already unreadable by compaction/scrub.

Maybe header in counter binary value is not written or what...

{code}
{
""0000000000cc71a4"": [[""clicks"",""000100002335e4a0b2c911e000006bb62b336af700000000000000010000000000000001"",1312201208019,""c"",-9223372036854775808]],
""0000000000748b45"": [[""clicks"",""000100002335e4a0b2c911e000006bb62b336af7000000000000009f000000000000009f23d44780b2c911e00000f03fd2ec87ffa6cd8bac6eb8a32d2fd3c2059ade464925188750b2c911e0000058b7809d76ff9a8b961aed94ef799a8b961aed94ef79"",1312201220709,""c"",-9223372036854775808]],
""00000000010a4465"": [[""clicks"",""00002335e4a0b2c911e000006bb62b336af700000000000045ad00000000000045ad23d44780b2c911e00000f03fd2ec87ff0000000000004bfd0000000001fae30325188750b2c911e0000058b7809d76ff000000000000038f000000000000038f"",1312201219737,""c"",-9223372036854775808]],
""0000000001319592"": [[""clicks"",""000100002335e4a0b2c911e000006bb62b336af70000000000000001000000000000000123d44780b2c911e00000f03fd2ec87ffdf1636876c1c1bc2df1636876c1c1bc225188750b2c911e0000058b7809d76ff0876b1020dc02ac97f12b88d9f9df751"",1312201215475,""c"",-9223372036854775808]],
""0000000001b9a53e"": [[""clicks"",""000100002335e4a0b2c911e000006bb62b336af700000000000000010000000000000001"",1312201202338,""c"",-9223372036854775808]],
""00000000004dd84f"": [[""clicks"",""000100002335e4a0b2c911e000006bb62b336af70000000000000001000000000000000123d44780b2c911e00000f03fd2ec87ff000000000017d1550000000059fb2ad925188750b2c911e0000058b7809d76ff00000000000173bf00000000000173bf"",1312201205636,""c"",-9223372036854775808]],
""0000000000d1d52f"": [[""clicks"",""000100002335e4a0b2c911e000006bb62b336af700000000000000020000000000000002"",1312201210515,""c"",-9223372036854775808]],
""0000000001410d4a"": [[""clicks"",""000100002335e4a0b2c911e000006bb62b336af700000000000000010000000000000001"",1312201220365,""c"",-9223372036854775808]]
}
{code};;;","01/Aug/11 17:22;slebresne;This is actually a pretty stupid bug (not that there is smart bug): the old NodeId for the local node were read from the system table in reversed order while they shouldn't. The wrong path was then taken based on that mistake. No data was lost due to that (i.e, the total value of the counters is preserved), but non-sensical counter context were created (hence triggering the assertion).

Fixing the root cause is pretty straightforward. Fixing the nonsensical counter contexts is more subtle, but it is doable up to the fact that the local NodeId on the node(s) where the assertion is triggered will have to be renewed. Attaching a patch that does both (fixing root cause and repairing the bad data). Also add two unit tests, one for the root cause and one to check that the bad data repair code does what it is supposed to do.

After applying that patch (or upgrading on a release shipping it), you will (potentially) need to restart the node with the -Dcassandra.renew_counter_id=true (compaction will still fail if you don't but with a message saying that you should restart with the startup flag).;;;","02/Aug/11 14:17;jbellis;+1 w/ println removed :);;;","02/Aug/11 15:05;slebresne;Committed (without println), thanks;;;","02/Aug/11 16:36;hudson;Integrated in Cassandra-0.8 #252 (See [https://builds.apache.org/job/Cassandra-0.8/252/])
    Fix assertion error during compaction of counter CFs
patch by slebresne; reviewed by jbellis for CASSANDRA-2968

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1153156
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/LazilyCompactedRow.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/SchemaLoader.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/utils/NodeId.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/context/CounterContext.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/PrecompactedRow.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/CounterMutationTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexRangeSliceQuery results include index column even though it is not in SliceRange,CASSANDRA-2964,12515619,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,rjtg,rjtg,28/Jul/11 09:19,16/Apr/19 09:32,14/Jul/23 05:52,13/Jun/12 10:02,0.7.9,,,,,,0,,,,When an IndexSlicwQuery is done the result contains the index column even though it was not in the slice range.,,rjtg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/11 17:37;jbellis;2964-0.7.txt;https://issues.apache.org/jira/secure/attachment/12489221/2964-0.7.txt","28/Jul/11 10:27;rjtg;TestIndexRangeSliceQuery.java;https://issues.apache.org/jira/secure/attachment/12488084/TestIndexRangeSliceQuery.java","28/Jul/11 10:27;rjtg;cassandra.yaml;https://issues.apache.org/jira/secure/attachment/12488083/cassandra.yaml",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20915,,,Wed Jun 13 10:02:59 UTC 2012,,,,,,,,,,"0|i0gejj:",93786,,rjtg,,rjtg,Normal,,,,,,,,,,,,,,,,,"28/Jul/11 09:21;rjtg;This was working correctly in 0.7.5 but after uzpgrading to 0.7.7 the ranges seem to be wrong.
Will add a testcase soon.;;;","28/Jul/11 10:27;rjtg;testng testcase and cassandra.yaml to be used;;;","28/Jul/11 10:31;rjtg;Creating the testcase makes this more awkward.
It only fails when run against our production cluster (which was migrated recently) but not against a fresh setup 0.7.7 (in the testcase the embedded server)

Therefor i was not able to check if the error is limited to TimeUUIDType Columns as that is the only index used in our production system.
I will try to figure out why it fails in that cluster but not in fresh one;;;","28/Jul/11 12:54;rjtg;some log output from when the above testcase fails (the columnfamily name is different from the one in the testcase, because it is the original system - i have not yet been able to reproduce the issue on any other system)

DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,103 CassandraServer.java (line 522) scan
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,103 StorageProxy.java (line 666) restricted ranges for query [-1,-1] are [[-1,0], (0,42535295865117307932921825928971026432], (425352958651173079329218259289710
26432,85070591730234615865843651857942052864], (85070591730234615865843651857942052864,127605887595351923798765477786913079296], (127605887595351923798765477786913079296,-1]]
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,103 StorageProxy.java (line 753) scan ranges are [-1,0],(0,42535295865117307932921825928971026432],(42535295865117307932921825928971026432,850705917302346158658
43651857942052864],(85070591730234615865843651857942052864,127605887595351923798765477786913079296],(127605887595351923798765477786913079296,-1]
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,104 ReadCallback.java (line 86) Blockfor/repair is 2/false; setting up requests to /10.234.81.208,/10.226.130.128
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,107 StorageProxy.java (line 780) reading org.apache.cassandra.db.IndexScanCommand@31bcc110 from /10.234.81.208
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,107 StorageProxy.java (line 780) reading org.apache.cassandra.db.IndexScanCommand@31bcc110 from /10.226.130.128
DEBUG [ReadStage:563] 2011-07-28 14:45:19,107 ColumnFamilyStore.java (line 1514) Primary scan clause is 00000000-0000-1000-0000-000000000000
DEBUG [ReadStage:563] 2011-07-28 14:45:19,108 ColumnFamilyStore.java (line 1569) Scanning index 'EventsByUser.00000000-0000-1000-0000-000000000000 EQ 41' starting with
DEBUG [ReadStage:563] 2011-07-28 14:45:19,108 SliceQueryFilter.java (line 123) collecting 0 of 100: 74657374526573756c74446f6573436f6e7461696e436f727265637452616e67654578636c7564696e67496e6465786564526f77:fa
lse:0@1311857119516000

DEBUG [ReadStage:555] 2011-07-28 14:45:19,173 ColumnFamilyStore.java (line 1581) fetched ColumnFamily(<anonymous> [74657374526573756c74446f6573436f6e7461696e436f727265637452616e67654578636c7564696e67496e6465
786564526f77:false:0@1311857119516000,])
DEBUG [ReadStage:555] 2011-07-28 14:45:19,173 IndexScanVerbHandler.java (line 46) Sending RangeSliceReply{rows=} to 14083152@/10.234.81.208
DEBUG [RequestResponseStage:3] 2011-07-28 14:45:19,173 ResponseVerbHandler.java (line 48) Processing response on a callback from 14083152@/10.234.81.208
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,173 StorageProxy.java (line 780) reading org.apache.cassandra.db.IndexScanCommand@22bb5662 from /10.234.81.208
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,173 StorageProxy.java (line 780) reading org.apache.cassandra.db.IndexScanCommand@22bb5662 from /10.227.141.155
DEBUG [RequestResponseStage:4] 2011-07-28 14:45:19,175 ResponseVerbHandler.java (line 48) Processing response on a callback from 14083153@/10.227.141.155
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,175 ReadCallback.java (line 86) Blockfor/repair is 2/false; setting up requests to /10.234.81.208,/10.226.130.128
DEBUG [ReadStage:560] 2011-07-28 14:45:19,175 ColumnFamilyStore.java (line 1514) Primary scan clause is 00000000-0000-1000-0000-000000000000
DEBUG [ReadStage:560] 2011-07-28 14:45:19,176 ColumnFamilyStore.java (line 1569) Scanning index 'EventsByUser.00000000-0000-1000-0000-000000000000 EQ 41' starting with
DEBUG [ReadStage:560] 2011-07-28 14:45:19,176 SliceQueryFilter.java (line 123) collecting 0 of 100: 74657374526573756c74446f6573436f6e7461696e436f727265637452616e67654578636c7564696e67496e6465786564526f77:fa
lse:0@1311857119516000
DEBUG [ReadStage:560] 2011-07-28 14:45:19,176 ColumnFamilyStore.java (line 1581) fetched ColumnFamily(<anonymous> [74657374526573756c74446f6573436f6e7461696e436f727265637452616e67654578636c7564696e67496e6465
786564526f77:false:0@1311857119516000,])
DEBUG [ReadStage:560] 2011-07-28 14:45:19,176 SliceQueryFilter.java (line 123) collecting 0 of 1000: 8ab6d400-1dd2-11b2-8bac-0024e8e90693:true:4@1311850071466000
DEBUG [ReadStage:560] 2011-07-28 14:45:19,177 SliceQueryFilter.java (line 123) collecting 0 of 1000: 8ab6d400-1dd2-11b2-93fb-0024e8e90693:true:4@1311849715863000
DEBUG [ReadStage:560] 2011-07-28 14:45:19,177 SliceQueryFilter.java (line 123) collecting 0 of 1000: 8ab6d400-1dd2-11b2-ba61-0024e8e90693:false:0@1311857119500000
DEBUG [ReadStage:560] 2011-07-28 14:45:19,177 ColumnFamilyStore.java (line 1605) fetched data row ColumnFamily(EventsByUser -deleted at 1311849448186000- [8ab6d400-1dd2-11b2-8bac-0024e8e90693:true:4@13118500
71466000,8ab6d400-1dd2-11b2-93fb-0024e8e90693:true:4@1311849715863000,8ab6d400-1dd2-11b2-ba61-0024e8e90693:false:0@1311857119500000,])
DEBUG [ReadStage:560] 2011-07-28 14:45:19,177 ColumnFamilyStore.java (line 1616) adding extraFilter to cover additional expressions
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,177 StorageProxy.java (line 780) reading org.apache.cassandra.db.IndexScanCommand@39244dbe from /10.234.81.208
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,177 StorageProxy.java (line 780) reading org.apache.cassandra.db.IndexScanCommand@39244dbe from /10.226.130.128
DEBUG [ReadStage:560] 2011-07-28 14:45:19,177 ColumnFamilyStore.java (line 1640) row ColumnFamily(EventsByUser -deleted at 1311849448186000- [00000000-0000-1000-0000-000000000000:false:1@1311857119516000,8ab
6d400-1dd2-11b2-8bac-0024e8e90693:true:4@1311850071466000,8ab6d400-1dd2-11b2-93fb-0024e8e90693:true:4@1311849715863000,8ab6d400-1dd2-11b2-ba61-0024e8e90693:false:0@1311857119500000,]) satisfies all clauses
DEBUG [ReadStage:560] 2011-07-28 14:45:19,177 IndexScanVerbHandler.java (line 46) Sending RangeSliceReply{rows=Row(key=DecoratedKey(162359547821047417387034252314579509322, 74657374526573756c74446f6573436f6e
7461696e436f727265637452616e67654578636c7564696e67496e6465786564526f77), cf=ColumnFamily(EventsByUser -deleted at 1311849448186000- [00000000-0000-1000-0000-000000000000:false:1@1311857119516000,8ab6d400-1dd
2-11b2-8bac-0024e8e90693:true:4@1311850071466000,8ab6d400-1dd2-11b2-93fb-0024e8e90693:true:4@1311849715863000,8ab6d400-1dd2-11b2-ba61-0024e8e90693:false:0@1311857119500000,]))} to 14083154@/10.234.81.208
DEBUG [RequestResponseStage:3] 2011-07-28 14:45:19,178 ResponseVerbHandler.java (line 48) Processing response on a callback from 14083154@/10.234.81.208
DEBUG [RequestResponseStage:4] 2011-07-28 14:45:19,180 ResponseVerbHandler.java (line 48) Processing response on a callback from 14083155@/10.226.130.128
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,181 SliceQueryFilter.java (line 123) collecting 0 of 2147483647: 00000000-0000-1000-0000-000000000000:false:1@1311857119516000
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,181 SliceQueryFilter.java (line 123) collecting 1 of 2147483647: 8ab6d400-1dd2-11b2-8bac-0024e8e90693:true:4@1311850071466000
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,181 SliceQueryFilter.java (line 123) collecting 1 of 2147483647: 8ab6d400-1dd2-11b2-93fb-0024e8e90693:true:4@1311849715863000
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,181 SliceQueryFilter.java (line 123) collecting 1 of 2147483647: 8ab6d400-1dd2-11b2-ba61-0024e8e90693:false:0@1311857119500000
DEBUG [pool-1-thread-8858] 2011-07-28 14:45:19,181 StorageProxy.java (line 788) read Row(key=DecoratedKey(162359547821047417387034252314579509322, 74657374526573756c74446f6573436f6e7461696e436f72726563745261
6e67654578636c7564696e67496e6465786564526f77), cf=ColumnFamily(EventsByUser -deleted at 1311849448186000- [00000000-0000-1000-0000-000000000000:false:1@1311857119516000,8ab6d400-1dd2-11b2-8bac-0024e8e90693:t
rue:4@1311850071466000,8ab6d400-1dd2-11b2-93fb-0024e8e90693:true:4@1311849715863000,8ab6d400-1dd2-11b2-ba61-0024e8e90693:false:0@1311857119500000,]))
DEBUG [pool-1-thread-27] 2011-07-28 14:45:19,211 StorageService.java (line 1456) computing ranges for 0, 42535295865117307932921825928971026432, 85070591730234615865843651857942052864, 1276058875953519237987
65477786913079296
;;;","03/Aug/11 17:38;jbellis;Hi Roland,

I think the attached patch should fix the problem.  If it does not, what we need to troubleshoot is the debug logs from the data nodes, e.g., 10.234.81.208 when it the coordinator logs, ""reading org.apache.cassandra.db.IndexScanCommand@22bb5662 from /10.234.81.208"".;;;","04/Aug/11 08:46;rjtg;great i will apply the patch and give it a try.;;;","05/Aug/11 09:17;rjtg;Issue seems to be fixed. thanks.;;;","05/Aug/11 15:29;jbellis;committed;;;","05/Aug/11 18:22;hudson;Integrated in Cassandra-0.7 #538 (See [https://builds.apache.org/job/Cassandra-0.7/538/])
    prune index scan resultset back to original request
patch by jbellis; tested by Roland Gude for CASSANDRA-2964

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1154267
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
;;;","12/Jun/12 14:34;rjtg;Unfortunately this one is back in 1.1.1 (maybe in other versions as well, but i just realized it for 1.1.1 and cannot say anythiing about the intermediate versions)


We just created a new cluster on 1.1.1, imported the data from the old 0.7 cluster with sstableloader and the issue is back in the new cluster.
;;;","12/Jun/12 14:51;jbellis;Can you create a new ticket with debug logs from 1.1.1?;;;","12/Jun/12 15:06;rjtg;opened CASSANDRA-4332 for it

will attach DEBUG logs asap;;;","13/Jun/12 10:02;rjtg;opened CASSANDRA-4332 for the new problem;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
replication_factor > 1 always causes cassandra to return null,CASSANDRA-2960,12515563,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,stevecorona,stevecorona,27/Jul/11 19:37,16/Apr/19 09:32,14/Jul/23 05:52,01/Aug/11 18:42,0.8.3,,,,,,0,,,,"On a brand new cluster:
	
[default@SimpleTest] create keyspace SimpleTest2 with strategy_options = [{replication_factor:3}];              
16babc60-b886-11e0-0000-c9ff69cb2dfb
Waiting for schema agreement...
... schemas agree across the cluster

[default@SimpleTest] use SimpleTest2;
Authenticated to keyspace: SimpleTest2

[default@SimpleTest2] create column family CFTest with comparator=UTF8Type and default_validation_class=UTF8Type;
1f108660-b886-11e0-0000-c9ff69cb2dfb
Waiting for schema agreement...
... schemas agree across the cluster

[default@SimpleTest2] set CFTest['1']['text'] = 'test';
null

[default@SimpleTest2] get CFTest['1'];
null

[default@SimpleTest2] list CFTest;
Using default limit of 100
null

[default@SimpleTest2] describe cluster;
Cluster Information:
   Snitch: org.apache.cassandra.locator.SimpleSnitch
   Partitioner: org.apache.cassandra.dht.RandomPartitioner
   Schema versions: 
	1f108660-b886-11e0-0000-c9ff69cb2dfb: [10.60.98.20, 10.60.98.24, 10.60.98.26]",Ubuntu 11.04,yulinyen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/11 03:57;jbellis;2960.txt;https://issues.apache.org/jira/secure/attachment/12488338/2960.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20914,,,Tue Nov 29 14:14:06 UTC 2011,,,,,,,,,,"0|i0geiv:",93783,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"27/Jul/11 19:38;stevecorona;No errors are being reported in the logs and all nodes are up w/ the same schema version.;;;","27/Jul/11 19:58;jbellis;i assume you do actually have >= 3 nodes?

what is the full exception from the cli when you run it with --debug?;;;","27/Jul/11 20:03;stevecorona;Yes, running with 3 nodes. Same exact thing happens if I set replication_factor to 2 nodes. replication_factor=1 works as expected, however.

Here is the exception:

[default@SimpleTest2] get CFTest['1'];
null
java.lang.RuntimeException
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:302)
	at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:217)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:345)
Caused by: UnavailableException()
	at org.apache.cassandra.thrift.Cassandra$get_slice_result.read(Cassandra.java:7652)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_get_slice(Cassandra.java:570)
	at org.apache.cassandra.thrift.Cassandra$Client.get_slice(Cassandra.java:542)
	at org.apache.cassandra.cli.CliClient.doSlice(CliClient.java:468)
	at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:603)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:200)
	... 2 more;;;","27/Jul/11 20:06;jbellis;do all the nodes' ""nodetool ring"" show everyone up?

what does the server log at debug level?  (should include which hosts it thinks are unavailable);;;","27/Jul/11 20:11;stevecorona;root@cassandra01:~# nodetool ring -h 10.60.98.26
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               113427455640312814857969558651062452224     
10.60.98.26     datacenter1 rack1       Up     Normal  59.21 KB        33.33%  0                                           
10.60.98.24     datacenter1 rack1       Up     Normal  67.87 KB        33.33%  56713727820156407428984779325531226112      
10.60.98.20     datacenter1 rack1       Up     Normal  67.87 KB        33.33%  113427455640312814857969558651062452224

Will turn on debug server logs now and follow up. ;;;","27/Jul/11 20:30;stevecorona;Okay- this is what I am seeing:

When I do a read:

==> system.log <==
DEBUG [pool-2-thread-11] 2011-07-27 15:26:28,641 CassandraServer.java (line 303) get_slice

==> output.log <==
DEBUG 15:26:28,646 Command/ConsistencyLevel is SliceFromReadCommand(table='SimpleTest2', key='01', column_parent='QueryPath(columnFamilyName='CFTest', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=1000000)/ONE

==> system.log <==
DEBUG [pool-2-thread-11] 2011-07-27 15:26:28,646 StorageProxy.java (line 509) Command/ConsistencyLevel is SliceFromReadCommand(table='SimpleTest2', key='01', column_parent='QueryPath(columnFamilyName='CFTest', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=1000000)/ONE

==> output.log <==
DEBUG 15:26:28,650 Blockfor/repair is 1/true; setting up requests to 

==> system.log <==
DEBUG [pool-2-thread-11] 2011-07-27 15:26:28,650 ReadCallback.java (line 84) Blockfor/repair is 1/true; setting up requests to 

==> output.log <==
DEBUG 15:26:28,650 Live nodes  do not satisfy ConsistencyLevel (1 required)

==> system.log <==
DEBUG [pool-2-thread-11] 2011-07-27 15:26:28,650 ReadCallback.java (line 211) Live nodes  do not satisfy ConsistencyLevel (1 required);;;","27/Jul/11 20:44;jbellis;Is it possible it defaulted to NTS?;;;","27/Jul/11 21:12;stevecorona;Bingo, that's it- looks like on creating a new keyspace with replication_factor it's defaulting to NTS.

[default@unknown] create keyspace SimpleTest12 with strategy_options = [{'replication_factor':'2'}];
[default@SimpleTest12] describe keyspace;
Keyspace: SimpleTest12:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
;;;","27/Jul/11 22:39;jbellis;ah, so then it's treating your options as ""I want 2 replicas in datacenter 'replication_factor'"" but all your nodes are in 'datacenter1'.

I thought we had logic to disallow naming a dc that to prevent this confusion but apparently not.  I'll look into that.;;;","31/Jul/11 03:57;jbellis;patch to raise an exception instead of ignoring replication_factor option in NTS.;;;","01/Aug/11 17:36;slebresne;+1;;;","01/Aug/11 18:42;jbellis;committed;;;","01/Aug/11 19:14;hudson;Integrated in Cassandra-0.8 #250 (See [https://builds.apache.org/job/Cassandra-0.8/250/])
    throw exception when NTS is given replication_factoras an option
patch by jbellis; reviewed by slebresne for CASSANDRA-2960

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152890
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/NetworkTopologyStrategy.java
;;;","29/Nov/11 08:49;guizhang;we are running 0.84, looks still have this problem.

[default@unknown] create keyspace Gui3;
9deaddb0-1a4b-11e1-0000-21e4216050ff
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] use Gui3;                      
Authenticated to keyspace: Gui3
[default@Gui3] create column family gui_test3;
a997c1f0-1a4b-11e1-0000-21e4216050ff
Waiting for schema agreement...
... schemas agree across the cluster
[default@Gui3] set gui_test3['jsmith']['first'] = 'John';
Null

if we use below script
 create keyspace Gui2 with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = [{replication_factor:3}];
it works fine.

[guzhang@slcdbx5001-01 bin]$ /opt/cassandra/bin/nodetool -host localhost version
ReleaseVersion: 0.8.4
;;;","29/Nov/11 14:14;jbellis;""on creating a new keyspace ... it's defaulting to [NetworkTopologyStrategy]."";;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
long-test fails to build,CASSANDRA-2959,12515557,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,mallen,mallen,27/Jul/11 18:30,16/Apr/19 09:32,14/Jul/23 05:52,27/Jul/11 18:34,,,,Legacy/Testing,,,0,,,,"build-test:
    [javac] /var/lib/jenkins/jobs/Cassandra/workspace/build.xml:910: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
    [javac] Compiling 125 source files to /var/lib/jenkins/jobs/Cassandra/workspace/build/test/classes
    [javac] /var/lib/jenkins/jobs/Cassandra/workspace/test/unit/org/apache/cassandra/service/RemoveTest.java:172: removingNonlocal(org.apache.cassandra.dht.Token) in org.apache.cassandra.gms.VersionedValue.VersionedValueFactory cannot be applied to (org.apache.cassandra.dht.Token,org.apache.cassandra.dht.Token)
    [javac]                     valueFactory.removingNonlocal(endpointTokens.get(1), removaltoken));
    [javac]                                 ^
    [javac] /var/lib/jenkins/jobs/Cassandra/workspace/test/unit/org/apache/cassandra/service/RemoveTest.java:189: removedNonlocal(org.apache.cassandra.dht.Token) in org.apache.cassandra.gms.VersionedValue.VersionedValueFactory cannot be applied to (org.apache.cassandra.dht.Token,org.apache.cassandra.dht.Token)
    [javac]                     valueFactory.removedNonlocal(endpointTokens.get(1), removaltoken));
    [javac]                                 ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 2 errors

BUILD FAILED
/var/lib/jenkins/jobs/Cassandra/workspace/build.xml:910: Compile failed; see the compiler error output for details.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20913,,,Wed Jul 27 18:46:35 UTC 2011,,,,,,,,,,"0|i0gein:",93782,,,,,Low,,,,,,,,,,,,,,,,,"27/Jul/11 18:32;cdaw;The check-ins for the build where this error started are:
{code}
Revision: 1150847

Changes

Gossip handles dead states, token removal actually works, gossip states
are held for aVeryLongTime.
Patch by brandonwilliams and Paul Cannon, reviewed by Paul Cannon for
CASSANDRA-2496. (detail)

add ability to drop local reads/writes that are going to timeout
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-2943 (detail)
{code};;;","27/Jul/11 18:46;brandon.williams;Removed obsolete tests in r1151587;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flush memtables on shutdown when durable writes are disabled,CASSANDRA-2958,12515554,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,electrum,electrum,27/Jul/11 17:26,16/Apr/19 09:32,14/Jul/23 05:52,01/Aug/11 18:44,0.8.3,,,,,,0,,,,"Memtables need to be flushed on shutdown when durable_writes is set to false, otherwise data loss occurs as the data is not available to be replayed from the commit log. ",,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/11 17:43;jbellis;2958.txt;https://issues.apache.org/jira/secure/attachment/12488002/2958.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19339,,,Tue Oct 02 18:38:17 UTC 2012,,,,,,,,,,"0|i0geif:",93781,,tjake,,tjake,Normal,,,,,,,,,,,,,,,,,"27/Jul/11 17:43;jbellis;You're right.  Here's a patch to add flushing of non-durable CFs to the shutdown hook.;;;","27/Jul/11 17:45;jbellis;(Original durable_writes option: CASSANDRA-2683);;;","27/Jul/11 20:38;tjake;+1;;;","30/Jul/11 02:48;jbellis;committed;;;","30/Jul/11 03:22;hudson;Integrated in Cassandra-0.8 #245 (See [https://builds.apache.org/job/Cassandra-0.8/245/])
    Flush memtables on shutdown when durable writes are disabled
patch by jbellis; reviewed by tjake for CASSANDRA-2958

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152419
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/LazilyCompactedRow.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionIterator.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
;;;","01/Aug/11 17:40;electrum;It looks like waitOnFutures() can return a null pointer exception if forceFlush() returns null?;;;","01/Aug/11 18:44;jbellis;thanks -- fixed in r1152891;;;","01/Aug/11 19:14;hudson;Integrated in Cassandra-0.8 #250 (See [https://builds.apache.org/job/Cassandra-0.8/250/])
    avoid NPE when flushing in shutdown hook
patch by jbellis for CASSANDRA-2958

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152891
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
;;;","24/Aug/12 22:40;rcoli;If I am running with ""durable_writes off,"" I know this means ""Data in a memtable is not shadowed by data in the commitlog and is therefore lost when memory is cleared.""

As a result, when my node crashes or is shut down, I *expect* to lose data that is not flushed. If I want to prevent this from happening, I can run ""nodetool drain"" to flush my memtables and prevent any new ones from being created.

This patch violates the principle of least surprise by triggering a flush as a side effect of shutdown, but only in certain cases. Now, instead of knowing that unflushed/drained data is lost with durable_writes off, I have to understand that I lose my data if my node crashes and keep my data if I manually shut it down. ""Data in a memtable is not shadowed by data in the commitlog and is therefore lost when memory is cleared"" becomes an incorrect heuristic for understanding durable_writes. After this patch, the formerly unambiguous ""durable_writes off"" setting seems to mean something like ""durable_writes sometimes_off"".

While I laud the goal of not losing peoples data, I assert that when you explicitly tell Cassandra to do something that by design loses data, Cassandra should do what you tell it to do.;;;","24/Aug/12 22:46;jbellis;if you want data loss from non-durable writes, kill -9 is your friend.  otherwise, our goal is to not lose more than necessary.;;;","31/Aug/12 21:49;rcoli;What I ""want"" is to be able to explain Cassandra to customers in simple sentences like ""a memtable holds changes until it is full enough to hit a flush condition or you explicitly flush it"" without having to pepper these sentences with caveats like ""except if you restart your node or if you stop it with durable_writes off, triggering an unexpected flush."" Patches such as this one, where vaguely defined ends appear to justify whatever ad-hoc inconsistent means, do not appear to further this goal.

Let me phrase my objection to this patch in another way...

""What does this patch gain us, and at what cost?""

Your stated goal is to not lose more than ""necessary"" when stopping a node. It seems your goal can be achieved without patching, by simply advising ""durable_writes off"" operators to run ""nodetool drain"" when stopping a node. They are, after all, the ones stopping their node and are perfectly capable of draining it if they do not want to lose the explicitly non-durable non-durable_writes contents of memtables.

From what I can tell, the only thing this patch gains us is ""people who are running with durable_writes off don't have to run 'nodetool drain' before stopping nodes.""

What we trade for that is the until-now universal expectation that stopping a Cassandra node never triggers a flush.

Is ""the very small group of operators who run with non-durable writes don't have to run 'nodetool drain'"" such a compelling win that we should change a fundamental behavior of Cassandra, making it less predictable, in order to obtain it? My answer is no.;;;","31/Aug/12 21:57;brandon.williams;bq. be able to explain Cassandra to customers in simple sentences like ""a memtable holds changes until it is full enough to hit a flush condition or you explicitly flush it""

""a memtable holds changes until it hits a flush condition like being full or graceful shutdown, or you explicitly flush it."";;;","04/Sep/12 19:25;rcoli;> ""a memtable holds changes until it hits a flush condition like being full or graceful shutdown, or you explicitly flush it.""

This is not an unambiguous summary of the current behavior. It is true only if durable_writes are off. If durable_writes are on, graceful shutdown does not flush.

""a memtable holds changes until it hits a flush condition like being full (or, if durable_writes are disabled, on graceful shutdown), or you explicitly flush it""

Seems to unambiguously describe the current behavior and doesn't read very well.

I gather from CASSANDRA-3564 that there is interest in extending this flush-on-graceful shutdown behavior to occur in all cases of graceful shutdown. If that happens, your sentence will be a correct summation of Cassandra's newly predictable behavior. It will also answer my primary objection here, which is that the flush only occurs in *some* graceful shutdown cases. :);;;","04/Sep/12 19:31;jbellis;Robert, we use assign-to to track who fixed an issue.  Assigning to yourself at this point is not appropriate.;;;","02/Oct/12 18:38;rcoli;Jonathan, I had no intent of assigning this ticket to myself or wiping the description body, both of which I apparently accidentally did in the process of clicking on this ticket. I will restore the description body from the history, sorry for the confusion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC Unit test failed to run due to spurious character in text and bad YAML entry,CASSANDRA-2957,12515546,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ardot,ardot,ardot,27/Jul/11 16:08,16/Apr/19 09:32,14/Jul/23 05:52,27/Jul/11 17:28,,,,,,,0,,,,"Problem #1:

Bad character in the text (line 294, col 1):
{code}
˜        PreparedStatement stmt = con.prepareStatement(""update JdbcInteger set ?=?, ?=? where key = ?"");
{code}

Problem #2:
Outdated YAML directive (line 17):

{code}
commitlog_rotation_threshold_in_mb: 128
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/11 16:09;ardot;jdbc-unittest-fix-v1.txt;https://issues.apache.org/jira/secure/attachment/12487991/jdbc-unittest-fix-v1.txt",,,,,,,,,,,,,,1.0,ardot,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20912,,,Wed Jul 27 17:28:06 UTC 2011,,,,,,,,,,"0|i0gei7:",93780,,,,,Low,,,,,,,,,,,,,,,,,"27/Jul/11 16:10;ardot;Patch is against trunk;;;","27/Jul/11 17:28;jbellis;+1, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC ResultSet improperly handles null column values,CASSANDRA-2956,12515544,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ardot,ardot,ardot,27/Jul/11 15:56,16/Apr/19 09:32,14/Jul/23 05:52,27/Jul/11 17:26,,,,,,,0,JDBC,,,"JDBC {{ResultSet}} getters return built-in datatypes such as {{int, long, short, byte, boolean}} that are not capable of handling a null value. As a consequence, it provides a method: {{wasNull}} which returns true if the value was null. The spec requires a zero numeric value (or false in the case of {{boolean}} ) is returned by the getter. This was being mis-handled and a null value was being cast (boxed) to the return value. An NPE would result.
",,ardot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/11 15:57;ardot;jdbc-bugfix-handling-null-values.txt;https://issues.apache.org/jira/secure/attachment/12487988/jdbc-bugfix-handling-null-values.txt",,,,,,,,,,,,,,1.0,ardot,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20911,,,Wed Jul 27 17:26:30 UTC 2011,,,,,,,,,,"0|i0gehz:",93779,,,,,Low,,,,,,,,,,,,,,,,,"27/Jul/11 17:26;jbellis;+1, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cassandra.bat fails when CASSANDRA_HOME contains a whitespace, again",CASSANDRA-2952,12515482,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,miau,miau,miau,27/Jul/11 07:24,16/Apr/19 09:32,14/Jul/23 05:52,12/Aug/11 22:05,0.8.5,,,Packaging,,,0,,,,"I installed cassandra into C:\Program Files\apache-cassandra and tried to start cassandra. But cassandra.bat fails with following error.

{code} 
C:\Program Files\apache-cassandra>bin\cassandra.bat
Starting Cassandra Server
Error opening zip file or JAR manifest missing : C:\Program
Error occurred during initialization of VM
agent library failed to init: instrument
{code}

This problem is similar to CASSANDRA-2237. I'll post a patch to fix the problem later.",Windows7 32bit,bcoverston,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/11 07:30;miau;CASSANDRA-2952.diff;https://issues.apache.org/jira/secure/attachment/12487945/CASSANDRA-2952.diff",,,,,,,,,,,,,,1.0,miau,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20909,,,Sat Aug 13 04:59:01 UTC 2011,,,,,,,,,,"0|i0gegv:",93774,,bcoverston,,bcoverston,Low,,,,,,,,,,,,,,,,,"12/Aug/11 21:12;bcoverston;+1, adding quotes around the string will work.
Also checked for other places where we may not have been adding quotes, and they are fine.;;;","12/Aug/11 22:05;jbellis;committed, thanks!;;;","12/Aug/11 23:21;hudson;Integrated in Cassandra-0.8 #276 (See [https://builds.apache.org/job/Cassandra-0.8/276/])
    fix cassandra.bat when CASSANDRA_HOME contains spaces
patch by Koji Ando; reviewed by bcoverston for CASSANDRA-2952

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1157268
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/bin/cassandra.bat
;;;","13/Aug/11 04:59;miau;I've verified that Cassandra-0.8 #276 works fine. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FreeableMemory can be accessed after it is invalid,CASSANDRA-2951,12515459,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,26/Jul/11 22:02,16/Apr/19 09:32,14/Jul/23 05:52,27/Jul/11 20:58,0.8.3,,,,,,0,,,,"SerializingCache.get looks like this:

{code}
    public V get(Object key)
    {
        FreeableMemory mem = map.get(key);
        if (mem == null)
            return null;
        return deserialize(mem);
    }
{code}

If a cache object is evicted or replaced after the get happens, but before deserialize completes, we will trigger an assertion failure (if asserts are enabled) or segfault (if they are not).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/11 22:03;jbellis;2951.txt;https://issues.apache.org/jira/secure/attachment/12487902/2951.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20908,,,Thu Jul 28 07:25:21 UTC 2011,,,,,,,,,,"0|i0gegn:",93773,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"26/Jul/11 22:03;jbellis;patch adds reference counting to FreeableMemory;;;","27/Jul/11 15:51;slebresne;I think there is a race between eviction and remove (or two removes even), so that the references can be < 0. So I think the '== 0' in reference() and finalize() (but not unreference() obviously) should become '<= 0'.

With that corrected, looks good. +1.;;;","27/Jul/11 20:58;jbellis;quite right.  committed with that change.;;;","27/Jul/11 21:16;hudson;Integrated in Cassandra #978 (See [https://builds.apache.org/job/Cassandra/978/])
    fix potential use of free'd native memory interface/SerializingCache
patch by jbellis; reviewed by slebresne for CASSANDRA-2951

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1151625
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/cache/FreeableMemory.java
* /cassandra/trunk/src/java/org/apache/cassandra/cache/SerializingCache.java
* /cassandra/trunk/CHANGES.txt
;;;","28/Jul/11 07:25;slebresne;You forgot to use <= 0 in finalize in you commit, I took the liberty to change it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data from truncated CF reappears after server restart,CASSANDRA-2950,12515455,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,cdaw,cdaw,26/Jul/11 20:56,16/Apr/19 09:32,14/Jul/23 05:52,11/Aug/11 19:36,0.8.5,,,,,,1,,,,"* Configure 3 node cluster
* Ensure the java stress tool creates Keyspace1 with RF=3

{code}
// Run Stress Tool to generate 10 keys, 1 column
stress --operation=INSERT -t 2 --num-keys=50 --columns=20 --consistency-level=QUORUM --average-size-values --replication-factor=3 --create-index=KEYS --nodes=cathy1,cathy2

// Verify 50 keys in CLI
use Keyspace1; 
list Standard1; 

// TRUNCATE CF in CLI
use Keyspace1;
truncate counter1;
list counter1;

// Run stress tool and verify creation of 1 key with 10 columns
stress --operation=INSERT -t 2 --num-keys=1 --columns=10 --consistency-level=QUORUM --average-size-values --replication-factor=3 --create-index=KEYS --nodes=cathy1,cathy2

// Verify 1 key in CLI
use Keyspace1; 
list Standard1; 

// Restart all three nodes

// You will see 51 keys in CLI
use Keyspace1; 
list Standard1; 
{code}


",,ardot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/11 19:46;jbellis;2950-v2.txt;https://issues.apache.org/jira/secure/attachment/12490029/2950-v2.txt","11/Aug/11 12:53;slebresne;2950-v3_0.8.patch;https://issues.apache.org/jira/secure/attachment/12490112/2950-v3_0.8.patch","10/Aug/11 14:57;jbellis;2950.txt;https://issues.apache.org/jira/secure/attachment/12489986/2950.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20907,,,Thu Aug 11 20:21:40 UTC 2011,,,,,,,,,,"0|i0gegf:",93772,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"27/Jul/11 01:37;cdaw;This is a general issue with all CF's. updating bug.;;;","27/Jul/11 01:47;cdaw;The other permutation of this bug looked like, assuming write with CL.Q:
* Insert 50 (3 nodes up)
* truncate CF (3 nodes up)
* Insert 1 (3 nodes up)
* Bring node3 down
* Delete 1  (2 nodes up)
* Bring up node3 and run repair
* Take down node1 and node2.
* Query node3 with CL.ONE: list Standard1;  --- 30 rows returned

Not sure, but this looked suspicious in my logs:
{code}
 INFO 01:19:45,616 Streaming to /50.57.114.45
 INFO 01:19:45,689 Finished streaming session 698609583499991 from /50.57.107.176
 INFO 01:19:45,690 Finished streaming session 698609609994154 from /50.57.114.45
 INFO 01:19:46,501 Finished streaming repair with /50.57.114.45 for (0,56713727820156410577229101238628035242]: 0 oustanding to complete session
 INFO 01:19:46,531 Compacted to /var/lib/cassandra/data/Keyspace1/Standard1-tmp-g-106-Data.db.  16,646,523 to 16,646,352 (~99% of original) bytes for 30 keys.  Time: 1,509ms.
 INFO 01:19:46,930 Finished streaming repair with /50.57.107.176 for (113427455640312821154458202477256070484,0]: 1 oustanding to complete session
 INFO 01:19:47,619 Finished streaming repair with /50.57.114.45 for (113427455640312821154458202477256070484,0]: 0 oustanding to complete session
 INFO 01:19:48,232 Finished streaming repair with /50.57.107.176 for (56713727820156410577229101238628035242,113427455640312821154458202477256070484]: 1 oustanding to complete session
 INFO 01:19:48,856 Finished streaming repair with /50.57.114.45 for (56713727820156410577229101238628035242,113427455640312821154458202477256070484]: 0 oustanding to complete session
{code};;;","09/Aug/11 21:31;brandon.williams;Currently, truncate does:
* force a flush
* record the time
* delete any sstables older than the time

This isn't quite enough if the machine crashes shortly afterward, however, since there can be mutations present in the commitlog that were previously truncated and are now resurrected by CL replay.

One thing we could do is record the truncate time for the CF in the system ks and then ignore mutations older than that, however this would require time synchronization between the client and the server to be accurate.
;;;","09/Aug/11 22:12;jbellis;but we record CL ""context"" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.

checked and we do wait for flush to complete in truncate.;;;","09/Aug/11 22:19;brandon.williams;bq. but we record CL ""context"" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.

I think there's something wrong with that, then:

{noformat}
 INFO 21:25:15,274 Replaying /var/lib/cassandra/commitlog/CommitLog-1312924388053.log
DEBUG 21:25:15,290 Replaying /var/lib/cassandra/commitlog/CommitLog-1312924388053.log starting at 0
DEBUG 21:25:15,291 Reading mutation at 0
DEBUG 21:25:15,295 replaying mutation for system.4c: {ColumnFamily(LocationInfo [47656e65726174696f6e:false:4@1312924388140000,])}
DEBUG 21:25:15,321 Reading mutation at 89
DEBUG 21:25:15,322 replaying mutation for system.426f6f747374726170: {ColumnFamily(LocationInfo [42:false:1@1312924388203,])}
DEBUG 21:25:15,322 Reading mutation at 174
DEBUG 21:25:15,322 replaying mutation for system.4c: {ColumnFamily(LocationInfo [546f6b656e:false:16@1312924388204,])}
DEBUG 21:25:15,322 Reading mutation at 270
DEBUG 21:25:15,324 replaying mutation for Keyspace1.3030: {ColumnFamily(Standard1 [C0:false:34@1312924813259,C1:false:34@1312924813260,C2:false:34@1312924813260,C3:false:34@1312924813260,C4:false:34@1312924813260,])}
{noformat}

The last entry there is the first of many errant mutations.;;;","10/Aug/11 14:57;jbellis;Ah, CASSANDRA-2419 keeps on giving...

bq. but we record CL ""context"" at time of flush in the sstable it makes, and we on replay we ignore any mutations from before that position.

The obvious problem with this is that the point of truncate is to blow away such sstables...  Patch attached.  Comment explains the core fix:

{noformat}
// Bonus complication: since we store replay position in sstable metadata,
// truncating those sstables means we will replay any CL segments from the
// beginning if we restart before they are discarded for normal reasons
// post-truncate.  So we need to (a) force a new segment so the currently
// active one can be discarded, and (b) flush *all* CFs so that unflushed
// data in others don't keep any pre-truncate CL segments alive.
{noformat}

Patch also fixes the bug in ReplayManagerTruncateTest that made it miss this.
;;;","10/Aug/11 16:06;slebresne;I think the forceFlush of all the CF is not safe, because if for a given column family the memtable is clean, forceFlush will return immediately, even though there could be a memtable being flush at the same time (or pending flush). So we cannot be sure all the old segment are clean after the waitFutures (I know, it took me some time to figure out some problem with repair for this very reason when the repairs were not properly synchronized).

What we would need is to add to the future we wait on the futures of all the flush being processed at that time. Sounds annoying though. ;;;","10/Aug/11 16:09;brandon.williams;+1, though this patch is against trunk, not 0.8.  Also mistakenly bumps the log4j level to debug.;;;","10/Aug/11 19:46;jbellis;v2:

{noformat}
// Bonus bonus: simply forceFlush of all the CF is not enough, because if
// for a given column family the memtable is clean, forceFlush will return
// immediately, even though there could be a memtable being flush at the same
// time.  So to guarantee that all segments can be cleaned out, we need
// ""waitForActiveFlushes"" after the new segment has been created.
{noformat};;;","11/Aug/11 12:53;slebresne;Attaching a v3 that is rebased against 0.8. I've also slightly change the logic in Truncate to submit all the flushes and then call waitForActiveFlushes, as this is slightly simpler and should work equally well as far as I can tell.
Apart from that, this lgtm.;;;","11/Aug/11 19:36;jbellis;committed;;;","11/Aug/11 20:21;hudson;Integrated in Cassandra-0.8 #272 (See [https://builds.apache.org/job/Cassandra-0.8/272/])
    make sure truncate clears out the commitlog
patch by jbellis; reviewed by slebresne for CASSANDRA-2950

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156763
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch mutation of counters in multiple supercolumns throws an exception during replication.,CASSANDRA-2949,12515424,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,soverton,soverton,26/Jul/11 16:19,16/Apr/19 09:32,14/Jul/23 05:52,01/Aug/11 15:13,0.8.3,,,,,,0,batch_mutate,counters,supercolumns,"Steps to reproduce:
* Perform a batch mutation of more than one counter in more than one super-column in the same column-family.
* The following exception is thrown during replication:

DEBUG [MutationStage:63] 2011-07-26 17:05:12,653 CounterMutationVerbHandler.java (line 52) Applying forwarded CounterMutation(RowMutation(keyspace='ks1', key='4ae71336e44bf9bf', modifications=[ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c30 [302c636f6c30:false:8@1311696312648,]),SuperColumn(302c7375706572636f6c31 [302c636f6c30:false:8@1311696312648,]),])]), QUORUM)
DEBUG [MutationStage:63] 2011-07-26 17:05:12,653 StorageProxy.java (line 432) insert writing local & replicate CounterMutation(RowMutation(keyspace='ks1', key='4ae71336e44bf9bf', modifications=[cf1]), QUORUM)
DEBUG [MutationStage:63] 2011-07-26 17:05:12,654 Table.java (line 398) applying mutation of row 4ae71336e44bf9bf
ERROR [ReplicateOnWriteStage:125] 2011-07-26 17:05:12,655 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[ReplicateOnWriteStage:125,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: ColumnFamily ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c31 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),]) already has modifications in this mutation: ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c30 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),])
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.IllegalArgumentException: ColumnFamily ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c31 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),]) already has modifications in this mutation: ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c30 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),])
        at org.apache.cassandra.db.RowMutation.add(RowMutation.java:123)
        at org.apache.cassandra.db.CounterMutation.makeReplicationMutation(CounterMutation.java:120)
        at org.apache.cassandra.service.StorageProxy$5$1.runMayThrow(StorageProxy.java:455)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/11 16:21;soverton;2949.patch;https://issues.apache.org/jira/secure/attachment/12487861/2949.patch","27/Jul/11 10:39;slebresne;2949_v2.patch;https://issues.apache.org/jira/secure/attachment/12487959/2949_v2.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20906,,,Mon Aug 01 15:22:36 UTC 2011,,,,,,,,,,"0|i0geg7:",93771,,soverton,,soverton,Critical,,,,,,,,,,,,,,,,,"26/Jul/11 16:21;soverton;Attached patch which fixes this issue by adding an ""addAll"" method to RowMutation which allows the same CF to be explicitly added to a mutation more than once.;;;","27/Jul/11 10:39;slebresne;The patch does fix it, but I think there is a better fix. The reason the assertion is triggered is because when we read to replicate and there is super columns, we end up adding one read command for each super column (and thus get back multiple time the same CF, which triggers the assertion). The rational being that we only query the subcolumns that are in the original mutation.

However, since we deserialize full super columns anyway, it will be more efficient to generate only one read command for all the super columns (reading full super columns) and to filter afterwards the subcolumns we don't want to bother sending to the other nodes.

Attaching a ""v2"" patch that does this. It also ship with a unit test.
;;;","27/Jul/11 12:22;soverton;Confirmed v2 patch fixes the issue and test passes.

If it's likely that supercolumns may get indexes (CASSANDRA-598) or the serialization format changes to make direct sub-column access possible (CASSANDRA-674) then this should probably be revisited to limit the scope of the read.;;;","27/Jul/11 15:12;slebresne;bq. If it's likely that supercolumns may get indexes (CASSANDRA-598) or the serialization format changes to make direct sub-column access possible (CASSANDRA-674) then this should probably be revisited to limit the scope of the read.

Regarding super columns, the more likely move will be to replace them internally by columns with composite names, in which case this issue will be moot anyway.;;;","01/Aug/11 15:13;slebresne;Committed, thanks;;;","01/Aug/11 15:22;hudson;Integrated in Cassandra-0.8 #248 (See [https://builds.apache.org/job/Cassandra-0.8/248/])
    don't throw exception on batch of counter super columns
patch by slebresne; reviewed by soverton for CASSANDRA-2949

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152795
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/RowMutation.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SuperColumn.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamily.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/CounterMutationTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/CounterMutation.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool move fails to stream out data from moved node to new endpoint.,CASSANDRA-2948,12515417,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,soverton,soverton,soverton,26/Jul/11 15:42,16/Apr/19 09:32,14/Jul/23 05:52,27/Jul/11 22:47,0.8.3,,,,,,0,move,streaming,,"When moving a node in the ring with nodetool move, that node streams its data to itself instead of to the new endpoint responsible for its old range.

Steps to reproduce:
* Create a cluster (A,B,C,D) with tokens (0,4,8,C) using ByteOrderedPartitioner
* Create a keyspace and CF with RF=1
* Insert keys (2,6,A,E). This should put one key on each node.
* Move node A to token 7. This should cause:
  + node C streams key 6 to node A
  + node A streams key E to node B
  However instead, node A streams key E to itself.

Selected log messages from node A:
 INFO [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:17,075 StorageService.java (line 1878) Moving miles/10.2.129.41 from Token(bytes[00]) to Token(bytes[07]).
DEBUG [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:17,080 StorageService.java (line 1941) Table ks: work map {/10.2.129.16=[(Token(bytes[04]),Token(bytes[07])]]}.
 INFO [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:17,080 StorageService.java (line 1946) Sleeping 30000 ms before start streaming/fetching ranges.
...
 INFO [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:46,728 StorageService.java (line 522) Moving: fetching new ranges and streaming old ranges
DEBUG [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:46,728 StorageService.java (line 1960) [Move->STREAMING] Work Map: {ks={(Token(bytes[0c]),Token(bytes[00])]=[miles/10.2.129.41]}}
DEBUG [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:46,729 StorageService.java (line 1965) [Move->FETCHING] Work Map: {ks={/10.2.129.16=[(Token(bytes[04]),Token(bytes[07])]]}}
DEBUG [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:46,730 StorageService.java (line 2411) Requesting from /10.2.129.16 ranges (Token(bytes[04]),Token(bytes[07])]
...
 INFO [StreamStage:1] 2011-07-26 16:29:46,737 StreamOut.java (line 90) Beginning transfer to miles/10.2.129.41
DEBUG [StreamStage:1] 2011-07-26 16:29:46,737 StreamOut.java (line 91) Ranges are (Token(bytes[0c]),Token(bytes[00])]

This appears to be caused because in StorageService.move we call
    Gossiper.instance.addLocalApplicationState(ApplicationState.STATUS, valueFactory.moving(newToken));
and then get the new token metadata in order to calculate where the new endpoint is that we should stream to
    TokenMetadata tokenMetaClone = tokenMetadata_.cloneAfterAllSettled();
however, in addLocalApplicationState there is no notification broadcast for the change in local state, so tokenMetadata_ never updates the list of moving nodes, and the tokenMetaClone is still the state of the ring from before the move.

",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/11 15:48;soverton;2948.patch;https://issues.apache.org/jira/secure/attachment/12487854/2948.patch",,,,,,,,,,,,,,1.0,soverton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20905,,,Wed Jul 27 23:21:20 UTC 2011,,,,,,,,,,"0|i0gefz:",93770,,brandon.williams,,brandon.williams,Critical,,,,,,,,,,,,,,,,,"26/Jul/11 15:48;soverton;Attached patch which will cause Gossiper to broadcast notifications when the local application state is updated.

It also fixes the calculation of the endpoints to stream from, which should use the old state of the ring, not the new one.;;;","27/Jul/11 22:47;brandon.williams;Committed, thanks!;;;","27/Jul/11 23:21;hudson;Integrated in Cassandra-0.8 #242 (See [https://builds.apache.org/job/Cassandra-0.8/242/])
    Gossiper notifies of local state changes.
Patch by Sam Overton, reviewed by brandonwilliams for CASSANDRA-2948

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1151659
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HintedHandoff fails with could not reach schema agreement,CASSANDRA-2946,12515285,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,richardlow,richardlow,25/Jul/11 16:38,16/Apr/19 09:32,14/Jul/23 05:52,27/Jul/11 15:14,0.8.3,,,,,,0,,,,"To reproduce, have two nodes A and B.

1. On node A, create a keyspace with replication factor 1 and add a column family
2. Ensure node B has created the keyspace and column family
3. Take down node B
4. Insert some keys to A at CL.ANY, ensuring some keys should be written to B
5. Bring up node B
6. When hints are delivered, I get the error:

ERROR [HintedHandoff:1] 2011-07-25 17:19:14,729 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.RuntimeException: Could not reach schema agreement with /10.2.129.9 in 60000ms
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: Could not reach schema agreement with /10.2.129.9 in 60000ms
        at org.apache.cassandra.db.HintedHandOffManager.waitForSchemaAgreement(HintedHandOffManager.java:290)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:301)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:89)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:394)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more

If I use DatabaseDescriptor.getDefsVersion() instead of gossiper.getEndpointStateForEndpoint(FBUtilities.getLocalAddress()).getApplicationState(ApplicationState.SCHEMA) then the error goes away, and the hints are correctly delivered.

This may be the same issue as Aaron saw here: http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/ApplicationState-Schema-has-drifted-from-DatabaseDescriptor-td6006576.html, and may be related to CASSANDRA-2083.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/11 13:57;jbellis;2946.txt;https://issues.apache.org/jira/secure/attachment/12487836/2946.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20903,,,Wed Jul 27 16:12:39 UTC 2011,,,,,,,,,,"0|i0gefj:",93768,,richardlow,,richardlow,Normal,,,,,,,,,,,,,,,,,"26/Jul/11 03:36;jbellis;does the disagreement show up with ""describe cluster"" from the cli?  if so, at what point do the schemas of A and B diverge?;;;","26/Jul/11 10:28;richardlow;describe cluster always shows agreement.  Selected log messages:

DEBUG [HintedHandoff:1] 2011-07-26 11:22:35,526 HintedHandOffManager.java (line 300) Checking remote schema before delivering hints
...
DEBUG [pool-2-thread-1] 2011-07-26 11:22:44,965 CassandraServer.java (line 1123) checking schema agreement
...
DEBUG [pool-2-thread-1] 2011-07-26 11:22:44,969 StorageProxy.java (line 823) Schemas are in agreement.
...
ERROR [HintedHandoff:1] 2011-07-26 11:23:36,788 AbstractCassandraDaemon.java (line 138) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.RuntimeException: Could not reach schema agreement with /10.2.129.9 in 60000ms

So StorageProxy thinks the schema agrees but HintedHandoffManager doesn't.;;;","26/Jul/11 13:57;jbellis;this should fix the gossip-out-of-sync bug.

if it does we can also change the HH code to use DD.;;;","26/Jul/11 19:21;brandon.williams;+1, what happened was node A never updated it's gossip state, but the other node learned of its schema version by RPC.  When node B restarted, it no longer knew the schema version for A, and tried to get it from gossip where it was still old.

bq. if it does we can also change the HH code to use DD.
I don't know about that, having this as a check to make sure this doesn't happen again is a far easier thing to catch than other ""gossip is out of date"" problems we might encounter.;;;","26/Jul/11 22:05;jbellis;okay, then I'll add a comment to that effect, assuming Richard confirms this fixes the bug.;;;","27/Jul/11 09:03;richardlow;Yes, that fix works.  Thanks.;;;","27/Jul/11 15:14;jbellis;Committed to 0.8.3 and trunk.  (And verified that this is not a problem on the 0.7 branch.);;;","27/Jul/11 16:12;hudson;Integrated in Cassandra-0.8 #240 (See [https://builds.apache.org/job/Cassandra-0.8/240/])
    keep gossipped version in sync with actual on migration coordinator
patch by jbellis; reviewed by brandonwilliams and tested by Richard Low for CASSANDRA-2946

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1151494
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/Migration.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/HintedHandOffManager.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dropped columnfamilies can leave orphaned data files that do not get cleared on restart,CASSANDRA-2942,12515146,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,cdaw,cdaw,23/Jul/11 00:57,16/Apr/19 09:32,14/Jul/23 05:52,29/Aug/11 15:12,1.0.0,,,,,,0,,,,"* Bring up 3 node cluster
* From node1: Run Stress Tool
{code} stress --num-keys=10 --columns=10 --consistency-level=ALL --average-size-values --replication-factor=3 --nodes=node1,node2 {code}
* Shutdown node3
* From node1: drop the Standard1 CF in Keyspace1
* Shutdown node2 and node3
* Bring up node1 and node2. Check that the Standard1 files are gone.
{code}
ls -al /var/lib/cassandra/data/Keyspace1/
{code}
* Bring up node3. The log file shows the drop column family occurs
{code}
 INFO 00:51:25,742 Applying migration 9a76f880-b4c5-11e0-0000-8901a7c5c9ce Drop column family: Keyspace1.Standard1
{code}
* Restart node3 to clear out dropped tables from the filesystem
{code}
root@cathy3:~/cass-0.8/bin# ls -al /var/lib/cassandra/data/Keyspace1/
total 36
drwxr-xr-x 3 root root 4096 Jul 23 00:51 .
drwxr-xr-x 6 root root 4096 Jul 23 00:48 ..
-rw-r--r-- 1 root root    0 Jul 23 00:51 Standard1-g-1-Compacted
-rw-r--r-- 2 root root 5770 Jul 23 00:51 Standard1-g-1-Data.db
-rw-r--r-- 2 root root   32 Jul 23 00:51 Standard1-g-1-Filter.db
-rw-r--r-- 2 root root  120 Jul 23 00:51 Standard1-g-1-Index.db
-rw-r--r-- 2 root root 4276 Jul 23 00:51 Standard1-g-1-Statistics.db
drwxr-xr-x 3 root root 4096 Jul 23 00:51 snapshots
{code}
*Bug:  The files for Standard1 are orphaned on node3*

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/11 02:38;jbellis;2942.txt;https://issues.apache.org/jira/secure/attachment/12491434/2942.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20902,,,Mon Aug 29 16:23:17 UTC 2011,,,,,,,,,,"0|i0geen:",93764,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"23/Jul/11 02:22;jbellis;You should be able to reproduce this even on a single node -- just drop a CF, then restart.  It only cleans out marked-for-delete files from known CFs.

May be fixed by CASSANDRA-2521.  Otherwise we can add ""go ahead and clear out marked-for-delete files, even if they don't belong to an active CF"" logic.;;;","23/Aug/11 03:00;jbellis;bq. May be fixed by CASSANDRA-2521

2521 makes it substantially better, but there's still a window where you can miss deletes.

The ""real"" fix is to commitlog-ify schema changes, but that's outside our scope for the forseeable future.

Adding ""wait for outstanding SSTableDeletingTasks"" to our jvm shutdown hook would be almost as good.;;;","24/Aug/11 02:38;jbellis;patch to wait for StorageService.tasks on shutdown.  Also moves CL segment deletion there.;;;","29/Aug/11 14:56;slebresne;nit: we could log an info message when awaitTermination returns false. It also look like DeletionService could just go away with with this.

But otherwise, +1. I agree it is good enough and not worth going for more complicated.;;;","29/Aug/11 15:12;jbellis;committed;;;","29/Aug/11 16:23;hudson;Integrated in Cassandra #1054 (See [https://builds.apache.org/job/Cassandra/1054/])
    reduce window where dropped CF sstables may not be deleted
patch by jbellis; reviewed by slebresne for CASSANDRA-2942

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1162849
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/DeletionService.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/util/FileUtils.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL regex to match column family in query,CASSANDRA-2939,12515057,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bcvisin,bcvisin,bcvisin,22/Jul/11 19:33,16/Apr/19 09:32,14/Jul/23 05:52,08/Aug/11 18:47,,,,,,,0,CQL,python,,"In the file cursor.py (https://svn.apache.org/repos/asf/cassandra/drivers/py/cql/cursor.py)

{code}Line 37: _cfamily_re = re.compile(""\s*SELECT\s+.+\s+FROM\s+[\']?(\w+)"", re.I | re.M){code}

The Regex will improperly match anything after the word  'from' even if it is in the WHERE clause

I believe the fix is:

{code}_cfamily_re = re.compile(""\s*SELECT\s+.+?\s+FROM\s+[\']?(\w+)"", re.I | re.M){code}

Added the ? so the regex is not so greedy

use this query to reproduce the results:

SELECT key FROM column_family WHERE key = 'break from chores'""",Python CQL,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/11 17:29;thobbs;2939-test.txt;https://issues.apache.org/jira/secure/attachment/12489714/2939-test.txt","08/Aug/11 17:29;thobbs;2939.txt;https://issues.apache.org/jira/secure/attachment/12489713/2939.txt",,,,,,,,,,,,,2.0,bcvisin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20901,,,Mon Aug 08 18:47:43 UTC 2011,,,,,,,,,,"0|i0gedz:",93761,,thobbs,,thobbs,Low,,,,,,,,,,,,,,,,,"08/Aug/11 17:29;thobbs;2939.txt updates the regex to the one provided by Blake. (In case you're not familiar with the syntax, ""\+?"" is a non-greedy version of ""+"".)

2939-test.txt adds a small unit test to exercise the Cursor._cfamily_re regex.;;;","08/Aug/11 18:47;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
certain generic type causes compile error in eclipse,CASSANDRA-2937,12514978,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,yangyangyyy,yangyangyyy,21/Jul/11 23:22,16/Apr/19 09:32,14/Jul/23 05:52,22/Jul/11 01:50,,,,,,,0,,,,"the code ColumnFamily and AbstractColumnContainer uses code similar to the following (substitute Blah with AbstractColumnContainer.DeletionInfo):



import java.util.concurrent.atomic.AtomicReference;
public class TestPrivateAtomicRef {
    protected final AtomicReference<Blah> b = new AtomicReference<Blah>(new Blah());
    // the following form would work for eclipse
//    protected final AtomicReference b = new AtomicReference(new Blah());

    private static class Blah {
    }
}


class Child extends TestPrivateAtomicRef {    
    public void aaa() {
        Child c = new Child();
        c.b.set(
        b.get()  //<==== eclipse shows error here
        );
    }
}


in eclipse, the above code generates compile error, but works fine under java command line. since many people use eclipse, it's better to 
make a temporary compromise and make DeletionInfo protected",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/11 23:27;yangyangyyy;0002-avoid-eclipse-compile-error-for-generic-type-on-Atom.patch;https://issues.apache.org/jira/secure/attachment/12487373/0002-avoid-eclipse-compile-error-for-generic-type-on-Atom.patch",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20899,,,Fri Jul 22 01:50:33 UTC 2011,,,,,,,,,,"0|i0gedj:",93759,,,,,Low,,,,,,,,,,,,,,,,,"21/Jul/11 23:27;yangyangyyy;minor fix to avoid Eclipse compile error;;;","22/Jul/11 01:50;jbellis;committed, although in general I'm against humoring broken tools;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool hangs (doesn't return prompt) if you specify a table that doesn't exist or a KS that has no CF's,CASSANDRA-2933,12514940,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,cdaw,cdaw,21/Jul/11 16:02,16/Apr/19 09:32,14/Jul/23 05:52,27/Jul/11 15:25,0.8.3,,,Tool/nodetool,,,0,lhf,,,"Invalid CF
{code}
ERROR 02:18:18,904 Fatal exception in thread Thread[AntiEntropyStage:3,5,main]
java.lang.IllegalArgumentException: Unknown table/cf pair (StressKeyspace.StressStandard)
	at org.apache.cassandra.db.Table.getColumnFamilyStore(Table.java:147)
	at org.apache.cassandra.service.AntiEntropyService$TreeRequestVerbHandler.doVerb(AntiEntropyService.java:601)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}


Empty KS
{code}
 INFO 02:19:21,483 Waiting for repair requests: []
 INFO 02:19:21,484 Waiting for repair requests: []
 INFO 02:19:21,484 Waiting for repair requests: []
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/11 12:30;yukim;ASF.LICENSE.NOT.GRANTED--2933.txt;https://issues.apache.org/jira/secure/attachment/12487966/ASF.LICENSE.NOT.GRANTED--2933.txt",,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20897,,,Wed Jul 27 16:12:39 UTC 2011,,,,,,,,,,"0|i0gecn:",93755,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"27/Jul/11 12:30;yukim;Patch attached to check validity of given column families.;;;","27/Jul/11 15:25;slebresne;+1 (Committed, thanks);;;","27/Jul/11 16:12;hudson;Integrated in Cassandra-0.8 #240 (See [https://builds.apache.org/job/Cassandra-0.8/240/])
    check column family input validity in nodetool repair
patch by yukim; reviewed by slebresne for CASSANDRA-2933

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1151497
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't include tmp files as sstable when create column families,CASSANDRA-2929,12514884,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,21/Jul/11 09:51,16/Apr/19 09:32,14/Jul/23 05:52,22/Jul/11 20:21,0.7.9,0.8.3,,,,,0,,,,"When we open a column family and populate the SSTableReader, we happen to include -tmp files. This has no change to actually happen in a real life situation, but that is what was triggering a race in the unit tests triggering spurious assertion failure in estimateRowsFromIndex.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/11 09:52;slebresne;0001-Don-t-include-tmp-files-as-sstables-when-creating-CF.patch;https://issues.apache.org/jira/secure/attachment/12487274/0001-Don-t-include-tmp-files-as-sstables-when-creating-CF.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20895,,,Fri Jul 22 21:36:18 UTC 2011,,,,,,,,,,"0|i0gebj:",93750,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"21/Jul/11 09:52;slebresne;Patch is against 0.7;;;","21/Jul/11 14:24;jbellis;+1, although the flag proliferation on files() is starting to concern me;;;","22/Jul/11 21:36;hudson;Integrated in Cassandra-0.7 #535 (See [https://builds.apache.org/job/Cassandra-0.7/535/])
    Don't include tmp file as sstable when creating cfs
patch by slebresne; reviewed by jbellis for CASSANDRA-2929

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1149716
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/io/sstable/SSTable.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Hinted Handoff replay,CASSANDRA-2928,12514860,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,21/Jul/11 03:10,16/Apr/19 09:32,14/Jul/23 05:52,21/Jul/11 03:23,0.7.8,0.8.2,,,,,0,,,,"Broken in CASSANDRA-2668. Brandon explains:

bq. the Ack and Ack2 verb handlers are applying a new ep state every time there is a generation change via Gossiper.applyStateLocally, so it's always unset initially when the node starts up. state.hasToken() is set in the Gossiper's status check, which won't have happened when the onAlive event is sent to SS.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/11 03:14;jbellis;2928.txt;https://issues.apache.org/jira/secure/attachment/12487251/2928.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20894,,,Thu Jul 21 03:40:55 UTC 2011,,,,,,,,,,"0|i0gebb:",93749,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"21/Jul/11 03:20;brandon.williams;+1;;;","21/Jul/11 03:23;jbellis;committed;;;","21/Jul/11 03:40;hudson;Integrated in Cassandra-0.7 #532 (See [https://builds.apache.org/job/Cassandra-0.7/532/])
    fix hint replay
patch by brandonwilliams and jbellis for CASSANDRA-2928

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1149015
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/gms/EndpointState.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/StorageService.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming estimatedKey calculation should never be 0,CASSANDRA-2916,12514491,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,18/Jul/11 19:41,16/Apr/19 09:32,14/Jul/23 05:52,18/Jul/11 19:56,1.0.0,,,,,,0,,,,"The new in-streaming SSTable rebuild uses the sender's estimated key calculation to determine which codepath to take: in some cases, samples can result in an estimated key count of 0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/11 19:44;stuhood;0001-CASSANDRA-2916-Assume-at-least-one-estimated-key-for-s.txt;https://issues.apache.org/jira/secure/attachment/12486911/0001-CASSANDRA-2916-Assume-at-least-one-estimated-key-for-s.txt",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20890,,,Mon Jul 18 20:28:10 UTC 2011,,,,,,,,,,"0|i0ge8n:",93737,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Jul/11 19:44;stuhood;Patch to ensure that we never estimate 0 keys for non-empty ranges. Also, properly handle the case where a range captures exactly one key.;;;","18/Jul/11 19:56;jbellis;committed, thanks!;;;","18/Jul/11 20:28;hudson;Integrated in Cassandra #960 (See [https://builds.apache.org/job/Cassandra/960/])
    ensure that we never estimate 0 keys when streaming non-empty ranges
patch by Stu Hood; reviewed by jbellis for CASSANDRA-2916

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1148029
Files : 
* /cassandra/trunk/test/unit/org/apache/cassandra/streaming/StreamingTransferTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/IncomingStreamReader.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL ignores client timestamp for full row deletion,CASSANDRA-2912,12514451,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,18/Jul/11 14:46,16/Apr/19 09:32,14/Jul/23 05:52,20/Jul/11 15:54,0.8.2,,,,,,0,cql,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/11 08:22;slebresne;0001-CQL-timestamp-row-deletion.patch;https://issues.apache.org/jira/secure/attachment/12486979/0001-CQL-timestamp-row-deletion.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20889,,,Tue Jul 19 22:16:48 UTC 2011,,,,,,,,,,"0|i0ge7r:",93733,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"18/Jul/11 16:02;jbellis;can you add a test for this?;;;","19/Jul/11 08:22;slebresne;Turns out setting timestamp for DELETE wasn't working at all anyway because the parser wasn't allowing it. New attached patch fixes this and add tests for all of that.;;;","19/Jul/11 19:49;xedin;{noformat}
usingClauseDelete[Attributes attrs]
    : K_USING usingClauseDeleteObjective[attrs] ( K_AND? usingClauseDeleteObjective[attrs] )*
    ;
{noformat}

This means that we allow user to use multiple number of usingClauseDeleteObjective statements, any motivation behind that I'm not getting?;;;","19/Jul/11 20:11;slebresne;A usingClauseDeleteObject is either TIMESTAMP or CONSISTENCY, so you at least want to allow setting both of them. Now if the question was why * instead of ? at the end, then it's mostly because I copied/pasted/modified usingClause where the * didn't seem to be a problem. I kind of think that having a * even if more than 2 doesn't make much sense is no big deal and will avoid, when we add a third possible usingClauseDeleteObjective, to be limited to 2 clauses just because we forgot that it was limited to 2 previously. But I don't really feel strongly about that.;;;","19/Jul/11 20:20;xedin;I'm asking because that also means that user will be able to specify as many timestamps and consistency levels as he wants, we will need to split usingClauseDeleteObjective into two statements - one for consistency level and one for timestamp and use them in usingClauseDelete like (K_USING (usingConsistencyLevel[attrs] (K_AND usingTimestamp[attrs])? | usingTimestamp[attrs] (K_AND usingConsistencyLevel[attrs])?) to support language consistency. ;;;","19/Jul/11 21:10;slebresne;bq. I'm asking because that also means that user will be able to specify as many timestamps and consistency levels as he wants

That's what I said. To rephrase:
* it's the case also for the UPDATE statement: we should do something for both or none.
* I personally don't think it's a problem, the last statement would be picked each time, which feels a reasonable behavior in that case. More precisely I don't think it's worth caring about.
* if we do want to add a restriction though, I think it would be easier to have Attributes throw an exception when one of its set method is called twice. Because your proposed fix will become a bit ugly for the UPDATE statement and it would be a pain to extend to future new clauses.;;;","19/Jul/11 21:23;xedin;Ok, lets leave it for now, +1.;;;","19/Jul/11 22:16;hudson;Integrated in Cassandra-0.8 #226 (See [https://builds.apache.org/job/Cassandra-0.8/226/])
    respect client timestamp on full row deletions
patch by slebresne; reviewed by pyaskevich for CASSANDRA-2912

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1148547
Files : 
* /cassandra/branches/cassandra-0.8/test/system/test_cql.py
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/Cql.g
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/DeleteStatement.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix bulkload JMX call,CASSANDRA-2908,12514404,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,18/Jul/11 07:23,16/Apr/19 09:32,14/Jul/23 05:52,18/Jul/11 13:15,0.8.2,,,,,,0,bulkloader,,,"The bulkload JMX call is supposed to simplify bulkloading when done from a Cassandra node (so you don't have to configure the bulkloading client to not conflict with the node itself), but that call doesn't work (it forgets to add the ranges to stream).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/11 07:23;slebresne;0001-Fix-JMX-call-bulkLoad.patch;https://issues.apache.org/jira/secure/attachment/12486799/0001-Fix-JMX-call-bulkLoad.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20888,,,Mon Jul 18 13:15:11 UTC 2011,,,,,,,,,,"0|i0ge6v:",93729,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Jul/11 11:49;jbellis;+1;;;","18/Jul/11 13:09;hudson;Integrated in Cassandra-0.8 #220 (See [https://builds.apache.org/job/Cassandra-0.8/220/])
    Fix JMX bulkload call
patch by slebresne; reviewed by jbellis for CASSANDRA-2908

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147838
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/BulkLoader.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/io/sstable/SSTableLoader.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
;;;","18/Jul/11 13:15;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
durable_writes flag cannot be changed via the CLI (system does not process KsDef.durable_writes option properly).,CASSANDRA-2907,12514395,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,benschrauwen,benschrauwen,18/Jul/11 03:53,16/Apr/19 09:32,14/Jul/23 05:52,18/Jul/11 17:13,0.8.2,,,,,,0,,,,"I am unable to change the durable_writes option in the CLI. Here are the commands to replicate the problem on a clean install:

create keyspace test;
update keyspace test with durable_writes=false;
show keyspaces;

It will still say:

Keyspace: test:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [datacenter1:1]
  Column Families:


PS: I looked in the tests of the CLI code of CASSANDRA-2683 and saw that the feature actually is not properly tested: the flag is set, but never tested.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/11 13:34;xedin;CASSANDRA-2907.patch;https://issues.apache.org/jira/secure/attachment/12486866/CASSANDRA-2907.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20887,,,Mon Jul 18 18:23:54 UTC 2011,,,,,,,,,,"0|i0ge6n:",93728,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Jul/11 03:54;benschrauwen;BTW, the logs show this:

INFO 23:44:02,549 Applying migration 2da39610-b0f0-11e0-0000-242d50cf1fbf Update keyspace testrep strategy:NetworkTopologyStrategy{}durable_writes: true to testrep strategy:NetworkTopologyStrategy{}durable_writes: true
;;;","18/Jul/11 12:55;xedin;I have discovered that this is not a CLI but rather a Core problem, I will change a title to reflect that.;;;","18/Jul/11 13:34;xedin;rebased with cassandra-0.8 (the latest commit 5616b0235aa4e32e49b58d707de2c98310cf0b9d) ;;;","18/Jul/11 14:08;benschrauwen;+1

I can confirm that this patch now allows to pass the test I mentioned in the initial post. Thanks for the super fast fix!;;;","18/Jul/11 17:13;jbellis;committed;;;","18/Jul/11 18:23;hudson;Integrated in Cassandra-0.8 #222 (See [https://builds.apache.org/job/Cassandra-0.8/222/])
    fix updating KS with durable_writes=false
patch by pyaskevich; reviewed by jbellis for CASSANDRA-2907

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147974
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/KSMetaData.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/migration/UpdateKeyspace.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming SSTable build does not use cleanupIfNecessary,CASSANDRA-2906,12514348,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,stuhood,stuhood,17/Jul/11 06:22,16/Apr/19 09:32,14/Jul/23 05:52,20/Jul/11 15:07,1.0.0,,,,,,0,,,,"The new streaming sstable rebuilding in IncomingStreamReader needs to wrap things in with {{try, finally, cleanupIfNecessary}} to ensure that the writer is cleaned up properly.",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/11 12:48;yukim;ASF.LICENSE.NOT.GRANTED--trunk-2906-v2.txt;https://issues.apache.org/jira/secure/attachment/12487148/ASF.LICENSE.NOT.GRANTED--trunk-2906-v2.txt","18/Jul/11 21:10;yukim;ASF.LICENSE.NOT.GRANTED--trunk-2906.txt;https://issues.apache.org/jira/secure/attachment/12486923/ASF.LICENSE.NOT.GRANTED--trunk-2906.txt",,,,,,,,,,,,,2.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20886,,,Wed Jul 20 16:16:48 UTC 2011,,,,,,,,,,"0|i0ge6f:",93727,,stuhood,,stuhood,Low,,,,,,,,,,,,,,,,,"18/Jul/11 21:10;yukim;Thank you for pointing it out. Patch attached to do perform closeIfNecessary.;;;","18/Jul/11 23:31;stuhood;You can actually return the reader object from inside the {{try, finally}}: the finally block isn't executed until after the return statement. See the uses of cleanupIfNecessary in SSTableWriter.Builder for an example.;;;","20/Jul/11 12:48;yukim;Attached v2. Just return SSTR instead of storing to local var.;;;","20/Jul/11 15:07;jbellis;committed;;;","20/Jul/11 16:16;hudson;Integrated in Cassandra #965 (See [https://builds.apache.org/job/Cassandra/965/])
    add cleanupIfNecessary for single-pass streaming SSTable build
patch by Yuki Morishita; reviewed by stuhood and jbellis for CASSANDRA-2906

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1148811
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/streaming/IncomingStreamReader.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""count"" doesn't accept UUIDs in CLI even though ""get"" does",CASSANDRA-2902,12514246,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,mdennis,mdennis,15/Jul/11 16:28,16/Apr/19 09:32,14/Jul/23 05:52,15/Jul/11 22:18,0.8.2,,,,,,0,,,,"[default@V360HC_SCHEMA1] get RawValues[7dc75c1c-8af0-462a-a920-bc1dafc44f31] limit 1;
=> (column=1310593550317, value=aced00057709053fe9cc17a95b9093, timestamp=1310593550583438)
Returned 1 results.

[default@V360HC_SCHEMA1] count RawValues[7dc75c1c-8af0-462a-a920-bc1dafc44f31];
UUIDs must be exactly 16 bytes
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/11 21:05;xedin;CASSANDRA-2902.patch;https://issues.apache.org/jira/secure/attachment/12486682/CASSANDRA-2902.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20885,,,Fri Jul 15 23:22:01 UTC 2011,,,,,,,,,,"0|i0ge5j:",93723,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"15/Jul/11 21:05;xedin;Proper support of key validation class and function calls for COUNT statement.

Rebased with latest cassandra-0.8 branch (last commit c1abc30b8c5deb67a2f13f0abe15868da24cb350);;;","15/Jul/11 22:18;brandon.williams;Committed.;;;","15/Jul/11 23:22;hudson;Integrated in Cassandra-0.8 #217 (See [https://builds.apache.org/job/Cassandra-0.8/217/])
    Proper support of key validation class and function calls for COUNT in
the cli.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2902

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147336
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/cli/CliTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cli silently fails when classes are quoted,CASSANDRA-2899,12514105,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,14/Jul/11 18:38,16/Apr/19 09:32,14/Jul/23 05:52,15/Jul/11 16:08,0.7.8,0.8.2,,,,,0,,,,"For example: CREATE COLUMN FAMILY autocomplete_meta WITH comparator = 'UTF8Type' AND default_validation_class = 'UTF8Type' AND key_validation_class = 'UTF8Type'

Neither validation class is actually set, but if you remove the quotes everything works.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/11 12:19;xedin;CASSANDRA-2899.patch;https://issues.apache.org/jira/secure/attachment/12486591/CASSANDRA-2899.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20884,,,Fri Jul 15 20:32:04 UTC 2011,,,,,,,,,,"0|i0ge4n:",93719,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"15/Jul/11 12:19;xedin;rebase with cassandra-0.8 (latest commit c3d275800284acca6300595db268ca0c92b9a581);;;","15/Jul/11 12:21;xedin;can be applied on both 0.7 and 0.8;;;","15/Jul/11 16:08;brandon.williams;Committed, thanks!;;;","15/Jul/11 20:32;hudson;Integrated in Cassandra-0.7 #529 (See [https://builds.apache.org/job/Cassandra-0.7/529/])
    Allow quoted class names in the cli.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2899

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147210
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/cli/CliClient.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Escape characters in CQL,CASSANDRA-2898,12513978,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,bcvisin,bcvisin,13/Jul/11 20:07,16/Apr/19 09:32,14/Jul/23 05:52,08/Aug/11 23:18,,,,,,,0,cql,,,"When trying to get all the columns named ""fmd:"" in cqlsh you can not escape : or ;

As per Jonathan Ellis:
You can escape quotes but I don't think you can escape semicolons.

Try:
sqlsh> select 'fmd:'..'fmd;' from feeds;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/11 21:19;brandon.williams;2898.txt;https://issues.apache.org/jira/secure/attachment/12488465/2898.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20883,,,Mon Aug 08 23:18:43 UTC 2011,,,,,,,,,,"0|i0ge47:",93717,,,,,Normal,,,,,,,,,,,,,,,,,"29/Jul/11 16:54;brandon.williams;I don't see the problem:
{noformat}
cqlsh> create columnfamily foo (KEY text primary key);
cqlsh> update foo set 'fmd:' = 'foo' where key = 'test';
cqlsh> select * from foo where key = 'test';
  KEY | fmd: |
 test |  foo |

cqlsh> 
cqlsh> select 'fmd:' from foo where key = 'test';
 fmd: |
  foo |

cqlsh> update foo set 'fmd;' = 'foo' where key = 'test';
cqlsh> select 'fmd:'..'fmd;' from foo where key = 'test';
 fmd: | fmd; |
  foo |  foo |

cqlsh>
{noformat};;;","01/Aug/11 17:25;bcvisin;I did not explain exactly.  

Try 

cqlsh> update foo set 'fmd:test' = 'foo' where key = 'test';
Unmatched named substitution: 'test' not given for update foo set 'fmd:test' = 'foo' where key = 'test';

You can not escape a colon.
;;;","01/Aug/11 21:17;brandon.williams;Patch to allow escaping colons with a backslash.;;;","08/Aug/11 22:59;bcvisin;Patch looks to have solved the problem.  I have been using it for about a week now, and have had no further problems.  You can escape colons by using \ (backslash).  

Thanks Brandon!;;;","08/Aug/11 23:18;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RPM classpath evaluation include current directory (-cp:),CASSANDRA-2881,12513657,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,wmeler,wmeler,wmeler,11/Jul/11 12:12,16/Apr/19 09:32,14/Jul/23 05:52,22/Aug/11 21:54,0.8.5,,,Packaging,,,0,classpath,rpm,,"/usr/share/cassandra/cassandra.in.sh builds CLASSPATH in a way that cause current directory inclusion (-cp:).
This should be avioded as can effect in config file change if one is present in current directory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/11 21:10;thepaul;classpath-both.patch;https://issues.apache.org/jira/secure/attachment/12491263/classpath-both.patch","11/Jul/11 12:29;wmeler;classpath.patch;https://issues.apache.org/jira/secure/attachment/12486051/classpath.patch",,,,,,,,,,,,,2.0,wmeler,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20879,,,Mon Aug 22 22:22:28 UTC 2011,,,,,,,,,,"0|i0ge0f:",93700,,thepaul,,thepaul,Low,,,,,,,,,,,,,,,,,"22/Aug/11 21:10;thepaul;Adds the same protection to the cassandra.in.sh snippet for Debian.

bin/cassandra.in.sh doesn't need it.

+1;;;","22/Aug/11 21:54;jbellis;committed;;;","22/Aug/11 22:22;hudson;Integrated in Cassandra-0.8 #289 (See [https://builds.apache.org/job/Cassandra-0.8/289/])
    avoid including cwd in classpath for deb and rpm packages
patch by Wojciech Meler and Paul Cannon for CASSANDRA-2881

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1160459
Files : 
* /cassandra/branches/cassandra-0.8/debian/cassandra.in.sh
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/redhat/cassandra.in.sh
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
example configuration of commitlog_sync: batch,CASSANDRA-2880,12513647,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,wmeler,wmeler,wmeler,11/Jul/11 09:17,16/Apr/19 09:32,14/Jul/23 05:52,12/Jul/11 19:20,0.8.2,,,,,,0,,,,"There is no example of commitlog_sync: batch configuration in default config file, and one have to guess that commitlog_sync_batch_window_in_ms should be configured instead of CommitLogSyncBatchWindowInMS.",,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/11 09:19;wmeler;commitlog_batch.patch;https://issues.apache.org/jira/secure/attachment/12486033/commitlog_batch.patch",,,,,,,,,,,,,,1.0,wmeler,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20878,,,Tue Jul 12 19:20:34 UTC 2011,,,,,,,,,,"0|i0ge07:",93699,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"11/Jul/11 09:19;wmeler;example config in comments;;;","11/Jul/11 09:31;wmeler;patch for config file ready;;;","12/Jul/11 19:20;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in src/java/org/apache/cassandra/cli/CliClient  ,CASSANDRA-2873,12513260,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,e1n,e1n,08/Jul/11 13:25,16/Apr/19 09:32,14/Jul/23 05:52,09/Jul/11 21:23,0.8.2,,,Legacy/Tools,,,0,,,,"I have read your documentation about syntax for creating column family and parameters that I can pass.
According to documentation i can use parameter :

"" - keys_cache_save_period: Duration in seconds after which Cassandra should
  safe the keys cache. Caches are saved to saved_caches_directory as
  specified in conf/Cassandra.yaml. Default is 14400 or 4 hours. ""

but then i was receiving error: ""No enum const class org.apache.cassandra.cli.CliClient$ColumnFamilyArgument.KEYS_CACHE_SAVE_PERIOD""


In class mentioned in title we have:

protected enum ColumnFamilyArgument
115 	{
116 	COLUMN_TYPE,
117 	COMPARATOR,
118 	SUBCOMPARATOR,
119 	COMMENT,
120 	ROWS_CACHED,
121 	ROW_CACHE_SAVE_PERIOD,
122 	KEYS_CACHED,
123 	KEY_CACHE_SAVE_PERIOD,   <---- TYPO !
124 	READ_REPAIR_CHANCE,
125 	GC_GRACE,
126 	COLUMN_METADATA,
127 	MEMTABLE_OPERATIONS,
128 	MEMTABLE_THROUGHPUT,
129 	MEMTABLE_FLUSH_AFTER,
130 	DEFAULT_VALIDATION_CLASS,
131 	MIN_COMPACTION_THRESHOLD,
132 	MAX_COMPACTION_THRESHOLD,
133 	REPLICATE_ON_WRITE,
134 	ROW_CACHE_PROVIDER,
135 	KEY_VALIDATION_CLASS
136 	} ",ubuntu linux 10.4,e1n,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20876,,,Sat Jul 09 22:22:29 UTC 2011,,,,,,,,,,"0|i0gdz3:",93694,,,,,Low,,,,,,,,,,,,,,,,,"09/Jul/11 21:23;jbellis;""key_cache_save_period"" is correct.  updated CliHelp.yaml to this.;;;","09/Jul/11 22:22;hudson;Integrated in Cassandra-0.8 #212 (See [https://builds.apache.org/job/Cassandra-0.8/212/])
    fix typo for CASSANDRA-2873
fix typo for CASSANDRA-2873

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1144749
Files : 
* /cassandra/branches/cassandra-0.8/src/resources/org/apache/cassandra/cli/CliHelp.yaml

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1144748
Files : 
* /cassandra/branches/cassandra-0.8/src/resources/org/apache/cassandra/cli/CliHelp.yaml
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"While dropping and recreating an index, incremental snapshotting can hang",CASSANDRA-2872,12513235,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,slebresne,slebresne,08/Jul/11 08:59,16/Apr/19 09:32,14/Jul/23 05:52,19/Jul/11 18:08,0.7.8,0.8.2,,,,,0,,,,"When creating a hard link (at list with JNA), link() hang if the target of the
link already exists. In theory though, we should not hit that situation
because we use a new directory for each manual snapshot and the generation
number of the sstables should prevent this from hapenning with increment
snapshot.

However, when you drop, then recreate a secondary index, if the sstables are
deleted after the drop and before we recreate the index, the recreated index
sstables will start with a generation to 0. Thus, when we start backuping them
incrementally, it will conflict with the sstables of the previously dropped
index.

First, we should check for the target existance because calling link() to at
least avoid hanging. But then we must make sure that when we drop, then
recreate an index, we will either not name the sstables the same way or the
incremental snapshot use a different directory.
",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2521,,,,"19/Jul/11 17:06;jbellis;2872-v2.txt;https://issues.apache.org/jira/secure/attachment/12487033/2872-v2.txt","19/Jul/11 16:08;jbellis;2872.txt;https://issues.apache.org/jira/secure/attachment/12487019/2872.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20875,,,Tue Jul 19 19:41:05 UTC 2011,,,,,,,,,,"0|i0gdyv:",93693,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"12/Jul/11 23:43;slebresne;One option could be to have some kind of ""index"" generation that we would persist in the system tables. What I mean here is that if you create a index on say column 'birthdate' for some CF test, the index would start being called test.birthdate.1 (or just test.birthdate for compatibility sake), then if you drop it and recreate it, it would be called test.birthdate.2. That way, we would avoid the name clash during the incremental backup.;;;","12/Jul/11 23:49;jbellis;I really don't like messing with the name.  That feels like the implementation is leaking too much into something user-visible.;;;","19/Jul/11 15:06;jbellis;Remind me why simply making sstable generation global doesn't fix this?

It's not like there's so much contention on that AtomicInteger that we need to partition it.;;;","19/Jul/11 15:22;slebresne;bq. Remind me why simply making sstable generation global doesn't fix this?

If you drop an index, then shutdown the node, then restart and recreate the index. Upon restart and crawling of the existing data files, it could be that the first available generation is the one of a sstable of the dropped index.

I guess there is two reasonably simple solutions:
# scan the (incremental) snapshot directories for the generation number too. If we do that, I guess we don't even have to make the generation global as long as we do this scanning each time a ColumnFamilyStore is created.
# make the generation number persistent in the system tables (again, no need to make the number global for that).

I think I prefer the second solution because it's more general and feels more elegant. But we would still have to scan the data dir and take the max(what we found during scan, what's in the system table) in case people force-feed data files that weren't created on that node (or the system tables are wiped).

That being said, I totally agree that the generation number don't have to be partitioned if we don't want to. But not sure it's a big deal that way either.;;;","19/Jul/11 16:07;jbellis;bq. we would still have to scan the data dir and take the max(what we found during scan, what's in the system table) in case people force-feed data files that weren't created on that node (or the system tables are wiped).

That's why I prefer the scan approach -- I'd rather have a single reliable source of truth (the contents of the fs) than an unreliable one that we have to supplement with the reliable one anyway.

Patch attached.;;;","19/Jul/11 16:54;slebresne;bq. That's why I prefer the scan approach

Yeah, I kind of wrote my comment before as things were coming to me, in particular I wrote that I was preferring the second solution before realizing we would still need to scan. Agreeing that the scan approach is cleaner/easier.

On the patch, shouldn't we only include the sstables for the column family we're creating. I know it's ok to take the max for all cfs, but it feels a bit random unless we have only one global generation. And could be worst avoiding 2-3 mails on the mailing of people wondering why that has changed (I mean, I'm sure someone will remark it) :). ;;;","19/Jul/11 17:06;jbellis;v2 restricts to the CF in question.;;;","19/Jul/11 17:13;slebresne;lgtm +1

And I can confirm that this fix the failing test of CASSANDRA-2521. ;;;","19/Jul/11 18:08;jbellis;committed;;;","19/Jul/11 19:41;hudson;Integrated in Cassandra-0.7 #531 (See [https://builds.apache.org/job/Cassandra-0.7/531/])
    fix re-using index CF sstable names after drop/recreate
patch by jbellis; reviewed by slebresne for CASSANDRA-2872

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1148466
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dynamic snitch + read repair off can cause LOCAL_QUORUM reads to return spurious UnavailableException,CASSANDRA-2870,12513176,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,07/Jul/11 19:22,16/Apr/19 09:32,14/Jul/23 05:52,08/Jul/11 16:38,0.7.8,0.8.2,,,,,0,,,,"When Read Repair is off, we want to avoid doing requests to more nodes than necessary to satisfy the ConsistencyLevel.  ReadCallback does this here:

{code}
        this.endpoints = repair || resolver instanceof RowRepairResolver
                       ? endpoints
                       : endpoints.subList(0, Math.min(endpoints.size(), blockfor)); // min so as to not throw exception until assureSufficient is called
{code}

You can see that it is assuming that the ""endpoints"" list is sorted in order of preferred-ness for the read.

Then the LOCAL_QUORUM code in DatacenterReadCallback checks to see if we have enough nodes to do the read:

{code}
        int localEndpoints = 0;
        for (InetAddress endpoint : endpoints)
        {
            if (localdc.equals(snitch.getDatacenter(endpoint)))
                localEndpoints++;
        }

        if (localEndpoints < blockfor)
            throw new UnavailableException();
{code}

So if repair is off (so we truncate our endpoints list) AND dynamic snitch has decided that nodes in another DC are to be preferred over local ones, we'll throw UE even if all the replicas are healthy.",,patrickubs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/11 19:34;jbellis;2870.txt;https://issues.apache.org/jira/secure/attachment/12485644/2870.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20874,,,Wed Jul 13 14:59:02 UTC 2011,,,,,,,,,,"0|i0gdyf:",93691,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"07/Jul/11 19:34;jbellis;patch extracts the ""give me the minimum set of endpoints necessary"" code into preferredEndpoints(), and makes it DC-aware for LOCAL_QUORUM reads.;;;","08/Jul/11 08:40;slebresne;+1;;;","08/Jul/11 16:38;jbellis;committed;;;","08/Jul/11 16:52;hudson;Integrated in Cassandra-0.7 #526 (See [https://builds.apache.org/job/Cassandra-0.7/526/])
    fix possibility of spuriousUnavailableException for LOCAL_QUORUM reads with dynamic snitch and read repair disabled
patch by jbellis; reviewed by slebresne for CASSANDRA-2870

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1144380
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/ReadCallback.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/DatacenterReadCallback.java
;;;","08/Jul/11 23:51;jeromatron;This also appears to affect 0.7.6 and when read repair is not off.  I didn't set read repair on my CFs (defaults to 100%) and tried a simple rowcount pig script using read consistency LOCAL_QUORUM and it fails with UE.  I would think if that's the case, the priority should be higher and it should go in 0.7.7.  Any thoughts?;;;","09/Jul/11 03:51;jbellis;This has been present since LOCAL_QUORUM was introduced, so it's not a new regression.  And a reasonable workaround exists (disable dynamic snitch).  So no, I don't think we should hold up 0.7.7 for this.;;;","09/Jul/11 05:38;jeromatron;Okay - it just seemed like a higher priority issue with the scope expanded.  We'll probably just disable dynamic snitch until the fix is in a release then.;;;","13/Jul/11 14:40;patrickubs;In the default configuration of 0.7.6-2 (and other versions) LOCAL_QUORUM reads dont work. This is not a minor bug and should be fixed in the next release.
By default configuration I mean the tar ball that is distributed by the cassandra website.
The fact that it is not a regression just shows that this functionality was never properly tested.
;;;","13/Jul/11 14:59;jbellis;It will be fixed in 0.7.8; 0.7.7 entered the release process before this was reported.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage does not function properly when used multiple times in a single pig script due to UDFContext sharing issues,CASSANDRA-2869,12513172,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jeromatron,gsingers,gsingers,07/Jul/11 18:26,16/Apr/19 09:32,14/Jul/23 05:52,21/Jul/11 20:11,0.7.9,0.8.2,,,,,0,,,,"CassandraStorage appears to have threading issues along the lines of those described at http://pig.markmail.org/message/oz7oz2x2dwp66eoz due to the sharing of the UDFContext.

I believe the fix lies in implementing
{code}
public void setStoreFuncUDFContextSignature(String signature)
    {
    }
{code}

and then using that signature when getting the UDFContext.

From the Pig manual:
{quote}
setStoreFunc!UDFContextSignature(): This method will be called by Pig both in the front end and back end to pass a unique signature to the Storer. The signature can be used to store into the UDFContext any information which the Storer needs to store between various method invocations in the front end and back end. The default implementation in StoreFunc has an empty body. This method will be called before other methods.
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/11 14:29;jeromatron;2869-2.txt;https://issues.apache.org/jira/secure/attachment/12486314/2869-2.txt","12/Jul/11 04:31;jeromatron;2869.txt;https://issues.apache.org/jira/secure/attachment/12486144/2869.txt",,,,,,,,,,,,,2.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20873,,,Thu Jul 21 20:30:48 UTC 2011,,,,,,,,,,"0|i0gdy7:",93690,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"12/Jul/11 04:30;jeromatron;Simple patch to use the load and store signatures instead of the udf context property keys we had been using.  We're running this in our data pipeline and appears to work correctly.  However, I haven't found evidence that the old way wasn't working - that seems to be more related to read consistency level we were using.  But, this is probably the way we should be doing it, as it appears to be the Pig approach.  Also there could be some corner cases that might trip up the current approach.;;;","12/Jul/11 16:57;brandon.williams;Looks like we can remove UDFCONTEXT_SCHEMA_KEY_PREFIX now too, no?;;;","12/Jul/11 21:30;jeromatron;Yes. I was about to post an updated patch last night but got sidetracked. Do you mind removing that if it's otherwise good to go? Otherwise I can do that later today.;;;","13/Jul/11 14:29;jeromatron;Removed that String.  Also removed adding mutation twice and put in the nested exception in putNext into the IOException.  We've been meaning to add those last two items to one of these tickets.;;;","21/Jul/11 20:11;brandon.williams;Committed;;;","21/Jul/11 20:30;hudson;Integrated in Cassandra-0.7 #534 (See [https://builds.apache.org/job/Cassandra-0.7/534/])
    Use a UDF-specific context signature.
Patch by Jeremy Hanna, reviewed by brandonwilliams for CASSANDRA-2869

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1149341
Files : 
* /cassandra/branches/cassandra-0.7/contrib/pig/src/java/org/apache/cassandra/hadoop/pig/CassandraStorage.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Native Memory Leak,CASSANDRA-2868,12513152,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,doubleday,doubleday,07/Jul/11 14:05,16/Apr/19 09:32,14/Jul/23 05:52,23/Aug/11 20:06,0.7.9,0.8.5,,,,,4,,,,"We have memory issues with long running servers. These have been confirmed by several users in the user list. That's why I report.

The memory consumption of the cassandra java process increases steadily until it's killed by the os because of oom (with no swap)

Our server is started with -Xmx3000M and running for around 23 days.

pmap -x shows

Total SST: 1961616 (mem mapped data and index files)
Anon  RSS: 6499640
Total RSS: 8478376

This shows that > 3G are 'overallocated'.

We will use BRAF on one of our less important nodes to check wether it is related to mmap and report back.",,cburroughs,codahale,hanzhu,jborgstrom,jjordan,kzadorozhny,scode,tbritz,yulinyen,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/11 01:13;cburroughs;2868-v1.txt;https://issues.apache.org/jira/secure/attachment/12486249/2868-v1.txt","29/Jul/11 21:10;brandon.williams;2868-v2.txt;https://issues.apache.org/jira/secure/attachment/12488246/2868-v2.txt","16/Aug/11 16:59;jbellis;2868-v3.txt;https://issues.apache.org/jira/secure/attachment/12490554/2868-v3.txt","27/Jul/11 17:49;cburroughs;48hour_RES.png;https://issues.apache.org/jira/secure/attachment/12488003/48hour_RES.png","14/Jul/11 13:16;cburroughs;low-load-36-hours-initial-results.png;https://issues.apache.org/jira/secure/attachment/12486435/low-load-36-hours-initial-results.png",,,,,,,,,,5.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20872,,,Tue Aug 23 20:20:06 UTC 2011,,,,,,,,,,"0|i0gdxz:",93689,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"11/Jul/11 10:45;doubleday;Hm after 3 days checking a node that does not use mmaped files it looks like this:

nativelib: 14128
locale-archive: 1492
ffiSwFShY(deleted): 8
javajar: 2292
[anon]: 3609388
[stack]: 132
java: 44
7008: 32
jna534482390478104336.tmp: 92

Total RSS: 3627608
Total SST: 0


Compared to start RSS increased by ~400MB. So it seems that this is not related to mem mapping.

We will deploy CASSANDRA-2654 this week. Will see if that changes anything but I suspect not ...;;;","11/Jul/11 18:31;cburroughs;At one point I was convinced this was a JVM bug and opened http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7037080  After seeing how totally broken NIO is after CASSANDRA-2654 I'm no longer sure of anything.

I was going to start a survey on the user list after the summit to see if any OS/jvm level pattern could be found, since clearly it doesn't happen to everyone in all cases.;;;","11/Jul/11 20:33;jbellis;Is your data size constant?  If not you are probably seeing growth in the index samples and bloom filters.;;;","12/Jul/11 08:30;doubleday;Next: [anon]: 3675224 (+47616KB in 1 day)

bq. Is your data size constant? If not you are probably seeing growth in the index samples and bloom filters.

Well no - the data size is increasing. But I thought that index and bf is good old plain java heap no? JVM heap stats are really relaxed. Yet I think that doesn't really matter because what we are seeing is an ever increasing rss mem consumption even though we have -Xmx3G and -Xms3G and mlockall (pmap shows these 3G as one block). So something seems to be constantly allocating native mem that has nothing to do with java heap.;;;","12/Jul/11 20:43;jbellis;We call getLastGcInfo several times a second.  http://twitter.com/#!/kimchy/status/90861039930970113

You could try turning GCInspector methods into a no-op and see if that makes it go away.;;;","13/Jul/11 00:11;cburroughs;http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7066129 will be the id when bugs.sun.com gets around to doing it's thing.

I confirmed that -XX:MaxDirectMemorySize does not protect you from this (ie it's a native native leak, not some DirectByteBuffer thing).  I'll be able to test this but not until the end of this week at the earliest (and it will then take at least another week to be sure).;;;","13/Jul/11 01:13;cburroughs;In case it is useful to anyone else this is what I intend to test with.;;;","14/Jul/11 13:16;cburroughs;Initial results.  Graph of VmRSS from /proc/PID/status at 10 second intervals from my last comment to now.  Box on the left has GCInspector disabled.  These are on two test boxes under trivial load so this is all still *very* tentative.  Will start testing under real load by early next week.;;;","14/Jul/11 13:21;jbellis;Promising!;;;","15/Jul/11 11:31;doubleday;It's indeed promising. We have been running this in production for 3 days now and rss increased only insignificantly by ~5MB a day. ;;;","15/Jul/11 15:33;hanzhu;{quote}
We have been running this in production for 3 days now and rss increased only insignificantly by ~5MB a day
{quote}

Do you mean it is very helpful to control RSS increasing by removing getLastGcInfo()? 

I have no idea why just some of us meets the problem.;;;","15/Jul/11 16:24;cburroughs;I interpreted Daniel's ""this"" to be the 2868-v1.txt patch (or something equivalent) with cassandra.enable_gc_inspector=false.  I did not find -XX:MaxDirectMemorySize to be helpful.;;;","16/Jul/11 06:45;doubleday;Yes - we did disable the GCInspector.;;;","16/Jul/11 07:04;hanzhu;Got it!

Do you have any idea why only some of us reports the problem?;;;","16/Jul/11 10:29;doubleday;Well either it's environment specific or (more likely) others didn't notice / care because they have enough memory and/or restart the nodes often enough.

We have 16GB of RAM and run Cassandra with 3GB. Within one month we loose ~3GB (13GB -> 10GB) files system cache because of the mem leak. Looking at our graphs I can't really tell a difference performance wise. So I guess only people with weaker servers (less memory headroom) will really notice. We noticed only because we got the system oom on a cluster that's not critical and which we didn't really monitor.;;;","18/Jul/11 07:59;doubleday;Looks good to me. Guess cassandra should just disable the inspector for now (probably make it jmx'able to start it manually)

Thu Jul 14 09:39:26 CEST 2011: [anon]: 3234068
Thu Jul 14 17:22:45 CEST 2011: [anon]: 3266888
Fri Jul 15 09:33:53 CEST 2011: [anon]: 3269160
Mon Jul 18 09:54:29 CEST 2011: [anon]: 3270188;;;","18/Jul/11 20:36;jbellis;We could try switching to what JConsole does, which is just log the total number and time spent for each compaction type.  This uses a different API which hopefully does not leak: http://www.java2s.com/Open-Source/Java-Document/6.0-JDK-Modules-sun/tools/sun/tools/jconsole/MemoryTab.java.htm

Logging the lifetime totals there with StatusLogger similar to what we do now for dropped messages would be better than nothing.;;;","27/Jul/11 17:49;cburroughs;48 hours under production load after C* had already been running for a few days.  Two on the left have GCInspector enabled.  The two on the right do not.  (Note that the scale on the lower right one reflects a change of only 10s of bytes.)

So it looks like victory to me.;;;","27/Jul/11 18:09;jbellis;Thanks, Chris.  We'll work on rewriting GCInspector to use the java.lang.management api instead, unless you have time to take a stab at that.;;;","29/Jul/11 20:15;jjordan;Depending how long the rewrite is going to take, can we get the config file option to disable gc inspector into a new 0.7.X and 0.8.X release?;;;","29/Jul/11 21:10;brandon.williams;v2 switches the GCInspector to us java.lang.managment.  I don't know if it too leaks or not yet.;;;","01/Aug/11 00:56;brandon.williams;I created three isolated nodes, all with a hack of setting the inspector interval to 1ms applied (not the tightest loop, but good enough and easy.)  One of the nodes had the inspector disabled entirely (the control), one was vanilla, and one had v2 applied.  After starting them up with a 128M heap and letting them run for a few minutes, here are the results:

||version||resident||
|control|72M|
|patched|72M|
|vanilla|540M|

I think it's safe to say java.lang.management doesn't share the leak.;;;","01/Aug/11 18:05;slebresne;Comments on v2:
* Couldn't we estimate the reclaimed size by recording the last memory used (that would need to be the first thing we do in logGCResults so that we record it each time) ?
* Wouldn't it be worth indicating that how many collection have been done since last log message if it's > 1, since it can (be > 1).
* Nit: especially if we decide to keep the last memory used, it may be more efficient (in cleaner imho) to have just one HashMap of string -> GCInfo where GCInfo would be a small struct with times, counts and usedMemory. Not that it is very performance sensitive... ;;;","01/Aug/11 18:55;jbellis;bq. Couldn't we estimate the reclaimed size

Well, not really, what we'd have is ""difference in size between last time it was called, and now"" which isn't all that close to ""amount reclaimed by a specific GC.""

bq. Wouldn't it be worth indicating that how many collection have been done since last log message

IMO the duration-based thresholds are hard to reason about here, where we're dealing w/ summaries and not individual GC results.  I think I'd rather have something like the dropped messages logger, where every N seconds we log the summary we get from the mbean.

The flushLargestMemtables/reduceCacheSizes stuff should probably be removed. :(;;;","09/Aug/11 18:43;brandon.williams;bq. Wouldn't it be worth indicating that how many collection have been done since last log message if it's > 1, since it can (be > 1).

The only reason I added count tracking was to prevent it from firing when there were no GCs (the api is flakey.)  I've never actually been able to get > 1 to happen, but we can add it to the logging.

bq. IMO the duration-based thresholds are hard to reason about here, where we're dealing w/ summaries and not individual GC results.

We are dealing with individual GCs at least 99% of the time in practice.  The worst case is >1 GC inflates the gctime enough that we errantly log when it's not needed, but I imagine to trigger that you would have to be in a gc pressure situation already.

bq. I think I'd rather have something like the dropped messages logger, where every N seconds we log the summary we get from the mbean.

That seems like it could be a lot of noise since GC is constantly happening.

bq. The flushLargestMemtables/reduceCacheSizes stuff should probably be removed. 

I think the logic there is still sound (""Did we just do a CMS? Is the heap still 80% full?"") and it seems to work as well as it always has.

;;;","16/Aug/11 16:59;jbellis;bq. I've never actually been able to get > 1 to happen, but we can add it to the logging

I'm sure it's possible w/ a small enough heap, especially since GCInspector is paused along w/ everything else for STW collections (including new gen).

v3 attached to accomodate this and add durationPerCollection.;;;","16/Aug/11 17:05;brandon.williams;Why is v3 touching compaction?;;;","16/Aug/11 17:09;jbellis;dirty working directory.  GCI is the only relevant file.;;;","16/Aug/11 23:36;brandon.williams;+1 to GCI changes.  Also, it is indeed possible to get >1 with a tiny heap.;;;","17/Aug/11 02:20;jbellis;committed;;;","17/Aug/11 03:20;hudson;Integrated in Cassandra-0.8 #282 (See [https://builds.apache.org/job/Cassandra-0.8/282/])
    work around native memory leak in com.sun.management.GarbageCollectorMXBean
patch by brandonwilliams and jbellis for CASSANDRA-2868

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1158490
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/GCInspector.java
;;;","17/Aug/11 03:28;jjordan;Can we get this in 0.7.X as well?;;;","23/Aug/11 19:18;brandon.williams;Reopening to backport to 0.7;;;","23/Aug/11 20:06;brandon.williams;Committed to 0.7 in r1160879;;;","23/Aug/11 20:20;hudson;Integrated in Cassandra-0.7 #543 (See [https://builds.apache.org/job/Cassandra-0.7/543/])
    work around native memory leak in com.sun.management.GarbageCollectorMXBean
patch by brandonwilliams and jbellis for CASSANDRA-2868

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1160879
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/GCInspector.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Starting 0.8.1 after upgrade from 0.7.6-2 fails,CASSANDRA-2867,12513118,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,kunda,kunda,07/Jul/11 10:20,16/Apr/19 09:32,14/Jul/23 05:52,02/Aug/11 16:05,0.8.3,,,,,,3,exception,index,starting,"After upgrading the binaries to 0.8.1 I get an exception when starting cassandra:

{noformat}
[root@bserv2 local]#  INFO 12:51:04,512 Logging initialized
 INFO 12:51:04,523 Heap size: 8329887744/8329887744
 INFO 12:51:04,524 JNA not found. Native methods will be disabled.
 INFO 12:51:04,531 Loading settings from file:/usr/local/apache-cassandra-0.8.1/conf/cassandra.yaml
 INFO 12:51:04,621 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 12:51:04,707 Global memtable threshold is enabled at 2648MB
 INFO 12:51:04,708 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,713 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,714 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,716 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,717 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,719 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,770 reading saved cache /vm1/cassandraDB/saved_caches/system-IndexInfo-KeyCache
 INFO 12:51:04,776 Opening /vm1/cassandraDB/data/system/IndexInfo-f-9
 INFO 12:51:04,792 reading saved cache /vm1/cassandraDB/saved_caches/system-Schema-KeyCache
 INFO 12:51:04,794 Opening /vm1/cassandraDB/data/system/Schema-f-194
 INFO 12:51:04,797 Opening /vm1/cassandraDB/data/system/Schema-f-195
 INFO 12:51:04,802 Opening /vm1/cassandraDB/data/system/Schema-f-193
 INFO 12:51:04,811 Opening /vm1/cassandraDB/data/system/Migrations-f-193
 INFO 12:51:04,814 reading saved cache /vm1/cassandraDB/saved_caches/system-LocationInfo-KeyCache
 INFO 12:51:04,815 Opening /vm1/cassandraDB/data/system/LocationInfo-f-292
 INFO 12:51:04,843 Loading schema version 586e70fd-a332-11e0-828e-34b74a661156
ERROR 12:51:04,996 Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 15
        at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:72)
        at org.apache.cassandra.config.CFMetaData.getDefaultIndexName(CFMetaData.java:971)
        at org.apache.cassandra.config.CFMetaData.inflate(CFMetaData.java:381)
        at org.apache.cassandra.config.KSMetaData.inflate(KSMetaData.java:172)
        at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:99)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:479)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:139)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:315)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 15
        at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:72)
        at org.apache.cassandra.config.CFMetaData.getDefaultIndexName(CFMetaData.java:971)
        at org.apache.cassandra.config.CFMetaData.inflate(CFMetaData.java:381)
        at org.apache.cassandra.config.KSMetaData.inflate(KSMetaData.java:172)
        at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:99)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:479)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:139)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:315)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
{noformat}

It seems this has something to do with indexes, and I do have a CF with an index on it, but it is not used.
I can try and remove the index with 0.7.x binaries, but I will wait a bit to see if anyone needs it to reproduce the bug.",CentOS 5.6,andrden,redpriest,tarasp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/11 04:15;jbellis;2867-v2.txt;https://issues.apache.org/jira/secure/attachment/12488340/2867-v2.txt","26/Jul/11 13:48;tarasp;trunk-2867.txt;https://issues.apache.org/jira/secure/attachment/12487834/trunk-2867.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20871,,,Tue Aug 02 16:36:05 UTC 2011,,,,,,,,,,"0|i0gdxr:",93688,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"23/Jul/11 20:18;andrden;upgrade from 0.8 to 0.8.1 
(update: tried daily trunc build apache-cassandra-2011-07-20_16-02-23 with the same effect)
 - just put new libararies and started cassandra (CentOS 5.6 64bit)
We are not using any CF indexes (only counters if that matters), 
but it won't start, so I don't see a way to upgrade to 0.8.1:

{code}
 INFO [main] 2011-07-23 19:32:23,446 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/NodeIdInfo-g-1
 INFO [main] 2011-07-23 19:32:23,516 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/Schema-g-74
 INFO [main] 2011-07-23 19:32:23,529 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/Schema-g-73
 INFO [main] 2011-07-23 19:32:23,548 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/Migrations-g-73
 INFO [main] 2011-07-23 19:32:23,561 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/Migrations-g-74
 INFO [main] 2011-07-23 19:32:23,570 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/LocationInfo-g-46
 INFO [main] 2011-07-23 19:32:23,587 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/LocationInfo-g-45
 INFO [main] 2011-07-23 19:32:23,597 SSTableReader.java (line 158) Opening /var/lib/cassandra/data/system/HintsColumnFamily-g-1
 INFO [main] 2011-07-23 19:32:23,656 DatabaseDescriptor.java (line 478) Loading schema version 1e26a500-b538-11e0-0000-f03fd2ec87ff
ERROR [main] 2011-07-23 19:32:23,889 AbstractCassandraDaemon.java (line 332) Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 4
        at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:72)
        at org.apache.cassandra.config.CFMetaData.getDefaultIndexName(CFMetaData.java:971)
        at org.apache.cassandra.config.CFMetaData.inflate(CFMetaData.java:381)
        at org.apache.cassandra.config.KSMetaData.inflate(KSMetaData.java:172)
        at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:99)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:479)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:139)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:315)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
 {code};;;","24/Jul/11 20:56;jbellis;You have an index name set on at least one CF.  If you don't actually have an index on it, Cassandra shouldn't have let you set the index name, but it's possible an older version was not as rigorous about that.

If you can't figure out where the index name is, you can also drop your schema and migrations system CFs and rebuild your schema from scratch.;;;","26/Jul/11 13:48;tarasp;The patch trunk-2867.txt fixes the MarshalException on startup.;;;","26/Jul/11 14:35;jbellis;Hmm, you have an index name on a subcolumn?  that's not supposed to be legal either :);;;","27/Jul/11 10:29;tarasp;No, just create the following column family and restart the server:

 create column family MailStat
  with column_type = Super
    and key_validation_class = UTF8Type
    and comparator = LongType
    and subcomparator = AsciiType
    and column_metadata = [
        {column_name : subj, validation_class : UTF8Type},
        {column_name : body, validation_class : UTF8Type},
        {column_name : comment, validation_class : UTF8Type},
        {column_name : categ, validation_class : AsciiType}
    ];
;;;","27/Jul/11 10:32;tarasp;This problem is not related to migration, just 0.8.1 - 0.8.2 cannot start when there is a super column family with different comparators for supercolumns and metadata.;;;","28/Jul/11 08:06;redpriest;Thanks Taras for commenting on the issue, I just ran into the same problem with a super column family with a TimeUUIDType comparator, and UTF8Type subcomparators; I can insert, get data all day long from the cluster while it is running fresh. But the moment I restart the cassandra node, I get the exception saying that TimeUUIDTypes must be exactly 16 bytes.

Edit: Just verified your fix and it fixes my case as well.;;;","31/Jul/11 04:15;jbellis;Thanks, I see the problem now -- it was generating an index name even if there was no index present on the column (which is always the case for supercolumns).

Alternate patch attached to fix this.;;;","02/Aug/11 15:27;slebresne;The attached patch has a lot of unrelated changes I believe, but considering the fix is the one line change in CFMetadata, then +1.;;;","02/Aug/11 16:05;jbellis;committed, minus the extra stuff from CASSANDRA-2894 that was in the diff;;;","02/Aug/11 16:36;hudson;Integrated in Cassandra-0.8 #252 (See [https://builds.apache.org/job/Cassandra-0.8/252/])
    avoid trying to create index names, when no index exists
patch by jbellis; reviewed by slebresne for CASSANDRA-2867

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1153175
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/config/CFMetaData.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when writing SSTable generated via repair,CASSANDRA-2863,12512969,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,hector,hector,06/Jul/11 11:32,16/Apr/19 09:32,14/Jul/23 05:52,10/Oct/11 13:58,0.8.8,,,,,,1,,,,"A NPE is generated during repair when closing an sstable generated via SSTable build. It doesn't happen always. The node had been scrubbed and compacted before calling repair.

 INFO [CompactionExecutor:2] 2011-07-06 11:11:32,640 SSTableReader.java (line 158) Opening /d2/cassandra/data/sbs/walf-g-730
ERROR [CompactionExecutor:2] 2011-07-06 11:11:34,327 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[CompactionExecutor:2,1,main] 
java.lang.NullPointerException
	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
	at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
	at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1103)
	at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1094)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
",,cce,j.casares,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/11 13:07;slebresne;2863-v2.patch;https://issues.apache.org/jira/secure/attachment/12498418/2863-v2.patch","10/Oct/11 11:07;slebresne;2863.patch;https://issues.apache.org/jira/secure/attachment/12498412/2863.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20870,,,Mon Oct 10 14:26:54 UTC 2011,,,,,,,,,,"0|i0gdx3:",93685,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"08/Jul/11 21:22;hector;I don't now if it's the same one, buy I got another during repair on another node:

ERROR [Thread-1710] 2011-07-08 21:21:00,514 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-1710,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:154)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:162)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:95)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:138)
	... 3 more
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
	at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
	at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1103)
	at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1094)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
;;;","21/Jul/11 12:28;slebresne;I'm a little bit baffled by that one. Trusting the stack trace, apparently when SSTW.RowIndexer.close() is called, the iwriter field is null. But iwriter is set in prepareIndexing() that is called the line before index() in SSTW.Builder. Thus if an exception happens in prepareIndexing, we shouldn't arrive to the index() method (which is the one triggering the close()). And looking at the use of iwriter, no other line set it (so it can't be set back to null after prepareIndexing()).

So I mean we can add a {{if (iwriter != null)}} before calling the close, but the truth is I have no clue how it could ever be null at that point.

Héctor: are you positive that you are using stock 0.8.1 ?;;;","21/Jul/11 12:35;hector.izquierdo;I have a patch from CASSANDRA-2818 (2818-v4) applied, if that's of any help. The patch only touches messaging classes though.;;;","21/Jul/11 20:13;jbellis;Doesn't make any sense to me, either.  The only place close() is called is from index() [as seen in the stacktrace here] and the only place index() is called is after prepareIndexing, which sets iwriter to non-null:

{code}
            long estimatedRows = indexer.prepareIndexing();

            // build the index and filter
            long rows = indexer.index();
{code}
;;;","20/Sep/11 23:19;cce;I just got a similar-looking NPE stack trace.  The node that raised the exception was receiving streams from a node being decommissioned (with ""nodetool decommission"").  Both nodes were running 0.8.6; both had been upgraded a few hours earlier from 0.8.4.

The first:  

ERROR [CompactionExecutor:72] 2011-09-20 22:34:20,892 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[CompactionExecutor:72,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Then half a minute later:

 INFO [CompactionExecutor:72] 2011-09-20 22:34:46,923 SSTableReader.java (line 162) Opening /mnt/cassandra/data/Keyspace/CF1-g-1536
ERROR [Thread-785] 2011-09-20 22:34:52,054 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[Thread-785,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:154)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:189)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:117)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:138)
        ... 3 more
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
;;;","21/Sep/11 14:58;cce;So I restarted the node making the NPE above and ran the decommission again (on the other node) and it successfully finished decommissioning.  A few hours later I tried to decommission another node and received this error, on a separate node from the two mentioned before, which is a new node running 0.8.6 for just about a day now.

INFO [COMMIT-LOG-WRITER] 2011-09-21 05:12:46,361 CommitLogSegment.java (line 58) Creating new commitlog segment /raid0/cassandra/commitlog/CommitLog-1316581966361.log
ERROR [Thread-224] 2011-09-21 05:12:52,738 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[Thread-224,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:154)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:189)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:117)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:138)
        ... 3 more
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
;;;","23/Sep/11 15:17;cce;Since my last two comments, I increased my RF and started a rolling repair on my nodes.  This has caused this NPE to pop up on all the boxes over the last couple of days as they process SSTables.  Again, all the nodes are fresh 0.8.6 installs from a few days ago using the ComboAMI.  I've seen backtraces like the one below appear at least a couple of times on each node in my cluster as I was repairing..

 INFO [CompactionExecutor:648] 2011-09-22 04:35:51,086 SSTableReader.java (line 162) Opening /raid0/cassandra/data/Keyspace/CF1-g-535
 INFO [CompactionExecutor:648] 2011-09-22 04:35:51,172 SSTableReader.java (line 162) Opening /raid0/cassandra/data/Keyspace/CF2-g-350
 INFO [CompactionExecutor:648] 2011-09-22 04:36:01,721 SSTableReader.java (line 162) Opening /raid0/cassandra/data/Keyspace/CF3-g-456
ERROR [Thread-3658] 2011-09-22 04:36:04,821 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[Thread-3658,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:154)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:189)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:117)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:138)
        ... 3 more
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [CompactionExecutor:648] 2011-09-22 04:36:04,823 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[CompactionExecutor:648,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
;;;","23/Sep/11 15:20;cce;Here's another from a node that was streaming out:


 INFO [AntiEntropyStage:1] 2011-09-22 17:56:33,737 StreamOut.java (line 181) Stream context metadata [/raid0/cassandra/data/Keyspace/CF1-g-315-Data.db sections=1073 progress=0/358
 INFO [AntiEntropyStage:1] 2011-09-22 17:56:33,738 StreamOutSession.java (line 174) Streaming to /10.207.38.196
 INFO [ValidationExecutor:14] 2011-09-22 17:56:33,740 ColumnFamilyStore.java (line 1128) Enqueuing flush of Memtable-CF4@516894197(9505947/21294113 serialized/live bytes, 5684 ops)
 INFO [FlushWriter:621] 2011-09-22 17:56:33,743 Memtable.java (line 237) Writing Memtable-CF4@516894197(9505947/21294113 serialized/live bytes, 5684 ops)
 INFO [FlushWriter:621] 2011-09-22 17:56:33,880 Memtable.java (line 254) Completed flushing /raid0/cassandra/data/Keyspace/CF4-g-434-Data.db (9914202 bytes)
 INFO [CompactionExecutor:884] 2011-09-22 17:56:45,912 CompactionManager.java (line 608) Compacted to /raid0/cassandra/data/Keyspace/CF4-tmp-g-332-Data.db.  32,824,572,147 to 18,506,0
ERROR [CompactionExecutor:884] 2011-09-22 17:56:46,134 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[CompactionExecutor:884,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
;;;","23/Sep/11 21:12;alienth;Getting the following after trying to 'move' on 0.8.5:

{code}
ERROR [CompactionExecutor:213] 2011-09-23 14:02:26,571
AbstractCassandraDaemon.java (line 139) Fatal exception in thread
Thread[CompactionExecutor:213,1,main]
java.lang.NullPointerException
       at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
       at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
       at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
       at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
       at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
{code};;;","24/Sep/11 05:25;alienth;Also got the same NPE when doing a repair on a different node. This time, the NPE was preceded by a different NPE:


{code}
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:154)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:177)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:114)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:138)
        ... 3 more
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1108)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1099)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

{code}
;;;","07/Oct/11 17:40;j.casares;We've seen the same error via autobootstrap.;;;","07/Oct/11 18:01;jbellis;Joaquin, can you give steps to reproduce?;;;","10/Oct/11 11:07;slebresne;Alright, I've tried boostrapping nodes a few times and I'm still not able to reproduce, so  it's likely a race of something.

That being said, I still have no clue how the iwriter field in SSTableWriter.RowIndexer can be null where the stack indicates it to be null. The assignment happens before the access, there is no other assignment of that field and the assignment/access are in the same thread.

Anyway, what we can easily do is to make iwrite being final and assign it in the constructor. If that doesn't make that field being non-null, I don't know what will. Patch attached to make that change.;;;","10/Oct/11 13:07;slebresne;First patch was buggy in that it was now creating the index file *before* we assert that this file doesn't exist.

v2 just moves the maybeOpenIndexer call after the asserts.;;;","10/Oct/11 13:12;jbellis;+1 v2;;;","10/Oct/11 13:59;slebresne;Committed and marking resolved for now. If that errors come up in another form, we'll see then.;;;","10/Oct/11 14:26;hudson;Integrated in Cassandra-0.8 #366 (See [https://builds.apache.org/job/Cassandra-0.8/366/])
    Make SSTW.RowIndexer.iwriter a final field to avoid NPE
patch by slebresne; reviewed by jbellis for CASSANDRA-2863

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1180958
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Versioning works *too* well,CASSANDRA-2860,12512897,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,05/Jul/11 21:28,16/Apr/19 09:32,14/Jul/23 05:52,19/Jul/11 22:03,0.8.2,,,,,,0,,,,"The scenario goes something like this: you upgrade from 0.7 to 0.8, but all the nodes remember that the remote side is 0.7, so they in turn speak 0.7, causing the local node to also think the remote is 0.7, even though both are really 0.8.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-7734,,CASSANDRA-3166,,,,,,"13/Jul/11 22:13;jbellis;2860-v2.txt;https://issues.apache.org/jira/secure/attachment/12486379/2860-v2.txt","06/Jul/11 16:10;jbellis;2860.txt;https://issues.apache.org/jira/secure/attachment/12485441/2860.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20868,,,Sat Jul 16 02:15:16 UTC 2011,,,,,,,,,,"0|i0gdwf:",93682,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"06/Jul/11 16:10;jbellis;patch to reset protocol-version-to-attempt when MessagingService resets a connection pool (when FD notices it's down);;;","13/Jul/11 22:13;jbellis;v2 moves reset call into OTC.disconnect, so it will work even if the restart time is too small for FD to kick in;;;","14/Jul/11 18:04;brandon.williams;+1;;;","16/Jul/11 02:15;hudson;Integrated in Cassandra-0.8 #218 (See [https://builds.apache.org/job/Cassandra-0.8/218/])
    reset protocol-version-to-attempt when reconnecting to a node
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-2860

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147355
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/OutboundTcpConnection.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
initialize log4j correctly in EmbeddedCassandraService,CASSANDRA-2857,12512883,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,05/Jul/11 19:51,16/Apr/19 09:32,14/Jul/23 05:52,29/Aug/11 15:24,0.8.5,,,,,,0,,,,"Currently, ECS.cleanUpOldStuff calls CleanupHelper.cleanupAndLeaveDirs(), which initialized DatabaseDescriptor which does some logging.  When we go to initialize log4j later in AbstractCassandraService, it's too late.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/11 19:55;jbellis;2857-drivers.txt;https://issues.apache.org/jira/secure/attachment/12485319/2857-drivers.txt","05/Jul/11 19:57;jbellis;2857.txt;https://issues.apache.org/jira/secure/attachment/12485320/2857.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19340,,,Mon Aug 29 16:23:13 UTC 2011,,,,,,,,,,"0|i0gdvr:",93679,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"05/Jul/11 19:52;jbellis;patch to move the log4j initialization into the ""real"" entry point classes;;;","29/Aug/11 14:59;tjake;needs rebase but +1;;;","29/Aug/11 15:24;jbellis;committed;;;","29/Aug/11 16:23;hudson;Integrated in Cassandra-0.8 #298 (See [https://builds.apache.org/job/Cassandra-0.8/298/])
    fix log4j initialization in EmbeddedCassandraService
patch by jbellis; reviewed by tjake for CASSANDRA-2857

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1162851
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AbstractCassandraDaemon.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraDaemon.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java Build Path is broken in Eclipse after running generate-eclipse-files Ant target,CASSANDRA-2854,12512681,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dallsopp,dallsopp,dallsopp,03/Jul/11 22:37,16/Apr/19 09:32,14/Jul/23 05:52,27/Oct/11 18:23,1.0.1,,,Packaging,,,1,,,,"Following the instructions in http://wiki.apache.org/cassandra/RunningCassandraInEclipse, but checking out v0.8.1:

After running the 'generate-eclipse-files' Ant target, the build path is set up to include all the libraries in build/lib/jars, but each library has ;C (semicolon, letter C) appended to it, so Eclipse can't find the libraries - there are about 55 errors like:

Project 'cassandra-0.8.1' is missing required library: '\Users\David\eclipse_workspace\cassandra-0.8.1\build\lib\jars\commons-cli-1.2.jar;C'
","Windows 7 64-bit, Eclipse Helios SR1, Subclipse",patricioe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/11 11:18;dallsopp;cassandra-2854.patch;https://issues.apache.org/jira/secure/attachment/12500320/cassandra-2854.patch",,,,,,,,,,,,,,1.0,dallsopp,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,1921,,,Thu Oct 27 18:23:44 UTC 2011,,,,,,,,,,"0|i0gdv3:",93676,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"03/Jul/11 22:44;dallsopp;Manually editing all the build path entries (and removing the one called just ';C' ) seems to resolve all the Eclipse project errors.;;;","03/Jul/11 23:46;dallsopp;The Ant target has the following (javascript:

{noformat}
jars = project.getProperty(""eclipse-project-libs"").split("":"");  
{noformat}

which is probably picking up on the colon in windows paths such as 
{noformat}
C:\Users\Foo\eclipse_workspace\cassandra-0.8.1\build\lib\foo.jar
{noformat};;;","22/Jul/11 16:54;patricioe;In my Case, Mac OS X 10.6.8 and Eclipse Helios 3.6.2

Even though the source folder is src/java I get this error (around 9 thousand of these)

{code}
The declared package ""org.apache.cassandra.utils.obs"" does not match the expected package ""java.org.apache.cassandra.utils.obs""	BitUtil.java	/cassandra-trunk/src/java/org/apache/cassandra/utils/obs	line 18	Java Problem
{code}
;;;","22/Jul/11 18:01;patricioe;I had this problem with Eclipse 3.6.2 always. Eclipse 3.7 works just fine.;;;","25/Jul/11 16:14;dallsopp;Patricio - I think that's a separate problem, but thanks for the tip, as I was getting that problem as well! I couldn't figure out why Eclipse acted as if that src was on the build path, but showed only src/java in the settings!;;;","13/Oct/11 22:17;jbellis;David, can you submit a patch to address the : problem?;;;","14/Oct/11 08:46;dallsopp;Will try to take a look. Probably needs to detect the OS in Ant to modify the regex passed to split(), e.g.:

<condition property=""path.sep"" value="";"">
   <os family=""windows""/>
</condition>
<condition property=""path.sep"" value="":"">
   <os family=""unix""/>
</condition>

<fail unless=""foo.path"">No path.sep set for this OS!</fail>

Or (possibly) use unix-style paths in the first place if Ant handles them OK on Windows (but not sure where they originate from; this may not be possible)...;;;","22/Oct/11 10:28;dallsopp;Wasn't having much success getting Eclipse happy on Windows, so tried on Linux but similar problems: after checking out trunk, running build.xml (runs OK) and then the generate-eclipse-files target (runs OK), I refresh the project and get:

{noformat}
Errors occurred during the build.
Errors running builder 'Integrated External Tool Builder' on project 'Cassandra-trunk'.
Variable references non-existent resource : ${workspace_loc:/cassandra-trunk/build.xml}
Variable references non-existent resource : ${workspace_loc:/cassandra-trunk/build.xml}
{noformat}

This is easy to fix - the project name I chose in Eclipse (Cassandra-trunk) is capitalized but the directory (cassandra-trunk) isn't. Go to Project->Properties->Builders, select Cassandra-Ant-Builder, Edit, and fix the Build File and Base Directory to match the name in Eclipse. This is probably caused by me checking out the code using SVN in the console rather than using Subversive within Eclipse...

*However, Eclipse report 2153 errors in the project at this point.*

If someone has this working nicely, could they please update the wiki page (http://wiki.apache.org/cassandra/RunningCassandraInEclipse), as it's got very outdated and confusing. Alternatively, if most Cassandra folks are using an environment other than Eclipse that works better, could you describe it on the wiki?;;;","22/Oct/11 10:40;dallsopp;After running generate-eclipse-files on either OS, some of the jars are duplicated, i.e. there are two copies on the build path, sometimes of different versions.

e.g commons-lang-2.1 and commons-lang-2.4
and servlet-api-2.5.20081211 (one copy from /lib and one from build/lib/jars);;;","22/Oct/11 11:18;dallsopp;Fix turns out to be trivial - in the javascript split() call, replace the hardcoded UNIX path separator "":"" with 

{code}project.getProperty(""path.separator""){code}

to acquire the correct OS-specific separator.;;;","22/Oct/11 11:18;dallsopp;One-liner to fix path separator;;;","22/Oct/11 13:09;dallsopp;I think I have found a workable way to get Eclipse set up reliably, so will nuke most of the wiki page (http://wiki.apache.org/cassandra/RunningCassandraInEclipse) and add a new version. There may well be a neater way, as I'm not yet sure _why_ things break when following the current instructions. I suspect it may be due to Eclipse compiling things into /bin until it is (later) reconfigured to user /build/classes/main.
;;;","27/Oct/11 18:23;urandom;committed; thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli has backwards index status message,CASSANDRA-2853,12512678,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stinkymatt,stinkymatt,stinkymatt,03/Jul/11 20:25,16/Apr/19 09:32,14/Jul/23 05:52,04/Jul/11 01:54,0.8.2,,,,,,0,,,,"When a secondary index is building, the total bytes and processed bytes are swapped in the message.  Example:
Currently building index cf1, completed 12052040551 of 18047343 bytes.

The problem is a call to CompactionInfo constructor with swapped parameters.  Patch to follow.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/11 20:27;stinkymatt;fix_idx_msg.patch;https://issues.apache.org/jira/secure/attachment/12485106/fix_idx_msg.patch",,,,,,,,,,,,,,1.0,stinkymatt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20867,,,Mon Jul 04 02:03:43 UTC 2011,,,,,,,,,,"0|i0gduv:",93675,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"04/Jul/11 01:54;jbellis;committed, thanks!;;;","04/Jul/11 02:03;hudson;Integrated in Cassandra-0.8 #202 (See [https://builds.apache.org/job/Cassandra-0.8/202/])
    fix index-building status display
patch by Matt Kennedy; reviewed by jbellis for CASSANDRA-2853

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1142530
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilySerializer.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/io/sstable/SSTableIdentityIterator.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ConfigHelper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/Table.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ColumnFamilyInputFormat.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/PendingFile.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/StreamInSession.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/io/sstable/IndexHelper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/IncomingStreamReader.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/StreamOut.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra CLI - Import Keyspace Definitions from File - Comments do partitially interpret characters/commands,CASSANDRA-2852,12512674,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,jensmueller,jensmueller,03/Jul/11 18:23,16/Apr/19 09:32,14/Jul/23 05:52,04/Jul/11 16:22,0.7.7,0.8.2,,Legacy/Tools,,,0,,,,"Hello, 

using: bin/cassandra-cli -host localhost --file conf/schema-sample.txt

with schema-sample.txt having contents like this:

/* here are a lot of comments,
like this sample create keyspace;
and so on
*/

Will result in an error: 
Line 1 => Syntax Error at Position 323: mismatched charackter '<EOF>' expecting '*'

The Cause is the keyspace; statement => the semicolon "";"" causes the error.

However:

Writing the word ""keyspace;"" with quotes, does NOT lead to the error.
so this works: 
/* here are a lot of comments,
like this sample create ""keyspace;""
and so on
*/

From my point of view this is an error. Everyting between the ""Start Comment"" => /* and ""End Comment"" => */ Should be treated as a comment and not be interpreted in any way. Thats the definition of a comment, to be not interpreted at all. 

Or this must be documented somewhere very prominently, otherwise this will lead to unnecessary wasting of time searching for this odd behavoiur. And it makes ""commenting out"" statements much more cumbersome.

Plattform: Windows Vista

thanks
",Win Vista ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/11 10:39;xedin;CASSANDRA-2852.patch;https://issues.apache.org/jira/secure/attachment/12485130/CASSANDRA-2852.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20866,,,Mon Jul 04 16:45:05 UTC 2011,,,,,,,,,,"0|i0gdun:",93674,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"04/Jul/11 10:39;xedin;can be applied on both 0.7 and 0.8 branches.;;;","04/Jul/11 16:22;jbellis;committed;;;","04/Jul/11 16:45;hudson;Integrated in Cassandra-0.7 #520 (See [https://builds.apache.org/job/Cassandra-0.7/520/])
    improve cli treatment of multiline comments
patch by pyaskevich; reviewed by jbellis for CASSANDRA-2852

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1142727
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/cli/CliMain.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
hex-to-bytes conversion accepts invalid inputs silently,CASSANDRA-2851,12512673,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,dallsopp,dallsopp,03/Jul/11 18:04,16/Apr/19 09:32,14/Jul/23 05:52,10/Feb/12 19:15,1.1.0,,,,,,0,,,,"FBUtilities.hexToBytes() has a minor bug - it copes with single-character inputs by prepending ""0"", which is OK - but it does this for any input with an odd number of characters, which is probably incorrect.

{noformat}
if (str.length() % 2 == 1)
    str = ""0"" + str;
{noformat}

Given 'fff' as an input, can we really assume that this should be '0fff'? Isn't this just an error?

Add the following to FBUtilitiesTest to demonstrate:

{noformat}
String[] badvalues = new String[]{""000"", ""fff""};
       
for (int i = 0; i < badvalues.length; i++)
    try
    {
        FBUtilities.hexToBytes(badvalues[i]);
        fail(""Invalid hex value accepted""+badvalues[i]);
    } catch (Exception e){}
{noformat}",,dallsopp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/12 11:30;slebresne;2851.patch;https://issues.apache.org/jira/secure/attachment/12514100/2851.patch","03/Jul/11 19:12;dallsopp;cassandra-2851.diff;https://issues.apache.org/jira/secure/attachment/12485099/cassandra-2851.diff",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20865,,,Fri Feb 10 19:15:15 UTC 2012,,,,,,,,,,"0|i0gduf:",93673,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"03/Jul/11 18:12;dallsopp;In addition to the invalid values, should probably also add a check in FBUtilitiesTest.testHexToBytesStringConversion() that """" is converted to byte[0], and that a null input is handled appropriately.

;;;","03/Jul/11 19:12;dallsopp;If there's agreement that odd-sized inputs should throw an exception, the attached patch does this and updates the unit test.;;;","04/Jul/11 08:57;slebresne;Why would it be ok for single-character inputs and not other odd-sized inputs ? An odd-sized input doesn't (ever) correspond to a valid byte array, so I'd say either we always silently add a 0 to ""make it fit"" or we never do it. I do actually am in favor of throwing an exception rather then coping with it silently since it's more likely to indicate a user error than to be helpful (but maybe that addition of a '0' in front was there for a reason?).
I'll note that even though I can't imagine why people would generate odd-sized hex input, since it is allowed so far, there is a chance someone out there does it, and it would be a ""regression"" for that guy. So maybe we should target 1.0 for the sake of making minor upgrade as smooth for everybody as can be.

On the patch side, we must make sure every consumer of hexToBytes() handles the new exception (or make it a NumberFormatException but I don't think this is a good idea). For instance, at least BytesType.fromString() should catch the IllegalArgumentException and rethrow a MarshalException, otherwise CQL will crap his pants on odd-sized inputs.;;;","04/Jul/11 12:35;jbellis;bq. maybe that addition of a '0' in front was there for a reason

I think it's there b/c of Integer.toHexString: ""This value is converted to a string of ASCII digits in hexadecimal (base 16) with no extra leading 0s.""

Our bytesToHex does pad... but only for single-digit results.  So if we fix hexToBytes we'll introduce an incompatibility. (Granted, a minor one.);;;","04/Jul/11 15:35;slebresne;bq. Our bytesToHex does pad... but only for single-digit results. So if we fix hexToBytes we'll introduce an incompatibility. (Granted, a minor one.)

I don't understand. There is no such thing as padding when you convert a byte array to hex (Integer.toHexString does return only the right number of hexadecimal digits because it has no reason to do otherwise, but it's an implementation detail of bytesToHex). A byte is always 8 bit, never 4, and the output of bytesToHex will *always* have a even number of characters (as it should). Our hexToBytes just happen to semi-randomly add 0 in front to transform a buggy input with an odd number of character to a even one, in the off chance that a client used the (stupid) ""optimization"" of removing at most 1 leading 0 to win some space or something. In my opinion, it would be better to simply refuse odd sized input because this is more likely a truncated input (and people using stupid clients should fix them, though I'm ok with saying that we'll force them to fix it only on a major upgrade).;;;","04/Jul/11 16:16;jbellis;You're right, I was misreading how we were using Integer.toHexString.;;;","04/Jul/11 17:51;dallsopp;The origin of the current behaviour is CASSANDRA-1411 https://issues.apache.org/jira/browse/CASSANDRA-1411 if that helps...

;;;","04/Jul/11 19:18;jbellis;Good point, David.

Sounds like the problem is thinking of this as a generic hex conversion function, rather than as ""hex that specifically represents bytes."";;;","08/Sep/11 09:48;slebresne;bq. Sounds like the problem is thinking of this as a generic hex conversion function, rather than as ""hex that specifically represents bytes.""

Well, in a way those methods never pretended being generic hex to *bits* conversion functions since they are called hex2bytes and bytes2hex. I still think that the correct behavior for hex2bytes is to throw an exception on odd sized input. And that CASSANDRA-1411 was a mistake: yes I understand that it's annoying the first time you use '0' as token to get an exception and to have to write '00', but if you had written '0', there is a good chance you had missed the meaning of the input, i.e that it is  an hex string representing a byte array.

Now changing that behavior would not be backward compatible, so I wonder if we really should do anything here. I would be in favor however to refactor the code slightly so that hex2bytes do throw an exception on odd sized input (it would be fine if the method was named hex2bits, but it's fishy with hex2bytes) and to move the code that pads the input into ByteOrderedPartitioner, since that is what it was meant to: simplify the input of tokens.;;;","08/Sep/11 13:03;jbellis;That sounds reasonable.;;;","13/Sep/11 10:32;dallsopp;Sounds good to me - I agree the underlying conversion should be strict to
avoid inadvertent misuse and hidden bugs in future - with a more lenient
wrapper for specific cases and backward compatibility.


;;;","10/Feb/12 11:30;slebresne;Attaching patch that implements the idea above, i.e. hexToBytes throw an exception on odd number of input characters, but silently adds a '0' in front when parsing a token for an ordered partitioner.

This does mean BytesType will throw a MarshalException when given a string of odd length. I think it is a good thing, it was a but that we were potentially silently adding some zeroes to column names or values given by the user. However, this does kind of break some form of backward compatibility so I suppose we should either push it for 1.1.0 or report to 1.2. I personally would be good for 1.1.0 (I consider the current behavior a bug) but I won't argue if someone prefers 1.2.;;;","10/Feb/12 13:44;jbellis;+1 for 1.1.0 from me;;;","10/Feb/12 19:15;slebresne;Ok, committed to 1.1.0. We'll revert if someone feels strongly this is a violation of the freeze.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InvalidRequestException when validating column data includes entire column value,CASSANDRA-2849,12512664,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dallsopp,dallsopp,dallsopp,03/Jul/11 11:41,16/Apr/19 09:32,14/Jul/23 05:52,11/Aug/11 19:19,0.8.5,,,,,,0,,,,"If the column value fails to validate, then ThriftValidation.validateColumnData() calls bytesToHex() on the entire column value and puts this string in the Exception. Since the value may be up to 2GB, this is potentially a lot of extra memory. The value is likely to be logged (and presumably returned to the thrift client over the network?). This could cause a lot of slowdown or an unnecessary OOM crash, and is unlikely to be useful (the client has access to the full value anyway if required for debugging).

Also, the reason for the exception (extracted from the MarshalException) is printed _after_ the data, so if there's any truncation in the logging system at any point, the reason will be lost. 

The reason should be displayed before the column value, and the column value should be truncated in the Exception message.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/11 16:52;jbellis;2849-v2.txt;https://issues.apache.org/jira/secure/attachment/12485302/2849-v2.txt","03/Jul/11 12:08;dallsopp;cassandra-2849.diff;https://issues.apache.org/jira/secure/attachment/12485085/cassandra-2849.diff",,,,,,,,,,,,,2.0,dallsopp,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20864,,,Thu Aug 11 20:21:40 UTC 2011,,,,,,,,,,"0|i0gdtz:",93671,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"03/Jul/11 12:08;dallsopp;Attached diff moves the reason for the exception before the data, and truncates the column value data in the exception to 64K.

Unable to test this properly today since working on a fresh Windows box over the weekend and unable to get build working in Eclipse 8-(

;;;","03/Jul/11 15:22;dallsopp;A lot of classes use ByteBufferUtil.bytesToHex(), often for logging and exceptions, so may make more sense to add a method there for getting a truncated hex string.;;;","03/Jul/11 16:13;dallsopp;On a related note, I just noticed that ByteBufferUtil.bytesToHex() is unnecessarily slow - will raise a new issue for this.;;;","05/Jul/11 16:52;jbellis;v2 adds column name decoding and moves the value to a debug log (presumably the client  knows what value it sent...);;;","11/Aug/11 14:28;jbellis;David, does v2 look ok to you?;;;","11/Aug/11 15:35;dallsopp;Yes, that looks good to me, thanks.;;;","11/Aug/11 19:19;jbellis;committed;;;","11/Aug/11 20:21;hudson;Integrated in Cassandra-0.8 #272 (See [https://builds.apache.org/job/Cassandra-0.8/272/])
    include column name in validation failure exceptions
patch by jbellis; reviewed by David Allsopp for CASSANDRA-2849

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156753
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Changing replication_factor using ""update keyspace"" not working",CASSANDRA-2846,12512469,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jborgstrom,jborgstrom,01/Jul/11 14:15,16/Apr/19 09:32,14/Jul/23 05:52,04/Jul/11 16:20,0.8.2,,,,,,0,,,,"Unless I've misunderstood the new way to do this with 0.8 I think ""update keyspace"" is broken:

{code}
[default@unknown] create keyspace Test with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = [{replication_factor:1}];
37f70d40-a3e9-11e0-0000-242d50cf1fbf
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] describe keyspace Test;
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
[default@unknown] update keyspace Test with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = [{replication_factor:2}];
489fe220-a3e9-11e0-0000-242d50cf1fbf
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] describe keyspace Test;                                                                                                                   Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
{code}

Isn't the second ""describe keyspace"" supposed to to say ""replication_factor:2""?

Relevant bits from system.log:
{code}
Migration.java (line 116) Applying migration 489fe220-a3e9-11e0-0000-242d50cf1fbf Update keyspace Testrep strategy:SimpleStrategy{}durable_writes: true to Testrep strategy:SimpleStrategy{}durable_writes: true
UpdateKeyspace.java (line 74) Keyspace updated. Please perform any manual operations
{code}
",A clean 0.8.1 install using the default configuration,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/11 15:36;jbellis;2846.txt;https://issues.apache.org/jira/secure/attachment/12484892/2846.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20862,,,Mon Jul 04 17:21:16 UTC 2011,,,,,,,,,,"0|i0gdtb:",93668,,jborgstrom,,jborgstrom,Low,,,,,,,,,,,,,,,,,"01/Jul/11 15:36;jbellis;        // server helpfully sets deprecated replication factor when it sends a KsDef back, for older clients.
        // we need to unset that on the new KsDef we create to avoid being treated as a legacy client in return.
;;;","01/Jul/11 23:07;jhermes;--1, doesn't update strategy_options for KS's that already have SimpleStrategy.-

+1, it's good.;;;","01/Jul/11 23:09;jbellis;Jonas's test case works for me.;;;","04/Jul/11 09:54;jborgstrom;Jonathan, thanks for your fast response. Your patch works for me.;;;","04/Jul/11 16:20;jbellis;committed;;;","04/Jul/11 17:21;hudson;Integrated in Cassandra-0.8 #204 (See [https://builds.apache.org/job/Cassandra-0.8/204/])
    fix CLI perpetuating obsolete KsDef.replication_factor
patch by jbellis; tested by Jonas Borgström for CASSANDRA-2846

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1142725
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive JDBC connections fail with InvalidUrlException when both the C* and Hive JDBC drivers are loaded,CASSANDRA-2842,12512391,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ardot,cdaw,cdaw,30/Jun/11 23:01,16/Apr/19 09:32,14/Jul/23 05:52,01/Jul/11 19:50,,,,,,,0,,,,"Hive connections fail with InvalidUrlException when both the C* and Hive JDBC drivers are loaded, and it seems the URL is being interpreted as a C* url.

{code}
	Caused an ERROR
    [junit] Invalid connection url:jdbc:hive://127.0.0.1:10000/default. should start with jdbc:cassandra
    [junit] org.apache.cassandra.cql.jdbc.InvalidUrlException: Invalid connection url:jdbc:hive://127.0.0.1:10000/default. should start with jdbc:cassandra
    [junit] 	at org.apache.cassandra.cql.jdbc.CassandraDriver.connect(CassandraDriver.java:90)
    [junit] 	at java.sql.DriverManager.getConnection(DriverManager.java:582)
    [junit] 	at java.sql.DriverManager.getConnection(DriverManager.java:185)
    [junit] 	at com.datastax.bugRepros.repro_connection_error.test1_runHiveBeforeJdbc(repro_connection_error.java:34)

{code}

*Code Snippet: intended to illustrate the connection issues* 
* Copy file to test directory
* Change package declaration
* run:  ant test -Dtest.name=repro_conn_error
{code}

package com.datastax.bugRepros;

import java.sql.DriverManager;
import java.sql.Connection;
import java.sql.SQLException;

import java.util.Enumeration;

import org.junit.Test;

public class repro_conn_error
{
    @Test
    public void jdbcConnectionError() throws Exception 
    {  
        // Create Hive JDBC Connection - will succeed if      
        try 
        {
            // Uncomment loading C* driver to reproduce bug
            Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
            
            // Load Hive driver and connect
            Class.forName(""org.apache.hadoop.hive.jdbc.HiveDriver"");
            Connection hiveConn = DriverManager.getConnection(""jdbc:hive://127.0.0.1:10000/default"", """", """");
            hiveConn.close();  
            System.out.println(""successful hive connection"");

        } catch (SQLException e) {
            System.out.println(""unsuccessful hive connection"");
            e.printStackTrace();
        }
        
        // Create C* JDBC Connection
        try 
        {
            Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
            Connection jdbcConn = DriverManager.getConnection(""jdbc:cassandra:root/root@127.0.0.1:9160/default"");     
            jdbcConn.close();    
            System.out.println(""successful c* connection"");

        } catch (SQLException e) {
            System.out.println(""unsuccessful c* connection"");

            e.printStackTrace();
        }
        
        // Print out all loaded JDBC drivers.
        Enumeration d = java.sql.DriverManager.getDrivers();
        
        while (d.hasMoreElements()) {
            Object driverAsObject = d.nextElement();
            System.out.println(""JDBC driver="" + driverAsObject);
        }
    }
}

{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/11 14:19;ardot;pass-if-not-right-driver-v1.txt;https://issues.apache.org/jira/secure/attachment/12484870/pass-if-not-right-driver-v1.txt",,,,,,,,,,,,,,1.0,ardot,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20860,,,Fri Jul 01 19:50:15 UTC 2011,,,,,,,,,,"0|i0gdsf:",93664,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"01/Jul/11 14:19;ardot;This test has been run against v1.0.3 of the driver. In that version the {{connect(...)}} method of {{CassandraDriver}} is called with an unsupported protocol:subprotocol in its URL. It recognizes it is not the proper protocol but erroneously throws an exception rather than returning a null to the caller stating that it can not handle it, so please move on. The patch is based on the current trunk of {{/drivers}} (v1.0.4).;;;","01/Jul/11 17:12;ardot;I took a quick look at the Hive sources and I believe you will find the Hive Driver suffers from this defect as well. So if you reversed the order I think it will be the Hive driver that throws an exception rather than deferring to the next driver in the chain of loaded drivers(C*).;;;","01/Jul/11 19:50;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Small typos in the cli,CASSANDRA-2839,12512152,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,j.casares,j.casares,j.casares,29/Jun/11 00:08,16/Apr/19 09:32,14/Jul/23 05:52,07/Jul/11 13:52,0.7.7,,,,,,0,,,,"Memtable thresholds: %s/%s/%s (millions of ops/minutes/MB) was displaying ops/MB/minutes.

placement_strategy: the fully qualified class used to place replicas in
                        this keyspace. Valid values are
                        org.apache.cassandra.locator.SimpleStrategy,
                        org.apache.cassandra.locator.NetworkTopologyStrategy,
                        and org.apache.cassandra.locator.OldNetworkTopologyStrategy
was being displayed but would only accept SimpleStrategy.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/11 00:10;j.casares;2839.diff;https://issues.apache.org/jira/secure/attachment/12484582/2839.diff",,,,,,,,,,,,,,1.0,j.casares,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20858,,,Thu Jul 07 13:52:36 UTC 2011,,,,,,,,,,"0|i0gdrr:",93661,,,,,Low,,,,,,,,,,,,,,,,,"06/Jul/11 14:51;slebresne;I've committed the typo on the memtable thresholds. On the placement_strategy help, are you sure ? Because CliTest does use a fully qualified class and don't seem to fail.;;;","06/Jul/11 15:02;hudson;Integrated in Cassandra-0.7 #523 (See [https://builds.apache.org/job/Cassandra-0.7/523/])
    Fix typo in CLI help
patch by j.casares; reviewed by slebresne for CASSANDRA-2839

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1143444
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/cli/CliClient.java
;;;","06/Jul/11 17:58;j.casares;This is my output of my test on cassandra-0.7.6 just right now:

[default@unknown] create keyspace abc with placement_strategy=org.apache.cassandra.locator.SimpleStrategy;
Syntax error at position 47: missing EOF at '.'
[default@unknown] create keyspace abc with placement_strategy=org.apache.cassandra.locator.NetworkTopologyStrategy;
Syntax error at position 47: missing EOF at '.'
[default@unknown] create keyspace abc with placement_strategy=org.apache.cassandra.locator.OldNetworkTopologyStrategy;
Syntax error at position 47: missing EOF at '.'
[default@unknown] create keyspace abc with placement_strategy=SimpleStrategy;                                         
844d2b67-a7f9-11e0-b6a3-e700f669bcfc
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] update keyspace abc with placement_strategy=NetworkTopologyStrategy;
9182d288-a7f9-11e0-b6a3-e700f669bcfc
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] update keyspace abc with placement_strategy=OldNetworkTopologyStrategy;
96344ca9-a7f9-11e0-b6a3-e700f669bcfc
Waiting for schema agreement...
... schemas agree across the cluster
;;;","06/Jul/11 18:15;slebresne;You'd have to use placement_strategy='org.apache.cassandra.locator.SimpleStrategy';;;","06/Jul/11 18:34;j.casares;Works! Could we get the help lines to reflect this?;;;","07/Jul/11 13:52;slebresne;Help is updated, though the example section was pretty clear already.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting RR Chance via CliClient results in chance being too low,CASSANDRA-2837,12512038,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jhermes,dehora,dehora,28/Jun/11 20:04,16/Apr/19 09:32,14/Jul/23 05:52,29/Jun/11 01:15,0.7.7,,,,,,0,,,,"running a command like 

{noformat}
""update column family shorturls with read_repair_chance=0.4;""
{noformat}

results in the value being set to 0.0040. Was expecting it to be 0.4.

Affects 0.7.6.-2; seems to be fixed on trunk/0.8.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/11 20:06;jhermes;2837.txt;https://issues.apache.org/jira/secure/attachment/12484494/2837.txt",,,,,,,,,,,,,,1.0,jhermes,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20857,,,Wed Jun 29 01:50:01 UTC 2011,,,,,,,,,,"0|i0gdrb:",93659,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"28/Jun/11 20:06;jhermes;Straightforward patch.
If the help says it expects a double from 0.0 to 1.0, it shouldn't then divide by 100 as if expecting a percentage.;;;","29/Jun/11 01:15;jbellis;committed, thanks!;;;","29/Jun/11 01:50;hudson;Integrated in Cassandra-0.7 #516 (See [https://builds.apache.org/job/Cassandra-0.7/516/])
    fix CLI parsing of read_repair_chance
patch by Jon Hermes; reviewed by jbellis for CASSANDRA-2837

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1140928
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/cli/CliClient.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFMetadata don't set the default for Replicate_on_write correctly,CASSANDRA-2835,12511983,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,28/Jun/11 12:47,16/Apr/19 09:32,14/Jul/23 05:52,28/Jun/11 13:15,0.8.2,,,,,,0,counters,,,"Replicate_on_write *must* default to true (defaulting to false is very dangerous and imho, the option of setting it to false shouldn't exist in the first place) and CFMetadata.DEFAULT_REPLICATE_ON_WRITE is correctly true. But it doesn't get set correctly. Instead, the code force the value of the cf_def even if it wasn't defined, resulting in setting it to false since that is the default value for a boolean in JAVA.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/11 12:48;slebresne;2835.patch;https://issues.apache.org/jira/secure/attachment/12484422/2835.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20856,,,Tue Jun 28 21:27:07 UTC 2011,,,,,,,,,,"0|i0gdqv:",93657,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"28/Jun/11 12:51;jbellis;+1;;;","28/Jun/11 12:51;jbellis;Add a note to NEWS suggesting checking counter settings on upgrade?;;;","28/Jun/11 13:15;slebresne;Committed, thanks (I've included a not in the NEWS file as well as updated the wiki).;;;","28/Jun/11 21:27;hudson;Integrated in Cassandra-0.8 #197 (See [https://builds.apache.org/job/Cassandra-0.8/197/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating or updating CF key_validation_class with the CLI doesn't works,CASSANDRA-2831,12511644,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,silvere,silvere,27/Jun/11 12:54,16/Apr/19 09:32,14/Jul/23 05:52,27/Jun/11 13:54,0.8.1,,,,,,0,,,,"In the command line:
{code}
create column family test with key_validation_class = 'AsciiType' and comparator = 'LongType' and default_validation_class = 'IntegerType';
describe keyspace;
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.AsciiType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.LongType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
{code}
The ""Default column value validator"" is BytesType instead of IntegerType. Also tested with other types or with the ""update column family"" command, same problem occur.

{code}
[default@Test] update column family test with default_validation_class = 'LongType';
51a37430-a0bb-11e0-0000-ef8993101fdf
Waiting for schema agreement...
... schemas agree across the cluster
[default@Test] describe keyspace;                                                   
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.AsciiType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.LongType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
{code}

Btw, they are a typo in file src/resources/org/apache/cassandra/cli/CliHelp.yaml line 642: key_valiation_class > key_validation_class
Very annoying for people like me who stupidly copy/paste the help.","Ubuntu 10.10, 32 bits
java version ""1.6.0_24""
Brisk beta-2 installed from Debian packages",silvere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20855,,,Mon Jun 27 13:54:38 UTC 2011,,,,,,,,,,"0|i0gdq7:",93654,,,,,Normal,,,,,,,,,,,,,,,,,"27/Jun/11 13:54;jbellis;fixed by r1137774;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memtable with no post-flush activity can leave commitlog permanently dirty,CASSANDRA-2829,12511589,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,amorton,amorton,27/Jun/11 01:54,16/Apr/19 09:32,14/Jul/23 05:52,01/Aug/11 15:12,0.8.3,,,,,,0,,,,"Only dirty Memtables are flushed, and so only dirty memtables are used to discard obsolete commit log segments. This can result it log segments not been deleted even though the data has been flushed.  

Was using a 3 node 0.7.6-2 AWS cluster (DataStax AMI's) with pre 0.7 data loaded and a running application working against the cluster. Did a rolling restart and then kicked off a repair, one node filled up the commit log volume with 7GB+ of log data, there was about 20 hours of log files. 

{noformat}
$ sudo ls -lah commitlog/
total 6.9G
drwx------ 2 cassandra cassandra  12K 2011-06-24 20:38 .
drwxr-xr-x 3 cassandra cassandra 4.0K 2011-06-25 01:47 ..
-rw------- 1 cassandra cassandra 129M 2011-06-24 01:08 CommitLog-1308876643288.log
-rw------- 1 cassandra cassandra   28 2011-06-24 20:47 CommitLog-1308876643288.log.header
-rw-r--r-- 1 cassandra cassandra 129M 2011-06-24 01:36 CommitLog-1308877711517.log
-rw-r--r-- 1 cassandra cassandra   28 2011-06-24 20:47 CommitLog-1308877711517.log.header
-rw-r--r-- 1 cassandra cassandra 129M 2011-06-24 02:20 CommitLog-1308879395824.log
-rw-r--r-- 1 cassandra cassandra   28 2011-06-24 20:47 CommitLog-1308879395824.log.header
...
-rw-r--r-- 1 cassandra cassandra 129M 2011-06-24 20:38 CommitLog-1308946745380.log
-rw-r--r-- 1 cassandra cassandra   36 2011-06-24 20:47 CommitLog-1308946745380.log.header
-rw-r--r-- 1 cassandra cassandra 112M 2011-06-24 20:54 CommitLog-1308947888397.log
-rw-r--r-- 1 cassandra cassandra   44 2011-06-24 20:47 CommitLog-1308947888397.log.header
{noformat}

The user KS has 2 CF's with 60 minute flush times. System KS had the default settings which is 24 hours. Will create another ticket see if these can be reduced or if it's something users should do, in this case it would not have mattered. 

I grabbed the log headers and used the tool in CASSANDRA-2828 and most of the segments had the system CF's marked as dirty.

{noformat}
$ bin/logtool dirty /tmp/logs/commitlog/

Not connected to a server, Keyspace and Column Family names are not available.

/tmp/logs/commitlog/CommitLog-1308876643288.log.header
Keyspace Unknown:
	Cf id 0: 444
/tmp/logs/commitlog/CommitLog-1308877711517.log.header
Keyspace Unknown:
	Cf id 1: 68848763
...
/tmp/logs/commitlog/CommitLog-1308944451460.log.header
Keyspace Unknown:
	Cf id 1: 61074
/tmp/logs/commitlog/CommitLog-1308945597471.log.header
Keyspace Unknown:
	Cf id 1000: 43175492
	Cf id 1: 108483
/tmp/logs/commitlog/CommitLog-1308946745380.log.header
Keyspace Unknown:
	Cf id 1000: 239223
	Cf id 1: 172211

/tmp/logs/commitlog/CommitLog-1308947888397.log.header
Keyspace Unknown:
	Cf id 1001: 57595560
	Cf id 1: 816960
	Cf id 1000: 0
{noformat}

CF 0 is the Status / LocationInfo CF and 1 is the HintedHandof CF. I dont have it now, but IIRC CFStats showed the LocationInfo CF with dirty ops. 

I was able to repo a case where flushing the CF's did not mark the log segments as obsolete (attached unit-test patch). Steps are:

1. Write to cf1 and flush.
2. Current log segment is marked as dirty at the CL position when the flush started, CommitLog.discardCompletedSegmentsInternal()
3. Do not write to cf1 again.
4. Roll the log, my test does this manually. 
5. Write to CF2 and flush.
6. Only CF2 is flushed because it is the only dirty CF. cfs.maybeSwitchMemtable() is not called for cf1 and so log segment 1 is still marked as dirty from cf1.

Step 5 is not essential, just matched what I thought was happening. I thought SystemTable.updateToken() was called which does not flush, and this was the last thing that happened.  

The expired memtable thread created by Table uses the same cfs.forceFlush() which is a no-op if the cf or it's secondary indexes are clean. 
    
I think the same problem would exist in 0.8. ",,hanzhu,yulinyen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2828,,,,,,"21/Jul/11 13:01;amorton;0001-2829-unit-test-v08.patch;https://issues.apache.org/jira/secure/attachment/12487299/0001-2829-unit-test-v08.patch","27/Jun/11 02:00;amorton;0001-2829-unit-test.patch;https://issues.apache.org/jira/secure/attachment/12483893/0001-2829-unit-test.patch","21/Jul/11 13:01;amorton;0002-2829-v08.patch;https://issues.apache.org/jira/secure/attachment/12487300/0002-2829-v08.patch","27/Jun/11 02:00;amorton;0002-2829.patch;https://issues.apache.org/jira/secure/attachment/12483892/0002-2829.patch","01/Aug/11 11:20;slebresne;2829.patch;https://issues.apache.org/jira/secure/attachment/12488395/2829.patch",,,,,,,,,,5.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20854,,,Mon Aug 01 15:22:36 UTC 2011,,,,,,,,,,"0|i0gdpr:",93652,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"27/Jun/11 02:00;amorton;2829-unit-test contains the unit test for the problem. 2829 is the fix. ;;;","09/Jul/11 23:04;jbellis;Good detective work finding this!

I'm not sure about the proposed fix, though -- I think this reasoning still applies:
{noformat}
                // we can't just mark the segment where the flush happened clean,
                // since there may have been writes to it between when the flush
                // started and when it finished.
{noformat}

... the memtable may have been clean when the flush started, but we don't block writes until flush finishes, so some may have finished in between (so the CL may have writes for this segment now).

(I don't have a better fix yet, this is a tough one.);;;","13/Jul/11 15:48;jbellis;Thinking out loud here.

CLHeader has a structure to tell us ""do we need to replay, and if so, from where?""

{code}
    private Map<Integer, Integer> cfDirtiedAt; // position at which each CF was last flushed

    boolean isDirty(Integer cfId)
    {
        return cfDirtiedAt.containsKey(cfId);
    } 

    boolean isDirty(Integer cfId)
    {
        return cfDirtiedAt.containsKey(cfId);
    } 
{code}

This is set in two places.  One is during a write:

{code}
                    if (!header.isDirty(id))
                    {
                        header.turnOn(id, logWriter.getFilePointer());
                        writeHeader();
                    }
{code}

The other is post-flush, as described above:

{code}
            if (segment.equals(context.getSegment()))
            {
                // we can't just mark the segment where the flush happened clean,
                // since there may have been writes to it between when the flush
                // started and when it finished. so mark the flush position as
                // the replay point for this CF, instead.
                if (logger.isDebugEnabled())
                    logger.debug(""Marking replay position "" + context.position + "" on commit log "" + segment);
                header.turnOn(id, context.position);
                segment.writeHeader();
                break;
            }
{code}

It feels like we need to add a ""most recent write at"" information as well as the ""oldest write/replay position at"" one.  This would not need to be persisted to disk.

(I thought that this is what 0.6 did, but looking at it that is not the case.  So this bug is present there as well, but I think at this point it just needs to be a known bug there.  Maybe even for 0.7.);;;","21/Jul/11 11:50;slebresne;bq. It feels like we need to add a ""most recent write at"" information as well as the ""oldest write/replay position at"" one. This would not need to be persisted to disk.

Agreed, I think this is the right fix too.;;;","21/Jul/11 13:01;amorton;I got to take another look at this tonight on the 0.8 trunk and ported the unit test to 0.8. 

The 002-2829-v08 patch was my second attempt. It changes CFS.forceFlush() to always flush and trusts maybeSwitchMemtable() will only flush non clean CF's. 

There are no changes to  CommitLog.discardCompletedSegmentsInternal(). The CF will be turned off in any segment that is not the context segment. It will always be turned on in the current / context segment. I think this gives the correct behaviour, i.e. the cf can never have dirty changes in a segment that is not current AND the cf may have changes in a segment that is current. It is a bit sloppy though as clean CF's will mark segments as dirty which may delay them been cleaned. 


I also think there is a theoretical risk of a race condition with access to the segments Deque.  The iterator runs in the postFlushExecutor and the segments are added on the appropriate commit log executor service.

;;;","21/Jul/11 14:55;jbellis;bq. I also think there is a theoretical risk of a race condition with access to the segments Deque. The iterator runs in the postFlushExecutor

discardCompletedSegments actually does the real work in a task on the CL executor. Unless that's not what you're thinking of, I think we're ok here.

bq. It changes CFS.forceFlush() to always flush and trusts maybeSwitchMemtable() will only flush non clean CF's

Hmm.  Interesting.

Part of me thinks it can't be that simple but I don't see a problem with it. :)

Sylvain?
;;;","21/Jul/11 17:02;slebresne;I think this kind of work, in that we won't keep commit log forever, but it still keep commit logs for much longer than necessary because:
# it relies on forceFlush being called, which unless client triggered will only be after the memtable expires and quite a bunch of commit log could pile up during that time. Quite potentially enough to be a problem (if the commit logs fills up you hard drive, it doesn't matter much that ""it would have been deleted in 5 hours""). I think we can do much better with not too much effort.
# when we do flush the expired memtable, we'll call maybeSwitchMemtable() will potentially clean memtables. This doesn't sound like a good use of resource: we'll grab the write lock, create a latch, create a new memtable, increment the memtable switch number, push an almost no-op job on the flush executor.

I think we should fix the real problem. The problem is that we discard segment, we always keep the current segment dirty because we don't know if there was some write since we grabbed the context. Let's add that information and fix that. This would make commit log being deleted much quicker, even if we don't consider the corner case of column family that have suddenly no write anymore, because CFs like the system ones, that have very low update volume can retain the logs longer than it's really need.

As for the fix, because the CL executor is mono-threaded, this is fairly easy, let's have an in-memory map of cfId->lastPositionWritten, and compare that to the context position in discardCompletedSegmentInternal (we could probably even just use a set of cfid who would meant: dirty since last getContext).;;;","30/Jul/11 13:56;hanzhu;{quote}Let's add that information and fix that. {quote}

Does it mean everytime an RowMutation is appended to the log, the log header should be fsynced again? It brings at least one extra disk seek at a critical path.;;;","31/Jul/11 03:42;jbellis;This can be kept purely in-memory.  No need to sync anything.  (BTW there is no header per se post CASSANDRA-2419.);;;","31/Jul/11 04:30;hanzhu;{quote}This can be kept purely in-memory{quote}

OK. So these these log segments might no be ignored during log replay. Maybe not a problem at all.;;;","01/Aug/11 11:20;slebresne;Houston, we have a problem.

In 0.8, we have a much bigger problem related to the commit log. Turns out we don't even turnOn the isDirty flag on writes. This means that typically if we fill a segment (with write of different cfs), starts a new one, and flush (one cf, say cf1), the previous segment will be removed even though it may be full of dirty writes for cf != cf1.

Attaching a patch that fix this issue as well as the original issue of this ticket (as it is not really more complicated). It adds two unit test, one for each issue (both fails in current 0.8). Bumping the priority of this too. ;;;","01/Aug/11 13:54;jbellis;+1;;;","01/Aug/11 15:12;slebresne;Committed, thanks;;;","01/Aug/11 15:22;hudson;Integrated in Cassandra-0.8 #248 (See [https://builds.apache.org/job/Cassandra-0.8/248/])
    fix bug where dirty commit logs were removed (and avoid keeping segment with no post-flush activity permanently dirty)
patch by slebresne; reviewed by jbellis for CASSANDRA-2829

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152793
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLogSegment.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/CommitLogTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debian Package for 0.8 is missing,CASSANDRA-2826,12511523,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,smitchell360,smitchell360,25/Jun/11 04:30,16/Apr/19 09:32,14/Jul/23 05:52,12/Aug/11 21:45,0.8.5,,,Packaging,,,1,,,,"In file

http://www.apache.org/dist/cassandra/debian/dists/08x/InRelease

Codename: sid

Should be changed to

Codename: 08x",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20853,,,Fri Aug 12 21:45:01 UTC 2011,,,,,,,,,,"0|i0gdp3:",93649,,,,,Low,,,,,,,,,,,,,,,,,"25/Jun/11 14:28;thepaul;I think Eric has to take care of this.;;;","12/Aug/11 21:45;urandom;This is no longer relevant as the unstable suite has been eliminated.  Closing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Auto bootstrapping the 4th node in a 4 node cluster doesn't work, when no token explicitly assigned in config.",CASSANDRA-2825,12511498,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,mallen,mallen,24/Jun/11 18:41,16/Apr/19 09:32,14/Jul/23 05:52,01/Aug/11 17:55,0.8.3,,,,,,0,,,,"This was done in sequence.  A, B, C, and D.  Node A with token 0 explicitly set in config.  The rest with auto_bootstrap: true and no token explicitly assigned.  B and C work as expected. D ends up stealing C's token.  

from system.log on C:

INFO [GossipStage:1] 2011-06-24 16:40:41,947 Gossiper.java (line 638) Node /10.171.47.226 is now part of the cluster
INFO [GossipStage:1] 2011-06-24 16:40:41,947 Gossiper.java (line 606) InetAddress /10.171.47.226 is now UP
INFO [GossipStage:1] 2011-06-24 16:42:09,432 StorageService.java (line 769) Nodes /10.171.47.226 and /10.171.55.77 have the same token 61078635599166706937511052402724559481.  /10.171.47.226 is the new owner
WARN [GossipStage:1] 2011-06-24 16:42:09,432 TokenMetadata.java (line 120) Token 61078635599166706937511052402724559481 changing ownership from /10.171.55.77 to /10.171.47.226


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/11 17:29;brandon.williams;2825-v2.txt;https://issues.apache.org/jira/secure/attachment/12487188/2825-v2.txt","29/Jul/11 18:16;brandon.williams;2825-v3.txt;https://issues.apache.org/jira/secure/attachment/12488234/2825-v3.txt","20/Jul/11 01:17;brandon.williams;2825.txt;https://issues.apache.org/jira/secure/attachment/12487095/2825.txt",,,,,,,,,,,,3.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20852,,,Mon Aug 01 18:16:19 UTC 2011,,,,,,,,,,"0|i0gdov:",93648,,,,,Normal,,,,,,,,,,,,,,,,,"30/Jun/11 17:59;brandon.williams;I'm unable to reproduce.  What I don't understand is how token 61078635599166706937511052402724559481 can be involved if the first node started at 0.  My ring ends up looking like:

{noformat}
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               148820376661671484777635343879628161952     
10.179.65.102   datacenter1 rack1       Up     Normal  315.59 MB       12.53%  0                                           
10.179.64.227   datacenter1 rack1       Up     Normal  157.76 MB       49.97%  85015475407349018891169115382886066923      
10.179.111.137  datacenter1 rack1       Up     Normal  78.99 MB        25.01%  127563185643974719468523634380546988377     
10.179.110.154  datacenter1 rack1       Up     Normal  39.55 MB        12.49%  148820376661671484777635343879628161952 
{noformat};;;","15/Jul/11 23:09;brandon.williams;Can you show the ring at each step when you repro?;;;","18/Jul/11 22:03;mallen;{noformat}

Address         DC          Rack        Status State   Load            Owns    Token                                       
10.171.46.195   datacenter1 rack1       Up     Normal  8.91 KB         100.00% 0   


Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
10.171.46.195   datacenter1 rack1       Up     Normal  8.91 KB         50.00%  0                                           
10.170.121.70   datacenter1 rack1       Up     Normal  4.55 KB         50.00%  85070591730234615865843651857942052864 

Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
10.171.46.195   datacenter1 rack1       Up     Normal  8.91 KB         50.00%  0                                           
10.171.34.247   datacenter1 rack1       Up     Normal  4.55 KB         35.90%  61078635599166706937511052402724559481      
10.170.121.70   datacenter1 rack1       Up     Normal  13.4 KB         14.10%  85070591730234615865843651857942052864 


AddressDC          Rack        Status State   Load            Owns    Token                                       
                                                                               85070591730234615865843651857942052864      
10.171.46.195   datacenter1 rack1       Up     Normal  8.91 KB         50.00%  0                                           
10.171.23.65    datacenter1 rack1       Up     Normal  13.47 KB        35.90%  61078635599166706937511052402724559481      
10.170.121.70   datacenter1 rack1       Up     Normal  13.4 KB         14.10%  85070591730234615865843651857942052864

{noformat};;;","19/Jul/11 22:45;brandon.williams;The difference here is that when I tried to repro, I started with data in the cluster.  Since I never ran cleanup, token 0's range was always being split.  But if you start without any data, for some reason it decides to split 85070591730234615865843651857942052864 twice.  It's not clear why this is happening, since 0's load was still greater at the time, but at least one problem is that we do not assert that the token isn't in use when we determine it.;;;","19/Jul/11 23:11;brandon.williams;Though 85070591730234615865843651857942052864 shows a load of 4.55KB when it is added, 90s later when another node checks it will have 13.4K because it will have updated.  The reason it has more load than the source at this point is because it flushed LocationInfo three times, where the source only flushed it twice.  That should be rare in practice, since typically if a cluster has no data you just start it up without bootstrapping, but still we should protect against handing out tokens that already are in use.;;;","20/Jul/11 01:17;brandon.williams;Patch to exclude the system ks from key sampling when choosing a token, and assert that the token is not our own.

What was happening is that when 0's range was split, it didn't have more than 3 keys, so it used the midpoint.  When 85070591730234615865843651857942052864 split, it did have enough keys (in the system ks) and it sampled 61078635599166706937511052402724559481.  After this, 61078635599166706937511052402724559481 had the highest load and when it sampled it also arrived at 61078635599166706937511052402724559481 since it was using the system ks.

This patches forces using the midpoint method when there is no data other than the system ks, which is the right thing to do since the system ks data is never moved.;;;","20/Jul/11 01:52;jbellis;can we generalize that to ""check that it's not any known endpoint"" and make it a ""real"" (non assert) check?;;;","20/Jul/11 17:29;brandon.williams;v2 detects any token conflicts and throws a proper exception when they occur.;;;","20/Jul/11 17:41;jbellis;let's compare w/ Table.SYSTEM_TABLE, otherwise +1;;;","20/Jul/11 17:53;brandon.williams;Committed w/change to Table.SYSTEM_TABLE comparison.;;;","20/Jul/11 18:18;hudson;Integrated in Cassandra-0.8 #228 (See [https://builds.apache.org/job/Cassandra-0.8/228/])
    Don't sample the system table keys when choosing a bootstrap token.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-2825

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1148866
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
;;;","21/Jul/11 10:04;slebresne;Reopening because the patch broke BootStrapperTest (it somehow hangs forever until junit timeout);;;","21/Jul/11 13:21;brandon.williams;Impressive, I have no idea how this is breaking testTokenRoundtrip():

{noformat}
    public void testTokenRoundtrip() throws Exception
    {  
        StorageService.instance.initServer();
        // fetch a bootstrap token from the local node
        assert BootStrapper.getBootstrapTokenFrom(FBUtilities.getLocalAddress()) != null;
    }
{noformat}

The log just shows a bunch of attempts to connect to the seed (127.0.0.2) which hasn't started yet.;;;","21/Jul/11 15:15;slebresne;I've reverted the patch too so we can do a release of 0.8.2 without having to wait on the unit test fix.;;;","29/Jul/11 18:16;brandon.williams;v3 adds a check that tokenMetadata_.getEndpoint(token) is not null when checking for existing membership.  This fixes the test.;;;","01/Aug/11 17:55;brandon.williams;Committed v3.;;;","01/Aug/11 18:16;hudson;Integrated in Cassandra-0.8 #249 (See [https://builds.apache.org/job/Cassandra-0.8/249/])
    Don't sample the system table keys when choosing a bootstrap token.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-2825

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152876
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assert err on SystemTable.getCurrentLocalNodeId during a cleanup,CASSANDRA-2824,12511491,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,cywjackson,cywjackson,24/Jun/11 16:34,16/Apr/19 09:32,14/Jul/23 05:52,28/Jun/11 07:59,0.8.2,,,,,,0,,,,"when running nodetool cleanup the following happened:

$ ./bin/nodetool cleanup --host localhost
Exception in thread ""main"" java.lang.AssertionError
at org.apache.cassandra.db.SystemTable.getCurrentLocalNodeId(SystemTable.java:383)
at org.apache.cassandra.utils.NodeId$LocalNodeIdHistory.<init>(NodeId.java:179)
at org.apache.cassandra.utils.NodeId.<clinit>(NodeId.java:38)
at org.apache.cassandra.utils.NodeId$OneShotRenewer.<init>(NodeId.java:159)
at org.apache.cassandra.service.StorageService.forceTableCleanup(StorageService.java:1317)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
at sun.rmi.transport.Transport$1.run(Transport.java:159)
at java.security.AccessController.doPrivileged(Native Method)
at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662) 

",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/11 14:30;slebresne;2824.patch;https://issues.apache.org/jira/secure/attachment/12483958/2824.patch","27/Jun/11 17:39;slebresne;2824_v2.patch;https://issues.apache.org/jira/secure/attachment/12483980/2824_v2.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20851,,,Tue Jun 28 08:23:06 UTC 2011,,,,,,,,,,"0|i0gdon:",93647,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Jun/11 14:30;slebresne;There is possibly a race here that triggers the assert. The code is relying on the fact that system tables have a gc_grace of 0 to assume it cannot get tombstone back, but given that gcbefore is at the precision of a second, and given than that on tie with the tombstone timestamp we do include the tombstone, we could get tombstone back.

Attaching patch that takes a safer road.;;;","27/Jun/11 15:03;jbellis;why not just call removeDeleted?;;;","27/Jun/11 17:39;slebresne;You're right, I skipped the fact that gcBefore==0 is different from gc_grace==0. Patch v2 attached.;;;","27/Jun/11 19:31;jbellis;+1 (can you include a link here in the comment when you commit?);;;","28/Jun/11 07:59;slebresne;Committed (with link to the issue in comment), thanks;;;","28/Jun/11 08:23;hudson;Integrated in Cassandra-0.8 #195 (See [https://builds.apache.org/job/Cassandra-0.8/195/])
    Avoids race in SystemTable.getCurrentLocalNodeId
patch by slebresne; reviewed by jbellis for CASSANDRA-2824

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1140472
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE during range slices with rowrepairs,CASSANDRA-2823,12511484,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,terjem,terjem,24/Jun/11 15:39,16/Apr/19 09:32,14/Jul/23 05:52,28/Jun/11 07:55,0.8.2,,,,,,0,,,,"Doing some heavy testing of relatively fast feeding (5000+ mutations/sec) + repair on all node + range slices.
Then occasionally killing a node here and there and restarting it.

Triggers the following NPE
 ERROR [pool-2-thread-3] 2011-06-24 20:56:27,289 Cassandra.java (line 3210) Internal error processing get_range_slices
java.lang.NullPointerException
	at org.apache.cassandra.service.RowRepairResolver.maybeScheduleRepairs(RowRepairResolver.java:109)
	at org.apache.cassandra.service.RangeSliceResponseResolver$2.getReduced(RangeSliceResponseResolver.java:112)
	at org.apache.cassandra.service.RangeSliceResponseResolver$2.getReduced(RangeSliceResponseResolver.java:83)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:161)
	at org.apache.cassandra.utils.MergeIterator.computeNext(MergeIterator.java:88)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.service.RangeSliceResponseResolver.resolve(RangeSliceResponseResolver.java:120)
	at org.apache.cassandra.service.RangeSliceResponseResolver.resolve(RangeSliceResponseResolver.java:43)

Looking at the code in getReduced:

{noformat}
                ColumnFamily resolved = versions.size() > 1
                                      ? RowRepairResolver.resolveSuperset(versions)
                                      : versions.get(0);
{noformat}
seems like resolved becomes null when this happens and versions.size is larger than 1.

RowRepairResolver.resolveSuperset() does actually return null if it cannot resolve anything, so there is definately a case here which can occur and is not handled.

It may also be an interesting question if it is guaranteed that                
versions.add(current.left.cf);
can never return null?

Jonathan suggested on IRC that maybe 
{noformat}
                ColumnFamily resolved = versions.size() > 1
                                      ? RowRepairResolver.resolveSuperset(versions)
                                      : versions.get(0);
                if (resolved == null)
                      return new Row(key, resolved);
{noformat}

could be a fix.
","This is a trunk build with 2521 and 2433
I somewhat doubt that is related however.",terjem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/11 13:54;slebresne;2823.patch;https://issues.apache.org/jira/secure/attachment/12483954/2823.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20850,,,Tue Jun 28 12:43:39 UTC 2011,,,,,,,,,,"0|i0gdof:",93646,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"27/Jun/11 13:54;slebresne;I think the problem is with the call to removeDeleted in resolveSuperset() (which is fairly new). Basically, the code is fine with resolved being null as long as this means that all the versions are null. But the removeDeleted call make it possible to have a null removeDeleted even if the versions are not null, if a row tombstone expires between the time it was returned by the node and the time it is resolved by the coordinator for instance.

Attaching patch that skips the maybeScheduleRepair() call if resolved == null since even in that case there is nothing to repair since the tombstone are now expired.;;;","27/Jun/11 15:04;jbellis;+1;;;","27/Jun/11 15:06;jbellis;although I slightly prefer the ""if == null return"" immediately after initializing resolved, to keep those two pieces of logic together.;;;","27/Jun/11 15:10;slebresne;Yeah, I didn't do that mostly because there is still a few lines of code (besides maybe scheduling repair) that we need to do even if resolved is null (debugging message in RowRepairResolver and more importantly, the clear of versions and versionSources in RangeSliceResolver). ;;;","27/Jun/11 15:14;jbellis;ah, right -- skipping the clear would be buggy.  +1 again. :);;;","28/Jun/11 07:55;slebresne;Committed, thanks;;;","28/Jun/11 08:23;hudson;Integrated in Cassandra-0.8 #195 (See [https://builds.apache.org/job/Cassandra-0.8/195/])
    Fix potential NPE in range slice read repair
patch by slebresne; reviewed by jbellis for CASSANDRA-2823

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1140470
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/RowRepairResolver.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/RangeSliceResponseResolver.java
;;;","28/Jun/11 11:57;jbellis;does 0.7 need this?;;;","28/Jun/11 12:15;slebresne;You're right, 0.7 needs that too. I've committed it there too.;;;","28/Jun/11 12:43;hudson;Integrated in Cassandra-0.7 #514 (See [https://builds.apache.org/job/Cassandra-0.7/514/])
    Fix potential NPE during read repair
patch by slebresne; reviewed by jbellis for CASSANDRA-2823

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1140550
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/RangeSliceResponseResolver.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/RowRepairResolver.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException after upgrade to 0.8.0,CASSANDRA-2822,12511479,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,vilda,vilda,24/Jun/11 14:46,16/Apr/19 09:32,14/Jul/23 05:52,24/Jun/11 16:48,0.8.2,,,,,,0,nullpointerexception,,,"I'm getting NullPointerException on a node upgraded from 0.7 to 0.8.0 (Debian package). The exception is thrown quickly several times after start. Then the Cassandra node is unresponsive. The Stack trace is:

ERROR 14:36:49,712 Fatal exception in thread Thread[WRITE-/10.228.243.191,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)

ERROR 14:36:49,758 Fatal exception in thread Thread[WRITE-/10.227.101.171,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)

ERROR 14:36:49,797 Fatal exception in thread Thread[WRITE-/10.228.243.191,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)

ERROR 14:36:50,756 Fatal exception in thread Thread[WRITE-/10.226.194.239,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)",Debian amd64,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/11 15:18;jbellis;2822.txt;https://issues.apache.org/jira/secure/attachment/12483719/2822.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20849,,,Fri Jun 24 17:19:13 UTC 2011,,,,,,,,,,"0|i0gdo7:",93645,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"24/Jun/11 14:54;jbellis;upgrade to the 0.8 cassandra.yaml;;;","24/Jun/11 14:59;slebresne;I suppose we should add a nice error message though instead of NPE in outboundTcpConnection;;;","24/Jun/11 15:02;jbellis;true;;;","24/Jun/11 16:08;slebresne;+1;;;","24/Jun/11 16:48;jbellis;committed;;;","24/Jun/11 17:19;hudson;Integrated in Cassandra-0.8 #192 (See [https://builds.apache.org/job/Cassandra-0.8/192/])
    tolerate missing encryption options
patch by jbellis; reviewed by slebresne for CASSANDRA-2822

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1139383
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/OutboundTcpConnection.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI remove ascii column,CASSANDRA-2821,12511478,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,sdolgy,sdolgy,24/Jun/11 14:44,16/Apr/19 09:32,14/Jul/23 05:52,15/Jul/11 17:34,0.7.8,0.8.2,,Legacy/Tools,,,0,,,,"[default@sdo] incr counters[ascii('EU')][ascii('null')];
Value incremented.
[default@sdo] list counters;
Using default limit of 100
-------------------
RowKey: 4555
=> (counter=6e756c6c, value=1)

1 Row Returned.
[default@sdo] del counters[ascii('EU')][ascii('null')];
org.apache.cassandra.db.marshal.MarshalException: cannot parse
'FUNCTION_CALL' as hex bytes
[default@sdo]

Suggested workaround, although not tested:

assume counters comparator as bytes;
del counters['EU'][0];",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/11 14:19;xedin;CASSANDRA-2821-0.7.patch;https://issues.apache.org/jira/secure/attachment/12486622/CASSANDRA-2821-0.7.patch","15/Jul/11 14:19;xedin;CASSANDRA-2821-0.8.patch;https://issues.apache.org/jira/secure/attachment/12486621/CASSANDRA-2821-0.8.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20848,,,Fri Jul 15 21:38:25 UTC 2011,,,,,,,,,,"0|i0gdnz:",93644,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"24/Jun/11 14:49;jbellis;Edit to clarify there is nothing magic about string 'null'.

Probably affects 0.7 too?;;;","15/Jul/11 14:19;xedin;Proper function support and key validation (0.8 only) for DEL command.

Both rebased with latest versions of their branches.

;;;","15/Jul/11 17:34;brandon.williams;Committed.;;;","15/Jul/11 20:32;hudson;Integrated in Cassandra-0.7 #529 (See [https://builds.apache.org/job/Cassandra-0.7/529/])
    Fix column deletion in the cli.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2821

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147258
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/branches/cassandra-0.7/test/unit/org/apache/cassandra/cli/CliTest.java
;;;","15/Jul/11 21:38;hudson;Integrated in Cassandra-0.8 #216 (See [https://builds.apache.org/job/Cassandra-0.8/216/])
    Proper function support and key validation for cli deletes.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2821

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147259
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/cli/CliTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0.8.0 is unable to participate with nodes using a _newer_ protocol version,CASSANDRA-2818,12511410,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,mallen,mallen,23/Jun/11 21:22,16/Apr/19 09:32,14/Jul/23 05:52,28/Jun/11 17:50,0.8.2,,,,,,0,,,,"When a 0.8.1 node tries to join a 0.8.0 ring, we see an endless supply of these in system.log:

INFO [Thread-4] 2011-06-23 21:14:04,149 IncomingTcpConnection.java (line 103) Received connection from newer protocol version. Ignorning message.

and the node never joins the ring.",,skamio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/11 16:42;jbellis;2818-disconnect.txt;https://issues.apache.org/jira/secure/attachment/12483733/2818-disconnect.txt","24/Jun/11 20:20;jbellis;2818-v2.txt;https://issues.apache.org/jira/secure/attachment/12483764/2818-v2.txt","27/Jun/11 20:40;brandon.williams;2818-v3.txt;https://issues.apache.org/jira/secure/attachment/12484006/2818-v3.txt","27/Jun/11 22:32;jbellis;2818-v4.txt;https://issues.apache.org/jira/secure/attachment/12484061/2818-v4.txt","24/Jun/11 19:23;brandon.williams;2818.txt;https://issues.apache.org/jira/secure/attachment/12483759/2818.txt",,,,,,,,,,5.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20847,,,Tue Jun 28 21:27:07 UTC 2011,,,,,,,,,,"0|i0gdnb:",93641,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"24/Jun/11 13:53;jbellis;Not sure what's going on. Here's what's supposed to happen:

If new node N is contacted first by old node M, N records the version of M and generates messages at that version. M never knows that N is actually newer.

If M is contacted first, we expect to see the above message a few times, but M adds N to its gossip list after the first time. Once N gets a gossip from M, it will know to use M's version when creating messages.

I don't see anything obviously wrong with this code. :(;;;","24/Jun/11 16:34;jbellis;It looks like this is a problem in 0.7 too, but you can avoid it if you happen to upgrade a seed node first.;;;","24/Jun/11 16:42;jbellis;Patch for part one of the problem: actually disconnect when we can't handle a new version, so the other end will retry.;;;","24/Jun/11 16:45;jbellis;The breakdown in how it's supposed to work is, Gossiper.setVersion does not actually add it to the set of nodes-to-contact (liveEndpoints and unreachableEndpoints).  We can't fix it directly by simply adding to liveEndpoints either, because Gossiper assumes that if we know about the node, we also know about its state (e.g. rack and DC information).;;;","24/Jun/11 19:23;brandon.williams;In 0.7, we did actually add the node to the endpoint state map by calling addSavedEndpoint.  I removed this in CASSANDRA-2092, probably because it makes the log message somewhat incorrect (""XXX has restarted, now UP again"") but if it was good enough for 0.7, I think it's good enough for 0.8. Note that even without the disconnect 0.7->0.8 works, but the disconnect is an optimization.  Protection from DC/RACK NPEs is guaranteed by addSavedEndpoint initially marking the node as down, so there's no reason to query the state information (other things that utilize getNaturalEndpoints may NPE like nodetool ring, but it's a short window to exploit and non-critical.)  Patch to restore the previous behavior to 0.8.;;;","24/Jun/11 20:20;jbellis;v2 incorporates the disconnect patch for 0.8 and removes a redundant endpointstate lookup.;;;","27/Jun/11 20:40;brandon.williams;v2 has two problems:

* It shuts the connection down slightly too aggressively, causing an exception on the remote side before setVersion gets called.

* It stores the remote's version even when it is greater, causing the lower version node to always report itself as the newer version to the newer node.

v3 address the first problem by sleeping for a half second before closing, and addresses the second by only calling setVersion if the remote side is compatible, otherwise it calls addSavedEndpoint before disconnecting so that it will reconnect.;;;","27/Jun/11 21:34;jbellis;bq. It shuts the connection down slightly too aggressively, causing an exception on the remote side before setVersion gets called

I can see that the initiating side could get pissed that the target closes the socket uncleanly -- what I don't get is how a sleep could make a difference.  Is it on the reconnect?  In which case the sleep is going to be fragile with a bigger cluster, since we depend on gossip to spread the version info.

Do you have a sample stacktrace?;;;","27/Jun/11 22:03;brandon.williams;This repeats infinitely:

{noformat}
TRACE 22:02:38,187 cassandra-2/10.179.64.227 sending GOSSIP_DIGEST_SYN to 9@/10.179.65.102
DEBUG 22:02:38,188 error writing to /10.179.65.102
java.net.SocketException: Connection reset
        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)
        at java.io.DataOutputStream.flush(DataOutputStream.java:106)
        at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:114)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:90)
{noformat};;;","27/Jun/11 22:32;jbellis;That makes sense for v2, yeah.

I realized that we don't actually need to reconnect to send old-version messages -- version is per-Message, the connection itself is basically just a queue around a socket.

v4 attached that doesn't drop (non-streaming) connections at all.  (This is part of the ""how did it possibly work on 0.7?"" answer, I think.);;;","27/Jun/11 22:33;jbellis;v4 also changes the current-version to 3, so we don't create a version exhaustion problem for ourselves (see comment in MS).;;;","27/Jun/11 23:19;brandon.williams;+1;;;","28/Jun/11 17:50;jbellis;committed;;;","28/Jun/11 21:27;hudson;Integrated in Cassandra-0.8 #197 (See [https://builds.apache.org/job/Cassandra-0.8/197/])
    fix Message version propagation fromold nodes to new ones
patch by brandonwilliams and jbellis for CASSANDRA-2818

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1140751
Files : 
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/InvalidRequestException.java
* /cassandra/branches/cassandra-0.8/contrib
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/IncomingTcpConnection.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SuperColumn.java
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/NotFoundException.java
* /cassandra/branches/cassandra-0.8
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Column.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose number of threads blocked on submitting a memtable for flush,CASSANDRA-2817,12511377,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,23/Jun/11 16:11,16/Apr/19 09:32,14/Jul/23 05:52,23/Jun/11 17:17,0.7.7,0.8.2,,,,,0,,,,"Writes can be blocked by a thread trying to submit a memtable while the flush queue is full. While this is the expected behavior (the goal being to prevent OOMing), it is worth exposing when that happens so that people can monitor it and modify settings accordingly if that happens too often.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/11 17:01;slebresne;0001-Expose-threads-blocked-on-submission-to-executor-v2.patch;https://issues.apache.org/jira/secure/attachment/12483620/0001-Expose-threads-blocked-on-submission-to-executor-v2.patch","23/Jun/11 16:12;slebresne;0001-Expose-threads-blocked-on-submission-to-executor.patch;https://issues.apache.org/jira/secure/attachment/12483615/0001-Expose-threads-blocked-on-submission-to-executor.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20846,,,Thu Jun 23 17:28:45 UTC 2011,,,,,,,,,,"0|i0gdn3:",93640,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"23/Jun/11 16:18;kingryan;+1;;;","23/Jun/11 16:18;jbellis;Very clever solution, I like it.

Can you add ""currently blocked"" to statuslogger and nodetool tpstats?;;;","23/Jun/11 17:01;slebresne;Attaching v2 with support in StatusLogger and tpstats.;;;","23/Jun/11 17:03;jbellis;+1;;;","23/Jun/11 17:17;slebresne;Committed, thanks;;;","23/Jun/11 17:28;hudson;Integrated in Cassandra-0.7 #510 (See [https://builds.apache.org/job/Cassandra-0.7/510/])
    Expose number of threads blocked on submitting a memtable for flush
patch by slebresne; reviewed by jbellis for CASSANDRA-2817

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1138996
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/concurrent/JMXEnabledThreadPoolExecutorMBean.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/utils/StatusLogger.java
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/concurrent/JMXEnabledThreadPoolExecutor.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/tools/NodeProbe.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/tools/NodeCmd.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair doesn't synchronize merkle tree creation properly,CASSANDRA-2816,12511343,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,23/Jun/11 11:35,16/Apr/19 09:32,14/Jul/23 05:52,21/Jul/11 11:13,0.8.2,,,,,,3,repair,,,"Being a little slow, I just realized after having opened CASSANDRA-2811 and CASSANDRA-2815 that there is a more general problem with repair.

When a repair is started, it will send a number of merkle tree to its neighbor as well as himself and assume for correction that the building of those trees will be started on every node roughly at the same time (if not, we end up comparing data snapshot at different time and will thus mistakenly repair a lot of useless data). This is bogus for many reasons:
* Because validation compaction runs on the same executor that other compaction, the start of the validation on the different node is subject to other compactions. 0.8 mitigates this in a way by being multi-threaded (and thus there is less change to be blocked a long time by a long running compaction), but the compaction executor being bounded, its still a problem)
* if you run a nodetool repair without arguments, it will repair every CFs. As a consequence it will generate lots of merkle tree requests and all of those requests will be issued at the same time. Because even in 0.8 the compaction executor is bounded, some of those validations will end up being queued behind the first ones. Even assuming that the different validation are submitted in the same order on each node (which isn't guaranteed either), there is no guarantee that on all nodes, the first validation will take the same time, hence desynchronizing the queued ones.

Overall, it is important for the precision of repair that for a given CF and range (which is the unit at which trees are computed), we make sure that all node will start the validation at the same time (or, since we can't do magic, as close as possible).

One (reasonably simple) proposition to fix this would be to have repair schedule validation compactions across nodes one by one (i.e, one CF/range at a time), waiting for all nodes to return their tree before submitting the next request. Then on each node, we should make sure that the node will start the validation compaction as soon as requested. For that, we probably want to have a specific executor for validation compaction and:
* either we fail the whole repair whenever one node is not able to execute the validation compaction right away (because no thread are available right away).
* we simply tell the user that if he start too many repairs in parallel, he may start seeing some of those repairing more data than it should.
",,colinkuo,scode,stuhood,terjem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/11 14:05;slebresne;0001-Schedule-merkle-tree-request-one-by-one.patch;https://issues.apache.org/jira/secure/attachment/12483711/0001-Schedule-merkle-tree-request-one-by-one.patch","18/Jul/11 14:26;jbellis;2816-v2.txt;https://issues.apache.org/jira/secure/attachment/12486871/2816-v2.txt","20/Jul/11 17:11;jbellis;2816-v4.txt;https://issues.apache.org/jira/secure/attachment/12487185/2816-v4.txt","20/Jul/11 18:08;jbellis;2816-v5.txt;https://issues.apache.org/jira/secure/attachment/12487191/2816-v5.txt","20/Jul/11 10:05;slebresne;2816_0.8_v3.patch;https://issues.apache.org/jira/secure/attachment/12487133/2816_0.8_v3.patch",,,,,,,,,,5.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20845,,,Thu Jul 21 12:31:54 UTC 2011,,,,,,,,,,"0|i0gdmv:",93639,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"23/Jun/11 12:40;jbellis;I guess a dedicated validation executor is ok as long as it still obeys the global ""compaction"" i/o limit.;;;","24/Jun/11 04:27;stuhood;I'm a fan of the snapshotting immediately after receiving the request approach. In general, polishing our snapshot support to allow for this kind of usecase is likely to open up other interesting possibilities.;;;","24/Jun/11 04:45;jbellis;Supporting actual live-reading of snapshotted sstables is a little more than ""polishing."" It would be cool, but I wouldn't want it to block fixing repair.;;;","24/Jun/11 07:41;scode;I've thought about this problem too, and it is really significant for some use-cases. Again because so few writes are needed in order to trigger large amounts of data being sent given the merklee tree granularity.

While I'm all for a fixing it by e.g. more immediate snapshotting, I would like to raise the issue that repairs overall have pretty significant side-effects; particularly ones that can self-magnify and cause further problems. Beyond the obvious ""it does disk I/O"" and ""It uses CPU"", we have:

* Over-repair due to merklee tree granularity can cause jumps in CF sizes, killing cache locality
* Combine that with concurrent repairs then repairing the ""size-jumped"" set of sstables and you can magnify that effect on other nodes causing huge size increases.
* Up to recently, mixing large and small cf:s was a significant problem if you wanted to have different frequencies and different gc grace times, due to one repair blocking on another. But fixes to this and the other JIRA about concurrency, might disable the ""fix"" for that that was concurrent compaction - so back to square one.

I guess overall, it seems very easy to shoot yourself in the foot with repair.

Any opinions on CASSANDRA-2699 for longer term changes to repair?

;;;","24/Jun/11 07:55;slebresne;I'm not sure what you mean by ""snapshotting immediately"" or ""polishing our snapshot support"", but one approach that I think is equivalent to that (or maybe that is what you meant by 'snapshotting') would be to grab references to the sstables at the very beginning for each request and use those all throughout the repair. This has however a problem: this means we retain sstables from being deleted during repair, including sstables that are compacted in the meantime. Because repair can take a while, this will be bad. This will also require changes to the wire protocol (because we'll need a way to indicate during streaming the set of sstables to consider), and since we've kind of decided to not do that in minor releases (at least until we've discussed that), this means this cannot be released quickly. Which is bad, because I'm pretty sure this is a good part of the reason why some people with big data sets have had huge pain with repair.

Scheduling the validation one by one avoids those problems. In theory this means we'll do less work in parallel, but in practice I doubt this is a big since the goal is probably to have repair have less impact on the node rather than more. It will also make this more easy to reason about.;;;","24/Jun/11 14:05;slebresne;Attaching patch against 0.8. The patch implements the idea of scheduling the merkle tree requests one by one, to make sure the tree are started as close as possible of ""the same time"". This also put validation compaction in their own executor (to avoid them to be queued up behind standard compactions). That specific executor is created with 2 core threads, to allow for Peter's use case of wanting to do multiple repairs at the same time. That is, by default, you can do 2 repairs involving the same node and be ok. More and you may experience crappy precision in repair. The new concurrent_validators parameter is exposed in case some would want more that 2. That being said, regular compactions and validations are not separated for everything and in particular throttling is shared.

As far as I can test, this successfully fixes the problems from CASSANDRA-2811 and CASSANDRA-2815. This also don't change anything on the on-wire protocol side, so I think we can target that for 0.8.2.

;;;","25/Jun/11 09:03;terjem;This sounds very interesting.

We have also spotted very noticable issues with full GCs when the merkle trees are passed around. Hopefully this could fix that too.

I will see if I can get this patch tested somewhere if it is ready for that.

On a side topic, given the importance of getting tombstones properly synchronized within GCGraceSeconds, would it be an potential interesting idea to separate tombstones in different sstables to reduce the need to scan the whole dataset very frequently in the first place?

Another thought may be to make compaction deterministic or synchronized by a master across nodes so for older data, all we needed was to compare pre-stored md5s of how whole sstables? 

That is, while keeping the masterless design for updates, we could consider a master based design for how older data is being organized by the compactor. so it would be much easier to verify that ""old"" data is the same without any large regular scans and that data is really the same after big compactions etc.
;;;","27/Jun/11 09:24;slebresne;bq. We have also spotted very noticable issues with full GCs when the merkle trees are passed around. Hopefully this could fix that too.

This do make sure that we don't do multiple validation at the same time and that we keep a small number of merkle tree in memory at the same time. So I suppose this could help on the GC side. But overall I don't know if I am too optimistic about that, in part because I'm not sure what causes your issues. But this can't hurt on that side at least.

bq. I will see if I can get this patch tested somewhere if it is ready for that.

I believe it should be ready for that.

bq. would it be an potential interesting idea to separate tombstones in different sstables.

The thing is that some tombstones may be irrelevant become some update supersedes it (this is specially true of row tombstones). Hence basing a repair on tombstone only may transfer irrelevant data. I suppose it may depend on the use case this will be more or less a big deal. Also, this means that a read will be impacted in that we will often have to hit twice as many sstables. Given that it's not a crazy idea either to want to repair data regularly (if only for durability guarantee), I don't know if it is worth the trouble (we would have to separate tombstones from data at flush time, we'll have to maintain the two separate set of data/tombstone sstables, etc...).

bq. make compaction deterministic or synchronized by a master across nodes

Pretty sure we want to avoid going to a master architecture for everything if we can. Having master means that failure handling is more difficult (think network partition for instance) and require leader election and such, and the whole point of the fully distribution of Cassandra is to avoid those. Even without consider those, synchronizing compaction means synchronizing flush somehow and you want to be precise if you're going to use whole sstable md5s, which will be hard and quite probably inefficient.;;;","27/Jun/11 10:36;terjem;I don't know what causes GC when doing repairs either, but fire off repair on a few nodes with 100 million docs/node and there is a reasonable chance that a node here and there will log messages about reducing cache sizes due to memory pressure (I am not really sure it is a good idea to do this at all, reducing caches during stress rarely improves anything) or full GC.

The thought about the master controlled compaction would not really affect network splits etc.

Reconciliation after a network split is really as complex with or without a master. We need to get back to a state where all the nodes have the same data anyway which is a complex task anyway.

This is more a consideration of the fact that we do not necessarily need to live in quorum based world during compaction and we are free to use alternative approaches in the compaction without changing read/write path or affecting availability. Master selection is not really a problem here. Start compaction, talk to other nodes with the same token ranges, select a leader. 

Does not even have to be the same master every time and could consider if we could make compaction part of a background read repair to reduce the amount of times we need to read/write data. 

For instance, if we can verify that the oldest/biggest sstables is 100% in sync with data on other replicas when it is compacted (why not do it during compaction when we go through the data anyway rather than later?),can we use that info to optimize the scans done during repairs by only using data in sstables with data received after some checkpoint in time as the starting point for the consistency check?;;;","27/Jun/11 12:21;jbellis;bq. I am not really sure it is a good idea to do this at all, reducing caches during stress rarely improves anything

(This is on by default because the most common cause of OOMing is people configuring their caches too large.)

It sounds odd to me that repair would balloon memory usage dramatically.  Do you have monitoring graphs that show the difference in heap usage between ""normal"" and ""repair in progress?"";;;","29/Jun/11 16:42;terjem;This is what heap looks like when GC start slowing things down so much that even gossip gets delayed long enough for nodes to be down for some seconds.

  num     #instances         #bytes  class name
----------------------------------------------
   1:       9453188      453753024  java.nio.HeapByteBuffer
   2:      10081546      392167064  [B
   3:       7616875      243740000  org.apache.cassandra.db.Column
   4:       9739914      233757936  java.util.concurrent.ConcurrentSkipListMap$Node
   5:       4131938       99166512  java.util.concurrent.ConcurrentSkipListMap$Index
   6:       1549230       49575360  org.apache.cassandra.db.DeletedColumn

I guess this really ends up maybe being the mix of everything going on in total and all the reading and writing that may occur when repair runs (valiadation compactions, streaming, normal compactions and regular traffic all at the same time and maybe many CFs at the same time).

However, I have suspected for some time that our young size was a bit on the small side and after increasing it and giving the heap a few more GB to work with, it seems like things are behaving quite a bit better.

I mentioned issues with this patch when testing for CASSANDRA-2521. That was a problem caused by me. Was playing around with git for the first time and I manage to apply 2816 to a different branch than the one I used for testing.... :(

My appologies. 

Initial testing with that corrected looks a lot better for my small scale test case, but I noticed one time where I deleted an sstable and restarted. It did not get repaired (repair scanned but did nothing).

Not entirely sure what to make out of that, I then tested to delete another sstable and repair started running.

I will test more over the next days. 
;;;","04/Jul/11 14:02;terjem;Things definitely seems to be improved overall, but weird things still happens.

So... 12 node cluster, this is maybe ugly, I know, but start repair on all of them.
Most nodes are fine, but one goes crazy. Disk use is now 3-4 times what it was before the repair started, and it does not seem to be done yet.

I have really no idea if this is the case, but I am getting the hunch that this node has ended up streaming out some of the data it is getting in. Would this be possible?
;;;","04/Jul/11 14:30;slebresne;bq. So... 12 node cluster, this is maybe ugly, I know, but start repair on all of them.

Is it started on all of them ? If so, this is ""kind of"" expected in the sense that the patch assumes that each node does not do more than 2 repairs (for any column family) at the same time (this is configurable through the new concurrent_validators option, but it's probably better to stick to 2 and stagger the repair). If you do more than that (that is, if you did repair on all node at the same time and RF>2), then we're back on our old demons.

bq. I have really no idea if this is the case, but I am getting the hunch that this node has ended up streaming out some of the data it is getting in. Would this be possible?

Not really. That is, it could be that you create a merkle tree on some data and once you start streaming you, you're picking up data that was just streamed to you and wasn't there when computing the tree. This patch is suppose to fixes this in parts, but this can still happen if you do repairs in parallel on neighboring nodes. However, you shouldn't get into a situation where 2 nodes stream forever because they pick up what is just streamed to them for instance, because what is streaming is determined at the very beginning of the streaming session.

So my first question would be, was all those repair started in parallel. If yes, you shall not do this :). CASSANDRA-2606 and CASSANDRA-2610 are here to help making the repair of a full cluster much easier (and efficient), but right now it's more about getting patch in one at a time.
If the repairs were started one at a time in a rolling fashion, then we do have a unknown problem somewhere.;;;","04/Jul/11 15:37;terjem;Cool!

Then you confirmed what I have sort of believed for a while, but my understanding of code has been a bit in conflict with:
http://wiki.apache.org/cassandra/Operations
which says:
""It is safe to run repair against multiple machines at the same time, but to minimize the impact on your application workload it is recommended to wait for it to complete on one node before invoking it against the next.""

I have always read that as ""if you have the HW, go for it!""

May I change to:
""It is safe to run repair against multiple machines at the same time. However, to minimize the amount of data transferred during a repair, careful synchronization is required between the nodes taking part of the repair. 

This is difficult to do if nodes with the same data replicas runs repair at the same time and doing so can in extreme cases generate excessive transfers of data. 

Improvements is being worked on, but for now, avoid scheduling repair on several nodes with replicas of the same data at the same time.""

;;;","04/Jul/11 23:18;terjem;Regardless of change of documentation however, I don't think it should be possible to actually trigger a scenario like this in the first place.

The system should protect the user from that.

I also noticed that in this case, we have RF3. The node which is going somewhat crazy is number ""6"", however during the repair, it does log that it talks compares and streams data with node 4, 5, 7 and 8.

Seems like a couple of nodes too many?;;;","04/Jul/11 23:18;terjem;Regardless of change of documentation however, I don't think it should be possible to actually trigger a scenario like this in the first place.

The system should protect the user from that.

I also noticed that in this case, we have RF3. The node which is going somewhat crazy is number ""6"", however during the repair, it does log that it talks compares and streams data with node 4, 5, 7 and 8.

Seems like a couple of nodes too many?;;;","05/Jul/11 00:05;jbellis;bq. May I change to

Sure.

bq. The system should protect the user from that

I'm not sure that in a p2p design we can posit an omniscient ""the system."";;;","05/Jul/11 01:23;terjem;bq.I'm not sure that in a p2p design we can posit an omniscient ""the system.""

Is that a philosophical statement? :)

As Cassandra, at least for now, is a p2p network with fairly clearly defined boundaries, I will continue calling it a ""system"" for now :)

However, looking at it from the p2p viewpoint, the user potentially have no clue about where replicas are stored and given this, it may be impossible for the user to issue repair manually on more than one node at a time without getting in trouble. Given a large enough p2p setup, it would also be non-trivial to actually schedule a complete repair without ending up with 2 or more repairs running on the same replica set.

Since Cassandra do no checkpoint the synchronization so it is forced to rescan everything on every repair, repairs easily take so long that you are forced to run it on several nodes at a time if you are going to manage to finish repairing all nodes in 10 days...

Anyway, this is way outside the scope of this jira :);;;","05/Jul/11 02:07;terjem;bq. I also noticed that in this case, we have RF3. The node which is going somewhat crazy is number ""6"", however during the repair, it does log that it talks compares and streams data with node 4, 5, 7 and 8.

This is maybe correct. Node 7 will replicate to node 6 and 8 so 6 and 8 would share data.

So, to make things safe, even with this patch, every 4th node can run repair at the same time if RF=3?, but you still need to run repair on each of those 4 nodes to make sure it is all repaired?

As for the comment I made earlier.

To me, it looks like if the repair start triggering transfers on a large scale, the file the node get streamed in will not be streamed out, but this may get compacted before the repair finished and the compacted file I suspect gets streamed out and the repair just never finishe;;;","05/Jul/11 15:42;jbellis;bq. The patch implements the idea of scheduling the merkle tree requests one by one, to make sure the tree are started as close as possible of ""the same time"". 

Can you point out where this happens in AES?

bq. This also put validation compaction in their own executor (to avoid them to be queued up behind standard compactions). That specific executor is created with 2 core threads, to allow for Peter's use case of wanting to do multiple repairs at the same time. That is, by default, you can do 2 repairs involving the same node and be ok

That feels like the wrong default to me.  I think you can make a case for one (minimal interference with the rest of the system) or unlimited (no weird ""cliff"" to catch the unwary repair operator).  But two is weird. :);;;","05/Jul/11 16:32;slebresne;bq. Can you point out where this happens in AES?

Mostly in AES.rendezvous and AES.RepairSession. Basically, RepairSession creates a queue of jobs, a job representing the repair of a given column family (for a given range, but that comes from the session itself). AES.rendezvous is then call for each received merkleTree. It waits to have all the merkeTree for the first job in the queue. When that is done, it dequeue the job (computing the merkle tree differences and scheduling streaming accordingly) and send the tree request for the next job in the queue.
Moreover, in StorageService.forceTableRepair(), when scheduling the repair for all the ranges of the node, we actually start the session for the first range and wait for all the ""jobs"" for this range to be done before starting the next session.

bq. That feels like the wrong default to me. I think you can make a case for one (minimal interference with the rest of the system) or unlimited (no weird ""cliff"" to catch the unwary repair operator). But two is weird.

Well the rational was the following one: if you set it to two, then you're saying that as soon as you start 2 repairs in parallel, they will start being inaccurate. But as Peter was suggesting (maybe in another ticket but anyway), if you have huge CF and tiny ones, it's nice to be able to repair on the tiny ones while a repair on the huge one(s) is running. Now, making it unlimited feels dangerous, because if you do so, it means that if the use start a lot of repair, all the validation compaction will start right away. This will kill the cluster (at least a few nodes if all those repair were started on the same node). It sounded better to have degraded precision for repair in those cases rather than basically killing the nodes. Maybe 2 or 4 may be a better default than 2, but 1 is a bit limited and unlimited is clearly much too dangerous.;;;","13/Jul/11 21:30;jbellis;bq. making it unlimited feels dangerous, because if you do so, it means that if the use start a lot of repair, all the validation compaction will start right away

But the easy solution is ""don't do that.""

By setting a finite number greater than one, you have to restart machines when you realize ""oh, I want to have 3 simultaneous now.""

I'd rather keep it simple: make it unbounded, no configuration settings.  If you ignore the instructions to only run one repair at once, then either you know what you're doing (maybe you have SSDs) or you will find out very quickly and never do it again. :);;;","13/Jul/11 21:57;scode;I'm kinda +1 on the simple version w/o bounds but not too fussy since I can obviously set it very high for my use case. In any case, the most important part for mixed small/large type of situation is that concurrent repair is possible, even if configuration changes are needed.
;;;","18/Jul/11 13:09;jbellis;bq. I'm kinda +1 on the simple version w/o bounds

Me too.  +1 with that change.;;;","18/Jul/11 13:24;jbellis;(I'll go ahead and submit a version with that change.);;;","18/Jul/11 14:26;jbellis;rebased and switched to unbounded executor for validations.

tests do not compile but I believe that was already the case w/ v1 -- not sure what to do with blockUntilRunning, which was removed.;;;","20/Jul/11 10:05;slebresne;bq. rebased

You rebased it against trunk while the fix version is still 0.8.2. I agree that this feel a bigger change that what we would want for a minor release, but repair is really a major pain point for users. And to tell the truth, it's worth in 0.8 than it is in 0.7 because even though the problem this patch solves exists in 0.7 as well, the splitting into range of repair made for 0.8 exacerbate those problems. Moreover this patch is fairly well delimited in what it changes, and it don't make any change to the wire protocol or anything that would make upgrades a problem. So I'm actually in favor of taking the small risk of pushing that in 0.8.2 (and be very vigilant to test repair extensively before the release). So for now, attaching a rebase with tests fixed against 0.8.

bq. and switched to unbounded executor for validations.

Ok, I realize that I'm not sure I understand what you mean by unbounded executor. In you rebased version, the ValidationExecutor apparently use the first constructor of DebuggableThreadPoolExecutor that will construct a mono-threaded executor, which is not what we want. Sure the queue of the executor will be unbounded, but if that was the problem, there was a misunderstanding, because the queue has always been unbounded, even in my initial patch. What concurrent_validators was dealing with is the number of core threads. And we need multiple core threads if we want to allow multiple concurrent repairs to work correctly.

Now the idea behind a default of 2 for the core threads was because I see a reason to want 2 concurrent repairs, but I don't see a very good reason to want more (and it's configurable if someone really need more). I'm glad to say it's not a marvelous default and the patch I've just attached used the same default than concurrent_compactors which is maybe less ""random"". Now we could have an executor with unbounded threads, that spawn a thread if needed making sure we never queue validation compaction, but that seems a little bit dangerous to me. It seems more reasonable to me to have a (configurable) reasonable number of threads and let validation queue up if the user start more than that number of concurrent repair (which will impact the precision of those repair, but it would be the user fault and it's a better way to deal with such fault than starting an unreasonable number of validation compaction that will starve memory (on likely more than one node btw)). But if you still think that it's better to have an unbounded number of threads, I won't fight over this.

bq. tests do not compile but I believe that was already the case w/ v1

Yes, I completely forgot to update the unit tests, sorry. Attached patch fixes those.
;;;","20/Jul/11 17:11;jbellis;v4 attached against 0.8 with a corrected uncapped validation executor.;;;","20/Jul/11 17:45;slebresne;I think that if we don't want validation executor of v4 to ever queue tasks (which is what we need), then we need the executor queue to be a bounded queue of size 0 (i.e. that doesn't accept element). Indeed, as per the documentation of ThreadPoolExecutor:
{noformat}
If corePoolSize or more threads are running, the Executor always prefers queuing a request rather than adding a new thread.
{noformat} ;;;","20/Jul/11 18:08;jbellis;You're right.  v5 attached.;;;","21/Jul/11 11:12;slebresne;Alright, v5 looks good to me. Committed, thanks.;;;","21/Jul/11 12:31;hudson;Integrated in Cassandra-0.8 #231 (See [https://builds.apache.org/job/Cassandra-0.8/231/])
    Properly synchronize merkle tree computation
patch by slebresne; reviewed by jbellis for CASSANDRA-2816

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1149121
Files : 
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/service/AntiEntropyServiceTestAbstract.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AntiEntropyService.java
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/conf/cassandra.yaml
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"RuntimeException in Pig when using ""dump"" command on column name",CASSANDRA-2810,12511243,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,silvere,silvere,22/Jun/11 14:49,16/Apr/19 09:32,14/Jul/23 05:52,28/Sep/11 22:01,0.8.7,,,,,,1,,,,"This bug was previously report on [Brisk bug tracker|https://datastax.jira.com/browse/BRISK-232].

In cassandra-cli:
{code}
[default@unknown] create keyspace Test
    with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy'
    and strategy_options = [{replication_factor:1}];

[default@unknown] use Test;
Authenticated to keyspace: Test

[default@Test] create column family test;

[default@Test] set test[ascii('row1')][long(1)]=integer(35);
set test[ascii('row1')][long(2)]=integer(36);
set test[ascii('row1')][long(3)]=integer(38);
set test[ascii('row2')][long(1)]=integer(45);
set test[ascii('row2')][long(2)]=integer(42);
set test[ascii('row2')][long(3)]=integer(33);

[default@Test] list test;
Using default limit of 100
-------------------
RowKey: 726f7731
=> (column=0000000000000001, value=35, timestamp=1308744931122000)
=> (column=0000000000000002, value=36, timestamp=1308744931124000)
=> (column=0000000000000003, value=38, timestamp=1308744931125000)
-------------------
RowKey: 726f7732
=> (column=0000000000000001, value=45, timestamp=1308744931127000)
=> (column=0000000000000002, value=42, timestamp=1308744931128000)
=> (column=0000000000000003, value=33, timestamp=1308744932722000)

2 Rows Returned.

[default@Test] describe keyspace;
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
{code}
In Pig command line:
{code}
grunt> test = LOAD 'cassandra://Test/test' USING CassandraStorage() AS (rowkey:chararray, columns: bag {T: (name:long, value:int)});

grunt> value_test = foreach test generate rowkey, columns.name, columns.value;

grunt> dump value_test;
{code}
In /var/log/cassandra/system.log, I have severals time this exception:
{code}
INFO [IPC Server handler 3 on 8012] 2011-06-22 15:03:28,533 TaskInProgress.java (line 551) Error from attempt_201106210955_0051_m_000000_3: java.lang.RuntimeException: Unexpected data type -1 found in stream.
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:478)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeBag(BinInterSedes.java:522)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:361)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:357)
	at org.apache.pig.impl.io.InterRecordWriter.write(InterRecordWriter.java:73)
	at org.apache.pig.impl.io.InterStorage.putNext(InterStorage.java:87)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:138)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:97)
	at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:638)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.collect(PigMapOnly.java:48)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:239)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:232)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:763)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:369)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:259)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.mapred.Child.main(Child.java:253)
{code}
and the request failed.

{code}
grunt> test = LOAD 'cassandra://Test/test' USING CassandraStorage() AS (rowkey:chararray, columns: bag {T: (name:long, value:int)});

grunt> value_test = foreach test generate rowkey, columns.value;

grunt> dump value_test;
{code}

This time, without the column name, it's work (but the value are displayed as char instead of integer). Result:
{code}
(row1,{(#),($),(&)})
(row2,{(-),(*),(!)})
{code}

Now we do the same test but we set a comparator to the CF.
{code}
[default@Test] create column family test with comparator = 'LongType';

[default@Test] set test[ascii('row1')][long(1)]=integer(35);
set test[ascii('row1')][long(2)]=integer(36);
set test[ascii('row1')][long(3)]=integer(38);
set test[ascii('row2')][long(1)]=integer(45);
set test[ascii('row2')][long(2)]=integer(42);
set test[ascii('row2')][long(3)]=integer(33);

[default@Test] list test;
Using default limit of 100
-------------------
RowKey: 726f7731
=> (column=1, value=35, timestamp=1308748643506000)
=> (column=2, value=36, timestamp=1308748643508000)
=> (column=3, value=38, timestamp=1308748643509000)
-------------------
RowKey: 726f7732
=> (column=1, value=45, timestamp=1308748643510000)
=> (column=2, value=42, timestamp=1308748643512000)
=> (column=3, value=33, timestamp=1308748645138000)

2 Rows Returned.

[default@Test] describe keyspace;
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.LongType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
{code}
{code}
grunt> test = LOAD 'cassandra://Test/test' USING CassandraStorage() AS (rowkey:chararray, columns: bag {T: (name:long, value:int)});

grunt> value_test = foreach test generate rowkey, columns.name, columns.value;

grunt> dump value_test;
{code}
This time it's work as expected (appart from the value displayed as char). Result:
{code}
(row1,{(1),(2),(3)},{(#),($),(&)})
(row2,{(1),(2),(3)},{(-),(*),(!)})
{code}
","Ubuntu 10.10, 32 bits
java version ""1.6.0_24""
Brisk beta-2 installed from Debian packages",jeromatron,silvere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/11 00:55;brandon.williams;2810-v2.txt;https://issues.apache.org/jira/secure/attachment/12489923/2810-v2.txt","23/Aug/11 16:25;brandon.williams;2810-v3.txt;https://issues.apache.org/jira/secure/attachment/12491357/2810-v3.txt","24/Jun/11 14:45;brandon.williams;2810.txt;https://issues.apache.org/jira/secure/attachment/12483714/2810.txt",,,,,,,,,,,,3.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20841,,,Wed Sep 28 22:19:49 UTC 2011,,,,,,,,,,"0|i0gdlj:",93633,,jeromatron,,jeromatron,Normal,,,,,,,,,,,,,,,,,"24/Jun/11 14:45;brandon.williams;Patch to use a custom AbstractType in place of BytesType to nip this in the bud, rather than have a bunch of one-off checks.  Also fixes a bug where the supercolumn name is never set.;;;","24/Jun/11 14:52;jbellis;DataByteArray is some kind of Pig thing?;;;","24/Jun/11 15:07;brandon.williams;Yes, basically a byte array, but it's the pig type.;;;","27/Jun/11 13:07;silvere;I try again after applying [^2810.txt] and the patch from bug [CASSANDRA-2777] and the bug is still here.
With the patch, you need to replace
{code}
test = LOAD 'cassandra://Test/test' USING CassandraStorage() AS (rowkey:chararray, columns: bag {T: (name:long, value:int)});
{code}
by
{code}
test = LOAD 'cassandra://Test/test' USING CassandraStorage() AS ();
{code}
because CassandraStorage takes care of the schema.

I try:
{code}
grunt> describe test;
test: {key: chararray,columns: {(name: long,value: int)}}
{code}
so we can see that the patch from bug 2777 works correctly (I also test with different types for value).
But when I dump test, I still have the same exception.;;;","27/Jun/11 14:47;silvere;After more test (with both patches), path [^2810.txt] doesn't seems to solve the bug.
Here is a new test case:
Create a _Test_ keyspace and a _test_ column family with key_validation_class = 'AsciiType' and comparator = 'LongType' and default_validation_class = 'IntegerType' (don't use the cli because of [#CASSANDRA-2831]).
Insert some data:
{code}
set test[ascii('row1')][long(1)]=integer(35);
set test[ascii('row1')][long(2)]=integer(36);
set test[ascii('row1')][long(3)]=integer(38);
set test[ascii('row2')][long(1)]=integer(45);
set test[ascii('row2')][long(2)]=integer(42);
set test[ascii('row2')][long(3)]=integer(33);
{code}

In Pig cli:
{code}
test = LOAD 'cassandra://Test/test' USING CassandraStorage() AS ();
dump test;
{code}
The same exception as before is raised:
{code}
 INFO [IPC Server handler 4 on 8012] 2011-06-27 16:40:28,562 TaskInProgress.java (line 551) Error from attempt_201106271436_0012_m_000000_1: java.lang.RuntimeException: Unexpected data type -1 found in stream.
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:478)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeBag(BinInterSedes.java:522)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:361)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:357)
	at org.apache.pig.impl.io.InterRecordWriter.write(InterRecordWriter.java:73)
	at org.apache.pig.impl.io.InterStorage.putNext(InterStorage.java:87)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:138)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:97)
	at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:638)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.collect(PigMapOnly.java:48)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:224)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:763)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:369)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:259)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.mapred.Child.main(Child.java:253)

{code};;;","08/Jul/11 19:41;brandon.williams;So is the conclusion that this patch by itself works fine, but there is a problem with CASSANDRA-2777?;;;","12/Jul/11 13:54;silvere;No, from my test I arrived to the inverse conclusion: [#CASSANDRA-2777] seems to works fine (Pig has the good type for my column family) but the bug is still here despite the 2 patches.;;;","10/Aug/11 00:55;brandon.williams;It looks like the final problem here is that IntegerType always returns a BigInteger, which pig does not like.  This is unfortunate since IntegerType can't be easily subclassed and overridden to return ints.

v2 instead adds a setTupleValue method that is always used for adding values to tuples, and houses all the special-casing currently needed and provides a spot for more in the future, rather than proliferating custom type converters since I'm sure IntegerType won't be alone here.;;;","23/Aug/11 16:25;brandon.williams;v3 also removes decomposing the values before inserting and instead forces them into a ByteBuffer with objToBB, since we actually don't care about the type. (why did we ever change this?)

This means that a UDF that doesn't preserve the schema and hands us back DataByteArrays when we fed it specific types can't make us fail anymore.;;;","28/Sep/11 21:30;steeve;Fixed it for me on Pig 0.9 and Cassandra 0.8.6 (Brisk).;;;","28/Sep/11 21:35;jeromatron;+1 - if we find any issues with it in production, we'll submit bug reports.;;;","28/Sep/11 22:01;brandon.williams;Committed.;;;","28/Sep/11 22:19;hudson;Integrated in Cassandra-0.8 #348 (See [https://builds.apache.org/job/Cassandra-0.8/348/])
    Fix handling of integer types in pig.
Patch by brandonwilliams, reviewed by Jeremy Hanna for CASSANDRA-2810

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1177084
Files : 
* /cassandra/branches/cassandra-0.8/contrib/pig/src/java/org/apache/cassandra/hadoop/pig/CassandraStorage.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In the Cli, update column family <cf> with comparator; create Column metadata",CASSANDRA-2809,12511233,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,silvere,silvere,22/Jun/11 12:54,16/Apr/19 09:32,14/Jul/23 05:52,17/Jul/11 14:13,0.7.8,0.8.2,,Legacy/Tools,,,0,,,,"Using cassandra-cli, I can't update the comparator of a column family with the type I want and when I did it with BytesType, Column metadata appear for each of my existing columns.
Step to reproduce:
{code}
[default@unknown] create keyspace Test
    with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy'
    and strategy_options = [{replication_factor:1}];

[default@unknown] use Test;
Authenticated to keyspace: Test

[default@Test] create column family test;

[default@Test] describe keyspace;
...
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
...

[default@Test] update column family test with comparator = 'LongType';
comparators do not match.
{code}
why?? the CF is empty
{code}
[default@Test] update column family test with comparator = 'BytesType';
f8e4dcb0-9cca-11e0-0000-d0583497e7ff
Waiting for schema agreement...
... schemas agree across the cluster

[default@Test] describe keyspace;
...
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
...

[default@Test] set test[ascii('row1')][long(1)]=integer(35);
set test[ascii('row1')][long(2)]=integer(36);
set test[ascii('row1')][long(3)]=integer(38);
set test[ascii('row2')][long(1)]=integer(45);
set test[ascii('row2')][long(2)]=integer(42);
set test[ascii('row2')][long(3)]=integer(33);

[default@Test] list test;
Using default limit of 100
-------------------
RowKey: 726f7731
=> (column=0000000000000001, value=35, timestamp=1308744931122000)
=> (column=0000000000000002, value=36, timestamp=1308744931124000)
=> (column=0000000000000003, value=38, timestamp=1308744931125000)
-------------------
RowKey: 726f7732
=> (column=0000000000000001, value=45, timestamp=1308744931127000)
=> (column=0000000000000002, value=42, timestamp=1308744931128000)
=> (column=0000000000000003, value=33, timestamp=1308744932722000)

2 Rows Returned.

[default@Test] update column family test with comparator = 'LongType';
comparators do not match.
{code}
same question than before, my columns contains only long, why I can't?

{code}
[default@Test] update column family test with comparator = 'BytesType';

[default@Test] describe keyspace;                                      
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name:  (0000000000000001)
          Validation Class: org.apache.cassandra.db.marshal.IntegerType
        Column Name:  (0000000000000003)
          Validation Class: org.apache.cassandra.db.marshal.IntegerType
        Column Name:  (0000000000000002)
          Validation Class: org.apache.cassandra.db.marshal.IntegerType
{code}
Column Metadata appear from nowhere. I don't think that it's expected.
","Ubuntu 10.10, 32bit
java version ""1.6.0_24""
installed from Debian packages of Brisk-beta2",silvere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/11 19:43;jbellis;2809-validate.txt;https://issues.apache.org/jira/secure/attachment/12483492/2809-validate.txt","15/Jul/11 22:13;xedin;CASSANDRA-2809.patch;https://issues.apache.org/jira/secure/attachment/12486692/CASSANDRA-2809.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20840,,,Sun Jul 17 14:30:59 UTC 2011,,,,,,,,,,"0|i0gdlb:",93632,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"22/Jun/11 19:42;jbellis;Changing comparators is not allowed, since the point of a comparator is that data within a row will be sorted on disk by the comparator's ordering.  Changing the comparator without rewriting the data would corrupt the sstable.

Not sure where that column_metadata comes from though.  Looks like a bug.;;;","22/Jun/11 19:43;jbellis;Noticed that update_cf doesn't validate the given CFMetaData.  patch to add this.  (Does not address the column_metadata problem.);;;","15/Jul/11 12:55;xedin;That column metadata is a feature: when you use a function call in SET statement it is storing information about validation class for individual columns locally (without sync with server) that was added in context of CASSANDRA-1635. 

+1 of validate patch.;;;","15/Jul/11 17:42;brandon.williams;Committed.;;;","15/Jul/11 21:19;jbellis;bq. That column metadata is a feature: when you use a function call in SET statement it is storing information about validation class for individual columns locally

Ah, I remember that.  But this shouldn't show up in a describe keyspace or we cause the above confusion. :);;;","15/Jul/11 21:26;xedin;How should it be stored on your opinion?;;;","15/Jul/11 21:29;jbellis;I'd probably keep a ""local"" CFMetadata around and check that first.;;;","15/Jul/11 21:38;hudson;Integrated in Cassandra-0.8 #216 (See [https://builds.apache.org/job/Cassandra-0.8/216/])
    cli validates CFMetaData on update.
Patch by jbellis, reviewed by Pavel Yaskevich for CASSANDRA-2809

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147261
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java
;;;","15/Jul/11 21:46;xedin;I took a look the code now and what I see is that describe keyspace always calling `thrift.describe_keyspace()` but in the example after adding rows you did a successful ""update column family test with comparator = 'BytesType';"" which persisted changes in column metadata. I think this is actually a good thing to have we just need to document that, what do you think, Jonathan?;;;","15/Jul/11 21:53;jbellis;To make sure we're on the same page, here's what I'm talking about:

{noformat}
      Column Metadata:
        Column Name:  (0000000000000001)
          Validation Class: org.apache.cassandra.db.marshal.IntegerType
        Column Name:  (0000000000000003)
{noformat}

We shouldn't be persisting that or showing it to the user as part of describe keyspace.;;;","15/Jul/11 21:54;xedin;Ok, I will attach a patch fixing this in a few.;;;","15/Jul/11 22:13;xedin;patch for version 0.7 but can be applied on 0.8 also.;;;","17/Jul/11 14:03;jbellis;does this fix the describe problem too, or just the update one?;;;","17/Jul/11 14:05;xedin;Describe too.;;;","17/Jul/11 14:13;jbellis;committed;;;","17/Jul/11 14:30;hudson;Integrated in Cassandra-0.7 #530 (See [https://builds.apache.org/job/Cassandra-0.7/530/])
    avoid including inferred types in CFupdate
patch by pyaskevich; reviewed by jbellis for CASSANDRA-2809

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1147621
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/cli/CliClient.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up mbeans that return Internal Cassandra types,CASSANDRA-2805,12511159,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,nickmbailey,nickmbailey,nickmbailey,21/Jun/11 20:46,16/Apr/19 09:32,14/Jul/23 05:52,31/Dec/11 16:19,1.1.0,,,,,,0,lhf,,,"We need to clean up wherever we return internal cassandra objects over jmx. Namely CompactionInfo objects as well as Tokens. There may be a few other examples.

This is bad for two reasons

1. You have to load the cassandra jar when querying these mbeans, which sucks.
2. Stuff breaks between versions when things are moved. For example, CASSANDRA-1610 moves the compaction related classes around. Any code querying those jmx mbeans in 0.8.0 is now broken in 0.8.2. (assuming those moves stay in the 0.8 branch)

For things like CompactionInfo we should just expose more mbean methods or serialize to something standard like json.

I'd like to target this for 0.8.2. Since we've already broken compatibility between 0.8.0 and 0.8.1, I'd say just fix this everywhere now.",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/11 21:06;nickmbailey;0001-Don-t-return-internal-types-over-jmx.patch;https://issues.apache.org/jira/secure/attachment/12508954/0001-Don-t-return-internal-types-over-jmx.patch","29/Dec/11 19:48;nickmbailey;0001-Don-t-return-internal-types-over-jmx.patch;https://issues.apache.org/jira/secure/attachment/12508879/0001-Don-t-return-internal-types-over-jmx.patch",,,,,,,,,,,,,2.0,nickmbailey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,1929,,,Sat Dec 31 16:19:08 UTC 2011,,,,,,,,,,"0|i0gdkf:",93628,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,"02/Jul/11 23:20;jbellis;If you can list which methods to clean up, this would be a good ticket for the surprisingly frequently asked, ""What's a good place to get my feet wet in the Cassandra code?"";;;","05/Jul/11 13:37;nickmbailey;Off the top of my head:

 * A CompactionInfo object is returned when getting compactions from the CompactionManager mbean.
 * Sub-types of Token objects are returned when calling getTokenToEndpointMap (StorageService mbean i think).

I'll see if i can't look around for any more.;;;","05/Jul/11 15:38;appodictic;Maybe the JMX CompositeType could be returned instead of JSON? ;;;","05/Jul/11 15:49;nickmbailey;Didn't know that existed. I guess it is specifically designed to address this kind of problem?

A quick look makes it seem overly complicated and verbose. If we want to avoid json we can still allow for more complicated types like Map<String> and whatnot. The main problem is just having to include the cassandra jar when querying jmx. From what I can tell it's basically impossible to do jmx-type stuff on a non-jvm language anyway, so maybe we don't really need json.;;;","05/Jul/11 17:53;appodictic;CompositeData is a JMX type that holds a key value map of other JMX types and can be nested. 

http://docs.jboss.org/jbossas/javadoc/4.0.1-sp1/jmx/javax/management/openmbean/CompositeDataSupport.html

This should allows us to return complex objects over JMX without having to resort to JSON serializing things.
;;;","06/Jul/11 22:38;gauravsh;Unless someone else has already started, I am planning to take this one up.;;;","07/Jul/11 16:23;nickmbailey;Ed,

I'm not sure I see the advantage of a JMX Composite type as opposed to using something like a Map<String, Int> approach in any place that needs 'complex' types. ;;;","15/Jul/11 16:28;gauravsh;Based on my research so far scanning the MBean's and their internal users (NodeProbe, NodeCmd and CliClient), there are 4 Cassandra-type dependencies: CompactionInfo, CompactionType, Token, Range. Addressing them individually and discussing my plan:

1. CompactionInfo/CompactionType
Now, CompactionInfo/CompactionType are manageable with a Map as suggested but Range and Token are a bit tightly coupled and more involved.

2. Range
Since Range already has the partitioner (either injected or implicit from StorageService), I believe I can add 2 new constructors that look like:
    public Range(String left, String right)
    public Range(String left, String right, IPartitioner partitioner)

and use the partioner.getTokenFactory().fromString() to curate the left and right Token's.

Also, to replace the StorageServiceMBean's:
    public Map<Range, List<String>> getRangeToEndpointMap(String keyspace);
    public Map<Range, List<String>> getPendingRangeToEndpointMap(String keyspace);

based on their usages, the Range in StorageService can be safely copied to something like a Pair/Tuple.

I noticed that the getRangeToAddressMap() is not exposed on the StorageServiceMBean interface - is that by design (not that I am complaining because right now, it is 1 less dependency to decouple but if it is an omission, I need to account for it)?

3. Token
I can change all MBean interfaces that need a Token to the corresponding String representation using partitioner.getTokenFactory().toString() and then reconstruct back using the fromString();;;","17/Jul/11 18:32;nickmbailey;I'm not sure you need to add any constructors to Range. How about just an asPair() method or something similar that returns the tokens that make up the range converted to strings?

Everything else looks fine. getRangeToAddressMap() isn't needed because getRangeToEndpointMap() is exposed.

Another thing that might be nice to fix here is the getNaturalEndpoints() method. It currently only takes a byte array or byteBuffer object which makes it impossible to call from something like jconsole. It would be nice to overload that with another method that takes a string as the key so you can call it from jconsole.;;;","26/Aug/11 22:01;jbellis;Guarav, are you still working on this?;;;","02/Sep/11 21:30;nickmbailey;I'm probably going to take this and try and get it done quickly before the 1.0 freeze. I'd really like to solve this problem before 1.0 and then be strict on preventing this in future mbean methods.;;;","11/Oct/11 19:52;nickmbailey;Obviously I didn't get this done before the 1.0 freeze. And of course, this breaks between 0.8 and 1.0 since the serial version of CompactionInfo changes.

Can we target this for 1.0.1?;;;","12/Oct/11 01:51;jbellis;I don't think we should break compatibility in a minor release.;;;","12/Oct/11 02:01;nickmbailey;I agree my main argument though is that we have a history of doing it unknowingly. Especially something like this where any update the the CompactionInfo requires updating the serialization version and then breaks compatibility.

I suppose I can live with this in 1.1 and a 'let's try really really really hard not to break compatibility in the 1.0.x releases.;;;","29/Dec/11 19:48;nickmbailey;Should take care of any instances where internal types are returned over jmx.;;;","30/Dec/11 20:10;yukim;Nick,

- I think double brace initialization should be avoided at CompactionInfo#asMap. (yeah, I know Java syntax is sucks.)
- I prefer {{Integer/Long.toString(val)}} over {{new Integer/Long(val).toString()}}.

but, otherwise +1.;;;","30/Dec/11 21:06;nickmbailey;Alright removed the double brace :(, and updated to use static toString.;;;","30/Dec/11 22:22;yukim;+1;;;","31/Dec/11 16:19;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tombstone are not purged when the row is in only one sstable,CASSANDRA-2801,12511068,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,21/Jun/11 09:57,16/Apr/19 09:32,14/Jul/23 05:52,21/Jun/11 12:48,0.8.1,,,,,,0,,,,"We messed up the refactor of compactionController. It echoes rows if they are present in only one sstable, even if they could be purged.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/11 09:57;slebresne;0001-Don-t-echo-row-when-purge-is-possible.patch;https://issues.apache.org/jira/secure/attachment/12483258/0001-Don-t-echo-row-when-purge-is-possible.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20839,,,Tue Jun 21 17:41:40 UTC 2011,,,,,,,,,,"0|i0gdjj:",93624,,,,,Normal,,,,,,,,,,,,,,,,,"21/Jun/11 12:32;jbellis;+1;;;","21/Jun/11 12:48;slebresne;Committed, thanks;;;","21/Jun/11 17:41;hudson;Integrated in Cassandra-0.8 #182 (See [https://builds.apache.org/job/Cassandra-0.8/182/])
    Purge tombstones even if the row is in only one sstable
patch by slebresne; reviewed by jbellis for CASSANDRA-2801

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1137982
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionController.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OPP#describeOwnership reports incorrect ownership,CASSANDRA-2800,12511003,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jhermes,jhermes,jhermes,20/Jun/11 21:07,16/Apr/19 09:32,14/Jul/23 05:52,06/Jul/11 14:40,0.7.7,0.8.2,,,,,0,,,,"OPP#describeOwnership relies on StorageService#getSplits and counts the received tokens as its basis of ownership.

When the number of result keys is less than the number of splits, the full count is omitted (to save work?). However, we don't care if a split would end up fractional in this case, we just need the full count.

The logic here is:
{code}
int splits = keycount * DatabaseDescriptor.getIndexInterval() / keysPerSplit;
if (keycount >= splits) { ... add count to result set }
{code}
We were passing in 1 key per split (since we just care about the count), but splits=keycount*IndexInterval is guaranteed to be > keycount, so the result set is not completely formed.
The better ""unit keysPerSplit"" to use is IndexInterval itself, which gives splits=keycount*II/II=keycount, so the logic runs correctly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/11 21:08;jhermes;2800.txt;https://issues.apache.org/jira/secure/attachment/12483208/2800.txt",,,,,,,,,,,,,,1.0,jhermes,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20838,,,Wed Jul 06 14:53:13 UTC 2011,,,,,,,,,,"0|i0gdjb:",93623,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"06/Jul/11 14:40;slebresne;+1
Committed, thanks;;;","06/Jul/11 14:53;hudson;Integrated in Cassandra-0.7 #522 (See [https://builds.apache.org/job/Cassandra-0.7/522/])
    Fix describeOwnership for OPP
patch by jhermes; reviewed by slebresne for CASSANDRA-2800

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1143437
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair hangs if a neighbor has nothing to send ,CASSANDRA-2797,12510951,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,20/Jun/11 12:33,16/Apr/19 09:32,14/Jul/23 05:52,20/Jun/11 17:29,0.8.1,,,,,,0,repair,streaming,,"This is actually a streaming problem. If a StreamOutSession has nothing to transfer (i.e, no sstables have the requested ranges), it will not even initiate the transfer and simply close the session right away. The problem is that if the session was initiated by a remote end (through a StreamRequestMessage), the remote end will never be notified and never run his callback.
",,colinkuo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/11 17:04;slebresne;0001-Always-initiate-streaming-transfer-to-notify-remote-.patch;https://issues.apache.org/jira/secure/attachment/12483174/0001-Always-initiate-streaming-transfer-to-notify-remote-.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20836,,,Mon Jun 20 19:20:28 UTC 2011,,,,,,,,,,"0|i0gdin:",93620,,,,,Low,,,,,,,,,,,,,,,,,"20/Jun/11 12:35;slebresne;Apparently the code was already wired to handle initiating a session with nothing to transfer, but a if prevented that code path. Patch attached against 0.7.;;;","20/Jun/11 15:51;jbellis;I thought transferSSTables is supposed to be only called for locally-initiated transfers, and transferRangesForRequest is for ones initiated remotely.  (Confusing, I know.  This got cleaned up in 0.8.);;;","20/Jun/11 17:04;slebresne;You are right. I hit this on 0.8 and wrongly assumed 0.7 was impacted too while this is actually a problem due to the mentioned refactor. Attaching patch against 0.8, 0.7 is not impacted.;;;","20/Jun/11 17:08;jbellis;+1;;;","20/Jun/11 17:29;slebresne;Committed, thanks;;;","20/Jun/11 19:20;hudson;Integrated in Cassandra-0.8 #178 (See [https://builds.apache.org/job/Cassandra-0.8/178/])
    fix repair hanging if a neighbor has nothing to send
patch by slebresne; reviewed by jbellis for CASSANDRA-2797

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1137711
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/StreamOut.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrapping node stalls. Bootstrapper thinks it is still streaming some sstables. The source nodes do not. Caused by IllegalStateException on source nodes.,CASSANDRA-2792,12510760,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,dccwilliams,dccwilliams,18/Jun/11 13:53,16/Apr/19 09:32,14/Jul/23 05:52,21/Jun/11 19:53,0.7.7,,,,,,0,,,,"I am bootstrapping a node into a 4 node cluster with RF3 (1 node is currently down due to sstable issues, but the cluster is running without issues). 

There are two keyspaces FightMyMonster and FMM_Studio. The first keyspace successfully streams and the whole operation is probably at 99% when it stalls on some sstables in the much smaller FMM_Studio keyspace.

Netstats on the bootstrapping node reports it is still streaming:

Mode: Bootstrapping
Not sending any streams.
Streaming from: /192.168.1.4
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-101-Data.db sections=1 progress=0/76453 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-103-Data.db sections=1 progress=0/90475 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-102-Data.db sections=1 progress=0/4304182 - 0%
Streaming from: /192.168.1.3
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-158-Data.db sections=2 progress=0/146990 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/AuthorClasses-f-81-Data.db sections=1 progress=0/3992 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/Studio-f-70-Data.db sections=1 progress=0/1776 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-159-Data.db sections=2 progress=0/136829 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-157-Data.db sections=2 progress=0/5779597 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/AuthorClasses-f-82-Data.db sections=1 progress=0/161 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/Studio-f-71-Data.db sections=1 progress=0/135 - 0%
Pool Name                    Active   Pending      Completed
Commands                        n/a         0            334
Responses                       n/a         0         421957

However, running netstats on the source nodes reports they are not streaming:

Mode: Normal
 Nothing streaming to /192.168.1.9
Not receiving any streams.
Pool Name                    Active   Pending      Completed
Commands                        n/a         0        1949476
Responses                       n/a         1        1778768

Examination of the logs on the source nodes show an IllegalStateException that has likely interrupted/broken the streaming process.

17 22:27:05,924 StreamOut.java (line 126) Beginning transfer to /192.168.1.9
 INFO [StreamStage:1] 2011-06-17 22:27:05,925 StreamOut.java (line 100) Flushing memtables for FMM_Studio...
 INFO [StreamStage:1] 2011-06-17 22:27:06,004 StreamOut.java (line 173) Stream context metadata [/var/opt/cassandra/data/FMM_Studio/Classes-f-107-Data.db sections=1 progress=0/1585378 - 0%, /var/opt/cas
sandra/data/FMM_Studio/PartsData-f-100-Data.db sections=1 progress=0/76453 - 0%, /var/opt/cassandra/data/FMM_Studio/PartsData-f-98-Data.db sections=1 progress=0/4309514 - 0%, /var/opt/cassandra/data/FMM
_Studio/PartsData-f-99-Data.db sections=1 progress=0/90475 - 0%], 11 sstables.
 INFO [StreamStage:1] 2011-06-17 22:27:06,005 StreamOutSession.java (line 174) Streaming to /192.168.1.9
 INFO [StreamStage:1] 2011-06-17 22:27:06,006 StreamOut.java (line 126) Beginning transfer to /192.168.1.9
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 StreamOut.java (line 100) Flushing memtables for FightMyMonster...
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-MonsterMarket_1@1054909557(338 bytes, 24 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-UserFights@239934867(1124836 bytes, 965 operations)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,007 Memtable.java (line 157) Writing Memtable-MonsterMarket_1@1054909557(338 bytes, 24 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-Users_CisIndex@1758504250(242 bytes, 8 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,008 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-Tribes@1510979736(18318 bytes, 703 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,008 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-ColumnViews_TimeUUID@864545260(2073 bytes, 63 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,008 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-MonsterMarket_0@537829218(2600 bytes, 129 operations)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,069 Memtable.java (line 172) Completed flushing /var/opt/cassandra/data/FightMyMonster/MonsterMarket_1-f-3799-Data.db (1774 bytes)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,069 Memtable.java (line 157) Writing Memtable-UserFights@239934867(1124836 bytes, 965 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,070 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-UserSigninLog@1692186117(4043 bytes, 137 operations)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,161 Memtable.java (line 172) Completed flushing /var/opt/cassandra/data/FightMyMonster/UserFights-f-8192-Data.db (1179202 bytes)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,161 Memtable.java (line 157) Writing Memtable-Users_CisIndex@1758504250(242 bytes, 8 operations)
 INFO [CompactionExecutor:1] 2011-06-17 22:27:06,161 CompactionManager.java (line 395) Compacting [SSTableReader(path='/var/opt/cassandra/data/FightMyMonster/UserFights-f-8189-Data.db'),SSTableReader(pa
th='/var/opt/cassandra/data/FightMyMonster/UserFights-f-8190-Data.db'),SSTableReader(path='/var/opt/cassandra/data/FightMyMonster/UserFights-f-8191-Data.db'),SSTableReader(path='/var/opt/cassandra/data/
FightMyMonster/UserFights-f-8192-Data.db')]
 INFO [StreamStage:1] 2011-06-17 22:27:06,162 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-TribeFights@321579649(138 bytes, 3 operations)
ERROR [MiscStage:1] 2011-06-17 22:27:06,168 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[MiscStage:1,5,main]
java.lang.IllegalStateException: target reports current file is /var/opt/cassandra/data/FMM_Studio/Classes-f-107-Data.db but is null
        at org.apache.cassandra.streaming.StreamOutSession.validateCurrentFile(StreamOutSession.java:166)
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:58)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MiscStage:1] 2011-06-17 22:27:06,168 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[MiscStage:1,5,main]
java.lang.IllegalStateException: target reports current file is /var/opt/cassandra/data/FMM_Studio/Classes-f-107-Data.db but is null
        at org.apache.cassandra.streaming.StreamOutSession.validateCurrentFile(StreamOutSession.java:166)
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:58)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619) 

There are two problems. Firstly the source nodes should report to the bootstrapping node that there has been a problem, and/or the bootstrapping node should timeout and report the the issue. 

Secondly, there is an issue with what is causing IllegalStateException.",Ubuntu ,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,"21/Jun/11 15:06;slebresne;0001-Make-StreamOutSession-threadSafe.patch;https://issues.apache.org/jira/secure/attachment/12483295/0001-Make-StreamOutSession-threadSafe.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20832,,,Wed Jun 22 07:47:04 UTC 2011,,,,,,,,,,"0|i0gdhj:",93615,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"21/Jun/11 15:06;slebresne;bq. the source nodes should report to the bootstrapping node that there has been a problem, and/or the bootstrapping node should timeout and report the the issue.

This will be addressed by CASSANDRA-2433 (or almost). This won't go into 0.7 however and since this change network protocol it's yet unclear it will even go into 0.8 (though my personnal opinion is that this should go in asap).

bq. Secondly, there is an issue with what is causing IllegalStateException.

Attaching a patch that hopefully fixes that. It's a bit of a guess because I don't know how to reproduce but I suppose that since the currentFile field is not volatile in streamOutSession, changing it may not propagate to all threads (at least that explication fits with the exception message). The patch also use a thread safe map for pendingFiles since those are modified in differnt thread, even though I have stronger doubt that this could be a problem (but it doesn't cost us anything to be on the safe side).

;;;","21/Jun/11 15:34;jbellis;+1 on the threadsafety changes;;;","21/Jun/11 19:53;slebresne;Thanks, committed.;;;","22/Jun/11 07:47;hudson;Integrated in Cassandra-0.7 #507 (See [https://builds.apache.org/job/Cassandra-0.7/507/])
    Improve thread safety in StreamOutSession
patch by slebresne; reviewed by jbellis for CASSANDRA-2792

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1138148
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/streaming/StreamOutSession.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java agent option missing in cassandra.bat file,CASSANDRA-2787,12510656,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,kochen,kochen,kochen,17/Jun/11 12:49,16/Apr/19 09:32,14/Jul/23 05:52,20/Jun/11 16:42,0.8.1,,,Packaging,,,0,,,,"This option must be included in cassandra.bat:

-javaagent:%CASSANDRA_HOME%\lib\jamm-0.2.2.jar

Otherwise you see the following warnings in cassandra log:

WARN 12:02:32,478 MemoryMeter uninitialized (jamm not specified as java agent); assuming liveRatio of 10.0. Usually this means cassandra-env.sh disabled jamm because you are using a buggy JRE; upgrade to the Sun JRE instead
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,kochen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20829,,,Mon Jun 20 19:20:28 UTC 2011,,,,,,,,,,"0|i0gdgf:",93610,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Jun/11 19:15;jbellis;Really, forward slashes?;;;","20/Jun/11 09:12;kochen;Changed them to backslashes;;;","20/Jun/11 16:42;jbellis;committed, thanks!;;;","20/Jun/11 19:20;hudson;Integrated in Cassandra-0.8 #178 (See [https://builds.apache.org/job/Cassandra-0.8/178/])
    add jamm agent to cassandra.bat
patch by rene kochen; reviewed by jbellis for CASSANDRA-2787

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1137695
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/bin/cassandra.bat
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After a minor compaction, deleted key-slices are visible again",CASSANDRA-2786,12510655,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,kochen,kochen,17/Jun/11 12:42,16/Apr/19 09:32,14/Jul/23 05:52,23/Nov/11 15:39,0.8.8,1.0.4,,,,,2,compaction,,,"After a minor compaction, deleted key-slices are visible again.

Steps to reproduce:

1) Insert a row named ""test"".
2) Insert 500000 rows. During this step, row ""test"" is included in a major compaction:
   file-1, file-2, file-3 and file-4 compacted to file-5 (includes ""test"").
3) Delete row named ""test"".
4) Insert 500000 rows. During this step, row ""test"" is included in a minor compaction:
   file-6, file-7, file-8 and file-9 compacted to file-10 (should include tombstoned ""test"").
After step 4, row ""test"" is live again.

Test environment:

Single node with empty database.

Standard configured super-column-family (I see this behavior with several gc_grace settings (big and small values):
create column family Customers with column_type = 'Super' and comparator = 'BytesType;

In Cassandra 0.7.6 I observe the expected behavior, i.e. after step 4, the row is still deleted.

I've included a .NET program to reproduce the problem. I will add a Java version later on.","Reproduced on single Cassandra node (CentOS 5.5)
Reproduced on single Cassandra node (Windows Server 2008)",brandon.williams,jborgstrom,mdennis,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/11 10:56;slebresne;0001-Fix-wrong-purge-of-deleted-cf.patch;https://issues.apache.org/jira/secure/attachment/12483265/0001-Fix-wrong-purge-of-deleted-cf.patch","30/Jun/11 15:04;slebresne;2786_part2.patch;https://issues.apache.org/jira/secure/attachment/12484782/2786_part2.patch","23/Nov/11 14:02;jbellis;2786_part3-v2.txt;https://issues.apache.org/jira/secure/attachment/12504873/2786_part3-v2.txt","22/Nov/11 17:18;slebresne;2786_part3.patch;https://issues.apache.org/jira/secure/attachment/12504774/2786_part3.patch","17/Jun/11 12:43;kochen;CassandraIssue.zip;https://issues.apache.org/jira/secure/attachment/12482927/CassandraIssue.zip","20/Jun/11 13:18;kochen;CassandraIssueJava.zip;https://issues.apache.org/jira/secure/attachment/12483154/CassandraIssueJava.zip",,,,,,,,,6.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20828,,,Wed Nov 23 16:24:35 UTC 2011,,,,,,,,,,"0|i0gdg7:",93609,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"17/Jun/11 14:51;slebresne;The java version would be really cool :);;;","20/Jun/11 13:19;kochen;I included the Java version. You have to play a little bit with the numbers of rows to insert in order to get the correct compaction timings.;;;","21/Jun/11 10:56;slebresne;We were wrongfully skipping deleted rows with no columns during compaction. This indeed don't affect 0.7 since this was due to a refactor of PrecompactedRow in 0.8. Patch attached with a unit test to catch the error.;;;","21/Jun/11 12:26;jbellis;+1

(can we make the ""testing"" constructor package-local?);;;","21/Jun/11 13:48;slebresne;Committed, thanks.

I did not made the ""testing"" constructor package-local because it is used in the AntiEntropyTests which are not on the same package. But I agree it's not the cleanest thing ever.;;;","21/Jun/11 17:41;hudson;Integrated in Cassandra-0.8 #182 (See [https://builds.apache.org/job/Cassandra-0.8/182/])
    Fix wrong purge of deleted cf during compaction
patch by slebresne; reviewed by jbellis for CASSANDRA-2786

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1137984
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/compaction/CompactionsTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/PrecompactedRow.java
;;;","29/Jun/11 16:44;kochen;Tested with 0.8.1 but still doesn't work;;;","30/Jun/11 15:04;slebresne;Yeah, turns out EchoedRow is also handling Row tombstones with no columns inside badly.

Attaching patch with fix and unit test. 0.7 is not really impacted because it uses EchoedRow only for cleanup and don't use its isEmpty() function there (but I suppose we could make it throw an UnsupporteOperationException to be on the safe side).

The patch actually ship with two changes that are not strictly related to the issue:
# It fixes testEchoedRow in CompactionsTest. It wasn't using EchoedRow anymore (i.e, the test was useless).
# It always forces deserialization for user submitted compaction (by opposition to only when the user submits only 1 sstable). It is done because exposing the forceDeserialization flag was necessary to write the test for this issue. Following that change, it was trivial to do the user submitted compaction change. It also fix a bad comment (forcing deserialization is only useful for forcing expired column to become tombstones, not for purging since purging will happen without force deserialization if it can).;;;","01/Jul/11 20:02;jbellis;Nit: wouldn't it be cleaner to just pass gcBefore rather than the entire controller to EchoedRow constructor?

+1 otherwise.;;;","06/Jul/11 12:16;hudson;Integrated in Cassandra-0.8 #205 (See [https://builds.apache.org/job/Cassandra-0.8/205/])
    Handle row tombstones correctly in EchoedRow
patch by slebresne; reviewed by jbellis for CASSANDRA-2786

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1143352
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/compaction/CompactionsTest.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionController.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/EchoedRow.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
;;;","06/Jul/11 12:19;slebresne;Committed, thanks.

bq. Nit: wouldn't it be cleaner to just pass gcBefore rather than the entire controller to EchoedRow constructor?

I passed the controller because Precompacted and LazilyCompacted do that too, so it felt slightly cleaner, and if we happen to need more info from the controller in the future, it'll be there. But really at the end I did not change it before committing out of laziness :);;;","06/Jul/11 13:10;jbellis;bq. Precompacted and LazilyCompacted do that too

That makes sense.;;;","21/Nov/11 16:44;kochen;Tested with 0.8.2 and 0.8.7, but still does not work. On 0.7.x it works fine.;;;","21/Nov/11 16:48;kochen;One note: I tested with several grace-periods. With a grace-period of one minute, it is easier to reproduce. On our production site (with grace-priod of 24 hours), the data resurrects after several days.;;;","21/Nov/11 21:14;brandon.williams;bq. With a grace-period of one minute, it is easier to reproduce.

This seems to me to suggest you aren't running repair often enough and are encountering the same effect as CASSANDRA-1316.;;;","21/Nov/11 21:17;jbellis;bq. This seems to me to suggest you aren't running repair often enough 

If you can only reproduce on multiple nodes, that is probably the issue here.;;;","21/Nov/11 23:50;kochen;With the attached program I'm able to reproduce it on a single node.;;;","22/Nov/11 00:59;mdennis;Per previous phone conversation, I tested this against 0.8.7 twice.

Both times the attached java test ran to completion against a single node and output ""Done"".

When was this last reproduced?;;;","22/Nov/11 01:03;mdennis;I should mention that I did change one line in the test.  I changed {noformat}column.timestamp = getTimestamp();{noformat} to {noformat}column.setTimestamp(getTimestamp());{noformat} because otherwise thrift complained that the timestamp wasn't set.;;;","22/Nov/11 08:49;kochen;Could you please check again with grace_period of 60.

I use the following to reproduce:

create column family Customers
    with column_type = 'Super' 
    and comparator = 'BytesType'
	and memtable_flush_after = 60
	and gc_grace = 60;

On my system, it crashes every time:

C:\Temp\JavaIssue>java -jar CassandraIssue.jar 127.0.0.1 Traxis
Exception in thread ""main"" java.lang.Exception: test row should be empty
        at cassandraissue.Main.start(Main.java:88)
        at cassandraissue.Main.main(Main.java:178)
		
Thanks;;;","22/Nov/11 13:12;slebresne;I had to tweak the variables a little bit but I'm able to reproduce. Will look into it.;;;","22/Nov/11 17:18;slebresne;Hopefully we get this right that time. The problem was that we were calling removeDeleted even in case where we shouldn't have been purging. Attaching 'part3' patch to fix, along with an updated unit test for that.;;;","23/Nov/11 14:02;jbellis;This is a little subtle so I'm going to spell it out:

The purpose of AbstractCompactedRow.isEmpty  is to skip rows that consist only of expired tombstones:

{code}
.           writer = cfs.createCompactionWriter(expectedBloomFilterSize, compactionFileLocation, sstables);
            while (nni.hasNext())
            {
                AbstractCompactedRow row = nni.next();
                if (row.isEmpty())
                    continue;
                ...
            }
{code}

However, we can't skip tombstones if we're only compacting some of the sstables for a row (CASSANDRA-1074).  The bug here is that the isEmpty test doesn't check the CompactionController.shouldPurge, which is how the controller lets us know it's okay to drop tombstones.  (In the PR case the shouldPurge check was done correctly during creation of compactedCf, but then we ignored it when checking a second time for isEmpty.)

Sylvain's patch fixes the bug.  Here is a v2 that simplifies isEmpty further:

- ER.isEmpty is actually trivial
- PR doesn't need to do a second check of no columns + no row level tombstone (i.e.: there were expired column tombstones); this case would be taken care of by the removeDeleted in compactedCf creation
;;;","23/Nov/11 14:51;slebresne;+1 on v2;;;","23/Nov/11 15:39;jbellis;committed;;;","23/Nov/11 16:24;hudson;Integrated in Cassandra-0.8 #403 (See [https://builds.apache.org/job/Cassandra-0.8/403/])
    avoid dropping tombstones when they might still be needed to shadow data in another sstable
patch by slebresne and jbellis for CASSANDRA-2786

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1205452
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/EchoedRow.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionController.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/LazilyCompactedRow.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/PrecompactedRow.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/SchemaLoader.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/compaction/CompactionsTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
should export JAVA variable in the bin/cassandra and use that in the cassandra-env.sh when check for the java version,CASSANDRA-2785,12510628,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,cywjackson,cywjackson,17/Jun/11 00:59,16/Apr/19 09:32,14/Jul/23 05:52,07/Aug/11 20:12,0.8.4,,,Packaging,,,0,,,,"I forgot which jira we add this java -version check in the cassandra-env.sh (for adding jamm to the javaagent), but we should probably use the variable JAVA set in bin/cassandra (will need export) and use $JAVA instead of ""java"" in the cassandra-env.sh

In a situation where JAVA_HOME may have been properly set as the Sun's java but the PATH still have the OpenJDK's java in front, the check will fail to add the jamm.jar, even though the cassandra jvm is properly started via the Sun's java.",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/11 21:22;thepaul;0001-use-JAVA-in-cassandra-env.sh.patch.txt;https://issues.apache.org/jira/secure/attachment/12488029/0001-use-JAVA-in-cassandra-env.sh.patch.txt","28/Jul/11 16:10;thepaul;0002-fix-usage-of-bash-n-tests.patch.txt;https://issues.apache.org/jira/secure/attachment/12488112/0002-fix-usage-of-bash-n-tests.patch.txt",,,,,,,,,,,,,2.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20827,,,Sat Aug 06 23:04:49 UTC 2011,,,,,,,,,,"0|i0gdfz:",93608,,,,,Low,,,,,,,,,,,,,,,,,"27/Jul/11 21:22;thepaul;patch 0001: Uses $JAVA in cassandra-env.sh instead of relying on $PATH.

Makes sure that JAVA is appropriately set from bin/cassandra and debian/init.

Changes debian initscript so that /etc/default/cassandra is sourced _before_ /etc/cassandra/cassandra-env.sh. I don't know of anyone using /etc/default/cassandra in such a way that this would be a problem. The change gives users a place to specify their own $JAVA_HOME if they so desire.;;;","28/Jul/11 07:15;urandom;The -n eval in bin/cassandra needs to be quoted or it'll read non-zero even when it's not.

{noformat}
--- a/bin/cassandra
+++ b/bin/cassandra
@@ -80,7 +80,7 @@ elif [ -r $CASSANDRA_INCLUDE ]; then
 fi
 
 # Use JAVA_HOME if set, otherwise look for java in PATH
-if [ -n $JAVA_HOME ]; then
+if [ -n ""$JAVA_HOME"" ]; then
     JAVA=$JAVA_HOME/bin/java
 else
     JAVA=java
{noformat}

Aside from that, +1;;;","28/Jul/11 16:10;thepaul;Doh, I never knew that one. I normally quote everything, but was trying to match style.

I found a couple other places where we need to quote the argument to test -n.;;;","29/Jul/11 10:30;urandom;+1;;;","29/Jul/11 14:40;urandom;committed; thanks!;;;","29/Jul/11 15:29;hudson;Integrated in Cassandra-0.8 #243 (See [https://builds.apache.org/job/Cassandra-0.8/243/])
    honor path to java when JAVA_HOME set

Patch by Paul Cannon; reviewed by eevans for CASSANDRA-2785

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152241
Files : 
* /cassandra/branches/cassandra-0.8/conf/cassandra-env.sh
* /cassandra/branches/cassandra-0.8/debian/cassandra.postinst
* /cassandra/branches/cassandra-0.8/debian/init
* /cassandra/branches/cassandra-0.8/bin/cassandra
;;;","04/Aug/11 16:31;slebresne;This is causing problems, see CASSANDRA-2992.

I have only reverted this patch on a specific branch so that it doesn't block 0.8.3 release. However, I have not reverted yet on 0.8 branch (or trunk), so that it could be fixed incrementally.

As a side note, it would be nice to set the correct fix version and reviewer when marking an issue resolved.;;;","06/Aug/11 23:04;thepaul;I think this patch is still correct, and that the bug causing the regression was the invalid hard-coding of JAVA_HOME in redhat/cassandra. If the fix for CASSANDRA-2992 is acceptable, this should probably be re-closed as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix build for removal of commons-collections,CASSANDRA-2784,12510617,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,16/Jun/11 20:42,16/Apr/19 09:32,14/Jul/23 05:52,16/Jun/11 21:04,1.0.0,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/11 20:53;stuhood;0001-Remove-references-to-the-commons.collections-package.txt;https://issues.apache.org/jira/secure/attachment/12482858/0001-Remove-references-to-the-commons.collections-package.txt",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20826,,,Thu Jun 16 21:04:13 UTC 2011,,,,,,,,,,"0|i0gdfr:",93607,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Jun/11 20:53;stuhood;0001 Removes the last references to the commons.collections package.;;;","16/Jun/11 21:04;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
regression: exposing cache size through MBean,CASSANDRA-2781,12510580,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cburroughs,cburroughs,cburroughs,16/Jun/11 14:39,16/Apr/19 09:32,14/Jul/23 05:52,16/Jun/11 16:23,0.8.1,,,,,,0,,,,"Looks like it was part of CASSANDRA-1969.  A method called size, as opposed to getSize, won't be exposed through jmx.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/11 14:43;cburroughs;2781-v1.txt;https://issues.apache.org/jira/secure/attachment/12482803/2781-v1.txt",,,,,,,,,,,,,,1.0,cburroughs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20825,,,Fri Jun 17 02:20:45 UTC 2011,,,,,,,,,,"0|i0gdf3:",93604,,kingryan,,kingryan,Low,,,,,,,,,,,,,,,,,"16/Jun/11 14:43;cburroughs;Just size --> getSize in InstrumentingCacheMBean;;;","16/Jun/11 16:14;kingryan;It would be nice if we had some tests around these things, but I'm +1 on this patch.;;;","16/Jun/11 16:23;jbellis;committed, thanks!;;;","17/Jun/11 02:20;hudson;Integrated in Cassandra-0.8 #173 (See [https://builds.apache.org/job/Cassandra-0.8/173/])
    fix cache mbean getSize
patch by Chris Burroughs; reviewed by Ryan King for CASSANDRA-2781

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1136529
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cache/InstrumentingCacheMBean.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/NodeCmd.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cache/InstrumentingCache.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstable2json needs to escape quotes,CASSANDRA-2780,12510538,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,tcn,tcn,16/Jun/11 07:49,16/Apr/19 09:32,14/Jul/23 05:52,16/Jun/11 20:04,0.8.2,,,,,,1,,,,"[default@foo] set transactions[test][data]='{""foo"":""bar""}'; 

$ cat /tmp/json
{
""74657374"": [[""data"", ""{""foo"":""bar""}"", 1308209845388000]]
}

$ ./json2sstable -s -c transactions -K foo /tmp/json /tmp/ss-g-1-Data.db
Counting keys to import, please wait... (NOTE: to skip this use -n <num_keys>)
org.codehaus.jackson.JsonParseException: Unexpected character ('f' (code 102)): was expecting comma to separate ARRAY entries
 at [Source: /tmp/json2; line: 2, column: 27]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:929)
	at org.codehaus.jackson.impl.JsonParserBase._reportError(JsonParserBase.java:632)
	at org.codehaus.jackson.impl.JsonParserBase._reportUnexpectedChar(JsonParserBase.java:565)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:128)
	at org.codehaus.jackson.impl.JsonParserBase.skipChildren(JsonParserBase.java:263)
	at org.apache.cassandra.tools.SSTableImport.importSorted(SSTableImport.java:328)
	at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:252)
	at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:476)
ERROR: Unexpected character ('f' (code 102)): was expecting comma to separate ARRAY entries
 at [Source: /tmp/json2; line: 2, column: 27]

http://www.mail-archive.com/user@cassandra.apache.org/msg14257.html
",,sdolgy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/11 18:48;xedin;CASSANDRA-2780-v2.patch;https://issues.apache.org/jira/secure/attachment/12482970/CASSANDRA-2780-v2.patch","16/Jun/11 16:16;xedin;CASSANDRA-2780.patch;https://issues.apache.org/jira/secure/attachment/12482812/CASSANDRA-2780.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20824,,,Mon Jun 20 10:08:41 UTC 2011,,,,,,,,,,"0|i0gdev:",93603,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"16/Jun/11 08:09;tcn;Fix is easy:

$ cat /tmp/json 
{
""74657374"": [[""data"", ""{\""foo\"":\""bar\""}"", 1308209845388000]]
}

$ ./json2sstable -s -c transactions -K foo /tmp/json /var/lib/cassandra/data/foo/transactions-g-1-Data.db
Counting keys to import, please wait... (NOTE: to skip this use -n <num_keys>)
Importing 1 keys...
1 keys imported successfully.

[default@foo] get transactions[test][data];
=> (column=data, value={""foo"":""bar""}, timestamp=1308209845388000);;;","16/Jun/11 20:04;brandon.williams;Committed.;;;","16/Jun/11 23:38;jbellis;Tatu's feedback:

{quote}
The best way to handle escaping/quoting is to use tools that deal with the format -- this patch only handles escaping of double-quotes, but similar problems would occur with linefeeds, or backslashes.

So using a JSON generator / writer would make sense, if project uses one already? If not, I would recommend adding handling of backslashes and white space other than space (char codes below 32), as those must be escaped in String values and keys.

Also: if unquoted keys were required (not valid JSON, but there's lots of data like that...), Jackson can be configured to accept unquoted field names. That has its own potential issues (whether escaping is allowed, which characters are legal in unquotes names), but appears to work well enough that users haven't complained.
{quote};;;","16/Jun/11 23:42;brandon.williams;{quote}
So using a JSON generator / writer would make sense, if project uses one already? If not, I would recommend adding handling of backslashes and white space other than space (char codes below 32), as those must be escaped in String values and keys.
{quote}

We don't use one.  I believe the historical reason is that one couldn't be found that would handle things incrementally, but I think that may have changed by now (sst2j is pretty long in the tooth) and we should probably switch to one if possible.;;;","17/Jun/11 02:20;hudson;Integrated in Cassandra-0.8 #173 (See [https://builds.apache.org/job/Cassandra-0.8/173/])
    sstable2json escapes quotes.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2780

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1136637
Files : 
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/SchemaLoader.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/SSTableExport.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/tools/SSTableExportTest.java
;;;","17/Jun/11 08:03;tcn;The patch will replace existing \"" with \\"" (backslash backslash quote) which may not be the desired behaviour.

Alternative (regex should be precompiled, of course):
String.format(""\""%s\"""", val.replaceAll(""(?<!\\\\)\"""", ""\\\\\""""));
 ;;;","17/Jun/11 09:47;xedin;changed replace with pattern/matcher with your regex in the escapeQuotes method. thanks!;;;","17/Jun/11 14:51;tcn;D'oh, actually an existing \"" must be replaced with \\\"" :-\;;;","17/Jun/11 14:56;xedin;Can you please post a correct regex so i can include it to patch?;;;","17/Jun/11 15:08;jbellis;Why are we talking about regexes instead of taking Tatu's advice?;;;","17/Jun/11 15:13;xedin;Agreed, I see now that would be the best option. Will be working at that direction.;;;","17/Jun/11 18:48;xedin;uses ObjectMapper.writeValue(PrintStream, Object) to serialize keys and columns to JSON instead of using regexs.;;;","17/Jun/11 19:20;brandon.williams;Committed v2, thanks!;;;","17/Jun/11 20:27;hudson;Integrated in Cassandra-0.8 #175 (See [https://builds.apache.org/job/Cassandra-0.8/175/])
    Use jackson's ObjectMapper instead of hand-crafting json in sstable2json.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2780

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1136991
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/SSTableExport.java
;;;","20/Jun/11 10:08;tcn;Sure, using a real serializer is always better (but usually somewhat slower). Works nicely. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to set compaction strategy in cli using create column family command,CASSANDRA-2778,12510510,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alanliang,alanliang,alanliang,16/Jun/11 00:10,16/Apr/19 09:32,14/Jul/23 05:52,16/Jun/11 20:45,,,,,,,0,,,,"The following command does not set compaction strategy and its options:
{code}
create column family Standard1
    with comparator = BytesType
    and compaction_strategy = 'org.apache.cassandra.db.compaction.TimestampBucketedCompactionStrategy'
    and compaction_strategy_options = [{max_sstable_size:504857600, retention_in_seconds:60}];
{code}",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/11 20:32;alanliang;0001-2778-allow-for-dynamic-changes-to-compaction-strateg.patch;https://issues.apache.org/jira/secure/attachment/12482852/0001-2778-allow-for-dynamic-changes-to-compaction-strateg.patch",,,,,,,,,,,,,,1.0,alanliang,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20822,,,Thu Jun 16 20:37:46 UTC 2011,,,,,,,,,,"0|i0gdef:",93601,,lenn0x,,lenn0x,Normal,,,,,,,,,,,,,,,,,"16/Jun/11 20:37;lenn0x;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Index manager cannot support deleting and inserting into a row in the same mutation""",CASSANDRA-2773,12510393,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,yulinyen,yulinyen,15/Jun/11 03:10,16/Apr/19 09:32,14/Jul/23 05:52,30/Jun/11 01:02,0.7.7,0.8.2,,,,,2,,,,"I use hector 0.8.0-1 and cassandra 0.8.

1. create mutator by using hector api, 
2. Insert a few columns into the mutator for key ""key1"", cf ""standard"". 
3. add a deletion to the mutator to delete the record of ""key1"", cf ""standard"".
4. repeat 2 and 3
5. execute the mutator.

the result: the connection seems to be held by the sever forever, it never returns. when I tried to restart the cassandra I saw unsupportedexception : ""Index manager cannot support deleting and inserting into a row in the same mutation"". and the cassandra is dead forever, unless I delete the commitlog. 

I would expect to get an exception when I execute the mutator, not after I restart the cassandra.",,colinkuo,jancona,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/11 16:49;jbellis;2773-v2.txt;https://issues.apache.org/jira/secure/attachment/12483172/2773-v2.txt","17/Jun/11 17:17;jbellis;2773.txt;https://issues.apache.org/jira/secure/attachment/12482957/2773.txt","27/Jun/11 20:00;jancona;ASF.LICENSE.NOT.GRANTED--v1-0001-allow-deleting-a-rowand-updating-indexed-columns-init-.txt;https://issues.apache.org/jira/secure/attachment/12483997/ASF.LICENSE.NOT.GRANTED--v1-0001-allow-deleting-a-rowand-updating-indexed-columns-init-.txt","27/Jun/11 20:00;jancona;ASF.LICENSE.NOT.GRANTED--v1-0002-CASSANDRA-2773-Add-unit-tests-to-verfy-fix-cherry-pick.txt;https://issues.apache.org/jira/secure/attachment/12483998/ASF.LICENSE.NOT.GRANTED--v1-0002-CASSANDRA-2773-Add-unit-tests-to-verfy-fix-cherry-pick.txt","23/Jun/11 21:34;jancona;cassandra.log;https://issues.apache.org/jira/secure/attachment/12483644/cassandra.log",,,,,,,,,,5.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20820,,,Thu Jun 30 02:18:15 UTC 2011,,,,,,,,,,"0|i0gddb:",93596,,slebresne,,slebresne,Critical,,,,,,,,,,,,,,,,,"15/Jun/11 03:33;jbellis;Like it says, you will have to send your deletes and mutations in different mutations.;;;","15/Jun/11 03:45;yulinyen;sure, I know what you mean.

however, when the user uses it the wrong way, the server should at least response an error or exception... not just hold the connection and do nothing...

In addition, the server should not be dead forever, when this kind of exception occurs. ;;;","15/Jun/11 03:51;jbellis;You're right, there's a validation problem here.;;;","17/Jun/11 17:17;jbellis;we could add some valdation logic but it looks like it's almost as easy to just remove this limitation.  patch to do this attached.;;;","20/Jun/11 09:19;slebresne;Hum, we cannot remove the column from cf in ignoreObsoleteMutations() because cf is the original column family from the row mutation and that's racy with commit log write (à la CASSANDRA-2604). We should clone the column family, but maybe it's simpler to add validation logic after all ? In any case, it could be worth it adding some comment in Table.apply() or Table.ignoreObsoleteMutations(). ;;;","20/Jun/11 16:49;jbellis;bq. we cannot remove the column from cf in ignoreObsoleteMutations()

You're right.  Fortunately I don't think that's actually necessary.  v2 attached.;;;","20/Jun/11 16:53;slebresne;Right, +1. I still think we should add a comment somewhere saying we shouldn't change cf (and that there is no reason to change it anyway).;;;","23/Jun/11 16:00;jbellis;Committed, with comment.  (I wish CF objects could be immutable AND efficient...);;;","23/Jun/11 17:27;hudson;Integrated in Cassandra-0.8 #187 (See [https://builds.apache.org/job/Cassandra-0.8/187/])
    allow deleting a rowand updating indexed columns init in the same mutation
patch by jbellis; reviewed by slebresne for CASSANDRA-2773

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1138959
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/Table.java
;;;","23/Jun/11 21:32;jancona;We are experiencing this issue on a cluster running 0.7.6-2. Once this occurs on a server, the same message is repeated over and over again in system.log. When we attempted to restart the box it failed. I will attach the log to this issue.;;;","23/Jun/11 21:34;jancona;Log of failed restart attempt;;;","24/Jun/11 17:04;jancona;Marked as critical, because when this occurs the server will not restart without removing commitlog and attending loss of data.;;;","24/Jun/11 17:55;jbellis;I explained on-list that I'm not comfortable committing this to 0.7.  The critical thing we need to deliver in 0.7 at this stage is stability.  

The rarity of the problem combined with the complexity of the code involved leads me to conclude that it's better to live with a bug we know how to avoid, than risk introducing new ones.  Remember, the risk of new bugs affects _everyone_, while the fix can only benefit those who were generating the unusual mutation pattern here.  It's our responsibility to take a balanced view.

For the rare people who do find themselves affected here, your options include
- stay on < 0.7.6
- drain the commitlog with an earlier version before upgrading, then re-upgrade to 0.7.6 after fixing your code to not generate problematic mutations
- run an unofficial build with this patch included
- upgrade to 0.8.2 when released;;;","24/Jun/11 19:03;jancona;I disagree. This is a bug which allows a client to make a cluster unresponsive by performing a seemingly innocuous series of operations. If that happens, the cluster is un-restartable without loss of data. I wouldn't call a release where this can occur ""stable"". So if the goal for 0.7 is stability...

WRT ""fixing your code to not generate problematic mutations,"" this may be difficult to do. I have so far identified code that does deletes followed by updates in the same mutation, but I haven't yet found any updates followed by deletes. Are we sure that only the update-followed-by-delete scenario is problematic?

In any case, even after reviewing all our code for the relevant scenarios, I would not feel comfortable deploying an 0.7 release with this vulnerability to production. The risk of a catastrophic failure is too great.
;;;","27/Jun/11 15:54;slebresne;Actually, there isn't really much risk for data loss given that as Jonathan said, if you hit that, it's fairly easy to go back to 0.7.5, fix client code and upgrade again. Granted this is not user friendly and not something you should expect from a minor upgrade, but let at least set the record straight on the data loss part.

That being said, I don't think the patch on this ticket could screw up indexes more that we use to prior to 0.7.6, so maybe we can commit to 0.7.7 on that ground.

I'd still suggest fixing client code in the meantime. ;;;","27/Jun/11 16:02;jbellis;bq. I don't think the patch on this ticket could screw up indexes more that we use to prior to 0.7.6

That's a valid way to frame the issue.

I'm good to commit for 0.7.7 if Jim can test the patch first, since he's the only one we've heard of hitting this in 0.7.x.  (Specifically, we want to make sure that if we query ""WHERE foo = X"" we don't get results back where foo is something other than X.  Ideally you'd start with an empty database, or at least drop + recreate indexes first to make sure the results aren't contaminated w/ corrupt entries from pre-0.7.6.);;;","27/Jun/11 20:03;jancona;I applied the 0.8 patch and added a couple of tests to ColumnFamilyStoreTest. The tests trigger the UnsupportedOperationException in 0.7.6 and return the correct values with the patch applied. Would you like me to test the same scenario against an actual patched server?;;;","27/Jun/11 21:27;jbellis;Thanks for the test case, Jim.

If by the same scenario, you mean the workload that left your commitlog throwing exceptions, then yes please.;;;","29/Jun/11 15:35;jancona;We have deployed and tested 0.7.6 plus this patch to the affected cluster. The cluster restarted successfully and the tests that caused the original failure ran successfully. In addition, functional tests of our applications show no regressions. I also reviewed the Cassandra system logs after the testing and saw no errors or obvious problems.
;;;","30/Jun/11 01:02;jbellis;committed.  Thanks, Jim!;;;","30/Jun/11 02:18;hudson;Integrated in Cassandra-0.7 #519 (See [https://builds.apache.org/job/Cassandra-0.7/519/])
    add additional tests for #2773
patch by Jim Ancona; reviewed by jbellis for CASSANDRA-2773
allow deleting and inserting into an indexed row in the same mutation
patch by jbellis; reviewed by slebresne and tested by Jim Ancona for CASSANDRA-2773

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1141354
Files : 
* /cassandra/branches/cassandra-0.7/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1141353
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/Table.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot Create Duplicate Compaction Marker,CASSANDRA-2769,12510353,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,bcoverston,bcoverston,14/Jun/11 20:15,16/Apr/19 09:32,14/Jul/23 05:52,17/Jun/11 15:01,0.8.2,1.0.0,,,,,0,,,,"Concurrent compaction can trigger the following exception when two threads compact the same sstable. DataTracker attempts to prevent this but apparently not successfully.

java.io.IOError: java.io.IOException: Unable to create compaction marker
	at org.apache.cassandra.io.sstable.SSTableReader.markCompacted(SSTableReader.java:638)
	at org.apache.cassandra.db.DataTracker.removeOldSSTablesSize(DataTracker.java:321)
	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:294)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:932)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:173)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:119)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:102)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.io.IOException: Unable to create compaction marker
	at org.apache.cassandra.io.sstable.SSTableReader.markCompacted(SSTableReader.java:634)
	... 12 more",,alanliang,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/11 11:43;slebresne;0001-0.8.0-Remove-useless-unmarkCompacting-in-doCleanup.patch;https://issues.apache.org/jira/secure/attachment/12482653/0001-0.8.0-Remove-useless-unmarkCompacting-in-doCleanup.patch","17/Jun/11 10:18;slebresne;0001-Do-compact-only-smallerSSTables-v2.patch;https://issues.apache.org/jira/secure/attachment/12482916/0001-Do-compact-only-smallerSSTables-v2.patch","15/Jun/11 11:43;slebresne;0001-Do-compact-only-smallerSSTables.patch;https://issues.apache.org/jira/secure/attachment/12482654/0001-Do-compact-only-smallerSSTables.patch","17/Jun/11 10:18;slebresne;0002-Only-compact-what-has-been-succesfully-marked-as-com-v2.patch;https://issues.apache.org/jira/secure/attachment/12482917/0002-Only-compact-what-has-been-succesfully-marked-as-com-v2.patch","15/Jun/11 11:43;slebresne;0002-Only-compact-what-has-been-succesfully-marked-as-com.patch;https://issues.apache.org/jira/secure/attachment/12482655/0002-Only-compact-what-has-been-succesfully-marked-as-com.patch",,,,,,,,,,5.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20819,,,Fri Jun 17 15:21:09 UTC 2011,,,,,,,,,,"0|i0gdcf:",93592,,,,,Normal,,,,,,,,,,,,,,,,,"15/Jun/11 05:35;stuhood;Is this in trunk, or in 0.8.0?;;;","15/Jun/11 05:36;bcoverston;Yes, in both.;;;","15/Jun/11 11:43;slebresne;Alright, there is a bunch of problems, one of which affects 0.8 and trunk and could cause this stackTrace. The others are due to CASSANDRA-1610 and thus only affect trunk (but one of those can also result in the attached stackTrace).

The problem affecting 0.8 and trunk is related to a left over line in doCleanup() that is wrongly unmarking a sstable from the compacting set before having removed it from the active set of sstables. Thus another compaction could start compacting this sstable and we'll end up marking the file as compacted twice (and we would have duplicated the sstable, which is a problem for counters).
Patch 0001-0.8.0-Remove-useless-unmarkCompacting-in-doCleanup.patch removes it and is against 0.8.

Trunk has a few problems of its own:
* If disk space is not sufficient to compact all sstables, it computes the smallestSSTables set that fits, but doesn't use it. Attached first patch (0001-Do-compact-only-smallerSSTables.patch) fixes that.
* The CompactionTask logic wrongly decorrelates the set of sstables that are successfully marked from the ones it did compact. That is, it grabs a list of sstables it wants to compact, then call markCompacting on them, but does not check if all of them are successfully marked and compact the original list instead.
  In effect, a task will recompact sstables that are already being compacted by other task and the given file will be compacted twice (or more) and marked compacted multiple times.
  Attached patch (0002-Only-compact-what-has-been-succesfully-marked-as-com.patch) fixes this by changing the sstables set of a given CompactionTask to whatever has been successfully marked only. Since the marking involves updating the task, I've move the logic to AbstractCompactionTask where it seems to make more sense to me.
* For some reason, the markCompacting added for CompactionTasks was refusing to mark (and compact) anything if the set of sstable was bigger that MaxCompactionThreshold. This means that as soon as the number of sstables (of same size) in the column family would exceed the threshold, no compaction would be started. This is not the expected behavior. The second patch also fixes this by reusing the original markCompacting that handles this correctly.
;;;","15/Jun/11 17:49;jbellis;the 0.8 patch looks good.  (i did notice that some of the other post-markCompactiong code checks for null or empty, others just check for null -- one of these is probably wrong.);;;","15/Jun/11 17:55;slebresne;Alright, I've committed the 0.8 patch. I'll have a look at the checks.;;;","15/Jun/11 22:22;alanliang;Instead of letting DataTracker#markCompacting modify the subset of sstables to be compacted, I think it might be cleaner if it didn't and relied on the CompactionStrategy to select the correct sstables. We can do this by having the CompactionStrategy get the non compacting sstables from the DataTracker and work with those to generate the buckets. The strategy should also be responsible for creating buckets that fit within the min/max thresholds. #markCompacting would then be changed such that it can either accept/reject a bucket to be compacted instead of modifying the subset. #markCompacting will also serve to handle the race condition of the DataTracker being inaccurate, whereby, it will move on to other buckets.

With this, we can avoid generating buckets that are already compacting and it gives full control of what actually is compacted by the CompactionStrategy.

What do you guys think?
;;;","16/Jun/11 17:31;jbellis;For trunk patches, I'm not comfortable w/ 0001 reassigning the sstables field on general principles either.  We could have the compaction proceed using smallerSSTables as a simpler alternative, but in general this organization feels like negative progress from the 0.8 doCompaction/doCompactionWithoutSizeEstimation.

trunk 0002 looks fine.;;;","17/Jun/11 02:20;hudson;Integrated in Cassandra-0.8 #173 (See [https://builds.apache.org/job/Cassandra-0.8/173/])
    ;;;","17/Jun/11 03:59;bcoverston;I think Alan has a good point. I don't think it's an appropriate role of the data tracker to modify the set of sstables to be compacted in a task. It's been a bit of a struggle to hack around the behavior of the DataTracker to ensure my compactions happen in the order I want with the SSTables that I want. That should probably be the exclusive domain compaction strategy.;;;","17/Jun/11 10:18;slebresne;bq. For trunk patches, I'm not comfortable w/ 0001 reassigning the sstables field on general principles either. We could have the compaction proceed using smallerSSTables as a simpler alternative, but in general this organization feels like negative progress from the 0.8 doCompaction/doCompactionWithoutSizeEstimation.

Attaching v2 that doesn't reassign the sstables field.

bq. I think Alan has a good point. I don't think it's an appropriate role of the data tracker to modify the set of sstables to be compacted in a task.

I do not disagree with that. However I'd like that we fix trunk as a first priority. It's a pain to work on other issues (CASSANDRA-2521 for instance) while it is broken (and the goal must be to do our best to always have a working trunk). The attached patches doesn't really change any behavior, it just fixes the bugs, so let's get that in first before thinking about refactoring.
;;;","17/Jun/11 14:11;bcoverston;I'm sorry I didn't mean to imply it should be fixed _here_. I'll find a more appropriate venue to vent these frustrations :);;;","17/Jun/11 14:54;jbellis;+1 v2;;;","17/Jun/11 15:01;slebresne;Committed, thanks.;;;","17/Jun/11 15:21;hudson;Integrated in Cassandra #931 (See [https://builds.apache.org/job/Cassandra/931/])
    Fix compaction of the same sstable by multiple thread
patch by slebresne; reviewed by jbellis for CASSANDRA-2769

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1136904
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionTask.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/DataTracker.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/AbstractCompactionTask.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException in AntiEntropyService.getNeighbors(),CASSANDRA-2767,12510277,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,14/Jun/11 08:01,16/Apr/19 09:32,14/Jul/23 05:52,14/Jun/11 09:49,0.8.1,,,,,,0,repair,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"14/Jun/11 08:43;slebresne;0001-Fix-ConcurrentModificationException.patch;https://issues.apache.org/jira/secure/attachment/12482530/0001-Fix-ConcurrentModificationException.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20817,,,Tue Jun 14 17:02:36 UTC 2011,,,,,,,,,,"0|i0gdbz:",93590,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"14/Jun/11 08:55;jbellis;+1;;;","14/Jun/11 09:49;slebresne;Committed;;;","14/Jun/11 17:02;hudson;Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException during node recovery,CASSANDRA-2766,12510276,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,terjem,terjem,14/Jun/11 07:14,16/Apr/19 09:32,14/Jul/23 05:52,16/Jun/11 17:40,0.8.1,,,,,,0,,,,"Testing some node recovery operations.

In this case:
1. Data is being added/updated as it would in production
2. repair is running on other nodes in the cluster
3. we wiped data on this node and started up again, but before repair was actually started on this node (but it had gotten data through the regular data feed) we got this error.

I see no indication in the logs that outgoing streams has been started, but the node have finished one incoming stream before this (I guess from some other node doing repair).

 INFO [CompactionExecutor:11] 2011-06-14 14:15:09,078 SSTableReader.java (line 155) Opening /data/cassandra/node1/data/JP/test-g-8
 INFO [CompactionExecutor:13] 2011-06-14 14:15:09,079 SSTableReader.java (line 155) Opening /data/cassandra/node1/data/JP/test-g-10
 INFO [HintedHandoff:1] 2011-06-14 14:15:26,623 HintedHandOffManager.java (line 302) Started hinted handoff for endpoint /1.10.42.216
 INFO [HintedHandoff:1] 2011-06-14 14:15:26,623 HintedHandOffManager.java (line 358) Finished hinted handoff of 0 rows to endpoint /1.10.42.216
 INFO [CompactionExecutor:9] 2011-06-14 14:15:29,417 SSTableReader.java (line 155) Opening /data/cassandra/node1/data/JP/Datetest-g-2
ERROR [Thread-84] 2011-06-14 14:15:36,755 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-84,5,main]
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:132)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
ERROR [Thread-79] 2011-06-14 14:15:36,755 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-79,5,main]
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:132)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
ERROR [Thread-83] 2011-06-14 14:15:36,755 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-83,5,main]
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:132)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
ERROR [Thread-85] 2011-06-14 14:15:36,755 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-85,5,main]
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:132)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
",,skamio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jun/11 13:24;jbellis;2766-v2.txt;https://issues.apache.org/jira/secure/attachment/12482553/2766-v2.txt","14/Jun/11 09:37;jbellis;2766.txt;https://issues.apache.org/jira/secure/attachment/12482534/2766.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20816,,,Thu Jun 16 17:40:35 UTC 2011,,,,,,,,,,"0|i0gdbr:",93589,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"14/Jun/11 09:37;jbellis;looks like a straightforward case of ""not using threadsafe collections means there is no happens-before guarantee where a naive reading of the code would expect one"" (i.e., since buildFutures.add is always called before files.remove, after files.isEmpty is true there should be no more changes to buildFutures.add).

patch attached that changes both of these from Arraylist to CSLS.;;;","14/Jun/11 13:08;muga_nishizawa;Thanks for your quick response.  But we've encountered the following exception.    

ERROR [Thread-18] 2011-06-14 21:54:04,065 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-18,5,main]
java.lang.ClassCastException: org.apache.cassandra.streaming.PendingFile cannot be cast to java.lang.Comparable
        at java.util.concurrent.ConcurrentSkipListMap.comparable(ConcurrentSkipListMap.java:621)
        at java.util.concurrent.ConcurrentSkipListMap.doPut(ConcurrentSkipListMap.java:862)
        at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(ConcurrentSkipListMap.java:1893)
        at java.util.concurrent.ConcurrentSkipListSet.add(ConcurrentSkipListSet.java:202)
        at org.apache.cassandra.streaming.StreamInSession.addFiles(StreamInSession.java:100)
        at org.apache.cassandra.streaming.IncomingStreamReader.<init>(IncomingStreamReader.java:49)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
;;;","14/Jun/11 13:24;jbellis;oops. v2 w/ collections that don't require a comparable.;;;","14/Jun/11 14:06;slebresne;lgtm +1 on v2;;;","14/Jun/11 17:02;hudson;Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])
    use threadsafe collections for StreamInSession
patch by jbellis; reviewed by slebresne for CASSANDRA-2766

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1135611
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/streaming/StreamInSession.java
;;;","16/Jun/11 17:40;jbellis;(committed);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataTracker.View.MarkCompacting adds ALL sstables and marks them as compacting,CASSANDRA-2765,12510131,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bcoverston,bcoverston,bcoverston,13/Jun/11 19:17,16/Apr/19 09:32,14/Jul/23 05:52,13/Jun/11 21:22,0.8.1,,,,,,0,,,,"At some point if the list isn't cleaned up with this symptom compactions will stop until the server is restarted.
",,ambroff,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"13/Jun/11 19:27;bcoverston;0001-removed-code-adding-all-sstables-to-the-view-builder.patch;https://issues.apache.org/jira/secure/attachment/12482365/0001-removed-code-adding-all-sstables-to-the-view-builder.patch","13/Jun/11 21:09;jbellis;2765-v2.txt;https://issues.apache.org/jira/secure/attachment/12482460/2765-v2.txt",,,,,,,,,,,,,2.0,bcoverston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20815,,,Tue Jun 14 17:02:36 UTC 2011,,,,,,,,,,"0|i0gdbj:",93588,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"13/Jun/11 19:27;bcoverston;Attached patch removes code that adds all sstables to the compacting set.;;;","13/Jun/11 21:09;jbellis;I think the intention was to add the new tomark set to the existing currently-being-compacted set.  v2 attached.;;;","13/Jun/11 21:14;bcoverston;Good call, I missed that part :).;;;","13/Jun/11 21:22;bcoverston;+1;;;","13/Jun/11 21:22;jbellis;committed;;;","14/Jun/11 17:02;hudson;Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
generate-eclipse-files still referencing drivers/ source,CASSANDRA-2764,12510076,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,kirktrue,kirktrue,kirktrue,13/Jun/11 05:00,16/Apr/19 09:32,14/Jul/23 05:52,13/Jun/11 11:59,1.0.0,,,,,,0,,,,"In trunk, running ant generate-eclipse-files will reference the old drivers top-level directory. The result is that the generated project, once loaded into Eclipse causes errors about the non-existent source directories.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/11 05:01;kirktrue;trunk-2764.txt;https://issues.apache.org/jira/secure/attachment/12482322/trunk-2764.txt",,,,,,,,,,,,,,1.0,kirktrue,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20814,,,Tue Jun 14 17:02:37 UTC 2011,,,,,,,,,,"0|i0gdbb:",93587,,,,,Low,,,,,,,,,,,,,,,,,"13/Jun/11 05:01;kirktrue;Patch which removes the two directories from the Eclipse project generation.;;;","13/Jun/11 11:59;jbellis;committed, thanks!;;;","13/Jun/11 12:20;hudson;Integrated in Cassandra #923 (See [https://builds.apache.org/job/Cassandra/923/])
    r/m drivers/ from generate-eclipse-files target
patch by Kirk True for CASSANDRA-2764

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1135104
Files : 
* /cassandra/trunk/build.xml
;;;","14/Jun/11 17:02;hudson;Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Token cannot contain comma (possibly non-alpha/non-numeric too?) in OrderPreservingPartitioner,CASSANDRA-2762,12509893,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,cywjackson,cywjackson,11/Jun/11 01:54,16/Apr/19 09:32,14/Jul/23 05:52,06/Jul/11 17:11,0.7.7,0.8.2,,,,,0,,,,"It'd appear that when the token contain comma in the OrderPreservingPartitioner case, C* will fail with assert error:

ERROR [GossipStage:1] 2011-06-09 16:01:05,063 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[GossipStage:1,5,main]
java.lang.AssertionError
    at org.apache.cassandra.service.StorageService.handleStateBootstrap(StorageService.java:685)
    at org.apache.cassandra.service.StorageService.onChange(StorageService.java:648)
    at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:772)
    at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:737)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:679)
    at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:60)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/11 15:51;jbellis;2762-v2.txt;https://issues.apache.org/jira/secure/attachment/12483612/2762-v2.txt","20/Jun/11 21:40;jbellis;2762.txt;https://issues.apache.org/jira/secure/attachment/12483212/2762.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20813,,,Wed Jul 06 16:14:36 UTC 2011,,,,,,,,,,"0|i0gdav:",93585,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"11/Jun/11 12:52;jbellis;{code}
    // this must be a char that cannot be present in any token
    public final static char DELIMITER = ',';
    public final static String DELIMITER_STR = new String(new char[] { DELIMITER });
{code}

Changing to something more obscure would be a backwards-incompatible change. Not worth it.;;;","16/Jun/11 03:54;jbellis;However we should make sure that we fail-fast invalid initial_token setting, as well as do not hand out bad tokens during auto bootstrap token selection;;;","20/Jun/11 21:40;jbellis;patch adds TokenFactory.validate, calls it on initial_token or move input, and adds code to prevent commas in autogenerated tokens;;;","23/Jun/11 15:51;jbellis;v2 does token hacking post-midpoint instead of in midpoint itself, which is confusing (and unnecessary for merkle tree uses of midpoint which is the main one).;;;","06/Jul/11 15:15;slebresne;nit: the comment "" // Hack to prevent giving nodes tokens with strings in them"" should probably read "" // Hack to prevent giving nodes tokens with DELIMITER_STR in them""?

+1;;;","06/Jul/11 15:57;jbellis;committed w/ that change.

also added a final check post-replaceall:

{code}
            if (tokenMetadata_.getTokenToEndpointMap().containsKey(token))
                throw new RuntimeException(""Unable to compute unique token for new node -- specify one manually with initial_token"");
{code};;;","06/Jul/11 15:59;jbellis;I also note for the record that if you must use an ordered partitioner, you should almost certainly be using ByteOrderedPartitioner instead of the obsolete OPP.;;;","06/Jul/11 16:14;hudson;Integrated in Cassandra-0.7 #524 (See [https://builds.apache.org/job/Cassandra-0.7/524/])
    ensure that string tokens do not contain commas
patch by jbellis; reviewed by slebresne for CASSANDRA-2762

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1143476
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/tools/NodeProbe.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/dht/Token.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/StorageServiceMBean.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/dht/RandomPartitioner.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/dht/AbstractByteOrderedPartitioner.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC driver does not build,CASSANDRA-2761,12509880,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ardot,jbellis,jbellis,10/Jun/11 21:36,16/Apr/19 09:32,14/Jul/23 05:52,25/Aug/11 14:27,,,,Legacy/CQL,,,2,,,,"Need a way to build (and run tests for) the Java driver.

Also: still some vestigal references to drivers/ in trunk build.xml.

Should we remove drivers/ from the 0.8 branch as well?",,ardot,gdusbabek,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/11 03:25;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2761-cleanup-nits.txt;https://issues.apache.org/jira/secure/attachment/12486401/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2761-cleanup-nits.txt","13/Jun/11 03:57;ardot;jdbc-driver-build-v1.txt;https://issues.apache.org/jira/secure/attachment/12482319/jdbc-driver-build-v1.txt",,,,,,,,,,,,,2.0,ardot,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20812,,,Thu Aug 25 02:42:17 UTC 2011,,,,,,,,,,"0|i0gdan:",93584,,urandom,,urandom,Normal,,,,,,,,,,,,,,,,,"12/Jun/11 15:04;ardot;Moved from CASSANDRA-2754.

Just a few clarifications as I am not that familiar with the C* infrastructure and build policies etc...

- I'm a Maven guy, but I guess that is out of the question?
- This needs to be Jenkins ready?
- This should build Source and Javadocs artifacts ready for Maven Repo deployment right?
- The resulting JDBC jar file would contain the C* dependancies from the build directory pointed to by the properties file/system property? It would be self contained?
- The testing part of the build is going to have to reach into the C* build area too I think?;;;","12/Jun/11 17:41;urandom;bq. I'm a Maven guy, but I guess that is out of the question?

I don't know, but if I were a Maven Guy (I am not), I would use Ant anyway.  The vast majority of the active committers wish that Maven would die in a fire, so using it would be the fastest way to engender apathy. 

bq. This needs to be Jenkins ready?

Not sure what this means.

bq. This should build Source and Javadocs artifacts ready for Maven Repo deployment right?

Yes.

bq. The resulting JDBC jar file would contain the C* dependancies from the build directory pointed to by the properties file/system property? It would be self contained?

Yes.

bq. The testing part of the build is going to have to reach into the C* build area too I think?

The classpath for tests will probably contain a few additional items that the runtime classpath does not.;;;","13/Jun/11 03:57;ardot;OK, Patch for an ANT Build for the JDBC Driver suite is attached. 

It builds all the artifacts, but at the moment it does NOT include any other classes but the ones in the o.a.c.cql.jdbc package.

I will update when a few more clarifications of exactly what dependancies we want to declare for the driver and what we want to include in the jar.

I think I got the Eclipse stuff in ther as well, but that could probably use some wringing out.. I was able to successfully run the junit tests from Eclipse.;;;","13/Jun/11 19:39;urandom;Thanks Rick, this is a good start, and a lot better than having no build at all, so I went ahead and committed it.

I made a fewer minor changes (mostly replacing tabs for spaces), so you'll need to rebase against svn before continuing.

bq. I will update when a few more clarifications of exactly what dependancies we want to declare for the driver and what we want to include in the jar.

There is probably some handy tool for this, but I don't know what it is.  Just looking at it I'd say you need to copy-in:

* {{o.a.c.db.marshall.*}}
* {{o.a.c.utils.ByteBufferUtil}}
* {{o.a.c.config.ColumnDefinition}}
* {{o.a.c.config.CFMetaData}}
* {{o.a.c.config.ConfigurationException}}

There might be some transitive dependencies though.;;;","13/Jun/11 20:08;ardot;Thanks Eric. 

Actually you list is the same as mine so we are probably close. Note you will still need the whole C*-Thrift jar. I will build a new driver with the additional classes and test it without depending on the full C*. I'll document the full dependancy list here along with the next patch.;;;","13/Jun/11 20:12;urandom;bq. Note you will still need the whole C*-Thrift jar.

Yeah, but don't copy those.  The driver jar will need to depend on the thrift jar.;;;","15/Jun/11 15:27;ardot;The v2 patch adds inclusion of classes that are required on the client side from the main C* build. ;;;","15/Jun/11 20:02;ardot;I put together a patch that covered the obvious but I had strange problems in test. So I dug deeper and found a ton (over 50 I think) additional transitive dependencies that I had missed... (silly me) 

I can try an get them all in but I think we should seriously rethink that strategy for now. With this many dependancies they should probably be put in their own jar(s). It would really make the driver jar have to keep up with detailed dependency changes in the server code base. Miss one and it's messy and the errors are non-obvious.

I was a big whiner about not carrying around the whole server just for client access, but until it is re-factored, I can now appreciate the horror that will commence if we piece-meal drag over classes from the server into the JAR.

So fo now I suggest we keep the jar the way it is with just the o.a.c.cql.jdbc.* classes.;;;","16/Jun/11 16:17;urandom;{quote}
I can try an get them all in but I think we should seriously rethink that strategy for now. With this many dependancies they should probably be put in their own jar(s). It would really make the driver jar have to keep up with detailed dependency changes in the server code base. Miss one and it's messy and the errors are non-obvious.
{quote}

What would you call such a jar?  When I looked at this, there didn't seem to be any delineation that made sense.

{quote}
I was a big whiner about not carrying around the whole server just for client access, but until it is re-factored, I can now appreciate the horror that will commence if we piece-meal drag over classes from the server into the JAR.
{quote}

What are these 50 other class dependencies?  Where are they being drug in?;;;","17/Jun/11 03:30;ardot;{quote}
What are these 50 other class dependencies? Where are they being drug in?
{quote}

- o.a.c.db.marshall.*
-- 23 various classes
- o.a.c.utils.ByteBufferUtil
-- org.apache.cassandra.io.util.FileDataInput
-- org.apache.cassandra.io.util.FileUtils
- o.a.c.config.ColumnDefinition
- o.a.c.config.CFMetaData
-- org.apache.cassandra.cache.IRowCacheProvider;
-- org.apache.cassandra.db.migration.avro.ColumnDef
-- org.apache.cassandra.db.ColumnFamilyType;
--- org.apache.cassandra.concurrent.JMXEnabledThreadPoolExecutor
--- org.apache.cassandra.config.DatabaseDescriptor
---- org.apache.cassandra.auth.AllowAllAuthenticator
---- org.apache.cassandra.auth.AllowAllAuthority
---- org.apache.cassandra.auth.IAuthenticator
---- org.apache.cassandra.auth.IAuthority
---- org.apache.cassandra.config.Config.RequestSchedulerId
---- org.apache.cassandra.db.ColumnFamilyStore
----- 13 new items
---- org.apache.cassandra.db.ColumnFamilyType
---- org.apache.cassandra.db.DefsTable
----- org.apache.cassandra.config.DatabaseDescriptor;
----- org.apache.cassandra.config.KSMetaData;
----- org.apache.cassandra.db.filter.QueryFilter;
----- org.apache.cassandra.db.filter.QueryPath;
----- org.apache.cassandra.db.migration.Migration;
----- org.apache.cassandra.io.SerDeUtils;
----- org.apache.cassandra.service.StorageService;
----- org.apache.cassandra.utils.ByteBufferUtil;
----- org.apache.cassandra.utils.UUIDGen;
---- org.apache.cassandra.db.migration.Migration
---- org.apache.cassandra.dht.IPartitioner
---- org.apache.cassandra.io.sstable.Descriptor
---- org.apache.cassandra.io.util.FileUtils
---- org.apache.cassandra.locator.*
---- org.apache.cassandra.scheduler.IRequestSchedule;
---- org.apache.cassandra.scheduler.NoScheduler
--- org.apache.cassandra.db.compaction.CompactionManager
--- org.apache.cassandra.db.filter.QueryFilter
--- org.apache.cassandra.db.filter.QueryPath
--- org.apache.cassandra.dht.IPartitioner
--- org.apache.cassandra.dht.Range
--- org.apache.cassandra.gms.FailureDetector
--- org.apache.cassandra.gms.Gossiper
--- org.apache.cassandra.gms.ApplicationState
--- org.apache.cassandra.net.MessagingService
---- 10 new classes
--- org.apache.cassandra.service.*
--- org.apache.cassandra.utils.WrappedRunnable
-- org.apache.cassandra.db.HintedHandOffManager;
-- org.apache.cassandra.db.SystemTable;
-- org.apache.cassandra.db.Table;
-- org.apache.cassandra.db.ColumnFamilyStore;
-- org.apache.cassandra.db.migration.Migration;
-- org.apache.cassandra.db.compaction.AbstractCompactionStrategy;
-- org.apache.cassandra.io.SerDeUtils;
-- org.apache.cassandra.utils.Pair;
- o.a.c.config.ConfigurationException


The point is there are lots and they are scattered all over the various packages; It will be very difficult to manage when they change from the driver package (client side), which is supposed to be able to change independent of the server code. If a subset of the server code is to be a dependency then that subset (jar/s) must be managed in the main build not the driver build. 


{quote}
What would you call such a jar? When I looked at this, there didn't seem to be any delineation that made sense.
{quote}

I agree it is not any clear set of packages. They are scattered all over.

As to a name for the jar... I'm not a good namer in the best of circumstances but I think the intent is to pick those files that are used in common between client and server. I guess I'd use that as the basis for the name.


;;;","17/Jun/11 15:31;urandom;{quote}
The point is there are lots and they are scattered all over the various packages; It will be very difficult to manage when they change from the driver package (client side), which is supposed to be able to change independent of the server code. If a subset of the server code is to be a dependency then that subset (jar/s) must be managed in the main build not the driver build.
{quote}

Right, I was curious to see the list of classes (that list is fantastic btw, thanks for that), to see if there was one point in the graph where breaking a dependency would drastically change the scope of the problem.  It looks like the answer is Yes, and the dependency is {{o.a.c.config.CFMetaData}}, (needed by {{ColumnDecoder}}).

Just skimming through the code, I don't think it would be hard to either re-implement the needed parts of CFMetaData, or refactor CFMetaData to limit what it pulled in.;;;","05/Jul/11 19:13;jbellis;What's the status here?  Not having ""ant test"" for the drivers anymore is causing me pain and suffering. :(;;;","05/Jul/11 20:16;ardot;Sorry. I didn't catch that there was no ant task to RUN them... :) {{build-test}} builds the included tests just fine. And I have been running them from Eclipse once built. I'll look into it. ;;;","14/Jul/11 03:29;urandom;Attached (v1-0001-CASSANDRA-2761-cleanup-nits.txt) is a patch to do a little cleanup (clearer variable naming, some simplification of classpaths, etc), basically stuff that set my OCD off and seemed better to get in *before* diving into test running and dependency management.;;;","21/Jul/11 22:20;urandom;To summarize, it is now possible to build and test w/ ant.  This is currently done by pointing to a local (built) working copy of Cassandra (a site config).  What's left, and seems reasonable to scope with this issue:

* Create an alternate mechanism for specifying the version of Cassandra to build/test against (in order to run the tests against prior releases).  I'm thinking Ivy could be used here to automatically download artifacts when a property is passed (-Dcassandra.release=0.8.0 for example).
* (Re)build Cassandra as needed from the drivers Ant build, or at the very least, handle the case when a build is needed.
* Fix the {{generate-eclipse-files}} target if possible, or remove it otherwise.

Work should also continue to reduce the cross-section of Cassandra that this driver depends on, but I'll open another issue for that.;;;","22/Jul/11 00:27;jbellis;+1 cleanup patch;;;","22/Jul/11 02:07;ardot;+1 for the cleanup patch

The {{generate-eclipse-files}} seems to be working for me? How does it fail?

;;;","10/Aug/11 15:35;jbellis;Note that CASSANDRA-3010 makes moving drivers out-of-""tree"" even sillier: as things stand, we'll need to grab cassandra from the maven repo to build jdbc, then back in tree we'll need to grab jdbc from the maven repo to build the cql shell.

I'm all for purity but at this point I'm ready to just put things back the way they were* so we can spend time before the 1.0 freeze building things instead of wrestling with maven.
;;;","10/Aug/11 15:53;urandom;The lib/ directory is full of dependency jars, some which exist solely for the cli.  ;;;","10/Aug/11 16:01;brandon.williams;bq. I'm all for purity but at this point I'm ready to just put things back the way they were* so we can spend time before the 1.0 freeze building things instead of wrestling with maven.

I for one would be delighted if I didn't have to use svn to get at the drivers or cqlsh.;;;","10/Aug/11 16:04;jbellis;bq. The lib/ directory is full of dependency jars, some which exist solely for the cli.

But we are updating none of those on a daily or weekly basis.;;;","10/Aug/11 16:41;urandom;bq. But we are updating none of those on a daily or weekly basis

I certainly hope we're not updating the JDBC driver that often, particularly in ways that would impact a simple application like a shell.

Our experience using the JDBC driver as an application dependency shouldn't be any better or worse than it would be for other users.;;;","10/Aug/11 17:30;jbellis;bq. I certainly hope we're not updating the JDBC driver that often

Then you're not very familiar with the current stability or lack there of of the JDBC driver. :);;;","10/Aug/11 20:53;urandom;bq. Then you're not very familiar with the current stability or lack there of of the JDBC driver. 

No, I guess not, but it can't be any worse on us than for anyone else using it.  A solution that only benefits us doesn't seem very... friendly.

----

I think the difference in opinion here comes from what determines whether the server and driver should be considered different projects.  I can see where people who feel a high degree of ownership over the code of both, and who by virtue of infrastructure have write access to both, might consider it contrived to treat them as separate projects.  I'm not discounting that point of view, but I do think that's more Social than Technical, and is limited to a relatively small group of Cassandra hackers.

As I've said elsewhere, I think it is very important that these drivers be allowed to evolve on their own release schedule with their own versioning that reflects compatibility with a CQL version and not any particular Cassandra version(s).  It's also quite likely, particularly where the driver language != Java that the group of developers is entirely different from those working on the server.  And there is no hard dependency between drivers and Cassandra in either direction (the JDBC->Cassandra dependency is one of convenience).  To me, this pretty solidly points to them being separate projects.

Keeping them separate won't be as convenient as treating them as one monolithic project, but it's no worse an experience than what other application developers are subjected to.  We should be able to eat our own dog food.

I also realize that there is more work needed here to decouple the JDBC driver and make this all work better, work that I've volunteered to do.  I haven't had as much time to spend on this lately, but that should be changing RSN.

;;;","24/Aug/11 13:17;tjake;Is their someway to get a git repo for just drivers? like cassandra.git but cassandra-drivers.git? This is causing major pain.;;;","24/Aug/11 18:19;jbellis;bq. I also realize that there is more work needed here to decouple the JDBC driver and make this all work better, work that I've volunteered to do. I haven't had as much time to spend on this lately, but that should be changing RSN.

It's been another two weeks and we're still in this no-man's land where git can't see drivers, the jdbc build is a big TODO, and anything that depends on jdbc like 3010 is basically SOL.

Again, I'm not against purity, but it's clear that the original breaking out of drivers/ was done prematurely (was there even a Jira ticket with a patch? It looks like we went from ""that's a good idea"" on -dev to ripping apart svn).  I've reverted things until the breakout can be done properly.;;;","24/Aug/11 19:20;hudson;Integrated in Cassandra #1045 (See [https://builds.apache.org/job/Cassandra/1045/])
    move drivers back in-tree until build issues can be fixed.  see CASSANDRA-2761

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1161216
Files : 
* /cassandra/trunk/interface/thrift/gen-java/org/apache/cassandra/thrift/Column.java
* /cassandra/trunk/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
* /cassandra/trunk/interface/thrift/gen-java/org/apache/cassandra/thrift/InvalidRequestException.java
* /cassandra/trunk/build.xml
* /cassandra/trunk/contrib
* /cassandra/trunk/drivers/java/README.txt
* /cassandra/trunk/interface/thrift/gen-java/org/apache/cassandra/thrift/SuperColumn.java
* /cassandra/trunk/drivers
* /cassandra/trunk/interface/thrift/gen-java/org/apache/cassandra/thrift/NotFoundException.java
* /cassandra/trunk
* /cassandra/trunk/drivers/java/build.properties.default
* /cassandra/trunk/drivers/java/build.xml
;;;","24/Aug/11 19:56;ardot;I don't see any driver code in the move? The {{/driver}} sub-directory now exists but there is nothing underneath?;;;","24/Aug/11 20:10;jbellis;https://svn.apache.org/repos/asf/cassandra/trunk/drivers/;;;","24/Aug/11 20:43;urandom;{quote}
It's been another two weeks and we're still in this no-man's land where git can't see drivers
{quote}

So does this mean you've decided Git is hard a requirement?  Our project is (unfortunately )managed in SVN.  You know if it were up to me we'd be using Git for source control, but it's not up to me and we don't.

{quote}
...anything that depends on jdbc like 3010 is basically SOL.
{quote}

As I've already mentioned, CASSANDAR-3010 is for a JDBC using _application_.  If it's SOL unless the driver is embedded in the tree, then so is every other application that would make use of the driver.  If the driver is so bad, maybe no one should be using it.  Why are we giving ourselves special treatment?

{quote}
Again, I'm not against purity...
{quote}

By saying ""purity"" it sounds like you're dismissing it as something that's based on aesthetics.  I assure that's not the case, it's about avoiding the inevitable lock-step relationship release-wise. 

{quote}
...but it's clear that the original breaking out of drivers/ was done prematurely (was there even a Jira ticket with a patch?
{quote}

There was a Jira yes, though I'm pretty sure it done as part of larger set of tasks, something with a description other than ""relocate drivers out of tree"".  And, I don't think it was in a patch, no, due to the fact that a large move with some minor changes was (a) straightforward, and (b) would have needed rebasing several times a day.

{quote}
It looks like we went from ""that's a good idea"" on -dev to ripping apart svn). I've reverted things until the breakout can be done properly.
{quote}

So all I need to do is find a ticket and a +1 and I can -1 this, and revert your revert?
;;;","24/Aug/11 21:00;tjake;bq.  Our project is (unfortunately )managed in SVN

putting a sub-project in a svn root isn't the way to do it for SVN.

Another issue is the drivers build depends on cassandra.  That makes this even more strange. Can't drivers live in /trunk and still have it's own releases?

;;;","24/Aug/11 21:13;jbellis;bq. There was a Jira yes

I couldn't find one, and the svn commit messages didn't mention it.  I still can't find one.

bq. So all I need to do is find a ticket and a +1 and I can -1 this, and revert your revert?

The point of making a ticket with a patch (or a shell script if you're throwing svn refactoring around -- you can do mv's from a working copy, not just on the repository, btw) is that people can review it and see if the result matches what they thought they were going to get out of it.  In this case it's clear that it didn't, and we ended up with making things worse in exchange for a promise to clean it up eventually.

(Of course, even with proper review, sometimes we've needed to revert things after unexpected problems arose.  But skipping the review makes that more likely.)
 
It's been *over two months.*  So let's reboot, and do it right.  Note that this time around I got everything in one commit (well, one for trunk, and one for drivers) so ""reverting the revert"" will be easy.;;;","24/Aug/11 21:16;urandom;bq. putting a sub-project in a svn root isn't the way to do it for SVN.

What is?

bq. Another issue is the drivers build depends on cassandra. That makes this even more strange. Can't drivers live in /trunk and still have it's own releases?

We had it that way originally.  It made releasing a pain, and left people with the expectation that the driver version to use was the one that corresponded to source it was with (which is unavoidable).;;;","24/Aug/11 21:17;jbellis;bq. Can't drivers live in /trunk and still have it's own releases?

Right, that seems like the best interim solution to me.  (I thought we could even delete it from old branches to make it clear that it's not in lockstep with the rest of the tree, but Eric pointed out that if something like 3010 is depending on it, we can't do that.  Still, having it present but ""frozen"" in the branches feels like a relatively minor downside.);;;","24/Aug/11 21:22;urandom;How do we reboot and do it right?  Your requirements as I understand them are structured in such a way that keeping them in-tree is the only solution.  ;;;","24/Aug/11 21:22;jbellis;bq. left people with the expectation that the driver version to use was the one that corresponded to source it was with

I suspect this was when svn was effectively the only way to get a driver.  My experience is that even most people building ""from source"" use a tarball, not svn.  So the combination of making drivers available on maven, PyPI, etc., with their own version numbers, should address this.;;;","24/Aug/11 21:24;urandom;bq. Right, that seems like the best interim solution to me. (I thought we could even delete it from old branches to make it clear that it's not in lockstep with the rest of the tree, but Eric pointed out that if something like 3010 is depending on it, we can't do that. Still, having it present but ""frozen"" in the branches feels like a relatively minor downside.)

Except that people's expectation will always be that the version they need is the one that came with their software (which will likely be something pre-release).  It is completely unrealistic to expect folks to just Know Better here. ;;;","24/Aug/11 21:38;jbellis;bq. people's expectation will always be that the version they need is the one that came with their software

I don't understand.  What version will ""come with their [server]?""

Consider postgresql: the server is distributed on postgresql.org, and psycopg is distributed over PyPI.  Nobody gets confused about staying on an obsolete version of psycopg.

That's the model I see us moving towards.  (As you know, we recently got the Python cql driver on PyPI.);;;","24/Aug/11 21:52;jbellis;One final thought: by coincidence, we broke the JDBC build again today with CASSANDRA-3039.  This isn't the first time this has happened.  It's a minor win but not negligible to catch those before commit because ""ant test"" runs both suites.;;;","24/Aug/11 22:07;urandom;{quote}
I don't understand. What version will ""come with their [server]?""
{quote}

Wherever we publish the source, be it an SVN checkout with an SVN revision ID, a date-based development snapshot, or a full-on release artifact, if there is driver source contained within then people are going to be encouraged to think of those drivers as being the same version as the node (e.g. 0.8.9).  They're also going to be encouraged to think that those drivers are the best choice to use with the corresponding node.  It's futile to think they won't, and having other vectors (www.a.o, PyPI, etc), will only add to the confusion. 

{quote}
Consider postgresql: the server is distributed on postgresql.org, and psycopg is distributed over PyPI. Nobody gets confused about staying on an obsolete version of psycopg.

That's the model I see us moving towards. (As you know, we recently got the Python cql driver on PyPI.)
{quote}

You make an excellent point.  Pyscopg is maintained in a completely different repository (git://luna.dndg.it/public/psycopg2.git) than Postgesql (git://git.postgresql.org/git/postgresql.git) and is released (and published) separately, (which is in fact consistent with best practice elsewhere).;;;","24/Aug/11 22:12;urandom;bq. One final thought: by coincidence, we broke the JDBC build again today with CASSANDRA-3039. This isn't the first time this has happened. It's a minor win but not negligible to catch those before commit because ""ant test"" runs both suites.

Broke why?  Because of Cassandra code that the driver depends on?  That would be an argument in favor of stabalizing the dependent code.

And, the inverse of this is that the drivers should ultimately be getting tested against trunk, each active branch, and all past released versions (limited of course to CQL availability).  That is only going to be practical through CI, and is made harder by your monolithic approach.;;;","24/Aug/11 22:19;jbellis;bq. if there is driver source contained

Almost all non-developers get the source from a release tarball, not svn.  Do we publish drivers/ in the source tarballs?  If so that's easy enough to fix.

Anyone actually using svn I'm willing to educate.  You can give them my email. :)

bq. Pyscopg is maintained in a completely different repository

But as far as users are concerned that is irrelevant.

If you want examples of drivers in the same tree that are also not causing confusion, I can point you to https://github.com/mongodb/mongo and https://github.com/mongodb/mongo/tree/master/client.  Also http://hg.basho.com/riak/src/5ffa6ae7e699 (http://hg.basho.com/riak/src/5ffa6ae7e699/client_lib/) and https://github.com/voldemort/voldemort (https://github.com/voldemort/voldemort/tree/master/clients).;;;","24/Aug/11 22:44;urandom;So to summarize: You've decided.

I know that sounds a bit snarky, but you summarily reverted the change, in part based on reasoning that wasn't true (it doesn't build), and in part pending a ""reboot"" to meet unmeetable requirements.;;;","25/Aug/11 02:27;jbellis;If waiting for two months of ""we'll fix it real soon now"" is ""summarily,"" then yeah, I guess guilty as charged.

But it looks like you've restarted discussion on -dev, so I'll move there.;;;","25/Aug/11 02:42;urandom;Up until the July 21st, the scope of the ticket was building and running the unit tests (and as of July 21st that much was working).  It wasn't until Aug 10th (and the creation of CASSANDRA-3010) that the discussion (and scope) changed.  Between the 10th and today there was no response to my last, then 4 hours after Jake's comment the revert was made.

I already told you once today that I ascribed no malice in this, and I don't, but I think ""summarily"" is a fair assessment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in sstable2json,CASSANDRA-2760,12509854,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasobrown,daningaddr,daningaddr,10/Jun/11 17:48,16/Apr/19 09:32,14/Jul/23 05:52,27/Jun/12 00:53,1.1.2,,,Legacy/Tools,,,0,,,,"./sstable2json /var/lib/cassandra/data/test/snapshots/1307649033076/User-g-4-Data.db 
{
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamily.<init>(ColumnFamily.java:82)
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:70)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:142)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:90)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:74)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:179)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:144)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:136)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:313)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:344)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:357)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:415)
",,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/12 00:06;jasobrown;0001-cassandra-2760.patch;https://issues.apache.org/jira/secure/attachment/12533139/0001-cassandra-2760.patch","10/Jun/11 17:51;daningaddr;User-g-4-Data.db;https://issues.apache.org/jira/secure/attachment/12482083/User-g-4-Data.db",,,,,,,,,,,,,2.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20811,,,Wed Jun 27 00:53:39 UTC 2012,,,,,,,,,,"0|i0gdaf:",93583,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"10/Jun/11 17:57;jbellis;Sounds like you're trying to run sstable2json on a columnfamily that isn't present in your schema.;;;","23/Jun/12 00:06;jasobrown;Instead of trying to process an unknown column family (and exiting with an exception), load the Descriptor at the top of the export process and check to see if the keyspace/column family exists inside the current cassandra setup. If not, print a warning message and exit.;;;","23/Jun/12 00:07;jasobrown;Note: I worked on this patch against the current (1.2) trunk. I can backport it to earlier versions, if we need.;;;","27/Jun/12 00:53;jbellis;patch applies cleanly to 1.1, so committed there as well.  thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrub could lose increments and replicate that loss,CASSANDRA-2759,12509845,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,10/Jun/11 16:20,16/Apr/19 09:32,14/Jul/23 05:52,14/Jun/11 09:18,0.8.1,,,,,,0,counters,,,"If scrub cannot 'repair' a corrupted row, it will skip it. On node A, if the row contains some sub-count for A id, those will be lost forever since A is the source of truth on it's current id. We should thus renew node A id when that happens to avoid this (not unlike we do in cleanup).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jun/11 07:38;slebresne;0001-Don-t-skip-rows-on-scrub-for-counter-CFs.patch;https://issues.apache.org/jira/secure/attachment/12482523/0001-Don-t-skip-rows-on-scrub-for-counter-CFs.patch","10/Jun/11 16:23;slebresne;0001-Renew-nodeId-in-scrub-when-skipping-rows.patch;https://issues.apache.org/jira/secure/attachment/12482074/0001-Renew-nodeId-in-scrub-when-skipping-rows.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20810,,,Tue Jun 14 17:02:37 UTC 2011,,,,,,,,,,"0|i0gda7:",93582,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"10/Jun/11 16:23;slebresne;Attached patch against 0.8.

The patch also add a new startup option to renew the node id on startup. This could be useful if someone lose one of it's sstable (because of a bad disk for instance) and don't want to fully decommission that node.

This could arguably be splitted in another ticket though.;;;","10/Jun/11 16:28;jbellis;what is ""renewing a node id?"";;;","10/Jun/11 16:54;slebresne;It's picking a new UUID for the current node to use for new counter increment.

The problem is that on a given node we store deltas for it's current nodeId (to avoid synchronized read-before-write, but I'm starting to wonder is that was the smartest ever). Anyway, if scrub skips a row, it may skip some of those deltas. Let's say at first there is no increments coming for this row for A as 'first distinguished replica'. So far we are still kind of good, because on a read (with CL > ONE) the result coming from A will have a 'version' for it's own sub-count smaller that the one on the other replica, so we will us the sub-count on those replica and return the correct value.

However, as soon as A acknowledge new increments for this row, it will start inserting new deltas while he is not intrinsically up to date. Which will result in an definitive undercount.

The goal of renewing the node id of A is to make sure that second part never happen (because after the renew A will add new deltas as A', not A anymore).

Anyway, now that I've plugged the brain this patch doesn't really works because A will never be repaired by the other nodes of it's now inconsistent value.

So I have no clue how to actually fix that.;;;","10/Jun/11 17:21;slebresne;It may be that the best short fix here is to make scrub *not* skipping row on counter column families (though CASSANDRA-2614 would change that to 'never ever skipping row') and just throw a RuntimeException.;;;","10/Jun/11 20:10;jbellis;bq. make scrub not skip rows on counter column families

+1

bq. CASSANDRA-2614 would change that to 'never ever skipping row'

Only if you actually did have a counter in the column_metadata, right?;;;","14/Jun/11 07:38;slebresne;Attaching patch to simply re-throw the exception instead of skipping the row for counter column families.

bq. Only if you actually did have a counter in the column_metadata, right?

right.;;;","14/Jun/11 08:50;jbellis;+1

can you add a link to this issue in the ""dangerous"" comment?;;;","14/Jun/11 09:18;slebresne;Committed with suggested comment update.;;;","14/Jun/11 17:02;hudson;Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool repair never finishes. Loops forever through merkle trees?,CASSANDRA-2758,12509829,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,terjem,terjem,10/Jun/11 13:39,16/Apr/19 09:32,14/Jul/23 05:52,15/Jun/11 11:55,0.8.1,,,,,,0,,,,"I am not sure all steps here is needed, but as part of testing something else, I set up
node1: initial_token: 1
node2: initial_token: 5

Then:
{noformat}
create keyspace myks 
 with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy'
 with strategy_options = [{ replication_factor:2 }];

use myks;

create column family test with comparator = AsciiType and column_metadata=[ {column_name: 'up_', validation_class: LongType, index_type: 0}, {column_name: 'del_', validation_class: LongType, index_type: 0} ]
 and keys_cached = 100000 and rows_cached = 10000 and min_compaction_threshold = 2;
quit;
{noformat}

Doing nodetool repair after this gets both nodes busy looping forever.

A quick look at one node in eclipse makes me guess its having fun spinning through  merkle trees, but I have to admit I have not look at it for a long time.





",,colinkuo,daningaddr,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jun/11 09:58;slebresne;0001-Fix-MerkleTree.init-to-not-create-non-sensical-trees.patch;https://issues.apache.org/jira/secure/attachment/12482537/0001-Fix-MerkleTree.init-to-not-create-non-sensical-trees.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20809,,,Fri Jun 17 02:20:45 UTC 2011,,,,,,,,,,"0|i0gd9z:",93581,,stuhood,,stuhood,Low,,,,,,,,,,,,,,,,,"14/Jun/11 09:58;slebresne;MerkleTree.init(), which is used to create the merkle tree in case there is no data, was creating a nonsensical tree by stopping it's iteration too late.

Attached patch to fix (and dumping priority to minor because it has very little chance to hit anyone in any real-life situation).;;;","14/Jun/11 20:15;stuhood;+1, thanks!;;;","15/Jun/11 11:55;slebresne;Committed, thanks;;;","17/Jun/11 02:20;hudson;Integrated in Cassandra-0.8 #173 (See [https://builds.apache.org/job/Cassandra-0.8/173/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyRecordWriter fails to throw a write exception encountered after the user begins to close the writer,CASSANDRA-2755,12509754,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mck,gregkatz,gregkatz,09/Jun/11 19:40,16/Apr/19 09:32,14/Jul/23 05:52,28/Jun/11 12:27,0.7.7,0.8.2,,,,,0,,,,"There appears to be a race condition in {{ColumnFamilyRecordWriter}} that can result in the loss of an exception. Here is how it can happen (W stands for the {{RangeClient}}'s worker thread; U stands for the {{ColumnFamilyRecordWriter}} user's thread):

# W: {{RangeClient}}'s {{run}} method catches an exception originating in the Thrift client/socket, but doesn't get a chance to set it on the {{lastException}} field before it the thread is preempted.
# U: The user calls {{close}} which calls {{stopNicely}}. Because the {{lastException}} field is null, {{stopNicely}} does not throw anything. {{close}} then joins on the worker thread.
# W: The {{RangeClient}}'s {{run}} method sets the {{lastException}} field and exits.
# U: Although the thread in {{close}} is waiting for the worker thread to exit, it has already checked the {{lastException}} field so it doesn't detect the presence of the last exception. Instead, {{close}} returns without throwing anything.

This race condition means that intermittently write failures will go undetected.",,gregkatz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/11 15:12;jbellis;2755-v2.txt;https://issues.apache.org/jira/secure/attachment/12482343/2755-v2.txt","12/Jun/11 10:28;mck;CASSANDRA-2755.patch;https://issues.apache.org/jira/secure/attachment/12482295/CASSANDRA-2755.patch",,,,,,,,,,,,,2.0,mck,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20807,,,Tue Jun 28 13:24:34 UTC 2011,,,,,,,,,,"0|i0gd9b:",93578,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"12/Jun/11 10:28;mck;In RangeClient i cannot see why close() needs to be called before lastException is assigned. The following patch should work: I have tested it against various jobs but i have no reproducible testcase to confirm this bug against.

Also in the patch is a slight cleanup to ColumnFamilyRecordWriter's close() methods: keeping implementation out of deprecated methods.;;;","13/Jun/11 15:12;jbellis;It looks to me that as long as we check for the exception before calling join, there will be a window to miss one.

v2 encapsulates RangeClient.close better to avoid this.;;;","13/Jun/11 15:46;mck;The check for the exception also occurs in ColumnFamilyRecordWriter.write(buf, value) -> RangeClient.put(pair)
Isn't it possible the put(..) is being called while the RangeClient thread is inside close() ?
(isn't write(..) called more often than close() ?)

For this reason inside RangeClient.run() i assigned lastException before calling close();;;","13/Jun/11 15:54;jbellis;bq. Isn't it possible the put(..) is being called while the RangeClient thread is inside close?

old close, new closeInternal?

Yes, but I don't see how that changes things.  I.e., it's always possible that the last put() will happen before an exception is set; hence, the extra check on close.;;;","13/Jun/11 18:08;mck;bq. it's always possible that the last put() will happen before an exception is set; hence, the extra check on close.
Quite right. ;;;","28/Jun/11 11:01;mck;Jonathan: Is your patch being applied?;;;","28/Jun/11 12:01;jbellis;waiting for a +1, wasn't clear if your last comment was intended that way.;;;","28/Jun/11 12:10;mck;Yes it was a +1;;;","28/Jun/11 12:27;jbellis;committed, thanks!;;;","28/Jun/11 13:24;hudson;Integrated in Cassandra-0.7 #515 (See [https://builds.apache.org/job/Cassandra-0.7/515/])
    fix race that could result in Hadoopwriter failing to throw exception for encountered error
patch by Mck SembWever and jbellis for CASSANDRA-2755

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1140565
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordWriter.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repair fails with java.io.EOFException,CASSANDRA-2752,12509699,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,terjem,terjem,09/Jun/11 11:24,16/Apr/19 09:32,14/Jul/23 05:52,14/Jun/11 07:56,0.8.1,,,,,,0,,,,"Issuing repair on node 1  (1.10.42.81) in a cluster quickly fails with
INFO [AntiEntropyStage:1] 2011-06-09 19:02:47,999 AntiEntropyService.java (line 234) Queueing comparison #<Differencer #<TreeRequest manual-repair-0c17c5f9-583f-4a31-a6d4-a9e7306fb46e, /1
.10.42.82, (JP,XXX), (Token(bytes[6e]),Token(bytes[313039])]>>
 INFO [AntiEntropyStage:1] 2011-06-09 19:02:48,026 AntiEntropyService.java (line 468) Endpoints somewhere/1.10.42.81 and /1.10.42.82 have 2 range(s) out of sync for (JP,XXX) on (Token(bytes[6e]),Token(bytes[313039])]
 INFO [AntiEntropyStage:1] 2011-06-09 19:02:48,026 AntiEntropyService.java (line 485) Performing streaming repair of 2 ranges for #<TreeRequest manual-repair-0c17c5f9-583f-4a31-a6d4-a9e7306
fb46e, /1.10.42.82, (JP,XXX), (Token(bytes[6e]),Token(bytes[313039])]>
 INFO [AntiEntropyStage:1] 2011-06-09 19:02:48,030 StreamOut.java (line 173) Stream context metadata [/data/cassandra/node0/data/JP/XXX-g-3-Data.db sections=1 progress=0/36592 - 0%], 1 sstables.
 INFO [AntiEntropyStage:1] 2011-06-09 19:02:48,031 StreamOutSession.java (line 174) Streaming to /1.10.42.82
ERROR [CompactionExecutor:9] 2011-06-09 19:02:48,970 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[CompactionExecutor:9,1,main]
java.io.EOFException
        at java.io.RandomAccessFile.readInt(RandomAccessFile.java:725)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.doIndexing(SSTableWriter.java:457)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:364)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1099)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1090)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


On .82
ERROR [CompactionExecutor:12] 2011-06-09 19:02:48,051 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[CompactionExecutor:12,1,main]
java.io.EOFException
        at java.io.RandomAccessFile.readInt(RandomAccessFile.java:725)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.doIndexing(SSTableWriter.java:457)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:364)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1099)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1090)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [Thread-132] 2011-06-09 19:02:48,051 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-132,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.EOFException
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:152)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
Caused by: java.util.concurrent.ExecutionException: java.io.EOFException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:136)
        ... 3 more
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readInt(RandomAccessFile.java:725)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.doIndexing(SSTableWriter.java:457)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:364)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1099)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1090)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Looks to me like the receiving side fails first.

",,muga_nishizawa,skamio,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/11 19:26;jbellis;2752.txt;https://issues.apache.org/jira/secure/attachment/12482364/2752.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20806,,,Tue Jun 14 17:02:36 UTC 2011,,,,,,,,,,"0|i0gd8n:",93575,,slebresne,,slebresne,Critical,,,,,,,,,,,,,,,,,"09/Jun/11 12:49;terjem;Need to test a bit more, but quite likely this is related to repair on CFs with secondary indexes.;;;","09/Jun/11 14:45;terjem;Seems confirmed, only able to reproduce this on a CF with secondary indexes.;;;","09/Jun/11 15:17;jbellis;Is this data from 0.7 that got upgraded?;;;","09/Jun/11 15:45;terjem;Fresh 0.8 data.

Just as a side test since the ""compactionexecutor"" is involved, we issued a full compaction of that CF and it completed without any error so the source SSTables seems good.

I was trying to reproduce this locally on my desktop before leaving office to get it in a debugger. Quickly generated 10k random inserts into a CF with secondary index, but then I experienced that repair got stuck eating 100% on both nodes instead...

I did not have time to figure out if it was due to some config issue or related to the same issue though.

;;;","09/Jun/11 22:57;lenn0x;I am trying to track down a similar issue. Instead was bootstrapping a new node in my case.;;;","10/Jun/11 03:50;muga_nishizawa;It seems tmp files (e.g. XXX-tmp-XXX-Data.db) that receiver node creates during repair process are broken.  EOFException occurs while RowIndexer is reading the broken files.  

According to result of scrub command, data files on sender nodes are not broken.  ;;;","10/Jun/11 14:18;terjem;Chris: Same as the ""compactionexecutor"" or the infinite loop? (made CASSANDRA-2758 for that just now).

Seems like the sstables here has been truncated for some reason.
Rowindexer iterates through a bunch of rows just fine.
Then it reaches the problem row.

For this row, it can get the key, it jumps pass the bloomfilter etc and when it is about to read the column count, it fails and is trying to read at an offset which equals the length of the file..
So far, all the stacktraces I have seen are all are on the column count read. 

A wild guess may be that it has failed to write the actual content (columns) of the last row that was stream?

Unfortunately it does not to happen all the time, but it does only happen on the CF with secondary indexes.



;;;","10/Jun/11 16:06;terjem;Also struck me on the way home from work today that the CF with secondary indexes also happen to be the only CF in this system which, I think, on a regular basis may actually update all columns for a key.

That is, sstables will on a regular basis have keys where no columns is valid anymore.

Not sure if that could for instance trigger something odd in the streaming?


;;;","13/Jun/11 16:33;muga_nishizawa;It seems like timing issue.  The exception doesn't always occur even with use of same data.  ;;;","13/Jun/11 17:00;terjem;SSTableWriter.java:
                ColumnFamily.serializer().deserializeFromSSTableNoColumns(ColumnFamily.create(cfs.metadata), dfile);

                // don't move that statement around, it expects the dfile to be before the columns
                updateCache(key, dataSize, null);

                rowSizes.add(dataSize);
                columnCounts.add(dfile.readInt());


I believe the problem is in updateCache.
If rowcache is enabled (and it is in this case) and the row needs to be updated in cache, this will read (deserialize) the row.

However, after all the columns is read, the offset in the file is not reset back to the location where the  column count is stored and things go bad.

I haven't actually tried to change the code to test, but I tried to disable the row cache, and so far, repair seems to work fine when it is disabled.;;;","13/Jun/11 19:26;jbellis;patch to seek back after deserializing a row to update cache with;;;","14/Jun/11 07:56;slebresne;Good catch. +1 (committed).
Thanks Terje.;;;","14/Jun/11 17:02;hudson;Integrated in Cassandra-0.8 #170 (See [https://builds.apache.org/job/Cassandra-0.8/170/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CliClient does not log root cause exception when catch it from executeCLIStatement,CASSANDRA-2746,12509492,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cywjackson,cywjackson,cywjackson,07/Jun/11 18:01,16/Apr/19 09:32,14/Jul/23 05:52,06/Jul/11 13:17,0.8.2,,,Legacy/Tools,,,0,,,,"When executing a statement from the cassandra-cli (with --debug) , if an exception is thrown from one of the cases in side the executeCLIStatement method, the root cause is swallowed. For specific case such as the InvalidRequestException or the SchemaDisagreementException, just the message itself maybe enough, but for the general Exception case, without the root cause, it could be difficult to debug the issue. 

For example, we have seen exception like:
{noformat}
null
java.lang.RuntimeException
at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:209)
at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:223)
at org.apache.cassandra.cli.CliMain.main(CliMain.java:351)
{noformat}

the null there would most likely indicate this is a NPE (though it could still be any Exception with null message). By adding a initCause to the caught exception, we could see the root cause, eg:

{noformat}
null
java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:212)
        at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:223)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:351)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.cli.CliClient.describeKeySpace(CliClient.java:1336)
        at org.apache.cassandra.cli.CliClient.executeShowKeySpaces(CliClient.java:1166)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:170)
        ... 2 more
{noformat}

submitting a patch here that would add the initCause to all caught exceptions here. But the most important one is the general Exception case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/11 18:02;cywjackson;patch2746.txt;https://issues.apache.org/jira/secure/attachment/12481727/patch2746.txt",,,,,,,,,,,,,,1.0,cywjackson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20803,,,Wed Jul 06 14:22:08 UTC 2011,,,,,,,,,,"0|i0gd7j:",93570,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"07/Jun/11 18:02;cywjackson;add initCause to exceptions caught from CliClient.executeCLIStatement;;;","06/Jul/11 13:17;jbellis;committed, thanks!;;;","06/Jul/11 14:22;hudson;Integrated in Cassandra-0.8 #206 (See [https://builds.apache.org/job/Cassandra-0.8/206/])
    add initCause to cliclient exceptions
patch by Jackson Chung; reviewed by jbellis for CASSANDRA-2746

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1143397
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cli/CliClient.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.jar is not executable,CASSANDRA-2744,12509377,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,thobbs,thobbs,06/Jun/11 18:43,16/Apr/19 09:32,14/Jul/23 05:52,09/Jun/11 22:10,0.8.1,,,Legacy/Tools,,,0,,,,"If you build stress.jar by running 'ant jar' from tools/stress/ and try to execute it with 'java -jar stress.jar', you get the following error:

{noformat}
Failed to load Main-Class manifest attribute from
stress.jar
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/11 22:07;xedin;CASSANDRA-2744.patch;https://issues.apache.org/jira/secure/attachment/12481986/CASSANDRA-2744.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20801,,,Thu Jun 09 23:41:07 UTC 2011,,,,,,,,,,"0|i0gd73:",93568,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"06/Jun/11 21:04;xedin;work branch: cassandra-0.8, the latest commit  504fb537fb15526a4558a94cfec04b8b7f2dc58e;;;","09/Jun/11 22:07;xedin;jar is default task now.;;;","09/Jun/11 22:10;brandon.williams;Committed.;;;","09/Jun/11 23:41;hudson;Integrated in Cassandra-0.8 #162 (See [https://builds.apache.org/job/Cassandra-0.8/162/])
    Stress.java creates a jar by default.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2744

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1134108
Files : 
* /cassandra/branches/cassandra-0.8/tools/stress/build.xml
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool decommission should throw an error when there are extra params,CASSANDRA-2740,12509253,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jhermes,brandon.williams,brandon.williams,04/Jun/11 03:24,16/Apr/19 09:32,14/Jul/23 05:52,14/Jul/11 23:42,0.7.8,,,,,,0,,,,"removetoken takes a token parameter, but decommission works against the node where the call is issued.  This allows confusion such as 'nodetool -h localhost decommission <ip or token>' actually decommissioning the local node, instead of whatever was passed to it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/11 22:51;jhermes;2740.txt;https://issues.apache.org/jira/secure/attachment/12481992/2740.txt",,,,,,,,,,,,,,1.0,jhermes,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20799,,,Fri Jul 15 00:06:42 UTC 2011,,,,,,,,,,"0|i0gd67:",93564,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"09/Jun/11 22:38;brandon.williams;For 1.0 I'd like to see decom work like rt, and take a token parameter, but I don't want to break existing jmx tools for 0.8, so let's just error on extra params there.;;;","09/Jun/11 22:41;jbellis;why add token param?;;;","09/Jun/11 22:51;jhermes;All zero-arg commands now complain when there's extra args.;;;","14/Jul/11 23:42;brandon.williams;Committed.;;;","15/Jul/11 00:06;hudson;Integrated in Cassandra-0.7 #528 (See [https://builds.apache.org/job/Cassandra-0.7/528/])
    Do not allow extra params to nodetool commands to prevent confusion.
Patch by Jon Hermes, reviewed by brandonwilliams for CASSANDRA-2740

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1146923
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/tools/NodeCmd.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The link for ""Latest Builds"" in the download page is incorrect",CASSANDRA-2736,12509063,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,kunda,kunda,02/Jun/11 06:56,16/Apr/19 09:32,14/Jul/23 05:52,06/Jun/11 14:21,,,,Legacy/Documentation and Website,,,0,,,,"The link in the download page (http://cassandra.apache.org/download/) is outdated, pointing to 
{noformat}http://hudson.zones.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/{noformat}
and should probably replaced by
{noformat}http://builds.apache.org/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/{noformat}

In addition, the wording ""Latest Builds"" is incorrect per the link, and should be changed to ""Latest Build"" or more precisely ""Latest Trunk Build"".
Alternatively, the link could instead point to either {noformat}https://builds.apache.org/job/Cassandra/changes{noformat} or {noformat}https://builds.apache.org/job/Cassandra{noformat}, both including a list of the latest trunk builds.

Furthermore, it might be sensible to have additional links to 0.6, 0.7 & 0.8 builds - as it's more probable users are running those rather than the trunk version.

Finally (I didn't think I'd right so much about a link...), I believe the text ""(Hudson)"" next to the link is not up-to-date, and should be replaced by ""(Jenkins)"" or removed altogether.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20796,,,Mon Jun 06 14:21:40 UTC 2011,,,,,,,,,,"0|i0gd5b:",93560,,,,,Low,,,,,,,,,,,,,,,,,"06/Jun/11 14:21;jbellis;updated to the jenkins latest-trunk-build url.  thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exception generate when using same index names,CASSANDRA-2730,12509001,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,wubn2000,wubn2000,01/Jun/11 17:13,16/Apr/19 09:32,14/Jul/23 05:52,05/Jun/11 14:08,0.8.1,,,,,,0,cql,,,"when using cqlsh tool to generate indexes, for example, suppose we have a column family Tuser, which has two columns: name and state.
cqlsh> create index name_key on Tuser(name);
cqlsh> create index name_key on Tuser(state);
note that name_key is used twice by mistake, then a javax.management.InstanceAlreadyExistsException will be thrown and this exception will prevent cassandra service from starting any more.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/11 19:43;jbellis;2730.txt;https://issues.apache.org/jira/secure/attachment/12481136/2730.txt",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20794,,,Sun Jun 05 14:08:35 UTC 2011,,,,,,,,,,"0|i0gd3z:",93554,,,,,Low,,,,,,,,,,,,,,,,,"01/Jun/11 18:30;cdaw;*For clarity, this occurs only when you create the same named index referencing different columns*
* Workaround #1:  In CQLSH:  Prior to restarting server, drop this column family after you see this error, then future server restarts will be fine.
* Workaround #2: In cassandra-cli:  Prior to restarting server, run the ""drop index cf column"" command for both attempts (birth_year and session_token in my example), then future server restarts will be fine.

{code}
cqlsh> CREATE INDEX birth_year_key ON users (session_token);
Bad Request: javax.management.InstanceAlreadyExistsException: org.apache.cassandra.db:type=IndexColumnFamilies,keyspace=cqldb,columnfamily=users.birth_year_key
{code}

*PASS: CQLSH allows multiple indexes without an issue*
{code}
cqlsh> CREATE INDEX gender_key ON users (gender);
cqlsh> CREATE INDEX state_key ON users (state);
cqlsh> CREATE INDEX birth_year_key ON users (birth_year);
{code}

*PASS: CQLSH correctly errors out creating the same index twice*
{code}
cqlsh> CREATE INDEX birth_year_key ON users (birth_year);
Bad Request: Index exists
{code}


;;;","01/Jun/11 19:36;wubn2000;so this ""drop index"" is in 0.8.1? I can't find it in my 0.8.0-rc1.;;;","01/Jun/11 19:42;jbellis;correct;;;","01/Jun/11 19:43;jbellis;Attached patch fixes the problem, but requires CASSANDRA-2617 to be finished first.;;;","05/Jun/11 14:08;jbellis;fix rolled into CASSANDRA-2617 patch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
examples/hadoop_word_count reducer to cassandra doesn't output into the output_words cf,CASSANDRA-2727,12508874,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tjake,jeromatron,jeromatron,31/May/11 21:41,16/Apr/19 09:32,14/Jul/23 05:52,17/Jun/11 15:26,0.8.2,,,,,,0,hadoop,,,"I tried the examples/hadoop_word_count example and could output to the filesystem but when I output to cassandra (the default), nothing shows up in output_words.  I can output to cassandra using pig so I think the problem is isolated to this example.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/11 15:12;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2727-fix-for-word-count-reducer.txt;https://issues.apache.org/jira/secure/attachment/12482942/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2727-fix-for-word-count-reducer.txt",,,,,,,,,,,,,,1.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20793,,,Fri Jun 17 16:20:10 UTC 2011,,,,,,,,,,"0|i0gd3b:",93551,,,,,Low,,,,,,,,,,,,,,,,,"17/Jun/11 15:14;jbellis;+1;;;","17/Jun/11 16:20;hudson;Integrated in Cassandra-0.8 #174 (See [https://builds.apache.org/job/Cassandra-0.8/174/])
    fix cassandra reducer example

Patch by tjake; reviewed by jbellis for CASSANDRA-2727

jake : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1136910
Files : 
* /cassandra/branches/cassandra-0.8/examples/hadoop_word_count/src/WordCount.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rows that don't exist get cached,CASSANDRA-2723,12508780,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,31/May/11 01:54,16/Apr/19 09:32,14/Jul/23 05:52,31/May/11 02:10,0.8.1,,,,,,0,,,,"We noticed that rows that don't exist were getting cached anyway. We end up storing an empty CF in cache.


",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/May/11 02:02;lenn0x;0001-Do-not-cache-rows-that-do-not-exist-v2.patch;https://issues.apache.org/jira/secure/attachment/12480890/0001-Do-not-cache-rows-that-do-not-exist-v2.patch","31/May/11 01:58;lenn0x;0001-Do-not-cache-rows-that-do-not-exist.patch;https://issues.apache.org/jira/secure/attachment/12480889/0001-Do-not-cache-rows-that-do-not-exist.patch",,,,,,,,,,,,,2.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20790,,,Tue May 31 03:52:17 UTC 2011,,,,,,,,,,"0|i0gd2f:",93547,,stuhood,,stuhood,Low,,,,,,,,,,,,,,,,,"31/May/11 02:03;stuhood;+1, although I'd move the relevant portion of the comment above {{return returnCF;}} near the new return.;;;","31/May/11 02:10;lenn0x;Thanks, commited based on comments.;;;","31/May/11 03:52;hudson;Integrated in Cassandra-0.8 #147 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/147/])
    Fixed rows being cached if they do not exist.
patch by goffinet; reviewed by stuhood for CASSANDRA-2723

goffinet : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1129462
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool statusthrift exception while node starts up,CASSANDRA-2721,12508683,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,30/May/11 07:16,16/Apr/19 09:32,14/Jul/23 05:52,30/May/11 08:37,0.8.1,,,,,,0,,,,"We noticed when calling nodetool statusthrift, while a node is starting up, it throws an exception. I think the proper behavior should be just return false, instead of throwing an exception if RPC server hasn't started yet. That way this stack trace won't have to be thrown in nodetool:

Exception in thread ""main"" 

java.lang.IllegalStateException: No configured RPC daemon
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/11 07:19;lenn0x;0001-If-RPCServer-isn-t-started-just-return-false-instead.patch;https://issues.apache.org/jira/secure/attachment/12480824/0001-If-RPCServer-isn-t-started-just-return-false-instead.patch",,,,,,,,,,,,,,1.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20789,,,Mon May 30 14:55:47 UTC 2011,,,,,,,,,,"0|i0gd1z:",93545,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"30/May/11 07:54;slebresne;I don't think nodetool statusthrift exists, but yeah, makes sense, +1. BUT, let's just put it in 0.8.1 however, for the sake of making Eric job's easier when he re-roll 0.8.0 (in the meantime, any hypothetical implementation of a nodetool statusthrift could just catch the IllegalStateException).;;;","30/May/11 08:09;lenn0x;Ah! I forgot to commit that patch. It was put in our build. CASSANDRA-2722;;;","30/May/11 14:44;hudson;Integrated in Cassandra #912 (See [https://builds.apache.org/hudson/job/Cassandra/912/])
    ;;;","30/May/11 14:55;hudson;Integrated in Cassandra-0.8 #146 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/146/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in SSTableWriter when no ReplayPosition availible,CASSANDRA-2718,12508550,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,27/May/11 13:40,16/Apr/19 09:32,14/Jul/23 05:52,27/May/11 14:01,0.8.1,,,,,,0,,,,"The following NPE occurs when durable_writes is set to false

{noformat}
ERROR 09:20:30,378 Fatal exception in thread Thread[FlushWriter:11,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.commitlog.ReplayPosition$ReplayPositionSerializer.serialize(ReplayPosition.java:127)
	at org.apache.cassandra.io.sstable.SSTableWriter.writeMetadata(SSTableWriter.java:209)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:187)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:173)
	at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:253)
	at org.apache.cassandra.db.Memtable.access$400(Memtable.java:49)
	at org.apache.cassandra.db.Memtable$3.runMayThrow(Memtable.java:270)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/May/11 13:42;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2718-avoide-NPE-when-bypassing-commitlog.txt;https://issues.apache.org/jira/secure/attachment/12480654/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2718-avoide-NPE-when-bypassing-commitlog.txt",,,,,,,,,,,,,,1.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20787,,,Fri May 27 14:01:26 UTC 2011,,,,,,,,,,"0|i0gd1b:",93542,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"27/May/11 13:49;slebresne;+1;;;","27/May/11 14:01;tjake;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
duplicate rows returned from SELECT where KEY term is duplicated,CASSANDRA-2717,12508504,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jancona,amorton,amorton,27/May/11 02:48,16/Apr/19 09:32,14/Jul/23 05:52,27/Jul/11 20:04,0.8.3,,,,,,0,cql,lhf,,"Noticed while working on CASSANDRA-2268 when random keys generated during a mutli_get test contain duplicate keys. 

The thrift multiget_slice() returns only the unique rows because of the map generated for the result. 

CQL will return a row for each KEY term in the SELECT. 

I could make QueryProcessor.getSlice() only create commands for the unique keys if we wanted to. 

Not sure it's a bug and it's definitely not something that should come up to often, reporting it because it's different to the thrift mutli_get operation. 

Happy to close if it's by design. 

",,jancona,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/11 13:19;jancona;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2717-Prevent-multiple-KEY-terms-properly-han.txt;https://issues.apache.org/jira/secure/attachment/12485815/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2717-Prevent-multiple-KEY-terms-properly-han.txt","19/Jul/11 02:01;jancona;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2717-Prevent-multiple-KEY-terms-properly-han.txt;https://issues.apache.org/jira/secure/attachment/12486957/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2717-Prevent-multiple-KEY-terms-properly-han.txt","19/Jul/11 02:01;jancona;ASF.LICENSE.NOT.GRANTED--v2-0002-Set-multiKey-state-from-within-grammar.txt;https://issues.apache.org/jira/secure/attachment/12486958/ASF.LICENSE.NOT.GRANTED--v2-0002-Set-multiKey-state-from-within-grammar.txt",,,,,,,,,,,,3.0,jancona,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20786,,,Wed Jul 27 20:04:01 UTC 2011,,,,,,,,,,"0|i0gd13:",93541,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"27/May/11 02:54;jbellis;You mean when using OR?

A little confused b/c OR support was added to SELECT for 0.8.1 but this is tagged 0.8b2.;;;","27/May/11 03:53;amorton;query was ""SELECT foo FROM my-cf WHERE KEY = 'bar' and KEY = 'bar';""

Nothing a sane person would do, I was just using the randomly generated keys for the stress test the same way the thrift based one does. 
;;;","27/May/11 04:47;jbellis;Definitely a bug.

(KEY=X and KEY=Y should result in zero rows, unless X=Y in which case it should be one, assuming X exists.);;;","27/May/11 22:32;amorton;Ah, I sent a stupid query and it worked so I continued along. 

I'll try to dig into it this weekend. ;;;","09/Jul/11 13:34;jancona;I added some tests to test/system/test_cql.py. I found that not only did ""WHERE KEY = 'bar' and KEY = 'bar'"" return two rows, so did ""WHERE KEY = 'bar' and KEY = 'baz'"" and ""WHERE KEY IN ('bar', 'bar')""

The attached patch makes having more than one ""KEY ="" clause be an error, and changes the List of keys in WhereClause to a Set. 

Jonathan mentioned that OR support was added, but I didn't see that in cassandra-0.8. Am I looking at the wrong branch? If so, this patch will have to be reworked, along with the logic in WhereClause.;;;","18/Jul/11 21:27;xedin;I think we should set isMultiKey flag in the grammar (in whereClause statement) instead of depending on andKeyEquals(...) method.;;;","19/Jul/11 02:02;jancona;Done.;;;","19/Jul/11 11:00;xedin;+1;;;","19/Jul/11 16:14;jbellis;bq. Jonathan mentioned that OR support was added

I was probably thinking of CASSANDRA-2553.

Committed this fix, thanks Jim and Pavel!;;;","19/Jul/11 18:04;hudson;Integrated in Cassandra-0.8 #224 (See [https://builds.apache.org/job/Cassandra-0.8/224/])
    CQL: include only one row per unique keyfor IN queries
patch by Jim Ancona; reviewed by pyaskevich for CASSANDRA-2717

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1148425
Files : 
* /cassandra/branches/cassandra-0.8/test/system/test_cql.py
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/Term.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/Cql.g
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/WhereClause.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/SelectStatement.java
;;;","27/Jul/11 20:04;jbellis;belatedly marking resolved;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null strategy_options on a KsDef leads to an NPE.,CASSANDRA-2713,12508463,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jhermes,jhermes,jhermes,26/May/11 19:14,16/Apr/19 09:32,14/Jul/23 05:52,26/May/11 19:26,0.8.0,,,,,,0,,,,"For add/update keyspace, a KsDef with null strategy_options will cause an NPE.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/May/11 19:16;jhermes;2713-allow.txt;https://issues.apache.org/jira/secure/attachment/12480571/2713-allow.txt","26/May/11 19:16;jhermes;2713-disallow.txt;https://issues.apache.org/jira/secure/attachment/12480572/2713-disallow.txt",,,,,,,,,,,,,2.0,jhermes,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20785,,,Thu Jun 02 01:42:19 UTC 2011,,,,,,,,,,"0|i0gd07:",93537,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"26/May/11 19:16;jhermes;Attaching two patches.

`2713-allow` will allow nulls and default them to an empty set on KSMD creation.
`2713-disallow` will explicitly block nulls during ThriftValidation.

Both avoid NPEs and either are technically correct. Looks like we need to allow nulls because of upgrade issues coming from 0.7.;;;","26/May/11 19:26;jbellis;committed the ""allow"" approach;;;","01/Jun/11 20:39;cdaw;I verified this fix by dropping in the c* 0.8.0 jar file into brisk.
Can we get this merged to the Cassandra 0.8 branch since brisk builds its jar file from there?;;;","02/Jun/11 01:42;jbellis;merged to 0.8 branch in r1130370;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memory leak in CompactionManager's estimatedCompactions,CASSANDRA-2708,12508349,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dalaro,dalaro,dalaro,25/May/11 20:51,16/Apr/19 09:32,14/Jul/23 05:52,22/Aug/11 21:49,0.7.9,0.8.5,,,,,0,,,,"CompactionManager's estimatedCompactions map seems to hold all or most ColumnFamilyStores in the system as keys.  Keys are never removed from estimatedCompactions.

I have a project that embeds Cassandra as a storage backend.  Some of my integration tests create and drop a single keyspace and pair of column families a hundred or 150 times in one JVM.  These tests always OOM'd.  Loading some near-death heapdumps in mat suggested CompactionManager's estimatedCompactions held over 80% of total heap via its ColumnFamilyStore keys.  estimatedCompactions had the only inbound reference to these CFSs, and the CFSs themselves had invalid = true.

As a workaround, I changed estimatedCompactions to a WeakReference-keyed map (using Guava MapMaker).  My integration tests no longer OOM.

I'm generally unfamiliar with Cassandra's guts.  I don't know whether weak referencing the keys of estimatedCompactions is correct (or ideal).  But, that did seem to confirm my guess that retained references to dead CFSs in estimatedCompactions were swamping my heap after lots of Keyspace+ColumnFamily drops.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/11 20:57;dalaro;cassandra-0.7-2708.txt;https://issues.apache.org/jira/secure/attachment/12480452/cassandra-0.7-2708.txt",,,,,,,,,,,,,,1.0,dalaro,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20781,,,Mon Aug 22 21:28:17 UTC 2011,,,,,,,,,,"0|i0gcz3:",93532,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"25/May/11 20:57;dalaro;Attaching 3-line patch against cassandra-0.7 r1127675;;;","22/Aug/11 21:09;jbellis;Sorry Dan, I missed this when you first submitted it.  Committed for 0.7.9.  Thanks for the help!;;;","22/Aug/11 21:28;hudson;Integrated in Cassandra-0.7 #540 (See [https://builds.apache.org/job/Cassandra-0.7/540/])
    avoid retaining references to dropped CFS objects in CompactionManager.estimatedCompactions
patch by Dan LaRocque; reviewed by jbellis for CASSANDRA-2708

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1160436
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/CompactionManager.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig output not working with 0.8.0 branch,CASSANDRA-2706,12508335,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,jeromatron,jeromatron,25/May/11 17:22,16/Apr/19 09:32,14/Jul/23 05:52,25/May/11 18:45,0.8.0,,,,,,0,pig,,,"For some reason running a simple column family copy with pig is not writing out, though pig reports that it is successful.
Steps to reproduce on a local node:
1. Create the schema:
http://aep.appspot.com/display/VgbvdtP6QExc3OTY3HBry9ncC3k/
2. Run the following pig script (I did it with pig 0.8.0 from cdh3) using contrib/pig/bin/pig_cassandra -x local:
http://aep.appspot.com/display/PaWJkCqRGbp7CRgjt7qoyx9izN8/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/11 18:35;brandon.williams;2706.txt;https://issues.apache.org/jira/secure/attachment/12480444/2706.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20779,,,Wed May 25 18:45:07 UTC 2011,,,,,,,,,,"0|i0gcyn:",93530,,,,,Normal,,,,,,,,,,,,,,,,,"25/May/11 18:35;brandon.williams;Patch to fix timestamp setting for thrift, and remove an extra addition of non-super mutations.;;;","25/May/11 18:41;jeromatron;+1
Fixed the case I mentioned and I tried on a simple use of from/to cassandra bag from the pygmalion stuff just to have something a little different.;;;","25/May/11 18:45;brandon.williams;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception adding validators to non-string columns,CASSANDRA-2696,12508130,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,24/May/11 03:47,16/Apr/19 09:32,14/Jul/23 05:52,24/May/11 05:29,0.8.0,,,,,,0,,,,"Adding column metadata to a column with a non-string name causes an Exception to be raised:

{noformat}
org.apache.cassandra.db.marshal.MarshalException: Expected 8 or 0 byte long (3)
	at org.apache.cassandra.db.marshal.LongType.validate(LongType.java:106)
	at org.apache.cassandra.config.CFMetaData.validateAliasCompares(CFMetaData.java:977)
	at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:699)
	at org.apache.cassandra.db.migration.UpdateColumnFamily.<init>(UpdateColumnFamily.java:59)
	at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:968)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.process(Cassandra.java:4032)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

For example, if the comparator type is LongType, adding a validator or index to column int(3) will cause this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/11 03:49;thobbs;2696-test.txt;https://issues.apache.org/jira/secure/attachment/12480200/2696-test.txt",,,,,,,,,,,,,,1.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20778,,,Tue May 24 05:29:58 UTC 2011,,,,,,,,,,"0|i0gcwn:",93521,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"24/May/11 03:49;thobbs;Attached patch reproduces using a thrift system test.;;;","24/May/11 03:50;thobbs;It's also worth noting that this does not affect 0.7.x.;;;","24/May/11 05:05;jbellis;I could have sworn we fixed this for CASSANDRA-2622. :(;;;","24/May/11 05:09;jbellis;It's totally in my patch!

{code}
-    public static void validateAliasCompares(org.apache.cassandra.thrift.CfDef cf_def) throws ConfigurationException
-    {
-        AbstractType comparator = DatabaseDescriptor.getComparator(cf_def.comparator_type);
-        if (cf_def.key_alias != null)
-            comparator.validate(cf_def.key_alias);
-    }
{code}

Either I mis-applied the patch on commit or there was a bad merge to reintroduce it...;;;","24/May/11 05:29;jbellis;re-patched CFMetaData and committed with the new test in r1126874, r1126875;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stop JDBC driver from needing access to cassandra.yaml,CASSANDRA-2694,12508119,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,23/May/11 23:31,16/Apr/19 09:32,14/Jul/23 05:52,24/May/11 13:34,0.8.0,,,,,,0,,,,"The JDBC driver uses CFMetaData.fromThrift(), it was calling validateMemtableSettings() which used static methods on  DatabaseDescriptor. This causes cassandra.yaml to be loaded and means the client side needs access to the file. 

I think this needs to be fixed for 0.8, I have the patch. 

**Updated** changed title from ""remove references to DatabaseDescriptor in CFMetaData""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2268,,,"24/May/11 10:13;amorton;0001-2694-v2.patch;https://issues.apache.org/jira/secure/attachment/12480229/0001-2694-v2.patch","23/May/11 23:34;amorton;0001-2694.patch;https://issues.apache.org/jira/secure/attachment/12480185/0001-2694.patch",,,,,,,,,,,,,2.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20777,,,Tue May 24 13:34:41 UTC 2011,,,,,,,,,,"0|i0gcw7:",93519,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"23/May/11 23:34;amorton;remove references to DatabaseDescriptor in CFMetaData;;;","24/May/11 01:50;jbellis;I think it would be cleaner if JDBC just accepted the CfDef as given by the server. It would be weird if we changed the validation code in a newer version, causing the JDBC code when run against an old server to say ""that can't be right!""

I'd move the validate code to ThriftValidation with the rest of the ""is this struct I got from the user reasonable"" checks, and have the fromThrift calls in CassandraServer call that method manually, so that fromThrift just does what it advertises.;;;","24/May/11 03:12;amorton;sounds good, will take a look soon.;;;","24/May/11 10:13;amorton;v2 patch moves thrift validation to ThriftValidation and calls it from ThriftValidation.validateCfDef()

I left the avro validation used by CFMetaData.apply() in CFMetaData. ;;;","24/May/11 13:34;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
generate-eclipse-files ant target throws StackOverflowError in eclipse,CASSANDRA-2687,12508074,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,23/May/11 16:08,16/Apr/19 09:32,14/Jul/23 05:52,23/May/11 19:04,0.8.0,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/11 16:09;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2687-fix-eclipse-stackoverflow-issue.txt;https://issues.apache.org/jira/secure/attachment/12480121/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2687-fix-eclipse-stackoverflow-issue.txt",,,,,,,,,,,,,,1.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20776,,,Mon May 23 19:04:10 UTC 2011,,,,,,,,,,"0|i0gcun:",93512,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"23/May/11 16:31;urandom;Nice. +1;;;","23/May/11 19:04;tjake;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in Table.createReplicationStrategy during sends from HintedHandOffManager,CASSANDRA-2685,12508047,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,ithkuil,ithkuil,23/May/11 11:59,16/Apr/19 09:32,14/Jul/23 05:52,23/May/11 14:21,0.7.7,0.8.1,,,,,0,,,,"After about 800k inserts in a column family with RF=1, I get this exception:

{code}
ERROR [HintedHandoff:2] 2011-05-20 18:38:25,089 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[HintedHandoff:2,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:266)
	at org.apache.cassandra.db.Table.<init>(Table.java:212)
	at org.apache.cassandra.db.Table.open(Table.java:106)
	at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:331)
	at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
	at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:409)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{code}","2.6.32-5-xen-amd64 #1 SMP Wed Jan 12 05:46:49 UTC 2011 x86_64 GNU/Linux
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/11 13:40;jbellis;2685.txt;https://issues.apache.org/jira/secure/attachment/12480111/2685.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20775,,,Tue May 24 19:13:31 UTC 2011,,,,,,,,,,"0|i0gcuf:",93511,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"23/May/11 13:40;jbellis;patch to skip replay of hints for dropped KS/CF;;;","23/May/11 13:47;gdusbabek;+1.;;;","23/May/11 14:21;jbellis;committed;;;","24/May/11 19:13;hudson;Integrated in Cassandra-0.7 #496 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/496/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IntergerType uses Thrift method that attempts to unsafely access backing array of ByteBuffer and fails,CASSANDRA-2684,12508014,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,edanuff,edanuff,edanuff,22/May/11 20:43,16/Apr/19 09:32,14/Jul/23 05:52,23/May/11 01:34,0.7.7,0.8.0,,,,,0,,,,"I get the following exception:

{noformat}
ERROR 13:27:38,153 Fatal exception in thread Thread[ReadStage:36,5,main]
java.lang.RuntimeException: java.lang.UnsupportedOperationException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:940)
	at org.apache.thrift.TBaseHelper.byteBufferToByteArray(TBaseHelper.java:264)
	at org.apache.thrift.TBaseHelper.byteBufferToByteArray(TBaseHelper.java:251)
	at org.apache.cassandra.db.marshal.IntegerType.getString(IntegerType.java:136)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getString(AbstractCompositeType.java:131)
	at org.apache.cassandra.db.Column.getString(Column.java:228)
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:123)
	at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:130)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1303)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1188)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1145)
	at org.apache.cassandra.db.Table.getRow(Table.java:385)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:61)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:641)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{noformat}

Tracing it down, I find that IntegerType's getString method() looks like this:

{code:title=IntegerType.java|borderStyle=solid}
    public String getString(ByteBuffer bytes)
    {
        if (bytes == null)
            return ""null"";
        if (bytes.remaining() == 0)
            return ""empty"";

        return new java.math.BigInteger(TBaseHelper.byteBufferToByteArray(bytes)).toString(10);
    }
{code} 
    
TBaseHelper.byteBufferToByteArray() looks like this:

{code:title=TBaseHelper.java|borderStyle=solid}
  public static byte[] byteBufferToByteArray(ByteBuffer byteBuffer) {
    if (wrapsFullArray(byteBuffer)) {
      return byteBuffer.array();
    }
    byte[] target = new byte[byteBuffer.remaining()];
    byteBufferToByteArray(byteBuffer, target, 0);
    return target;
  }

  public static boolean wrapsFullArray(ByteBuffer byteBuffer) {
    return byteBuffer.hasArray()
      && byteBuffer.position() == 0
      && byteBuffer.arrayOffset() == 0
      && byteBuffer.remaining() == byteBuffer.capacity();
  }

  public static int byteBufferToByteArray(ByteBuffer byteBuffer, byte[] target, int offset) {
    int remaining = byteBuffer.remaining();
    System.arraycopy(byteBuffer.array(),
        byteBuffer.arrayOffset() + byteBuffer.position(),
        target,
        offset,
        remaining);
    return remaining;
  }
{code} 

The second overloaded implementation of byteBufferToByteArray is calling the bytebuffer's array() method.

Suggested fixes:

1) Don't use TBaseHelper in IntegerType.getString(), use ByteBufferUtil.getArray()

2) Report problem upstream to Thrift.

3) Find a better way to deserialize BigIntegers that doesn't require an array copy.






",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/11 20:54;edanuff;2684.txt;https://issues.apache.org/jira/secure/attachment/12480058/2684.txt",,,,,,,,,,,,,,1.0,edanuff,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20774,,,Mon May 23 01:56:47 UTC 2011,,,,,,,,,,"0|i0gcu7:",93510,,,,,Low,,,,,,,,,,,,,,,,,"23/May/11 01:34;jbellis;committed;;;","23/May/11 01:56;hudson;Integrated in Cassandra-0.7 #492 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/492/])
    fix IntegerType.getString with direct buffers
patch by Ed Anuff; reviewed by jbellis for CASSANDRA-2684

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1126290
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/marshal/IntegerType.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UUIDType assumes ByteBuffer has an accessible backing array,CASSANDRA-2682,12507976,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,edanuff,edanuff,edanuff,21/May/11 17:08,16/Apr/19 09:32,14/Jul/23 05:52,23/May/11 01:30,0.8.0,,,,,,0,,,,"I'm very embarrassed to say this got left out in the UUIDType, but it's not doing a hasArray() check on the bytebuffers passed to it, causing it to break.  I'll make a patch to fix it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/11 00:35;edanuff;2682.txt;https://issues.apache.org/jira/secure/attachment/12480011/2682.txt",,,,,,,,,,,,,,1.0,edanuff,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20773,,,Mon May 23 01:30:55 UTC 2011,,,,,,,,,,"0|i0gctr:",93508,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"21/May/11 23:43;jbellis;looks like converting array access to ByteBuffer.get(i) would be a better fix, since you'd avoid an unnecessary copy for direct buffers.  (this is what TimeUUIDType does.);;;","22/May/11 00:35;edanuff;New patch that uses ByteBuffer.get() instead of direct array accesses;;;","23/May/11 01:30;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
range scan doesn't repair missing rows,CASSANDRA-2680,12507952,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,21/May/11 02:08,16/Apr/19 09:32,14/Jul/23 05:52,23/May/11 14:23,0.7.7,0.8.1,,,,,0,,,,"Range scans do not do digest queries but they do compare all the replicas they receive and repair any discrepancies in the background.  (Thus, to get comparable behavior to normal read repair, CL.ALL must be used.)

The bug is that currently, replicas that omit a row entirely will be ignored and that row will not be sent to them.  ",,bcoverston,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/11 02:22;jbellis;2680.txt;https://issues.apache.org/jira/secure/attachment/12479978/2680.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20772,,,Tue May 24 19:13:31 UTC 2011,,,,,,,,,,"0|i0gctb:",93506,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"21/May/11 02:22;jbellis;patch to add placeholders for missing rows;;;","23/May/11 14:06;slebresne;+1;;;","23/May/11 14:23;jbellis;committed;;;","24/May/11 19:13;hudson;Integrated in Cassandra-0.7 #496 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/496/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect NetworkTopolgyStrategy Options on upgrade from 0.7.5,CASSANDRA-2678,12507930,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,ctrahman,ctrahman,20/May/11 20:57,16/Apr/19 09:32,14/Jul/23 05:52,23/May/11 19:12,0.8.0,,,,,,0,,,,"After an upgrade from 0.7.5 to 0.8.0-rc1 on a 10 node, single DC ring configured with NTS, operations fail due to an inability to reach replicas in the 'second datacenter':

ERROR [pool-2-thread-8] 2011-05-17 12:15:23,145 Cassandra.java (line 3294) Internal error processing insert
java.lang.IllegalStateException: datacenter (replication_factor) has no more endpoints, (3) replicas still needed
        at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalEndpoints(NetworkTopologyStrategy.java:118)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:100)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1611)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1599)
        at org.apache.cassandra.service.StorageProxy.getWriteEndpoints(StorageProxy.java:217)
        at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:202)
        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:154)
        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:557)
        at org.apache.cassandra.thrift.CassandraServer.internal_insert(CassandraServer.java:434)
        at org.apache.cassandra.thrift.CassandraServer.insert(CassandraServer.java:442)
        at org.apache.cassandra.thrift.Cassandra$Processor$insert.process(Cassandra.java:3286)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
DEBUG [ScheduledTasks:1] 2011-05-17 12:15:33,975 StorageLoadBalancer.java (line 334) Disseminating load info ...

On checking the keyspace definition with cassandra-cli, it appears that 0.8.0-rc1 considered the 'replication_factor:3' configuration in the older version as a DC name in part of the DC replication strategy:

  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
    Options: [replication_factor:3, DC1:3]

I attempted to remove replication_factor as a DC using the 'update keyspace' command, but it would persist.  I was able to remove the DC1:3 and use:

update keyspace MyKeyspace with strategy_options=[{replication_factor:3}];

then changed the topology properties file, renamed DC1 to replication_factor, and it worked - so there is a workaround. ","10 node cluster, RHELL6.0
0.7.5, Single DC using NTS and RF=3 (DC1:3) -> upgraded to 0.8.0-rc1. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/11 16:20;jbellis;2678-v2.txt;https://issues.apache.org/jira/secure/attachment/12480122/2678-v2.txt","20/May/11 21:49;jbellis;2678.txt;https://issues.apache.org/jira/secure/attachment/12479961/2678.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20771,,,Mon May 23 19:12:48 UTC 2011,,,,,,,,,,"0|i0gcsv:",93504,,jhermes,,jhermes,Low,,,,,,,,,,,,,,,,,"20/May/11 21:49;jbellis;patch to only add r_f to strategy_options for simplestrategy and oldnetworktopologystrategy;;;","23/May/11 16:03;jhermes;KSMetaData:55
{noformat}
maybeAddReplicationFactor(options, ks_def.strategy_class, ks_def.isSetReplication_factor() ? ks_def.replication_factor : null);
{noformat}
...needs to be
{noformat}
maybeAddReplicationFactor(options, ks_def.strategy_class.toString(), ks_def.replication_factor);
{noformat}

It's much clearer to not ternary on the value (since you're not going to use it in the `ne SS || ONTS` case), and more importantly that ks_def.strategy_class is a CharSequence, so you'll hit an error if you expect it to be a String in maybe...().

Otherwise straightforward.;;;","23/May/11 16:20;jbellis;bq. It's much clearer to not ternary on the value

you need to though, to convert int -> Integer.  Otherwise it will autobox the 0 default value, instead of the correct null.  (maybeAdd wasn't checking for null though -- v2 corrects this.)

bq. more importantly that ks_def.strategy_class is a CharSequence

no, this is a Thrift object so it's a String.;;;","23/May/11 16:44;jhermes;If the def has a null value for RF (as it correctly should post-RF change), should we verify that 'replication_factor' already exists in the map inside of maybe() and do nothing?

Line 55 is unnecessary/duped, and line 142 looks like it might also need to pass null, otherwise it's just avro/thriftisms.;;;","23/May/11 18:35;jbellis;bq. If the def has a null value for RF (as it correctly should post-RF change)

As above, it can't (since it is an int not an Integer), but it can be ""unset.""

bq. should we verify that 'replication_factor' already exists in the map inside of maybe() and do nothing?

No, the strategy class is responsible for actually validating its options.  We're just trying to stick a thin compatibility layer in here so people can continue using 0.7 clients w/ 0.8 server temporarily.;;;","23/May/11 18:48;jhermes;bq. As above, it can't (since it is an int not an Integer), but it can be ""unset.""
I meant the null that we pass in line 55 because it was 'unset'.

I agree with the latter choice. We could doubly validate here and catch the error just a _little_ bit faster than waiting for the class to validate, but it's not a big deal. Assuming the duplicate code was removed, +1.;;;","23/May/11 19:12;jbellis;committed minus the bogus maybeAdd call.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.io.IOError: java.io.EOFException with version 0.7.6,CASSANDRA-2675,12507853,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,kochen,kochen,20/May/11 09:09,16/Apr/19 09:32,14/Jul/23 05:52,25/May/11 15:43,0.7.7,0.8.1,,,,,0,,,,"I use the following data-model

column_metadata: []
name: Customers
column_type: Super
gc_grace_seconds: 60

I have a super-column-family with a single row.
Within this row I have a single super-column.
Within this super-column, I concurrently create, read and delete columns.

I have three threads:

- Do in a loop: add a column to the super-column.
- Do in a loop: delete a random column from the super-column.
- Do in a loop: read the super-column (with all columns).

After running the above threads concurrently, I always receive one of the following errors:

ERROR 17:09:57,036 Fatal exception in thread Thread[ReadStage:81,5,main]
java.io.IOError: java.io.EOFException
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:252)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:268)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:227)
        at java.util.concurrent.ConcurrentSkipListMap.buildFromSorted(Unknown Source)
        at java.util.concurrent.ConcurrentSkipListMap.<init>(Unknown Source)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:379)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:362)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:322)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:79)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:40)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:116)
        at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:130)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1390)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1267)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1195)
        at org.apache.cassandra.db.Table.getRow(Table.java:324)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:451)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readByte(Unknown Source)
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:324)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:335)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:71)
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:248)
        ... 30 more

java.io.IOError: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:252)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:268)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:227)
        at java.util.concurrent.ConcurrentSkipListMap.buildFromSorted(Unknown Source)
        at java.util.concurrent.ConcurrentSkipListMap.<init>(Unknown Source)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:379)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:362)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:322)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:79)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:40)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:116)
        at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:130)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1385)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1262)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1190)
        at org.apache.cassandra.db.Table.getRow(Table.java:324)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:451)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:73)
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:248)
        ... 30 more 

ERROR 11:02:19,824 Fatal exception in thread Thread[ReadStage:3404,5,main]
java.io.IOError: java.io.IOException: mmap segment underflow; remaining is 660267 but 758592100 requested
	at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:252)
	at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:268)
	at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:227)
	at java.util.concurrent.ConcurrentSkipListMap.buildFromSorted(Unknown Source)
	at java.util.concurrent.ConcurrentSkipListMap.<init>(Unknown Source)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:379)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:362)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:322)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:79)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:40)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
	at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
	at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
	at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:116)
	at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:130)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1390)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1267)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1195)
	at org.apache.cassandra.db.Table.getRow(Table.java:324)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:451)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)","Reproduced on single Cassandra node (CentOS 5.5)
Reproduced on single Cassandra node (Windows Server 2008)",skamio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/11 12:51;slebresne;0001-Don-t-remove-columns-from-super-columns-in-memtable.patch;https://issues.apache.org/jira/secure/attachment/12480236/0001-Don-t-remove-columns-from-super-columns-in-memtable.patch","24/May/11 15:17;slebresne;0002-Avoid-modifying-super-column-in-memtable-being-flush-v2.patch;https://issues.apache.org/jira/secure/attachment/12480266/0002-Avoid-modifying-super-column-in-memtable-being-flush-v2.patch","24/May/11 12:51;slebresne;0002-Avoid-modifying-super-column-in-memtable-being-flush.patch;https://issues.apache.org/jira/secure/attachment/12480237/0002-Avoid-modifying-super-column-in-memtable-being-flush.patch","20/May/11 09:11;kochen;CassandraIssue.zip;https://issues.apache.org/jira/secure/attachment/12479889/CassandraIssue.zip","23/May/11 13:37;kochen;CassandraIssueJava.zip;https://issues.apache.org/jira/secure/attachment/12480110/CassandraIssueJava.zip",,,,,,,,,,5.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20770,,,Thu Jun 16 09:34:40 UTC 2011,,,,,,,,,,"0|i0gcs7:",93501,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"20/May/11 09:11;kochen;Test application (.NET 4) to reproduce problem;;;","23/May/11 13:37;kochen;Included the Java version of the test program.

Usage:

java -jar CassandraIssue.jar [<ip> [<keyspace> [<column-family> [<port>]]]]

;;;","24/May/11 12:51;slebresne;I was able to reproduce, thanks for the java version.

I think the problem is that reads can remove subcolumns from a super-column that happens to be in a memtable being flushed. If a subcolumn become gc-able after when the super column count size was written on disk and the time the subcolumn itself is written we won't write it and will end up with short super columns (hence the EOFException). Note that this should not happen with a reasonable gc_grace value (one such that nothing that gets flushed will be gcable).

First attached patch fixes this by making reads copy the super-column before modifying it (0.7 patch).

I think there is a related second bug, in that when we reduce super columns (in QueryFilter), if we merge multiple super column with the same name, we'll ""merge"" them in the first super column. That is, we may end up adding subcolumns to a super column that is in an in-memory memtable. Most of the time this will be harmless, except some useless data duplication. But if that happens for a super column (in a memtable) being flushed and, as above, between the write of the number of column and the actual column writes, we may end up with too long super column. With could result in unreachable columns (i.e, data loss effectively) and quite probably some weird corruption during a compaction.

Second patch fixes this second problem.

I haven't been able to reproduce with the 2 attached patches and the thing is running since more than an hour.
;;;","24/May/11 13:42;jbellis;does the copyOnWrite optimization in patch 1 make sense, given that patch 2 will do a copy anyway?  might be simpler to just force the copy in getCF so any higher-level callers don't have to worry about it.;;;","24/May/11 14:07;slebresne;Yes I agree, patch 2 is actually enough.;;;","24/May/11 14:35;jbellis;Is it?  Don't you still have the problem of a tombstone cleanup modifying things mid-flush?

I was thinking patch 1 modified to always-copy would be enough but there is actually an efficiency gain when you don't have multiple CF/SC merging, so +1 on both if I understand correctly.

Nit: would prefer to expose a CF.isEmpty method than make column size public -- CSLM.size is O(N) and while that doesn't matter here it would be easy to introduce inefficiency w/o realizing it.;;;","24/May/11 15:17;slebresne;bq. Is it? Don't you still have the problem of a tombstone cleanup modifying things mid-flush?

Patch 2 make sure that the cf returned by a getTopLevelColumns() doesn't have any super column that is an alias of a super column in some memtable. So then we don't care what consumers of the result getTopLevelColumns() do. Even if they remove columns the 'being flushed' super column won't be affected.

The idea of not always copying in the first patch was to not incure the copy to all the part of the code that doesn't care (mainly compaction). But anyway, I do think that patch 2 is enough.

Attaching v2 of patch 2 to use isEmpty.;;;","24/May/11 15:26;jbellis;+1;;;","24/May/11 19:13;hudson;Integrated in Cassandra-0.7 #496 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/496/])
    Clone super column to avoid modifying them mid-flush
patch by slebresne; reviewed by jbellis for CASSANDRA-2675

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1127130
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/ColumnFamily.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/filter/QueryFilter.java
;;;","16/Jun/11 09:34;kochen;According to the changes of Cassandra 0.8.0, this is fixed in 0.8.0. The fixed versions in this issue says 0.7.7 and 0.8.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError post truncate,CASSANDRA-2673,12507836,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,ithkuil,ithkuil,20/May/11 01:06,16/Apr/19 09:32,14/Jul/23 05:52,01/Jun/11 15:39,0.7.7,0.8.1,,,,,0,,,,"I had 3 nodes with about 100G in a CF. I run truncate on that CF from cassandra-cli. Then I run cleanup for that CF. I saw this exception shortly after.

 INFO [FlushWriter:5] 2011-05-20 02:56:42,699 Memtable.java (line 157) Writing Memtable-body@1278535630(26722 bytes, 1 operations)
 INFO [FlushWriter:5] 2011-05-20 02:56:42,706 Memtable.java (line 172) Completed flushing /var/lib/cassandra/data/dnet/body-f-1892-Data.db (26915 bytes)
 INFO [NonPeriodicTasks:1] 2011-05-20 02:59:55,981 SSTable.java (line 147) Deleted /var/lib/cassandra/data/dnet/body-f-1892
 INFO [NonPeriodicTasks:1] 2011-05-20 02:59:55,982 SSTable.java (line 147) Deleted /var/lib/cassandra/data/dnet/body-f-1889
 INFO [NonPeriodicTasks:1] 2011-05-20 02:59:55,983 SSTable.java (line 147) Deleted /var/lib/cassandra/data/dnet/body-f-1890
 INFO [NonPeriodicTasks:1] 2011-05-20 02:59:55,983 SSTable.java (line 147) Deleted /var/lib/cassandra/data/dnet/body-f-1888
 INFO [NonPeriodicTasks:1] 2011-05-20 02:59:55,984 SSTable.java (line 147) Deleted /var/lib/cassandra/data/dnet/body-f-1887
 INFO [CompactionExecutor:1] 2011-05-20 03:02:08,724 CompactionManager.java (line 750) Cleaned up to /var/lib/cassandra/data/dnet/body-tmp-f-1891-Data.db.  25,629,365,173 to 25,629,365,173 (~100% of original) bytes for 884,546 keys.  Time: 1,165,900ms.
ERROR [CompactionExecutor:1] 2011-05-20 03:02:08,727 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
	at org.apache.cassandra.io.sstable.SSTableTracker.replace(SSTableTracker.java:108)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:1037)
	at org.apache.cassandra.db.CompactionManager.doCleanupCompaction(CompactionManager.java:769)
	at org.apache.cassandra.db.CompactionManager.access$500(CompactionManager.java:56)
	at org.apache.cassandra.db.CompactionManager$2.call(CompactionManager.java:173)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
",linux 64-bit ubuntu. deb package (datastax). (Random partitioner),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/11 16:33;jbellis;2673.txt;https://issues.apache.org/jira/secure/attachment/12480427/2673.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20768,,,Wed Jun 01 17:47:22 UTC 2011,,,,,,,,,,"0|i0gcrr:",93499,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"20/May/11 20:10;jbellis;Looks like we need to move truncate to the compaction executor so they don't race.

In the meantime, retrying the truncate until it works should be fine.;;;","25/May/11 16:33;jbellis;patch to move the truncation operation to CompactionManager instead of postFlushExecutor. (Not sure what the thinking was there, to be honest. Which may mean it's something subtle I'm missing now, or maybe just that I was smoking crack when I wrote that code originally. CASSANDRA-531, the original truncate ticket, doesn't shed any light here.);;;","01/Jun/11 14:38;slebresne;+1;;;","01/Jun/11 15:39;jbellis;committed;;;","01/Jun/11 17:47;hudson;Integrated in Cassandra-0.7 #501 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/501/])
    fix truncate/compaction race
patch by jbellis; reviewed by slebresne for CASSANDRA-2673

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1130191
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/Table.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/CompactionManager.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/TruncateVerbHandler.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The class o.a.c.cql.jdbc.TypedColumn needs to be declared public,CASSANDRA-2672,12507821,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ardot,ardot,ardot,19/May/11 22:14,16/Apr/19 09:32,14/Jul/23 05:52,19/May/11 22:30,0.8.0,,,Legacy/CQL,,,0,,,,"The implementation of {{ResultSet}} in the JDBC package provides a method: {{unwrap( Class<T> interfaceName)}} in order to allow some of the methods in the {{ResultSet}} implementation {{Class}} to be exposed.

The implementation currently restricts the access to only one acceptable interface: {{CassandraResultSet}}.

Two of the getters in that interface return {{TypedColumn}} which cleverly contains the ""Cassandra"" details of the desired column such as raw column details, and the {{AbstractType}} of the validator and the comparator among others. (Nice!) Unfortunately the {{TypedColumn}} class is not public so is it is not accessible to the callers code.


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ardot,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20767,,,Thu May 19 22:30:55 UTC 2011,,,,,,,,,,"0|i0gcrj:",93498,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"19/May/11 22:30;jbellis;done in r1125144.

Note that CassandraResultSet in general and TypedColumn in particular expose implementation details that may change in the future; use with appropriate caution. :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrub does not close files,CASSANDRA-2669,12507784,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,doubleday,doubleday,19/May/11 17:14,16/Apr/19 09:32,14/Jul/23 05:52,15/Jun/11 12:06,0.7.7,0.8.1,,Legacy/Tools,,,0,,,,"After scrubbing I find that cassandra process still holds file handles to the deleted sstables:

{noformat}
root@blnrzh047:/mnt/cassandra# jps
6932 Jps
32359 CassandraDaemon
32398 CassandraJmxHttpServer

root@blnrzh047:/mnt/cassandra# du -sh .
315G	.

root@blnrzh047:/mnt/cassandra# df -h .
Filesystem            Size  Used Avail Use% Mounted on
/dev/md0              1.1T  626G  420G  60% /mnt/cassandra


root@blnrzh047:/mnt/cassandra# lsof | grep /mnt
java      32359        root  356r      REG                9,0           24    4194599 /mnt/cassandra/data/system/Migrations-f-13-Index.db (deleted)
java      32359        root  357r      REG                9,0       329451    4194547 /mnt/cassandra/data/system/HintsColumnFamily-f-588-Data.db (deleted)
java      32359        root  358r      REG                9,0           22    4194546 /mnt/cassandra/data/system/HintsColumnFamily-f-588-Index.db (deleted)
java      32359        root  359r      REG                9,0       313225    4194534 /mnt/cassandra/data/system/HintsColumnFamily-f-587-Data.db (deleted)
java      32359        root  360r      REG                9,0           22    4194494 /mnt/cassandra/data/system/HintsColumnFamily-f-587-Index.db (deleted)
java      32359        root  361r      REG                9,0        30452    4194636 /mnt/cassandra/data/system/Schema-f-13-Data.db (deleted)
java      32359        root  362r      REG                9,0          484    4194635 /mnt/cassandra/data/system/Schema-f-13-Index.db (deleted)
{noformat}

I guess there's a missing dataFile.close() in CompactionManager:648
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/11 16:52;jbellis;2669.txt;https://issues.apache.org/jira/secure/attachment/12480429/2669.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20764,,,Wed Jun 15 12:06:05 UTC 2011,,,,,,,,,,"0|i0gcqv:",93495,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"25/May/11 16:53;jbellis;patch attached to add finally { close } block.;;;","25/May/11 17:07;slebresne;+1;;;","25/May/11 18:21;hudson;Integrated in Cassandra-0.7 #497 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/497/])
    close scrub file handles
patch by jbellis; reviewed by slebresne for CASSANDRA-2669

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1127589
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/CompactionManager.java
;;;","15/Jun/11 12:06;slebresne;Seems correctly committed, closing;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
don't perform HH to client-mode nodes,CASSANDRA-2668,12507780,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,19/May/11 16:54,16/Apr/19 09:32,14/Jul/23 05:52,19/May/11 17:11,0.7.7,0.8.1,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/May/11 16:55;jbellis;2668.txt;https://issues.apache.org/jira/secure/attachment/12479800/2668.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20763,,,Thu May 19 17:39:07 UTC 2011,,,,,,,,,,"0|i0gcqn:",93494,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"19/May/11 16:55;jbellis;one-line fix;;;","19/May/11 17:03;brandon.williams;+1;;;","19/May/11 17:11;jbellis;committed;;;","19/May/11 17:39;hudson;Integrated in Cassandra-0.7 #490 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/490/])
    don't perform HH to client-mode
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-2668

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1125002
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/gms/EndpointState.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/StorageService.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodes get ignored by dynamic snitch when read repair chance is zero,CASSANDRA-2662,12507590,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,doubleday,doubleday,18/May/11 10:55,16/Apr/19 09:32,14/Jul/23 05:52,28/Jul/11 23:43,0.7.9,0.8.5,,,,,0,,,,"DynamicEndpointSnitch falls back to subsnitch when one of the scores of the endpoints being compared is missing.

This leads to a stable order of hosts until reads will lead to recorded scores. 
If setting read repair chance to 0 and reads are performed with quorum then (rf - # quorum nodes) will never get reads.",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/May/11 18:05;brandon.williams;2662.txt;https://issues.apache.org/jira/secure/attachment/12480974/2662.txt","18/May/11 10:58;doubleday;dynsnitch.patch;https://issues.apache.org/jira/secure/attachment/12479566/dynsnitch.patch",,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20759,,,Fri Aug 19 22:50:14 UTC 2011,,,,,,,,,,"0|i0gcpj:",93489,,thepaul,,thepaul,Low,,,,,,,,,,,,,,,,,"18/May/11 10:58;doubleday;One very simple fix is to initialize scores with 0 which forces at least one read. 

Dunno if thats a good idea when using multi dc snitches ...;;;","18/May/11 15:20;brandon.williams;To clarify what this patch does: on the first round, behavior is unchanged, it will fall back to the subsnitch as usual.  On the second round however, the nodes will have scores so the dynamic snitch will determine the best host.  This seems like a good idea, but I think we should proceed cautiously with regard to CASSANDRA-1314.  This is certainly going to blow the cache on the second round, since the unpinned hosts at zero are guaranteed to be ranked higher than the pinned one.  However, in the current implementation, the badness threshold in CASSANDRA-1519 can never take effect at RR = 0 since the scores of the other replicas will always be null, and thus diversion to the subsnitch will always occur, returning the pinned replica.  With this patch, the second replica is likely to end up being the pinned one, which I think will be ok, as long as all coordinators agree, though this might not always be the case.;;;","18/May/11 19:18;brandon.williams;Thinking this through a bit more:

Given coordinator A, and replicas X, Y, and Z (in subsnitch order), on the first round X will be chosen, and let's say it receives a score of 1.  With the patch, at this point Y and Z will be initialized with zero.  On the next round, Y will be chosen, and let's say it receives a score of or near 1, depending on network latency.  On the third round, Z will be chosen, and let's say it also receives a score similar to Y.  Now the cache is hot on all nodes, and subsequent reads have the possibility to oscillate between all three based on network latency variance.  This can be mitigated though with the badness threshold.  With the badness threshold on, the first round will occur as before, but subsequent rounds will continue to use X until it degrades past the threshold, at which point they will use Y, until the dynamic snitch reset()s, at which point everything will repeat.  I don't think this is harmful to CASSANDRA-1314 after all.

;;;","18/May/11 19:47;brandon.williams;There is one problem with this patch, rather that initializing the score to zero, it needs to create an empty AdaptiveLatencyTracker for them before checking the scores, otherwise DES.reset() will never have an effect on those hosts.;;;","31/May/11 18:05;brandon.williams;Updated patch ensures ALTs are created for the hosts by using receiveTiming;;;","28/Jul/11 22:56;thepaul;+1;;;","28/Jul/11 23:43;brandon.williams;Committed.;;;","29/Jul/11 00:24;hudson;Integrated in Cassandra-0.7 #537 (See [https://builds.apache.org/job/Cassandra-0.7/537/])
    DES shouldn't ignore nodes when the read repair chance is zero.
Patch by brandonwilliams reviewed by Paul Cannon for CASSANDRA-2662

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1152038
Files : 
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/locator/DynamicEndpointSnitch.java
;;;","19/Aug/11 22:50;jbellis;(This didn't get merged to 0.8 until just now.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BRAF.sync() bug can cause massive commit log write magnification,CASSANDRA-2660,12507480,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,17/May/11 12:22,16/Apr/19 09:32,14/Jul/23 05:52,17/May/11 14:55,0.7.7,0.8.1,,,,,0,,,,"This was discovered, fixed and tested on 0.7.5. Cursory examination shows it should still be an issue on trunk/0.8. If people otherwise agree with the patch I can rebase if necessary.

Problem:

BRAF.flush() is actually broken in the sense that it cannot be called without close co-operation with the caller. rebuffer() does the co-op by adjusting bufferOffset and validateBufferBytes appropriately, by sync() doesn't. This means sync() is broken, and sync() is used by the commit log.

The attached patch moves the bufferOffset/validateBufferBytes handling out into resetBuffer() and has both flush() and rebuffer() call that. This makes sync() safe.

What happened was that for batched commit log mode, every time sync() was called the data buffered so far would get written to the OS and fsync():ed. But until rebuffer() is called for other reasons as part of the write path, all subsequent sync():s would result in the very same data (plus whatever was written since last time) being re-written and fsync():ed again. So first you write+fsync N bytes, then N+N1, then N+N1+N2... (each N being a batch), until at some point you trigger a rebuffer() and it starts all over again.

The result is that you see *a lot* more writes to the commit log than are in fact written to the BRAF. And these writes translate into actual real writes to the underlying storage device due to fsync(). We had crazy numbers where we saw spikes upwards of 80 mb/second where the actual throughput was more like ~ 1 mb second of data to the commit log.

(One can make a possibly weak argument that it is also functionally incorrect as I can imagine implementations where re-writing the same blocks does copy-on-write in such a way that you're not necessarily guaranteed to see before-or-after data, particularly in case of partial page writes. However that's probably not a practical issue.)

Worthy of noting is that this probably causes added difficulties in fsync() latencies since the average fsync() will contain a lot more data. Depending on I/O scheduler and underlying device characteristics, the extra writes *may* not have a detrimental effect, but I think it's pretty easy to point to cases where it will be detrimental - in particular if the commit log is on a non-battery backed drive. Even with a nice battery backed RAID with the commit log on, the size of the writes probably contributes to difficulty in making the write requests propagate down without being starved by reads (but this is speculation, not tested, other than that I've observed commit log writer starvation that seemed excessive).

This isn't the first subtle BRAF bug. What are people's thoughts on creating separate abstractions for streaming I/O that can perhaps be a lot more simple, and use BRAF only for random reads in response to live traffic? (Not as part of this JIRA, just asking in general.)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/11 12:23;scode;CASSANDRA-2660-075.txt;https://issues.apache.org/jira/secure/attachment/12479455/CASSANDRA-2660-075.txt",,,,,,,,,,,,,,1.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20758,,,Tue May 17 16:18:38 UTC 2011,,,,,,,,,,"0|i0gcp3:",93487,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"17/May/11 14:07;jbellis;bq. What are people's thoughts on creating separate abstractions for streaming I/O that can perhaps be a lot more simple, and use BRAF only for random reads in response to live traffic? (Not as part of this JIRA, just asking in general.)

Every time I've looked at doing this I've put it aside because making all writes two-pass (first pass to compute size, so we don't have to seek back after serializing the row itself) is such a pain.;;;","17/May/11 14:55;jbellis;Ideally we wouldn't wipe out the buffer for read purposes but since we are mixing rw in the same buffer (see comments above) this is the best option.  Committed.;;;","17/May/11 15:50;hudson;Integrated in Cassandra-0.7 #487 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/487/])
    mark BRAF buffer invalid post-flush so we don't re-flush partial buffers again
patch by Peter Schuller; reviewed by jbellis for CASSANDRA-2660
;;;","17/May/11 16:18;jbellis;merged to 0.8 branch cleanly;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
index scan errors out when zero columns are requested,CASSANDRA-2653,12507224,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,jbellis,jbellis,14/May/11 15:02,16/Apr/19 09:32,14/Jul/23 05:52,29/Jun/11 19:41,0.7.7,0.8.2,,,,,0,,,,"As reported by Tyler Hobbs as an addendum to CASSANDRA-2401,

{noformat}
ERROR 16:13:38,864 Fatal exception in thread Thread[ReadStage:16,5,main]
java.lang.AssertionError: No data found for SliceQueryFilter(start=java.nio.HeapByteBuffer[pos=10 lim=10 cap=30], finish=java.nio.HeapByteBuffer[pos=17 lim=17 cap=30], reversed=false, count=0] in DecoratedKey(81509516161424251288255223397843705139, 6b657931):QueryPath(columnFamilyName='cf', superColumnName='null', columnName='null') (original filter SliceQueryFilter(start=java.nio.HeapByteBuffer[pos=10 lim=10 cap=30], finish=java.nio.HeapByteBuffer[pos=17 lim=17 cap=30], reversed=false, count=0]) from expression 'cf.626972746864617465 EQ 1'
	at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1517)
	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:42)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}",,cburroughs,cherro,skamio,stinkymatt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/11 18:07;slebresne;0001-Fix-scan-issue.patch;https://issues.apache.org/jira/secure/attachment/12484669/0001-Fix-scan-issue.patch","22/Jun/11 12:29;slebresne;0001-Handle-data-get-returning-null-in-secondary-indexes.patch;https://issues.apache.org/jira/secure/attachment/12483436/0001-Handle-data-get-returning-null-in-secondary-indexes.patch","22/Jun/11 13:22;slebresne;0001-Handle-null-returns-in-data-index-query-v0.7.patch;https://issues.apache.org/jira/secure/attachment/12483440/0001-Handle-null-returns-in-data-index-query-v0.7.patch","30/May/11 11:07;slebresne;0001-Reset-SSTII-in-EchoedRow-constructor.patch;https://issues.apache.org/jira/secure/attachment/12480836/0001-Reset-SSTII-in-EchoedRow-constructor.patch","27/Jun/11 17:34;slebresne;2653_v2.patch;https://issues.apache.org/jira/secure/attachment/12483978/2653_v2.patch","28/Jun/11 08:10;slebresne;2653_v3.patch;https://issues.apache.org/jira/secure/attachment/12484407/2653_v3.patch","27/May/11 20:34;tjake;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2653-reproduce-regression.txt;https://issues.apache.org/jira/secure/attachment/12480690/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2653-reproduce-regression.txt",,,,,,,,7.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20755,,,Wed Jun 29 19:41:51 UTC 2011,,,,,,,,,,"0|i0gcnj:",93480,,,,,Low,,,,,,,,,,,,,,,,,"27/May/11 20:34;tjake;Attached testcase reproduces the error every time

Run like:

ant long-test -Dtest.name=IndexCorruptionTest;;;","27/May/11 20:37;tjake;{noformat}
long-test:
     [echo] running long tests
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/Users/jake/workspace/cassandra-git/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
    [junit] Testsuite: org.apache.cassandra.db.IndexCorruptionTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 247.427 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 16:31:03,330 Fatal exception in thread Thread[ReadStage:5,5,main]
    [junit] java.lang.AssertionError: No data found for NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17]) in DecoratedKey(Token(bytes[004600460048004d00540049005900590048005400460059004a0048004b0055004e00550048004b00530055005400480055004b004f004b004a00460058004600000001000100010001000100010001000100e3000100010001000100e3000100010001000100e3000100010001000100e30001000100010001000100010001000100010001000100010000000100010001000100010001000100010003000100010001000100030001000100010001000300010001000100010003000100010001000100010001000100010001000100010001]), 30303237623366662d326230662d343235632d386332352d616362326335393534306530):QueryPath(columnFamilyName='inode', superColumnName='null', columnName='null') (original filter NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17])) from expression 'inode.73656e74696e656c EQ 78'
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1517)
    [junit] 	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:42)
    [junit] 	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:680)
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: runTest(org.apache.cassandra.db.IndexCorruptionTest):	Caused an ERROR
    [junit] TimedOutException()
    [junit] java.io.IOException: TimedOutException()
    [junit] 	at org.apache.cassandra.db.IndexCorruptionTest.listDeepSubPaths(IndexCorruptionTest.java:107)
    [junit] 	at org.apache.cassandra.db.IndexCorruptionTest.runTest(IndexCorruptionTest.java:64)
    [junit] Caused by: TimedOutException()
    [junit] 	at org.apache.cassandra.thrift.Cassandra$get_indexed_slices_result.read(Cassandra.java:13801)
    [junit] 	at org.apache.cassandra.thrift.Cassandra$Client.recv_get_indexed_slices(Cassandra.java:810)
    [junit] 	at org.apache.cassandra.thrift.Cassandra$Client.get_indexed_slices(Cassandra.java:782)
    [junit] 	at org.apache.cassandra.db.IndexCorruptionTest.listDeepSubPaths(IndexCorruptionTest.java:90)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.IndexCorruptionTest FAILED

BUILD FAILED
/Users/jake/workspace/cassandra-git/build.xml:1082: The following error occurred while executing this line:
/Users/jake/workspace/cassandra-git/build.xml:1037: Some long test(s) failed.

Total time: 4 minutes 14 seconds

{noformat};;;","27/May/11 21:12;tjake;Definitely related to compaction...

The memtable flushes minute and the test fails > 4

If you change the min compaction thresh to 2 it fails > 2;;;","30/May/11 11:07;slebresne;This is indeed compaction related (but not related to secondary indexing at
all). The problem is that compaction may lose some rows.

Because of the way the ReducingIterator works, when we create a new
{Pre|Lazy|Echoed}CompactedRow, we have already decoded the next row key and
the file pointer if after that next row key. Both PreCompactedRow and
LazyCompactedRow handle this correctly by ""resetting"" their
SSTableIdentityIterator before reading (SSTII.getColumnFamilyWithColumns()
does it for PreCompactedRow and LazilyCompactedRow calls SSTII.reset()
directly). But EchoedRow doesn't handle this correctly. Hence when
EchoedRow.isEmpty() is called, it will call SSTII.hasNext(), that will compare
the current file pointer to the finishedAt value of the iterator. The pointer
being on the next row, this test will always fail and the row will be skipped.

Attaching a patch against 0.8 with a (smaller) unit test.

Note that luckily this doesn't affect 0.7, because it only uses EchoedRow for
cleanup compactions and clean compactions does not use ReducingIterator (and
thus, the underlying SSTII won't have changed when the EchoedRow is built).
I would still be in favor of committing the patch there too, just to make sure
we don't hit this later.;;;","30/May/11 12:47;jbellis;+1 for 0.7 / 0.8;;;","30/May/11 13:27;hudson;Integrated in Cassandra-0.7 #500 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/500/])
    Reset SSTII in EchoedRow iterator (see CASSANDRA-2653)

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1129151
Files : 
* /cassandra/branches/cassandra-0.7/test/unit/org/apache/cassandra/db/CompactionsTest.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/CompactionManager.java
;;;","20/Jun/11 04:14;jbellis;Is this actually fixed for the zero-columns-requested original problem?;;;","20/Jun/11 04:44;slebresne;This really primarily fixes the error from Jake's test cases. I'll have to admit that's the only I looked. I did not realize the original problem was not necessarily related and so it is very possible (even likely) this does not fix the zero-columns-requested problem.;;;","20/Jun/11 16:53;jbellis;reopening to fix ""the tyler issue."";;;","22/Jun/11 12:29;slebresne;The ""Tyler"" problem is actually not limited to 0 column query. The problem is that when we query the rows for data, we use whatever filter the user provided (there's a number of optimiziation in the case we have more than 1 clause but that doesn't really matter for our problem). The thing is, there is no guarantee that whatever that filter is, it will include the column of the primary clause (having a column count of 0 is just one case where we're sure it won't include it). Thus the assertion that something will be returned is bogus.

Attaching a patch (against 0.8) to fix. Note that this mean we have no way to assert the sanity of the index during a read, unless we force the querying of the primary index clause, but this will have a performance impact (and a non negligible one in cases this would force us to do a new query just for that).
;;;","22/Jun/11 13:22;slebresne;This also affects 0.7 actually so attaching a patch for 0.7.;;;","22/Jun/11 13:49;jbellis;Is there a way we can keep a sanity check here?  CASSANDRA-2401 was not so long ago.;;;","22/Jun/11 13:59;slebresne;As I said earlier, I think the only way to keep one would be to force the querying of the primary index clause column name. In some cases, when we already do a NameQuery, either as part of the first data query or because we need a query for the extraFilter, this won't be a big deal. If it's a slice query and the primary index clause name is part of the return, we're good to. But otherwise, we'll have to do a specific query to validate the assert. Maybe the cases where we'll have to do an extra query are considered low enough than we think it's worth. But then there is the other problem.

The other problem is that this assertion is not thread safe, because the query to the index and the data is not atomic. ;;;","22/Jun/11 18:46;jbellis;bq. I think the only way to keep one would be to force the querying of the primary index clause column name... but this will have a performance impact

I think we should take the impact.  (The common query that we want to be fast is name-based and this won't affect that.);;;","27/Jun/11 17:34;slebresne;I actually agree with taking the impact. Especially given that there is actually very little cases where it will make an actual difference anyway.

Attaching patch (2653_v2, based on 0.7) that implement the idea and add back the sanity check.;;;","27/Jun/11 21:48;jbellis;doesn't this assert still have the ""the query to the index and the data is not atomic"" problem?;;;","28/Jun/11 08:24;slebresne;bq. doesn't this assert still have the ""the query to the index and the data is not atomic"" problem?

No you're right, I focused on adding back the assert forgetting it wasn't safe in the first place. Attaching v3 based on v2, but instead of asserting that the row return contains the primary clause column, it skips the row if it doesn't contain it. That is, instead of asserting the non-corruption of the index, it ignores any possible corruption. But more importantly (one could hope we don't have a bug that corrupt indexes), it will avoid returning incoherent result to the user in the event of a race between reads and writes.

Trying to prevent the race from happening would require synchronization with write, which will be much harder and less efficient. And we probably need to have a fix for that out sooner than later (both the error when zero columns are requested and the possibly to throw assertion errors wrongly).

In the longer term, I think we should explore the possibility of stopping to care whether our secondary indexes are coherent at all time and repair them at read time as  this may allow us to get rid of the read-before-write. But it's a longer term goal at best and work for another ticket.

 ;;;","28/Jun/11 15:16;jbellis;+1;;;","29/Jun/11 15:48;hudson;Integrated in Cassandra-0.7 #517 (See [https://builds.apache.org/job/Cassandra-0.7/517/])
    Fix scan wrongly throwing assertion errors
patch by slebresne; reviewed by jbellis for CASSANDRA-2653

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1141129
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
;;;","29/Jun/11 18:07;slebresne;Actually, after having committed it, I realize there is a few issue with the previous patch. Two mostly:
# If the extraFilter query finds nothing (which it will only in case of the race between write and reads), getColumnFamily() will return null and the data.addAll() will NPE
# For 0.8 and for counters, we must make really sure that this extra query won't add column that were returned by the first query (which can happen in the current code), otherwise we'll overcount. I think this is actually a bug that predate the fix for this.

Anyway, attaching 0001-Fix-scan-issue that fixes both of those issue. It also add a slight optimization that avoids doing extra work if we know an extra query won't help.;;;","29/Jun/11 19:27;jbellis;+1;;;","29/Jun/11 19:41;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted handoff needs to adjust page size for lage columns to avoid OOM,CASSANDRA-2652,12507223,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,14/May/11 14:52,16/Apr/19 09:32,14/Jul/23 05:52,16/May/11 21:16,0.7.7,,,,,,0,,,,"Example OOM:
{noformat}
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:269)
	at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:356)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:318)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:99)
	at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:248)
	at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:268)
	at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:227)
	at java.util.concurrent.ConcurrentSkipListMap.buildFromSorted(ConcurrentSkipListMap.java:1493)
	at java.util.concurrent.ConcurrentSkipListMap.<init>(ConcurrentSkipListMap.java:1443)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:379)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:362)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:322)
	at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.getNextBlock(IndexedSliceReader.java:179)
	at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:121)
	at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:49)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
	at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
	at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
	at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:116)
	at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:130)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1390)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1267)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1195)
	at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:138)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:331)
	at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/11 14:52;jbellis;2652.txt;https://issues.apache.org/jira/secure/attachment/12479227/2652.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20754,,,Mon May 16 21:36:14 UTC 2011,,,,,,,,,,"0|i0gcnb:",93479,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"14/May/11 14:52;jbellis;(also renames sendMessage to sendRow.);;;","16/May/11 21:10;brandon.williams;+1;;;","16/May/11 21:16;jbellis;committed;;;","16/May/11 21:36;hudson;Integrated in Cassandra-0.7 #486 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/486/])
    adjust hinted handoff page size to avoid OOM with large columns
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-2652
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inferred Rack and DC Values Should be Unsigned,CASSANDRA-2651,12507209,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,jpisk,jpisk,14/May/11 08:27,16/Apr/19 09:32,14/Jul/23 05:52,16/May/11 21:44,0.8.1,,,,,,0,,,,RackInferringSnitch formats IP address octets as signed byte values when inferring rack and data center values.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/11 08:29;jpisk;trunk-2651.txt;https://issues.apache.org/jira/secure/attachment/12479219/trunk-2651.txt",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20753,,,Mon May 16 21:44:22 UTC 2011,,,,,,,,,,"0|i0gcn3:",93478,,,,,Low,,,,,,,,,,,,,,,,,"14/May/11 08:29;jpisk;RackInferringSnitch patch;;;","16/May/11 21:44;brandon.williams;Committed to 0.8.1 to go along with CASSANDRA-2531;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
work-around schema disagreements from cqlsh,CASSANDRA-2649,12507173,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,13/May/11 20:16,16/Apr/19 09:32,14/Jul/23 05:52,07/Jun/11 03:15,0.8.0,,,Legacy/Tools,,,0,cql,,,"It is handy to be able to put CQL statements in a flat-file and load them by redirecting to {{cqlsh}} stdin, but this can fail on a cluster when executing statements that modify schema.

The attached patch works around this problem by retrying up to 3 times, with a progressive delay after each attempt.  A better solution would probably be to compare schema versions, but this seems to work well enough, and is better than _not_ handling it at all.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/May/11 20:17;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2649-try-to-work-around-schema-disagreement-.txt;https://issues.apache.org/jira/secure/attachment/12479162/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2649-try-to-work-around-schema-disagreement-.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20752,,,Mon May 23 21:18:23 UTC 2011,,,,,,,,,,"0|i0gcmn:",93476,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"20/May/11 20:05;jbellis;bq. A better solution would probably be to compare schema versions

Agreed.  Why not just do that?  describe_schema_versions makes it extremely easy.;;;","20/May/11 22:51;urandom;Well, because this currently employs no use of the thrift RPC API, (so it would be much more invasive to implement).

Also, after having some time to think about this, I'm not sure it would be that much better.  You'd still need to wait and retry between each disagreement.  The only advantage would be access to the actual versions for error reporting (and considering the nature of cqlsh, it's arguably not the right audience for that level of detail anyway).;;;","23/May/11 15:43;jbellis;The right thing to do is wait for agreement after each schema change. Relying on integrityerror is broken since inserts will just fail.  using describe_schema_versions let's this be 100% correct instead of hoping you slept long enough before proceeding.

bq. this currently employs no use of the thrift RPC API

since cql is built on thrift, it wouldn't be difficult to use the thrift method until there is a ""native"" replacement.

;;;","23/May/11 20:17;jbellis;Let's create a ticket to do this ""right"" for 0.8.1, but this patch is definitely a minimally inasive improvement on the existing multiple-schema-changes-are-guaranteed-to-hose-cqlsh status quo, so +1 for 0.8.0.;;;","23/May/11 21:18;hudson;Integrated in Cassandra-0.8 #129 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/129/])
    work-around schema disagreements

Patch by eevans; review by jbellis for CASSANDRA-2649

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1126736
Files : 
* /cassandra/branches/cassandra-0.8/drivers/py/cqlsh
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra can't find jamm on startup,CASSANDRA-2647,12507147,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,synchrom,synchrom,13/May/11 16:47,16/Apr/19 09:32,14/Jul/23 05:52,19/May/11 18:21,0.8.0,,,Packaging,,,0,,,,"I installed the Debian package (from http://www.apache.org/dist/cassandra/debian unstable) of Cassandra 0.8beta2 on Ubuntu 10.04 with the sun jdk over a working copy of 0.7.2. It broke on restart.
On startup it gives this:
{code}
Error occurred during initialization of VM
agent library failed to init: instrument
Error opening zip file or JAR manifest missing : /lib/jamm-0.2.2.jar
{code}
/etc/cassandra/cassandra-env.sh contains this:
{code}
# add the jamm javaagent
check_openjdk=$(java -version 2>&1 | awk '{if (NR == 2) {print $1}}')
if [ ""$check_openjdk"" != ""OpenJDK"" ]
then
    JVM_OPTS=""$JVM_OPTS -javaagent:$CASSANDRA_HOME/lib/jamm-0.2.2.jar""
fi
{code}
By default, CASSANDRA_HOME is not set, so it's looking in /lib for this jar. It seems CASSANDRA_HOME should be set to /usr/share/cassandra, since that's where jamm-0.2.2.jar is installed, but that means the path is still wrong.

I set CASSANDRA_HOME to /usr/share/cassandra and changed the JVM_OPTS line to this:
{code}
    JVM_OPTS=""$JVM_OPTS -javaagent:$CASSANDRA_HOME/jamm-0.2.2.jar""
{code}

and then cassandra started ok.

Is this a bug or did I miss something?

I also noticed that this line appears to be the only use of CASSANDRA_HOME.",Ubuntu 10.04 Lucid,jtrav,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/May/11 22:13;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2647-move-jars-under-lib-to-satisfy-hard-cod.txt;https://issues.apache.org/jira/secure/attachment/12479180/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2647-move-jars-under-lib-to-satisfy-hard-cod.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20750,,,Tue May 24 19:54:21 UTC 2011,,,,,,,,,,"0|i0gcm7:",93474,,thepaul,,thepaul,Low,,,,,,,,,,,,,,,,,"13/May/11 19:51;brandon.williams;CASSANDRA_HOME should be getting set in cassandra.in.sh which is the first thing sourced by bin/cassandra and /etc/init.d/cassandra;;;","13/May/11 21:01;synchrom;Thanks for that. I think I'd normally be looking for /etc/defaults/cassandra to set that kind of thing. Also, the supplied cassandra.in.sh does not set CASSANDRA_HOME, not even commented-out. It might be nice to mention that file in a comment in /etc/cassandra/cassandra.yaml;;;","13/May/11 22:15;urandom;the attached (untested )patch should deal with this (it's one way of dealing with it anyway).;;;","16/May/11 08:39;synchrom;Not tested that either, but it looks like it would make cassandra work 'out of the box' on Debian/Ubuntu, fixing both my issues. Thanks.;;;","16/May/11 17:29;thepaul;Tested and verified Eric's patch, both via initscript and invocation through /usr/sbin/cassandra; the right path to jamm (and all the other lib jars) gets set correctly. +1 for the patch.

I think ideally the rest of the initscript would also make use of $CASSANDRA_HOME (like when specifying the location of the jars to add to the classpath), but since the deb isn't expected to support a change to $CASSANDRA_HOME, it's fine as is.;;;","16/May/11 19:27;jtrav;Forgive the awfully dumb question - I am a new user trying to get Cassandra running (for the first time) and ran into this issue as well.

What's the preferred method for applying this patch to the code base?  Running Natty x86_64 server.

Thanks in advance.;;;","16/May/11 19:43;thepaul;bq. What's the preferred method for applying this patch to the code base? Running Natty x86_64 server.

Joe- I would say your options are:

1. Apply the patch to a source checkout from the 0.8.0-beta2 tag (using ""patch -p1 < patch.txt"" from the toplevel directory), then build your own custom .debs (using, maybe, ""dpkg-buildpackage -us -uc"").

2. Just change this line in /etc/cassandra/cassandra-env.sh:

{code}
    JVM_OPTS=""$JVM_OPTS -javaagent:$CASSANDRA_HOME/lib/jamm-0.2.2.jar""
{code}

to

{code}
    JVM_OPTS=""$JVM_OPTS -javaagent:/usr/share/cassandra/jamm-0.2.2.jar""
{code}

That solves the same problem in a different way. So when you upgrade to an official package with the real fix, you will want to change /etc/cassandra/cassandra-env.sh back to the way it was.

3. Just wait for 0.8.0 beta3 to be released and use that;;;","16/May/11 19:46;jtrav;Thanks for the quick response, Paul.  I've taken approach #2 for now, and will loop back when I upgrade.  Much appreciated.;;;","19/May/11 18:21;jbellis;committed;;;","24/May/11 11:19;synchrom;I just noticed this [on the wiki|http://wiki.apache.org/cassandra/RunningCassandra]:
{quote}
Startup options should be configured in /etc/default/cassandra (not /usr/share/cassandra/cassandra.in.sh) on Debian systems.
{quote};;;","24/May/11 17:34;thepaul;bq. Startup options should be configured in /etc/default/cassandra (not /usr/share/cassandra/cassandra.in.sh) on Debian systems.

That's correct, dpkg won't track changes to config files outside of /etc without special treatment. Is something wrong with that?;;;","24/May/11 19:20;synchrom;It was just that this bug's current patch does the opposite. I mentioned earlier that I'd expect things like CASSANDRA_HOME to be set in /etc/defaults/cassandra rather than somewhere in /usr.;;;","24/May/11 19:54;thepaul;bq. It was just that this bug's current patch does the opposite. I mentioned earlier that I'd expect things like CASSANDRA_HOME to be set in /etc/defaults/cassandra rather than somewhere in /usr.

CASSANDRA_HOME can be overridden by the user in /etc/cassandra/cassandra-env.sh; it's sourced after CASSANDRA_HOME is set to a default in both /etc/init.d/cassandra and /usr/sbin/cassandra .;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make bootstrap retry,CASSANDRA-2644,12507049,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,12/May/11 22:22,16/Apr/19 09:32,14/Jul/23 05:52,30/May/11 08:52,0.8.1,,,,,,1,,,,"We ran into a situation where we had rpc_timeout set to 1 second, and the node needing to compute the token took over a second (1.6 seconds). The bootstrapping node hangs forever without getting a token because the expiring map removes it before the reply comes back.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/11 22:41;lenn0x;0001-Make-ExpiringMap-have-objects-with-specific-timeouts.patch;https://issues.apache.org/jira/secure/attachment/12479021/0001-Make-ExpiringMap-have-objects-with-specific-timeouts.patch","12/May/11 22:41;lenn0x;0002-Make-bootstrap-retry-and-increment-timeout-for-every.patch;https://issues.apache.org/jira/secure/attachment/12479022/0002-Make-bootstrap-retry-and-increment-timeout-for-every.patch",,,,,,,,,,,,,2.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20749,,,Mon May 30 14:55:47 UTC 2011,,,,,,,,,,"0|i0gclj:",93471,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"12/May/11 22:23;lenn0x;I have a patch for this I'll be adding within the next day. I make ExpiringMap support custom timeouts per object, and make bootstrap getToken retry, while exponentially increasing the timeout until retries is met.;;;","12/May/11 22:52;stuhood;+1
Thanks Chris!;;;","12/May/11 23:09;jbellis;I think the retry logic is a distraction here. If it doesn't work the first time because of anything other than ""we didn't wait long enough"" (i.e. it errored out) it's not likely to magically unbreak for the second.

Suggest just giving it a long retry to begin with.;;;","13/May/11 04:37;stuhood;Good point. But there are still cases that retries will recover from... flapping/down nodes, etc.

IMO, we should fix the performance issue independently (opened CASSANDRA-2645), and consider retrying to be a best practice that should spread to more places in the codebase.;;;","13/May/11 07:35;slebresne;Not fully related to the discussion here but streaming is another part of bootstrap so I'll mention that CASSANDRA-2433 introduces some mechanism to handle unrecoverable failures during streaming (that is, streaming already retry on errors but 1) it retries indefinitely while the CASSANDRA-2433 introduce a max retry and 2) it doesn't detect the other end being dead). Anyway, just referencing the ticket so that if this ticket becomes ""make bootstrap handle failures better"", we don't duplicate efforts.   ;;;","17/May/11 22:17;jbellis;bq. But there are still cases that retries will recover from... flapping/down nodes

Fair enough, but increasing the timeout is still unwarranted.  Let's just make it wait for max(DEFAULT_TIMEOUT, BOOTSTRAP_TIMEOUT) with B_T equal to, say, 30s.

Committed patch 01 to 0.8.1 branch, btw.;;;","30/May/11 08:48;lenn0x;Made changes to 02 patch, and commited to 0.8.1. Thanks!;;;","30/May/11 14:44;hudson;Integrated in Cassandra #912 (See [https://builds.apache.org/hudson/job/Cassandra/912/])
    ;;;","30/May/11 14:55;hudson;Integrated in Cassandra-0.8 #146 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/146/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
read repair/reconciliation breaks slice based iteration at QUORUM,CASSANDRA-2643,12507010,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,byronclark,scode,scode,12/May/11 16:27,16/Apr/19 09:32,14/Jul/23 05:52,02/Aug/11 13:27,1.0.0,,,,,,0,,,,"In short, I believe iterating over columns is impossible to do reliably with QUORUM due to the way reconciliation works.

The problem is that the SliceQueryFilter is executing locally when reading on a node, but no attempts seem to be made to consider limits when doing reconciliation and/or read-repair (RowRepairResolver.resolveSuperset() and ColumnFamily.resolve()).

If a node slices and comes up with 100 columns, and another node slices and comes up with 100 columns, some of which are unique to each side, reconciliation results in > 100 columns in the result set. In this case the effect is limited to ""client gets more than asked for"", but the columns still accurately represent the range. This is easily triggered by my test-case.

In addition to the client receiving ""too many"" columns, I believe some of them will not be satisfying the QUORUM consistency level for the same reasons as with deletions (see discussion below).

Now, there *should* be a problem for tombstones as well, but it's more subtle. Suppose A has:

  1
  2
  3
  4
  5
  6

and B has:

  1
  del 2
  del 3
  del 4
  5
  6 

If you now slice 1-6 with count=3 the tombstones from B will reconcile with those from A - fine. So you end up getting 1,5,6 back. This made it a bit difficult to trigger in a test case until I realized what was going on. At first I was ""hoping"" to see a ""short"" iteration result, which would mean that the process of iterating until you get a short result will cause spurious ""end of columns"" and thus make it impossible to iterate correctly.

So; due to 5-6 existing (and if they didn't, you legitimately reached end-of-columns) we do indeed get a result of size 3 which contains 1,5 and 6. However, only node B would have contributed columns 5 and 6; so there is actually no QUORUM consistency on the co-ordinating node with respect to these columns. If node A and C also had 5 and 6, they would not have been considered.

Am I wrong?

In any case; using script I'm about to attach, you can trigger the over-delivery case very easily:

(0) disable hinted hand-off to avoid that interacting with the test
(1) start three nodes
(2) create ks 'test' with rf=3 and cf 'slicetest'
(3) ./slicetest.py hostname_of_node_C insert # let it run for a few seconds, then ctrl-c
(4) stop node A
(5) ./slicetest.py hostname_of_node_C insert # let it run for a few seconds, then ctrl-c
(6) start node A, wait for B and C to consider it up
(7) ./slicetest.py hostname_of_node_A slice # make A co-ordinator though it doesn't necessarily matter

You can also pass 'delete' (random deletion of 50% of contents) or 'deleterange' (delete all in [0.2,0.8]) to slicetest, but you don't trigger a short read by doing that (see discussion above).
",,byronclark,scode,segy,skamio,slebresne,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/11 03:58;byronclark;ASF.LICENSE.NOT.GRANTED--short_read_0.8.sh;https://issues.apache.org/jira/secure/attachment/12488060/ASF.LICENSE.NOT.GRANTED--short_read_0.8.sh","29/Jul/11 06:25;byronclark;CASSANDRA-2643-poc.patch;https://issues.apache.org/jira/secure/attachment/12488173/CASSANDRA-2643-poc.patch","30/Jul/11 02:45;byronclark;CASSANDRA-2643-v2.patch;https://issues.apache.org/jira/secure/attachment/12488280/CASSANDRA-2643-v2.patch","02/Aug/11 04:11;byronclark;CASSANDRA-2643-v3.patch;https://issues.apache.org/jira/secure/attachment/12488855/CASSANDRA-2643-v3.patch","29/Jul/11 03:39;byronclark;reliable_short_read_0.8.sh;https://issues.apache.org/jira/secure/attachment/12488169/reliable_short_read_0.8.sh","13/May/11 15:01;slebresne;short_read.sh;https://issues.apache.org/jira/secure/attachment/12479111/short_read.sh","12/May/11 16:28;scode;slicetest.py;https://issues.apache.org/jira/secure/attachment/12478987/slicetest.py",,,,,,,,7.0,byronclark,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20748,,,Tue Aug 02 14:22:27 UTC 2011,,,,,,,,,,"0|i0gclb:",93470,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"12/May/11 19:04;scode;Unless I'm off, the problem with deleted columns is fairly similar to that of deleted rows. Like with range ghosts, an option would be to propagate tombstones to clients.

Another option is to make read responses include the range for which they are authoritative, and then only consider the intersection of all responses' authoritative ranges when reconciling results. The authoritative range of the response would have to be communicated back to the client, such that it can continue iterating from the appropriate column name even without actually receiving a column for that name.

Other than failing requests with a new kind of error, I don't see a good way to fix the tombstone case (the over-shoot case can be fixed by just capping results) without changing the protocol. For obvious reason's we don't want the co-originating node to go into a potentially unbounded re-try spin until sufficient results are obtained from all nodes participating.

FWIW, returning iteration state feels pretty clean to me (it's what our layer implements on top to enable easier iteration). It is also compatible with future improvements to e.g. cap the size of a response by bytes for safely slicing over columns whose size you do not know. By removing the strict requirement to deliver exactly the number of asked-for columns else it be interpreted as ""out of columns"", significant flexibility is gained. So if the only option for a clean fix is truly to change the protocol, at least other positive benefits may be had.;;;","13/May/11 08:13;scode;I realized I failed to make the possibly most important point: That you can indeed get short reads such that iteration will stop early. Consider 3 nodes, RF=3, QUORUM.

* Node A has [10,40] of columns
* Node B has [10,40] of columns
* Node C has [10,20] of column deletions for the columns that A/B has,
but does NOT have any for [21,40] because it was down when those were written

Now a client slices [10,1000] with count=11. The co-ordinating node will reconcile that; C's tombstones will override A/B (I'm assuming tombstones are later than A+B's columns), but since C is lacking the ""remainder"" of columns you don't just get some columns at lowered consistency level - you actually get a ""short"" result, and the application or high-level client will believe that the iteration is complete.

This was the primary reason why I said in the OP that I believed ""iterating over columns is impossible to do reliably with QUORUM"". I somehow lost this when re-phrasing the JIRA post a couple of times.

Note: The short read case is not something I have tested and triggered, so is based on extrapolation from my understanding of the code.

;;;","13/May/11 15:01;slebresne;You are right, there is a problem here.

I'll just note that you example is not a good example for QUORUM, because the fact that only C ""has [10,20] of column deletions"" means this situation did not happen with QUORUM writes (and the consistency guarantee for QUORUM involves read and write being at QUORUM).

However, this still show that there is a problem for ONE (writes) + ALL (reads). And it's not hard to come up with an example for QUORUM (reads and writes). Just consider the case where you insert like 10 columns and then delete the 3 first ones but with each time 1 node down, but never the same one.

To make this concrete, I'm attaching a script that produce this ""short read"" effect. Disclaimer: it uses https://github.com/pcmanus/ccm and require the patch I've attached to CASSANDRA-2646 (to be able to do a bounded slice with the cli).

The simplest way to fix that I see (which doesn't imply simple per se) would be to requests more columns if we're short after a resolve on the coordinator.  Yes in theory it means we may have to do a unknown number of such re-request, but in practice I strongly doubt this will be a problem. The problem has very little chance to happen in real life to start with (for QUORUM, my script is simple but implements something that has very very little change to actually happen in real life -- especially with HH, read repair and repair), but the chances that if that happens we need more that 1 re-request are ridiculously small.
;;;","17/May/11 20:34;scode;You're right of course - my example was bogus. I'll also agree about re-try being reasonable under the circumstances, though perhaps not optimal.

With regards to the fix. Let me just make sure I understand you correctly. So given a read command with a limit N that yields <N columns (post-reconciliation), we may need to re-request from one or more nodes. But how do we distinguish between a legitimate short read and a spurious short read? The criteria seems to me to be, that a read is potentially spuriously short if ""one or more of the nodes involved returned a NON-short read"". If all of them returned short reads, it's fine; only if we have results from a node that we cannot prove did indeed exhaust its list of available columns do we need to check.

That is my understanding of your proposed solution, and that does seem doable on the co-ordinator side without protocol changes since we obviously know what we actually got from each node; it's just a matter of coding acrobatics (not sure how much work).

However, would you agree with this claim: This would fix the spurious short read problem specifically, but does not address the more general problem of consistency - i.e., one might receive columns that have not gone through reconciliation by QUORUM?

If we are to solve that, while still not implying protocol changes, I believe we need to do re-tries whenever a more general condition is true: That we do not have confirmed QUORUM for the full range implied by the start+limit range that we are being asked for. In other words, if one or more of the nodes participating in the read returned a response that satisfies:

  (1) The response was *not* short.
    AND
  (2) The response ""last"" column was < than the ""last"" column that we are to return post-reconciliation.

Lacking a protocol change to communicate authoritative ranges of responses, and given that the premise is that we *must* deliver start+limit unless there are < limit number of columns available, we necessarily can only consider the full range (first-to-last column) of a response as authoritative (except in the case of a short read, in which case it's authoritative to infinity).

Without revisiting the code to try to figure out what the easiest way to implement it is, one thought is that if you agree that a clean long-term fix would be to communicate authoritativeness in responses, perhaps one can at least make the logic to handle this compatible with that way of thinking. It's just that until protocol changes can happen, we'd (1) infer authoritativeness from columns/tombstones in the result instead of from explicit indicators in a response, and (2) since we cannot propagate short ranges to clients, we must re-request instead of cleanly return a short-but-not-eof-indicating range to the client.

Thoughts?;;;","28/Jul/11 03:58;byronclark;[^short_read_0.8.sh] is an update to [^short_read.sh] that works with 0.8.x.;;;","29/Jul/11 02:18;byronclark;I'm having a really hard time reproducing this issue consistently on trunk. That's not to say that it doesn't happen, but I'm only seeing it one time out of fifteen using short_read.sh.

I'm running this on Linux and, from the logs, it looks like the downed node isn't being detected as down, even when I sleep 10 seconds before doing the set.

Any hints on getting this failure to happen more reliably?;;;","29/Jul/11 03:39;byronclark;[^reliable_short_read_0.8.sh] reproduces the issue for me every time.  This script requires the following commit to ccm: https://github.com/byronclark/ccm/commit/974a5773228e783d4a91d7ba46d744e5a1216377;;;","29/Jul/11 06:25;byronclark;The attached [^CASSANDRA-2643-poc.patch], while extremely ugly, serves as a proof of concept that all the data is available and the short read problem can be corrected.;;;","30/Jul/11 02:45;byronclark;[^CASSANDRA-2643-v2.patch] makes the following improvements on [^CASSANDRA-2643-poc.patch]:
* Cleans up duplicated code for counting live columns.
* Removes the need to store the ReadCommand in the RepairCallback.

Remaining issue to be dealt with:
* Storing maxLiveColumns in the resolver feels wrong, but that's where the data is generated.  We need to know how many live rows the biggest return had to detect if this is actually a short read. I'm open to suggestions a better place for that count to live.
;;;","01/Aug/11 22:52;jbellis;Looks good on the whole.  One point to clear up:

{code}
if ((maxLiveColumns >= sliceCommand.count) && (liveColumnsInRow < sliceCommand.count))
{code}

maxLiveColumns is the max from a single response, so how can it be greater than sliceCommand.count?  Would this be a valid reformulation?

{code}
assert maxLiveColumns <= sliceCommand.count;
if ((maxLiveColumns == sliceCommand.count) && (liveColumnsInRow < sliceCommand.count))
{code}

Minor things I'd like to clean up:

- is maxLiveColumns valid on any AbstractRR subclass other than RRR? If not I'd rather move it in there and throw an UnsupportedOperation in ARR.
- Would prefer initializing commandsToRetry to Collections.emptyList, to avoid allocating that list in the common case that no retries are needed.  (Then clear of course needs to become allocate.)
;;;","02/Aug/11 04:11;byronclark;[^CASSANDRA-2643-v3.patch] incorporates Jonathan's suggestions.;;;","02/Aug/11 13:27;jbellis;added a skeleton getMaxLiveColumns to RangeSliceResponseResolver (and created CASSANDRA-2986 to follow up on that) and committed.

thanks!;;;","02/Aug/11 14:22;hudson;Integrated in Cassandra #992 (See [https://builds.apache.org/job/Cassandra/992/])
    fix ""short reads"" in [multi]get
patch by Byron Clark; reviewed by jbellis for CASSANDRA-2643

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1153115
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/service/IResponseResolver.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/RepairCallback.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/AbstractRowResolver.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/RowRepairResolver.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/AbstractColumnContainer.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/RangeSliceResponseResolver.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CounterColumn Increments lost after restart,CASSANDRA-2642,12506986,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,uctopcu,uctopcu,12/May/11 13:47,16/Apr/19 09:32,14/Jul/23 05:52,12/May/11 15:22,0.8.0,,,,,,0,,,,"While testing the 0.8.0-rc1; 

I've come across this problem. In order to reproduce please follow the steps:

- create a ColumnFamily named Counters
- do a few increments on a column
- get column value
- kill cassandra
- start cassandra
- get the column value

please see the cli-history.txt or pastebin http://pastebin.com/9jYdDiRY",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/11 14:07;slebresne;0001-Don-t-consider-CL-mutation-from-remote-host.patch;https://issues.apache.org/jira/secure/attachment/12478966/0001-Don-t-consider-CL-mutation-from-remote-host.patch","12/May/11 13:48;uctopcu;cli-history.txt;https://issues.apache.org/jira/secure/attachment/12478963/cli-history.txt",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20747,,,Fri May 13 00:52:38 UTC 2011,,,,,,,,,,"0|i0gcl3:",93469,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"12/May/11 14:07;slebresne;Patch attach with a unit test. We were considering the mutation from the CL as coming from remote host and thus not considering them as new increment (and thus we were keeping the max instead of summing);;;","12/May/11 14:45;jbellis;comment needs update:
{noformat}
                 // This is coming from a remote host
{noformat}

otherwise, +1;;;","12/May/11 15:22;slebresne;Committed with comment removed;;;","13/May/11 00:52;hudson;Integrated in Cassandra-0.8 #103 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/103/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrations announce on startup attempts to set local gossip state before gossiper is started.,CASSANDRA-2638,12506881,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,11/May/11 17:37,16/Apr/19 09:32,14/Jul/23 05:52,11/May/11 20:08,0.7.6,0.8.0,,,,,0,,,,"AbstractCassandraDemon calls MigrationManager.applyMigrations() before the gossiper is initialized (via SS.initServer()).

MM.applyMigrations tries to set the local gossip state before it is initialized via G.start().",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/May/11 18:42;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-init-ep-state-when-prior-to-gossiper-start.txt;https://issues.apache.org/jira/secure/attachment/12478857/ASF.LICENSE.NOT.GRANTED--v1-0001-init-ep-state-when-prior-to-gossiper-start.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20745,,,Fri May 13 00:52:38 UTC 2011,,,,,,,,,,"0|i0gck7:",93465,,,,,Normal,,,,,,,,,,,,,,,,,"11/May/11 17:57;gdusbabek;java.lang.NullPointerException
	at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:883)
	at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:125)
	at org.apache.cassandra.service.MigrationManager.applyMigrations(MigrationManager.java:183)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:182)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:313)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80);;;","11/May/11 18:45;gdusbabek;patch is for trunk, but the same thing basically needs to be done for 0.7.;;;","11/May/11 19:05;brandon.williams;+1;;;","11/May/11 19:44;hudson;Integrated in Cassandra-0.7 #480 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/480/])
    initialize local ep state prior to gossip startup if needed. patch by gdusbabek, reviewed by brandonwilliams. CASSANDRA-2638
;;;","13/May/11 00:52;hudson;Integrated in Cassandra-0.8 #103 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/103/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bloom filter true positives not counted unless key cache is enabled,CASSANDRA-2637,12506865,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,11/May/11 15:42,16/Apr/19 09:32,14/Jul/23 05:52,11/May/11 16:07,0.7.6,0.8.1,,,,,0,,,,,,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/May/11 15:43;jbellis;2637.txt;https://issues.apache.org/jira/secure/attachment/12478833/2637.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20744,,,Wed May 11 19:44:04 UTC 2011,,,,,,,,,,"0|i0gcjz:",93464,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"11/May/11 15:49;slebresne;+1;;;","11/May/11 19:44;hudson;Integrated in Cassandra-0.7 #480 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/480/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Keys get lost in bootstrap,CASSANDRA-2633,12506829,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,richardlow,richardlow,richardlow,11/May/11 10:36,16/Apr/19 09:32,14/Jul/23 05:52,11/May/11 20:55,0.7.6,0.8.1,,,,,0,,,,"When bootstrapping a new node, the key at the upper end of the new node's range can get lost.  To reproduce:

* Set up one cassandra node, create a keyspace and column family and perform some inserts
* Read every row back
* Bootstrap a second node
* Read every row back

You find one row is missing, whose row key is exactly equal to the token the new node gets (for OPP - for RP it's the key whose hash is equal to the token).  If you don't do the reads after the inserts, the key is not lost.  I tracked the problem down to o.a.c.io.sstable.SSTableReader in getPosition.  The problem is that the cached position is used if it is there (so only if the reads were performed).  But this is incorrect because the cached position is the start of the row, not the end.  This means the end row itself is not transferred.  This causes the last key in the range to get lost.

Although I haven't seen it, this may occur during antientropy repairs too.

The attached patch (against the 0.7 branch) fixes it by not using the cache for Operator.GT.  I haven't tested with 0.8 but from looking at the code I think the problem is present.

This might be related to CASSANDRA-1992",,,,,,,,,,,,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,"11/May/11 10:38;richardlow;0.7-2633.txt;https://issues.apache.org/jira/secure/attachment/12478797/0.7-2633.txt","11/May/11 15:32;slebresne;0001-2633-unit-test.patch;https://issues.apache.org/jira/secure/attachment/12478832/0001-2633-unit-test.patch",,,,,,,,,,,,,2.0,richardlow,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20743,,,Wed May 11 19:44:05 UTC 2011,,,,,,,,,,"0|i0gcj3:",93460,,slebresne,,slebresne,Critical,,,,,,,,,,,,,,,,,"11/May/11 14:43;jbellis;So say we have a node A with rows A B C D on it.

We bootstrap a node C.

C requests (A, C] from A.

A will do a GT scan starting with A.  So a cache hit will result in [A, C] being transferred instead. That is a bug, I'll see if I can create a unit test that demonstrates that separately.

But I don't see how this affects the C row?
;;;","11/May/11 15:07;richardlow;A cache hit results in [A, C) being returned.  All GT scans with cache hits give positions at the start of the row rather than the end.  The above patch fixes both ends - skip over A, but include C.;;;","11/May/11 15:22;jbellis;getPosition only affects start of scan, not end.;;;","11/May/11 15:32;slebresne;Good catch. Attaching a unit test to catch the bug.;;;","11/May/11 15:36;richardlow;It looks to me that for client reads, getPosition is just used for the start of an iterator, as you say.  But for streaming, getPosition is used for the end position too in SSTableReader.getPositionsForRanges.  Or have I misunderstood what's going on?;;;","11/May/11 15:39;slebresne;bq. But I don't see how this affects the C row?

This affects the C row because it will use the position of C found as the position where to stop scanning. But the position of C is the start of C, so when used as an end position, it excludes it. That is, getPositionForRanges will return (start of A, start of C), which results in scanning [A, C) as Richard says.

So +1 on this.;;;","11/May/11 16:03;jbellis;got it.  committed, thanks!;;;","11/May/11 19:44;hudson;Integrated in Cassandra-0.7 #480 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/480/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigurationException when starting a node after deleting LocationInfo SStables,CASSANDRA-2632,12506792,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,11/May/11 01:19,16/Apr/19 09:32,14/Jul/23 05:52,11/May/11 09:10,0.7.6,,,,,,0,,,,"from http://www.mail-archive.com/user@cassandra.apache.org/msg13170.html

SystemTable.checkHealth() assumes that if the LOCATION_KEY row is not in the STATUS system CF their should be no other files in the system data directory. If it's safe to delete the LocationInfo sstables this stops the server restarting.

I think the intention of the check is to assert that the reason the row was not found is that there is no data in the STATUS CF. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/May/11 01:47;amorton;0001-only-check-for-existing-LocationInfo-SSTables-during.patch;https://issues.apache.org/jira/secure/attachment/12478755/0001-only-check-for-existing-LocationInfo-SSTables-during.patch",,,,,,,,,,,,,,1.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20742,,,Wed May 11 09:38:52 UTC 2011,,,,,,,,,,"0|i0gciv:",93459,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"11/May/11 01:47;amorton;Attached patch checks the SSTable count for the STATUS CF to determine if the reason the location info was not found was because the partitioner changed. ;;;","11/May/11 09:10;slebresne;+1, committed. Thanks.;;;","11/May/11 09:38;hudson;Integrated in Cassandra-0.7 #477 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/477/])
    Allow removing LocationInfo sstables (to allow cluster rename)
patch by amorton; reviewed by slebresne for CASSANDRA-2632
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replaying a commitlog entry from a dropped keyspace will cause an error,CASSANDRA-2631,12506778,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,10/May/11 23:35,16/Apr/19 09:32,14/Jul/23 05:52,11/May/11 14:23,0.7.6,0.8.1,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/May/11 23:55;jbellis;2631-0.7.txt;https://issues.apache.org/jira/secure/attachment/12478748/2631-0.7.txt","10/May/11 23:55;jbellis;2631-0.8.txt;https://issues.apache.org/jira/secure/attachment/12478747/2631-0.8.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20741,,,Wed May 11 14:34:25 UTC 2011,,,,,,,,,,"0|i0gcin:",93458,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"10/May/11 23:55;jbellis;also not sure why we were copying the mutations into a new arraylist;;;","11/May/11 08:46;slebresne;+1;;;","11/May/11 14:34;hudson;Integrated in Cassandra-0.7 #478 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/478/])
    avoid attempting to replay mutationsfrom dropped keyspaces
patch by jbellis; reviewed by slebresne for CASSANDRA-2631
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Empty Result with Secondary Index Queries with ""limit 1""",CASSANDRA-2628,12506607,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,muga_nishizawa,muga_nishizawa,09/May/11 13:29,16/Apr/19 09:32,14/Jul/23 05:52,12/May/11 15:14,0.7.6,0.8.0,,Feature/2i Index,,,0,,,,"Empty result is returned by secondary index queries with ""limit 1"".  Cassandra returns correct result for other numbers than ""1"" (e.g. limit 2, limit 3, etc.).  

You can reproduce the problem with programs attached on CASSANDRA-2406.  

- 1. Start Cassandra cluster. It consists of 3 cassandra nodes and distributes data by ByteOrderedPartitioner. Initial tokens of those nodes are [""31"", ""32"", ""33""].
- 2. Create keyspace and column family, according to ""create_table.cli"",
- 3. Execute ""secondary_index_insertv2.py"", inserting a few hundred columns to cluster
- 4. Here, when you first use cassandra-cli and execute following lines, you can get correct result.  

{quote}
% bin/cassandra-cli
[default@unknown] connect localhost/9160;
[default@unknown] use SampleKS;
[default@SampleKS] get SampleCF where up = 'up' limit 3;               
-------------------
RowKey: 150
=> (column=date, value=150, timestamp=1304937931)
=> (column=up, value=up, timestamp=1304937931)
-------------------
RowKey: 151
=> (column=date, value=151, timestamp=1304937932)
=> (column=up, value=up, timestamp=1304937932)
-------------------
RowKey: 152
=> (column=date, value=152, timestamp=1304937932)
=> (column=up, value=up, timestamp=1304937932)
3 Rows Returned.  
{quote}

On the other hand, if you set limit to ""1"", you can reproduce the problem.

{quote}
[default@SampleKS] get SampleCF where up = 'up' and date > 150 limit 1;
0 Row Returned.
{quote}

There are two factors to cause this problem:
- 1. scanned first column doesn't match at specified clause like ""date > 150"".
- 2. ""limit 1""

Only one factor doesn't cause problem.  For example, I can get correct data when I specify as following:

- ""limit 1"" -> ""limit 2""
{quote}
[default@SampleKS] get SampleCF where up = 'up' and date > 150 limit 2;
-------------------
RowKey: 151
=> (column=date, value=151, timestamp=1304937932)
=> (column=up, value=up, timestamp=1304937932)
-------------------
RowKey: 152
=> (column=date, value=152, timestamp=1304937932)
=> (column=up, value=up, timestamp=1304937932)
2 Rows Returned.
{quote}

- ""date > 150"" -> ""date >= 150""
{quote}
[default@SampleKS] get SampleCF where up = 'up' and date >= 150 limit 1;
-------------------
RowKey: 150
=> (column=date, value=150, timestamp=1304937931)
=> (column=up, value=up, timestamp=1304937931)
1 Row Returned.
{quote}",CentOS 5.5,skamio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/11 11:07;slebresne;0001-2628.patch;https://issues.apache.org/jira/secure/attachment/12478955/0001-2628.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20740,,,Thu May 12 20:56:28 UTC 2011,,,,,,,,,,"0|i0gci7:",93456,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"12/May/11 11:07;slebresne;Attaching fix along with a unit test.

Problem was we do some paging on the index using clause.count as the page size. And 1 is a pretty bad page size.

Thanks Muga for the report and the clear instructions to reproduce.;;;","12/May/11 14:55;jbellis;To clarify, the problem is that since slice is inclusive-from-start, if our page size is one a 2nd pass will get back the same row the first did, so the scan logic thinks there is no more data and breaks. (since that is exactly what you will see if you just scanned the last column in the index row.)

+1 on the fix.;;;","12/May/11 15:14;slebresne;Committed;;;","12/May/11 20:56;hudson;Integrated in Cassandra-0.7 #483 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/483/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Don't allow {LOCAL|EACH}_QUORUM unless strategy is NTS",CASSANDRA-2627,12506574,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,09/May/11 08:48,16/Apr/19 09:32,14/Jul/23 05:52,09/May/11 15:13,0.7.6,0.8.0,,Legacy/CQL,,,0,,,,"There is not check when {LOCAL|EACH}_QUORUM is used than we do use NTS, hence using such CL with simpleStrategy for instance result in
{noformat}
ERROR [pool-1-thread-1] 2011-05-09 10:44:29,728 Cassandra.java (line 2960) Internal error processing insert
java.lang.ClassCastException: org.apache.cassandra.locator.SimpleStrategy cannot be cast to org.apache.cassandra.locator.NetworkTopologyStrategy
...
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/11 10:26;slebresne;0001-validate-CL-compatible-with-strategy.patch;https://issues.apache.org/jira/secure/attachment/12478582/0001-validate-CL-compatible-with-strategy.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20739,,,Mon May 09 15:13:13 UTC 2011,,,,,,,,,,"0|i0gchz:",93455,,,,,Low,,,,,,,,,,,,,,,,,"09/May/11 10:26;slebresne;Patch against 0.7;;;","09/May/11 13:21;jbellis;+1 (it's possible to make these work w/ non-NTS but it's more work than it's worth);;;","09/May/11 13:44;hudson;Integrated in Cassandra-0.7 #475 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/475/])
    Don't allow {LOCAL|EACH}_QUORUM unless strategy is NTS
patch by slebresne; reviewed by jbellis for CASSANDRA-2627
;;;","09/May/11 15:13;slebresne;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stack overflow while compacting,CASSANDRA-2626,12506563,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,skamio,terjem,terjem,09/May/11 05:04,16/Apr/19 09:32,14/Jul/23 05:52,26/May/11 18:28,0.8.0,,,,,,0,,,,"This is a trunk build from May 3.

After adding  CASSANDRA-2401, I have gotten the following on several nodes.
I am not 100% sure right now if it is related to 2401 but it may seem likely.

Unfortunately, as often is the case with stack overflows, I don't see the start of the stack

ERROR [CompactionExecutor:17] 2011-05-09 07:56:32,479 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:17,1,main]
java.lang.StackOverflowError
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
",,brandon.williams,cburroughs,cm,skamio,terjem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/May/11 14:03;jbellis;2626-v3.txt;https://issues.apache.org/jira/secure/attachment/12480545/2626-v3.txt","25/May/11 17:50;jbellis;2626.txt;https://issues.apache.org/jira/secure/attachment/12480437/2626.txt","26/May/11 08:24;skamio;CASSANDRA-2626.patch;https://issues.apache.org/jira/secure/attachment/12480519/CASSANDRA-2626.patch",,,,,,,,,,,,3.0,skamio,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20738,,,Thu May 26 18:28:20 UTC 2011,,,,,,,,,,"0|i0gchr:",93454,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"09/May/11 05:22;jbellis;The only place we construct a UnmodifableCollection explicitly is Table.getColumnFamilyStores, which clearly doesn't nest enough UCs to blow a stack.  Something else must be doing that under the hood...

Doesn't look 2401-related at all.;;;","25/May/11 13:28;skamio;I was able to reproduce the stack overflow problem on cassandra 0.8.0 trunk.
The DataTracker creates unmodifiableSet of 'Set<SSTableReader> compacting' on switching memtable and other operations. It creates unmodifiableSet one level deeper every time memtables is switched. When the number of switching memtable reaches some level and its size() is called, the stack overflow exception occurs if the call stack exceeds stack limit.

Stack trace below (dumped by simple wrapper class MyUnmodifiableSet) shows size() is called by constructor of HashSet in View.markCompacting() (DataTracker.java).
If the nesting is unavoidable, a solution is to use UnmodifiableSet in apache commons collections library. It doesn't create nests of unmodifiable collection.


Steps to reproduce the stack overflow:
1. Create single cassandra node with standard column families.
2. Disable compaction and set MemtableThroughputInMB and MemtableOperationsInMillions to small value in order to flush memtables frequently.
3. Insert many data for many keys.
4. When number of sstables exceeds 2000 (this parameter may vary in environments), run ""nodetool compact"". The error will be logged.


-------
* Change for debug in DataTracker.java:
{quote}
        public View(Memtable memtable, Set<Memtable> pendingFlush, Set<SSTableReader> sstables, Set<SSTableReader> compacting)
        \{
            this.memtable = memtable;
  //             this.memtablesPendingFlush = Collections.unmodifiableSet(pendingFlush);
  //             this.sstables = Collections.unmodifiableSet(sstables);
  //             this.compacting = Collections.unmodifiableSet(compacting);

            this.memtablesPendingFlush = new MyUnmodifiableSet(pendingFlush);
            this.sstables              = new MyUnmodifiableSet(sstables);
            this.compacting            = new MyUnmodifiableSet(compacting);
        \}
{quote}
-------
* Stacktrace by Thread.dumpStack() in MyUnmodifiableSet.

        at java.lang.Thread.dumpStack(Thread.java:1249)
        at org.apache.cassandra.db.MyUnmodifiableSet.size(MyUnmodifiableSet.java:35)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at org.apache.cassandra.db.MyUnmodifiableSet.size(MyUnmodifiableSet.java:36)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at org.apache.cassandra.db.MyUnmodifiableSet.size(MyUnmodifiableSet.java:36)
   ......
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at org.apache.cassandra.db.MyUnmodifiableSet.size(MyUnmodifiableSet.java:36)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at org.apache.cassandra.db.MyUnmodifiableSet.size(MyUnmodifiableSet.java:36)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at org.apache.cassandra.db.MyUnmodifiableSet.size(MyUnmodifiableSet.java:36)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at org.apache.cassandra.db.MyUnmodifiableSet.size(MyUnmodifiableSet.java:36)
        at java.util.HashSet.<init>(HashSet.java:99)
        at org.apache.cassandra.db.DataTracker$View.markCompacting(DataTracker.java:495)
        at org.apache.cassandra.db.DataTracker.markCompacting(DataTracker.java:188)
        at org.apache.cassandra.db.CompactionManager$4.call(CompactionManager.java:312)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

------;;;","25/May/11 17:50;jbellis;Ah, you've nailed it. Thanks for tracking that down.

Patch attached to avoid re-wrapping an already unmodifiable collection. (Surprised Collections.unmodifiable... doesn't do this.)

We may also want to look at using ImmutableSet (http://guava-libraries.googlecode.com/svn/trunk/javadoc/com/google/common/collect/ImmutableSet.html) instead of HashSet wrapped in UnmodifiableSet. (ImmutableSet claims to ""perform significantly better than HashSet for objects with very fast Object.hashCode() implementations."");;;","25/May/11 18:01;jbellis;(Did a quick check on our other uses of Collections.unmodifiable*, didn't see any red flags.);;;","25/May/11 18:26;mdennis;+1;;;","25/May/11 19:09;jbellis;committed;;;","26/May/11 01:24;muga_nishizawa;Thanks for your quick response.  

I'm not sure but I think that the fix doesn't work.  Types of pendingFlush, sstables and compacting objects are java.util.Collections.UnmodifiableSet.  Those are not instances of org.apache.commons.collections.set.UnmodifiableSet.  ;;;","26/May/11 02:07;brandon.williams;Confirmed, stack gets blown with >2k sstables still.;;;","26/May/11 08:24;skamio;This patch prevents stack overflow. Apache commons UnmodifiableSet is used instead of java.util.Collections.UnmodifiableSet because the latter is not accessible due to package private.
;;;","26/May/11 13:16;jbellis;Oh no! Auto-import for the lose :(;;;","26/May/11 14:03;jbellis;v3 takes a slightly different approach -- instead of accepting any Set in the constructor and decorating it w/ Unmodifiable, v3 restricts constructor to private access and makes the factory methods responsible for ensuring the set is unmodifiable, by using ImmutableSet as mentioned above.;;;","26/May/11 18:06;brandon.williams;+1, I like this approach better too.;;;","26/May/11 18:28;jbellis;committed v3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
auto bootstrapping a node into a cluster without a schema silently fails,CASSANDRA-2625,12506550,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,amorton,amorton,08/May/11 22:38,16/Apr/19 09:32,14/Jul/23 05:52,11/May/11 08:08,0.7.6,,,,,,0,,,,"from http://www.mail-archive.com/user@cassandra.apache.org/msg13001.html

StorageService.joinRing() aborts the auto bootstrap process if the cluster does not have a schema defined. It looks like the node is left in the ""Joining"" mode and there is no logging. 

There could be a schema defined and no data loaded, so just having a schema does not make the token selection any better. And BootStrapper.bootstrap() handles their been no non system tables.

Can we let the bootstrap process continue ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/May/11 19:32;slebresne;0001-2625.patch;https://issues.apache.org/jira/secure/attachment/12478723/0001-2625.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20737,,,Wed May 11 08:08:49 UTC 2011,,,,,,,,,,"0|i0gchj:",93453,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"10/May/11 19:32;slebresne;This is a regression from CASSANDRA-2435. Attaching fix.;;;","10/May/11 19:46;jbellis;+1;;;","11/May/11 07:35;hudson;Integrated in Cassandra-0.7 #476 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/476/])
    Fix regression where boostrapping a node without schema defined fails
patch by slebresne; reviewed by jbellis for CASSANDRA-2625
;;;","11/May/11 08:08;slebresne;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleStrategy w/o replication_factor,CASSANDRA-2624,12506525,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,urandom,urandom,08/May/11 02:10,16/Apr/19 09:32,14/Jul/23 05:52,10/May/11 18:57,0.8.0,,,Legacy/CQL,Legacy/Tools,,0,,,,"It is possible to create a new keyspace using {{SimpleStrategy}} _without_ specifying the {{replication_factor}} option.  Things get more interesting if you shut the node down, since it will refuse to restart (throwing a {{ConfigurationException}}).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/May/11 16:54;jbellis;2624-v2.txt;https://issues.apache.org/jira/secure/attachment/12478709/2624-v2.txt","10/May/11 15:28;jbellis;2624.txt;https://issues.apache.org/jira/secure/attachment/12478704/2624.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20736,,,Tue May 10 22:30:29 UTC 2011,,,,,,,,,,"0|i0gchb:",93452,,xedin,,xedin,Normal,,,,,,,,,,,,,,,,,"10/May/11 15:28;jbellis;patch to have CQL call validateKsDef.  Also extracts code from CassandraServer into KSM.fromThrift for re-use in QP.;;;","10/May/11 16:54;jbellis;rebased on top of r1101542;;;","10/May/11 18:40;xedin;+1;;;","10/May/11 18:57;jbellis;committed;;;","10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI escaped single quote parsing gives errors,CASSANDRA-2623,12506513,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,rday,rday,07/May/11 21:21,16/Apr/19 09:32,14/Jul/23 05:52,12/May/11 15:31,0.7.6,0.8.0,,Legacy/Tools,,,0,cli,,,"Escaping quotes in cli commands causes parsing errors.


some examples::::
No need to create columns etc, it doesn't get through parsing the expression::

cassandra-cli

1. 
set column['KEY+vals'][VALUE] = 'VAL\'' ;
Syntax error at position 41: mismatched character '<EOF>' expecting '''

2.
set column['KEY+val\'s'][VALUE] = 'VAL' ;
Syntax error at position 41: mismatched character '<EOF>' expecting '''

3.
set column['KEY+vals\''][VALUE] = 'VAL\'' ;
Syntax error at position 38: unexpected ""\"" for `set column['KEY+vals\''][VALUE] = 'VAL\'' ;`.
","windows vista, linux",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/May/11 12:24;xedin;CASSANDRA-2623-0.7.patch;https://issues.apache.org/jira/secure/attachment/12478692/CASSANDRA-2623-0.7.patch","10/May/11 12:24;xedin;CASSANDRA-2623-trunk.patch;https://issues.apache.org/jira/secure/attachment/12478693/CASSANDRA-2623-trunk.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20735,,,Thu May 12 20:56:28 UTC 2011,,,,,,,,,,"0|i0gch3:",93451,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"09/May/11 05:19;rday;In the Cli.g file the string literal does not allow for escaped single quote.

http://svn.apache.org/repos/asf/cassandra/trunk/src/java/org/apache/cassandra/cli/Cli.g


StringLiteral
    :
    '\'' (~'\'')* '\'' ( '\'' (~'\'')* '\'' )*
    ;

I'm not sure yet what is the purpose of the repeating last half of the expression.
First Attempt to allow escaped characters seemed to cause other problems, still investigating that.


;;;","09/May/11 05:24;jbellis;single quotes are escaped by doubling them, as in SQL:

{noformat}
'aren''t you glad you can escape quotes this way'
{noformat}
;;;","09/May/11 14:27;rday;Unforuntately using 2 single quotes causes insertions of both quotes in to the data set,

create keyspace KEYS with
   placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and
   replication_factor = 1;
use KEYS;

create column family CF with
  comparator=UTF8Type and
  rows_cached=1.0 and
  memtable_throughput=128 and
  min_compaction_threshold=10 and
  column_metadata =
  [
    {column_name:VERSION, validation_class:LongType},
    {column_name:VALUE, validation_class:UTF8Type}
  ];
  
  
  set CF['key''1'][VERSION] = 'aren''t happy' ; 
  set CF['key''1'][VALUE] = 'aren''t happy' ; 

  list CF

RowKey: key''1
=> (column=VALUE, value=aren''t happy, timestamp=1304952103638000)
=> (column=VERSION, value=1, timestamp=1304952057641000)


Through a thrift java client insert we are able to insert and find single quote keys,
But if data is inserted via CLI,  the java thrift single quote lookups do not match.;;;","09/May/11 14:34;jbellis;You're right. That's a bug.;;;","09/May/11 15:47;rday;Possibly the unescapeSQLString in CliUtils.java
can do something to get rid of the single quotes.
The unit test doesn't cover escaping via 2 single quotes.

    public static String unescapeSQLString(String b)
    {
        if (b.charAt(0) == '\'' && b.charAt(b.length()-1) == '\'')
            b = b.substring(1, b.length()-1);
        return StringEscapeUtils.unescapeJava(b);
    }

also method escapeSQLString which 
(only used in the unit test but public ) 
is assuming a backslash to escape single quotes.

    public static String escapeSQLString(String b)
    {
        // single quotes are not escaped in java, need to be for cli
        return StringEscapeUtils.escapeJava(b).replace(""\'"", ""\\'"");
    }


The StringLiteral Definition makes sense for using 2 single quotes escaping since it should always have an even number of quotes so just the backing code needs to be enlightened.
;;;","09/May/11 16:44;rday;Just replacing the 2 quotes with one seems to work for the simple case in 7.5.

{code:title=CliUtils.java|borderStyle=solid}
    public static String unescapeSQLString(String b) 
    { 
        if (b.charAt(0) == '\'' && b.charAt(b.length()-1) == '\'') 
            b = b.substring(1, b.length()-1); 
        b = b.replaceAll(""''"",""'""); 
        return StringEscapeUtils.unescapeJava(b); 
    } 
{code};;;","10/May/11 12:24;xedin;fix for CLI grammar to support escaped quotes + tests.;;;","10/May/11 14:18;jbellis;you really like the backslash approach better than the SQL quote-doubling one?;;;","10/May/11 14:41;xedin;Yes, I personally don't like SQL way of dealing with string interpolation.;;;","12/May/11 15:31;jbellis;committed;;;","12/May/11 20:56;hudson;Integrated in Cassandra-0.7 #483 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/483/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Select * doesn't include row key,CASSANDRA-2622,12506497,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,07/May/11 13:08,16/Apr/19 09:32,14/Jul/23 05:52,09/May/11 19:55,0.8.0,,,Legacy/CQL,,,0,,,,,,thobbs,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/11 05:11;jbellis;2622.txt;https://issues.apache.org/jira/secure/attachment/12478568/2622.txt","11/May/11 10:18;xedin;grammar-fix.patch;https://issues.apache.org/jira/secure/attachment/12478796/grammar-fix.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20734,,,Wed May 11 10:18:13 UTC 2011,,,,,,,,,,"0|i0gcgv:",93450,,thobbs,,thobbs,Normal,,,,,,,,,,,,,,,,,"07/May/11 15:15;jbellis;... in fact, we don't support KEY at all in the select list.;;;","07/May/11 15:26;jbellis;- adds validation that key_alias must be ascii
- adds key to wildcard columns
- adds support for key in named columns list

Stuck getting antlr to be happy with using key as a column name though:

{noformat}
SELECT KEY, birthdate FROM IndexedA WHERE birthdate = 100
{noformat}

gives ""no viable alternative at input 'key'"".

I believe the problem is that KEY is a keyword (K_KEY) but not sure how to make that allowed in column list.;;;","07/May/11 15:56;jbellis;... found the ""term returns"" section of the grammar, proceeding w/ testing now.;;;","08/May/11 01:40;urandom;I thought we had agreed not to do this.  Wasn't the plan to introduce virtual columns, which would allow you to include the key (by name) in the projection, and to return it in results like any other column?

CASSANDRA-2480;;;","08/May/11 02:05;jbellis;Right. But that implies that in the meantime * needs to include KEY or we break semantics when we go from no-key to including-key.

Currently, the Python driver cheats and adds the row key at the beginning of each column list whether it was asked for or not (obviously incorrect) and JDBC forces you to unwrap the CassandraResultSet to get at it, which encourages relying on implementation details that should really be private (and isn't very user-friendly either: http://twitter.com/#!/ampedandwired/status/66795646228762624).

It also means the two drivers we're shipping have different semantics which is also broken.

As you can see from the patch (updated version attached) the actual changes to QueryProcessor are small; the work of key-wrangling is done in extractThriftColumns and the rest is just adjustments to pass the metadata object there.

The patch is complete for QP and cql.py; I'm still working on the JDBC side. Note that all the existing JDBC tests continue to pass (since there weren't actually any wildcards tested there).;;;","08/May/11 02:09;jbellis;bq. The patch is complete for QP and cql.py; I'm still working on the JDBC side

In fact drivers/java is enough of a mess right now that I uploaded a new patch without that lest anyone think the current state is a useful indication of the right way to go there. :);;;","09/May/11 05:11;jbellis;complete patch including jdbc.;;;","09/May/11 05:41;thobbs;These semantics make much more sense to me.

I'm +1 on the code changes, as well.;;;","09/May/11 19:55;jbellis;committed;;;","10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ;;;","11/May/11 10:18;xedin;grammar should build without warnings.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
secondary index not dropped until restart,CASSANDRA-2619,12506473,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,cywjackson,cywjackson,07/May/11 00:36,16/Apr/19 09:32,14/Jul/23 05:52,09/May/11 07:09,0.7.6,0.8.0,,Feature/2i Index,,,0,,,,"when dropping the secondary index (via cassandra-cli), the describe keyspace still shows the Built index entry. Only after a restart of the CassandraDaemon then the Built Index entry is gone. This seems indicate a problem with the index not really been dropped completed.

to test, use a single node, create an index, then drop it from the cli (issue an update column family ... with metadata fields but not the index info)

below is the original:

  Column Families:
    ColumnFamily: inode
    ""Stores file meta data""
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 0.0/14400
      Memtable thresholds: 0.103125/22/1440 (millions of ops/MB/minutes)
      GC grace seconds: 60
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      {color:red}Built indexes: [inode.path, inode.sentinel]{color}
      Column Metadata:
        Column Name: path (70617468)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
          {color:red}Index Name: path
          Index Type: KEYS{color}
        Column Name: sentinel (73656e74696e656c)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
          {color:red}Index Name: sentinel
          Index Type: KEYS{color}

issue an update:
{noformat}

[default@unknown] use cfs;
Authenticated to keyspace: cfs
[default@cfs] update column family inode with comparator=BytesType and column_metadata=[{column_name:70617468, validation_class:BytesType}, {column_name:73656e74696e656c,validation_class:BytesType}];
fca46d00-783c-11e0-0000-242d50cf1fff
Waiting for schema agreement...
... schemas agree across the cluster
{noformat}

describe the keyspace again:
Keyspace: cfs:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
    Options: [Brisk:1, Cassandra:0]
  Column Families:
    ColumnFamily: inode
    ""Stores file meta data""
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 0.0/14400
      Memtable thresholds: 0.103125/22/1440 (millions of ops/MB/minutes)
      GC grace seconds: 60
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      {color:red}Built indexes: [inode.path, inode.sentinel]{color}
      Column Metadata:
        Column Name: path (70617468)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
        Column Name: sentinel (73656e74696e656c)
          Validation Class: org.apache.cassandra.db.marshal.BytesType

*notice the red line on Built Indexes*

restart CassandraDaemon, describe again:

Keyspace: cfs:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
    Options: [Brisk:1, Cassandra:0]
  Column Families:
    ColumnFamily: inode
    ""Stores file meta data""
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 0.0/14400
      Memtable thresholds: 0.103125/22/1440 (millions of ops/MB/minutes)
      GC grace seconds: 60
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      {color:red}Built indexes: []{color}
      Column Metadata:
        Column Name: path (70617468)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
        Column Name: sentinel (73656e74696e656c)
          Validation Class: org.apache.cassandra.db.marshal.BytesType


on another note, upon re-create the index, it does not appear the index is actually rebuilt. There is no need to restart CassandraDaemon for the Built Index to show up from the describe. But the update goes very fast. We could tell the index is not being rebuilt because we were getting NPE from:

{noformat}
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:51)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.satisfies(ColumnFamilyStore.java:1647)
	at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1594)
	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:42)
{noformat}
and after re-create the index, the exception resurface (the exception does not surface upon drop).

If we drop the index files and remove them, then re-create the index, the NPE is resolved: 

{noformat}
$ find /var/lib/cassandra/data/cfs -name ""*path*"" -o -name ""*sentinel* -exec rm {} \;""
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/11 04:54;jbellis;2619.txt;https://issues.apache.org/jira/secure/attachment/12478490/2619.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20732,,,Mon May 09 07:09:24 UTC 2011,,,,,,,,,,"0|i0gcg7:",93447,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"07/May/11 04:54;jbellis;Patch attached. There are 3 main things going on here:

- move DefsTest to config package (so it can access CFMetaData.column_metadata) and add a test to expose the bug
- clarify the CFMetaData.apply code to make it clear where it's dealing w/ column addition/removal and not indexes per se (this is where I thought the bug was at first)
- the actual fix, which is the 3 line change to CFS.reload

(Note that as a workaround, you can simply drop the column definition altogether instead of leaving the validator intact while removing the index type.);;;","07/May/11 05:08;jbellis;NOTE: because svn is retarded, you need to help patch figure out how to apply this, by first running

{noformat}
cp test/unit/org/apache/cassandra/db/DefsTest.java test/unit/org/apache/cassandra/config/DefsTest.java
{noformat};;;","07/May/11 06:23;brandon.williams;+1;;;","07/May/11 07:16;hudson;Integrated in Cassandra-0.7 #474 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/474/])
    recognize attempt todrop just the index while leaving the column definition
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-2619
;;;","09/May/11 07:09;jbellis;(committed);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DynamicSnitch race in adding latencies,CASSANDRA-2618,12506465,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,06/May/11 22:27,16/Apr/19 09:32,14/Jul/23 05:52,07/May/11 01:43,0.7.6,,,,,,0,,,,"ERROR 15:33:48,614 Fatal exception in thread Thread[ReadStage:264,5,main]
java.lang.RuntimeException: java.util.NoSuchElementException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.util.NoSuchElementException
	at java.util.concurrent.LinkedBlockingDeque.removeFirst(LinkedBlockingDeque.java:401)
	at java.util.concurrent.LinkedBlockingDeque.remove(LinkedBlockingDeque.java:621)
	at org.apache.cassandra.locator.AdaptiveLatencyTracker.add(DynamicEndpointSnitch.java:288)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.receiveTiming(DynamicEndpointSnitch.java:202)
	at org.apache.cassandra.net.MessagingService.addLatency(MessagingService.java:152)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:642)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
ERROR 15:33:48,615 Fatal exception in thread Thread[ReadStage:264,5,main]
java.lang.RuntimeException: java.util.NoSuchElementException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.util.NoSuchElementException
	at java.util.concurrent.LinkedBlockingDeque.removeFirst(LinkedBlockingDeque.java:401)
	at java.util.concurrent.LinkedBlockingDeque.remove(LinkedBlockingDeque.java:621)
	at org.apache.cassandra.locator.AdaptiveLatencyTracker.add(DynamicEndpointSnitch.java:288)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.receiveTiming(DynamicEndpointSnitch.java:202)
	at org.apache.cassandra.net.MessagingService.addLatency(MessagingService.java:152)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:642)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more

What is happening that AdaptiveLatencyTracker.add is trying to add a latency, but the deque is full, so it makes a second effort to remove an entry from the deque and then try to add again.  However, when it tries to remove, the deque has already been emptied by DES.reset call clear() on all the ALTs.  This bug has existed for a long time, but it's very rare and difficult to trigger.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/May/11 22:37;brandon.williams;2618.txt;https://issues.apache.org/jira/secure/attachment/12478475/2618.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20731,,,Sat May 07 05:20:30 UTC 2011,,,,,,,,,,"0|i0gcfz:",93446,,,,,Low,,,,,,,,,,,,,,,,,"06/May/11 22:37;brandon.williams;Patch to avoid the race between add() and clear().;;;","07/May/11 00:10;tjake;+1;;;","07/May/11 01:43;brandon.williams;Committed.;;;","07/May/11 02:10;stuhood;Are we sure this fixes the race? ArrayDeque isn't a concurrent datastructure, so it's possible that its internals could race in such a way that it gets into a bad state. The size field not matching the index, for example.;;;","07/May/11 02:16;brandon.williams;I can't say for certain since this is terribly difficult to repro (it's been there since DES inception and I've only seen it once) but the patch definitely addressed a likely cause and is certainly needed either way.;;;","07/May/11 02:17;brandon.williams;Also, we're using LinkedBlockingDeque which is a concurrent structure.;;;","07/May/11 05:20;hudson;Integrated in Cassandra-0.7 #473 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/473/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"in cassandra-cli, the help command output on validation types should be updated",CASSANDRA-2615,12506456,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,cywjackson,cywjackson,06/May/11 20:45,16/Apr/19 09:33,14/Jul/23 05:52,12/May/11 22:27,0.8.0,,,,,,0,,,,"from cassandra-cli, say type ""help assume""

you will find:
  Supported values are:
    - AsciiType
    - BytesType
    - CounterColumnType (distributed counter column)
    - IntegerType (a generic variable-length integer type)
    - LexicalUUIDType
    - LongType
    - UTF8Type


ok now:
[default@cfs] assume inode comparator as UTF8Type;   
Type 'UTF8Type' was not found. Available: bytes, integer, long, lexicaluuid, timeuuid, utf8, ascii.


so looks like the ""supported type list should be update by taking away the ""Type"" post-fix..

however, on the other hand, you can't really use it:

[default@cfs] update column family inode;                         
Unable to find abstract-type class 'org.apache.cassandra.db.marshal.utf8'

looks like from the update, you still need the ""Type"" (case insensitive?)",,brandon.williams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/11 22:09;xedin;CASSANDRA-2615.patch;https://issues.apache.org/jira/secure/attachment/12479016/CASSANDRA-2615.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20730,,,Thu May 12 22:27:35 UTC 2011,,,,,,,,,,"0|i0gcfb:",93443,,cywjackson,,cywjackson,Low,,,,,,,,,,,,,,,,,"11/May/11 11:00;xedin;Added CounterColumnType to functions + fixed doc and assume function.;;;","12/May/11 22:21;cywjackson;+1

[default@testks] assume Super4 comparator as ascii
...     ;
Assumption for column family 'Super4' added successfully.
[default@testks] update column family Super4;     
 INFO 15:20:59,545 Applying migration 1d332bd0-7ce6-11e0-0000-fd7033aa10e7 Update column family to org.apache.cassandra.config.CFMetaData@6e72d873[cfId=1000,ksName=testks,cfName=Super4,cfType=Super,comparator=org.apache.cassandra.db.marshal.AsciiType,subcolumncomparator=org.apache.cassandra.db.marshal.BytesType,comment=,rowCacheSize=10000.0,keyCacheSize=200000.0,readRepairChance=1.0,replicateOnWrite=false,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.UTF8Type,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,memtableFlushAfterMins=1440,memtableThroughputInMb=242,memtableOperationsInMillions=1.134375,mergeShardsChance=0.1,keyAlias=java.nio.HeapByteBuffer[pos=475 lim=478 cap=480],column_metadata={}]
 INFO 15:20:59,546 Enqueuing flush of Memtable-Migrations@903913131(7137/8921 serialized/live bytes, 1 ops)
 INFO 15:20:59,547 Writing Memtable-Migrations@903913131(7137/8921 serialized/live bytes, 1 ops)
 INFO 15:20:59,547 Enqueuing flush of Memtable-Schema@1257515479(2960/3700 serialized/live bytes, 3 ops)
 INFO 15:20:59,737 Completed flushing /var/lib/cassandra/data/system/Migrations-g-3-Data.db (7201 bytes)
 INFO 15:20:59,739 Writing Memtable-Schema@1257515479(2960/3700 serialized/live bytes, 3 ops)
 INFO 15:20:59,912 Completed flushing /var/lib/cassandra/data/system/Schema-g-3-Data.db (3110 bytes)
1d332bd0-7ce6-11e0-0000-fd7033aa10e7
Waiting for schema agreement...
... schemas agree across the cluster
;;;","12/May/11 22:27;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL test failures,CASSANDRA-2613,12506419,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,xedin,jbellis,jbellis,06/May/11 14:53,16/Apr/19 09:33,14/Jul/23 05:52,06/May/11 16:31,0.8.0,,,Legacy/CQL,,,0,,,,"{noformat}
FAIL: delete columns from a row
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose-0.11.3-py2.6.egg/nose/case.py"", line 186, in runTest
    self.test(*self.arg)
  File ""/Users/jonathan/projects/cassandra/svn-0.8/test/system/test_cql.py"", line 360, in test_delete_columns
    assert ['kd', None, None] == r, r
AssertionError: [u'kd']

======================================================================
FAIL: delete columns from multiple rows
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose-0.11.3-py2.6.egg/nose/case.py"", line 186, in runTest
    self.test(*self.arg)
  File ""/Users/jonathan/projects/cassandra/svn-0.8/test/system/test_cql.py"", line 379, in test_delete_columns_multi_rows
    assert ['kc', None] == r, r
AssertionError: [u'kc']

======================================================================
FAIL: delete entire rows
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose-0.11.3-py2.6.egg/nose/case.py"", line 186, in runTest
    self.test(*self.arg)
  File ""/Users/jonathan/projects/cassandra/svn-0.8/test/system/test_cql.py"", line 397, in test_delete_rows
    assert ['kd', None, None] == r, r
AssertionError: [u'kd']

======================================================================
FAIL: retrieve multiple columns
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose-0.11.3-py2.6.egg/nose/case.py"", line 186, in runTest
    self.test(*self.arg)
  File ""/Users/jonathan/projects/cassandra/svn-0.8/test/system/test_cql.py"", line 149, in test_select_columns
    assert ['Row Key', 'ca1', 'col', 'cd1'] == [col_dscptn[0] for col_dscptn in d], d
AssertionError: [('Row Key', 'org.apache.cassandra.db.marshal.UTF8Type', None, None, None, None, None, False), ('col', 'org.apache.cassandra.db.marshal.AsciiType', None, None, None, None, True), ('cd1', 'org.apache.cassandra.db.marshal.AsciiType', None, None, None, None, True)]

======================================================================
FAIL: range should not fail when keys were not set
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose-0.11.3-py2.6.egg/nose/case.py"", line 186, in runTest
    self.test(*self.arg)
  File ""/Users/jonathan/projects/cassandra/svn-0.8/test/system/test_cql.py"", line 252, in test_select_range_with_single_column_results
    assert len(r) == 2
AssertionError
{noformat}",,cdaw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/May/11 16:20;xedin;CASSANDRA-2613.patch;https://issues.apache.org/jira/secure/attachment/12478447/CASSANDRA-2613.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20729,,,Tue May 10 22:30:30 UTC 2011,,,,,,,,,,"0|i0gcev:",93441,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"06/May/11 15:02;xedin;I have just fetched a fresh cassandra-0.8 branch and there is no support for BATCH with multiple statements (and no test for it, of course) it should be only in trunk right now CASSANDRA-2537, what branch have you been using to run tests?;;;","06/May/11 15:13;jbellis;(original discription was tests on trunk; updated for 0.8);;;","06/May/11 15:51;xedin;I see that the problem exists, I just didn't reinstall my cql driver, will fix ASAP, we don't return a ""null"" columns any more for a key so need to fix tests, sorry...;;;","06/May/11 15:59;jbellis;bq. we don't return a ""null"" columns any more for a key so need to fix tests

We need to keep returning the nulls as discussed in CASSANDRA-2593.;;;","06/May/11 16:31;jbellis;committed, thanks!;;;","10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merkle tree splitting can exit early,CASSANDRA-2605,12506185,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,04/May/11 15:51,16/Apr/19 09:33,14/Jul/23 05:52,05/May/11 08:53,0.8.0,,,,,,0,,,,"There was a small bug introduced by CASSANDRA-2324 that, depending on the key sample token, can make the merkle tree splitting process exit early, potentially resulting in a unnecessary imprecise tree.",,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"04/May/11 15:52;slebresne;0001-Avoid-stopping-merkle-tree-splitting-too-soon.patch;https://issues.apache.org/jira/secure/attachment/12478169/0001-Avoid-stopping-merkle-tree-splitting-too-soon.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20727,,,Wed May 11 21:28:24 UTC 2011,,,,,,,,,,"0|i0gcdb:",93434,,stuhood,,stuhood,Low,,,,,,,,,,,,,,,,,"04/May/11 17:56;stuhood;+1;;;","05/May/11 08:53;slebresne;Committed, thanks;;;","10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ;;;","11/May/11 21:28;hudson;Integrated in Cassandra #892 (See [https://builds.apache.org/hudson/job/Cassandra/892/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOFException on commitlogs,CASSANDRA-2604,12506154,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,terjem,terjem,04/May/11 10:00,16/Apr/19 09:33,14/Jul/23 05:52,06/May/11 10:17,0.8.0,,,,,,0,,,,"I have seen this occasionally since we started testing 0.8.

It happens when reading commitlogs on startups.

However, I have seen it a lot less on 0.8 beta2 (although this is from beta 2)

ERROR [main] 2011-05-04 18:02:38,134 AbstractCassandraDaemon.java (line 330) Exception encountered during startup.
java.io.EOFException
	at java.io.DataInputStream.readByte(DataInputStream.java:250)
	at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:357)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:368)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:252)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:43)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumns(ColumnFamilySerializer.java:136)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:126)
	at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:368)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:256)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:157)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:173)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:313)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)

Note that the line numbers on columnserializer may be off due to some local changes, but those changes are in code not executed in this case and I am 100% sure they do not trigger this problem.

I looked on this in the debugger in eclipse on a trunk from 0.8 2 weeks ago, and the interesting thing I saw was that according to the debugger, the offset of the inputstream to the deserializer was already at the end (very last byte) of the underlying bytebuffer but according to the stack, it was trying to read the length of the column name (first read done in the deserialized).
",,terjem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/May/11 13:47;slebresne;0001-avoid-modifying-original-mutation-during-apply.patch;https://issues.apache.org/jira/secure/attachment/12478274/0001-avoid-modifying-original-mutation-during-apply.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20726,,,Fri May 06 10:17:38 UTC 2011,,,,,,,,,,"0|i0gcd3:",93433,,,,,Normal,,,,,,,,,,,,,,,,,"04/May/11 18:24;jbellis;I strongly suspect this is present in 0.7, too: CASSANDRA-2197 looks like the same stacktrace.;;;","04/May/11 23:09;terjem;Time for some sleep, but I stepped through this code with a bad commit table (not the same which caused the above error though, but produces same stack).

    public void deserializeColumns(DataInput dis, ColumnFamily cf, boolean intern, boolean fromRemote) throws IOException
    {
        int size = dis.readInt();
        ColumnFamilyStore interner = intern ? Table.open(CFMetaData.getCF(cf.id()).left).getColumnFamilyStore(cf.id()) : null;
        for (int i = 0; i < size; ++i)
        {
            IColumn column = cf.getColumnSerializer().deserialize(dis, interner, fromRemote, (int) (System.currentTimeMillis() / 1000));
            cf.addColumn(column);
        }
    }

The size of the buffer underlying ""dis"" is 153 byte.

""size"" in the above code is 4.

With i == 2 finished, dis has just reached position 153 (perfectly) in the underlying buffer, although I cannot guarantee that the value is completely read.

Obviously, when trying to read at i==3, it crashes as it is already at the end of the buffer.

I would expect the entry which it is trying to decode in this case to have 4, not 3 columns.

I do see some of my own code is actually involved here, but unless the commitlog does its own serialization fully or partly and uses the ""standard"" deserializer my code is unlikely to cause any problems like this.;;;","05/May/11 13:47;slebresne;Terje, do you use secondary indexes in your tests ?

I think I've found a race (related to secondary indexes) that would explain the observed behavior. But without a good way to reproduce, hard to say if that is the problem.

The problem is in Table.apply(), where ignoreObsoleteMutations() may remove some columns from the original mutation. And since the commit log aliases the same mutation and, unless you are in batch mode, can write it anytime during apply(), it is possible for a column to be removed between the time where the commit log serialize the number of columns and the actual serialization of the columns (explaining why there is less columns than advertised on replay).

Attaching a patch that lazily clone the column family in ignoreObsoleteMutation to avoid modifying the original one. Note that 0.7 is not concerned, because in 0.7 we give to the commit log the mutation already serialized.;;;","05/May/11 14:27;terjem;Secondary indexes - Check  (the error at least in the case I studied now is on a CF with secondary index)
Not in batch mode - Check 

I am not 100% sure I understand in what cases a column may be removed. 

There should be no deletes here and I think no updates to columns that already exists (I will have to check that...), but I am messing a bit around with the system while a couple of feeders are continuously running and they will refeed if there are errors. It is very likely that there are are quite a few retries going on at times which I guess could trigger what you are referring too?

I will apply the patch at once and test.

Thanks!;;;","05/May/11 15:47;jbellis;I don't think it's actually necessary to even modify the original mutation, since applying obsolete columns to the data path is harmless.  I removed that entirely in the CASSANDRA-2401 patch (not yet applied).;;;","05/May/11 17:56;slebresne;I propose to see if Terje can reproduce. If not, this was the actual problem and I guess we'll close this on the account that CASSANDRA-2401 fixed it as a side effect.;;;","05/May/11 18:50;terjem;After stressing the system a bit with a bunch of restarts, I have not manage to reproduce the problem again, so looks good so far.

I will leave it feeding overnight and try one more time in the morning and then I will try to reproduce with 2401 instead.

On a side notice... I sometimes see quite a few commitlog segments. Right now, one node had 110 of them and it took 20 minutes to read all the logs.

This seems a bit excessive?;;;","05/May/11 19:02;jbellis;We're working on CASSANDRA-2427 to deal with commitlog segment proliferation.;;;","06/May/11 10:15;terjem;Not able to reproduce the problem with the patch in this ticket.

Currently using CASSANDRA-2401 and so far so good.

I think you can just close this ticket for now and I will reopen if I see the problem again.

Thanks!;;;","06/May/11 10:16;slebresne;Alright, great. Thanks for the testing.;;;","06/May/11 10:17;slebresne;Resolving as fixed by CASSANDRA-2401;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: Range query throws errors when run thru cqlsh but passes in system test,CASSANDRA-2600,12506117,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,03/May/11 22:54,16/Apr/19 09:33,14/Jul/23 05:52,09/May/11 07:14,0.8.0,,,,,,0,cql,,,"*It appears the following nose test breaks when run via cqlsh*
{code}
CREATE COLUMNFAMILY StandardLongA (KEY text PRIMARY KEY) WITH comparator = bigint AND default_validation = ascii;

UPDATE StandardLongA SET 1='1', 2='2', 3='3', 4='4' WHERE KEY='aa';
UPDATE StandardLongA SET 5='5', 6='6', 7='8', 9='9' WHERE KEY='ab';
UPDATE StandardLongA SET 9='9', 8='8', 7='7', 6='6' WHERE KEY='ac';
UPDATE StandardLongA SET 5='5', 4='4', 3='3', 2='2' WHERE KEY='ad';
UPDATE StandardLongA SET 1='1', 2='2', 3='3', 4='4' WHERE KEY='ae';
UPDATE StandardLongA SET 1='1', 2='2', 3='3', 4='4' WHERE KEY='af';
UPDATE StandardLongA SET 5='5', 6='6', 7='8', 9='9' WHERE KEY='ag';

cqlsh> SELECT 4 FROM StandardLongA WHERE KEY > 'ad' AND KEY < 'ag';
Internal application error

cqlsh> SELECT * FROM StandardLongA WHERE KEY > 'ad' AND KEY < 'ag';
Internal application error
{code}


{code}

ERROR 21:43:16,880 Internal error processing execute_cql_query
java.lang.AssertionError: [109302822465993666080409141220504733189,104027502549504462599318918375258179002]
	at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:40)
	at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:33)
	at org.apache.cassandra.cql.QueryProcessor.multiRangeSlice(QueryProcessor.java:142)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:507)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1127)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{code}

*This test case runs nightly in the system tests and passes*
[http://173.203.89.16:8080/job/CassandraSystem/]
{code}
jenkins@mallen2:~/jobs/Cassandra/workspace$ nosetests test/system/test_cql.py
..................................
----------------------------------------------------------------------
Ran 34 tests in 147.040s

OK

{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/May/11 10:39;xedin;CASSANDRA-2600.patch;https://issues.apache.org/jira/secure/attachment/12478264/CASSANDRA-2600.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20724,,,Tue May 10 22:30:30 UTC 2011,,,,,,,,,,"0|i0gcc7:",93429,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"04/May/11 17:44;xedin;Can you post your environment/cluster settings here please?...;;;","04/May/11 19:47;xedin;Does select work from you with ByteOrderedPartitioner?;;;","04/May/11 23:24;brandon.williams;If you are using RandomPartioner, what is happening is it's comparing the md5 value of 'ad' against 'ag' and it turns out 'ad' is higher when hashed.  This isn't a useful query to perform with RP (it will never do what you want) so CQL should probably just throw a better error here to let the user know that.;;;","05/May/11 06:01;cdaw;The test now passes after changing cassandra.yaml to use 
partitioner: org.apache.cassandra.dht.ByteOrderedPartitioner

There is a comment in the test_cql.py file that says:
{code}
# FIXME: The above is woefully inadequate, but the test config uses
# CollatingOrderPreservingPartitioner which only supports UTF8.
{code}

;;;","05/May/11 09:39;xedin;It seems though that that partitioner is needed for some of the tests, so let it be for now. I will throw a prettier error if user tries to do a key range slice with RandomPartitioner, will attach file asap.;;;","05/May/11 10:19;jbellis;Here's what we do in classic thrift if end < start:

{code}
                if (p instanceof RandomPartitioner)
                    throw new InvalidRequestException(""start key's md5 sorts after end key's md5.  this is not allowed; you probably should not specify end key at all, under RandomPartitioner"");
                else
                    throw new InvalidRequestException(""start key must sort before (or equal to) finish key in your partitioner!"");
{code};;;","05/May/11 10:39;xedin;Please apply after CASSANDRA-2592;;;","09/May/11 07:14;jbellis;committed;;;","10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Command Line Client has Memtable thresholds: (millions of ops/minutes/MB) last two parameters reversed,CASSANDRA-2599,12506103,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,joelastpass,joelastpass,03/May/11 21:31,16/Apr/19 09:33,14/Jul/23 05:52,03/May/11 21:36,0.8.0,,,,,,0,cli,,,"Here's a patch

Index: src/java/org/apache/cassandra/cli/CliClient.java
===================================================================
--- src/java/org/apache/cassandra/cli/CliClient.java	(revision 1099262)
+++ src/java/org/apache/cassandra/cli/CliClient.java	(working copy)
@@ -1325,7 +1325,7 @@
                 sessionState.out.printf(""      Row cache size / save period in seconds: %s/%s%n"", cf_def.row_cache_size, cf_def.row_cache_save_period_in_seconds);
                 sessionState.out.printf(""      Key cache size / save period in seconds: %s/%s%n"", cf_def.key_cache_size, cf_def.key_cache_save_period_in_seconds);
                 sessionState.out.printf(""      Memtable thresholds: %s/%s/%s (millions of ops/minutes/MB)%n"",
-                                cf_def.memtable_operations_in_millions, cf_def.memtable_throughput_in_mb, cf_def.memtable_flush_after_mins);
+                                cf_def.memtable_operations_in_millions, cf_def.memtable_flush_after_mins, cf_def.memtable_throughput_in_mb);
                 sessionState.out.printf(""      GC grace seconds: %s%n"", cf_def.gc_grace_seconds);
                 sessionState.out.printf(""      Compaction min/max thresholds: %s/%s%n"", cf_def.min_compaction_threshold, cf_def.max_compaction_threshold);
                 sessionState.out.printf(""      Read repair chance: %s%n"", cf_def.read_repair_chance);
",All,,,,,,,,,,,,,,,,,,,,,,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20723,,,Tue May 03 21:36:29 UTC 2011,,,,,,,,,,"0|i0gcbz:",93428,,,,,Low,,,,,,,,,,,,,,,,,"03/May/11 21:36;joelastpass;jbellis fixed this in April;;;","03/May/11 21:36;joelastpass;jbellis fixed this in April
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incremental_backups and snapshot_before_compaction duplicate hard links,CASSANDRA-2598,12506094,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,mck,mck,03/May/11 20:19,16/Apr/19 09:33,14/Jul/23 05:52,05/May/11 15:42,0.8.0,,,,,,0,,,,"See discussion @ http://thread.gmane.org/gmane.comp.db.cassandra.user/15933/

Enabling both incremental_backups and snapshot_before_compaction leads to the same hard links trying to be created.

This gives stacktraces like 

java.io.IOError: java.io.IOException: Unable to create hard link from
/cassandra-data/<keyspace>/<cf>-f-3875-Data.db
to
/cassandra-data/<keyspace>/snapshots/compact-<cf>/<cf>-f-3875-Data.db
(errno 17)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1629)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1654)
	at org.apache.cassandra.db.Table.snapshot(Table.java:198)
	at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:504)
	at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:146)
	at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:112)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Unable to create hard link from
/cassandra-data/<keyspace>/<cf>-f-3875-Data.db
to
/cassandra-data/<keyspace>/snapshots/compact-<cf>/<cf>-f-3875-Data.db
(errno 17)
	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:155)
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:713)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1622)
	... 10 more
",linux & jna,edevil,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/11 20:40;jbellis;2598.txt;https://issues.apache.org/jira/secure/attachment/12478091/2598.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20722,,,Tue May 08 21:22:09 UTC 2012,,,,,,,,,,"0|i0gcbr:",93427,,mck,,mck,Low,,,,,,,,,,,,,,,,,"03/May/11 20:39;jbellis;It looks like I was wrong -- this isn't a problem w/ incremental_backups, but a regression in snapshot_before_compaction from CASSANDRA-1791.;;;","03/May/11 20:40;jbellis;Patch to make snapshot_before_compaction directory names unique again;;;","05/May/11 14:08;mck;the patch fixes the problem;;;","05/May/11 15:42;jbellis;committed, thanks!;;;","10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ;;;","08/May/12 11:08;edevil;I think I have a similar issue but I don't have incremental_backups or snapshot_before_compaction enabled, and I'm using 1.1.

Since I upgraded to Cassandra 1.1, I get the following error when trying to delete a CF. After this happens the CF is not accessible anymore, but I cannot create another one with the same name until I restart the server.

INFO [MigrationStage:1] 2012-05-07 18:10:12,682 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-schema_columnfamilies@1128094887(978/1222 serialized/live bytes, 21 ops)
INFO [FlushWriter:2] 2012-05-07 18:10:12,682 Memtable.java (line 266) Writing Memtable-schema_columnfamilies@1128094887(978/1222 serialized/live bytes, 21 ops)
INFO [FlushWriter:2] 2012-05-07 18:10:12,720 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-28-Data.db (1041 bytes)
INFO [MigrationStage:1] 2012-05-07 18:10:12,721 ColumnFamilyStore.java (line 634) Enqueuing flush of Memtable-schema_columns@1599271050(392/490 serialized/live bytes, 8 ops)
INFO [FlushWriter:2] 2012-05-07 18:10:12,722 Memtable.java (line 266) Writing Memtable-schema_columns@1599271050(392/490 serialized/live bytes, 8 ops)
INFO [CompactionExecutor:8] 2012-05-07 18:10:12,722 CompactionTask.java (line 114) Compacting [SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-26-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-28-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfam
ilies-hc-27-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-25-Data.db')]
INFO [FlushWriter:2] 2012-05-07 18:10:12,806 Memtable.java (line 307) Completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-hc-23-Data.db (447 bytes)
INFO [CompactionExecutor:8] 2012-05-07 18:10:12,811 CompactionTask.java (line 225) Compacted to [/var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-hc-29-Data.db,].  24,797 to 21,431
(~86% of original) bytes for 2 keys at 0.232252MB/s.  Time: 88ms.
ERROR [MigrationStage:1] 2012-05-07 18:10:12,895 CLibrary.java (line 158) Unable to create hard link
com.sun.jna.LastErrorException: errno was 17
at org.apache.cassandra.utils.CLibrary.link(Native Method)
at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:150)
at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
at java.util.concurrent.FutureTask.run(FutureTask.java:138)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
ERROR [Thrift:17] 2012-05-07 18:10:12,898 CustomTThreadPoolServer.java (line 204) Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/
Disco/Client/snapshots/1336410612893-Client/Client.json (errno 17)
at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:372)
at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:191)
at org.apache.cassandra.service.MigrationManager.announceColumnFamilyDrop(MigrationManager.java:182)
at org.apache.cassandra.thrift.CassandraServer.system_drop_column_family(CassandraServer.java:948)
at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family.getResult(Cassandra.java:3348)
at org.apache.cassandra.thrift.Cassandra$Processor$system_drop_column_family.getResult(Cassandra.java:3336)
at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336410612893-Client/Client.json (errno 17)
at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
at java.util.concurrent.FutureTask.get(FutureTask.java:83)
at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:368)
... 11 more
Caused by: java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336410612893-Client/Client.json (errno 17)
at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1454)
at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
at java.util.concurrent.FutureTask.run(FutureTask.java:138)
... 3 more
Caused by: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336410612893-Client/Client.json (errno 17)
at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:163)
at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
... 10 more
ERROR [MigrationStage:1] 2012-05-07 18:10:12,899 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[MigrationStage:1,5,main]
java.io.IOError: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336410612893-Client/Client.json (errno 17)
at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1454)
at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1483)
at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:512)
at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:403)
at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:270)
at org.apache.cassandra.service.MigrationManager$1.call(MigrationManager.java:214)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
at java.util.concurrent.FutureTask.run(FutureTask.java:138)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Unable to create hard link from /var/lib/cassandra/data/Disco/Client/Client.json to /var/lib/cassandra/data/Disco/Client/snapshots/1336410612893-Client/Client.json (errno 17)
at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:163)
at org.apache.cassandra.db.Directories.snapshotLeveledManifest(Directories.java:343)
at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1450)
... 10 more


I've tried recreating the data dirs, to see if this was some kind of permissions problem, but the error happens every time and with any CF. I'm using the Cassandra Debian package on Debian squeeze and the Sun JVM (build 1.6.0_26-b03).;;;","08/May/12 21:22;jbellis;Please create a new issue with steps to reproduce.  Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inconsistent implementation of 'cumulative distribution function' for Exponential Distribution,CASSANDRA-2597,12506089,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,jbellis,jbellis,03/May/11 19:28,16/Apr/19 09:33,14/Jul/23 05:52,23/May/11 19:44,0.8.1,,,,,,0,,,,"As reported on the mailing list (http://mail-archives.apache.org/mod_mbox/cassandra-dev/201104.mbox/%3CAANLkTimdMSLE8-z0x+0kvzqp7za3AEMLaOFXvd4Z=tvc@mail.gmail.com%3E),

{quote}
I just found there are two implementations of 'cumulative distribution
function' for Exponential Distribution and there are inconsistent :

*FailureDetector*
{code:java}
org.apache.cassandra.gms.ArrivalWindow.p(double)
   double p(double t)
   {
       double mean = mean();
       double exponent = (-1)*(t)/mean;
       return *Math.pow(Math.E, exponent)*;
   }
{code}

*DynamicEndpointSnitch*
{code:java}
org.apache.cassandra.locator.AdaptiveLatencyTracker.p(double)
   double p(double t)
   {
       double mean = mean();
       double exponent = (-1) * (t) / mean;
       return *1 - Math.pow( Math.E, exponent);*
   }
{code}

According to the  Exponential Distribution cumulative distribution function
definition<http://en.wikipedia.org/wiki/Exponential_distribution#Cumulative_distribution_function>,
the later one is correct
{quote}

... however FailureDetector has been working as advertised for some time now.  Does this mean the Snitch version is actually wrong?",,mauzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/May/11 23:57;thepaul;0001-simplify-failure-detection-calculations.txt;https://issues.apache.org/jira/secure/attachment/12479860/0001-simplify-failure-detection-calculations.txt",,,,,,,,,,,,,,1.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20721,,,Mon May 23 20:32:28 UTC 2011,,,,,,,,,,"0|i0gcbj:",93426,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"19/May/11 21:15;thepaul;Neither are wrong, as far as getting valid answers. Both are wrong in that they do much more work than necessary.

h3. FailureDetector/ArrivalWindow

In creating their failure predictor between gossip nodes in a distributed system, the original Cassandra authors made a modification to the sample φ Accrual Failure Detector implementation from the [original paper|http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.80.7427&rep=rep1&type=pdf]. They mention in their own [Cassandra paper|http://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf] that ""Although the original paper suggests that the distribution is approximated by the Gaussian distribution we found the Exponential Distribution to be a better approximation, because of the nature of the gossip channel and its impact on latency."" Nothing more is said on the topic, but it was likely because the original Phi Accrual paper implementation was expecting regular heartbeat messages, while ArrivalWindow measures only the intervals between reception of Gossip 'Syn', 'Ack', and 'Ack2' messages from a given endpoint. Regular message transmissions experiencing typical random jitter [will follow a normal distribution|http://www.maxim-ic.com/app-notes/index.mvp/id/1916/CMP/WP-34], but since gossip messages from endpoint A to endpoint B are sent at random intervals, they likely make up a [Poisson process|http://en.wikipedia.org/wiki/Poisson_process], making the exponential distribution appropriate.

I'll show the math here, since someone at some point will probably wonder wtf is going on in this code again, and hopefully I can save them the depth of exploration I went to.

Take the definition of {{P_later}}, which determines how likely it is that endpoint B has failed. The {{t}} parameter is the amount of time elapsed since the last Syn/Ack/Ack2 gossip message seen from endpoint B:

{code}
    P_later(t) = 1 - F(t)
{code}

where {{F(t)}} is the [CDF|http://en.wikipedia.org/wiki/Cumulative_distribution_function] of the event distribution. For the exponential distribution, the CDF is {{1 - e^(-Lt)}}, where {{L}} is the rate parameter.

{code}
    P_later(t) = 1 - (1 - e^(-Lt))
{code}

The maximum likelihood estimation for the rate parameter L is given by {{1/mean}}, where mean is the arithmetic mean of observed times from the actual data (here, the most recent gossip message inter-arrival times from endpoint B). It is this rate parameter we expect to vary over time, making necessary the storage of the sliding window of arrival intervals.

{code}
    P_later(t) = 1 - (1 - e^(-t/mean))
{code}

The original Cassandra authors stopped here. The Apache Cassandra developers made the obvious simplification:

{code}
    P_later(t) = e^(-t/mean)
{code}

But I will go further and look at the way {{P_later}} is used in the phi calculation:

{code}
    phi(t) = -log10(P_later(t))
{code}

Expanding to:

{code}
    phi(t) = -log10(e^(-t/mean))
{code}

But wait, the log of an exponentiation? Doesn't that mean...

{code}
    phi(t) = -log(e^(-t/mean)) / log(10)
           = (t/mean) / log(10)
{code}

so approximately

{code}
    phi(t) = 0.4342945 * t/mean
{code}

Yep, that's a hell of a lot simpler for computers to calculate than

{code}
    (-1) * Math.log10(Math.pow(Math.e, ((-1) * (t)/mean)))
{code}

, the way we have been doing it.

h3. DynamicEndpointSnitch/AdaptiveLatencyTracker

This version originated as an optimization of BoundedStatsDeque plus ArrivalWindow. The aim was to sort known endpoints by their {{phi}} values, assuming the same constant {{t}} value for each.

{code}
    score(E) = phi_E(0.0001)
{code}

(E is the endpoint, and {{phi_E}} is the {{phi}} function which uses the mean of recent message inter-arrival times from E, which we'll call {{E_mean}}). Expanded:

{code}
score(E) = -log10(P_later_E(0.0001))

P_later_E(t) = e^(-t/E_mean)
{code}

However, when the developer found that the {{score()}} values were going _down_ for nodes with higher average latency instead of up, he most likely looked at the original version of {{P_later()}} and added back one of the ""one minuses"", and not the other, because that made the signs work as expected. It currently uses:

{code}
    P_later_E(t) = 1 - e^(-t/E_mean)
{code}

However, this was probably misguided: the scores were going down because the phi accrual failure detector assigns higher badness values to nodes with a low recent latency than to nodes with high recent latency, given the same {{t}} values for both (because it waits longer to convict a node with high recent latency). There's no good mathematical reason for the {{AdaptiveLatencyTracker.p()}} method to look exactly like it does. However, it's mathematically correct in that it will indeed sort endpoints by recent latency.

{code}
    score(E_mean) = -log10(1 - e^(-0.0001/E_mean))
{code}

The {{score()}} method is only used as a measure to sort endpoints. However, as defined, it is monotonically increasing \(*), meaning that the exact same sorting could be made using the identity function:

{code}
    score(E_mean) = E_mean
{code}

I will attach a patch which makes these simplifications and adjusts the related unit tests accordingly.

\(*) The math to show this seems unnecessary here. It's not too hard to work through though.

Jira wiki syntax sucks.;;;","19/May/11 23:57;thepaul;No changes to the tests were necessary.;;;","23/May/11 19:44;brandon.williams;Great analysis!  Committed.;;;","23/May/11 20:32;hudson;Integrated in Cassandra-0.8 #128 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/128/])
    Simplify FD/DES calculations.
Patch by Paul Cannon, reviewed by brandonwilliams for CASSANDRA-2597

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1126682
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/FailureDetector.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/DynamicEndpointSnitch.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
include indexes in snapshots,CASSANDRA-2596,12506088,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,03/May/11 19:18,16/Apr/19 09:33,14/Jul/23 05:52,05/May/11 14:25,0.7.6,0.8.0,,,,,1,,,,CFS.snapshot should include index sstables as well.  Since flushing the parent CF (which we do as part of snapshot) also flushes index CFs consistently w/ the parent data this should work as expected.,,muyen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/11 19:19;jbellis;2596.txt;https://issues.apache.org/jira/secure/attachment/12478087/2596.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20720,,,Tue May 10 22:30:29 UTC 2011,,,,,,,,,,"0|i0gcbb:",93425,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"05/May/11 14:25;slebresne;+1, committed.;;;","10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tame excessive logging during repairs,CASSANDRA-2595,12506077,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,doubleday,doubleday,doubleday,03/May/11 17:02,16/Apr/19 09:33,14/Jul/23 05:52,04/May/11 08:43,0.7.6,,,,,,0,,,,"PendingFile.toString is called from logging (i.e. StreamOut:173) which lists all sections in the pending file.

This leads to (in our case multi mb ) ... (59352638,59354005),(59373477,59379520),(59381952,59385368) ... in the log.",,doubleday,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/11 17:04;doubleday;PendingFileToString.patch;https://issues.apache.org/jira/secure/attachment/12478069/PendingFileToString.patch",,,,,,,,,,,,,,1.0,doubleday,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20719,,,Wed May 04 09:28:57 UTC 2011,,,,,,,,,,"0|i0gcb3:",93424,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"03/May/11 17:22;slebresne;+1, repair is clearly too verbose (I'll commit that soonish);;;","04/May/11 08:39;hudson;Integrated in Cassandra-0.7 #467 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/467/])
    Tame excessive logging during repairs
patch by doubleday; reviewed by slebresne for CASSANDRA-2595
;;;","04/May/11 08:43;slebresne;Committed, thanks;;;","04/May/11 09:28;hudson;Integrated in Cassandra-0.8 #64 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/64/])
    merge CASSANDRA-2595 from 0.8
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL: Errors when running unqualified ""select column"" statement (no where clause)",CASSANDRA-2593,12505999,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,03/May/11 00:49,16/Apr/19 09:33,14/Jul/23 05:52,03/May/11 17:46,0.8.0,,,Legacy/CQL,,,0,cql,,,"*Seed Data*
{code}
CREATE KEYSPACE cqldb with strategy_class = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options:replication_factor=2;

USE cqldb;

CREATE COLUMNFAMILY users (KEY varchar PRIMARY KEY, password varchar, gender varchar, session_token varchar, state varchar, birth_year bigint);


INSERT INTO users (KEY, password) VALUES ('user0', 'ch@ngem3');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3a', 'f', 'TX', '1968');
INSERT INTO users (KEY, password) VALUES ('user2', 'ch@ngem3b');
INSERT INTO users (KEY, password) VALUES ('user3', 'ch@ngem3c');
{code}

*Query #1 - select varchar column*
{code}
cqlsh> select state from users;
u'user1' | u'state',u'TX'
Exception: 'NoneType' object has no attribute 'decode'

cqlsh> select state from users where KEY='user1';
u'user1' | u'state',u'TX'
{code}

*Query #2 - select bigint column*
{code}
cqlsh> select birth_year from users;
Exception: unpack requires a string argument of length 8

cqlsh> select birth_year from users where KEY='user1';
u'user1' | u'birth_year',1968
{code}

*A simple 'SELECT *' with no WHERE clause works fine*
{code}
cqlsh> select * from users;
u'user1' | u'birth_year',1968 | u'gender',u'f' | u'password',u'ch@ngem3a' | u'state',u'TX'
u'user0' | u'password',u'ch@ngem3'
u'user3' | u'password',u'ch@ngem3c'
u'user2' | u'password',u'ch@ngem3b'
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/11 16:58;xedin;CASSANDRA-2593-v2.patch;https://issues.apache.org/jira/secure/attachment/12478067/CASSANDRA-2593-v2.patch","03/May/11 15:35;xedin;CASSANDRA-2593.patch;https://issues.apache.org/jira/secure/attachment/12478060/CASSANDRA-2593.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20718,,,Tue May 03 20:30:00 UTC 2011,,,,,,,,,,"0|i0gcan:",93422,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"03/May/11 15:35;xedin;branch: cassandra-0.8 (can be applied to trunk also). In the cql/decoders.py we skip a column with an empty value instead of trying to decode it. ;;;","03/May/11 15:43;jbellis;{code}
+                if len(row) < 2:
+                    continue # if there are no columns, we skip
{code}

This looks unrelated to the empty column fix.  (It sort of looks like an attempt at addressing CASSANDRA-2548.);;;","03/May/11 15:47;xedin;If there are no columns to show why should we show an empty key?;;;","03/May/11 16:00;jbellis;Short answer, same reason Cathy linked over there: http://wiki.apache.org/cassandra/FAQ#range_ghosts.

Longer answer, I plan to address on 2548.  For now let's fix the empty column bug.

Can you add a test_cql.py test that catches the problem, to prevent a regression?;;;","03/May/11 16:10;xedin;The thing is that when you do ""select name from users"" and only few of the rows have a ""name"" column, cqlsh will output all other keys as empty. If you want I can remove check from the CQLSH;;;","03/May/11 16:20;jbellis;bq. The thing is that when you do ""select name from users"" and only few of the rows have a ""name"" column, cqlsh will output all other keys as empty.

Right. (And this part, at least, *is* intuitive to someone coming from SQL: if I have a null column, I still want to see the row included.);;;","03/May/11 16:58;xedin;removed check from CQLSH and added a test.;;;","03/May/11 17:46;jbellis;committed, thanks!;;;","03/May/11 19:19;hudson;Integrated in Cassandra-0.8 #62 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/62/])
    fix returning null column values in the python cql driver
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-2593
;;;","03/May/11 20:30;cdaw;The patch works as expected:

{code}
cqlsh> select state from users;
u'user1' | u'state',u'TX'
u'user0'
u'user3'
u'user2'

cqlsh> select birth_year from users;
u'user1' | u'birth_year',1968
u'user0'
u'user3'
u'user2'
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL greater-than and less-than operators (> and <) result in key ranges that are inclusive of the terms,CASSANDRA-2592,12505959,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,02/May/11 17:42,16/Apr/19 09:33,14/Jul/23 05:52,21/May/11 03:07,0.8.0,,,Legacy/CQL,,,0,,,,"This affects range queries against keys, but not index queries.

One possible solution: let the coordinator strip out the extra row in QueryProcessor.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/11 02:30;jbellis;2592-v3-in-progress.txt;https://issues.apache.org/jira/secure/attachment/12478486/2592-v3-in-progress.txt","20/May/11 20:38;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2592-handle-empty-result-sets.txt;https://issues.apache.org/jira/secure/attachment/12479951/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2592-handle-empty-result-sets.txt","20/May/11 00:07;xedin;CASSANDRA-2592-fix-when-rows-are-empty.patch;https://issues.apache.org/jira/secure/attachment/12479862/CASSANDRA-2592-fix-when-rows-are-empty.patch","04/May/11 23:13;xedin;CASSANDRA-2592-v2.patch;https://issues.apache.org/jira/secure/attachment/12478228/CASSANDRA-2592-v2.patch","07/May/11 13:09;xedin;CASSANDRA-2592-v3.patch;https://issues.apache.org/jira/secure/attachment/12478510/CASSANDRA-2592-v3.patch","04/May/11 16:15;xedin;CASSANDRA-2592.patch;https://issues.apache.org/jira/secure/attachment/12478174/CASSANDRA-2592.patch",,,,,,,,,6.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20717,,,Sat May 21 03:52:54 UTC 2011,,,,,,,,,,"0|i0gcaf:",93421,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"04/May/11 21:51;jbellis;I think this patch makes it so if I say ""LIMIT 10"" I might only get 8 back because one result got chopped off.  QP will need to request more than LIMIT to give back the right number.

Can you add a test for this?;;;","04/May/11 23:13;xedin;added a check which will prevent removing a first/last row if it wasn't start/end key. Tests for LIMIT added.;;;","07/May/11 02:30;jbellis;Some issues around LIMIT. I've added more tests in v3 to demonstrate (but not fix) the problem.;;;","07/May/11 13:09;xedin;v3 with your tests included and insured to pass.;;;","07/May/11 23:16;jbellis;committed w/ minor changes;;;","10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ;;;","19/May/11 23:43;urandom;
This blows up on queries that return no results.

{noformat}
java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.cql.QueryProcessor.multiRangeSlice(QueryProcessor.java:194)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:534)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1131)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat};;;","20/May/11 21:55;jbellis;I think Pavel's patch is more correct since the result could come back with exactly one row equal to the start key on a ""KEY > X AND KEY < Y"" query.  Then we want to remove the extra row on the first check, w/o erroring out on the second.;;;","21/May/11 03:07;urandom;you're right; +1.  committed.;;;","21/May/11 03:52;hudson;Integrated in Cassandra-0.8 #119 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/119/])
    properly handle empty result set

Patch by Pavel Yaskevich; reviewed by eevans for CASSANDRA-2592

eevans : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1125622
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
row delete breaks read repair,CASSANDRA-2590,12505899,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,02/May/11 03:44,16/Apr/19 09:33,14/Jul/23 05:52,09/Jun/11 13:51,0.7.7,0.8.1,,,,,0,,,,"related to CASSANDRA-2589 

Working at CL ALL can get inconsistent reads after row deletion. Reproduced on the 0.7 and 0.8 source. 

Steps to reproduce:

# two node cluster with rf 2 and HH turned off
# insert rows via cli 
# flush both nodes 
# shutdown node 1
# connect to node 2 via cli and delete one row
# bring up node 1
# connect to node 1 via cli and issue get with CL ALL 
# first get returns the deleted row, second get returns zero rows.

RowRepairResolver.resolveSuperSet() resolves a local CF with the old row columns, and the remote CF which is marked for deletion. CF.resolve() does not pay attention to the deletion flags and the resolved CF has both markedForDeletion set and a column with a lower timestamp. The return from resolveSuperSet() is used as the return for the read without checking if the cols are relevant. 

Also when RowRepairResolver.mabeScheduleRepairs() runs it sends two mutations. Node 1 is given the row level deletation, and Node 2 is given a mutation to write the old (and now deleted) column from node 2. I have some log traces for this if needed. 

A quick fix is to check for relevant columns in the RowRepairResolver, will attach shortly.    ",,cywjackson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2621,,,,,,,,,,,,,"09/May/11 10:04;amorton;0001-2590-v3.patch;https://issues.apache.org/jira/secure/attachment/12478579/0001-2590-v3.patch","02/May/11 04:06;amorton;0001-cf-resolve-test-and-possible-solution-for-read-repai.patch;https://issues.apache.org/jira/secure/attachment/12477928/0001-cf-resolve-test-and-possible-solution-for-read-repai.patch","03/May/11 22:01;jbellis;2590-v2.txt;https://issues.apache.org/jira/secure/attachment/12478099/2590-v2.txt","09/Jun/11 00:45;jbellis;2590-v4-0.7.txt;https://issues.apache.org/jira/secure/attachment/12481882/2590-v4-0.7.txt",,,,,,,,,,,4.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20715,,,Thu Jun 09 14:08:59 UTC 2011,,,,,,,,,,"0|i0gc9z:",93419,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"02/May/11 04:06;amorton;unit test to show columns in a deleted CF after calling resolve() and a hack fix for the use case described above.;;;","03/May/11 21:20;jbellis;So the problem is that something in repair isn't calling removeDeleted?  That's the approach we normally take; it's like your ensureRelevant, but it doesn't mutate the original copy (which is important since the original might be part of a cache).  Here's your test modified to use that:

{code}
    @Test
    public void testEnsureRelevant()
    {
        ColumnFamily cf1 = ColumnFamily.create(""Keyspace1"", ""Standard1"");
        cf1.addColumn(column(""one"", ""A"", 0));

        ColumnFamily cf2 = ColumnFamily.create(""Keyspace1"", ""Standard1"");
        cf2.delete((int) (System.currentTimeMillis() / 1000), 1);

        cf2.resolve(cf1);
        assert cf2.getColumnCount() == 1;
        
        ColumnFamily cleaned = ColumnFamilyStore.removeDeleted(cf2, Integer.MAX_VALUE);
        assert cleaned == null;
    }
{code};;;","03/May/11 22:01;jbellis;... but that's not what we want for RowRepairResolver. (I freely admit that dealing with tombstones is subtle and tricky. :)

removeDeleted will give you back a version of the row with any GC-able tombstones removed. That's not what we want for read repair; we want to preserve tombstones, but we want a ""canonical"" representation of only the minimum tombstones necessary. (Technically, this doesn't matter for the repair per se, because repairing obsolete data is harmless. What we're concerned with is getting the right result back to the client, and thriftifyColumns & friends in CassandraServer assume that canonicalization has been performed previously.)

So we do want to do what you were doing with ensureRelevant, but it's a little more complex than that because we have the same problem at the supercolumn level, as at the row level.

QueryFilter.collectCollatedColumns is responsible for doing this when merging different versions from memtables and sstables, so we just need to wire it up in RRR. Here's a patch that uses an IdentityQueryFilter to do this.;;;","07/May/11 03:08;amorton;Thanks, am doing some more tests on super columns;;;","09/May/11 10:04;amorton;2590-v3 uses removeDeleted() in RowRepairResolver.resolveSuperset() and includes tests in RowResolverTest.Patch is for 0.8.

CASSANDRA-2621 shows that QueryFilter.collectCollatedColumns() returns a CF with deleted columns and the caller should use removeDeleted. 

Continuing to use CF.resolve() seemed like the minimum change. Let me know if you think we should still use QueryFilter to resolve the differences. ;;;","09/Jun/11 00:45;jbellis;I think we're almost there.

The trick is you actually need _both_ collectCollatedColumns and removeDeleted, since rD assumes cCC has already been called (which it is, when we're merging versions from different sstables...  but not when we're merging versions from different replicas, as in RRR).

Added a test (testResolveDeletedSuper) to illustrate this.  Fails against v3 (rD but no cCC) but passes w/ v4 (cCC and rD).;;;","09/Jun/11 08:37;slebresne;+1 on v4, we do need both calls.

That being said, we should probably refactor that part of the code someday because it is not the cleanest thing ever. And there is probably ways to avoid those two phases (which does do some duplicate works I believe).;;;","09/Jun/11 13:51;jbellis;committed.  thanks Aaron!;;;","09/Jun/11 14:08;hudson;Integrated in Cassandra-0.7 #504 (See [https://builds.apache.org/job/Cassandra-0.7/504/])
    fix removing columns and subcolumns that are supressed by a row orsupercolumn tombstone during replica resolution
patch by Aaron Morton and jbellis; reviewed by slebresne for CASSANDRA-2590

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1133873
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/test/unit/org/apache/cassandra/service/RowResolverTest.java
* /cassandra/branches/cassandra-0.7/test/unit/org/apache/cassandra/Util.java
* /cassandra/branches/cassandra-0.7/test/unit/org/apache/cassandra/db/TableTest.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/RowRepairResolver.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rebuffer called excessively during seeks,CASSANDRA-2581,12505751,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,29/Apr/11 04:34,16/Apr/19 09:33,14/Jul/23 05:52,03/May/11 15:50,0.7.6,0.8.0,,,,,0,,,,"When doing an strace tonight, I noticed during memtable flushes that we were only writing 1KB per every write() system call...After diving more into it, it's because of a bug in the seek() code. 

if (newPosition >= bufferOffset + validBufferBytes || newPosition < bufferOffset)

vs.

if (newPosition > (bufferOffset + validBufferBytes) || newPosition < bufferOffset)

Two things I noticed, we shouldn't need to rebuffer if newPosition is equal to bufferOffset + validBufferBytes, second the evaluation was doing (newPosition >= bufferOffset) + validBufferBytes which always seemed to be true.
",,alanliang,cburroughs,doubleday,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/11 04:37;lenn0x;0001-Rebuffer-called-excessively-during-seeks.patch;https://issues.apache.org/jira/secure/attachment/12477725/0001-Rebuffer-called-excessively-during-seeks.patch","29/Apr/11 15:53;jbellis;2581.txt;https://issues.apache.org/jira/secure/attachment/12477807/2581.txt",,,,,,,,,,,,,2.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20711,,,Tue May 03 16:04:04 UTC 2011,,,,,,,,,,"0|i0gc7z:",93410,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"29/Apr/11 15:53;jbellis;bq. we shouldn't need to rebuffer if newPosition is equal to bufferOffset + validBufferBytes

I think that is correct, patch attached. I will ask Pavel to also review since this is extremely important not to break.

bq. second the evaluation was doing (newPosition >= bufferOffset) + validBufferBytes

No, addition is higher precedence than comparison (or the logical operations). In fact if you force the grouping you suggest, javac will reject it since you cannot add a boolean and an int.;;;","03/May/11 10:49;xedin;+1, we can just change ""current"" if we point on the last byte of the the buffer.;;;","03/May/11 13:57;jbellis;bq. we can just change ""current"" if we point on the last byte of the the buffer

Meaning you suggest an additional code change?;;;","03/May/11 14:01;xedin;No, I mean that the patch is correct, sorry :);;;","03/May/11 15:50;jbellis;committed goffinet's patch with the additional parentheses for clarity;;;","03/May/11 16:04;hudson;Integrated in Cassandra-0.7 #466 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/466/])
    fix excessively pessimistic rebuffering in BRAF writes
patch by goffinet; reviewed by jbellis and Pavel Yaskevich for CASSANDRA-2581
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress will not use existing keyspaces,CASSANDRA-2580,12505741,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,brandon.williams,brandon.williams,29/Apr/11 01:43,16/Apr/19 09:33,14/Jul/23 05:52,29/Apr/11 19:44,0.8.0 beta 2,,,Legacy/Tools,,,0,,,,"cassandra-3:/srv/cassandra# tools/stress/bin/stress -n 1 -d cassandra-2 -i 1 -o insert
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
1,1,1,0.049,0
cassandra-3:/srv/cassandra# tools/stress/bin/stress -n 1 -d cassandra-2 -i 1 -o insert
Unable to create stress keyspace: Keyspace already exists.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/11 11:08;xedin;CASSANDRA-2580.patch;https://issues.apache.org/jira/secure/attachment/12477746/CASSANDRA-2580.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20710,,,Mon May 02 19:54:50 UTC 2011,,,,,,,,,,"0|i0gc7r:",93409,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"29/Apr/11 11:08;xedin;System.exit(1); was added by Gary Dusbabek at 2011-04-15 12:48:15 +0000, only cassandra-0.8 and trunk are affected this patch applies for both of them.;;;","29/Apr/11 19:44;brandon.williams;Looks like Jonathan actually added this on the 14th (Gary merged it to trunk), probably errantly while debugging something else.  Committed.;;;","02/May/11 19:54;hudson;Integrated in Cassandra-0.8 #58 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/58/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
apache-cassandra-cql-*.jar as a separate artifact,CASSANDRA-2579,12505738,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,28/Apr/11 23:52,16/Apr/19 09:33,14/Jul/23 05:52,06/May/11 15:14,0.8.0 beta 2,,,Legacy/Tools,Packaging,,0,cql,,,"The CQL jar should not be stored in the -bin.tar.gz artifact, but created separately with its own checksum files.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/11 23:53;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2579-apache-cassandra-cql-.jar-as-a-separate.txt;https://issues.apache.org/jira/secure/attachment/12477714/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2579-apache-cassandra-cql-.jar-as-a-separate.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19342,,,Fri May 06 15:14:50 UTC 2011,,,,,,,,,,"0|i0gc7j:",93408,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"02/May/11 15:23;tjake;WFM +1;;;","02/May/11 19:54;hudson;Integrated in Cassandra-0.8 #58 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/58/])
    ;;;","06/May/11 15:14;jbellis;(committed 2011-05-02);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress performance is artificially limited,CASSANDRA-2578,12505731,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,brandon.williams,brandon.williams,28/Apr/11 22:09,16/Apr/19 09:33,14/Jul/23 05:52,29/Apr/11 01:41,0.7.6,,,Legacy/Tools,,,0,,,,"With stress I only get about 7k inserts/s against a single server, and the load and cpu usage from stress is higher than the server.  Pystress gets 15-20k inserts/s against the same machine.  Stress isn't cpu-limited however, so there must be something else holding it back.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/11 00:08;xedin;CASSANDRA-2578-0.7.patch;https://issues.apache.org/jira/secure/attachment/12477717/CASSANDRA-2578-0.7.patch","29/Apr/11 00:01;xedin;CASSANDRA-2578.patch;https://issues.apache.org/jira/secure/attachment/12477715/CASSANDRA-2578.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20709,,,Fri Apr 29 01:55:45 UTC 2011,,,,,,,,,,"0|i0gc7b:",93407,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"29/Apr/11 00:01;xedin;caching generateValues in Inserter/IndexedRangeSlicer. Patch against trunk but can be applied to cassandra-0.8 without any problem.;;;","29/Apr/11 01:41;brandon.williams;Great! Committed.;;;","29/Apr/11 01:55;hudson;Integrated in Cassandra-0.7 #461 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/461/])
    cache generateValues in Inserter/IndexedRangeSlicer.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2578
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make forceUserDefinedCompaction actually do what it says,CASSANDRA-2575,12505353,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,27/Apr/11 21:07,16/Apr/19 09:33,14/Jul/23 05:52,28/Apr/11 16:16,0.8.0 beta 2,,,,,,0,,,,See http://www.mail-archive.com/user@cassandra.apache.org/msg12621.html for motivation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/11 15:39;jbellis;2575-v2.txt;https://issues.apache.org/jira/secure/attachment/12477663/2575-v2.txt","28/Apr/11 03:43;jbellis;2575.txt;https://issues.apache.org/jira/secure/attachment/12477607/2575.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20708,,,Mon May 02 19:54:50 UTC 2011,,,,,,,,,,"0|i0gc6n:",93404,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"27/Apr/11 21:17;jbellis;patch splits out doCompactionWithoutSizeEstimates and calls that from submitUserDefined.;;;","28/Apr/11 02:31;stuhood;The patch for 2552 is attached.;;;","28/Apr/11 03:43;jbellis;corrected patch attached.;;;","28/Apr/11 10:32;slebresne;I suppose we could backport to 0.7 too.
Also wondering if it couldn't make sense to not compact if we end up with only 1 sstable in doCompaction, now that you still can do a userDefinedCompaction on only 1 sstable. Not being able to compact the 2 smallest sstables seems pathological enough and it would be nice to avoid the 'same sstable compacted forever' problem seen in the mail thread above.

Anyway, +1 on the patch. ;;;","28/Apr/11 15:39;jbellis;bq. Also wondering if it couldn't make sense to not compact if we end up with only 1 sstable in doCompaction, now that you still can do a userDefinedCompaction on only 1 sstable

Good idea. v2 attached with a rewrite of the retry loop to do this.;;;","28/Apr/11 15:47;slebresne;+1 on v2;;;","28/Apr/11 16:16;jbellis;committed;;;","02/May/11 19:54;hudson;Integrated in Cassandra-0.8 #58 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/58/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra_cli: CREATE CF HELP should list option as key_cache_save_period instead of keys_cached_save_period,CASSANDRA-2572,12505258,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,27/Apr/11 00:36,16/Apr/19 09:33,14/Jul/23 05:52,27/Apr/11 15:33,0.8.0 beta 2,,,Legacy/Tools,,,0,,,,"*cassandra-cli: output from create cf command*
{noformat}
[default@cqldb]  create column family supa_dupa2 with keys_cached_save_period = 124000;

No enum const class org.apache.cassandra.cli.CliClient$ColumnFamilyArgument.KEYS_CACHED_SAVE_PERIOD
{noformat}

*cassandra-cli: help create column family*
{noformat}
- keys_cached_save_period: Duration in seconds after which Cassandra should
  safe the keys cache. Caches are saved to saved_caches_directory as
  specified in conf/Cassandra.yaml. Default is 14400 or 4 hours.

  Saved caches greatly improve cold-start speeds, and is relatively cheap in
  terms of I/O for the key cache. Row cache saving is much more expensive and
  has limited use.
{noformat}

*cqlsh: documentation for create column family options*
{noformat}
key_cache_save_period_in_seconds	14400	Number of seconds between saving key caches.
{noformat}

*cqlsh: this actually works*
{noformat}
cqlsh>  CREATE COLUMNFAMILY cf1 (KEY varchar PRIMARY KEY) WITH key_cache_save_period_in_seconds=10000;
{noformat}

*cassandra-cli: CF definition via show keyspace*
{noformat}
    ColumnFamily: cf1
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/10000
      Memtable thresholds: 0.140625/30/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
{noformat}
",,dw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/11 02:31;dw;CASSANDRA-0.7-2572.patch;https://issues.apache.org/jira/secure/attachment/12477471/CASSANDRA-0.7-2572.patch","27/Apr/11 10:17;xedin;CASSANDRA-2572.patch;https://issues.apache.org/jira/secure/attachment/12477492/CASSANDRA-2572.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20705,,,Wed Apr 27 17:18:12 UTC 2011,,,,,,,,,,"0|i0gc5z:",93401,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Apr/11 01:34;jbellis;may also be a bug in 0.7;;;","27/Apr/11 01:53;dw;I'm seeing the following in org.apache.cassandra.cli.CliClient:

 protected enum ColumnFamilyArgument
    {
    ....
    KEY_CACHE_SAVE_PERIOD,
    ....
    }

Instead of executing:

create column family supa_dupa2 with keys_cached_save_period = 124000;

one executes:

create column family supa_dupa2 with key_cache_save_period = 124000;

The second statement successfully executes.;;;","27/Apr/11 03:33;cdaw;Thanks David.  Will make this a bug about help.;;;","27/Apr/11 10:17;xedin;changed doc for 0.8 (can be applied on both cassandra-0.8 and trunk without any problems), 0.7 is not affected by this bug.;;;","27/Apr/11 15:33;jbellis;committed;;;","27/Apr/11 17:18;hudson;Integrated in Cassandra-0.8 #46 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/46/])
    fix cli help typo
patch by Pavel Yaskevich for CASSANDRA-2572
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check for null super column for SC CF in ThriftValidation (and always validate the sc key),CASSANDRA-2571,12505255,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,mbulman,mbulman,26/Apr/11 22:53,16/Apr/19 09:33,14/Jul/23 05:52,29/Apr/11 17:11,0.7.6,0.8.0 beta 2,,,,,0,,,,"Run the following via cli:
{noformat}
[default@test] use test;
Authenticated to keyspace: test
[default@test] create column family super with column_type=Super and default_validation_class=CounterColumnType;
d41df8e0-7055-11e0-0000-242d50cf1fbf
Waiting for schema agreement...
... schemas agree across the cluster
[default@test] incr super['0']['0'];
Value incremented.
[default@test] incr super['0']['0']['0'];
null
{noformat}

Obviously the first incr call is invalid, even though it reports otherwise, as well as generates this exception:
{noformat}
ERROR 17:38:05,871 Fatal exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.cassandra.db.CounterColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.ClassCastException: org.apache.cassandra.db.CounterColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:353)
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:337)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:88)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:74)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.serialize(RowMutation.java:353)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:236)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:480)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more
{noformat}

But the second, proper incr call results in a bunch of exceptions and not a real increment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/11 16:50;slebresne;0001-Improve-ThriftValidation.patch;https://issues.apache.org/jira/secure/attachment/12477563/0001-Improve-ThriftValidation.patch","28/Apr/11 19:26;jbellis;2571.txt;https://issues.apache.org/jira/secure/attachment/12477683/2571.txt",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20704,,,Mon May 02 19:54:50 UTC 2011,,,,,,,,,,"0|i0gc5r:",93400,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"27/Apr/11 16:50;slebresne;Turns out this is not a counter related bug. We just don't check when doing a (single) insert (and thus a add) on a super CF that the super column is not null.

Attached patch add a system test and fix it. It also fix another hole in ThriftValidation where the sc key was not validated. The patch is against 0.7 because it's not 0.8 specific.

Note that in 0.8 the cli ""hides"" this error for non counter column, but it is still not counter specific. The fact that the cluster is then ""in a bad state"", is because since we don't refuse the insert, some insert with a 'null' key is inserted in the column family map and thus messed up following insert (I think at least, haven't check super closely).;;;","28/Apr/11 19:26;jbellis;I find boolean (or other :) flags that change method behavior subtly confusing.  Attached is a version that inlines the new check into insert(), which is the only place that wants it.;;;","29/Apr/11 16:27;hudson;Integrated in Cassandra-0.7 #463 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/463/])
    Reject queries with missing mandatory super column and always validate super column name
patch by jbellis and slebresne for CASSANDRA-2571
;;;","29/Apr/11 17:11;slebresne;Committed;;;","29/Apr/11 17:36;hudson;Integrated in Cassandra #872 (See [https://builds.apache.org/hudson/job/Cassandra/872/])
    merge CASSANDRA-2571 from 0.8
;;;","02/May/11 19:54;hudson;Integrated in Cassandra-0.8 #58 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/58/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: incorrect error message running truncate on CF that does not exist,CASSANDRA-2570,12505248,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,26/Apr/11 21:23,16/Apr/19 09:33,14/Jul/23 05:52,27/Apr/11 15:18,0.8.0 beta 2,,,Legacy/CQL,,,0,cql,,,"Run truncate on a CF that does not exist. The error message is misleading.

*CQLSH*
{code}
cqlsh> truncate aaaa;
Unable to complete request: one or more nodes were unavailable.
{code}

*cassandra-cli*
{code}
[default@cqldb] truncate aaaaaaaaa;
aaaaaaaaa not found in current keyspace.
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/11 14:46;xedin;CASSANDRA-2570-v2-0.8.patch;https://issues.apache.org/jira/secure/attachment/12477548/CASSANDRA-2570-v2-0.8.patch","27/Apr/11 15:03;xedin;CASSANDRA-2570-v2-trunk.patch;https://issues.apache.org/jira/secure/attachment/12477551/CASSANDRA-2570-v2-trunk.patch","27/Apr/11 10:47;xedin;CASSANDRA-2570.patch;https://issues.apache.org/jira/secure/attachment/12477494/CASSANDRA-2570.patch",,,,,,,,,,,,3.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20703,,,Wed Apr 27 17:18:12 UTC 2011,,,,,,,,,,"0|i0gc5j:",93399,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Apr/11 10:47;xedin;Working branch: trunk. QP now checks if the given CF exists in the current KS before executing ""truncate"" operation.;;;","27/Apr/11 14:46;xedin;patch for version 0.8 instead of trunk + test for truncate validation.;;;","27/Apr/11 15:18;jbellis;committed to 0.8 and trunk w/ some changes:

 - test uses assert_raises, combined w/ existing test_truncate method
 - server uses validateColumnFamily to test existance;;;","27/Apr/11 17:18;hudson;Integrated in Cassandra-0.8 #46 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/46/])
    validate cql TRUNCATE columnfamily before truncating
patch by Pavel Yaskevich and jbellis for CASSANDRA-2570
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: DELETE documentation uses UPDATE examples,CASSANDRA-2567,12505242,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,cdaw,cdaw,26/Apr/11 20:15,16/Apr/19 09:33,14/Jul/23 05:52,26/Apr/11 21:26,0.8.0 beta 2,,,,,,0,cql,,,"
{panel}
h2. DELETE

_Synopsis:_

bc. 
DELETE [COLUMNS] FROM <COLUMN FAMILY> [USING <CONSISTENCY>] WHERE KEY = keyname1
DELETE [COLUMNS] FROM <COLUMN FAMILY> [USING <CONSISTENCY>] WHERE KEY IN (keyname1, keyname2);

A @DELETE@ is used to perform the removal of one or more columns from one or more rows.

h3. Specifying Columns

bc. 
DELETE [COLUMNS] ...

Following the @DELETE@ keyword is an optional comma-delimited list of column name terms. When no column names are specified, the remove applies to the entire row(s) matched by the ""WHERE clause"":#deleterows

h3. Column Family

bc. 
DELETE ... FROM <COLUMN FAMILY> ...

The column family name follows the list of column names.

h3. Consistency Level

bc. 
UPDATE ... [USING <CONSISTENCY>] ...

Following the column family identifier is an optional ""consistency level specification"":#consistency.

h3(#deleterows). Specifying Rows

bc. 
UPDATE ... WHERE KEY = keyname1
UPDATE ... WHERE KEY IN (keyname1, keyname2)

The @WHERE@ clause is used to determine which row(s) a @DELETE@ applies to.  The first form allows the specification of a single keyname using the @KEY@ keyword and the @=@ operator.  The second form allows a list of keyname terms to be specified using the @IN@ notation and a parenthesized list of comma-delimited keyname terms.
     
{panel}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20701,,,Tue Apr 26 21:26:32 UTC 2011,,,,,,,,,,"0|i0gc4v:",93396,,,,,Low,,,,,,,,,,,,,,,,,"26/Apr/11 21:26;jbellis;fixed in r1096915;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: Batch Updates: some consistency levels not working,CASSANDRA-2566,12505240,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,26/Apr/11 20:00,16/Apr/19 09:33,14/Jul/23 05:52,16/May/11 22:01,0.8.0,,,Legacy/CQL,,,0,cql,,,"Testing the batch updates, and running into some issues with different consistency levels

+*Summary*+
* UNTESTED: CONSISTENCY ANY
* PASS: CONSISTENCY  ONE
* PASS: CONSISTENCY  QUORUM
* PASS: CONSISTENCY  ALL
* CQL ERROR: CONSISTENCY  LOCAL_QUORUM
* CQL ERROR: CONSISTENCY  EACH_QUORUM

 
+*Test Setup*+
{code}
CREATE KEYSPACE cqldb with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy'  
and strategy_options:replication_factor=1;

use cqldb;

CREATE COLUMNFAMILY users (KEY varchar PRIMARY KEY, password varchar, gender varchar, 
session_token varchar, state varchar, birth_year bigint);

INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3', 'f', 'CA', '1971');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user2', 'ch@ngem3', 'f', 'CA', '1972');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user3', 'ch@ngem3', 'f', 'CA', '1973');
{code}


+*Bug Details*+

*CONSISTENCY LOCAL_QUORUM*
{code}
BEGIN BATCH USING CONSISTENCY  LOCAL_QUORUM
UPDATE users SET state = 'UT' WHERE KEY = 'user1';
UPDATE users SET state = 'UT' WHERE KEY = 'user2';
UPDATE users SET state = 'UT' WHERE KEY = 'user3';
APPLY BATCH

cqlsh>  Bad Request: line 1:31 mismatched input 'LOCAL_QUORUM' expecting K_LEVEL
{code}

*CONSISTENCY EACH_QUORUM*
{code}
BEGIN BATCH USING CONSISTENCY  EACH_QUORUM
UPDATE users SET state = 'TX' WHERE KEY = 'user1';
UPDATE users SET state = 'TX' WHERE KEY = 'user2';
UPDATE users SET state = 'TX' WHERE KEY = 'user3';
APPLY BATCH

cqlsh> Bad Request: line 1:31 mismatched input 'EACH_QUORUM' expecting K_LEVEL
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/11 21:56;xedin;CASSANDRA-2566.patch;https://issues.apache.org/jira/secure/attachment/12479391/CASSANDRA-2566.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20700,,,Mon May 16 22:30:27 UTC 2011,,,,,,,,,,"0|i0gc4n:",93395,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/May/11 20:58;jbellis;Updated CQL CL documentation and edited issue description.
;;;","16/May/11 21:56;xedin;Added missing consistency levels to CQL grammar.;;;","16/May/11 22:01;jbellis;committed (also removed obsolete DCQUORUM* values);;;","16/May/11 22:30;hudson;Integrated in Cassandra-0.8 #111 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/111/])
    update cql consistency levels
patch by pyaskevich; reviewed by jbellis for CASSANDRA-2566
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodes are not reported as alive after being marked down,CASSANDRA-2565,12505235,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,26/Apr/11 19:36,16/Apr/19 09:33,14/Jul/23 05:52,26/Apr/11 22:12,0.8.0 beta 2,,,,,,0,,,,"To reproduce: start two nodes in foreground mode, then suspend (^Z) one of them.  Once it is marked down, resume the process (fg) and it will not be marked up again.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/11 19:48;brandon.williams;2565.txt;https://issues.apache.org/jira/secure/attachment/12477435/2565.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20699,,,Tue Apr 26 22:56:43 UTC 2011,,,,,,,,,,"0|i0gc4f:",93394,,,,,Normal,,,,,,,,,,,,,,,,,"26/Apr/11 19:38;brandon.williams;Looks like in the Big Gossip Refactor there was a slight regression.  We avoid notifying if there's not a major state change, however we also need to see if the node was previously dead and if so, notify.;;;","26/Apr/11 21:32;jbellis;+1;;;","26/Apr/11 22:12;brandon.williams;Committed.;;;","26/Apr/11 22:56;hudson;Integrated in Cassandra-0.8 #43 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/43/])
    Mark nodes that were previously down as alive, even without a major
state change.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-2565
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error starting up a cassandra cluster after creating a table in the system keyspace: Attempt to assign id to existing column family.,CASSANDRA-2563,12505230,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,cdaw,cdaw,26/Apr/11 18:50,16/Apr/19 09:33,14/Jul/23 05:52,27/Apr/11 15:55,0.8.0 beta 2,,,,,,0,,,,"*Repro Steps*
* rm -rf /var/lib/cassandra/*
* rm -rf /var/log/cassandra/*
* Start Cassandra
* In cqlsh, create a column family and insert data
{noformat}
cqlsh> CREATE COLUMNFAMILY users (
   ...   KEY varchar PRIMARY KEY,
   ...   password varchar,
   ...   gender varchar,
   ...   session_token varchar,
   ...   state varchar,
   ...   birth_year bigint);

cqlsh> INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3', 'f', 'CA', '1971');
cqlsh> INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user2', 'ch@ngem3', 'f', 'CA', '1972');
cqlsh> INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user3', 'ch@ngem3', 'f', 'CA', '1973');
{noformat}

* Quit cqlsh
* Kill Cassandra
* Startup Cassandra and get error

{noformat}
 INFO 18:38:24,509 Loading schema version 087af100-7034-11e0-0000-242d50cf1fde
ERROR 18:38:24,774 Exception encountered during startup.
java.io.IOError: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:489)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:138)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:313)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.CFMetaData.map(CFMetaData.java:126)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:485)
	... 3 more
Exception encountered during startup.
java.io.IOError: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:489)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:138)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:313)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.CFMetaData.map(CFMetaData.java:126)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:485)
	... 3 more
{noformat}



*UPDATE:  This issue happens if I create the CF in the default keyspace.*

*Workaround*
{noformat}
cqlsh> CREATE KEYSPACE cqldb with 
   ...   strategy_class =  
   ...     'org.apache.cassandra.locator.SimpleStrategy' 
   ...   and strategy_options:replication_factor=1;
cqlsh> use cqldb;

The create the table and insert data.
{noformat}
","Branch: cassandra-0.8; git pull @ 11:30amPST on 4/26
Server: RHEL5.5 single node",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/11 21:31;jbellis;2563.txt;https://issues.apache.org/jira/secure/attachment/12477442/2563.txt","26/Apr/11 19:01;cdaw;cassandra.2563.tar;https://issues.apache.org/jira/secure/attachment/12477431/cassandra.2563.tar",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20698,,,Wed Apr 27 17:18:12 UTC 2011,,,,,,,,,,"0|i0gc3z:",93392,,cdaw,,cdaw,Critical,,,,,,,,,,,,,,,,,"26/Apr/11 19:01;cdaw;The /var/lib/cassandra directory tarball.;;;","26/Apr/11 19:06;cdaw;I was able to reproduce this again ...  cleared out /var/lib/cassandra/* and then started up server and repeated steps.;;;","26/Apr/11 19:49;jbellis;is default keyspace = system?;;;","26/Apr/11 19:56;cdaw;The CF was created in the system keyspace:

{noformat}
Keyspace: system:
  Replication Strategy: org.apache.cassandra.locator.LocalStrategy
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: users
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.140625/30/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
      Column Metadata:
        Column Name: session_token
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
        Column Name: state
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
        Column Name: birth_year
          Validation Class: org.apache.cassandra.db.marshal.LongType
        Column Name: password
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
        Column Name: gender
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
{noformat}
;;;","26/Apr/11 21:31;jbellis;patch to disallow screwing w/ system keyspace;;;","26/Apr/11 21:59;cdaw;The patch works good and gives this error when creating a new column family:
Bad Request: system keyspace is not user-modifiable

The only thing to think about is whether or not this fix limits the ability of support to fix corruption issues.
;;;","27/Apr/11 15:55;jbellis;committed.

note that this only disallows schema changes; modifying system *data* is still allowed (and still dangerous :);;;","27/Apr/11 17:18;hudson;Integrated in Cassandra-0.8 #46 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/46/])
    disallow making schema changes to system keyspace
patch by jbellis; tested by cdaw for CASSANDRA-2563
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parent POM does not get deployed to the maven repository,CASSANDRA-2562,12505194,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stephenc,stephenc,stephenc,26/Apr/11 13:48,16/Apr/19 09:33,14/Jul/23 05:52,29/Apr/11 17:28,0.7.6,,,,,,0,,,,"The parent pom does not get deployed to the Maven Central

(for 0.7.5 I am fixing this by hand)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/11 13:53;stephenc;CASSANDRA-2562.patch;https://issues.apache.org/jira/secure/attachment/12477413/CASSANDRA-2562.patch","26/Apr/11 13:49;stephenc;CASSANDRA-2562.patch;https://issues.apache.org/jira/secure/attachment/12477412/CASSANDRA-2562.patch",,,,,,,,,,,,,2.0,stephenc,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20697,,,Mon May 02 19:54:50 UTC 2011,,,,,,,,,,"0|i0gc3r:",93391,,,,,Normal,,,,,,,,,,,,,,,,,"26/Apr/11 13:49;stephenc;This patch is for post 0.7.5 unless there is a take#2 of 0.7.5;;;","26/Apr/11 13:53;stephenc;updated patch as I missed one line;;;","29/Apr/11 17:28;slebresne;Committed;;;","29/Apr/11 17:44;hudson;Integrated in Cassandra-0.7 #464 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/464/])
    Deploy parent POM to maven central
patch by stephenc for CASSANDRA-2562
;;;","29/Apr/11 18:30;hudson;Integrated in Cassandra #873 (See [https://builds.apache.org/hudson/job/Cassandra/873/])
    merge records of CASSANDRA-2562 from 0.8
;;;","02/May/11 19:54;hudson;Integrated in Cassandra-0.8 #58 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/58/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy sends same message multiple times,CASSANDRA-2557,12505158,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,skamio,skamio,26/Apr/11 06:45,16/Apr/19 09:33,14/Jul/23 05:52,26/Apr/11 07:00,0.8.0 beta 2,,,,,,0,,,,"A cassandra node gets multiple mutation messages (in number of times of replication factor at maximum) for an insert. It may cause high load on the node. The mutation should be only once for each insert.

This bug is visible via MutationStage count in nodetool tpstats.
For instance, if you have 6 node cluster (initial keys are 31, 32, 33, 34, 35 and 36) with replication factor = 4 and a single data (for example, key='2') is inserted, MutationStage count will be as follows:

node 1: MutationStage 0 0 4
node 2: MutationStage 0 0 3
node 3: MutationStage 0 0 2
node 4: MutationStage 0 0 1
node 5: MutationStage 0 0 0
node 6: MutationStage 0 0 0

As you can see, the counts are different in each node.
",linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/11 06:48;skamio;StorageProxy.java.patch;https://issues.apache.org/jira/secure/attachment/12477367/StorageProxy.java.patch",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20696,,,Tue Apr 26 15:22:04 UTC 2011,,,,,,,,,,"0|i0gc2n:",93386,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"26/Apr/11 06:48;skamio;The problem occurs because the sendMessages() is within for-loop of StorageProxy. A patch to fix that is attached.;;;","26/Apr/11 07:00;slebresne;Good catch. Committed, thanks!;;;","26/Apr/11 07:29;hudson;Integrated in Cassandra-0.8 #40 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/40/])
    Fix sending mutation messages multiple times
patch by skamio; reviewed by slebresne for CASSANDRA-2557
;;;","26/Apr/11 15:22;hudson;Integrated in Cassandra #865 (See [https://builds.apache.org/hudson/job/Cassandra/865/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DatacenterReadResolver not triggering repair,CASSANDRA-2556,12505139,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,stuhood,stuhood,25/Apr/11 21:39,16/Apr/19 09:33,14/Jul/23 05:52,28/Apr/11 13:37,0.7.6,0.8.0 beta 2,,,,,0,,,,DatacenterReadResolver only calls maybeResolveForRepair for local reads.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/11 20:46;jbellis;2556.txt;https://issues.apache.org/jira/secure/attachment/12477580/2556.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20695,,,Thu May 12 14:42:59 UTC 2011,,,,,,,,,,"0|i0gc2f:",93385,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"27/Apr/11 20:46;jbellis;patch that extracts the ""is this message one that should count towards blockfor"" logic into the waitingFor method; DatacenterReadCallback now only overrides those instead of all of the response methods, which fixes the bug and reduces the surface for introducing similar bugs in the future.  (applies on top of CASSANDRA-2552 v2.);;;","28/Apr/11 11:50;slebresne;+1
The patch applies on top of 0.8 (that is on top of CASSANDRA-2552 v2 for 0.8). Should be backported to 0.7 first I think.;;;","28/Apr/11 13:37;jbellis;committed;;;","28/Apr/11 13:59;hudson;Integrated in Cassandra-0.7 #460 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/460/])
    trigger read repair correctly forLOCAL_QUORUM reads
patch by jbellis; reviewed by slebresne for CASSANDRA-2556
;;;","12/May/11 14:42;jbellis;This also fixed a second bug: the old code checked for n == blockFor on LOCAL_QUORUM, so there was a race where if we exceeded blockFor with two responses coming in at nearly the same time, it would never test at the moment of equality and the coordinator would throw TOE even though enough responses were actually received.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move gossip heartbeats [back] to its own thread,CASSANDRA-2554,12505126,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,25/Apr/11 17:20,16/Apr/19 09:33,14/Jul/23 05:52,26/Apr/11 17:11,0.7.6,,,,,,0,,,,"Gossip heartbeat *really* needs to run every 1s or other nodes may mark us down. But gossip currently shares an executor thread with other tasks.

I see at least two of these could cause blocking: hint cleanup post-delivery and flush-expired-memtables, both of which call forceFlush which will block if the flush queue + threads are full.

We've run into this before (CASSANDRA-2253); we should move Gossip back to its own dedicated executor or it will keep happening whenever someone accidentally puts something on the ""shared"" executor that can block.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/11 17:30;jbellis;2554-0.7.txt;https://issues.apache.org/jira/secure/attachment/12477316/2554-0.7.txt","25/Apr/11 17:40;jbellis;2554-0.8.txt;https://issues.apache.org/jira/secure/attachment/12477317/2554-0.8.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20694,,,Tue Apr 26 17:36:04 UTC 2011,,,,,,,,,,"0|i0gc1z:",93383,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"25/Apr/11 17:40;jbellis;patches against 0.7 and 0.8 to move Gossip to its own executor, and move hint deletion + flush expired memtables + cache saving to the long-execution-time executor.;;;","26/Apr/11 11:24;slebresne;+1;;;","26/Apr/11 17:11;jbellis;committed;;;","26/Apr/11 17:36;hudson;Integrated in Cassandra-0.7 #458 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/458/])
    movegossip heartbeat back to its own thread
patch by jbellis; reviewed by slebresne for CASSANDRA-2554
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadResponseResolver Race,CASSANDRA-2552,12505091,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,stuhood,stuhood,25/Apr/11 01:21,16/Apr/19 09:33,14/Jul/23 05:52,28/Apr/11 13:26,0.7.6,0.8.0 beta 2,,,,,0,,,,"When receiving a response, ReadResponseResolver uses a 3 step process to decide whether to trigger the condition that enough responses have arrived:
# Add new response
# Check response set size
# Check that data is present

I think that these steps must have been reordered by the compiler in some cases, because I was able to reproduce a case for a QUORUM read where the condition is not properly triggered:
{noformat}
INFO [RequestResponseStage:15] 2011-04-25 00:26:53,514 ReadResponseResolver.java (line 87) post append for 1087367065: hasData=false in 2 messages
INFO [RequestResponseStage:8] 2011-04-25 00:26:53,514 ReadResponseResolver.java (line 87) post append for 1087367065: hasData=true in 1 messages
INFO [pool-1-thread-54] 2011-04-25 00:27:03,516 StorageProxy.java (line 623) Read timeout: java.util.concurrent.TimeoutException: ReadResponseResolver@1087367065(/10.34.131.109=false,/10.34.132.122=true,)
{noformat}
The last line shows that both results were present, and that one of them was holding data.",,johanoskarsson,skamio,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/11 07:36;stuhood;0001-Move-Resolvers-to-atomic-append-count.txt;https://issues.apache.org/jira/secure/attachment/12477369/0001-Move-Resolvers-to-atomic-append-count.txt","27/Apr/11 20:24;jbellis;2552-v2-07.txt;https://issues.apache.org/jira/secure/attachment/12477577/2552-v2-07.txt","27/Apr/11 20:18;jbellis;2552-v2.txt;https://issues.apache.org/jira/secure/attachment/12477576/2552-v2.txt","26/Apr/11 06:08;stuhood;ResolveRaceTest.java;https://issues.apache.org/jira/secure/attachment/12477366/ResolveRaceTest.java",,,,,,,,,,,4.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20693,,,Thu Apr 28 13:44:17 UTC 2011,,,,,,,,,,"0|i0gc1j:",93381,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"25/Apr/11 02:06;stuhood;I have a patch ready that I believe fixes this: testing it out before posting.;;;","25/Apr/11 13:25;jbellis;Hard to tell exactly what's going on here w/o knowing where your logging was added.

In particular it's important to note that we don't prevent responses from being processed after we've already given up and decided to call a timeout (but before we've torn down the request callback).;;;","26/Apr/11 06:08;stuhood;Here is a cut down testcase that reproduces the race: it looks like two threads can race on step 2 such that neither accounts for the item added by the other, and both think the set of responses is too small.

I have a patch that makes append + size an atomic operation: I'll post it as soon as I clean it up a bit.;;;","26/Apr/11 07:10;stuhood;Chris pointed out that the example passes if you replace NBHM with CHM, but I don't think NBHM is necessarily to blame here: each thread views a locally consistent copy, likely due to Cliff's use of sun.misc.Unsafe references.

It's possible that a similar race applies to RangeSliceResponseResolver, but I think changes to LBQ (like CHM) will be broadcast to all threads.;;;","26/Apr/11 07:36;stuhood;Attaching a patch that replaces NBHM with an AtomicReferenceArray that is appended to and counted atomically. This patch eliminated the timeouts we were seeing.

CHM may also be a legitimate solution, but it feels a bit like an abuse of a map.;;;","26/Apr/11 17:21;jbellis;That sure sounds like a NBHM bug to me. The javadoc says,

bq. Retrievals reflect the results of the most recently completed update operations holding upon their onset... Similarly, Iterators and Enumerations return elements reflecting the state of the hash table at some point at or since the creation of the iterator/enumeration.

I.e., for at least one thread, *both* update operations will have completed when the iterator is created, so it should see all the entries.

(Will review the actual patch shortly, I'm just saying I think we should report a bug too.);;;","26/Apr/11 22:39;stuhood;> I.e., for at least one thread, both update operations will have completed when the iterator is created
Note that the race I observed via the debug output for that test was actually on the size() operation, which doesn't put any such guarantees in its javadocs.;;;","26/Apr/11 23:46;jbellis;You're right: size() is implemented as a org.cliffc.high_scale_lib.Counter object, which says

{code}
  // Add the given value to current counter value.  Concurrent updates will
  // not be lost, but addAndGet or getAndAdd are not implemented because but
  // the total counter value is not atomically updated.
  //public void add( long x );

...

  // Current value of the counter.  Since other threads are updating furiously
  // the value is only approximate, but it includes all counts made by the
  // current thread.  Requires a pass over all the striped counters.
  //public long get();
{code};;;","27/Apr/11 09:38;slebresne;I am no expert of the Java Memory Model, but I can't find anything that preclude this behavior in the CHM docs either (there really is not much on the size function). So I would have liked the CHM solution if we could be sure it always fix that problem (I would have liked it because it was a one line change and I think maps are here to be ""abused""), but as far as I can tell, it may well only make the bug much less frequent or fix it only on some architecture (the code of CHM seems to indicate it is safe but it's complicated enough that I wouldn't bet my life on it).

Note that if that's true, LBQ too could well allow for a race here without breaking it's specification (it seems to use a AtomicInteger for the size internally so it is trivially ok, but if the spec doesn't force anything, I suppose that could change).

So I suppose if we want to do right by the spec, we should probably update both AbstractRowResolver and RangeSliceResponseResolver (note that using an AtomicInteger to count the number of responses could be slightly simpler, but I'm fine with an AtomicReferenceArray). ;;;","27/Apr/11 15:38;jbellis;is there a reason RSRR can't inherit ARR or does it just predate that refactoring?;;;","27/Apr/11 16:07;jbellis;bq. is there a reason RSRR can't inherit ARR or does it just predate that refactoring?

To answer my own ARR assumes we're returning Rows, which would be easy to fix, and that Messages turn into ReadResponse objects, which would be harder since we'd need to have a <T extends ISerializable> interface where ISerializeable gave us a Serializer class declaring ""void serialize(T, outputstream) and T deserialize(inputstream)"", i.e., we start to get into fixing ICompactSerializer and all the mess that would be.;;;","27/Apr/11 16:29;jbellis;I'm not sure I'm a fan of the ARR solution.  Wouldn't it be similar complexity (one O(N) operation per message received) to keep NBHM and implement getMessageCount as an iterate-entries operation?  (the O(N) op in ARR is of course the search-for-free-slot in append.)

I'm -0 on changing RSRR away from LBQ when LBQ is known to work fine in practice.;;;","27/Apr/11 16:37;jbellis;bq. Wouldn't it be similar complexity (one O(N) operation per message received) to keep NBHM and implement getMessageCount as an iterate-entries operation?

We can actually do size-by-iteration for basically free (with a little refactoring), since we're already iterating for isDataPresent. We can just push the iteration into the callers who care about size-and-data-present and do it with one loop.

But if I am honest that is premature optimization.  We are already using the AtomicInteger approach in DatacenterReadCallback.  I'll submit a patch to standardize on that.
;;;","27/Apr/11 17:23;jbellis;v2 w/ AtomicInteger approach;;;","27/Apr/11 20:18;jbellis;updated v2 fixes AsyncRepairCallback and RepairCallback as well;;;","27/Apr/11 20:24;jbellis;and a v2 for 0.7;;;","27/Apr/11 20:45;stuhood;Although I didn't reproduce a race between size() and isDataPresent(), isn't that one still possible? IMO, two operations that are atomic independently shouldn't be trusted to compose. The point of the ARR wasn't to improve runtime, it was simply to make all three steps atomic.;;;","27/Apr/11 20:53;jbellis;the correctness criterion is that once the messages are received, at least one thread running response will see that both blockfor and data are satisfied; this meets that need.

note that received(-messages-that-count-towards-blockfor) is NOT the same as size (see: DRC) so you need a separate variable anyway even with ARR.;;;","28/Apr/11 08:45;stuhood;+1

I still think a Map is overkill here, but I can't reproduce a race with the v2 algorithm.;;;","28/Apr/11 13:26;jbellis;committed;;;","28/Apr/11 13:44;hudson;Integrated in Cassandra-0.7 #459 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/459/])
    fix incorrect use ofNBHM.size in ReadCallback
patch by jbellis; reviewed by stuhood for CASSANDRA-2552
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool setcompactionthroughput requiring wrong number of arguments?,CASSANDRA-2550,12505058,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,terjem,terjem,terjem,23/Apr/11 17:58,16/Apr/19 09:33,14/Jul/23 05:52,25/Apr/11 13:39,0.8.0 beta 2,,,Tool/nodetool,,,0,nodetool,,,"---
            case SETCOMPACTIONTHROUGHPUT :
                if (arguments.length != 2) { badUse(""Missing value argument.""); }
                probe.setCompactionThroughput(Integer.valueOf(arguments[1]));
                break;
---

I would think arguments.length should be just 1?

",,terjem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,terjem,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20692,,,Mon Apr 25 14:19:26 UTC 2011,,,,,,,,,,"0|i0gc13:",93379,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"25/Apr/11 13:39;jbellis;fixed in r1096479, thanks!;;;","25/Apr/11 14:19;hudson;Integrated in Cassandra-0.8 #37 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/37/])
    fix nodetool setcompactionthroughput
patch by Terje Marthinussen; reviewed by jbellis for CASSANDRA-2550
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Start up of 0.8-beta1 on Ubuntu,CASSANDRA-2549,12505024,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,selam,drewbroadley,drewbroadley,23/Apr/11 02:09,16/Apr/19 09:33,14/Jul/23 05:52,02/May/11 15:19,0.8.0 beta 2,,,Packaging,,,0,start,,,"root@home:/home/drew# cassandra -f
 INFO 14:06:03,261 Logging initialized
 INFO 14:06:03,323 Heap size: 1543831552/1543831552
 INFO 14:06:03,332 JNA not found. Native methods will be disabled.
 INFO 14:06:03,379 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 14:06:03,899 DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
ERROR 14:06:04,028 Exception encountered during startup.
java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/UnavailableException
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2444)
	at java.lang.Class.privateGetPublicMethods(Class.java:2564)
	at java.lang.Class.getMethods(Class.java:1427)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.initMaps(MBeanAnalyzer.java:126)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.<init>(MBeanAnalyzer.java:116)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.analyzer(MBeanAnalyzer.java:104)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.getAnalyzer(StandardMBeanIntrospector.java:66)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.getPerInterface(MBeanIntrospector.java:181)
	at com.sun.jmx.mbeanserver.MBeanSupport.<init>(MBeanSupport.java:136)
	at com.sun.jmx.mbeanserver.StandardMBeanSupport.<init>(StandardMBeanSupport.java:64)
	at com.sun.jmx.mbeanserver.Introspector.makeDynamicMBean(Introspector.java:174)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:330)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:516)
	at org.apache.cassandra.service.StorageService.<init>(StorageService.java:231)
	at org.apache.cassandra.service.StorageService.<clinit>(StorageService.java:171)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.<init>(DynamicEndpointSnitch.java:78)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:429)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:294)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:98)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.UnavailableException
	at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:266)
	... 23 more
Exception encountered during startup.
java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/UnavailableException
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2444)
	at java.lang.Class.privateGetPublicMethods(Class.java:2564)
	at java.lang.Class.getMethods(Class.java:1427)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.initMaps(MBeanAnalyzer.java:126)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.<init>(MBeanAnalyzer.java:116)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.analyzer(MBeanAnalyzer.java:104)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.getAnalyzer(StandardMBeanIntrospector.java:66)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.getPerInterface(MBeanIntrospector.java:181)
	at com.sun.jmx.mbeanserver.MBeanSupport.<init>(MBeanSupport.java:136)
	at com.sun.jmx.mbeanserver.StandardMBeanSupport.<init>(StandardMBeanSupport.java:64)
	at com.sun.jmx.mbeanserver.Introspector.makeDynamicMBean(Introspector.java:174)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:330)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:516)
	at org.apache.cassandra.service.StorageService.<init>(StorageService.java:231)
	at org.apache.cassandra.service.StorageService.<clinit>(StorageService.java:171)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.<init>(DynamicEndpointSnitch.java:78)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:429)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:294)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:98)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.UnavailableException
	at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:266)
	... 23 more
","Linux home.broadley.org.nz 2.6.32-29-generic-pae #58-Ubuntu SMP Fri Feb 11 19:15:25 UTC 2011 i686 GNU/Linux
",selam,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/11 20:07;selam;cassandra-0.8.0beta1-debian-package.patch;https://issues.apache.org/jira/secure/attachment/12477225/cassandra-0.8.0beta1-debian-package.patch","25/Apr/11 10:41;selam;cassandra_multiple_package_v2.patch;https://issues.apache.org/jira/secure/attachment/12477292/cassandra_multiple_package_v2.patch","26/Apr/11 15:27;selam;cassandra_multiple_package_v3.patch;https://issues.apache.org/jira/secure/attachment/12477420/cassandra_multiple_package_v3.patch",,,,,,,,,,,,3.0,selam,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20691,,,Mon May 02 19:54:49 UTC 2011,,,,,,,,,,"0|i0gc0v:",93378,,urandom,,urandom,Normal,,,,,,,,,,,,,,,,,"23/Apr/11 20:06;selam;deb package doesn't contain apache-cassandra-thrift-${VERSION}.jar and apache-cassandra-cql-*.jar 

My patch does not change package version or release number of deb package.;;;","24/Apr/11 00:51;prystupa;Timu, can your patch be applied to the currently latest binary 0.7.4 distribution?;;;","24/Apr/11 02:31;jbellis;No, there is only one jar in 0.7.;;;","24/Apr/11 16:32;urandom;The cql jar is not a dependency here.  If it needs to be packaged, it's something that would get its own package.;;;","25/Apr/11 01:26;jbellis;But from the stacktrace, the thrift jar is a dependency, no?;;;","25/Apr/11 02:36;urandom;Yes, the thrift jar is a dependency, the cql jar is not, the patch adds both.;;;","25/Apr/11 10:41;selam;my second patch generates multiple binary package.
this packages for: libthrift-java, cassandra, cassandra-thrift, cassandra-cql

cassandra and cassandra-cql depends cassandra-thrift.
cassandra-thrift depends libthrift-java

All package versions points to 0.8.0 but for Cql it must be 1.0.0 and for libthrift-java it must be 0.6. i working on for fix this, but i guess deb packaging system doesn't allowed to do this. another solution create separate debian directory for each package, but i guess this is not acceptable.

i build new packages from using this patch and i install 2 nodes without problem. 

patch name: cassandra_multiple_package_v2.patch;;;","26/Apr/11 06:44;drewbroadley;Thanks for the patch, this was enough to get things going again.

Would it be possible to create a separate 0.8 & 0.7 package as I wasn't wanting to jump to 0.8.;;;","26/Apr/11 15:27;selam;sorry,  i was remove some jars from cassandra package and added in dependencies, then after i remove dependencies but i forget to add package install dir. so i believe that patch is final patch. ;;;","26/Apr/11 18:08;urandom;I probably shouldn't have said ""If it needs to be packaged, it's something that would get its own package."", because I don't think it should (not yet at least, and not like this).

For one thing, the cql jar depends on more than the apache-cassandra-thrift jar; I'm not sure what your process was for testing, but it won't work like this.

Secondly, those dependencies (the ones between jars) will eventually be sorted, but then you're faced with the dependency on Thrift.  Our bundling of a private copy of Thrift is already something that brings shame to my ancestors, breaking that out into its own package is going to end in someone being reincarnated as a lemur, or a muskrat, or as the exhaust system of a 1997 Land Rover.  I'll have no part in it.;;;","26/Apr/11 23:32;urandom;+1(ish) to cassandra-0.8.0beta1-debian-package.patch;;;","02/May/11 15:19;jbellis;committed cassandra-0.8.0beta1-debian-package.patch;;;","02/May/11 19:54;hudson;Integrated in Cassandra-0.8 #58 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/58/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: cqlsh error running batch update commands,CASSANDRA-2545,12505019,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,cdaw,cdaw,23/Apr/11 00:55,16/Apr/19 09:33,14/Jul/23 05:52,27/Apr/11 02:18,,,,,,,0,,,,"*CQL Test Case*
{code}
//TEST CASE #1
BEGIN BATCH
UPDATE users SET gender = 'm', birth_year = '1981' WHERE KEY = 'user1';
UPDATE users SET gender = 'm', birth_year = '1982' WHERE KEY = 'user2';
UPDATE users SET gender = 'm', birth_year = '1983' WHERE KEY = 'user3';
APPLY BATCH	

//TEST CASE #2
BEGIN BATCH USING CONSISTENCY ZERO
UPDATE users SET state = 'TX' WHERE KEY = 'user1';
UPDATE users SET state = 'TX' WHERE KEY = 'user2';
UPDATE users SET state = 'TX' WHERE KEY = 'user3';
APPLY BATCH	


//ERROR
Bad Request: line 0:-1 mismatched input '<EOF>' expecting K_APPLY
{code}

*Test Setup*
{code}
CREATE COLUMNFAMILY users (
  KEY varchar PRIMARY KEY,
  password varchar,
  gender varchar,
  session_token varchar,
  state varchar,
  birth_year bigint);

INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3', 'f', 'CA', '1971');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user2', 'ch@ngem3', 'f', 'CA', '1972');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user3', 'ch@ngem3', 'f', 'CA', '1973');
{code}

*Documented Syntax*
{panel}
BEGIN BATCH [USING <CONSISTENCY>]
UPDATE CF1 SET name1 = value1, name2 = value2 WHERE KEY = keyname1;
UPDATE CF1 SET name3 = value3 WHERE KEY = keyname2;
UPDATE CF2 SET name4 = value4, name5 = value5 WHERE KEY = keyname3;
APPLY BATCH
{panel}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/11 02:47;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2545-also-consider-APPLY-BATCH-for-terminati.txt;https://issues.apache.org/jira/secure/attachment/12477184/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2545-also-consider-APPLY-BATCH-for-terminati.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20688,,,Wed Apr 27 03:41:23 UTC 2011,,,,,,,,,,"0|i0gbzz:",93374,,,,,Normal,,,,,,,,,,,,,,,,,"23/Apr/11 02:32;urandom;This is a cqlsh bug.  The attached patch is an improvement, but could probably be improved upon.;;;","23/Apr/11 02:39;jbellis;i don't think ""cqlsh requires commands to end with semicolon"" is a bug;;;","23/Apr/11 02:52;urandom;bq. i don't think ""cqlsh requires commands to end with semicolon"" is a bug

It's not; That's not the bug.

What's buggy is the way that a multi-line statement is parsed from the input.  The attached patch should take care of it.  It does not require the semi-colon for the individual UPDATE statements (APPLY BATCH is the terminator here), but using them won't hurt anything.;;;","26/Apr/11 19:05;cdaw;The patch works fine.;;;","27/Apr/11 02:18;urandom;committed;;;","27/Apr/11 03:41;hudson;Integrated in Cassandra-0.8 #44 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/44/])
    CASSANDRA-2545 also consider APPLY BATCH for terminating statements

Patch by eevans for CASSANDRA-2545
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL: cqlsh does shows Exception, but not error message when running truncate while a node is down.",CASSANDRA-2539,12504937,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,cdaw,cdaw,22/Apr/11 00:28,16/Apr/19 09:33,14/Jul/23 05:52,23/Apr/11 00:18,0.8.0 beta 2,,,,,,0,cql,,,"This is really just a usability bug, but it would nice to bubble the error message that is printed in the log file up to the interface.

*cqlsh output*
{noformat}
cqlsh> truncate users;
Exception: UnavailableException()
{noformat}

*log file error*
{noformat}
 INFO [pool-2-thread-5] 2011-04-21 23:53:30,466 StorageProxy.java (line 1021) Cannot perform truncate, some hosts are down
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/11 20:49;jbellis;2539-v3.txt;https://issues.apache.org/jira/secure/attachment/12477161/2539-v3.txt","22/Apr/11 19:47;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2539-print-friendlier-message-for-Unavailabl.txt;https://issues.apache.org/jira/secure/attachment/12477141/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2539-print-friendlier-message-for-Unavailabl.txt",,,,,,,,,,,,,2.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20684,,,Tue Apr 26 22:10:19 UTC 2011,,,,,,,,,,"0|i0gbyn:",93368,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"22/Apr/11 08:19;slebresne;UnavailableException does mean, by definition, ""some hosts are down"" (which means that we don't really have anything to bubble anything, we can just have cqlsh write it in plain english if we'd like).

But this made me think, maybe we could attach a String to the UnavailableException saying which nodes are down. Not sure what are the consequence in term of thrift compatibility though and if it has any, it's probably not worth the trouble.;;;","22/Apr/11 19:48;urandom;patch attached;;;","22/Apr/11 20:49;jbellis;v3 also adds a wrapper for TimedOutException;;;","22/Apr/11 21:51;urandom;+1;;;","23/Apr/11 00:18;jbellis;committed;;;","23/Apr/11 01:26;hudson;Integrated in Cassandra-0.8 #36 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/36/])
    more user-friendly messages for UE/TOE in cqlsh
patch by eevans and jbellis for CASSANDRA-2539
;;;","26/Apr/11 22:10;cdaw;Re-tested in current build.  cqlsh now returns the message:

Unable to complete request: one or more nodes were unavailable.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: NPE running SELECT with an IN clause,CASSANDRA-2538,12504935,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,cdaw,cdaw,21/Apr/11 23:41,16/Apr/19 09:33,14/Jul/23 05:52,26/Apr/11 22:12,0.8.0 beta 2,,,,,,0,cql,,,"*Test Case to Run*
{noformat}
cqlsh> select * from users where key in ('user2', 'user3');
Internal application error
{noformat}


*Test Setup*
{noformat}
CREATE COLUMNFAMILY users (
  KEY varchar PRIMARY KEY,
  password varchar);

INSERT INTO users (KEY, password) VALUES ('user1', 'ch@ngem3a');
{noformat}


*Log Files*
{noformat}
ERROR [RequestResponseStage:17] 2011-04-21 23:36:41,600 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[RequestResponseStage:17,5,main]
java.lang.AssertionError
	at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR [RequestResponseStage:17] 2011-04-21 23:36:41,600 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[RequestResponseStage:17,5,main]
java.lang.AssertionError
	at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR [pool-2-thread-5] 2011-04-21 23:37:12,026 Cassandra.java (line 4082) Internal error processing execute_cql_query
java.lang.NullPointerException
	at org.apache.cassandra.cql.WhereClause.and(WhereClause.java:59)
	at org.apache.cassandra.cql.WhereClause.<init>(WhereClause.java:44)
	at org.apache.cassandra.cql.CqlParser.whereClause(CqlParser.java:816)
	at org.apache.cassandra.cql.CqlParser.selectStatement(CqlParser.java:502)
	at org.apache.cassandra.cql.CqlParser.query(CqlParser.java:191)
	at org.apache.cassandra.cql.QueryProcessor.getStatement(QueryProcessor.java:834)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:463)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1134)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/11 02:33;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2538-NPE-running-SELECT-with-an-IN-clause.txt;https://issues.apache.org/jira/secure/attachment/12477069/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2538-NPE-running-SELECT-with-an-IN-clause.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20683,,,Tue Apr 26 22:56:43 UTC 2011,,,,,,,,,,"0|i0gbyf:",93367,,cdaw,,cdaw,Low,,,,,,,,,,,,,,,,,"22/Apr/11 01:42;jbellis;Eric, can you make it so CQL is more clear about not supporting IN, without internal errors?;;;","22/Apr/11 02:36;urandom;This should result in a syntax error like any other mis-statement, but the NPE was beating it to the punch.  Patch attached.;;;","22/Apr/11 22:20;cdaw;This is just a consistency thing, but the documentation is misleading.

The DELETE documentation shows you can use an 'IN' clause:
DELETE [COLUMNS] FROM <COLUMN FAMILY> [USING <CONSISTENCY>] WHERE KEY IN (keyname1, keyname2);

The example for DELETE uses UPDATE, so I assumed 'IN' would work for DELETE/UPDATE/SELECT.
UPDATE ... WHERE KEY IN (keyname1, keyname2)
;;;","23/Apr/11 00:17;jbellis;Totally didn't know we support IN for DELETE. :)

Would prefer to fix by offering IN for for the other statements as well. The Antlr doesn't look too daunting w/ DELETE as an example (knock on wood). Can take a stab at that tomorrow.;;;","23/Apr/11 03:12;urandom;{quote}
Totally didn't know we support IN for DELETE. 

Would prefer to fix by offering IN for for the other statements as well. The Antlr doesn't look too daunting w/ DELETE as an example (knock on wood). Can take a stab at that tomorrow.
{quote}

This is {{multiget_slice()}} / {{multiget_count()}} and was intentionally omitted (remember?).  Not saying this is decision that can't be revisited, just reminding.

Oh, and if we do move forward, it's post-0.8, we're in a feature-freeze. :);;;","25/Apr/11 13:43;jbellis;Okay, created CASSANDRA-2553 for adding IN support to 0.8.1.

+1 on the NPE fix.;;;","26/Apr/11 21:46;cdaw;The patch generates this message:
{code}
cqlsh> select * from users where KEY in ('cathy', 'ed');
Bad Request: line 1:30 mismatched input 'in' expecting set null
{code}
;;;","26/Apr/11 22:10;urandom;This says that it's a bad request, a problem at line 1, column 30, where it encountered mismatched input in the form of the word IN.  This is because `KEY IN ...' is not supported.;;;","26/Apr/11 22:12;urandom;committed.;;;","26/Apr/11 22:17;cdaw;Sorry if I wasn't clear. The patch passed in testing because it no longer generated an NPE and gave an ERROR message. I included the message just for info.;;;","26/Apr/11 22:56;hudson;Integrated in Cassandra-0.8 #43 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/43/])
    NPE running SELECT with an IN clause

Patch by eevans; reviewed by jbellis for CASSANDRA-2538
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Schema disagreements when using connections to multiple hosts,CASSANDRA-2536,12504930,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,21/Apr/11 22:55,16/Apr/19 09:33,14/Jul/23 05:52,29/Apr/11 15:38,0.7.6,0.8.0 beta 2,,,,,1,,,,"If you have two thrift connections open to different nodes and you create a KS using the first, then a CF in that KS using the second, you wind up with a schema disagreement even if you wait/sleep after creating the KS.

The attached script reproduces the issue using pycassa (1.0.6 should work fine, although it has the 0.7 thrift-gen code).  It's also reproducible by hand with two cassandra-cli sessions.",Two node 0.8-beta1 cluster with one seed and JNA.,gdusbabek,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/11 22:26;thobbs;2536-compare-timestamp.txt;https://issues.apache.org/jira/secure/attachment/12477706/2536-compare-timestamp.txt","21/Apr/11 22:56;thobbs;schema_disagree.py;https://issues.apache.org/jira/secure/attachment/12477050/schema_disagree.py",,,,,,,,,,,,,2.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20682,,,Fri Apr 29 15:56:06 UTC 2011,,,,,,,,,,"0|i0gbxz:",93365,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"21/Apr/11 23:20;mbulman;I feel like a better, more critical sounding explanation, is:  create a keyspace on node1, create a cf in that keyspace on node2 = hang + schema disagreement.;;;","22/Apr/11 01:37;jbellis;And this is fine in 0.7?;;;","22/Apr/11 16:03;thobbs;Actually, I can also reproduce this with a two node 0.7.4 cluster.  I'm pretty sure that this does not happen with 0.7.3, but I'll go ahead and verify that.;;;","22/Apr/11 16:20;thobbs;Nevermind my thoughts that it doesn't happen in 0.7.3 -- it seems to happen there too.  It appears this is not a recent problem.;;;","25/Apr/11 14:09;1eightt;Just bumped into this on a fresh 0.7.4 install on our test cluster. Does this only happen in a 2 node ring?;;;","27/Apr/11 18:38;ceocoder;encountered this on fresh 0.7.4 - 5 nodes - 100G+ per node. 

decommissioning the bad node and rejoin fixed the problem.;;;","28/Apr/11 02:10;jbellis;Gary, any thoughts on where to start looking?;;;","28/Apr/11 12:42;gdusbabek;bq. any thoughts...
I was going to add some jmx to get the last N schema versions (seems like it would be handy anyway and will be necessary if we ever get the rollback pony). Send schema to node A, verify that schema is propagated to B, send schema to B and watch the problem happen.  The code to start looking at are the Definitions*VerbHandlers.

Schema version is tracked in two places: gossip and in DatabaseDescriptor.defsVersion.  Make sure those are reasonably in sync (was the sourced of one bug in the past). ;;;","28/Apr/11 20:58;thobbs;The issue is the clocks being out of sync between nodes.  Sometimes the v1 UUID generated by the second node has an earlier timestamp than the current schema UUID has.

There are a couple of things that could be fixed here:

1. A node shouldn't accept a schema change if the timestamp for the new schema would be earlier than its current schema.
2. Schema modification calls should accept an optional client-side timestamp that will be used for the v1 UUID.;;;","28/Apr/11 21:08;gdusbabek;bq. Sometimes the v1 UUID generated by the second node has an earlier timestamp than the current schema UUID has.
Wouldn't that update be DOA then?  I thought we checked to make sure the new migration compared after the current migration (as well as making sure the new migration's previous version matches with the current version).

bq. A node shouldn't accept a schema change if the timestamp for the new schema would be earlier than its current schema.
If the clocks are *that* far off sync, I think the cluster has bigger problems (like writes not being applied). Plus, it would be easy for a node whose clock is way head to 'poison' schema updates from the rest of the cluster who are, in effect, behind the times.

bq. Schema modification calls should accept an optional client-side timestamp that will be used for the v1 UUID.
Seems like a better approach.

;;;","28/Apr/11 21:21;jbellis;bq. A node shouldn't accept a schema change if the timestamp for the new schema would be earlier than its current schema.

You need this with or without the client-side timestamp, though; there's no sense in letting people blow their leg off.

And once you have that you don't need to add a client-side timestamp with all the PITA-ness that involves.

(And unlike with data modification, I can't think of a use for doing ""clever"" things w/ a client side timesamp.  So pushing it to the client doesn't really solve anything, just means you need to sync clocks across more machines.);;;","28/Apr/11 21:33;thobbs;{quote}
Wouldn't that update be DOA then? I thought we checked to make sure the new migration compared after the current migration (as well as making sure the new migration's previous version matches with the current version).
{quote}
We do check that the previous version matches, but the migration is applied locally without comparing the current and new uuids.

{quote}
If the clocks are that far off sync, I think the cluster has bigger problems (like writes not being applied).
{quote}
This can theoretically happen with clocks being off by only tens of milliseconds.

{quote}
And unlike with data modification, I can't think of a use for doing ""clever"" things w/ a client side timesamp. So pushing it to the client doesn't really solve anything, just means you need to sync clocks across more machines.
{quote}
Not for clever purposes -- it seems to me that clients making schema modifications are more likely to be centralized, so schema changes coming from a single client will (almost) always have increasing timestamps.;;;","28/Apr/11 22:26;thobbs;Attached patch compares version timestamps before applying migration locally.;;;","28/Apr/11 23:08;thobbs;I personally think the timestamp comparison is good enough for now.  Any interest in opening a new ticket for client-side timestamps?;;;","29/Apr/11 01:11;jbellis;bq. it seems to me that clients making schema modifications are more likely to be centralized

I would have also argued that they are likely to use the same connection (to the same server), and look where that got us. :)

bq. I personally think the timestamp comparison is good enough for now

I am okay with this. What do you think, Gary?

(Nit: the exception message says ""older"" but the comparison is ""older or equal."");;;","29/Apr/11 07:38;slebresne;I'll hijack this conversation by saying that I think we should start advertising that people should try to keep their server clocks in sync unless they have a good reason not too (which would legitimize the fact that ""timestamp comparison is good enough""). Counter removes for instance use server side timestamps and would be screwed up by diverging clocks (and by that I mean more screwed up than they already are by design). And really, is there any reason not to install a ntpd server in the first place anyway?;;;","29/Apr/11 12:35;gdusbabek;I think timestamp comparisons will be fine.;;;","29/Apr/11 15:38;jbellis;committed, thanks!;;;","29/Apr/11 15:56;hudson;Integrated in Cassandra-0.7 #462 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/462/])
    refuse to apply migrations with older timestamps than the current schema
patch by Tyler Hobbs; reviewed by jbellis and gdusbabek for CASSANDRA-2536
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL documentation missing INSERT syntax,CASSANDRA-2533,12504923,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,21/Apr/11 21:44,16/Apr/19 09:33,14/Jul/23 05:52,27/Apr/11 13:24,0.8.0 beta 2,,,Legacy/Documentation and Website,,,0,cql,,,"Both documents are missing syntax for the INSERT statement:

https://github.com/apache/cassandra/blob/trunk/doc/cql/CQL.textile
https://github.com/apache/cassandra/raw/trunk/doc/cql/CQL.html",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/11 17:30;xedin;CASSANDRA-2533.patch;https://issues.apache.org/jira/secure/attachment/12477427/CASSANDRA-2533.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20679,,,Wed Apr 27 17:18:12 UTC 2011,,,,,,,,,,"0|i0gbxb:",93362,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"21/Apr/11 22:26;cdaw;The more I look at the code, docs and existing tests, it looks like we use UPDATE since it behaves like an upsert.  If that is the case, just let me know.;;;","26/Apr/11 17:18;xedin;This is correct, Insert statement even generates UpdateStatement Insert is an alias for update.;;;","26/Apr/11 17:30;xedin;I only update textile as html version going to be auto-generated from it on each Cassandra build.;;;","27/Apr/11 13:24;jbellis;committed;;;","27/Apr/11 17:18;hudson;Integrated in Cassandra-0.8 #46 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/46/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log read timeouts at the StorageProxy level,CASSANDRA-2532,12504922,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,21/Apr/11 21:26,16/Apr/19 09:33,14/Jul/23 05:52,25/Apr/11 13:54,0.8.0 beta 2,,,,,,0,,,,"We log successful reads, but not timeouts (although we have a lovely TimeoutException message, it doesn't look like we are printing it).",,skamio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/11 21:27;stuhood;0001-Log-timeouts-in-SP.txt;https://issues.apache.org/jira/secure/attachment/12477041/0001-Log-timeouts-in-SP.txt","21/Apr/11 21:48;stuhood;0002-Remove-useless-timeout-entries-in-thrift.CS.txt;https://issues.apache.org/jira/secure/attachment/12477044/0002-Remove-useless-timeout-entries-in-thrift.CS.txt","21/Apr/11 22:53;stuhood;0003-Log-write-timeouts-in-SP.txt;https://issues.apache.org/jira/secure/attachment/12477049/0003-Log-write-timeouts-in-SP.txt",,,,,,,,,,,,3.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20678,,,Mon Apr 25 14:19:26 UTC 2011,,,,,,,,,,"0|i0gbx3:",93361,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"21/Apr/11 21:31;jbellis;as discussed on irc, we should centralize timeout logging in SP rather than mixing it into both CassandraServer + SP.;;;","21/Apr/11 21:48;stuhood;0002 removes the useless timeout entry from thrift.CassandraServer for symmetry.;;;","21/Apr/11 21:53;jbellis;can we do inserts too for consistency?;;;","21/Apr/11 22:55;stuhood;Done in 0003;;;","25/Apr/11 13:54;jbellis;committed, thanks!;;;","25/Apr/11 14:19;hudson;Integrated in Cassandra-0.8 #37 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/37/])
    centralize timeout logging in StorageProxy, and preserve the timeoutexception message
patch by Stu Hood; reviewed by jbellis for CASSANDRA-2532
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE from PrecompactedRow,CASSANDRA-2528,12504909,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,cywjackson,cywjackson,21/Apr/11 17:55,16/Apr/19 09:33,14/Jul/23 05:52,21/Apr/11 22:48,0.8.0 beta 2,,,,,,0,,,,"received a NPE from trunk (0.8) on PrecompactedRow:

ERROR [CompactionExecutor:2] 2011-04-21 17:21:31,610 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:86)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:167)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:124)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:44)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:553)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:146)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:112)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


size of data in /var/lib/cassandra is 11G on this, but there is also report that 1.7G also see the same.

data was previously populated from 0.7.4 cassandra

added debug logging, not sure how much this help (this is logged before the exception.)

 INFO [CompactionExecutor:2] 2011-04-21 17:21:31,588 CompactionManager.java (line 534) Compacting Major: [SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-10-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-7-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-6-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-8-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-9-Data.db')]
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-10-Data.db   : 256
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-7-Data.db   : 512
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-6-Data.db   : 768
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,589 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-8-Data.db   : 1024
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,589 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-9-Data.db   : 1280
 INFO [CompactionExecutor:2] 2011-04-21 17:21:31,609 CompactionIterator.java (line 185) Major@1181554512(cfs, inode.path, 523/10895) now compacting at 16777 bytes/ms.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/11 21:04;jbellis;2528.txt;https://issues.apache.org/jira/secure/attachment/12477037/2528.txt","21/Apr/11 19:18;jbellis;2528.txt;https://issues.apache.org/jira/secure/attachment/12477027/2528.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20676,,,Thu Apr 21 23:18:50 UTC 2011,,,,,,,,,,"0|i0gbw7:",93357,,cywjackson,,cywjackson,Normal,,,,,,,,,,,,,,,,,"21/Apr/11 18:29;patricioe;I'm seeing the same stacktrace when writing 1GB of data. ;;;","21/Apr/11 19:18;jbellis;can you try w/ this patch?  it adds additional asserts to narrow down where the error is;;;","21/Apr/11 19:38;cywjackson;sure, here is the Assertion Error, the null is on metadata:
;;;","21/Apr/11 19:38;cywjackson;DEBUG [CompactionExecutor:2] 2011-04-21 19:31:46,797 PrecompactedRow.java (line 85) debugging controoler true
DEBUG [CompactionExecutor:2] 2011-04-21 19:31:46,798 PrecompactedRow.java (line 87) debugging compactedCF: ColumnFamily(<anonymous> [3636363663643736663936393536343639653762653339643735306363376439:false:0@1303340825329,])
 INFO [CompactionExecutor:2] 2011-04-21 19:31:46,799 CompactionIterator.java (line 185) Major@31583366(cfs, inode.path, 772/11720) now compacting at 8388 bytes/ms.
ERROR [CompactionExecutor:2] 2011-04-21 19:31:46,850 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.AssertionError
    at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:91)
    at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:167)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:124)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:44)
    at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
    at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
    at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
    at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:553)
    at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:146)
    at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:112)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662);;;","21/Apr/11 19:39;cywjackson;{code:title=PrepcompactedRow.java}
    85          logger.debug(String.format(""debugging controoler %b"", controller.shouldPurge(key)));
    86          compactedCf = controller.shouldPurge(key) ? ColumnFamilyStore.removeDeleted(cf, controller.gcBefore) : cf;
    87          logger.debug(String.format(""debugging compactedCF: %s"", compactedCf.toString()));
    88          //if (compactedCf != null && compactedCf.metadata().getDefaultValidator().isCommutative())
    89          if (compactedCf != null)
    90          {
    91                  assert compactedCf.metadata() != null;
    92                  assert compactedCf.metadata().getDefaultValidator() != null;
    93                  if (compactedCf.metadata().getDefaultValidator().isCommutative())
    94                  {
    95                          CounterColumn.removeOldShards(compactedCf, controller.gcBefore);
    96                  }
{code};;;","21/Apr/11 19:40;cywjackson;$ grep java.lang.AssertionError -A3 /var/log/cassandra/system.log 
{noformat}

java.lang.AssertionError
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:91)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:167)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:124)
--
java.lang.AssertionError
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:91)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:167)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:124)
{noformat};;;","21/Apr/11 21:04;jbellis;Patch to preserve the CFMetaData object used to construct the ColumnFamily, instead of looking it up from the registry (which does not contain metadata for index CFs).;;;","21/Apr/11 22:38;cywjackson;the fix does address the NPE as now all CF has metadata.;;;","21/Apr/11 22:48;jbellis;committed;;;","21/Apr/11 23:18;hudson;Integrated in Cassandra-0.8 #32 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/32/])
    fix NPE compacting index CFs
patch by jbellis; reviewed by Jackson Chung for CASSANDRA-2528
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: create keyspace does not the replication factor argument and allows invalid sql to pass thru,CASSANDRA-2525,12504840,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,cdaw,cdaw,21/Apr/11 01:35,16/Apr/19 09:33,14/Jul/23 05:52,22/Apr/11 20:50,0.8.0 beta 2,,,,,,0,cql,,,"I ran the following SQL in cqlsh and immediately received a socket closed error.  After that point, I couldn't run nodetool, and then got an exception starting up the cluster.

Please Note:  The following syntax is valid in 0.74 but invalid in 0.8.
The 0.8 cassandra-cli errors on the following statement, so the resolution of the bug is to have cqlsh block this incorrect syntax.

{code}
create keyspace testcli with replication_factor=1
and placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy';
{code}

{code}
CREATE KEYSPACE testcql with replication_factor = 1 and strategy_class = 'org.apache.cassandra.locator.SimpleStrategy';	
{code}


{code}
ERROR 01:29:26,989 Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:278)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:79)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:328)
	at org.apache.cassandra.db.Table.<init>(Table.java:274)
	... 4 more
Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:278)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:79)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:328)
	at org.apache.cassandra.db.Table.<init>(Table.java:274)
	... 4 more
{code}","Cluster: 3 node Rackspace cluster running Centos 5.5.

Build: https://builds.apache.org/hudson/job/Cassandra/859/artifact/cassandra/build/apache-cassandra-2011-04-20_10-01-29-bin.tar.gz
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/11 05:06;jbellis;2525.txt;https://issues.apache.org/jira/secure/attachment/12476960/2525.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20675,,,Tue Apr 26 22:14:18 UTC 2011,,,,,,,,,,"0|i0gbvj:",93354,,urandom,,urandom,Critical,,,,,,,,,,,,,,,,,"21/Apr/11 02:27;yukim;I got the same problem, and after making change in the statement like following

{code}
CREATE KEYSPACE CqlTest WITH strategy_class = SimpleStrategy AND strategy_options:replication_factor = 1;
{code}

everything works fine.
(I'm using apache-cassandra-2011-04-12_18-15-17 build version.)

I think CQL doc should also be updated, because it states that ""replication_factor"" is required.;;;","21/Apr/11 05:06;jbellis;patch to reject malformed keyspaces from cql.

(also patches CQL.textile docs source, but I have not yet figured out how to generate the html from that.);;;","21/Apr/11 14:24;jbellis;opened CASSANDRA-2526 for doc generation;;;","22/Apr/11 20:28;urandom;+1;;;","22/Apr/11 20:50;jbellis;committed;;;","23/Apr/11 01:26;hudson;Integrated in Cassandra-0.8 #36 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/36/])
    validate CQL create keyspace options
patch by jbellis; reviewed by eevans for CASSANDRA-2525
;;;","26/Apr/11 22:14;cdaw;retested and verified;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Distributed test scripts not working with Whirr 0.4.0,CASSANDRA-2523,12504822,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mallen,stuhood,stuhood,20/Apr/11 19:58,16/Apr/19 09:33,14/Jul/23 05:52,03/Aug/11 22:10,0.8.4,,,,,,0,,,,"I suspect that our runurl based script execution is not working with Whirr 0.4.0, which is causing distributed tests that kill/wipe nodes to timeout. See [this FAQ entry|http://incubator.apache.org/whirr/faq.html#how-can-i-modify-the-instance-installation-and-configuration-scripts] for a description of the change.",,stuhood,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2464,,,,,CASSANDRA-2504,,,,,,"22/Apr/11 02:31;stuhood;0001-WIP-Update-to-use-jclouds-magical-functions-directory-.txt;https://issues.apache.org/jira/secure/attachment/12477068/0001-WIP-Update-to-use-jclouds-magical-functions-directory-.txt","02/Aug/11 08:13;mallen;2523-1-use-jclouds-magical-functions-directory-revised.txt;https://issues.apache.org/jira/secure/attachment/12488864/2523-1-use-jclouds-magical-functions-directory-revised.txt","02/Aug/11 16:20;mallen;2523-2-use-jclouds-magical-functions-directory-revised.txt;https://issues.apache.org/jira/secure/attachment/12488916/2523-2-use-jclouds-magical-functions-directory-revised.txt","03/Aug/11 21:45;mallen;2523-3-use-jclouds-magical-functions-directory-revised.txt;https://issues.apache.org/jira/secure/attachment/12489258/2523-3-use-jclouds-magical-functions-directory-revised.txt",,,,,,,,,,,4.0,mallen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20674,,,Wed Aug 03 23:25:43 UTC 2011,,,,,,,,,,"0|i0gbvb:",93353,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"20/Apr/11 20:01;stuhood;This issue probably relates-to/causes 2504.;;;","22/Apr/11 02:31;stuhood;A work-in-progress patch to include our own ""functions"" directory as alluded to in the Whirr FAQ. I'm not yet certain that it's getting picked up at all.;;;","22/Apr/11 09:42;xedin;Standard approach works for me most of the time but in like 2-3% of the cases it fails, I don't think that runurl is a good and stable solution, maybe we should hold our scripts in the test/distributed/scripts directory so we always sure that they are in place instead of using whirr's half-broken approach? ;;;","09/May/11 20:16;jbellis;Any more clarity here on what the right way to fix this is?;;;","09/May/11 21:57;stuhood;I'd like to put some more time into the distributed tests once the next revision of 2319/674 is posted: hopefully 2 weeks from now.;;;","02/Aug/11 08:14;mallen;Stu was headed in the right direction here.  I've attached a working revised version of his patch with a few adjustments.  

The main difference is the use of whirr's StatementBuilder, which uses jclouds ScriptBuilder.  When an instance of ScriptBuilder is rendered and during the 'resolve function dependencies stage', jclouds will search the class path for a directory named ""functions"".  It will then look in that directory for a file named after the function that it is trying to resolve.  For example, if in the ScriptBuilder there was a call to the function ""start_cassandra"", then jclouds would look for a file named functions/start_cassandra.sh and if found, include its content in the ScriptBuilder.  

functions/start_cassandra.sh is expected to contain a single function definition, namely ""start_cassandra"".

This seems to work fairly well in practice, and we no longer use runurl with this solution.

;;;","02/Aug/11 16:20;mallen;updated to fix my use of the incorrect aws-s3 version.  should be 1.0-beta-9b rather than 1.0.;;;","03/Aug/11 20:32;brandon.williams;why does install_cassandra.sh use 0.7.0?;;;","03/Aug/11 20:44;mallen;Currently, the version is always passed to that function so the default never gets used, but I guess we should update that to default to 0.8 instead?;;;","03/Aug/11 20:46;mallen;and to actually answer the question :)  that install script is provided by whirr and in 0.4.0 release, they've got it set to 0.7.;;;","03/Aug/11 20:52;brandon.williams;bq. Currently, the version is always passed to that function so the default never gets used, but I guess we should update that to default to 0.8 instead?

I think I'd rather not have a default so it's always explicit.  Accidentally testing the wrong version would be pretty bad.

bq. and to actually answer the question  that install script is provided by whirr and in 0.4.0 release, they've got it set to 0.7.

Ah, I see :);;;","03/Aug/11 21:45;mallen;Attached another version to address the ""let's not use a default version of cassandra"" in install_cassandra.sh;;;","03/Aug/11 22:10;brandon.williams;Committed.;;;","03/Aug/11 23:25;hudson;Integrated in Cassandra-0.8 #255 (See [https://builds.apache.org/job/Cassandra-0.8/255/])
    Update distributed tests to work with whirr 0.4
Patch by Michael Allen, reviewed by brandonwilliams for CASSANDRA-2523

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1153688
Files : 
* /cassandra/branches/cassandra-0.8/test/distributed/org/apache/cassandra/CassandraServiceController.java
* /cassandra/branches/cassandra-0.8/test/resources/functions/stop_cassandra.sh
* /cassandra/branches/cassandra-0.8/test/resources/functions/start_cassandra.sh
* /cassandra/branches/cassandra-0.8/test/distributed/org/apache/cassandra/TestBase.java
* /cassandra/branches/cassandra-0.8/test/resources/functions/install_cassandra.sh
* /cassandra/branches/cassandra-0.8/test/resources/functions
* /cassandra/branches/cassandra-0.8/test/resources/functions/nodetool_cassandra.sh
* /cassandra/branches/cassandra-0.8/test/resources/functions/wipe_cassandra.sh
* /cassandra/branches/cassandra-0.8/test/resources/functions/configure_cassandra.sh
* /cassandra/branches/cassandra-0.8/test/distributed/org/apache/cassandra/MutationTest.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
row deletions do not add to memtable op count,CASSANDRA-2519,12504779,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,20/Apr/11 10:45,16/Apr/19 09:33,14/Jul/23 05:52,20/Apr/11 15:54,0.7.5,,,,,,0,,,,"from discussion http://www.mail-archive.com/user@cassandra.apache.org/msg12531.html

Memtable.resolve() uses the count of columns in the CF to bump the op count however RowMutation.delete() does not add any columns to the CF when an entire row is deleted. If a super column or column is deleted it adds 1 towards the op count. Deleting many named columns will add many to the op count. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/11 10:58;amorton;0001-include-row-deletions-in-memtable-op-count.patch;https://issues.apache.org/jira/secure/attachment/12476875/0001-include-row-deletions-in-memtable-op-count.patch",,,,,,,,,,,,,,1.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20672,,,Wed Apr 20 16:26:11 UTC 2011,,,,,,,,,,"0|i0gbuf:",93349,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"20/Apr/11 10:58;amorton;patch 0001 adds one to the memtable op count if the CF contains zero columns and cf.isMarkedForDeletion();;;","20/Apr/11 15:54;jbellis;committed, thanks!;;;","20/Apr/11 16:26;hudson;Integrated in Cassandra-0.7 #450 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/450/])
    count a row deletionas one operation towards memtable threshold
patch by Aaron Morton; reviewed by jbellis for CASSANDRA-2519
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update distributed tests for optional Column fields,CASSANDRA-2517,12504752,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,20/Apr/11 03:49,16/Apr/19 09:33,14/Jul/23 05:52,20/Apr/11 04:28,0.8.0 beta 2,,,Legacy/Testing,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/11 03:51;stuhood;0001-Remove-duped-code-from-MutationTest.txt;https://issues.apache.org/jira/secure/attachment/12476827/0001-Remove-duped-code-from-MutationTest.txt","20/Apr/11 03:51;stuhood;0002-Update-distributed-tests-for-thrift-api-changes.txt;https://issues.apache.org/jira/secure/attachment/12476828/0002-Update-distributed-tests-for-thrift-api-changes.txt",,,,,,,,,,,,,2.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20670,,,Wed Apr 20 04:28:42 UTC 2011,,,,,,,,,,"0|i0gbtz:",93347,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"20/Apr/11 03:51;stuhood;0001 Remove duped code that was left behind when generic utilities were boosted into TestBase
0002 Updates Column construction for optional timestamp and value;;;","20/Apr/11 04:28;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Allow LOCAL_QUORUM, EACH_QUORUM CLs to work w/ any Strategy class",CASSANDRA-2516,12504742,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,20/Apr/11 02:00,16/Apr/19 09:33,14/Jul/23 05:52,21/Apr/11 15:19,0.7.5,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/11 02:01;jbellis;2516.txt;https://issues.apache.org/jira/secure/attachment/12476817/2516.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20669,,,Thu Apr 21 18:52:40 UTC 2011,,,,,,,,,,"0|i0gbtr:",93346,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"21/Apr/11 12:47;slebresne;+1
(but note that it's a 0.8 patch, so it should probably be rebased to 0.7 and applied there first);;;","21/Apr/11 15:19;jbellis;committed to 0.7, 0.8;;;","21/Apr/11 18:52;hudson;Integrated in Cassandra-0.7 #452 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/452/])
    support LOCAL_QUORUM, EACH_QUORUM CLs outside of NTS
patch by jbellis; reviewed by slebresne for CASSANDRA-2516
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
batch_mutate operations with CL=LOCAL_QUORUM throw TimeOutException when there aren't sufficient live nodes,CASSANDRA-2514,12504738,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,nar3ndra,nar3ndra,nar3ndra,20/Apr/11 00:29,16/Apr/19 09:33,14/Jul/23 05:52,20/Apr/11 18:16,0.7.5,,,,,,0,,,,"We have a 2 DC setup with RF = 4. There are 2 nodes in each DC. Following is the keyspace definition:
<snip>
keyspaces:
    - name: KeyspaceMetadata
      replica_placement_strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
      strategy_options:
        DC1 : 2
        DC2 : 2
      replication_factor: 4
</snip>

I shutdown all except one node and waited for the live node to recognize that other nodes are dead. Following is the nodetool ring output on the live node:
Address         Status State   Load            Owns    Token                                       
                                                       169579575332184635438912517119426957796     
10.17.221.19    Down   Normal  ?               29.20%  49117425183422571410176530597442406739      
10.17.221.17    Up     Normal  81.64 KB        4.41%   56615248844645582918169246064691229930      
10.16.80.54     Down   Normal  ?               21.13%  92563519227261352488017033924602789201      
10.17.221.18    Down   Normal  ?               45.27%  169579575332184635438912517119426957796     

I expect UnavailableException when I send batch_mutate request to node that is up. However, it returned TimeOutException:
TimedOutException()
    at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:16493)
    at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:916)
    at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:890)

Following is the cassandra-topology.properties
# Cassandra Node IP=Data Center:Rack
10.17.221.17=DC1:RAC1
10.17.221.19=DC1:RAC2

10.17.221.18=DC2:RAC1
10.16.80.54=DC2:RAC2
","1. Cassandra 0.7.4 running on RHEL 5.5
2. 2 DC setup
3. RF = 4 (DC1 = 2, DC2 = 2)
4. CL = LOCAL_QUORUM",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/11 16:32;jbellis;2514-v2.txt;https://issues.apache.org/jira/secure/attachment/12476909/2514-v2.txt","20/Apr/11 00:41;nar3ndra;CASSANDRA-2514.patch;https://issues.apache.org/jira/secure/attachment/12476809/CASSANDRA-2514.patch",,,,,,,,,,,,,2.0,nar3ndra,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20667,,,Wed Apr 20 19:46:08 UTC 2011,,,,,,,,,,"0|i0gbtb:",93344,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"20/Apr/11 00:37;nar3ndra;I think the issue is because DatacenterWriteResponseHandler.assureSufficientLiveNodes is not checking for live nodes.

DatacenterWriteResponseHandler.assureSufficientLiveNodes works on writeEndpoints. writeEndpoints contains list of the all the endpoints (may be more if there are nodes bootstrapping).

I think either writeEndpoints should ignore dead/unreachable nodes or DatacenterWriteResponseHandler.assureSufficientLiveNodes should use hintedEndpoints.keySet() as that contains the live endpoints.
I compared the implementation with WriteResponseHandler.assureSufficientLiveNodes and found that it uses hintedEndpoints.


I am attaching the patch that works for me.;;;","20/Apr/11 00:41;nar3ndra;Use hintedEndpoints instead of writeEndpoints to work on live endpoints only.;;;","20/Apr/11 00:52;nar3ndra;The code to reproduce this issue is a simple batch mutate operation. The operation I performed involved adding 2 columns to a SuperColumn. Let me know if it is not reproducible. I will provide the sample code.;;;","20/Apr/11 16:32;jbellis;Good catch, that is a bug.

v2 adds a couple improvements:

- only count the hinted endpoint towards live count if it's a normal write destination (hints can be sent elsewhere if all the write destinations are dead)
- similar fix for DSWRH (EACH_QUORUM)
- unrelated fix in WRH for CL.ANY not to continue through to the CL.Q/ALL code;;;","20/Apr/11 16:32;jbellis;how does that look to you?;;;","20/Apr/11 17:22;nar3ndra;Looks good to me. 

Just one comment/question:
hintedEndpoints is subset of writeEndpoints. So is the additional check writeEndpoints.contains(destination), while we are iterating over hintedEndpoints, needed? I think assert would be better here.

;;;","20/Apr/11 17:32;jbellis;That's the point, hintedEndpoints is *usually* but not always a subset of writeEndpoints. Here is the code from getHintedEndpoints:

{code}
        // assign dead endpoints to be hinted to the closest live one, or to the local node
        // (since it is trivially the closest) if none are alive.  This way, the cost of doing
        // a hint is only adding the hint header, rather than doing a full extra write, if any
        // destination nodes are alive.
        //
        // we do a 2nd pass on targets instead of using temporary storage,
        // to optimize for the common case (everything was alive).
        InetAddress localAddress = FBUtilities.getLocalAddress();
        for (InetAddress ep : targets)
        {
            if (map.containsKey(ep))
                continue;
            if (!StorageProxy.shouldHint(ep))
            {
                if (logger.isDebugEnabled())
                    logger.debug(""not hinting "" + ep + "" which has been down "" + Gossiper.instance.getEndpointDowntime(ep) + ""ms"");
                continue;
            }

            InetAddress destination = map.isEmpty()
                                    ? localAddress
                                    : snitch.getSortedListByProximity(localAddress, map.keySet()).get(0);
            map.put(destination, ep);
        }
{code};;;","20/Apr/11 17:33;jbellis;that is: our last-resort local hint storage may not be part of writeEndpoints (probably won't be, on a large cluster).;;;","20/Apr/11 17:53;nar3ndra;Got it. In my setup I had HH disabled. So I overlooked the rest of the getHintedEndpoints.

The change looks good to me now. Thanks!

;;;","20/Apr/11 18:16;jbellis;committed, thanks!;;;","20/Apr/11 19:46;hudson;Integrated in Cassandra-0.7 #451 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/451/])
    fixes for verifying destinationavailability under hinted conditions
patch by Narendra Sharma and jbellis for CASSANDRA-2514
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updating a column's validation class from AsciiType to UTF8Type does not actually work,CASSANDRA-2512,12504720,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,cdaw,cdaw,19/Apr/11 21:59,16/Apr/19 09:33,14/Jul/23 05:52,21/Apr/11 13:29,0.7.5,,,,,,0,,,,"Please note this is reproducible on both Cassandra 0.74 and the April 18th trunk build.

*Reproduction Steps*
{code}
create column family users with comparator = UTF8Type
and column_metadata = [{column_name: password, validation_class: UTF8Type},
{column_name: gender, validation_class: AsciiType}];

update column family users with comparator = UTF8Type
and column_metadata = [{column_name: password, validation_class: UTF8Type}
{column_name: gender, validation_class: UTF8Type}];
{code}

*Before & After quitting cassandra-cli:  Notice the validation class for the gender client still shows AsciiType*
{code}
[default@demo] describe keyspace demo;
Keyspace: demo:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
    Options: [datacenter1:1]
  Column Families:
    ColumnFamily: users
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.29062499999999997/62/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name: gender
          Validation Class: org.apache.cassandra.db.marshal.AsciiType
        Column Name: password
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type

{code}
","Single Node on MacOSX, installed from 0.7.4 binary .tar ball.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/11 16:41;jbellis;2512.txt;https://issues.apache.org/jira/secure/attachment/12476912/2512.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20666,,,Tue Apr 26 22:22:11 UTC 2011,,,,,,,,,,"0|i0gbsv:",93342,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"20/Apr/11 16:41;jbellis;patch fixes CFMetaData.apply to include validation class.;;;","21/Apr/11 13:29;slebresne;+1
Committed.;;;","21/Apr/11 18:52;hudson;Integrated in Cassandra-0.7 #452 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/452/])
    Fix updating column metadata validation class
patch by jbellis; reviewed by slebresne for CASSANDRA-2512
;;;","21/Apr/11 20:01;hudson;Integrated in Cassandra #861 (See [https://builds.apache.org/hudson/job/Cassandra/861/])
    merge CASSANDRA-2512 from 0.8
;;;","21/Apr/11 20:03;hudson;Integrated in Cassandra-0.8 #31 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/31/])
    merge CASSANDRA-2512 from 0.7
;;;","26/Apr/11 22:22;cdaw;Retested and verified this is fixed in current build.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to forward merge parts of r1088800 to make the pig CassandraStorage build,CASSANDRA-2511,12504715,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jeromatron,jeromatron,jeromatron,19/Apr/11 21:22,16/Apr/19 09:33,14/Jul/23 05:52,19/Apr/11 21:54,0.8 beta 1,,,,,,0,hadoop,pig,,"Parts of revision 1088800 weren't forward merged into 0.8/trunk.  So the 0.8/trunk version of CassandraStorage doesn't currently build.  Specifically, it needs the decompose method in the AbstractType hierarchy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/11 23:06;jeromatron;2511-newer-types.txt;https://issues.apache.org/jira/secure/attachment/12476799/2511-newer-types.txt","19/Apr/11 21:50;jeromatron;2511.txt;https://issues.apache.org/jira/secure/attachment/12476786/2511.txt",,,,,,,,,,,,,2.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20665,,,Tue Apr 19 23:06:29 UTC 2011,,,,,,,,,,"0|i0gbsn:",93341,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"19/Apr/11 21:50;jeromatron;Forward merged the parts from CASSANDRA-2387 that hadn't made it in.  The only question I had was whether it was worth it to embed a call to decompose in the TimeUUIDType.fromString method.  It may not be worth it.;;;","19/Apr/11 21:51;jeromatron;So this makes it so the pig CassandraStorage can compile and run in 0.8.  Patch is against 0.8-branch.;;;","19/Apr/11 21:54;jbellis;committed, thanks!;;;","19/Apr/11 23:06;jeromatron;The counter type may have an incorrect decompose (in its parent class) but I think the rest should be fine.  Sorry about missing these the first time.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Word count example won't compile,CASSANDRA-2510,12504713,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,19/Apr/11 21:02,16/Apr/19 09:33,14/Jul/23 05:52,19/Apr/11 21:19,0.8 beta 1,,,,,,0,hadoop,,,"On the 0.8 branch, the word count stuff isn't compiling.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/11 21:04;jeromatron;2510.txt;https://issues.apache.org/jira/secure/attachment/12476781/2510.txt",,,,,,,,,,,,,,1.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20664,,,Wed Apr 20 02:20:33 UTC 2011,,,,,,,,,,"0|i0gbsf:",93340,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"19/Apr/11 21:04;jeromatron;Updating the way it creates the keyspace and the data so it compiles and works properly.;;;","19/Apr/11 21:19;jbellis;committed;;;","20/Apr/11 02:20;hudson;Integrated in Cassandra-0.8 #26 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/26/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update py_test to use strategy_options,CASSANDRA-2509,12504708,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,rwjblue,rwjblue,rwjblue,19/Apr/11 20:19,16/Apr/19 09:33,14/Jul/23 05:52,19/Apr/11 20:26,0.8 beta 1,,,Legacy/Tools,,,0,,,,"KsDef was changed in cassandra.thrift to accept a hash of options as strategy_options.  py_test/stress.py needs to be updated with the new method arguments.

CASSANDRA-1263 changed the parameters to KsDef.
CASSANDRA-2462 fixed this issue in the native Java stress package.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/11 20:21;rwjblue;0001-Update-py_stress-to-make-replication_factor-part-of-.patch;https://issues.apache.org/jira/secure/attachment/12476772/0001-Update-py_stress-to-make-replication_factor-part-of-.patch",,,,,,,,,,,,,,1.0,rwjblue,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20663,,,Tue Apr 19 21:28:03 UTC 2011,,,,,,,,,,"0|i0gbs7:",93339,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"19/Apr/11 20:21;rwjblue;Attached patch fixes issue with py_test.;;;","19/Apr/11 20:25;rwjblue;Patch uploaded.;;;","19/Apr/11 20:26;jbellis;committed, thanks!;;;","19/Apr/11 21:28;hudson;Integrated in Cassandra-0.8 #23 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/23/])
    update stress.py for KsDef replication_factor change
patch by Robert Jackson; reviewed by jbellis for CASSANDRA-2509
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
missing imports in CQL Python driver,CASSANDRA-2508,12504694,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,urandom,urandom,19/Apr/11 17:44,16/Apr/19 09:33,14/Jul/23 05:52,25/Apr/11 17:56,0.8.0 beta 2,,,Legacy/Tools,,,0,cql,,,"Try:

bq. cd drivers/py && python -c 'from cql import DateFromTicks; DateFromTicks(1)'

Also:
{{cql.connection}} is missing an import of {{AuthenticationRequest}} from {{ttypes}}, and the exceptions {{NotSupportedError}}, and {{InternalError}}.

Also:
{{marshal.unmarshal_long}} has a NameError waiting to happen in the form of ""unpack""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/11 20:13;jbellis;2508.txt;https://issues.apache.org/jira/secure/attachment/12476770/2508.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20662,,,Tue Apr 19 21:28:03 UTC 2011,,,,,,,,,,"0|i0gbrz:",93338,,thobbs,,thobbs,Normal,,,,,,,,,,,,,,,,,"19/Apr/11 20:13;jbellis;bq. python -c 'from cql import DateFromTicks; DateFromTicks(1)'

fixed

bq. cql.connection is missing an import of AuthenticationRequest from ttypes

fixed

bq. NotSupportedError

changed to builtin NotImplementedError

bq. InternalError

ttypes InternalError should be internal errors on the server; change this to a no-op (in the case of repeated closes) and ValueError (in the case of operation-on-closed-handle), which match the behavior of the file class.

bq. marshal.unmarshal_long has a NameError waiting to happen in the form of ""unpack""

fixed, and also pulled the if out of the unmarshal definition to only execute once.;;;","19/Apr/11 20:28;thobbs;Looks good with the exception of changing the exceptions that are raised.

Internal error refers to cql.InternalError here, and PEP 249 outlines its usage in a way that closely matches the way it was used.
{noformat}
InternalError 
                      
Exception raised when the database encounters an internal
error, e.g. the cursor is not valid anymore, the
transaction is out of sync, etc.  It must be a subclass of
DatabaseError.
{noformat}

PEP 249 also mentions using NotSupportedError explicitly in reference to rollback():
{noformat}
NotSupportedError
          
Exception raised in case a method or database API was used
which is not supported by the database, e.g. requesting a
.rollback() on a connection that does not support
transaction or has transactions turned off.  It must be a
subclass of DatabaseError.
{noformat}
I think it should be kept here.;;;","19/Apr/11 20:40;jbellis;You're right re NSE. Fixed.

InternalError is for _database_ errors not driver errors. Left re-close() as ok; changed cursor()-from-closed-conn to ProgrammingError, matching the sqlite3 behavior.

committed w/ above changes.;;;","19/Apr/11 21:28;hudson;Integrated in Cassandra-0.8 #23 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/23/])
    fix imports in python cql driver
patch by jbellis; reviewed by thobbs for CASSANDRA-2508
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python CQL driver does not decode most values,CASSANDRA-2507,12504681,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,urandom,urandom,19/Apr/11 16:33,16/Apr/19 09:33,14/Jul/23 05:52,19/Apr/11 20:02,0.8 beta 1,,,Legacy/CQL,,,0,cql,,,"Most keys, and column name/values are not decoded properly.  The attached CQL input can be used to demonstrate:

_Note: requires the patch from CASSANDRA-2505 to be applied_

{noformat}
$ drivers/py/cqlsh localhost 9170 < repro.cql 
 | '\x00\x00\x00\x00\x00\x00\x00\x01','\x00\x00\x00\x00\x00\x00\x00\x01' | '\x00\x00\x00\x00\x00\x00\x00\x02','\x00\x00\x00\x00\x00\x00\x00\x02'
e�#j������ | 'e\xe2#\x01j\xa2\x11\xe0\x00\x00\xfe\x8e\xbe\xea\xd9\xff','e\xe2#\x02j\xa2\x11\xe0\x00\x00\xfe\x8e\xbe\xea\xd9\xff'
{noformat}

For all practical purposes, this renders the driver useless for everything but strings.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/11 19:34;thobbs;2507.txt;https://issues.apache.org/jira/secure/attachment/12476762/2507.txt","19/Apr/11 17:34;urandom;repro.cql;https://issues.apache.org/jira/secure/attachment/12476749/repro.cql",,,,,,,,,,,,,2.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20661,,,Tue Apr 19 21:28:03 UTC 2011,,,,,,,,,,"0|i0gbrr:",93337,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"19/Apr/11 19:34;thobbs;Attached patch fixes the issue by:

1. Not creating a new cursor for every line of CQL.
2. Updating the driver's view of the schema after DDL statements instead of before them.;;;","19/Apr/11 20:02;jbellis;committed;;;","19/Apr/11 21:28;hudson;Integrated in Cassandra-0.8 #23 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/23/])
    cqlsh fixes
patch by thobbs; reviewed by jbellis for CASSANDRA-2507
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh can't decode column names/values,CASSANDRA-2505,12504678,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,19/Apr/11 16:27,16/Apr/19 09:33,14/Jul/23 05:52,19/Apr/11 17:35,0.8 beta 1,,,Legacy/CQL,Legacy/Tools,,0,cql,,,"The way results are accessed, cqlsh is displaying the raw thrift results and not the decoded values.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/11 16:29;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2505-use-dbapi-interface-for-decoded-values.txt;https://issues.apache.org/jira/secure/attachment/12476746/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2505-use-dbapi-interface-for-decoded-values.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20660,,,Tue Apr 19 19:05:56 UTC 2011,,,,,,,,,,"0|i0gbrb:",93335,,,,,Normal,,,,,,,,,,,,,,,,,"19/Apr/11 17:15;jbellis;+1;;;","19/Apr/11 17:35;urandom;commmitted;;;","19/Apr/11 19:05;hudson;Integrated in Cassandra-0.8 #22 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/22/])
    use dbapi interface for decoded values

Patch by eevans; reviewed by jbellis for CASSANDRA-2505
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error running cqlsh from .tar file -- global name 'SchemaDisagreementException' is not defined,CASSANDRA-2501,12504612,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,cdaw,cdaw,18/Apr/11 22:51,16/Apr/19 09:33,14/Jul/23 05:52,19/Apr/11 13:33,0.8 beta 1,,,,,,0,cql,,,"*Error when running cqlsh*
{code}
[cassandra@cdaw-qa1 cql-1.0.0]$ cqlsh cdaw-qa1
Traceback (most recent call last):
  File ""/usr/bin/cqlsh"", line 212, in <module>
    password=options.password)
  File ""/usr/bin/cqlsh"", line 55, in __init__
    self.conn = cql.connect(hostname, port, user=username, password=password)
  File ""/usr/lib/python2.6/site-packages/cql/__init__.py"", line 51, in connect
    return connection.Connection(host, port, keyspace, user, password)
  File ""/usr/lib/python2.6/site-packages/cql/connection.py"", line 53, in __init__
    c.execute('USE %s;' % keyspace)
  File ""/usr/lib/python2.6/site-packages/cql/cursor.py"", line 126, in execute
    except SchemaDisagreementException, sde:
NameError: global name 'SchemaDisagreementException' is not defined
{code}


*Build*
* Install the cassandra binary from the nightly build
wget https://builds.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/apache-cassandra-2011-04-18_11-02-29-bin.tar.gz

* Install cql from .tar file on nightly build
wget https://builds.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/cql-1.0.0.tar.gz

*CQL Install Output*
{code}
[cassandra@cdaw-qa1 cql-1.0.0]$ sudo python2.6 ./setup.py install
[sudo] password for cassandra: 
running install
running build
running build_py
running build_scripts
creating build/scripts-2.6
copying and adjusting cqlsh -> build/scripts-2.6
changing mode of build/scripts-2.6/cqlsh from 644 to 755
running install_lib
creating /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/results.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/marshal.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/connection.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/cursor.py -> /usr/lib/python2.6/site-packages/cql
creating /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/__init__.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/Cassandra.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/constants.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/ttypes.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/decoders.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/__init__.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/errors.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/connection_pool.py -> /usr/lib/python2.6/site-packages/cql
byte-compiling /usr/lib/python2.6/site-packages/cql/results.py to results.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/marshal.py to marshal.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/connection.py to connection.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cursor.py to cursor.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/__init__.py to __init__.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/Cassandra.py to Cassandra.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/constants.py to constants.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/ttypes.py to ttypes.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/decoders.py to decoders.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/__init__.py to __init__.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/errors.py to errors.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/connection_pool.py to connection_pool.pyc
running install_scripts
copying build/scripts-2.6/cqlsh -> /usr/bin
changing mode of /usr/bin/cqlsh to 755
running install_egg_info
Writing /usr/lib/python2.6/site-packages/cql-1.0.0-py2.6.egg-info

{code}
","Running on 3-node Centos 5.5. The cql package was installed with Python 2.6 and prior to installation, I downloaded and installed thrift05-0.5.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/11 23:18;jbellis;2501.txt;https://issues.apache.org/jira/secure/attachment/12476664/2501.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20658,,,Tue Apr 19 14:16:54 UTC 2011,,,,,,,,,,"0|i0gbqf:",93331,,cdaw,,cdaw,Normal,,,,,,,,,,,,,,,,,"18/Apr/11 23:18;jbellis;i think the SDE error is masking another problem -- try with this patch (""ant release"" to build the same artifacts hudson does) and see if that exposes something else.;;;","19/Apr/11 02:05;cdaw;The error still occurs after running ""ant release"".  I also upgraded to thrift 6.0.0 and that didn't resolve the issue either.;;;","19/Apr/11 02:12;jbellis;just to doublecheck: you applied the patch (patch -p0 < 2501.txt) before running ant release?;;;","19/Apr/11 02:41;cdaw;Sorry about that ... applied the patch and got a new error:

{code}

[cassandra@cdaw-qa1 cql-1.0.0]$ cqlsh cdaw-qa1
Traceback (most recent call last):
  File ""/usr/bin/cqlsh"", line 212, in <module>
    password=options.password)
  File ""/usr/bin/cqlsh"", line 55, in __init__
    self.conn = cql.connect(hostname, port, user=username, password=password)
  File ""/usr/lib/python2.6/site-packages/cql/__init__.py"", line 51, in connect
    return connection.Connection(host, port, keyspace, user, password)
  File ""/usr/lib/python2.6/site-packages/cql/connection.py"", line 53, in __init__
    c.execute('USE %s;' % keyspace)
  File ""/usr/lib/python2.6/site-packages/cql/cursor.py"", line 133, in execute
    raise cql.InternalError(""Internal application error"")
cql.InternalError: Internal application error
{code};;;","19/Apr/11 02:56;jbellis;Good, that's what I thought.

Cassandra always logs a stacktrace for ""internal application error,"" can you grab that from /var/log/cassandra?

(if i were to take a wild-ass guess I would say it's not handling USE correctly when given a keyspace that doesn't exist.);;;","19/Apr/11 05:08;cdaw;User Error.  I noticed in my log file that the Cassandra version was 0.74 but I was running from the trunk.  I had put $CASSANDRA_HOME in my .bashrc file, but when I removed it, everything was fine.

I would normally resolve this as Will Not Fix, but not sure about the patch you provided, and if you want to associate this with that.;;;","19/Apr/11 13:33;jbellis;Will mark Fixed since the missing SchemaDisagreementException import was a real bug.;;;","19/Apr/11 13:34;jbellis;(committed in r1095082);;;","19/Apr/11 14:16;hudson;Integrated in Cassandra-0.8 #21 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/21/])
    add SchemaDisagreementException import
patch by jbellis; tested by Cathy Daw for CASSANDRA-2501
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-env.sh pattern matching for OpenJDK broken in some cases,CASSANDRA-2499,12504595,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cywjackson,thobbs,thobbs,18/Apr/11 20:37,16/Apr/19 09:33,14/Jul/23 05:52,20/Apr/11 19:16,0.8.0 beta 2,,,Packaging,,,0,,,,"With bash version 4.1.5, the section of cassandra-env that tries to match the JDK distribution seems to have some kind of syntax error.  I get the following message when running bin/cassandra:

{noformat}
bin/../conf/cassandra-env.sh: 99: [[: not found
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,cywjackson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20657,,,Wed Apr 20 20:02:24 UTC 2011,,,,,,,,,,"0|i0gbpz:",93329,,thobbs,,thobbs,Low,,,,,,,,,,,,,,,,,"20/Apr/11 01:11;cywjackson;chances are the default /bin/sh is linked to dash, please run a ls -al /bin/sh to confirm

a minor fix could be remove the [[ and add """" around the variables and values.

-if [[ $java_version != \*OpenJDK\* ]]
+if [ ""$java_version"" != ""\*OpenJDK\*"" ]

a more drastic fix is to update all the /bin/sh with /bin/bash (if desired to only support on bash)

see ref: https://wiki.ubuntu.com/DashAsBinSh

suggest to be reviewed to make a decision.;;;","20/Apr/11 16:27;thobbs;/bin/sh was a link to dash, and the current script seems to work fine with bash.
{noformat}
[ ""$java_version"" != ""*OpenJDK*"" ]
{noformat} works for me in both dash and bash.;;;","20/Apr/11 16:56;thobbs;Correction: the suggested replacement does *not* seem to detect OpenJDK.;;;","20/Apr/11 17:46;cywjackson;try this:

-java_version=`java -version 2>&1`
-if [[ $java_version != *OpenJDK* ]]
+check_openjdk=$(java -version 2>&1 | awk '{if (NR == 2) {print $1}}')
+if [ ""$check_openjdk"" != ""OpenJDK"" ]

{noformat}$ bash /tmp/testjdk java
version: Java(TM)
not OpenJDK
$ dash /tmp/testjdk /usr/bin/java
version: OpenJDK
yes OpenJDK
$ cat /tmp/testjdk
check_openjdk=$($1 -version 2>&1 | awk '{if (NR ==2) {print $1}}')
echo ""version: $check_openjdk""
if [ ""$check_openjdk"" != ""OpenJDK"" ]
then
    echo ""not OpenJDK""
    JVM_OPTS=""$JVM_OPTS -javaagent:$CASSANDRA_HOME/lib/jamm-0.2.1.jar""
else
    echo ""yes OpenJDK""
fi
{noformat};;;","20/Apr/11 19:16;jbellis;committed;;;","20/Apr/11 20:02;hudson;Integrated in Cassandra-0.8 #30 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/30/])
    fix jdk verison check for sh/dash
patch by Jackson Chung; reviewed by thobbs and jbellis for CASSANDRA-2499
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI: issue with keys being interpreted as hex and causing SET statement to fail,CASSANDRA-2497,12504586,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,cdaw,cdaw,18/Apr/11 19:40,16/Apr/19 09:33,14/Jul/23 05:52,20/Apr/11 16:01,0.8.0 beta 2,,,,,,0,,,,"*Original Summary*: Issues with Update Column Family and adding a key_validation_class
_Changed summary because the issue repros on drop/create.  see comment._

*Reproduction Steps*
{code}
create column family users with comparator = UTF8Type 
and column_metadata = [{column_name: password, validation_class: UTF8Type}];

update column family users with key_validation_class=UTF8Type;

set users['jsmith']['password']='ch@ngem3';          
{code}


*EXPECTED RESULT:* After the UPDATE statement, the SET statement should go through successfully.


*ACTUAL RESULT:*  The SET statement gives the same error message, regardless of the UPDATE statement: 
{code}
org.apache.cassandra.db.marshal.MarshalException: cannot parse 'jsmith' as hex bytes
{code}


*Output from describe keyspace*
{code}
    ColumnFamily: users
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.29062499999999997/62/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name: password
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type

{code}
","* Single Node instance on MacOSX

* Nightly Compiled Build from 4/18/2011

* Installed from: https://builds.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/apache-cassandra-2011-04-18_11-02-29-bin.tar.gz",bilal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/11 22:55;xedin;CASSANDRA-2497.patch;https://issues.apache.org/jira/secure/attachment/12476663/CASSANDRA-2497.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20656,,,Fri Jul 29 02:52:34 UTC 2011,,,,,,,,,,"0|i0gbpj:",93327,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Apr/11 19:49;cdaw;This may not be related to the UPDATE.
I dropped and recreated the CF and still had the same issue.

{code}
[default@cathy] drop column family users;
72a86490-69f4-11e0-0000-242d50cf1fd4
Waiting for schema agreement...
... schemas agree across the cluster

[default@cathy] create column family users with comparator = UTF8Type and key_validation_class=UTF8Type and column_metadata = [{column_name: password, validation_class: UTF8Type}];
8a09a720-69f4-11e0-0000-242d50cf1fd4
Waiting for schema agreement...
... schemas agree across the cluster

[default@cathy] set users['jsmith']['password']='ch@ngem3';
org.apache.cassandra.db.marshal.MarshalException: cannot parse 'jsmith' as hex bytes

{code}


*Output from describe keyspace after drop/create*
{code}
    ColumnFamily: users
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.29062499999999997/62/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name: password
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type
{code};;;","18/Apr/11 21:02;jbellis;I think the cli is not reloading the CF metadata.  Try quitting and restarting the cli, after the recreate.

Pavel: we also want an ""assume"" for key types.;;;","18/Apr/11 21:53;cdaw;Restarting the CLI did not fix the issue.
Adding an assume for the key type did.

{code}
[default@cathy] set users['jsmith']['password']='ch@ngem3';
org.apache.cassandra.db.marshal.MarshalException: cannot parse 'jsmith' as hex bytes

[default@cathy] assume users keys as ascii;
Assumption for column family 'users' added successfully.

[default@cathy] set users['jsmith']['password']='ch@ngem3';
Value inserted.
{code}
;;;","18/Apr/11 22:11;xedin;I figured out that problem is in the CLI, will attach a patch tomorrow morning! (sorry for previous misleading comment);;;","20/Apr/11 00:11;jbellis;Pavel, can you describe the problem + fix?;;;","20/Apr/11 09:45;xedin;The problem was in the getKeyAsBytes method - it wasn't using information provided by cfdef.getKey_validation_class() (only comparator set by 'assume' statement or BytesType if it wasn't set). 

The fix was pretty trivial - make getKeyAsBytes use cfdef.getKey_validation_class() + printSliceList method was fixed to use getKeyComparatorForCF instead of just value from 'assume' statement.;;;","20/Apr/11 16:01;jbellis;committed, w/ the addition of ""assert defaultValidationClass != null;""
;;;","20/Apr/11 20:02;hudson;Integrated in Cassandra-0.8 #30 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/30/])
    ;;;","26/Apr/11 22:24;cdaw;Retested and verified this is fixed in current build.;;;","10/May/11 08:24;bilal;I am getting the same issue in 0.8.0.beta2 version.

set Constructors['Ferrari']['principal'] = 'Stefano Domenicali';
org.apache.cassandra.db.marshal.MarshalException: cannot parse 'Ferrari' as hex bytes

It worked after adding:
assume Constructors keys as Ascii;  ;;;","10/May/11 12:47;jbellis;That is working as designed, if you don't want to use assume you need to add a key_validation_class.;;;","10/May/11 16:51;bilal;Thanks Jonathan!

Regards,
Bilal

Sent from my iPhone


;;;","17/Jun/11 01:07;dongfan;Thanks !

I Have been very helpful;;;","05/Jul/11 06:52;mohctp;apache-cassandra-0.8.1

set Users['user1']['fname']='fname1';
rg.apache.cassandra.db.marshal.MarshalException: cannot parse 'fname' as hex bytes

assume Users keys as ascii;

no effect, set still gives the same error.;;;","05/Jul/11 09:53;xedin;As you can see from your example this is not a problem with key but rather with column name. So use `assume <cf> comparator as <type>;` or re-create your CF with a valid comparator.;;;","29/Jul/11 02:52;hollo08;Thanks !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossip should handle 'dead' states,CASSANDRA-2496,12504582,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,18/Apr/11 18:57,16/Apr/19 09:33,14/Jul/23 05:52,25/Jul/11 18:58,0.8.3,,,,,,1,,,,"For background, see CASSANDRA-2371",,jeromatron,mauzhang,scode,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-957,,,"01/Jun/11 01:39;brandon.williams;0001-Rework-token-removal-process.txt;https://issues.apache.org/jira/secure/attachment/12481028/0001-Rework-token-removal-process.txt","01/Jun/11 00:28;brandon.williams;0002-add-2115-back.txt;https://issues.apache.org/jira/secure/attachment/12481019/0002-add-2115-back.txt","14/Jul/11 20:54;thepaul;0003-update-gossip-related-comments.patch.txt;https://issues.apache.org/jira/secure/attachment/12486503/0003-update-gossip-related-comments.patch.txt","14/Jul/11 20:56;thepaul;0004-do-REMOVING_TOKEN-REMOVED_TOKEN.patch.txt;https://issues.apache.org/jira/secure/attachment/12486504/0004-do-REMOVING_TOKEN-REMOVED_TOKEN.patch.txt","14/Jul/11 20:57;thepaul;0005-drain-self-if-removetoken-d-elsewhere.patch.txt;https://issues.apache.org/jira/secure/attachment/12486505/0005-drain-self-if-removetoken-d-elsewhere.patch.txt","21/Jul/11 20:09;thepaul;0006-acknowledge-unexpected-repl-fins.patch.txt;https://issues.apache.org/jira/secure/attachment/12487346/0006-acknowledge-unexpected-repl-fins.patch.txt","23/Jul/11 00:20;brandon.williams;0007-Always-update-epstate-timestamps-when-the-node-is-al.patch;https://issues.apache.org/jira/secure/attachment/12487575/0007-Always-update-epstate-timestamps-when-the-node-is-al.patch","23/Jul/11 00:20;brandon.williams;0008-only-handleStateRemoving-if-the-node-is-a-member.patch;https://issues.apache.org/jira/secure/attachment/12487576/0008-only-handleStateRemoving-if-the-node-is-a-member.patch",,,,,,,8.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20655,,,Mon Jul 25 19:43:56 UTC 2011,,,,,,,,,,"0|i0anvr:",60142,,thepaul,,thepaul,Normal,,,,,,,,,,,,,,,,,"01/Jun/11 00:08;brandon.williams;The first patch allows gossip to track dead states and completely changes how removetoken works, since it is the problem with keeping dead state around, and currently broken in many scenarios.  The second patch adds the previously reverted CASSANDRA-2115 back now that removetoken is more resilient.;;;","01/Jun/11 16:02;brandon.williams;Some explanation of what changed and why it was necessary:

Consider nodes A through D. D is partitioned, and C is dead and needs to be removed. A removetoken will be issued to A for this.

In the current way we do things, A will modify it's own state by appending information to its status indicating that it will be removing C. B will see this, re-replicate as needed, then report to A that is is done. The problem however, is that since A modified its own state, A is also free to wipe that state out, either by restarting, or simple remove another token, because there's only space for one. If A reboots and then D's partition heals, D will never know C was removed. Worse, it will still have state for C that neither A nor B do, and so it will repopulate the ring with C again.

This patch changes this by instead having A sleep for RING_DELAY to make sure the generation for C is stable, and then it modifies C's state to indicate it is being removed, just as if C itself had done this. It also appends some extra state to indicate that A will be the removal coordinator. The others nodes see this, re-replicate and report back to A, which then modifies C's state once more to indicate it is completely removed. At this point, it doesn't matter if A dies completely and D's partition heals, since the state is stored in C's gossip information. If A reboots, it will be able to get the correct state information from B, or any other node.

If A fails while the other nodes are re-replicating, a new removetoken can be started elsewhere, or in the case of other replicas being down preventing removetoken from completing, a removetoken force will remove the node and then repair can be run to restore the replica count.;;;","14/Jul/11 17:40;brandon.williams;I see two more things to be done with this patch.  First, when re-replicating nodes report back to the removal coordinator, if the coordinator has restarted it won't understand them, and they will infinitely loop retrying the confirmation.  Second, since we're holding dead states, we need to make sure that bootstrapping/moving nodes can take over these dead tokens.;;;","14/Jul/11 20:54;thepaul;These small patches build on the others.

0003-update-gossip-related-comments.patch.txt: updates gossip-related comments derp derp.;;;","14/Jul/11 20:56;thepaul;0004-do-REMOVING_TOKEN-REMOVED_TOKEN.patch.txt: use REMOVED_TOKEN instead of STATUS_LEFT (would probably be ok either way, but otherwise, the REMOVED_TOKEN state would not be used). Seems this is more the way it was intended.;;;","14/Jul/11 20:57;thepaul;0005-drain-self-if-removetoken-d-elsewhere.patch.txt : when node X was partitioned and removetoken'd but then it shows up again, it should shut itself down, rather than becoming a zombie;;;","14/Jul/11 20:58;thepaul;I'll see what I can do to test the ""infinitely loop retrying the confirmation"" and ""bootstrapping/moving nodes can take over these dead tokens"" situations.;;;","20/Jul/11 23:48;thepaul;Ok, nodes do indeed infinitely retry the replication confirmation in some cases, but it appears it's not just when the former removal coordinator has restarted in the interim- it seems to be when the removetoken is reissued to another, new removal coordinator. In this case, I get this traceback every 10 seconds:

{noformat}
ERROR [MiscStage:9] 2011-07-20 23:42:06,599 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[MiscStage:9,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.StorageService.confirmReplication(StorageService.java:2088)
        at org.apache.cassandra.streaming.ReplicationFinishedVerbHandler.doVerb(ReplicationFinishedVerbHandler.java:38)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

I'll look into this.

Second, it seems that moving/joining nodes can take over the removed token fine, once the removetoken is complete. I haven't tried having a node take over the removed token while the removal is ongoing- I assume we can just document that that probably isn't a great idea?;;;","21/Jul/11 19:50;thepaul;0006-acknowledge-unexpected-repl-fins.patch.txt: don't assert and drop the message when we see an unexpected REPLICATION_FINISHED. Ack it instead, so the sender doesn't continually retry.;;;","21/Jul/11 20:09;thepaul;0006-acknowledge-unexpected-repl-fins.patch.txt (updated): also log at info when acknowledging the unexpected messages;;;","21/Jul/11 20:52;thepaul;ok, +1 with these patches.;;;","23/Jul/11 00:20;brandon.williams;0007 handles problems when a node has been down longer than aVeryLongTime, and ensures that we advertise the new token states long enough.

0008 makes sure that SS only get involved with removal if the token is a member.;;;","25/Jul/11 18:33;thepaul;+1;;;","25/Jul/11 18:58;brandon.williams;Committed.;;;","25/Jul/11 19:43;hudson;Integrated in Cassandra-0.8 #238 (See [https://builds.apache.org/job/Cassandra-0.8/238/])
    Gossip handles dead states, token removal actually works, gossip states
are held for aVeryLongTime.
Patch by brandonwilliams and Paul Cannon, reviewed by Paul Cannon for
CASSANDRA-2496.

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1150847
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/VersionedValue.java
* /cassandra/branches/cassandra-0.8/NEWS.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/HeartBeatState.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/ApplicationState.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Quorum reads are not monotonically consistent,CASSANDRA-2494,12504486,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,sbridges,sbridges,17/Apr/11 14:29,04/Dec/19 19:38,14/Jul/23 05:52,11/Aug/11 19:29,1.0.0,,,,,,2,,,,"As discussed in this thread,

http://www.mail-archive.com/user@cassandra.apache.org/msg12421.html

Quorum reads should be consistent.  Assume we have a cluster of 3 nodes (X,Y,Z) and a replication factor of 3. If a write of N is committed to X, but not Y and Z, then a read from X should not return N unless the read is committed to at  least two nodes.  To ensure this, a read from X should wait for an ack of the read repair write from either Y or Z before returning.

Are there system tests for cassandra?  If so, there should be a test similar to the original post in the email thread.  One thread should write 1,2,3... at consistency level ONE.  Another thread should read at consistency level QUORUM from a random host, and verify that each read is >= the last read.",,aaaron,cherro,donnchadh,doubleday,hanzhu,jay.zhuang,jeromatron,jjordan,scode,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15442,,,,,,,,"03/Aug/11 16:34;jbellis;2494-v2.txt;https://issues.apache.org/jira/secure/attachment/12489205/2494-v2.txt","27/Jul/11 01:46;jbellis;2494.txt;https://issues.apache.org/jira/secure/attachment/12487930/2494.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20654,,,Thu Sep 24 20:45:58 UTC 2015,,,,,,,,,,"0|i0gbp3:",93325,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"17/Apr/11 21:01;scode;As far as I can tell the consistency being asked for was never promised by Cassandra is in fact not expected.

The expected behavior of writes is that they propagate; the difference between ONE and QUORUM is just how many are required to receive a write prior to a return to the client with a successful error code. For reads, that means you may get lucky at ONE or you may get lucky at QUORUM; the positive guarantee is in the case of a *completing* QUORUM write followed by a QUORUM read.

So just to be clear, although I don't think this is what is being asked for: As far as I know, it has never been the case, nor the intent to promise, that a write which fails is guaranteed not to eventually complete. Simply ""fixing"" reads is not enough; by design the data will be replicated during read-repair and AES - this is how consistency is achieved in Cassandra.

However, it sounds like what is being asked for is not that they don't propagate in the event of a write ""failure"", but just that reads don't see the writes until they are sufficiently propagated to guarantee that any future QUORUM read will also see the data. I can understand that is desirable, in the sense of achieving monotonically forward-moving data as the benchmark/test from the e-mail thread does. Another way to look at is that maybe you never want to read data successfully prior to achieving a certain level of replication, in order to avoid a client ever seeing data that may suddenly go away due to e.g. a node failure in spite of said failure not exceeding the number of failures the cluster was designed to survive.

So the key point would be the bit about guaranteeing that any ""future QUORUM read will see the data or data subsequently overwritten"", and actively read-repairing and waiting for it to happen would take care of that. It would be important to ensure that the act of ensuring a quorum of nodes have seen the data is the important part; one should not await for a quorum to agree on the *current* version of the data as that would create potentially unbounded round-trips on hotly contended data.

Thing to consider: One might think about cases where read-repair is currently not done, like range slices, and how an implementation that requires read repair for consistency affects that.

;;;","18/Apr/11 04:20;sbridges;Peter Shuller wrote,

""However, it sounds like what is being asked for is not that they don't propagate in the event of a write ""failure"", but just that reads don't see the writes until they are sufficiently propagated to guarantee that any future QUORUM read will also see the data.""

Yes, that is the issue.  The comment in the bug about writing at ONE and reading at QUORUM is just a way of testing this new guarantee in a distributed test, if Cassandra has those.;;;","18/Apr/11 20:41;jjordan;I would think that reads at QUORUM should never go backwards.  Even if the Write was at ZERO.  If there were writes to the cluster of a=1 time=5, a=2 time=10, a=3 time=15, and I do a read at QUORUM which tells me a=3 time=15, I should not be able to do another read at QUORUM and get a=2 time=10.;;;","22/Apr/11 05:17;stuhood;W plus R must be _greater than_ N for consistency.

EDIT: And adding a blocking implicit write step to QUORUM reads by waiting for read repair is not reasonable.;;;","22/Apr/11 08:26;scode;I don't think anyone is claiming otherwise, unless I'm misunderstanding. The problem is that while the ""if sucessfully written to quorum, subsequent quorum reads will see it"" guarantee is indeed maintained, it is possible for quorum reads to see data go backwards (on a timeline) in the event of a *failed* attempted quorum write. This includes the possibility of reads seeing data that then permanently vanishes, even though you only lost say 1 node that you designed your cluster for surviving (RF >= 3, QUORUM). (""lost 1 node"" can be substituted with ""killed 1 node in periodic commit mode"")

I still don't think this is a violation of what was promised, but I can see how making the further guarantee would make for more useful consistency semantics in some cases.

With respect to implicit write: An alternative is to adjust reconciliation logic when applied as part of reads (as opposed to AES,  hinted hand-off, writes) to take consistency level into account and only consider columns whose timestamp is >= the greatest timestamp that has quorum (off the top of my head I think that should be correct in call cases, but I didn't think this through terribly).
;;;","22/Apr/11 08:34;scode;Ok, so my last suggestion is in fact broken. A counter example is:

 A: column @ t1
 B: column @ t2
 C: column @ t3

If A + B is participating, A's column @ t1 has timestamp quorum and would be selected. If B + C is participating, B's column is picked. Thus, a read where B + C participates will see data that will be reverted once A + B happens to be picked.

Note to self: Think before posting.
;;;","22/Apr/11 14:45;sbridges;I think the guarantee of quorum reads not seeing old writes once a quorum read sees a new write is  very useful.  I suspect most people already think that this guarantee occurs, including, it seems, Jonathan Ellis whose quote can be found in the email thread linked to in the bug,

""The important guarantee this gives you is that once one quorum read sees the new value, all others will too.   You can't see the newest version, then see an older version on a subsequent write [sic, I
assume he meant read], which is the characteristic of non-strong consistency""



;;;","22/Apr/11 15:01;slebresne;The problem is you are considering the consistency of reads but not write. The guarantee is: ""quorum reads will not see old quorum write once a quorum read sees a new quorum"". Period. I you don't consider the consistency of a write, consider the case of a CL.ANY write. In this case, the update may not be at all on any replica. How can we ensure the quorum read property that you want ? We query all nodes for quorum reads in case there is an hint somewhere ?

If you look at the Consistency part of http://wiki.apache.org/cassandra/ArchitectureOverview, it seems to me that it is pretty clear that the consistency of reads *and* writes is involved to achieve strong consistency. So I would hope 'most people' are aware of that.;;;","22/Apr/11 15:11;scode;The issue is that of *failed* QUORUM writes. I.e., you design your system to use QUORUM writes and QUORUM reads, and expect that once a QUORUM read sees a given piece of data a subsequent QUORUM read will also see it (or a later data). A *failed* QUORUM write that was replicated to less than a QUORUM would be visible as part of QUORUM reads that happen to touch one of those replicas, but there is no guarantee that subsequent reads see it.

I was under the impression this was never an intended guarantee. Apparently I may be wrong about that given the jbellis quote above. In either case, it is certainly not an *actual* guarantee given by the current implementation.

The guarantee that a *successful* QUORUM write is seen by a subsequent QUORUM read is, as far as I can tell, not in question here.

;;;","22/Apr/11 15:22;sbridges;To be clear, this is a new guarantee.  The current guarantee is R+W>N gives you consistency.  This bug is asking that a successful quorum read of A means that A has been committed to a quorum of nodes.

""How can we ensure the quorum read property that you want ?""

If when reading at quorum, and no quorum can be found which agrees on a particular value, then the coordinator ( ? ) will wait for acks of read repair writes (or perhaps just do normal writes) to be returned from a sufficient number of nodes to ensure that the value has been committed to a quorum of nodes.

Without this new guarantee it is hard for readers to function correctly.  The reader does not know that the quorum write failed, or is still in progress, so without reading at ALL, the R+W>N guarantee does not help the reader.



;;;","27/Jul/11 01:46;jbellis;I don't see any reason not to guarantee that the replicas we read from provide monotonic read consistency.

Patch attached to do this.
;;;","27/Jul/11 11:08;slebresne;Ok, I now see what you mean :)
Makes perfect sense.

Comments on the patch:
* There is a number of case where scheduleRepairs may not have been called (if the read for repair timeout and/or we had no or only 1 response or we have the situation were removeDeleted removes everything), so repairResults will be null in those cases. In SP, we should check for it.
* That new wait can extend the rpc timeout to almost twice what it should be. I agree that it is not a huge deal, but by exposing the 'startTime' stored in RepairCallback we can make it so we don't extend it that way.
* Shouldn't we give the same love to range requests, now that we do repairs there too ?;;;","03/Aug/11 16:34;jbellis;bq. There is a number of case where scheduleRepairs may not have been called

defaulted repairResults to emptyList.

bq. That new wait can extend the rpc timeout to almost twice what it should be

Well, sort of -- rpctimeout is working exactly as intended, i.e., to prevent waiting indefinitely for a node that died after we sent it a request.  Treating it as ""max time to respond to client"" has never really been correct.  (E.g., in the CL > ONE case we can already wait up to rpctimeout twice, one for the original digest read set, and again for the data read after mismatch.)  So I don't think we should try to be clever with that here.

bq. Shouldn't we give the same love to range requests, now that we do repairs there too

done.;;;","11/Aug/11 15:34;slebresne;bq. Well, sort of – rpctimeout is working exactly as intended, i.e., to prevent waiting indefinitely for a node that died after we sent it a request. Treating it as ""max time to respond to client"" has never really been correct. (E.g., in the CL > ONE case we can already wait up to rpctimeout twice, one for the original digest read set, and again for the data read after mismatch.) So I don't think we should try to be clever with that here.

Fair enough. It would probably be useful to make rpctimeout meaning closer to ""max time to respond to client"". Created CASSANDRA-3018 for that though.

+1 on v2.;;;","11/Aug/11 19:29;jbellis;committed;;;","11/Aug/11 20:14;hudson;Integrated in Cassandra #1017 (See [https://builds.apache.org/job/Cassandra/1017/])
    provide monotonic read consistency
patch by jbellis; reviewed by slebresne for CASSANDRA-2494

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1156758
Files : 
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageProxy.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/RepairCallback.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/RowRepairResolver.java
* /cassandra/trunk/src/java/org/apache/cassandra/utils/FBUtilities.java
* /cassandra/trunk/src/java/org/apache/cassandra/service/RangeSliceResponseResolver.java
;;;","24/Sep/15 20:45;aaaron;The relevant code in the patch has changed significantly. Is the monotonic read consistency guarantee still provided?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL does not preserve column order in select statement,CASSANDRA-2493,12504467,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,17/Apr/11 03:36,16/Apr/19 09:33,14/Jul/23 05:52,17/Apr/11 05:10,0.8 beta 1,,,Legacy/CQL,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/11 04:44;jbellis;2493.txt;https://issues.apache.org/jira/secure/attachment/12476548/2493.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20653,,,Sun Apr 17 06:24:46 UTC 2011,,,,,,,,,,"0|i0gbov:",93324,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"17/Apr/11 03:48;jbellis;patch to preserve column order.  columns that do not exist in a given row come back as null; this requires changing value and ts fields in thrift Column struct to optional. manual checking for set-ness is added to ThriftValidation to make up for this.;;;","17/Apr/11 03:53;jbellis;updated patch fixes uses of Column constructor that no longer exists;;;","17/Apr/11 04:44;jbellis;new patch also fixes bugs in CassandraResultSet & updates jdbc tests for new behavior;;;","17/Apr/11 04:55;urandom;lgtm. +1;;;","17/Apr/11 05:10;jbellis;committed;;;","17/Apr/11 06:24;hudson;Integrated in Cassandra-0.8 #12 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/12/])
    preserve column order in CQL result sets
patch by jbellis; reviewed by eevans for CASSANDRA-2493
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add an escapeSQLString function and fix unescapeSQLString,CASSANDRA-2492,12504465,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,17/Apr/11 02:37,16/Apr/19 09:33,14/Jul/23 05:52,19/Apr/11 13:42,0.7.5,,,,,,0,,,,"CliUtils.unescapeSqlString repeats the escape character e.g. 
{noformat}""my \\t tab"" becomes ""my \tt""{noformat}
because {{i}} is not bumped when an escape is processed.
 
Also for Cassandra-2221 I need a function to escape strings back so they will work if processed by the cli again. 

There are a number of non [standard escapes|http://java.sun.com/docs/books/jls/second_edition/html/lexical.doc.html#101089] which I assume is a hang over from is original source https://github.com/apache/cassandra/blob/1aeca2b6257b0ad6680080b1756edf7ee9acf8c8/src/java/org/apache/cassandra/cli/CliUtils.java

Will change to use the [StringEscapeUtils|http://commons.apache.org/lang/api-2.5/org/apache/commons/lang/StringEscapeUtils.html] class  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/11 08:24;amorton;0001-use-StringEscapeUtils-to-escape-and-unescape.patch;https://issues.apache.org/jira/secure/attachment/12476699/0001-use-StringEscapeUtils-to-escape-and-unescape.patch",,,,,,,,,,,,,,1.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20652,,,Tue Apr 19 13:42:27 UTC 2011,,,,,,,,,,"0|i0gbon:",93323,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"19/Apr/11 08:24;amorton;Attached patch to use StringEscapeUtils to escape and unescape cli strings, includes unit test.;;;","19/Apr/11 13:42;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DatabaseDescriptor.defsVersion should be volatile,CASSANDRA-2490,12504433,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,paladin8,paladin8,paladin8,16/Apr/11 04:57,16/Apr/19 09:33,14/Jul/23 05:52,16/Apr/11 16:15,0.7.5,,,,,,0,,,,"(Probably affects other versions, but I am on 0.7.3).

DatabaseDescriptor.defsVersion should be protected by volatile since it is written to and read by multiple threads from unsynchronized methods. This can manifest itself in schema agreement never occurring due to a node broadcasting the wrong schema version.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,paladin8,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20651,,,Sat Apr 16 16:36:47 UTC 2011,,,,,,,,,,"0|i0gbo7:",93321,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Apr/11 16:14;jbellis;committed volatile change in r1094011. thanks!;;;","16/Apr/11 16:36;hudson;Integrated in Cassandra-0.7 #435 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/435/])
    make DD.defsVersion volatile
patch by Jeffrey Wang; reviewed by jbellis for CASSANDRA-2490
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh errors on comments that end with a semicolon,CASSANDRA-2488,12504418,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,angryparsley,angryparsley,15/Apr/11 22:25,20/Aug/20 12:03,14/Jul/23 05:52,19/Apr/11 17:54,0.8 beta 1,,,Legacy/Tools,,,0,cql,,,"Commented-out lines that end in a semicolon cause an error.

Examples:

cqlsh> -- CREATE KEYSPACE ELE WITH replication_factor = 3 AND strategy_class = SimpleStrategy AND strategy_options:replication_factor=3;
Bad Request: line 0:-1 no viable alternative at input '<EOF>'
cqlsh> -- CREATE KEYSPACE ELE WITH replication_factor = 3 AND strategy_class = SimpleStrategy AND strategy_options:replication_factor=3
   ... 
   ... 
   ... ;
Bad Request: line 2:0 no viable alternative at input ';'
cqlsh> -- ;
Bad Request: line 0:-1 no viable alternative at input '<EOF>'
cqlsh> --;
Bad Request: line 0:-1 no viable alternative at input '<EOF>'

As long as there's a line with valid CQL before the semicolon, things work fine though.

I'm pretty sure the problem is on line 75 of cqlsh:
        if not line.endswith("";""):
            self.set_prompt(Shell.continue_prompt)
            return None

A quick workaround would be to kill the pretty continue prompt. A more involved fix would detect whether or not the semicolon was in a comment. This is harder than it sounds, since /* and */ allow multi-line comments.","OS X 10.6.7

$ java -version
java version ""1.6.0_24""
Java(TM) SE Runtime Environment (build 1.6.0_24-b07-334-10M3326)
Java HotSpot(TM) 64-Bit Server VM (build 19.1-b02-334, mixed mode)

(This stuff isn't really important. It's a bug in a Python script)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15802,,,,,,,,"17/Apr/11 02:28;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2488-teach-cqlsh-to-ignore-comments.txt;https://issues.apache.org/jira/secure/attachment/12476545/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2488-teach-cqlsh-to-ignore-comments.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20650,,,Tue Apr 19 19:05:56 UTC 2011,,,,,,,,,,"0|i0gbnr:",93319,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"17/Apr/11 02:19;urandom;the attached patch should deal with any supported comment;;;","19/Apr/11 17:41;gdusbabek;+1;;;","19/Apr/11 17:54;urandom;committed;;;","19/Apr/11 19:05;hudson;Integrated in Cassandra-0.8 #22 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/22/])
    teach cqlsh to ignore comments

Patch by eevans; reviewed by gdusbabek for CASSANDRA-2488
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig example script no longer working,CASSANDRA-2487,12504385,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,15/Apr/11 16:34,16/Apr/19 09:33,14/Jul/23 05:52,29/Apr/11 19:28,0.7.6,,,,,,0,hadoop,pig,,"There is a strange error given when trying to run the example-script.pig.

java.io.IOException: Type mismatch in key from map: expected org.apache.pig.impl.io.NullableBytesWritable, recieved org.apache.pig.impl.io.NullableText
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:870)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:573)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:116)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:238)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:646)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:322)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)

Looks like it has to do with PIG-919 and PIG-1277.  For now we can just cast the var as a chararray and it works though.  Will attach a patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/11 16:36;jeromatron;2487.txt;https://issues.apache.org/jira/secure/attachment/12476461/2487.txt",,,,,,,,,,,,,,1.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20649,,,Fri Apr 29 19:45:49 UTC 2011,,,,,,,,,,"0|i0gbnj:",93318,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"15/Apr/11 16:36;jeromatron;Did the workaround they use in PIG-919 by casting as a chararray for now.  Also put the schema in the load.  Made the name of the keyspace and column family not like the old 0.6 stuff.  Also updated the readme a bit and included an example of setting env vars for running locally since a FAQ.;;;","29/Apr/11 19:28;brandon.williams;Thanks, committed.;;;","29/Apr/11 19:45;hudson;Integrated in Cassandra-0.7 #465 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/465/])
    Update pig example script to work again.
Patch by Jeremy Hanna, reviewed by brandonwilliams for CASSANDRA-2487
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
preserve KsDef backwards compatibility for Thrift clients,CASSANDRA-2486,12504380,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,15/Apr/11 15:07,16/Apr/19 09:33,14/Jul/23 05:52,18/Apr/11 15:51,0.8 beta 1,,,Legacy/CQL,,,0,,,,"CASSANDRA-1263 broke client compatibility; we can't preserve it entirely (we'll continue to resturn replication_factor in strategy option rather than try to guess somehow if client is an old one) but we can accommodate old clients on write and leave the KsDef signature compatible which (I think) will make it easier for client authors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/11 15:08;jbellis;2486.txt;https://issues.apache.org/jira/secure/attachment/12476456/2486.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20648,,,Mon Apr 18 15:51:42 UTC 2011,,,,,,,,,,"0|i0gbnb:",93317,,zznate,,zznate,Normal,,,,,,,,,,,,,,,,,"15/Apr/11 15:08;jbellis;patch attached.;;;","15/Apr/11 18:00;zznate;Patch applies clean (provided thrift classes are rebuilt). Unit tests pass as do hector's system_*_keyspace tests.;;;","16/Apr/11 01:29;jbellis;The compatibility story doesn't change substantially here -- either way you can do the 99% of operations that don't deal w/ schema changes. But does this make it easier to build a Hector compatible w/ both 0.7 and 0.8 (by looking at the server api version), since you can use the same generated Thrift now?

If so then I'll go ahead and commit.;;;","18/Apr/11 15:51;jbellis;committed, with some fixes to the python tests;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage LoadPushDown implementation causes heisenbugs,CASSANDRA-2484,12504329,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jeromatron,jeromatron,jeromatron,15/Apr/11 00:49,16/Apr/19 09:33,14/Jul/23 05:52,17/Apr/11 21:11,0.7.5,,,,,,0,hadoop,pig,,"After pulling hair out about why weird errors were happening loading data from cassandra with seemingly irrelevant changes to the pig scripts (mostly changing the script trying to debug other problems), it looks like the weird errors were because of the implementation we currently have for LoadPushDaown in CassandraStorage.  Unless there is a good reason to implement it, I feel like we should just remove the few lines that are in there until we can spend some serious time doing an implementation of it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/11 18:51;jeromatron;2484-trunk.txt;https://issues.apache.org/jira/secure/attachment/12476645/2484-trunk.txt","15/Apr/11 00:55;jeromatron;2484.txt;https://issues.apache.org/jira/secure/attachment/12476399/2484.txt",,,,,,,,,,,,,2.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20647,,,Mon Apr 18 18:51:21 UTC 2011,,,,,,,,,,"0|i0gbn3:",93316,,,,,Normal,,,,,,,,,,,,,,,,,"15/Apr/11 00:55;jeromatron;Removed LoadStoreFunc because it was causing heisenbugs.  Also updated JavaDocs a bit.;;;","15/Apr/11 20:01;brandon.williams;Can you provide the errors you encountered?  At the least, it seems like RequiredFieldResponse should essentially be a no-op.;;;","15/Apr/11 20:09;jeromatron;What would happen was we would be troubleshooting a bug in a complicated script and if we took out part of it we would get errors with isEmpty (something we were using but unrelated at all to the change) and then investigate more and Cassandra wouldn't return anything at all but only with that particular portion of the script running.  Then someone else was going through another kind of complicated script and getting odd null pointer exceptions and traced it back to something similar.  That made us think that pig was trying to ""optimize"" or something behind the scenes.  Then we thought it might be trying to project certain data out of the column family based on the changed script.  That led us to wonder if it had something to do with the LoadPushDown - since that is called when pig thinks it can project things out of a data store/file if it doesn't need the rest.  That would explain some of the odd behavior at least.  We commented out it out like in the patch and both errors that were independent and unexplainable any other way, were gone.

I know that's kind of a round about way of going about it, but it seems like good evidence to me that something is up with it - and if it's essentially a no-op, and it works without it, then I didn't see why we wouldn't just take it out.;;;","17/Apr/11 21:11;brandon.williams;Committed.;;;","17/Apr/11 21:37;hudson;Integrated in Cassandra-0.7 #436 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/436/])
    Remove LoadPushDown methods from pig storage.
Patch by Jeremy Hanna, reviewed by brandonwilliams for CASSANDRA-2484
;;;","18/Apr/11 18:51;jeromatron;Attaching a patch for trunk - not sure why the other one didn't apply.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stress.java fails to run if the keyspace already exists,CASSANDRA-2483,12504322,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,nickmbailey,nickmbailey,14/Apr/11 23:00,16/Apr/19 09:33,14/Jul/23 05:52,04/May/11 21:16,0.8.0,,,Legacy/Tools,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20646,,,Wed May 04 21:16:02 UTC 2011,,,,,,,,,,"0|i0gbmv:",93315,,,,,Low,,,,,,,,,,,,,,,,,"04/May/11 21:16;nickmbailey;Someone already fixed this in the 0.8 branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make compaction type an enum,CASSANDRA-2482,12504311,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,nickmbailey,nickmbailey,nickmbailey,14/Apr/11 19:33,16/Apr/19 09:33,14/Jul/23 05:52,14/Apr/11 21:37,0.8 beta 1,,,,,,0,,,,"Compaction type should be an enum, half of the places we set it we use it as a message and include the keyspace/cf although that is already included in the compaction info object.

I realize this is minor and a pedantic but from the standpoint of someone writing a monitoring application its kind of annoying.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/11 21:11;nickmbailey;0001-Make-compactiontype-an-enum.patch;https://issues.apache.org/jira/secure/attachment/12476372/0001-Make-compactiontype-an-enum.patch",,,,,,,,,,,,,,1.0,nickmbailey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20645,,,Thu Apr 14 22:19:45 UTC 2011,,,,,,,,,,"0|i0gbmn:",93314,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"14/Apr/11 21:33;jbellis;committed with a couple fixes:

 - row cache save uses correct enum
 - CompactionInfo.getTaskType returns enum instead of string so callers don't have to do string inspection anymore;;;","14/Apr/11 22:19;hudson;Integrated in Cassandra-0.8 #7 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/7/])
    madeCompactionInfo.getTaskType return an enum
patch by nickmbailey; reviewed by jbellis for CASSANDRA-2482
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C* .deb installs C* init.d scripts such that C* comes up before mdadm and related,CASSANDRA-2481,12504307,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thepaul,mdennis,mdennis,14/Apr/11 18:51,16/Apr/19 09:33,14/Jul/23 05:52,23/May/11 15:45,0.7.6,0.8.0,,Packaging,,,0,,,,the C* .deb packages install the init.d scripts at S20 which is before mdadm and various other services.  This means that when a node reboots that C* is started before the RAID sets are up and mounted causing C* to think it has no data and attempt bootstrapping again.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/11 16:44;thepaul;2481-fix.txt;https://issues.apache.org/jira/secure/attachment/12479618/2481-fix.txt","12/May/11 18:26;thepaul;2481.txt;https://issues.apache.org/jira/secure/attachment/12479001/2481.txt",,,,,,,,,,,,,2.0,thepaul,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20644,,,Fri May 20 16:51:36 UTC 2011,,,,,,,,,,"0|i0gbmf:",93313,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"18/Apr/11 16:26;hudson;Integrated in Cassandra-0.8 #14 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/14/])
    add optional replication_factor fields to KsDef to make supporting both 0.8 and 0.7 easier for client devs
patch by jbellis; reviewed by Nate McCall for CASSANDRA-2481
;;;","12/May/11 18:27;thepaul;patch solverizes this problem. uses priority 50, or requires mdadm to be started first if using dependency-based init ordering.;;;","12/May/11 19:27;jbellis;Out of curiosity, why aren't start/stop symmetrical?

{noformat}
+	dh_installinit -u'start 50 2 3 4 5 . stop 50 0 1 6'
{noformat};;;","12/May/11 20:05;thepaul;bq. Out of curiosity, why aren't start/stop symmetrical?

they are: ((start = 50) == 100 - (end = 50))

If you were referring to the ""2 3 4 5"" vs ""0 1 6"", those are the standard ""starting stuff"" and ""stopping stuff"" runlevels for SysV init.;;;","12/May/11 20:46;brandon.williams;Committed.;;;","12/May/11 21:10;hudson;Integrated in Cassandra-0.7 #484 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/484/])
    Start/stop cassandra after more important services such as mdadm in
debian packaging.
Patch by Paul Cannon, reviewed by brandonwilliams for CASSANDRA-2481
;;;","18/May/11 10:05;slebresne;When installing the debian package for 0.7.6 and 0.8.0-rc1 on ubuntu 11.04 (natty), I get
{noformat}
Installing new version of config file /etc/init.d/cassandra ...
update-rc.d: error: start|stop arguments not terminated by "".""
usage: update-rc.d [-n] [-f] <basename> remove
       update-rc.d [-n] <basename> defaults [NN | SS KK]
       update-rc.d [-n] <basename> start|stop NN runlvl [runlvl] [...] .
       update-rc.d [-n] <basename> disable|enable [S|2|3|4|5]
		-n: not really
		-f: force
{noformat}

Given that it works like a charm with 0.7.5, I strongly suspect this is this patch doing.;;;","18/May/11 15:32;gasolwu;{quote}
Installing new version of config file /etc/init.d/cassandra ...
update-rc.d: error: start|stop arguments not terminated by "".""
{quote}

find dh_installinit in debian/rules and edit like following line.

dh_installinit -u'start 50 2 3 4 5 . stop 50 0 1 6 .'

it works for me.;;;","18/May/11 16:44;thepaul;fix for previous;;;","18/May/11 17:10;brandon.williams;Fix committed.;;;","18/May/11 17:48;hudson;Integrated in Cassandra-0.7 #488 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/488/])
    Fix for dh_installinit syntax for CASSANDRA-2481
Patch by Paul Cannon, reviewed by brandonwilliams

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1124338
Files : 
* /cassandra/branches/cassandra-0.7/debian/rules
;;;","20/May/11 15:24;sword2;This is still broken for 0.8.0~rc1.  The last commit to it shows a merge with 0.7 but this fix is not in it.

quick workaround is to modify the cassandra.postinst file.  You can get that from dpkg-query -c cassandra, mine is under /var/lib/dpkg/info/:
{noformat}
sudo cp /var/lib/dpkg/info/cassandra.postinst /var/lib/dpkg/info/cassandra.postinst.original
sudo sed -i 's/50 2 3 4 5 \. stop 50 0 1 6 >/50 2 3 4 5 \. stop 50 0 1 6 \.>/' /var/lib/dpkg/info/cassandra.postinst
#make sure it's correct then run the below line.
#sudo dpkg --configure cassandra
{noformat}

;;;","20/May/11 16:51;jbellis;the fix is in the 0.8.0 branch for -final, but you are right that it is not in rc1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up after failed compaction,CASSANDRA-2468,12504212,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,amorton,jbellis,jbellis,13/Apr/11 21:05,16/Apr/19 09:33,14/Jul/23 05:52,23/Jun/11 05:49,1.0.0,,,,,,1,,,,(Started in CASSANDRA-2088.),,skamio,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2602,,CASSANDRA-2576,CASSANDRA-2629,,,,,,,,,,"22/Jun/11 02:03;stuhood;0001-CASSANDRA-2468-clean-up-temp-files-after-failed-compac.txt;https://issues.apache.org/jira/secure/attachment/12483390/0001-CASSANDRA-2468-clean-up-temp-files-after-failed-compac.txt","06/Jun/11 01:59;amorton;0001-clean-up-temp-files-after-failed-compaction-v08-2.patch;https://issues.apache.org/jira/secure/attachment/12481527/0001-clean-up-temp-files-after-failed-compaction-v08-2.patch","08/Jun/11 10:11;amorton;0001-clean-up-temp-files-after-failed-compaction-v08-3.patch;https://issues.apache.org/jira/secure/attachment/12481796/0001-clean-up-temp-files-after-failed-compaction-v08-3.patch","06/May/11 02:57;amorton;0001-clean-up-temp-files-after-failed-compaction-v08.patch;https://issues.apache.org/jira/secure/attachment/12478361/0001-clean-up-temp-files-after-failed-compaction-v08.patch","06/May/11 02:57;amorton;0001-cleanup-temp-files-after-failed-compaction-v07.patch;https://issues.apache.org/jira/secure/attachment/12478360/0001-cleanup-temp-files-after-failed-compaction-v07.patch",,,,,,,,,,5.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20642,,,Thu Jun 23 07:09:36 UTC 2011,,,,,,,,,,"0|i0gbjz:",93302,,stuhood,,stuhood,Low,,,,,,,,,,,,,,,,,"29/Apr/11 14:01;slebresne;@Aaron: are you still working on this ? It would be nice to fix this quickly. And we should probably target 0.7 for that one.;;;","29/Apr/11 21:51;amorton;should have time this weekend. ;;;","01/May/11 11:16;amorton;Attached patches for 0.7 and 0.8 rely on CASSANDRA-2588 to correctly detect sstables on disk. 

Added closeAndDeleteQuietly() to SSTableWriter and SSTableWriter.Builder to close open files and delete the SSTable files found on disk.

Checks for failures in CompactionManager doCompaction(), doScrub() and doCleanupCompaction() and submitSSTableBuild() 

Does not check in CompactionManager.submitIndexBuild() because the builder works against memtables and does not use an SSTableWriter. 

Checks for failures in Memtable.writeSortedContents();;;","03/May/11 09:59;slebresne;Comment:
    * I'm uncomfortable with having closeAndDeleteQuietly() delete the non tmp files (those really do not belong to the writer). Sure the calls are always conditioned so that we shouldn't messed up, but I'm not sure it's worth the risk of foot-shooting (it makes the function harder to use). I'd rather remove that part, rename closeAndDeleteQuietly() to something like cleanupIfNecessary() and call it every time in the finally block (which would remove the need to check if the reader was successfully created every damn time).
    * Nitpick: I'd prefer moving the code for SSTableWriter.Builder inside build() itself.
;;;","03/May/11 11:50;amorton;The check to delete the non temp files was to cover the unlikely event that rename() threw an IOError part way through renaming the files. I thought about potentially deleting the wrong files but nothing else see the non temp files until the SSTR is returned. 

Happy to make the change tomorrow if you still want, it makes things a little safer. ;;;","03/May/11 12:05;slebresne;bq. Happy to make the change tomorrow if you still want, it makes things a little safer.

Thanks, I prefer safer :) (I'm not too worried about rename failing);;;","04/May/11 19:42;amorton;Updated patches with suggested changes:
- only deletes temp files
- cleanupIfNecessary() used
- SSTW.Builder.build() cleans up self

NOTE: requires CASSANDRA-2602 to detect temp files correctly.;;;","05/May/11 13:59;slebresne;Looks good but this needs rebasing (at least for 0.7, don't bother too much with 0.8, I'll try to merge it unless this is a pain);;;","06/May/11 02:57;amorton;rebased both the v07 and v08;;;","02/Jun/11 19:18;stuhood;What's shakin' here?;;;","06/Jun/11 01:59;amorton;rebased the v08 version today, attached as 0001-clean-up-temp-files-after-failed-compaction-v08-2

Have not updated v07, let me know if you need it.  ;;;","06/Jun/11 18:52;stuhood;* SSTable.delete will throw an IOError on IOException, which might kill cleanupIfNecessary... should we consider having delete throw IOException? I'd prefer not to catch IOError.
* {{SSTable.tempComponentsFor}} could probably be merged with componentsFor and an enum {{LIVE}}, {{TEMP}} or {{LIVE | TEMP}}? Not a blocker, just sugar.

After the IOError is fixed, +1 from me. Thanks Aaron!;;;","06/Jun/11 19:00;jbellis;bq. SSTable.tempComponentsFor could probably be merged with componentsFor

I prefer the distinct, flag-less methods.;;;","06/Jun/11 19:43;stuhood;bq. I prefer the distinct, flag-less methods.
componentsFor already has a boolean flag... my suggestion was to make it a ternary flag. Again, no big deal.;;;","06/Jun/11 20:34;jbellis;Eh, you're right.  Then flags is probably better than flags AND multiple methods. :);;;","08/Jun/11 10:11;amorton;version 3 for v0.8 modified SSTable.delete() to raise an IOException so cleanupIfNecessary() can catch it. Also changes componentsFor to accept an enum. 

Do we want this in 0.7?;;;","10/Jun/11 18:20;stuhood;+1 on the patch for 0.8/trunk
Thanks Aaron!;;;","12/Jun/11 06:29;stuhood;Rebased for post-1610-trunk as v4.;;;","22/Jun/11 02:03;stuhood;Rebased for trunk (assuming r1138084 is reverted).;;;","23/Jun/11 05:49;jbellis;committed, thanks all!;;;","23/Jun/11 07:09;hudson;Integrated in Cassandra #936 (See [https://builds.apache.org/job/Cassandra/936/])
    clean up tmpfiles after failed compaction
patch by Aaron Morton; reviewed by slebresne and Stu Hood for CASSANDRA-2468

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1138740
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTable.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionTask.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/Memtable.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/Descriptor.java
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/io/util/FileUtils.java
* /cassandra/trunk/test/unit/org/apache/cassandra/io/sstable/SSTableTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableDeletingReference.java
* /cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
key validator not getting set when adding a keyspace,CASSANDRA-2467,12504200,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,13/Apr/11 19:05,16/Apr/19 09:33,14/Jul/23 05:52,15/Apr/11 12:54,0.8 beta 1,,,,,,0,,,,needs to be applied CassandraServer.convertToCFMetaData(),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2311,,,,,,"13/Apr/11 19:19;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-set-requested-key-validator-when-creating-a-keyspace.txt;https://issues.apache.org/jira/secure/attachment/12476274/ASF.LICENSE.NOT.GRANTED--v1-0001-set-requested-key-validator-when-creating-a-keyspace.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20641,,,Fri Apr 15 12:54:19 UTC 2011,,,,,,,,,,"0|i0gbjr:",93301,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"13/Apr/11 19:32;gdusbabek;todo: we could use a system test to verify that non-default CF and KS options are actually applied.;;;","14/Apr/11 20:30;jbellis;+1;;;","15/Apr/11 12:54;gdusbabek;committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bloom filters should avoid huge array allocations to avoid fragmentation concerns,CASSANDRA-2466,12504130,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,m0nstermind,scode,scode,13/Apr/11 03:45,16/Apr/19 09:33,14/Jul/23 05:52,26/Oct/11 19:34,1.0.1,,,,,,0,,,,"The fact that bloom filters are backed by single large arrays of longs is expected to interact badly with promotion of objects into old gen with CMS, due to fragmentation concerns (as discussed in CASSANDRA-2463).

It should be less of an issue than CASSANDRA-2463 in the sense that you need to have a lot of rows before the array sizes become truly huge. For comparison, the ~ 143 million row key limit implied by the use of 'int' in BitSet prior to the switch to OpenBitSet translates roughly to 238 MB (assuming the limitation factor there was the addressability of the bits with a 32 bit int, which is my understanding).

Having a preliminary look at OpenBitSet with an eye towards replacing the single long[] with multiple arrays, it seems that if we're willing to drop some of the functionality that is not used for bloom filter purposes, the bits[i] indexing should be pretty easy to augment with modulo to address an appropriate smaller array. Locality is not an issue since the bloom filter case is the worst possible case for locality anyway, and it doesn't matter whether it's one huge array or a number of ~ 64k arrays.

Callers may be affected like BloomFilterSerializer which cares about the underlying bit array.

If the full functionality of OpenBitSet is to be maintained (e.g., xorCount) some additional acrobatics would be necessary and presumably at a noticable performance cost if such operations were to be used in performance critical places.

An argument against touching OpenBitSet is that it seems to be pretty carefully written and tested and has some non-trivial details and people have seemingly benchmarked it quite carefully. On the other hand, the improvement would then apply to other things as well, such as the bitsets used to keep track of in-core pages (off the cuff for scale, a 64 gig sstable should imply a 2 mb bit set, with one bit per 4k page).


",,cburroughs,kingryan,m0nstermind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2521,,"16/Oct/11 16:03;m0nstermind;2466v1.patch;https://issues.apache.org/jira/secure/attachment/12499202/2466v1.patch","17/Oct/11 19:34;m0nstermind;2466v2.patch;https://issues.apache.org/jira/secure/attachment/12499421/2466v2.patch","14/Oct/11 17:44;m0nstermind;BloomFilterSerializer.java;https://issues.apache.org/jira/secure/attachment/12499069/BloomFilterSerializer.java","14/Oct/11 06:52;m0nstermind;OpenBitSet.java;https://issues.apache.org/jira/secure/attachment/12498988/OpenBitSet.java",,,,,,,,,,,4.0,m0nstermind,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20640,,,Wed Oct 26 20:06:23 UTC 2011,,,,,,,,,,"0|i0gbjj:",93300,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"13/Apr/11 06:14;cscotta;Peter, it's interesting that you mention this. During the first run of the memory profiling I ran while investigating CASSANDRA-2463 on vanilla 0.7.4, I saw a tremendous amount of allocation of long[]/longs (comprising ~70% of heap allocations at that specific moment), but was not sure of the source. I doubt I'll have time to look into this at the office this week, but if I'm able to outside of work I may check into the allocation a bit.

If you're on it, by all means go for it. Thanks for mentioning this!;;;","13/Apr/11 17:48;kingryan;Moving to smaller arrays would make the allocation easier, but wouldn't reduce the raw amount of memory needed for a large bloom filter.

Would it be worth moving these off-heap completely?;;;","13/Apr/11 18:55;scode;Yes, it's not decreasing the total amount. The idea is that with small values, at least you're now left with a tweakable situation where a suitable heap size and a suitable initial occupancy trigger should be sufficient to avoid concurrent mode failures, as long as you're avoiding the fragmentation induced promotion failures (pragmatically, even if theoretically unclean, one can also help CMS along by inserting some minor application level pauses during allocations of many chunks as part of a larger allocation). But the memory pressure remains.

Regarding off-heap: I didn't suggest it because I'd personally like to avoid JNI/JNA if possible for ""safety"" reasons, and I would also like to avoid adding further dependencies on CMS sweeps for external resources (files, off-heap mem, etc) for the usual reasons. But maybe I'm more paranoid about that than most.

It would be really nice to just have an explicitly managed pool of fixed-size off-heap buffers of a reasonable size (say, a meg a piece, mmap():ed). I'm thinking maybe the bloom filters can be explicitly managed more easily than the mmaps/brafs for data reading; for example by accepting that for the few requests that race with the removal of an sstable, the bloom filter would just pretend all bits are set.

Hmmm...

;;;","21/Apr/11 07:02;scode;If/once the bullet is bitten in CASSANDRA-2521 this should be much simpler to accomplish without the GC tradeoffs.;;;","13/Oct/11 22:41;jbellis;The main problem with moving BF off-heap is that single-item gets from direct buffers are about 1/2 the speed of accessing an on-heap byte[].  That's a pretty big hit to take.;;;","14/Oct/11 06:19;scode;Good point. What's your feeling on the approach of modifying the bitset to use a number of small byte arrays? (Probably at some fixed size to help fragmentation.)

It does mean an additional level of indirection in an array of arrays, and it's not clear (to me at least) that it is expected to usually reside in CPU cache.
;;;","14/Oct/11 06:52;m0nstermind;I made it already for our cassandra deployment (because we have huge bloom filters). Attaching it as is (will rebase it to head if you'll find it useful).;;;","14/Oct/11 12:47;jbellis;What effect did you observe when deploying this, Oleg?;;;","14/Oct/11 14:44;m0nstermind;Promotion failures caused by bloom filters gone away completely.
 
I did not measured performance overhead specifically on bloom filters, so all I can tell that it was not introduced measureable performance degradation on production cluster operation from client perspective. 

(currently we have ~3300 thrift single column read reqs/sec per node with ~1ms avg call duration measured from client; 98% reads are bounced by bloom filters; size of single bloom filter is up to 250Mb)
;;;","14/Oct/11 17:32;jbellis;Is serialization the same, or at least backwards-compatible w/ existing OBS BFs?;;;","14/Oct/11 17:44;m0nstermind;You mean on-disk format ? Yes. Specifics are handled by BloomFilterSerializer. Attached serializer as implemented for 0.6.
(as far as i see no changes are required in BloomFilter itself);;;","14/Oct/11 17:58;jbellis;Are there any other moving parts to this? Could we ask you to submit a patch against trunk?;;;","14/Oct/11 18:14;m0nstermind;Of course. I'll try to prepare it till monday.;;;","14/Oct/11 21:27;jbellis;Great!;;;","16/Oct/11 16:03;m0nstermind;Rebased to trunk. ant test passed.;;;","17/Oct/11 13:11;jbellis;Some comments:

{code}
            for (int i = 0; i < pageSize && bitLength-- > 0; i++)
{code}

Isn't one of those bounds checks redundant?  It looks like wlen/bitLength is the right one to use.

* lots of commented-out methods in OBS. Let's remove them entirely if we're not going to implement.
* OBS.setPage is unnecessary;;;","17/Oct/11 19:41;m0nstermind;Attached v2:
Removed there commented out code as well as OBS.setPage.

regarding  ""i < pageSize && bitLength-- > 0"":
both bound checks are neccessary:
* ""i<pageSize"" ensures there are no out of bounds for every single page, while
* ""bitLength-- > 0"" ensures there are no more than neccessary bytes are written from the last page (otherwise extra zeroes will appear in file, which will break backwards compatibility).;;;","25/Oct/11 18:52;jbellis;Patch looks good to me, just waiting for performance testing now.;;;","26/Oct/11 19:34;jbellis;Tested (by Brandon) and committed. Thanks, Oleg!;;;","26/Oct/11 19:49;scode;Awesome!;;;","26/Oct/11 20:06;m0nstermind;Youre welcome ;-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig load/storefunc loads only one schema and BytesType validation class needs fix,CASSANDRA-2465,12504126,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jeromatron,jeromatron,jeromatron,13/Apr/11 01:39,16/Apr/19 09:33,14/Jul/23 05:52,13/Apr/11 17:42,0.7.5,,,,,,0,hadoop,pig,,"With a recent optimization, it appears that the Pig load/store func gets only one schema from Cassandra and tries to apply it to all CFs in the pig script.  Also, the BytesType validation tries to cast the object in putNext as a DataByteArray and wrap it as a ByteBuffer.  Instead it should just call objToBB which should take care of it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/11 01:41;jeromatron;2465.txt;https://issues.apache.org/jira/secure/attachment/12476202/2465.txt",,,,,,,,,,,,,,1.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20639,,,Wed Apr 13 17:42:17 UTC 2011,,,,,,,,,,"0|i0gbjb:",93299,,,,,Normal,,,,,,,,,,,,,,,,,"13/Apr/11 01:43;jeromatron;Attaching patch to make the udf context key specific to the keyspace and column family so it doesn't get overwritten.  Also changed the putNext case where the validation class is BytesType to use objToBB the way it should to handle things like Strings.;;;","13/Apr/11 01:47;jeromatron;Tested with basic row count pig script as well as multiple join/cogroup operations against multiple column families.  Tested input and output.;;;","13/Apr/11 17:42;brandon.williams;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flush and Compaction Unnecessarily Allocate 256MB Contiguous Buffers,CASSANDRA-2463,12504105,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,cscotta,cscotta,cscotta,12/Apr/11 21:23,16/Apr/19 09:33,14/Jul/23 05:52,13/Apr/11 05:34,0.7.5,,,,,,0,,,,"Currently, Cassandra 0.7.x allocates a 256MB contiguous byte array at the beginning of a memtable flush or compaction (presently hard-coded as Config.in_memory_compaction_limit_in_mb). When several memtable flushes are triggered at once (as by `nodetool flush` or `nodetool snapshot`), the tenured generation will typically experience extreme pressure as it attempts to locate [n] contiguous 256mb chunks of heap to allocate. This will often trigger a promotion failure, resulting in a stop-the-world GC until the allocation can be made. (Note that in the case of the ""release valve"" being triggered, the problem is even further exacerbated; the release valve will ironically trigger two contiguous 256MB allocations when attempting to flush the two largest memtables).

This patch sets the buffer to be used by BufferedRandomAccessFile to Math.min(bytesToWrite, BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE) rather than a hard-coded 256MB. The typical resulting buffer size is 64kb.

I've taken some time to measure the impact of this change on the base 0.7.4 release and with this patch applied. This test involved launching Cassandra, performing four million writes across three column families from three clients, and monitoring heap usage and garbage collections. Cassandra was launched with 2GB of heap and the default JVM options shipped with the project. This configuration has 7 column families with a total of 15GB of data.

Here's the base 0.7.4 release:
http://cl.ly/413g2K06121z252e2t10

Note that on launch, we see a flush + compaction triggered almost immediately, resulting in at least 7x very quick 256MB allocations maxing out the heap, resulting in a promotion failure and a full GC. As flushes proceeed, we see that most of these have a corresponding CMS, consistent with the pattern of a large allocation and immediate collection. We see a second promotion failure and full GC at the 75% mark as the allocations cannot be satisfied without a collection, along with several CMSs in between. In the failure cases, the allocation requests occur so quickly that a standard CMS phase cannot completed before a ParNew attempts to promote the surviving byte array into the tenured generation. The heap usage and GC profile of this graph is very unhealthy.

Here's the 0.7.4 release with this patch applied:
http://cl.ly/050I1g26401B1X0w3s1f

This graph is very different. At launch, rather than a immediate spike to full allocation and a promotion failure, we see a slow allocation slope reaching only 1/8th of total heap size. As writes begin, we see several flushes and compactions, but none result in immediate, large allocations. The ParNew collector keeps up with collections far more ably, resulting in only one healthy CMS collection with no promotion failure. Unlike the unhealthy rapid allocation and massive collection pattern we see in the first graph, this graph depicts a healthy sawtooth pattern of ParNews and an occasional effective CMS with no danger of heap fragmentation resulting in a promotion failure.

The bottom line is that there's no need to allocate a hard-coded 256MB write buffer for flushing memtables and compactions to disk. Doing so results in unhealthy rapid allocation patterns and increases the probability of triggering promotion failures and full stop-the-world GCs which can cause nodes to become unresponsive and shunned from the ring during flushes and compactions.",Any,cburroughs,cscotta,eonnen,stuhood,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,"12/Apr/11 22:07;jbellis;2463-v2.txt;https://issues.apache.org/jira/secure/attachment/12476182/2463-v2.txt","12/Apr/11 21:28;cscotta;patch.diff;https://issues.apache.org/jira/secure/attachment/12476177/patch.diff",,,,,,,,,,,,,2.0,cscotta,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20638,,,Wed Apr 13 05:34:50 UTC 2011,,,,,,,,,,"0|i0gbiv:",93297,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"12/Apr/11 21:26;cscotta;[ Patch attached ];;;","12/Apr/11 21:28;cscotta;Patch attached. Applies cleanly to tag 'cassandra-0.7.4'. All tests pass.;;;","12/Apr/11 22:07;jbellis;I started making it more complicated:

{code}
        // the gymnastics here are because
        //  - we want the buffer large enough that we're not re-buffering when we have to seek back to the
        //    start of a row to write the data size.  Here, ""10% larger than the average row"" is ""large enough,""
        //    meaning we expect to seek and rebuffer about 1/10 of the time.
        //  - but we don't want to allocate a huge buffer unnecessarily for a small amount of data
        //  - and on the low end, we don't want to be absurdly stingy with the buffer size for small rows
        assert estimatedSize > 0;
        long maxBufferSize = Math.min(DatabaseDescriptor.getInMemoryCompactionLimit(), 1024 * 1024);
        int bufferSize;
        if (estimatedSize < 64 * 1024)
        {
            bufferSize = (int) estimatedSize;
        }
        else
        {
            long estimatedRowSize = estimatedSize / keyCount;
            bufferSize = (int) Math.min(Math.max(1.1 * estimatedRowSize, 64 * 1024), maxBufferSize);
        }
{code}

...  but the larger our buffer is, the larger the penalty for guessing wrong when we have to seek back and rebuffer.

Then I went through and added size estimation to the CompactionManager, until I thought ""it's kind of ridiculous to be worrying about saving a few bytes less than 64KB, especially when we expect most memtables to have more data in them than 64K when flushed.""

Thus, I arrived at the patch Antoine de Saint-Exupery would have written, attached as v2.;;;","12/Apr/11 22:44;scode;A noteworthy factor here is that unless an fsync()+fadvise()/madvise() have evicted data, in the normal case this stuff should still be in page cache for any reasonably sized row. For truly huge rows, the penalty of seeking back should be insignificant anyway.

Total +1 on avoiding huge allocations. I was surprised to realize, when this ticket came along, that this was happening ;)

I have been suspecting that the bloom filters are a major concern too with respect to triggering promotion failures (but I haven't done testing to confirm this). Are there other cases than this and the bloom filters where we know that we're doing large allocations?;;;","12/Apr/11 23:58;jbellis;(I wonder if this is the cause of the intermittent load-spikes-after-upgrade-to-0.7 reports we've seen.);;;","13/Apr/11 00:56;eonnen;As a data point to that question, we hardly ever had CMS collections on 0.6.8 and maybe one full GC ever that I can think of for what was years of cumulative uptime. It surely differs for workloads, but in our case 0.7 got much worse along the CMS dimension.;;;","13/Apr/11 03:45;scode;Filed CASSANDRA-2466 for the bloom filter case.
;;;","13/Apr/11 05:34;jbellis;First time I got a +1 via Twitter: http://twitter.com/#!/cscotta/status/58031493565513728

committed.;;;","13/Apr/11 05:34;jbellis;Thanks for tracking this down, Scott and Erik!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix build for distributed and stress tests,CASSANDRA-2462,12504104,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,12/Apr/11 20:46,16/Apr/19 09:33,14/Jul/23 05:52,14/Apr/11 18:38,0.8 beta 1,,,Legacy/Testing,Legacy/Tools,,0,,,,Distributed and stress tests are not compiling for trunk.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/11 20:46;stuhood;0001-Update-stress-and-tests-for-trunk.txt;https://issues.apache.org/jira/secure/attachment/12476167/0001-Update-stress-and-tests-for-trunk.txt",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20637,,,Thu Apr 14 18:38:41 UTC 2011,,,,,,,,,,"0|i0gbin:",93296,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"12/Apr/11 21:23;jbellis;committed, minus the default value for RF in Stress.Session (which we want to omit for NTS);;;","12/Apr/11 23:33;stuhood;Not having a default will break standard stress runs unless people specify the RF flag, right?;;;","14/Apr/11 18:38;jbellis;you're right. added a default for when RS=SimpleStrategy in r1092435;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LongCompactionSpeedTest fails,CASSANDRA-2461,12504066,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,jbellis,jbellis,12/Apr/11 15:02,16/Apr/19 09:33,14/Jul/23 05:52,14/Apr/11 18:20,0.8 beta 1,,,,,,0,,,,"ant long-test -Dtest.name=LongCompactionSpeedTest fails.

There are several errors. Here is the first:

{noformat}
    [junit] java.lang.IllegalArgumentException
    [junit] 	at java.nio.ByteBuffer.allocate(ByteBuffer.java:311)
    [junit] 	at org.apache.cassandra.db.context.CounterContext.clearAllDelta(CounterContext.java:444)
    [junit] 	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:100)
    [junit] 	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:36)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:158)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:41)
    [junit] 	at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:284)
    [junit] 	at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
    [junit] 	at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
    [junit] 	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
    [junit] 	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
    [junit] 	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
    [junit] 	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
    [junit] 	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
    [junit] 	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
    [junit] 	at org.apache.cassandra.db.ColumnIndexer.serializeInternal(ColumnIndexer.java:76)
    [junit] 	at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:50)
    [junit] 	at org.apache.cassandra.io.LazilyCompactedRow.<init>(LazilyCompactedRow.java:87)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableWriter$CommutativeRowIndexer.doIndexing(SSTableWriter.java:462)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:364)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:317)
    [junit] 	at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1089)
    [junit] 	at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1080)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/11 10:10;slebresne;0001-Fix-LongCompactionSpeedTest.patch;https://issues.apache.org/jira/secure/attachment/12476227/0001-Fix-LongCompactionSpeedTest.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20636,,,Thu Apr 14 18:20:22 UTC 2011,,,,,,,,,,"0|i0gbif:",93295,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"13/Apr/11 10:10;slebresne;That was not updated correctly with CASSANDRA-1938, sorry. Patch attached.;;;","14/Apr/11 18:20;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cli divides read repair chance by 100,CASSANDRA-2458,12504040,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,12/Apr/11 11:40,16/Apr/19 09:33,14/Jul/23 05:52,12/Apr/11 15:18,0.8 beta 1,,,,,,0,cli,,,cli incorrectly divides the read_repair chance by 100 when creating / updating CF's,,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/11 11:50;amorton;0001-do-not-divide-read_repair_chance-by-100.patch;https://issues.apache.org/jira/secure/attachment/12476112/0001-do-not-divide-read_repair_chance-by-100.patch",,,,,,,,,,,,,,1.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20635,,,Tue Apr 12 16:42:13 UTC 2011,,,,,,,,,,"0|i0gbhr:",93292,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"12/Apr/11 11:50;amorton;now expects read repair chance to be between 0 and 1 for create and update CF.;;;","12/Apr/11 15:18;jbellis;committed;;;","12/Apr/11 16:42;hudson;Integrated in Cassandra-0.8 #2 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/2/])
    cli no longer divides read_repair_chance by 100
patch by Aaron Morton; reviewed by jbellis for CASSANDRA-2458
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch_mutate is broken for counters,CASSANDRA-2457,12504037,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,12/Apr/11 11:29,16/Apr/19 09:33,14/Jul/23 05:52,20/Apr/11 09:22,0.8.0 beta 2,,,,,,0,,,,"CASSANDRA-2384 allowed for batch_mutate to take counter and non counter operation, but the code was not updated correctly to handle that case. As it is, the code will use the first mutation in the batch list to decide whether to apply the write code path of counter or not, and will thus break if those are mixed.",,cburroughs,stuhood,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,"18/Apr/11 15:37;slebresne;0001-Fix-batch_mutate-for-counters-v2.patch;https://issues.apache.org/jira/secure/attachment/12476619/0001-Fix-batch_mutate-for-counters-v2.patch","13/Apr/11 14:31;slebresne;0001-Fix-batch_mutate-for-counters.patch;https://issues.apache.org/jira/secure/attachment/12476243/0001-Fix-batch_mutate-for-counters.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20634,,,Wed Apr 20 19:16:47 UTC 2011,,,,,,,,,,"0|i0gbhj:",93291,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"13/Apr/11 14:31;slebresne;Attaching patch against 0.8.

Basically this makes SP.mutate() handle a list of mixed RowMutation and CounterMutation (but then each mutation use the right path). This is needed since a batch_mutate() can now mix those.

This has an unfortunate consequence however in that I don't think there is a way to distinguish the writeStats of standard insert of the ones of counter insert anymore, and thus this patch remove the latter ones (and count everything in writeStat).;;;","14/Apr/11 21:18;stuhood;* writeLocallyAndReplicate doesn't always perform a local mutation, so it should probably be renamed
* Since mutateCounter and writeLocallyAndReplicate are symmetrical and are called depending on whether an IMutation is an instance of CounterMutation, could we move them onto IMutation, and polymorphically decide the behavior?

> ...and thus this patch remove the latter ones (and count everything in writeStat).
I'm fine with this, since a counter is as ""real"" a write as any other. _But_ I do think we should record the latencies for the replicate-on-write stage like we do for the read and mutation stages on a per column family basis. I can tackle it in a separate ticket if you'd like.

PS: Thanks for removing this line!
{code}mutations.iterator().next().getColumnFamilies().iterator().next().metadata().getDefaultValidator().isCommutative(){code}

Thanks Sylvain!;;;","18/Apr/11 15:44;slebresne;Attached rebased version (post-CASSANDRA-2454 in particular)

bq. writeLocallyAndReplicate doesn't always perform a local mutation, so it should probably be renamed

Renamed it to performWrite (since it mostly simply imply a so-called writePerformer).

bq. Since mutateCounter and writeLocallyAndReplicate are symmetrical and are called depending on whether an IMutation is an instance of CounterMutation, could we move them onto IMutation, and polymorphically decide the behavior?

Hum, they are not really so symmetrical. In particular writeLocallyAndReplicate (or performWrite as it is called nowadays) really is polymorphic over the IMutation used.
So the only seem we we could do (at least easily) is moving some of mutateCounter in CounterMutation, but not sure it will look so great. Also, it is probably nice to keep all the code related to the write/read protocol in StorageProxy (doing otherwise would be like moving the query code out of CFStore, nobody wants that :D)

bq. I'm fine with this, since a counter is as ""real"" a write as any other. But I do think we should record the latencies for the replicate-on-write stage like we do for the read and mutation stages on a per column family basis. I can tackle it in a separate ticket if you'd like.

Make sense, but I'm also in favor of moving this to some other ticket. ;;;","20/Apr/11 03:48;stuhood;+1;;;","20/Apr/11 09:22;slebresne;Committed, thanks;;;","20/Apr/11 10:21;hudson;Integrated in Cassandra #859 (See [https://hudson.apache.org/hudson/job/Cassandra/859/])
    merge CASSANDRA-2457 from 0.8
;;;","20/Apr/11 19:16;stuhood;Thanks Sylvain. Opened CASSANDRA-2522;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible deadlock for counter mutations,CASSANDRA-2454,12503982,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,kelvin,stuhood,stuhood,11/Apr/11 21:38,16/Apr/19 09:33,14/Jul/23 05:52,14/Apr/11 21:46,0.8 beta 1,,,,,,0,,,,"{{StorageProxy.applyCounterMutation}} is executed on the mutation stage, but it also submits tasks to the mutation stage, and then blocks for them. If there are more than a few concurrent mutations, this can lead to deadlock.",,cburroughs,omid,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/11 21:48;stuhood;0001-Don-t-re-submit-to-the-mutation-stage.txt;https://issues.apache.org/jira/secure/attachment/12476061/0001-Don-t-re-submit-to-the-mutation-stage.txt","12/Apr/11 10:13;slebresne;0001-Submit-counters-update-on-mutation-stage-only-if-not.patch;https://issues.apache.org/jira/secure/attachment/12476104/0001-Submit-counters-update-on-mutation-stage-only-if-not.patch",,,,,,,,,,,,,2.0,kelvin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20632,,,Thu Apr 14 22:19:44 UTC 2011,,,,,,,,,,"0|i0gbgv:",93288,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"11/Apr/11 21:48;stuhood;Patch. (credit to Kelvin Kakugawa);;;","12/Apr/11 10:13;slebresne;Looking closer that this there is 2 places from which we execute counter write:
  # if the coordinator is a replica, from the thrift thread.
  # otherwise in the CounterMutationVerbHandler on a replica, that is on the MUTATION stage.

In the latter case, we must indeed avoid re-submitting to the MUTATION stage to avoid deadlock. But in the former we should not skip the stage.

Attaching a v2 patch that distinguishes between the 2 cases and 'do the right thing'.

Note that another way to fix this would be to make CounterMutationVerbHandler execute on some other stage that the MUTATION one. Even though that would simpler in the number of line modified, I don't think an existing stage would fit the bill and creating a new one for that doesn't feel right.;;;","14/Apr/11 21:39;stuhood;+1
Looks great, thanks!;;;","14/Apr/11 21:46;jbellis;committed (in case sylvain's not back before we roll a beta);;;","14/Apr/11 22:19;hudson;Integrated in Cassandra-0.8 #7 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/7/])
    fix possible counter deadlock
patch by Kelvin Kakugawa, Stu Hood, and slebresne for CASSANDRA-2454
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incompatibility w/ 0.7 schemas,CASSANDRA-2450,12503968,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,11/Apr/11 19:32,16/Apr/19 09:33,14/Jul/23 05:52,15/Apr/11 14:38,0.8 beta 1,,,,,,0,,,,"If you create a SimpleStrategy keyspace under 0.7, then switch to 0.8, you will get this error on startup:

{noformat}
ERROR 14:31:41,725 Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:277)
	at org.apache.cassandra.db.Table.open(Table.java:109)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:75)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:327)
	at org.apache.cassandra.db.Table.<init>(Table.java:273)
	... 4 more
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/11 13:58;jbellis;2450.txt;https://issues.apache.org/jira/secure/attachment/12476455/2450.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20631,,,Fri Apr 15 14:38:15 UTC 2011,,,,,,,,,,"0|i0gbfz:",93284,,gdusbabek,,gdusbabek,Normal,,,,,,,,,,,,,,,,,"15/Apr/11 13:58;jbellis;patch add back replication_factor to the avro class as union { int, null } so we can upgrade old-style schema information seamlessly in KSMetaData.;;;","15/Apr/11 14:24;gdusbabek;+1;;;","15/Apr/11 14:38;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove loadbalance command,CASSANDRA-2448,12503953,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,nickmbailey,nickmbailey,nickmbailey,11/Apr/11 17:00,16/Apr/19 09:33,14/Jul/23 05:52,11/Apr/11 17:35,0.8 beta 1,,,Legacy/Tools,,,0,,,,"With the update to how the move command works, the loadbalance command is even less useful that it was previously.  The loadbalance command now calculates the token it is going to move to before it leaves which means it isn't considering the load it is giving away. Given that, I think we should just remove the loadbalance command entirely. Anyone who wants to do an old style loadbalance can just do decommission then bootstrap.

This is a minor change, and honestly I think it might count as a 'bug' so I think we should squeeze it into 0.8, post-freeze. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/11 17:00;nickmbailey;0001-Remove-loadbalance-command-from-nodetool.patch;https://issues.apache.org/jira/secure/attachment/12476019/0001-Remove-loadbalance-command-from-nodetool.patch",,,,,,,,,,,,,,1.0,nickmbailey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20630,,,Wed Apr 20 23:46:43 UTC 2011,,,,,,,,,,"0|i0gbfj:",93282,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"11/Apr/11 17:35;jbellis;lgtm, committed;;;","11/Apr/11 19:11;nickmbailey;this should also be applied to trunk;;;","11/Apr/11 19:36;jbellis;merging to trunk is asynchronous but will happen;;;","20/Apr/11 23:43;cdaw;Not sure how documentation gets updated, but we still mention using the loadbalance command on the wiki:

http://wiki.apache.org/cassandra/Operations#Moving_or_Removing_nodes;;;","20/Apr/11 23:46;nickmbailey;Updated the wiki to indicate that the loadbalance command is only available in versions 0.7.* and lower.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support SQL data types in CQL,CASSANDRA-2445,12503843,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,09/Apr/11 21:40,16/Apr/19 09:33,14/Jul/23 05:52,10/Apr/11 04:45,0.8 beta 1,,,,,,0,,,,"We should support SQL data types where possible:

(sql -> cassandra)
varint -> int
bigint -> long
varchar, text -> utf8
ascii -> ascii (not strictly correct -- sql defines collations for this -- but close enough for a 1.0)
uuid -> uuid (see CASSANDRA-2233)
bytea -> bytes

IMO the right thing to do is _only_ support the SQL types in CQL CREATE statements, because there is ambiguity otherwise (in particular with ""int"").
",,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/11 00:28;jbellis;2445.txt;https://issues.apache.org/jira/secure/attachment/12475919/2445.txt","10/Apr/11 02:14;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2445-update-documentation.txt;https://issues.apache.org/jira/secure/attachment/12475920/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2445-update-documentation.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20629,,,Sun Apr 10 05:21:51 UTC 2011,,,,,,,,,,"0|i0gbev:",93279,,urandom,,urandom,Normal,,,,,,,,,,,,,,,,,"10/Apr/11 02:15;urandom;The docs should be updated too (patch attached), otherwise, LGTM. +1;;;","10/Apr/11 04:45;jbellis;thanks!  committed.;;;","10/Apr/11 05:21;hudson;Integrated in Cassandra #842 (See [https://hudson.apache.org/hudson/job/Cassandra/842/])
    use SQL-ish data types
patch by jbellis and Eric Evans for CASSANDRA-2445
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove checkAllColumnFamilies on startup,CASSANDRA-2444,12503814,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,09/Apr/11 03:50,16/Apr/19 09:33,14/Jul/23 05:52,22/Apr/11 22:21,0.8.0 beta 2,,,,,,0,compaction,,,"We've ran into many times where we do not want compaction to run right away against CFs when booting up a node. If the node needs to compact, it will do so at the first flush",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/11 22:13;lenn0x;0001-CASSANDRA-2444-Remove-checking-all-column-families-o.patch;https://issues.apache.org/jira/secure/attachment/12477170/0001-CASSANDRA-2444-Remove-checking-all-column-families-o.patch",,,,,,,,,,,,,,1.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20628,,,Fri Apr 22 23:31:41 UTC 2011,,,,,,,,,,"0|i0gben:",93278,,stuhood,,stuhood,Low,,,,,,,,,,,,,,,,,"21/Apr/11 23:30;stuhood;Rather than adding an option for this, I think we should remove the check entirely. If the node needs to compact, it will do so at the first flush, which is more likely to be staggered across the cluster (and if this flush is forced by commitlogs, then AOK).;;;","22/Apr/11 04:47;jbellis;bq. Rather than adding an option for this, I think we should remove the check entirely. If the node needs to compact, it will do so at the first flush

+1;;;","22/Apr/11 23:31;hudson;Integrated in Cassandra-0.8 #35 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/35/])
    Remove checking all column families on startup for compaction candidates
patch by goffinet; reviewed by stuhood for CASSANDRA-2444
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra crashes with segmentation fault on Debian 5.0 and Ubuntu 10.10,CASSANDRA-2441,12503787,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,xedin,xedin,08/Apr/11 21:35,16/Apr/19 09:33,14/Jul/23 05:52,12/Apr/11 15:09,0.8 beta 1,,,,,,0,,,,"Last working commit is c8d1984bf17cab58f40069e522d074c7b0077bc1 (merge from 0.7), branch: trunk.

What I did is cloned git://git.apache.org/cassandra.git and did git reset each commit with `ant clean && ant && ./bin/cassandra -f` until I got cassandra started","Both servers have identical hardware configuration: Quad-Core AMD Opteron(tm) Processor 2374 HE, 4 GB RAM (rackspace servers)

Java version ""1.6.0_20""
OpenJDK Runtime Environment (IcedTea6 1.9.7) (6b20-1.9.7-0ubuntu1)
OpenJDK 64-Bit Server VM (build 19.0-b09, mixed mode)",alexiswilke,amram99,mauzhang,ralph@massrelevance.com,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5517,,,,,,,,"11/Apr/11 20:51;jbellis;2441.txt;https://issues.apache.org/jira/secure/attachment/12476056/2441.txt","11/Apr/11 20:21;jbellis;2441.txt;https://issues.apache.org/jira/secure/attachment/12476053/2441.txt","11/Apr/11 19:28;jbellis;jamm-0.2.1.jar;https://issues.apache.org/jira/secure/attachment/12476042/jamm-0.2.1.jar",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20627,,,Mon Oct 15 06:18:30 UTC 2012,,,,,,,,,,"0|i0839r:",45129,,xedin,,xedin,Critical,,,,,,,,,,,,,,,,,"08/Apr/11 22:12;jbellis;are you using sun jdk?;;;","08/Apr/11 22:17;xedin;no, I installed OpenJDK using apt-get `apt-get install openjdk-6-jdk openjdk-6-jre`;;;","08/Apr/11 22:24;jbellis;give sun jdk a try.;;;","08/Apr/11 22:26;xedin;Ok, I will give it a try and comment back!;;;","08/Apr/11 23:14;xedin;Works with Sun JDK

{noformat}
java version ""1.6.0_24""
Java(TM) SE Runtime Environment (build 1.6.0_24-b07)
Java HotSpot(TM) 64-Bit Server VM (build 19.1-b02, mixed mode)
{noformat}

But this could be a problem if, for example, Whirr sets up openjdk instead of one from the Sun, I will check that ASAP and comment...;;;","08/Apr/11 23:23;jbellis;Looks like this isn't the only time someone has seen segfaults due to using javaagent with OpenJDK: http://liteforums.appdynamics.com/discussion/143/appagent-causing-segfault/p1

I would prefer to fix by changing our packaging to explicitly use Sun JDK instead of ""whatever.""

Would also be useful to try w/ the IBM JDK: http://www.ibm.com/developerworks/java/jdk/linux/download.html;;;","08/Apr/11 23:26;xedin;Thats a good idea, I agree! Let me test IBM JDK tomorrow...;;;","10/Apr/11 15:23;jbellis;Also jrockit: http://www.oracle.com/technetwork/middleware/jrockit/downloads/index.html;;;","10/Apr/11 17:44;xedin;Latest code (branch trunk) works with JRockit (jrockit-jdk1.6.0_22-R28.1.1-4.0.1) but note that JVM does not support following options: -Xmn<size> -XX:ThreadPriorityPolicy -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio=8  -XX:MaxTenuringThreshold=1 -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly;;;","10/Apr/11 18:23;xedin;using IBM JDK started without any changes to the conf/cassandra-env.sh and everything worked fine.

{noformat}
java version ""1.6.0""
Java(TM) SE Runtime Environment (build pxi3260sr9fp1-20110208_03(SR9 FP1))
IBM J9 VM (build 2.4, JRE 1.6.0 IBM J9 2.4 Linux x86-32 jvmxi3260sr9-20110203_74623 (JIT enabled, AOT enabled)
J9VM - 20110203_074623
JIT  - r9_20101028_17488ifx3
GC   - 20101027_AA)
JCL  - 20110203_01
{noformat}
;;;","11/Apr/11 03:11;jbellis;So, definitely an OpenJDK-only bug.

I'll see if I can give it a no-java-agent mode so we can limp along without it for OpenJDK.;;;","11/Apr/11 10:23;xedin;+1;;;","11/Apr/11 19:28;jbellis;attached.  (requires jamm 0.2.1 in lib/, also attached.)

(most of the patch is svn deleting the 0.2 jar.  silly svn.);;;","11/Apr/11 20:21;jbellis;On Brandon's advice I moved the entire warning into the log4j call, even though this makes it unwieldy and we don't have perfect information as to what the cause is at that point.;;;","11/Apr/11 20:32;xedin;Can you please re-attach git apply and patch both say that patch is corrupted at line 90?..;;;","11/Apr/11 20:51;jbellis;manually ripped the binary portion out of the patch.;;;","11/Apr/11 21:00;xedin;+1;;;","12/Apr/11 15:09;jbellis;committed;;;","12/Apr/11 16:42;hudson;Integrated in Cassandra-0.8 #2 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/2/])
    hack to allow OpenJDK to run w/o javaagent (otherwise it segfaults)
patch by jbellis and brandonwilliams; reviewed by Pavel Yaskevich for CASSANDRA-2441
;;;","12/Apr/12 09:42;richardlow;Tracked this down to the stack size setting.  In cassandra-env.sh, the stack size is set to 128k.  From running gdb, the segfault is tracked down to a stack overflow in parse_manifest.c:234 within openjdk.  It's clear what's going on: there's a huge statically allocated variable.

Setting the stack size to 256k means Cassandra can start up with javaagent.  So we could reenable this for openjdk if people are prepared to take the memory hit on stack size.;;;","02/Oct/12 15:22;amram99;Thanks Richard! Changing the stack size from 180k to 256k worked for me on Ubuntu 12.04 with Cassandra 1.1.5 and OpenJDK 64-Bit Server VM/1.6.0_24;;;","10/Oct/12 22:07;alexiswilke;I can confirm Bubba Gump comment. I have the same setup and increasing the stack solved the crash issue with OpenJDK.;;;","15/Oct/12 06:18;mauzhang;180k works fine with OpenJDK 1.7.0_07. So update OpenJDK could be an option;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClusterTool throws exception for get_endpoints,CASSANDRA-2437,12503667,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,07/Apr/11 19:21,16/Apr/19 09:33,14/Jul/23 05:52,07/Apr/11 19:41,0.8 beta 1,,,,,,0,,,,"ByteBuffer is not serializable over JMX.

Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
	at $Proxy0.getNaturalEndpoints(Unknown Source)
	at org.apache.cassandra.tools.NodeProbe.getEndpoints(NodeProbe.java:446)
	at org.apache.cassandra.tools.ClusterCmd.printEndpoints(ClusterCmd.java:146)
	at org.apache.cassandra.tools.ClusterCmd.main(ClusterCmd.java:240)
Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1156)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1338)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1146)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:326)
	at java.rmi.MarshalledObject.<init>(MarshalledObject.java:101)
	at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.invoke(RMIConnector.java:990)
	at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:288)
	... 4 more",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/11 19:23;lenn0x;0001-CASSANDRA-2437-Support-a-byte-key-for-getNaturalEndp.patch;https://issues.apache.org/jira/secure/attachment/12475737/0001-CASSANDRA-2437-Support-a-byte-key-for-getNaturalEndp.patch",,,,,,,,,,,,,,1.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20626,,,Thu Apr 07 21:50:06 UTC 2011,,,,,,,,,,"0|i0gbdb:",93272,,,,,Normal,,,,,,,,,,,,,,,,,"07/Apr/11 19:27;stuhood;+1;;;","07/Apr/11 21:50;hudson;Integrated in Cassandra #835 (See [https://hudson.apache.org/hudson/job/Cassandra/835/])
    Support a byte[] key for getNaturalEndpoints so clustertool get_endpoints does not throw exception
patch by goffinet; reviewed by stuhood for CASSANDRA-2437
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
auto bootstrap happened on already bootstrapped nodes,CASSANDRA-2435,12503656,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,scode,scode,07/Apr/11 17:29,16/Apr/19 09:33,14/Jul/23 05:52,11/Apr/11 03:10,0.7.5,,,,,,1,,,,"I believe the following was observed on 0.7.2. I meant to dig deeper, but never had the time, and now I want to at least file this even if I don't have extremely helpful information.

A piece of background is that we consciously made the decision to have the default configuration on nodes have auto_bootstrap set to true. The logic was that if one accidentally were to start a new node, we'd rather have it join with data than join *without* data and cause bogus read results in the cluster.

We executed this policy (by way of having the puppet managed config have auto_bootstrap set to true).

On one of our clusters with 5 nodes, we did some moves. All looked well; the moves completed. For unrelated reasons, we wanted to restart nodes after they had been moved. When we did, three of the 5, specifically those 3 that were *NOT* seed nodes, initiated a bootstrap procedure! Before the moves the cluster had been running for several days at least.

The logs indicated the automatic token selection, and they joined the ring under a new automatically selected token.

Presumably, this violated consistency but at the time there was no live traffic to the cluster and we didn't confirm (put traffic on it after repair+cleanup).

I did look a little bit at the code in light of this but didn't see anything obvious, so I don't really know what the likely culprit is.

A potential complication was that seed nodes were moved without using the correct procedure of de-seeding them first. This was clearly wrong, but it is not obvious to me that it would cause other nodes to incorrectly bootstrap since a node should *never* bootstrap more than once if the local system tables say it's been bootstrapped.
",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/11 21:15;jbellis;2435.txt;https://issues.apache.org/jira/secure/attachment/12475840/2435.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20624,,,Mon Apr 11 03:26:05 UTC 2011,,,,,,,,,,"0|i0gbcv:",93270,,nickmbailey,,nickmbailey,Critical,,,,,,,,,,,,,,,,,"08/Apr/11 21:14;jbellis;recall that move (until 0.8) consists of

- unbootstrap
- bootstrap to new location

unbootstrap calls storageservice.leavering (same as decommission), which marks the node as not-bootstrapped with setBootstrapped(false).  

in one of the refactorings during 0.7 development we removed the call to setBootstrapped(true) from finishBootstrapping.  So next restart it will indeed autobootstrap if that is enabled in the config file.;;;","08/Apr/11 21:15;jbellis;patch to fix.;;;","10/Apr/11 12:13;scode;FWIW, looks good to me (but I only did visual inspection and some code jumping in the 0.7 branch; haven't tested it).
;;;","10/Apr/11 19:04;nickmbailey;+1;;;","11/Apr/11 03:10;jbellis;committed;;;","11/Apr/11 03:26;hudson;Integrated in Cassandra-0.7 #430 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/430/])
    re-set bootstrapped flag after move finishes
patch by jbellis; reviewed by Peter Schuller and Nick Bailey for CASSANDRA-2435
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
range movements can violate consistency,CASSANDRA-2434,12503655,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tjake,scode,scode,07/Apr/11 17:23,16/Apr/19 09:33,14/Jul/23 05:52,01/May/14 13:52,2.1 beta2,,,Legacy/Streaming and Messaging,,,0,,,,"My reading (a while ago) of the code indicates that there is no logic involved during bootstrapping that avoids consistency level violations. If I recall correctly it just grabs neighbors that are currently up.

There are at least two issues I have with this behavior:

* If I have a cluster where I have applications relying on QUORUM with RF=3, and bootstrapping complete based on only one node, I have just violated the supposedly guaranteed consistency semantics of the cluster.

* Nodes can flap up and down at any time, so even if a human takes care to look at which nodes are up and things about it carefully before bootstrapping, there's no guarantee.

A complication is that not only does it depend on use-case where this is an issue (if all you ever do you do at CL.ONE, it's fine); even in a cluster which is otherwise used for QUORUM operations you may wish to accept less-than-quorum nodes during bootstrap in various emergency situations.

A potential easy fix is to have bootstrap take an argument which is the number of hosts to bootstrap from, or to assume QUORUM if none is given.

(A related concern is bootstrapping across data centers. You may *want* to bootstrap to a local node and then do a repair to avoid sending loads of data across DC:s while still achieving consistency. Or even if you don't care about the consistency issues, I don't think there is currently a way to bootstrap from local nodes only.)

Thoughts?

",,alienth,azotcsit,bradfordcp,cburroughs,christianmovi,elubow,jay.zhuang,jeromatron,jjordan,kohlisankalp,liqusha,rcoli,rfwagner@gmail.com,scode,thobbs,tjake,vijay2win@yahoo.com,wadey,weideng,Yasuharu,,,,,,,,,,,,CASSANDRA-3516,,,,,,,,,,,"08/Sep/11 21:57;thepaul;2434-3.patch.txt;https://issues.apache.org/jira/secure/attachment/12493677/2434-3.patch.txt","07/Sep/11 20:11;thepaul;2434-testery.patch.txt;https://issues.apache.org/jira/secure/attachment/12493374/2434-testery.patch.txt",,,,,,,,,,,,,2.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20623,,,Mon Jul 21 21:44:49 UTC 2014,,,,,,,,,,"0|i07z8f:",44475,,thobbs,,thobbs,Normal,,,,,,,,,,,,,,,,,"08/Apr/11 21:00;jbellis;ISTM the easiest fix is to always stream from the node that will be removed from the replicas for each range, unless given permission from the operator to choose a replica that is closer / less dead.;;;","03/May/11 13:41;jbellis;Related: CASSANDRA-833;;;","25/Aug/11 23:30;thepaul;So, it looks like it will be possible for the node-that-will-be-removed to change between starting a bootstrap and finishing it (other nodes being bootstrapped/moved/decom'd during that time period); in some cases, that could still lead to a consistency violation.  Is that unlikely enough that we don't care, here?  At least the situation would be better with the proposed fix than it is now.

Second question: what might the ""permission from the operator to choose a replica that is closer/less dead"" look like?  Maybe just a boolean flag saying ""it's ok to stream from any node for any range you need to stream""?  Or would we want to allow specifying precise source nodes for any/all affected address ranges?;;;","26/Aug/11 16:20;jbellis;bq. it looks like it will be possible for the node-that-will-be-removed to change between starting a bootstrap and finishing it

It's always been unsupported to bootstrap a second node into the same ""token arc"" while a previous one is ongoing.  Does that cover what you're thinking of or are we still on the hook?

bq. what might the ""permission from the operator to choose a replica that is closer/less dead"" look like?

It seems to me that the two valid choices are

- Stream from ""correct"" replica
- Stream from closest replica

I can't think of a reason to stream from an arbitrary replica other than those options.;;;","26/Aug/11 17:14;thepaul;bq. It's always been unsupported to bootstrap a second node into the same ""token arc"" while a previous one is ongoing. Does that cover what you're thinking of or are we still on the hook?

Is it also unsupported to decom within X's token arc, or move into/out of that arc, while X is bootstrapping? I think we're safe if so.;;;","26/Aug/11 17:19;jbellis;Yes.;;;","31/Aug/11 19:03;thepaul;This still needs some testing, but I'm putting it up now in case anyone has some time to take a look and make sure my approach is sane.

Adds an optional ""-n"" argument to ""nodetool join"" to allow bootstrapping from closest live node (n == non-strict). Also recognizes an optional property ""cassandra.join_strict"" which can be set to false when a bootstrap is triggered by cassandra.join_ring.;;;","31/Aug/11 19:30;nickmbailey;Seems to me like the option to stream from the closest replica might just add more confusion without really gaining anything. The node that is leaving the replica set will never be in another datacenter. It could be on a different rack, but if you are following best practices and alternating racks then it is likely either on the same rack or there is only one copy on that rack and the best case possible is streaming from another rack anyway.;;;","31/Aug/11 19:56;thepaul;Judging by the irc channel and user list, assuming people will follow best practices seems a bit of a dead end. Plus, what about the case where the node leaving the replica set is dead? You still want the option to allow choosing another to stream from. And we probably shouldn't default to choosing another without explicit permission, because of the consistency violation stuff.;;;","31/Aug/11 19:56;jbellis;bq. The node that is leaving the replica set will never be in another datacenter

I think that's only strictly true for NTS, but I'm fine leaving it out.  Not worth adding complexity for ONTS at this point.;;;","31/Aug/11 19:57;jbellis;bq. what about the case where the node leaving the replica set is dead

Good point.  We do need something to make that possible.;;;","31/Aug/11 20:02;nickmbailey;Yeah I was assuming ONTS is basically deprecated at this point. Didn't think about the dead case though. I suppose just a 'force' type of option and a warning indicating the possible consistency issues works.;;;","31/Aug/11 20:03;jbellis;As long as we need to handle the dead case I don't see any harm in having a slightly more generally-useful ""use closest"" option instead of ""force to pick random live replica"" option.;;;","31/Aug/11 20:16;nickmbailey;Well, I imagine the 'force' option would pick the nearest live node. By 'force' I mean the option should be posed to the user as ""We can't guarantee consistency in your cluster after this bootstrap since a node is down, if you would like to do this anyway, specify option X"". Just saying you can either bootstrap or bootstrap from the closest node doesn't convey the implications as well I don't think.

Maybe we are on the same page and arguing over wording though.;;;","02/Sep/11 17:10;nickmbailey;Just as initial feedback, I'm not sure we need a new getRangesWithSources method, especially with so much duplication between them. Seems like strict could be passed to the current method. Also, what about leaving getRangesWithSources how it is and passing strict to getWorkMap? That method can do the endpoint set math if it needs to and throw a more informative exception in the case that strict is set and the endpoint we want to fetch from is dead.;;;","02/Sep/11 20:03;thepaul;I did that (passing strict to getWorkMap) at first, but it wasn't too clean since it required adding a 'table' argument as well as 'strict', and it ended up replacing too much of the getRangesWithSources functionality. So then I added 'strict' as a parameter to getRangesWithSources (actually, it looks like I neglected to update its javadoc comment), but the differences between getRangesWithSources and getRangesWithStrictSource are such that a combined method feels a lot more special-casey and clunky. I like this way best in the end.;;;","02/Sep/11 20:21;nickmbailey;Well I guess it kind of depends on which approach we take as well. Is the option A) bootstrap from the right token or bootstrap from the closest token, or B) bootstrap from the right token, but if that one isn't up, bootstrap from any other token preferring the closer ones.

Like I said, I'd say B, but if you and Jonathan both disagree.;;;","02/Sep/11 21:50;thepaul;Yeah. B is probably easier on everyone, but I would say we simply can't do anything that might violate the consistency guarantee without explicit permission from the user.;;;","02/Sep/11 21:55;nickmbailey;Ok, so if we always prefer to bootstrap from the correct token, then I still think we should combine getRangesWithStrictSource and getRangesWithSources. Basically the logic should be, find the 'best' node to stream from. If the user requested it, also find a list of other candidates and order them by proximity. Right?;;;","04/Sep/11 03:38;jbellis;I'm okay with either A or B.

bq. I would say we simply can't do anything that might violate the consistency guarantee without explicit permission from the user

I'm not sure I understand, are you saying that B would violate this, or just that the status quo does?;;;","05/Sep/11 02:05;hanzhu;Is it possible to make the node does not reply to any request before bootstrap and anti-entrophy repair is finished?

This could fix the consistency problem brought by bootstrap.;;;","05/Sep/11 03:09;jbellis;Repair is a much, much more heavyweight solution to the problem than just ""stream from the node that is 'displaced.'"";;;","05/Sep/11 10:16;hanzhu;Sometimes, the node is replaced because the hardware is crashed. If so, ""streaming from the node being replaced"" is not available.

How about force the repair happens if the user specifies he needs the consistency of quorum while the original node has gone.
;;;","05/Sep/11 15:57;thepaul;bq. Ok, so if we always prefer to bootstrap from the correct token, then I still think we should combine getRangesWithStrictSource and getRangesWithSources. Basically the logic should be, find the 'best' node to stream from. If the user requested it, also find a list of other candidates and order them by proximity. Right?

I don't think so. I would still want to leave the option to stream from the closest even if the strict best node is available.;;;","05/Sep/11 16:00;thepaul;bq. I'm not sure I understand, are you saying that B would violate this, or just that the status quo does?

I'm saying B would violate this, yes. B was ""bootstrap from the right token, but if that one isn't up, bootstrap from any other token preferring the closer ones"", right? I'm saying we can't just automatically choose another token if the user didn't specifically say it's ok.;;;","05/Sep/11 16:38;nickmbailey;Paul,

The suggestion was that if the 'correct' node is down, you can force the bootstrap to complete anyway (probably from the closest node, but that is transparent to the user), but only if the 'correct' node is down. It sounds like you agree with Jonathan on the more general approach though.

Zhu,

Repair doesn't help in the case when you lost data due to a node going down. Also if only one node is down you should still be able to read/write at quorum and achieve consistency (assuming your replication factor is greater than 2).;;;","05/Sep/11 19:25;jbellis;bq. I'm saying we can't just automatically choose another token if the user didn't specifically say it's ok.

Oh, ok.  Right.  (I thought we were just bikeshedding over whether to call the ""manual override"" option ""use closest"" or ""force bootstrap."")

bq. Repair doesn't help in the case when you lost data due to a node going down

Additionally, I don't like the idea of automatically doing expensive things like repair; it feels cleaner to not do it automatically, and allow using the existing tool to perform one if desired, than to do it by default and have to add an option to skip it for when that's not desirable.;;;","05/Sep/11 23:19;hanzhu;bq. Also if only one node is down you should still be able to read/write at quorum and achieve consistency

I suppose quorum read plus quorum write should provide monotonic read consistency. [1] Supposing  quorum write on key1 hits node A and node B, not on node C due to temporal network partition. After that node B is replaced by node D since it is down, and node D streams data from node C. If the following quorum read on key1 hits only node C and node D, the monotonic consistency is violated. This is rare but not unrealistic, especially when hint handoff is disabled. 

Maybe it is more resonable to give the admin an option, to specify that the bootstrapped node should not accept any read request until the admin turn it on manually. So the admin can start a manual repair if he wants to assure everything goes fine.

[1]http://www.allthingsdistributed.com/2007/12/eventually_consistent.html;;;","06/Sep/11 01:41;thepaul;bq. The suggestion was that if the 'correct' node is down, you can force the bootstrap to complete anyway (probably from the closest node, but that is transparent to the user), but only if the 'correct' node is down.

Oh, ok. I misunderstood. This seems reasonable. I'd lean for the more general solution, yeah, but I don't feel very strongly about it.;;;","06/Sep/11 02:00;hanzhu;As peter suggested before, another approach to fix the consistency problem is streaming sstables from all alive peers if the ""correct"" node is down. And then leave them to normal compaction.  

This could be much lightweight than anti-entrophy repair, except the network IO pressure on the bootstrapping node.;;;","07/Sep/11 20:08;thepaul;updated patch fixes the docstring for getRangesWithStrictSource().;;;","07/Sep/11 20:11;thepaul;Patch 2434-testery.patch.txt adds a bit to unit tests to exercise o.a.c.dht.BootStrapper.getRangesWithStrictSource().;;;","08/Sep/11 21:57;thepaul;2434-3.patch.txt removes the bits that add the ""-n"" option to nodetool join. Apparently no ""nodetool join"" should ever result in a bootstrap, so it doesn't matter whether the caller wants ""strict"" or not.;;;","14/Sep/11 15:29;jbellis;Do we need to do anything special for move/decommission as well?;;;","14/Sep/11 16:21;nickmbailey;I had a note to remember to create a ticket for that, but if we want to do it here that works as well.

In any case, yes the same concerns exist when giving away ranges as when gaining ranges.;;;","14/Sep/11 17:23;thepaul;bq. Do we need to do anything special for move/decommission as well?

Yes, it looks like we do need to add similar logic for move. Expand the scope of this ticket accordingly?

I don't see any way decommission could be affected by this sort of problem.;;;","14/Sep/11 17:27;thepaul;bq. In any case, yes the same concerns exist when giving away ranges as when gaining ranges.

Oh? I must be missing something. What would a consistency violation failure scenario look like for giving away ranges?;;;","14/Sep/11 17:30;jbellis;bq. Expand the scope of this ticket accordingly?

Yes, let's solve them both here.;;;","14/Sep/11 17:32;nickmbailey;My comment wasn't very clear. Both decom and move currently, attempt to do the right thing. When a node is leaving, there should be one new replica for all the ranges it is responsible for. If it can't stream data to that replica there is a consistency problem.

Both operations currently try to do stream to that replica, but we should use the 'strict' logic in those cases as well and fail if we can't guarantee consistency and the user hasn't disabled strict.;;;","14/Sep/11 17:41;thepaul;If decom can't stream data to the appropriate replica, then it should just fail, right? Do we support decom in cases where a consistency violation would result? Seems like it has to be the user's responsibility to bring up or decom the other node first.

move could introduce a violation when it gains a new range, though, in the same cases as the bootstrap issue explained above.;;;","14/Sep/11 17:47;nickmbailey;If we support it for bootstrapping I don't see why we shouldn't support it for decom. Right, move has the problem in both cases (giving away ranges, gaining ranges).;;;","14/Sep/11 17:53;thepaul;I think we're talking about different things. Requiring the user to have the right nodes available for operation X is not the same as ""cassandra can 'lose' writes when it happens to stream from the wrong node, even if the user did everything right"".

This ticket is about the latter, I think.;;;","14/Sep/11 18:32;thepaul;Conversation on #cassandra-dev resulted in the conclusion that we'll fix this bug for range acquisition (bootstrap and move) now, and plan to allow the same looseness (non-strict mode, or whatever) for range egress (move and decom) in the future.

I think.;;;","20/Sep/11 11:58;jbellis;bq. It's always been unsupported to bootstrap a second node into the same ""token arc"" while a previous one is ongoing.

I'm pretty sure now that this is incorrect; we fixed it back in CASSANDRA-603.  I'm updating the comments in TokenMetadata as follows:

{noformat}
    // Prior to CASSANDRA-603, we just had <tt>Map<Range, InetAddress> pendingRanges<tt>,
    // which was added to when a node began bootstrap and removed from when it finished.
    //
    // This is inadequate when multiple changes are allowed simultaneously.  For example,
    // suppose that there is a ring of nodes A, C and E, with replication factor 3.
    // Node D bootstraps between C and E, so its pending ranges will be E-A, A-C and C-D.
    // Now suppose node B bootstraps between A and C at the same time. Its pending ranges
    // would be C-E, E-A and A-B. Now both nodes need to be assigned pending range E-A,
    // which we would be unable to represent with the old Map.  The same thing happens
    // even more obviously for any nodes that boot simultaneously between same two nodes.
    //
    // So, we made two changes:
    //
    // First, we changed pendingRanges to a <tt>Multimap<Range, InetAddress></tt> (now
    // <tt>Map<String, Multimap<Range, InetAddress>></tt>, because replication strategy
    // and options are per-KeySpace).
    //
    // Second, we added the bootstrapTokens and leavingEndpoints collections, so we can
    // rebuild pendingRanges from the complete information of what is going on, when
    // additional changes are made mid-operation.
    //
    // Finally, note that recording the tokens of joining nodes in bootstrapTokens also
    // means we can detect and reject the addition of multiple nodes at the same token
    // before one becomes part of the ring.
    private BiMap<Token, InetAddress> bootstrapTokens = HashBiMap.create();
    // (don't need to record Token here since it's still part of tokenToEndpointMap until it's done leaving)
    private Set<InetAddress> leavingEndpoints = new HashSet<InetAddress>();
    // this is a cache of the calculation from {tokenToEndpointMap, bootstrapTokens, leavingEndpoints}
    private ConcurrentMap<String, Multimap<Range, InetAddress>> pendingRanges = new ConcurrentHashMap<String, Multimap<Range, InetAddress>>();
{noformat}
;;;","20/Sep/11 15:55;thepaul;bq. I'm pretty sure now that this is incorrect;

Well, doh. That puts us back at my first question:

bq. So, it looks like it will be possible for the node-that-will-be-removed to change between starting a bootstrap and finishing it (other nodes being bootstrapped/moved/decom'd during that time period); in some cases, that could still lead to a consistency violation. Is that unlikely enough that we don't care, here? At least the situation would be better with the proposed fix than it is now.;;;","20/Sep/11 16:01;jbellis;Can you give an example for illustration?;;;","21/Sep/11 02:48;nickmbailey;Ok, so I think there are really two consistency issues here. Firstly, picking the 'right' node to stream data to/from when making changes to the ring. Secondly, disallowing concurrent changes that have overlapping ranges.

Currently we only disallow nodes from moving/decommissioning when they may potentially have data being streamed to them. There are a few examples of things we currently allow which I think are generally a bad idea.

1) Say you have nodes A and D, if you bootstrap nodes B and C at the same time in between A and D, it may turn out that the correct node to stream from for both nodes is D. Now say node C finishes bootstrapping before node B. At that point, the correct node for B to bootstrap from is technically C, although D still has the data. However, since D is no longer technically responsible for the data, the user could run cleanup on D and delete the data that B is attempting to stream.

2) The above case is also a problem when you bootstrap a node and the node it decides it needs to stream from is moving. Once that node finishes moving you could run cleanup on that node and delete data that the bootstrapping node needs. In this case, all documentation indicates you should do a cleanup after a move in order to remove old data, so it seems possibly more likely.

3) A variation of the above case is when you bootstrap a node and the node it streams from is leaving. In that case the decom may finish and the user could terminate the cassandra process and/or node breaking any streams. Not to mention the idea of a node in a decommissioned state continuing to stream seems like a bad idea. I believe it would work currently, but I'm not sure and it seems likely to break.

I can't really think of any other examples but I think thats enough to illustrate that overlapping concurrent ring changes are a bad idea and we should just attempt to prevent them in all cases. An argument could be made that this would prevent you from doubling your cluster (the best way to grow) all at once, but I don't think that's really a huge deal. At most you would need RF steps to double your cluster.;;;","21/Sep/11 18:21;thepaul;I think we can still allow overlapping concurrent ring changes, with the right set of invariants and/or operational rules. I've been trying to work on defining those. It's pretty tricky but I think I have the right way of approaching the model now. Will update later today with more.

Nick is right though, c* probably shouldn't support overlapping changes as is. Think bootstrapping >N nodes between the same two old nodes, decom'ing one node and bootstrapping another in such a way that the bootstrap source stream node becomes the wrong source, etc.;;;","21/Sep/11 18:30;nickmbailey;So the fact that the 'correct' bootstrap source switches mid stream isn't really the problem I don't think. We set up pending ranges so that when a node gets ready to bootstrap, writes start getting duplicated to both the currently correct replica, and the bootstrapping node. Since all new writes are duplicated we can stream from that node and as long as we get the entire dataset, consistency should be fine. The problem is there is nothing in place preventing someone from running cleanup or killing a decommed node or something.

I'm doubtful that the complexity of a correct set of rules for allowing overlapping ring changes is really worth the time/effort/fragility. It doesn't seem like that much of a loss to me to disallow them. Perhaps your set of rules will be super simple though :).;;;","21/Sep/11 18:40;thepaul;Right, I know how the pending-ranges writes work. But there are still possible openings for consistency violations for previously-written data, similar to the one outlined in the original ticket description. If the bootstrap stream source has an outdated value and it gets duplicated to a bootstrapping node, but then the bootstrap stream source doesn't leave the replication set (because something else changed in the interim), the outdated value is now on more nodes than it used to be- possibly now QUORUM, when previously it was safely below.;;;","22/Sep/11 21:01;thepaul;
Ok, prospective approach to totally safe range movements:

Operational rules:
* Cassandra will not allow two range motion operations (move, bootstrap, decom) at the same time on the same node.
* When a range motion operation is already pending, User should refrain from starting another range motion operation (if either motion operation overlaps the arc-of-effect of the other) until the gossip info about the first change has propagated to all affected nodes. (This is more simply approximated by the ""two minute rule"".)
* Every point in the tokenspace has the same number of natural endpoints, and they're ordered the same from the perspective of all nodes (is this an ok assumption?).
* It is User's responsibility to make sure that the right streaming source nodes are available. If they're not, the range motion operation may fail.

Procedure:
* For any motion involving range _R_, there will be a stream from endpoint _EP_source_ to endpoint _EP_dest_. Given the same information about what range motion operations are pending (_TokenMetadata_) and the range _R_, there is a bijection from _EP_source_ to _EP_dest_, shared by all nodes in the ring.
* Procedure to determine _EP_source_ from _EP_dest_:
** Let _REP_current_ be the existing (ordered) list of natural endpoints for _R_.
** Let _TM_future_ be a clone of the current _TokenMetadata_, but with all ongoing bootstraps, moves, and decoms resolved and completed.
** Let _REP_future_ be the list of (ordered) natural endpoints for _R_ according to _TM_future_.
** Let _EPL_entering_ be the list of endpoints in _REP_future_ which are not in _REP_current_ (preserving their order in _REP_future_).
** Let _EPL_leaving_ be the list of endpoints in _REP_current_ which are not in _REP_future_ (preserving their order in _REP_current_).
** _EPL_entering_ and _EPL_leaving_ are of the same length.
** Let _Pos_ be the position/index of _EP_dest_ in _EPL_entering_.
** Let _EP_source_ be the endpoint at position _Pos_ in _EPL_leaving_.
* Intuitively, this is the same as the rule expressed earlier in this ticket (stream from the node you'll replace), but also handles other ongoing range movements in the same token arc.
* These rules can be pretty trivially inverted to determine _EP_dest_ from _EP_source_.
* When any node gets gossip about a range motion occurring with its token arc-of-effect, it calculates (or recalculates) the streams in which it should be involved. Any ongoing streams which are no longer necessary are canceled, and any newly necessary streams are instigated.

I tried to construct a ruleset without that last rearrange-ongoing-streams rule, but it ended up with a pretty complicated set of extra restrictions, and a more complicated set of procedures than this.

This set of rules might look complicated, but I think it should be fairly straightforward to implement, and may even end up simpler overall than our current code.

Note that this procedure even maintains the consistency guarantee in cases like:

* In an RF=3 cluster with nodes A, E, and F, bootstrap B, C, and D in quick succession (E streams to B, F streams to C, A streams to D)
* In an RF=3 cluster with nodes A, C, and E, bootstrap B, D, and F, and decommission A, C, and E, all in quick succession (A streams to B, C streams to D, E streams to F)
* In an RF=3 cluster with nodes A, B, C, D, and E, decommission B and C in quick succession (B streams to D, C streams to E);;;","22/Sep/11 22:13;thepaul;Clarification: the cleanup operation would need to consider pending ranges in addition to the current natural range-endpoint mapping.

It would be possible with this proposal for a node _M_ to be streaming range _S_ to a bootstrapping node _Y_, and midway, for _M_ to stop being part of the replication set for _S_ (perhaps some other bootstraps nearby completed first). This should be ok for consistency, but a cleanup operation on _M_ while the stream is ongoing could potentially remove all the data in _S_ unless this change is made.

Further clarification: instead of the W+1 special case we now have for writing to a range _T_ with a pending motion, we would need to write to W+C replicas instead, where C is the number of pending motions within the replication set of _T_.;;;","16/Nov/11 22:12;jbellis;Coming back to this after the 1.0 scramble.

It sounds like you have a reasonable solution here, is there any reason not to implement it for 1.1?;;;","16/Nov/11 23:21;thepaul;bq. It sounds like you have a reasonable solution here, is there any reason not to implement it for 1.1?

Just that it's quite a bit more complex than simply disallowing overlapping ring movements, and the extra problems that come with higher complexity. I think this feature is worth it, on its own, but when i think of how much pain Brandon seems to be going through dealing with streaming code, maybe it's not.;;;","16/Nov/11 23:41;jbellis;No longer very optimistic on the ""may even end up simpler overall than our current code"" front?

TBH this area of the code is fragile and hairy and maybe starting from a clean slate with a real plan instead of trying to patch things in haphazardly would be a good thing.

But, I'd be okay with re-imposing the ""no overlapping moves"" rule and fixing the stream source problem if that's going to be substantially simpler.;;;","17/Nov/11 16:26;thepaul;No, I do think that if we tore out the existing code and replaced it, it would be simpler overall, but (a) that would probably also be true if we rewrote the existing code without implementing this; (b) it will be rather a lot of work; and (c) it may engender a whole new generation of subtle corner-case bugs (or maybe it will eliminate a lot of such bugs that already exist).;;;","17/Nov/11 16:52;jbellis;How much work would it be to add ""just one more bandaid"" for the stream source thing, in comparison?;;;","17/Nov/11 17:06;thepaul;Do you mean to implement the new ""safe range movements"" procedure outlined above, without rewriting the rest of the range movement code?

If so, I submit a SWAG of ""1/3-1/2 the cost of the rewrite option"". The bandaid would still touch a lot of different moving parts.;;;","28/Nov/11 20:01;jbellis;So either way, a substantial amount of work, but the bandaid still leaves us with rules that the operator must enforce or hit subtle problems.

My hang-up with the bandaid is the part where C* can't effectively enforce the guidelines to be safe.  Even ""wait X seconds between bootstrap operations"" is not a prereq I am comfortable with.

Unless the above is incorrect, I think we should bite the bullet and fix it ""right."";;;","08/Feb/12 04:16;thepaul;So, I believe that the rules outlined above can still work without the ""wait X seconds between bootstrap operations"" prereq, if a pretty simple extra step is added:

If any node learns about conflicting move operations, then some rules are applied to choose which will be honored and which will return an error to its caller (if still possible).

Those rules are:
* A decom for node X beats a move or bootstrap for node X
* Two decoms for node X from coordinator nodes Y and Z: the coordinator with the higher token wins
* Any other conflicts between move/bootstrap operations for the same node (which can arise in certain partition situations) are easily resolved by latest VersionedValue.

This should guarantee convergence of TokenMetadata across any affected parts of a cluster.;;;","09/Feb/12 21:44;jbellis;Sounds reasonable.;;;","02/May/12 17:32;jbellis;As a half measure, we can stream from the ""right"" node very easily if we continue to make the simplifying assumption that no other node movement happens in overlapping ranges during the operation.;;;","19/Feb/14 17:31;tjake;I've taken a crack at this, initially for 1.2 since it solves my pain. Appreciate a review.

As [~jbellis] mentions above it requires only one node to be added at a time. Also bootstrapping node must add -Dconsistent.bootstrap=true

#code
https://github.com/tjake/cassandra/tree/2434

#dtest showing it works (use ENABLE_VNODES=yes)
https://github.com/tjake/cassandra-dtest/tree/2434

;;;","19/Feb/14 18:33;jbellis;([~thobbs] to review);;;","19/Feb/14 21:05;thobbs;Thanks, Jake.

I strongly prefer to default to the strict/safe behavior and make the user supply a ""force"" option for non-strict behavior, like Nick and Paul agreed on above.  If the bootstrapping node cannot stream from the correct replica and the ""force"" option isn't set, it should abort the bootstrap with an error that describes the implications and mentions how to use the ""force"" option.

Additionally, I think your logic for picking the preferred replica could be greatly simplified.  Paul's 2434-3.patch.txt has a really simple version of this and also has the strict-by-default behavior.  It might be worthwhile to look at rebasing that patch as a start.

Paul mentioned this:

bq. Conversation on #cassandra-dev resulted in the conclusion that we'll fix this bug for range acquisition (bootstrap and move) now, and plan to allow the same looseness (non-strict mode, or whatever) for range egress (move and decom) in the future.

Looking at the irc logs, there wasn't a strong reason for this.  There's a lot of code overlap there, so it would be ideal to fix both types of operations at once.  Do you think you could take a stab at that?;;;","19/Feb/14 22:29;tjake;bq. Additionally, I think your logic for picking the preferred replica could be greatly simplified. Paul's 2434-3.patch.txt has a really simple version of this and also has the strict-by-default behavior. It might be worthwhile to look at rebasing that patch as a start.

I did look at the patch and I'll see how I can simplify my version.  Most of the complexity comes from multiple ranges living on the same address (vnodes). Which the old version didn't have to worry about.

I do think we can fix the other operations but those are less of a priority IMO and should be part of a follow up.  (does anyone use move with vnodes?)  

;;;","19/Feb/14 22:39;brandon.williams;bq. does anyone use move with vnodes?

No, but now they can use relocate (taketoken in nodetool);;;","20/Feb/14 18:57;tjake;Pushed an update to https://github.com/tjake/cassandra/tree/2434 that addresses the comments.  I'm going to work on support for move/relocate.;;;","20/Feb/14 20:41;tjake;Do we care about decommissions?  It seems when we ""push"" data to other nodes there isn't anything todo.  Only when we pick the replica to stream from does this ticket apply;;;","20/Feb/14 21:27;thobbs;bq. Do we care about decommissions? It seems when we ""push"" data to other nodes there isn't anything todo. Only when we pick the replica to stream from does this ticket apply

I checked that {{StorageService.getChangedRangesForLeaving()}} pushes to the correct nodes (those that are gaining a range), so you're right, we don't need to do anything new for decom.;;;","20/Feb/14 22:22;tjake;Updated branches with move/relocate support and added a dtest for move (in dtest branch linked above);;;","20/Feb/14 22:33;brandon.williams;We should probably add a test for relocate too since it's fundamentally different from move.;;;","21/Feb/14 20:12;thobbs;I tested bootstrapping a node while the preferred replica was down.  It turns out that CASSANDRA-6385 makes the bootstrapping node consider the replica up for long enough to pass the checks.  It looks like we need to special case the 6385 behavior for bootstraps if we want this patch to work.;;;","21/Feb/14 20:24;brandon.williams;You can test this now by setting cassandra.fd_initial_value_ms.;;;","22/Feb/14 00:42;thobbs;Okay, with the workaround on the FD, bootstrap seems to work.  Do we want to split that fix into a separate ticket?

However, relocate seems to be seriously broken.  With a three node cluster and one of the nodes down, I can make relocate fail in a couple of ways:
* {{oldEndpoints}} == {{newEndpoints}}, so the assertion that the difference between them has length 1 fails
* There are no ranges that contain the ""desiredRange"", resulting in the IllegalStateException being thrown (""No sources found for "" + toFetch);

With that said, nothing (including the tools) uses relocate.  (EDIT: shuffle uses it, but nobody uses shuffle in practice due to other problems.) The JMX version doesn't work with jconsole, so I had to add a method to test this.  I'm not even sure that relocate worked before this patch for vnodes, because there's only minimal test coverage for relocate.  IMO, we shouldn't even try to modify this without good test coverage.  But if nothing even uses relocate... I'm not sure what to do.  Thoughts?;;;","26/Feb/14 18:09;tjake;I will try working on a relocate test and look at addressing this;;;","14/Mar/14 13:22;tjake;What version should this go into?  I personally need this for 1.2 but I would put it in with the default of non-strict.;;;","18/Mar/14 19:45;thobbs;[~tjake] I think we want 2.1 with a default of strict.;;;","29/Apr/14 19:37;tjake;Rebased to 2.1 branch and pushed to https://github.com/tjake/cassandra/tree/2434-2

Also added a relocation dtest https://github.com/tjake/cassandra-dtest/tree/2434 ;;;","30/Apr/14 21:57;thobbs;+1 overall, with a few minor nitpicks on the dtest:

You can replace that string-building loop with:
{noformat}
tl = "" "".join(str(t) for t in tokens[0][:8])
{noformat}

There's also a leftover line: {{#assert 1 == 0}}

Don't forget to mark the new tests with a {{@since('2.1')}} decorator.;;;","01/May/14 13:52;tjake;Committed c* code , I'll push to dtests now;;;","21/Jul/14 21:18;kohlisankalp;This will be very nice to have in 2.0
cc [~brandon.williams];;;","21/Jul/14 21:44;brandon.williams;Does seem like mostly new code we could just add with the flag defaulting to off to give 2.0 the option.;;;",,,,
Failed Streams Break Repair,CASSANDRA-2433,12503646,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,bcoverston,bcoverston,07/Apr/11 15:40,16/Apr/19 09:33,14/Jul/23 05:52,31/Aug/11 16:36,0.8.5,,,,,,5,repair,,,"Running repair in cases where a stream fails we are seeing multiple problems.

1. Although retry is initiated and completes, the old stream doesn't seem to clean itself up and repair hangs.
2. The temp files are left behind and multiple failures can end up filling up the data partition.

These issues together are making repair very difficult for nearly everyone running repair on a non-trivial sized data set.

This issue is also being worked on w.r.t CASSANDRA-2088, however that was moved to 0.8 for a few reasons. This ticket is to fix the immediate issues that we are seeing in 0.7.",,bcoverston,cherro,colinkuo,ijuma,jborgstrom,jeromatron,mdennis,stuhood,vijay2win@yahoo.com,yulinyen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2610,,,"15/Jun/11 12:27;slebresne;0001-Put-repair-session-on-a-Stage-and-add-a-method-to-re-v4.patch;https://issues.apache.org/jira/secure/attachment/12482658/0001-Put-repair-session-on-a-Stage-and-add-a-method-to-re-v4.patch","15/Jun/11 12:27;slebresne;0002-Register-in-gossip-to-handle-node-failures-v4.patch;https://issues.apache.org/jira/secure/attachment/12482659/0002-Register-in-gossip-to-handle-node-failures-v4.patch","15/Jun/11 12:27;slebresne;0003-Report-streaming-errors-back-to-repair-v4.patch;https://issues.apache.org/jira/secure/attachment/12482660/0003-Report-streaming-errors-back-to-repair-v4.patch","15/Jun/11 12:27;slebresne;0004-Reports-validation-compaction-errors-back-to-repair-v4.patch;https://issues.apache.org/jira/secure/attachment/12482661/0004-Reports-validation-compaction-errors-back-to-repair-v4.patch","02/Aug/11 17:52;slebresne;2433.patch;https://issues.apache.org/jira/secure/attachment/12489094/2433.patch","30/Aug/11 15:25;slebresne;2433_v2.patch;https://issues.apache.org/jira/secure/attachment/12492251/2433_v2.patch","31/Aug/11 14:52;slebresne;2433_v3.patch;https://issues.apache.org/jira/secure/attachment/12492464/2433_v3.patch","31/Aug/11 15:46;slebresne;2433_v4.patch;https://issues.apache.org/jira/secure/attachment/12492468/2433_v4.patch",,,,,,,8.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20622,,,Wed Aug 31 17:16:01 UTC 2011,,,,,,,,,,"0|i0gbcn:",93269,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"21/Apr/11 11:45;slebresne;Attached patches are against 0.8.

This tries to catch what can go wrong with repair and reports it back to the user by making the full repair throw an exception. More precisely:
  * patch 0001: add a method to repair for reporting failure and propagate that up to the repair session. This puts repair session on a specific stage (instead of having RepairSession be a Thread) and use a future to allow waiting on completion. This allows a cleaner API to deal with errors (the Future.get() simply throw an ExecutionException) and this add the advantage of stage management to repair sessions.
  * patch 0002: Make repair session register through gossip to be informed of node dying and failing the session when that happens.
  * patch 0003: Reports errors during streaming to the repair session. This actually introduces a generic way to handle streaming failures and after that we should probably update the other user of streaming to deal correctly with failure too.
  * patch 004: Catch errors during validation compaction and push them up to repair (whether those happens on the coordinator of the repair or not).

Note that this includes streaming failures and thus includes stuffs from the patch of Aaron Morton attached on CASSANDRA-2088, but contrarily to that patch, it takes the approach of failing fast. This means that if streaming fails on a file, it fails the streaming altogether (same for repair). I think this is simpler code-wise and more useful from the point of view of the user, since a failure means the use will have to retry anyway.

Last but not least, this makes some modification to messages. So either this goes into 0.8.0 (which I think it should, because this really is a bug fix and fixes something that is a pain for users), or we should had a new messaging version for 0.8.0 and modify this to take it into account (we should probably add a 0.8.0 version to the messaging service anyway).
;;;","17/May/11 20:01;slebresne;Attaching rebased patch (against 0.8.1). It also change the behavior a little bit so as to not fail repair right away if a problem occur (it still throw an exception at the end if any problem had occured). It turns out to be slightly simpler that way. Especially for CASSANDRA-1610.;;;","23/May/11 20:09;stuhood;0001
* Since we're not trying to control throughput or monitor sessions, could we just use Stage.MISC?

0002
* I think RepairSession.exception needs to be volatile to ensure that the awoken thread sees it
* Would it be better if RepairSession implemented IEndpointStateChangeSubscriber directly?
* The endpoint set needs to be threadsafe, since it will be modified by the endpoint state change thread, and the AE_STAGE thread

0003
* Should StreamInSession.retries be volatile/atomic? (likely they won't retry quickly enough for it to be a problem, but...)

0004
* Playing devil's advocate: would sending a half-built tree in case of failure still be useful?
* success might need to be volatile as well

Thanks Sylvain!;;;","09/Jun/11 14:09;slebresne;Attaching v3 rebased (on 0.8).

bq. Since we're not trying to control throughput or monitor sessions, could we just use Stage.MISC?

The thing is that repair session are very long lived. And MISC is single threaded. So that would block other task that are not supposed to block. We could make MISC multi-threaded but even then it's not a good idea to mix short lived and long lived task on the same stage.

bq. I think RepairSession.exception needs to be volatile to ensure that the awoken thread sees it

Done in v3.

bq. Would it be better if RepairSession implemented IEndpointStateChangeSubscriber directly?

Good idea, it's slightly simpler, done in v3.

bq. The endpoint set needs to be threadsafe, since it will be modified by the endpoint state change thread, and the AE_STAGE thread

Done in v3. That will probably change with CASSANDRA-2610 anyway (which I have to update)

bq. Should StreamInSession.retries be volatile/atomic? (likely they won't retry quickly enough for it to be a problem, but...)

I did not change that, but if it's a problem for retries to not be volatile, I suspect having StreamInSession.current not volatile is also a problem. But really I'd be curious to see that be a problem.

bq. Playing devil's advocate: would sending a half-built tree in case of failure still be useful?

I don't think it is. Or more precisely, if you do send half-built tree, you'll have to be careful that the other doesn't consider what's missing as ranges not being in sync (I don't think people will be happy with tons of data being stream just because we happen to have a bug that make compaction throw an exception during the validation). So I think you cannot do much with a half-built tree, and it will add complication. For a case where people will need to restart a repair anyway once whatever happened is fixed

bq. success might need to be volatile as well

Done in v3.
;;;","15/Jun/11 12:27;slebresne;Attaching v4 that is rebased and simply set the reties variable in StreamInSession volatile after all (I've removed old version because it was a mess).;;;","18/Jul/11 08:08;stuhood;Hey Sylvain: sorry it took me so long to get back to this one. Would you mind rebasing it?;;;","02/Aug/11 17:52;slebresne;Attaching a rebase of the two previous first patches as '2433.patch'. That is, this patch adds registering in gossip so that repair fails and report it to the user when a node participating to the repair dies. Compared to the previous version, it fails fast because it's the easier thing to do now and a better option imho.

I should mention that while it is lame that repair get stuck when a node dies and we should fix it, this means that if a node is wrongly marked down, we will fail repair for no reason (but I suppose it's a failure detector problem).

Attached patch is against 0.8. This has no upgrade consequence of any sort and is a reasonably simple patch, so I think it could be worth committing in 0.8.
The rest of what was in previous patch 0003 and 0004 cannot go into 0.8 because it changes the wire protocol, so I will rebase against trunk directly, and maybe in another ticket. Having this first patch committed would help with that though :);;;","08/Aug/11 17:46;jbellis;Yuki, can you review this patch?;;;","30/Aug/11 15:25;slebresne;Attached v2 is rebased and use a higher conviction threshold before deciding to fail the repair, as the goal here is to avoid having a repair getting stuck for hours, but we want to avoid stopping a repair just because a node got into a longer than usual GC pause.

The threshold used is twice the configured phi_convict_threshold. This give 16 by default, which if I trust the original 'phi accrual failure detection' should give an order of magnitude less false positive than 8 (for about an order of magnitude in the detection time though). It feels reasonable to me but if a FD specialist want to voice his opinion, please do.;;;","30/Aug/11 15:42;jbellis;- Why do we need the new AE_SESSIONS stage?
- I prefer using WrappedRunnable to a Callable when you want to allow exceptions but don't care about a return value
- I think we can avoid a bunch of no-op onConvicts if RepairSession were to subscribe to FD directly instead of going through Gossip (i.e., leave IEndpointStateChangeSubscriber unchanged and expose convict in IFailureDetectionEventListener for when we need to go low-level).  Gossip is about high-level ""events"" which doesn't really fit here.;;;","30/Aug/11 16:37;slebresne;bq. Why do we need the new AE_SESSIONS stage?

If you mean ""why AE_SESSIONS when we already have the AE stage?"", then it is because repair push stuffs on the AE stage that it wait for, so we would deadlock. If you mean ""why a stage?"", it felt cleaner that just a Thread now that we want to check for exception at the end of the exception. If you mean ""why a stage rather than a simple ThreadExecutor?"", it is a good question. I guess it was just some reflex of mine to get a JMXEnabledThreadPool, but it's probably not worth a stage, not even the jmx enabledness maybe.

bq. I prefer using WrappedRunnable to a Callable when you want to allow exceptions but don't care about a return value

Agreed. I'll update the patch.

bq. I think we can avoid a bunch of no-op onConvicts if RepairSession were to subscribe to FD directly instead of going through Gossip

Yeah, I kind of started with that but the problem is that we must deal with the case of a node restarting before it has been convicted (especially if the conviction threshold is higher), which the FD won't see. We could deal of that last situation separately and have Gossip call some trigger into AntiEntropy on a gossip generation change to indicate to stop every started session involving the given endpoint, but creating a dependency of gossip to anti-entropy didn't felt like a good idea a priori.;;;","30/Aug/11 16:44;jbellis;bq. it's probably not worth a stage, not even the jmx enabledness maybe

Someone's probably going to want the JMX information but let's keep Stages for Verb-associated tasks.

bq. the problem is that we must deal with the case of a node restarting before it has been convicted (especially if the conviction threshold is higher), which the FD won't see

How about splitting onDead and onRestart in EndpointStateChange, then?  Then RS could implement convict and onRestart (ignoring onDead); other ESCS listeners could implement onRestart == onDead.  That would maintain the ""ESCS is about events, FDEL is low-level convict information"" separation of roles.;;;","31/Aug/11 14:46;slebresne;bq. Someone's probably going to want the JMX information but let's keep Stages for Verb-associated tasks

Sounds good, updated patch add a new executor directly into AntiEntropy.

bq. How about splitting onDead and onRestart in EndpointStateChange, then?

Done.

bq. I prefer using WrappedRunnable to a Callable

I changed to use WrappedRunnable. However, we still need to have access to both the repair session and the future from the executor so the implementation returns a pair of those two objects. I'm only marginally convinced this is cleaner than the previous solution...
;;;","31/Aug/11 14:52;slebresne;(Sorry, I had attached the wrong version of v3, corrected now);;;","31/Aug/11 14:56;jbellis;bq. we still need to have access to both the repair session and the future from the executor so the implementation returns a pair of those two objects

You can still use the RepairFuture approach, just use the FutureTask(Runnable, V) constructor;;;","31/Aug/11 15:46;slebresne;You're right, don't know why I got carried away like that. v4 ""fixes"" this.;;;","31/Aug/11 15:51;jbellis;+1;;;","31/Aug/11 16:36;slebresne;Committed, thanks.

This probably solves most of the case where repair was hanging infinitely. I've created CASSANDRA-3112 to handle the remaining cases, but it is much less urgent imho. Marking that one as resolved;;;","31/Aug/11 17:16;hudson;Integrated in Cassandra-0.8 #306 (See [https://builds.apache.org/job/Cassandra-0.8/306/])
    Make repair report failure when a participating node dies
patch by slebresne; reviewed by jbellis for CASSANDRA-2433

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1163677
Files : 
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/FailureDetector.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/IEndpointStateChangeSubscriber.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/gms/IFailureDetectionEventListener.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/AntiEntropyService.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/MigrationManager.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageLoadBalancer.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/service/AntiEntropyServiceTestAbstract.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add key_validation_class support to cli,CASSANDRA-2432,12503630,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,erny1803,erny1803,07/Apr/11 12:42,16/Apr/19 09:33,14/Jul/23 05:52,09/Apr/11 18:20,0.8 beta 1,,,Legacy/Documentation and Website,,,0,,,,Also update README to include utf8type key validator.,"svn: 1089671
Ubuntu 10.10 / amd64
Java(TM) SE Runtime Environment (build 1.6.0_24-b07)
Java HotSpot(TM) 64-Bit Server VM (build 19.1-b02, mixed mode)",erny1803,,,,,,,,,,,,,,,,,,,,,1200,1200,,0%,1200,1200,,,,,,,,,,,,,,,,"09/Apr/11 13:40;xedin;CASSANDRA-2432.patch;https://issues.apache.org/jira/secure/attachment/12475888/CASSANDRA-2432.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20621,,,Sat Apr 09 19:17:18 UTC 2011,,,,,,,,,,"0|i0gbcf:",93268,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"09/Apr/11 18:20;jbellis;committed;;;","09/Apr/11 19:17;hudson;Integrated in Cassandra #839 (See [https://hudson.apache.org/hudson/job/Cassandra/839/])
    add key_validation_class support to cli
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-2432
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Try harder to close scanners after a failed compaction,CASSANDRA-2431,12503617,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stuhood,stuhood,stuhood,07/Apr/11 09:30,16/Apr/19 09:33,14/Jul/23 05:52,07/Apr/11 20:15,0.7.5,,,,,,0,,,,Forked from 2191.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/11 09:32;stuhood;0001-Try-harder-to-close-scanners-in-compaction-close.txt;https://issues.apache.org/jira/secure/attachment/12475692/0001-Try-harder-to-close-scanners-in-compaction-close.txt",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20620,,,Thu Apr 07 23:44:17 UTC 2011,,,,,,,,,,"0|i0gbc7:",93267,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"07/Apr/11 20:15;slebresne;+1

(Committed as r1089976 and merged to trunk as 1089980);;;","07/Apr/11 21:50;hudson;Integrated in Cassandra #835 (See [https://hudson.apache.org/hudson/job/Cassandra/835/])
    Merge CASSANDRA-2431 from 0.7
;;;","07/Apr/11 23:44;hudson;Integrated in Cassandra-0.7 #426 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/426/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running cleanup on a node with join_ring=false removes all data,CASSANDRA-2428,12503560,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,lenn0x,lenn0x,06/Apr/11 21:58,16/Apr/19 09:33,14/Jul/23 05:52,07/Apr/11 21:00,0.7.5,,,,,,0,,,,"If you need to bring up a node with join_ring=false for operator maintenance, and this node already has data, it will end up removing the data on the node. We noticed this when we were calling cleanup on a specific CF.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/11 20:01;slebresne;0001-Don-t-allow-cleanup-when-node-hasn-t-join-the-ring.patch;https://issues.apache.org/jira/secure/attachment/12475742/0001-Don-t-allow-cleanup-when-node-hasn-t-join-the-ring.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20619,,,Thu Apr 07 23:44:17 UTC 2011,,,,,,,,,,"0|i0gbbj:",93264,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"06/Apr/11 21:58;lenn0x;If curious why we were running join_ring = false after the node had data, it was because we had ran out of space on a node, and many SSTables had been created, but unable to compact. When we brought the node back online after moving some data off, the number of SSTables being read w/ live traffic caused us to run out of file descriptors. So we decided to run join_ring=false on bootup, and let compaction run without taking on traffic. This worked really well, and we wanted to finalize our cleanup process.
;;;","07/Apr/11 20:01;slebresne;The ability of not joining on startup is a convenience feature but is honestly a bit half backed. In particular the token ranges are not set correctly. That's the reason why cleanup gets an empty list of ranges when querying the local ranges and thus clean everything.

Attached patch band-aid this by refusing to do a cleanup compaction. I don't think we want to do more for 0.7. We could make join_ring=false better so that it sets the ranges based and what's in the system tables, but let's fix the dangerous behavior first and let that for another ticket.;;;","07/Apr/11 20:38;kingryan;Sylvain-

That seems like the right plan.;;;","07/Apr/11 20:40;jbellis;+1;;;","07/Apr/11 21:00;slebresne;Committed as r1090007;;;","07/Apr/11 23:44;hudson;Integrated in Cassandra-0.7 #426 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/426/])
    Fix unit tests for CASSANDRA-2428
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTables per read metric is incorrect,CASSANDRA-2422,12503468,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,05/Apr/11 22:25,16/Apr/19 09:33,14/Jul/23 05:52,05/Apr/11 22:47,0.8 beta 1,,,,,,0,,,,"The sstables per read metric is currently recording the count of live sstables, rather than the number that are being read.",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/11 22:43;stuhood;0001-Ugh.txt;https://issues.apache.org/jira/secure/attachment/12475538/0001-Ugh.txt",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20616,,,Wed Apr 06 00:03:16 UTC 2011,,,,,,,,,,"0|i0gba7:",93258,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"05/Apr/11 22:33;jbellis;Unless this is a trunk-only regression, let's fix for 0.7.5;;;","05/Apr/11 22:43;stuhood;Patch for trunk: not a problem in the 0.7 branch.;;;","05/Apr/11 22:47;jbellis;committed, thanks!;;;","06/Apr/11 00:03;hudson;Integrated in Cassandra #830 (See [https://hudson.apache.org/hudson/job/Cassandra/830/])
    fix sstable read count regression
patch by Stu Hood; reviewed by jbellis for CASSANDRA-2422
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make sure we get the output schema when we output to cassandra from pig,CASSANDRA-2421,12503461,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jeromatron,jeromatron,jeromatron,05/Apr/11 21:44,16/Apr/19 09:33,14/Jul/23 05:52,05/Apr/11 21:59,,,,,,,0,contrib,hadoop,pig,see summary,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/11 21:46;jeromatron;2421.txt;https://issues.apache.org/jira/secure/attachment/12475532/2421.txt",,,,,,,,,,,,,,1.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20615,,,Tue Apr 05 22:58:55 UTC 2011,,,,,,,,,,"0|i0gb9z:",93257,,,,,Normal,,,,,,,,,,,,,,,,,"05/Apr/11 21:46;jeromatron;Added initSchema(), made the initSchema method only do its thing once per job, and changed the UDFContext stuff from ResourceSchema to CassandraStorage.;;;","05/Apr/11 21:59;brandon.williams;Committed.;;;","05/Apr/11 22:58;hudson;Integrated in Cassandra-0.7 #422 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/422/])
    Optimize schema fetch/store.
Patch by Jeremy Hanna, reviewed by brandonwilliams for CASSANDRA-2421
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
row cache / streaming aren't aware of each other,CASSANDRA-2420,12503457,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,mdennis,mdennis,05/Apr/11 21:26,16/Apr/19 09:33,14/Jul/23 05:52,18/Apr/11 14:52,0.7.5,0.8 beta 1,,,,,0,,,,"SSTableWriter.Builder.build() takes tables that resulted from streaming, repair, bootstrapping, et cetera and builds the indexes and bloom filters before ""adding"" it so the current node is aware of it.

However, if there is data present in the cache for a row that is also present in the streamed table the row cache can over shadow the data in the newly built table.  In other words, until the row in row cache is removed from the cache (e.g. because it's pushed out because of size, the node is restarted, the cache is manually cleared) the data in the newly built table will never be returned to clients.

The solution that seems most reasonable at this point is to have SSTableWriter.Builder.build() (or something below it) update the row cache if the row key in the table being built is also present in the cache.
",,cburroughs,skamio,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/11 16:03;slebresne;0001-Handle-the-row-cache-for-streamed-row-v2.patch;https://issues.apache.org/jira/secure/attachment/12476252/0001-Handle-the-row-cache-for-streamed-row-v2.patch","11/Apr/11 19:54;slebresne;0001-Handle-the-row-cache-for-streamed-row.patch;https://issues.apache.org/jira/secure/attachment/12476045/0001-Handle-the-row-cache-for-streamed-row.patch","18/Apr/11 14:32;slebresne;2420-for-0.7.patch;https://issues.apache.org/jira/secure/attachment/12476612/2420-for-0.7.patch",,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20614,,,Mon Apr 18 15:06:45 UTC 2011,,,,,,,,,,"0|i0gb9r:",93256,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"11/Apr/11 19:54;slebresne;There is a very simple patch for this issue. It consists in invalidating the cache for each key we index. The downside is that this will invalidate all key that gets repaired, but updating the cache (instead of invalidating) implies reading on disk so doing this during the indexing or at the next read may not matter much. In any case, this is better that the current situation and after all .

I however attached a patch (against trunk for now) that 'do the right thing' and will update the cache in the case of repair instead of invalidating. I mentioned the first solution in case we consider that the 'right one' is too disruptive for 0.7 for instance (not that the patch is very complicated).

Note that the patch fixes a tiny unrelated issue: the writeStat are not updated during a write if the used cache has 'isPutCopying' (this could be fixed separately).
;;;","11/Apr/11 20:49;jbellis;I would be more comfortable having LCR throw UnsupportedOperation if asked for full row, since You Shouldn't Do That.

Would prefer the updateCache case to be AES: ... default: invalidate and break; it's more obvious looking at it what the point is, and ""unnecessary"" invalidate calls will be harmless.;;;","13/Apr/11 16:06;slebresne;bq. I would be more comfortable having LCR throw UnsupportedOperation if asked for full row, since You Shouldn't Do That.

Updated patch defines getFullColumnFamily() only for AbstractCompactedRow. However I think it would be a bad idea to fail in the Builder, so the Builder now simply invalidate the cache if he is facing a big row (hence not fitting it in memory) and log a warning since if that happens ""you're doing it wrong"".

I've also changed the switch case in updateCache.;;;","14/Apr/11 20:30;jbellis;+1;;;","14/Apr/11 20:32;jbellis;nit: s/higly/highly/ in the logged warning;;;","18/Apr/11 11:21;hudson;Integrated in Cassandra #854 (See [https://hudson.apache.org/hudson/job/Cassandra/854/])
    Merge CASSANDRA-2420 from 0.8
;;;","18/Apr/11 11:34;hudson;Integrated in Cassandra-0.8 #13 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/13/])
    Update row cache post streaming
patch by slebresne; reviewed by jbellis for CASSANDRA-2420
;;;","18/Apr/11 13:21;slebresne;Committed to 0.8 and trunk.
Was should we do about 0.7 ? I realized that we do not differentiate between the different reason for streaming in 0.7, so the simplest way to deal with this would probably be to just blindly invalidate the cache. Sounds reasonable ?;;;","18/Apr/11 13:30;jbellis;Yes.;;;","18/Apr/11 14:32;slebresne;Attaching simple patch targeting 0.7. I put it for review individually because it's different enough from previous patch (but it's a one-liner, so should be too long to review anyway);;;","18/Apr/11 14:38;jbellis;+1;;;","18/Apr/11 14:52;slebresne;Committed to 0.7 and 0.7/trunk;;;","18/Apr/11 15:06;hudson;Integrated in Cassandra-0.7 #437 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/437/])
    Invalidate cache for streamed rows
patch by slebresne; reviewed by jbellis for CASSANDRA-2420
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Risk of counter over-count when recovering commit log,CASSANDRA-2419,12503450,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,05/Apr/11 20:49,16/Apr/19 09:33,14/Jul/23 05:52,08/May/11 01:42,0.8.0,,,,,,1,counters,,,"When a memtable was flush, there is a small delay before the commit log replay position gets updated. If the node fails during this delay, all the updates of this memtable will be replay during commit log recovery and will end-up being over-counts.",,cburroughs,rcoli,stuhood,,,,,,,,,,,,,,,,,,,28800,28800,,0%,28800,28800,,,,,,,,,,,,,,,,"29/Apr/11 13:57;slebresne;0001-Record-CL-replay-infos-alongside-sstables-v2.patch;https://issues.apache.org/jira/secure/attachment/12477754/0001-Record-CL-replay-infos-alongside-sstables-v2.patch","07/Apr/11 00:38;slebresne;0001-Record-and-use-sstable-replay-position.patch;https://issues.apache.org/jira/secure/attachment/12475649/0001-Record-and-use-sstable-replay-position.patch","02/May/11 19:53;jbellis;2419-v3.txt;https://issues.apache.org/jira/secure/attachment/12477981/2419-v3.txt","03/May/11 01:57;jbellis;2419-v4.txt;https://issues.apache.org/jira/secure/attachment/12478017/2419-v4.txt","03/May/11 15:37;jbellis;2419-v6.txt;https://issues.apache.org/jira/secure/attachment/12478061/2419-v6.txt",,,,,,,,,,5.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20613,,,Tue May 10 22:30:29 UTC 2011,,,,,,,,,,"0|i0gb9j:",93255,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"05/Apr/11 21:02;slebresne;One solution I see to this problem would be to record along with the replay position the time when we last updated this replay position. The during recover, we would first look at all the sstables (for the CF) and if a sstable is freshly flushed (which implies that we have a marker to know that a sstable was never compacted) and have a modification time higher that the last time we updated the replay position, then we'll just remove the sstable since we know it will be fully replayed.

Note that to work correctly we also need a way to mark a freshly flushed sstable as 'non compactable' during the time it takes to mark the commit log.

We would probably only do this for counter CF just to be on the safe side.

Opinions ?;;;","05/Apr/11 21:25;jbellis;What if instead of the CL ""header"" we record the CL context as part of an sstable footer?  (footer is less likely to cause bugs w/ sstable math that assumes 0 = start of first row.)  then there is no race.;;;","05/Apr/11 21:34;jbellis;Hmm, I think we need both the CL header and this information, since this flush footer would only give us when we flushed which is not the same as ""do I need to replay.""

For instance: if there is no flush marker for a commitlog segment in any existing sstable, that does not necessarily mean no data is in the commitlog for that CF.

So replay position would be max(dirty at from CL header, flushed at from sstable footers).

(You would need to allow multiple flush contexts in a single sstable footer, to preserve them during compaction.);;;","05/Apr/11 21:39;slebresne;What about a new component .metadata for each sstable instead of a footer. I actually think we will have a use for other sstable metadata at some point anyway. For instance we could keep the file format version. That way we wouldn't rely so much on the data file name.;;;","05/Apr/11 21:44;jbellis;A separate component is a better idea.;;;","05/Apr/11 21:52;slebresne;Actually I don't think this really solves the race condition. We really shouldn't compact the newly flushed sstable until we marked the commit log, because if we compact it, even if we're able to detect that 'some parts' of a sstable will be replay during recover, there is nothing we can do about it.;;;","05/Apr/11 22:01;jbellis;I don't understand the problem.  Say we have this situation:

CommitLog-1302036825548.log: [full of writes to CF Foo counters, up to position 100.  header reads dirty-at 50, our last flush position]
Foo-g-45-Metadata.db: [flushed at position (1302036825548, 50)]
Foo-g-46-Metadata.db: [flushed at position (1302036825548, 100)]

We compact and get
Foo-g-47-Metadata.db: [flushed at position (1302036825548, 100)]

If we die and restart here we will correctly start reply of Foo at position 100 in this segment.

(we can combine to a single flushed-at entry in this case since they were from the same CL segment.  if they were from different segments we would keep both.);;;","05/Apr/11 22:10;slebresne;Right, that was just me not getting you idea at first. Make sense, sorry.;;;","07/Apr/11 00:38;slebresne;Attaching patch implementing Jonathan's idea to record the CL replay position along with the sstable.;;;","07/Apr/11 00:40;slebresne;I'm wondering, couldn't we just drop the commit log header if we do that.;;;","07/Apr/11 01:56;jbellis;Yes, I think we can.;;;","14/Apr/11 21:39;jbellis;bq. I'm wondering, couldn't we just drop the commit log header if we do that.

Were you planning to update w/ that change?;;;","29/Apr/11 13:57;slebresne;v2 removes commit log header completely in favor of sstable metadata about where to replay (patch against 0.8).

This differs from v1 in that instead of keeping every (segment, replay_position) pair, we keep for a given sstable, only the position for the most recent segment (that is, we leverage the fact that we use increasing timestamps for commit logs).

The reason for this is twofold:
  # this more compact (and simple)
  # if we remove the commit log header, we need to be able to say if a given segment is dirty or not for a given column family. That is, we don't want to know if some replay position existed on this segment, but if a relevant one still exist. So for a given column family we really only care about the newest (segment, replay_position) pair.

Now there is the question of the update path. With this patch, the (existing) commit log headers will be ignored. This means that ideally before updating to a version having this patch people would use drain. If they do not, then the commit logs will be fully replayed. Pre-0.8, it's not a big deal. With counters, this could mean over-counts (that's exactly what this ticket is about). So I would be in favor of putting this for 0.8.0, since it is a bug fix and it will avoids the problem of upgrading from a version already having counters. But I would admit this is not trivial patch, so ...
;;;","02/May/11 19:53;jbellis;v3 attached with some changes:

- SSTableMetadata removed; replayposition becomes a field in SSTable that is serialized w/ statistics
- RP is final and part of the SSTable constructor
- RP implements Comparable instead of a one-off resolve API; RP.getReplayPosition encapsulates the find-replay-point logic
- RP moved to a top-level class and replaces CLContext

I'd like to make the metadata a json blob so we can extend it more easily, so I probably need to re-introduce SSTM. Consider v3 a work in progress.;;;","03/May/11 01:57;jbellis;I tried two ways of storing metadata as yaml -- first with the metadata as java beans that were stored directly as yaml, and second half-manually serializing to a yaml Map<String, Object> -- and both feel clunkier than just using the version field to deal with adding things. (Especially when you need to do version checks anyway when modifying things rather than just adding new fields.)

So, v4 is substantially the same as v3 but Descriptor version is bumped to g and we use that instead of EOF to when reading RP. (Also, writeStatistics is renamed to writeMetadata.);;;","03/May/11 12:32;slebresne;v4 looks good, I like those changes (note: I committed r1099037 to fix CFSTest, it had a test with hardcoded sstable filenames using version 'f' and thus was failing);;;","03/May/11 15:00;jbellis;v5 updates CL replay to use RP.getReplayPosition as well;;;","03/May/11 15:37;jbellis;v6 fixes a test failure in v5;;;","03/May/11 17:05;slebresne;Minor nitpick: in CommitLog.java:recover(), was there a reason to create a new List<ReplayPosition> positions instead of using cfPositions.values() ?

but other than that, +1 on v6.;;;","03/May/11 17:35;jbellis;bq. in CommitLog.java:recover(), was there a reason to create a new List<ReplayPosition> positions instead of using cfPositions.values()

Nope. I'll fix that before commit.

Asked Paul Cannon to also review first though, since obviously we want to be extra sure not to cause regressions here.;;;","08/May/11 01:42;jbellis;committed;;;","09/May/11 19:12;stuhood;Thanks a ton for this work! Transactionality here we come.;;;","10/May/11 22:30;hudson;Integrated in Cassandra-0.8 #93 (See [https://builds.apache.org/hudson/job/Cassandra-0.8/93/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
default gc log settings overwrite previous log,CASSANDRA-2418,12503444,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cburroughs,cburroughs,cburroughs,05/Apr/11 20:16,16/Apr/19 09:33,14/Jul/23 05:52,07/Apr/11 15:54,0.7.5,,,,,,0,,,,"For those spoiled by nice rolling and appending syslogs log4js etc the JVM gc log can be jarring:

{noformat} 
# GC logging options -- uncomment to enable
# JVM_OPTS=""$JVM_OPTS -XX:+PrintGCDetails""
# JVM_OPTS=""$JVM_OPTS -XX:+PrintGCTimeStamps""
# JVM_OPTS=""$JVM_OPTS -XX:+PrintClassHistogram""
# JVM_OPTS=""$JVM_OPTS -XX:+PrintTenuringDistribution""
# JVM_OPTS=""$JVM_OPTS -XX:+PrintGCApplicationStoppedTime""
# JVM_OPTS=""$JVM_OPTS -Xloggc:/var/log/cassandra/gc.log""
{noformat} 

Will result in gc.log with days of data being overwritten on restart, which leads to sad faces.

The simplest change would be along these lines:
{noformat} 
GC_LOG_TS=`date +%s`
JVM_OPTS=""$JVM_OPTS -XX:+PrintGCDetails""
JVM_OPTS=""$JVM_OPTS -XX:+PrintGCTimeStamps""
JVM_OPTS=""$JVM_OPTS -XX:+PrintClassHistogram""
JVM_OPTS=""$JVM_OPTS -XX:+PrintTenuringDistribution""
JVM_OPTS=""$JVM_OPTS -XX:+PrintGCApplicationStoppedTime""
JVM_OPTS=""$JVM_OPTS -Xloggc:/var/log/cassandra/gc-$GC_LOG_TS.log""
{noformat} 

There are probably prettier approaches.",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/11 22:43;jbellis;2418.txt;https://issues.apache.org/jira/secure/attachment/12475537/2418.txt",,,,,,,,,,,,,,1.0,cburroughs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20612,,,Thu Apr 07 16:30:06 UTC 2011,,,,,,,,,,"0|i0gb9b:",93254,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"05/Apr/11 22:43;jbellis;patch based on Chris's suggestion;;;","07/Apr/11 15:44;cburroughs;Patch looks good to me.;;;","07/Apr/11 15:54;jbellis;committed;;;","07/Apr/11 16:30;hudson;Integrated in Cassandra-0.7 #424 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/424/])
    add date in seconds-since-epoch to default gc log filename
patch by Chris Burroughs and jbellis for CASSANDRA-2418
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
convert mmap assertion to if/throw,CASSANDRA-2417,12503397,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,05/Apr/11 14:06,16/Apr/19 09:33,14/Jul/23 05:52,05/Apr/11 17:29,0.7.5,,,,,,0,,,,"This will allow scrub to catch this:

{noformat}
java.lang.AssertionError: mmap segment underflow; remaining is 73936639 but 1970430821 requested

                at org.apache.cassandra.io.util.MappedFileDataInput.readBytes(MappedFileDataInput.java:119)

                at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:315)

                at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:272)

                at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:76)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/11 14:11;jbellis;2417.txt;https://issues.apache.org/jira/secure/attachment/12475480/2417.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20611,,,Tue Apr 05 18:32:16 UTC 2011,,,,,,,,,,"0|i0gb93:",93253,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"05/Apr/11 17:28;slebresne;+1 (committed to 0.7 and trunk);;;","05/Apr/11 17:44;hudson;Integrated in Cassandra-0.7 #421 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/421/])
    Convert mmap assertion to if/throw
patch by jbellis; reviewed by slebresne for CASSANDRA-2417
;;;","05/Apr/11 18:32;hudson;Integrated in Cassandra #829 (See [https://hudson.apache.org/hudson/job/Cassandra/829/])
    Merge CASSANDRA-2417 from 0.7
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in CacheWriter.saveCache(),CASSANDRA-2416,12503379,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,skamio,skamio,05/Apr/11 09:28,16/Apr/19 09:33,14/Jul/23 05:52,18/Apr/11 16:56,0.7.5,,,,,,0,,,,"I've seen NullPointerException of CacheWriter in our cluster (replication 3).

ERROR [CompactionExecutor:1] 2011-04-05 09:57:42,968 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.utils.ByteBufferUtil.writeWithLength(ByteBufferUtil.java:275)
        at org.apache.cassandra.io.sstable.CacheWriter.saveCache(CacheWriter.java:84)
        at org.apache.cassandra.db.CompactionManager$10.runMayThrow(CompactionManager.java:960)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
",linux,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/11 16:20;jbellis;2416-v2.txt;https://issues.apache.org/jira/secure/attachment/12476624/2416-v2.txt","08/Apr/11 21:23;jbellis;2416.txt;https://issues.apache.org/jira/secure/attachment/12475841/2416.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20610,,,Mon Apr 18 18:07:44 UTC 2011,,,,,,,,,,"0|i0gb8v:",93252,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"08/Apr/11 21:23;jbellis;Patch to avoid storing DecoratedKeys with null key (which are generated by getRangeSlice) in the key cache.;;;","08/Apr/11 21:41;slebresne;Why not do that in cacheKey directly to be sure we don't miss places ? ;;;","18/Apr/11 16:20;jbellis;Because attempting to cache a DK that is really just a token is a semantic error that we shouldn't just paper over.  v2 adds an assert to make that clear.;;;","18/Apr/11 16:46;slebresne;+1;;;","18/Apr/11 16:56;jbellis;committed;;;","18/Apr/11 18:07;hudson;Integrated in Cassandra-0.7 #439 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/439/])
    avoid caching token-only decoratedkeys
patch by jbellis; reviewed by slebresne for CASSANDRA-2416
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce default memtable size,CASSANDRA-2413,12503152,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,01/Apr/11 16:51,16/Apr/19 09:33,14/Jul/23 05:52,05/Apr/11 15:30,0.7.5,,,,,,0,,,,"I'm going to wimp out on targeting CASSANDRA-2006 for 0.7.5 so to mitigate OOMing by newcomers let's reduce the default memtable size -- what we have now predates indexes, which can dramatically increase memory requirements.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/11 17:01;jbellis;2413.txt;https://issues.apache.org/jira/secure/attachment/12475240/2413.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20608,,,Tue Apr 05 16:06:41 UTC 2011,,,,,,,,,,"0|i0gb87:",93249,,bcoverston,,bcoverston,Low,,,,,,,,,,,,,,,,,"05/Apr/11 15:05;bcoverston;+1, patch is good.;;;","05/Apr/11 15:30;jbellis;committed;;;","05/Apr/11 16:06;hudson;Integrated in Cassandra-0.7 #420 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/420/])
    halve default memtable thresholds
patch by jbellis; reviewed by bcoverston for CASSANDRA-2413
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC ResultSet does not honor column value typing for the CF and uses default validator for all column value types.,CASSANDRA-2410,12503071,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,ardot,ardot,31/Mar/11 20:55,16/Apr/19 09:33,14/Jul/23 05:52,16/Apr/11 23:10,0.8 beta 1,,,Legacy/CQL,,,0,cql,,,"Assume a CF declared in CQL as :
{code}
CREATE COLUMNFAMILY TestCF(KEY utf8 PRIMARY KEY,description utf8, anumber int)
  WITH comparator = ascii AND default_validation = long;
{code}

If the {{ResultSet}} is fetched thusly:


{code}
Statement stmt = con.createStatement();
ResultSet rs = stmt.executeQuery(query);

String description;
Integer anumber;

    while (rs.next())
    {
      description = rs.getString(1);
      System.out.print(""description : ""+ description);
      anumber = rs.getInt(2);
      System.out.print(""anumber     : ""+ anumber);
    }
{code}

It will immediately fail with a message of: 

{code}
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 16
	at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:66)
	at org.apache.cassandra.cql.jdbc.TypedColumn.<init>(TypedColumn.java:45)
	at org.apache.cassandra.cql.jdbc.ColumnDecoder.makeCol(ColumnDecoder.java:158)
	at org.apache.cassandra.cql.jdbc.CassandraResultSet.next(CassandraResultSet.java:1073)
	at da.access.testing.TestJDBC.selectAll(TestJDBC.java:83)
         ...
{code}


It appears that the {{makeCol}} method of {{ColumnDecoder.java}} chooses NOT to use the {{CfDef}} to look up the possible occurrence of a column? That's not right. Right? 

{code}
    /** constructs a typed column */
    public TypedColumn makeCol(String keyspace, String columnFamily, byte[] name, byte[] value)
    {
        CfDef cfDef = cfDefs.get(String.format(""%s.%s"", keyspace, columnFamily));
        AbstractType comparator = getComparator(keyspace, columnFamily, Specifier.Comparator, cfDef);
        AbstractType validator = getComparator(keyspace, columnFamily, Specifier.Validator, null);
        return new TypedColumn(comparator, name, validator, value);
    }
{code}



",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/11 21:08;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-honor-specific-column-validators-in-JDBC-driver.txt;https://issues.apache.org/jira/secure/attachment/12476536/ASF.LICENSE.NOT.GRANTED--v1-0001-honor-specific-column-validators-in-JDBC-driver.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20607,,,Sun Apr 17 01:11:50 UTC 2011,,,,,,,,,,"0|i0gb7j:",93246,,,,,Low,,,,,,,,,,,,,,,,,"16/Apr/11 21:09;gdusbabek;I noticed a failure of org.apache.cassandra.db.RecoveryManagerTest after applying the patch.  Chances are it was failing before--I can't see how a jdbc change would affect that.;;;","16/Apr/11 21:57;jbellis;+1;;;","16/Apr/11 23:10;gdusbabek;committed.;;;","17/Apr/11 01:11;hudson;Integrated in Cassandra-0.8 #10 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/10/])
    honor specific column validators in JDBC driver. patch by gdusbabek, reviewed by jbellis. CASSANDRA-2410
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add INSERT support to CQL,CASSANDRA-2409,12503053,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,31/Mar/11 18:13,16/Apr/19 09:33,14/Jul/23 05:52,04/Apr/11 20:05,0.8 beta 1,,,Legacy/CQL,,,0,,,,"There are two reasons to support INSERT:

- It helps new users feel comfortable (everyone's first statement will be to try to INSERT something, we should make that a positive experience instead of slapping them)
- Even though it is synonymous with update it is still useful in your code to have both to help communicate intent, similar to choosing good variable names

The only downside is explaining how INSERT isn't a ""true"" insert because it doesn't error out if the row already exists -- but we already have to explain that same concept for UPDATE; the cognitive load is extremely minor.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/11 18:05;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2409-CQL-INSERT-implementation.txt;https://issues.apache.org/jira/secure/attachment/12475392/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2409-CQL-INSERT-implementation.txt","02/Apr/11 17:09;xedin;CASSANDRA-2409.patch;https://issues.apache.org/jira/secure/attachment/12475286/CASSANDRA-2409.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20606,,,Mon Apr 04 20:05:45 UTC 2011,,,,,,,,,,"0|i0gb7b:",93245,,urandom,,urandom,Normal,,,,,,,,,,,,,,,,,"31/Mar/11 18:45;urandom;I don't think this is a good idea. {{UPDATE}} already has semantics that are going to be a surprise to folks coming from SQL. Implementing {{INSERT}} to have identical semantics is going to be (IMO) more surprising still, not less.

But, I don't want to get wrapped up in a protracted debate about it, so I'll leave this as ""-0"", and let you guys carry-on as you see fit.

P.S. Please don't wait too long, the freeze is looming.;;;","31/Mar/11 18:50;xedin;Maybe I have a solution which will satisfy both sides - we can just make INSERT as an alias for UPDATE, so no need to implement anything just a simple change in the grammar, what do you think? ;;;","01/Apr/11 23:04;urandom;I don't think this what Jonathan is after, he's looking for the equivalent of SQL's {{INSERT}}. For example:

{code:style=SQL}
INSERT INTO cf (foo, bar, baz) VALUES (1, 20 300) WHERE KEY = fnord;
{code};;;","02/Apr/11 00:56;jbellis;Close.

{code}
INSERT INTO cf (KEY, foo, bar, baz) VALUES (fnord, 1, 20 300);
{code}
;;;","02/Apr/11 17:09;xedin;Format:
{noformat}
INSERT INTO
    <CF>
    (KEY, <column>, <column>, ...)
VALUES
    (<key>, <value>, <value>, ...)
(USING
  CONSISTENCY <level>)?;
{noformat}

Consistency level is set to ONE by default;;;","02/Apr/11 17:10;xedin;Only thing for you, Eric, to change is exception message when column names size not equal to values size...;;;","02/Apr/11 18:40;urandom;Thanks Pavel, I'll have a look at it and get back to you.;;;","04/Apr/11 18:08;urandom;Attached is an updated version of the patch that refactors things a bit.  The biggest problem here is that we need to raise Thrift exceptions.

Also included is some minimal system tests.;;;","04/Apr/11 18:58;xedin;Can you please explane a part about exceptions, what should be done?;;;","04/Apr/11 19:28;urandom;If you raise a RuntimeException from the lexer or parser, it will not be propagated down to the client (it's swallowed and turned into an internal server error).

If you look at the revised patch, I moved the size equality test and conversion to Map into {{UpdateStatement.getColumns()}} where an {{InvalidRequestException}} can be raised (everything that calls {{getColumns()}} already throws {{InvalidRequestException}}. ;;;","04/Apr/11 19:33;xedin;Oh, yeah, I've seen it, thats why I wrote you to change exception... I misunderstood your previous comment and I thought that there are still problems with exceptions left...

+1 on your patch.;;;","04/Apr/11 20:05;urandom;committed; thanks Pavel!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Secondary index and index expression problems,CASSANDRA-2406,12502972,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,muga_nishizawa,muga_nishizawa,31/Mar/11 03:18,16/Apr/19 09:33,14/Jul/23 05:52,18/Apr/11 13:28,0.7.5,,,Feature/2i Index,,,0,,,,"When I iteratively get data with secondary index and index clause, result of data acquired by consistency level ""one"" is different from the one by consistency level ""quorum"".  The one by consistecy level ""one"" is correct result.  But the one by consistecy level ""quorum"" is incorrect and is dropped by Cassandra.  

You can reproduce the bug by executing attached programs.

- 1. Start Cassandra cluster.  It consists of 3 cassandra nodes and distributes data by ByteOrderedPartitioner.  Initial tokens of those nodes are [""31"", ""32"", ""33""].  
- 2. Create keyspace and column family, according to ""create_table.cli"",
- 3. Execute ""secondary_index_insertv2.py"", inserting a few hundred columns to cluster
- 4. Execute ""secondary_index_checkv2.py"" and get data with secondary index and index clause iteratively.  ""secondary_index_insertv2.py"" and ""secondary_index_checkv2.py"" require pycassa.

You will be able to execute  4th ""secondary_index_checkv2.py"" script with following option so that 
you get data with consistency level ""one"".  

% python ""secondary_index_checkv2.py"" -one

On the other hand, to acquire data with consistency level ""quorum"", you will need to use following option.  

% python ""secondary_index_checkv2.py"" -quorum

You can check that result of data acquired by consistency level ""one"" is different from one by consistency level ""quorum"".  ","CentOS 5.5 (64bit), JDK 1.6.0_23",cherro,skamio,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/11 13:20;skamio;CASSANDRA-2406-debug.patch;https://issues.apache.org/jira/secure/attachment/12475709/CASSANDRA-2406-debug.patch","13/Apr/11 13:20;xedin;CASSANDRA-2406.patch;https://issues.apache.org/jira/secure/attachment/12476234/CASSANDRA-2406.patch","31/Mar/11 03:19;muga_nishizawa;create_table.cli;https://issues.apache.org/jira/secure/attachment/12475053/create_table.cli","08/Apr/11 02:52;skamio;node-1.system.log;https://issues.apache.org/jira/secure/attachment/12475775/node-1.system.log","31/Mar/11 03:20;muga_nishizawa;secondary_index_checkv2.py;https://issues.apache.org/jira/secure/attachment/12475055/secondary_index_checkv2.py","31/Mar/11 03:20;muga_nishizawa;secondary_index_insertv2.py;https://issues.apache.org/jira/secure/attachment/12475054/secondary_index_insertv2.py",,,,,,,,,6.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20604,,,Mon Apr 18 04:51:21 UTC 2011,,,,,,,,,,"0|i0gb6n:",93242,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"07/Apr/11 13:20;skamio;an experimental patch;;;","07/Apr/11 13:32;skamio;I've attached an experimental patch. The problem is gone with this patch. But it's inefficient when a large number of rows are requested.

The main problem was that the rows collected in ColumnFamilyStore.scan() can have duplicates. So, it returns less unique rows than requested. Then, StorageProxy.scan() asks more results from next range. That means the last returned row gets wrong.

As for inefficiency of the patch, if the rows are added in order, the uniqueness check should be done only for the last row. But I don't known if I can assume the order or not. So, please improve the patch if so.
;;;","07/Apr/11 15:34;jbellis;Hmm. There are two ways we could get duplicate rows:

1. the ranges we iterate through overlap.  but if that were the bug, we would see it on ONE as well as QUORUM
2. the ReadCallback/RangeSliceResponseResolver object (probably the resolver) returns duplicates from incorrect merging of the quorum replies

If 2. is the problem we should fix it in callback/resolver instead of in StorageProxy.

Even with it narrowed down there the problem is not obvious to me -- each response should come back in sorted (token) order, and RSRR uses a collating + reducing iterator to merge duplicates, in theory.;;;","08/Apr/11 02:49;skamio;I forgot to say, but 1. is right. The duplicate problem is visible on CL.ONE as well.
The above test script returns rows of ""173"", ""174"", ""174"" (duplicate), ""175"", ""176"" in the first iteration. It has duplicate.
And if you see my debug log attached, ColumnFamilyStore collects the row ""174"" twice in CL.ONE.

The only case it works correcly is when I specify start_key. In the above script, set start_key = ""173"". The result is ""173"", ""174"", ""175"", ""176"", ""177"" in the first iteration. It is correct, no duplicate.
;;;","08/Apr/11 02:52;skamio;I've attached debug log (node-1.system.log) with various debug prints. This is debug log in running the first iteration of test script with CL.ONE. The result has duplicate.;;;","12/Apr/11 15:24;jbellis;Pavel, can you take a stab at figuring out why the ranges overlap?  They are not supposed to.

(I am using https://github.com/pcmanus/ccm for testing, it saves a lot of time.);;;","15/Apr/11 14:33;jbellis;Pavel's patch fixes a 3rd kind of bug: CFS.scan itself can return duplicate rows, even for a single node and range.  Added a unit test and committed.

Shotaro, does this fix what you are seeing?;;;","18/Apr/11 04:51;skamio;Yes, it fixes our problem. Thanks.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"getColumnFamily() return null, which is not checked in ColumnFamilyStore.java scan() method, causing Timeout Exception in query",CASSANDRA-2401,12502672,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,karshiang,karshiang,29/Mar/11 07:15,16/Apr/19 09:33,14/Jul/23 05:52,14/May/11 15:03,0.7.6,,,,,,0,,,,"ColumnFamilyStore.java, line near 1680, ""ColumnFamily data = getColumnFamily(new QueryFilter(dk, path, firstFilter))"", the data is returned null, causing NULL exception in ""satisfies(data, clause, primary)"" which is not captured. The callback got timeout and return a Timeout exception to Hector.

The data is empty, as I traced, I have the the columns Count as 0 in removeDeletedCF(), which return the null there. (I am new and trying to understand the logics around still). Instead of crash to NULL, could we bypass the data?

About my test:
A stress-test program to add, modify and delete data to keyspace. I have 30 threads simulate concurrent users to perform the actions above, and do a query to all rows periodically. I have Column Family with rows (as File) and columns as index (e.g. userID, fileType).

No issue on the first day of test, and stopped for 3 days. I restart the test on 4th day, 1 of the users failed to query the files (timeout exception received). Most of the users are still okay with the query.
","Hector 0.7.0-28, Cassandra 0.7.4, Windows 7, Eclipse",cburroughs,cdaw,cywjackson,karshiang,rjtg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/May/11 17:12;jbellis;2401-v2.txt;https://issues.apache.org/jira/secure/attachment/12478298/2401-v2.txt","05/May/11 17:31;jbellis;2401-v3.txt;https://issues.apache.org/jira/secure/attachment/12478303/2401-v3.txt","04/May/11 17:47;jbellis;2401.txt;https://issues.apache.org/jira/secure/attachment/12478188/2401.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20603,,,Sat May 14 15:03:01 UTC 2011,,,,,,,,,,"0|i0gb5r:",93238,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"29/Mar/11 13:35;jbellis;Are you querying for zero columns?;;;","29/Mar/11 16:19;karshiang;Hi, nope. 

It is a query for 4 columns. 

I cheked that only 1 row has this problem (no column found), out of the 948 records returned; I skipped the row with zero columns. 

In my stress-test, all rows have 4 columns; i.e. row is the file, the 4 columns (index) are like its version, modified time, type, etc. I added all the columns when added each file. The addition should be working since there is no such exception on day 1, and I start and stop the stress tests until each users have around 1500 files. Row with 0 column only found on the 4th day after I continue to run it.

I will keep picking up cassandra logics, as I have little understanding about how data loaded, stored and deleted. Any suggestion / guide on how I should go on with my study is greatly appreciated. Thank you!

Btw, for this test, I have not yet going to 2 nodes / 3 nodes. It is only a single-node cassandra runnning on my localhost.
;;;","29/Mar/11 16:32;jbellis;Is there any data from earlier than 0.7.4?;;;","30/Mar/11 01:15;karshiang;Hi, 
This is a clean 0.7.4 setup, with zero data to start with. Dynamically, the keyspace schema is creted on the run, when required keyspace does not exist.;;;","30/Mar/11 02:18;karshiang;Hi,

New finding here:
For the 0-column data, it is because it is never read from the file. As I step through the line, here it returns -1 position from org.apache.cassandra.io.sstable.SSTableReader.java::getPosition(DecoratedKey decoratedKey, Operator op), line 448 (bf.isPresent(decoratedKey.key) is returning false) - key is missing.

There seem to be a missing record which is indexed or indexed column itself not updated when the record is removed (?). 

As for the data returned with 0-column, simply because a container is always created (final ColumnFamily returnCF = ColumnFamily.create(metadata)) and returned from getTopLevelColumns even if there is no read taken.

As for this case, it causes Timeout exception to Hector when null exception thrown without captured.;;;","03/Apr/11 18:55;rjtg;can you provide some unit tests that reproduce your error? i'd like to look into it, but i am not sure whether i understand the issue correctly.;;;","05/Apr/11 09:42;karshiang;Hi Roland,

Sure, as we are trying to do that. In the mean time, I would like to update you more about our findings:
We built a test case on the PC with the existing DB and to produce same issue, without hector API. The test case works (able to create null exception) on the original PC. 

java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.satisfies(ColumnFamilyStore.java:1787)
	at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1727)
	at TestScan.main(TestScan.java:74)

line 1787: IColumn column = data.getColumn(expression.column_name); where data is NULL


Zipping the 0.7.4 cassandra data to another new PC gives the same issue, but the missing key order may slightly different, e.g. on original PC it is at 430th, on the new PC it is 431th. Both keys appears to be same though (content in ByteBuffer).
(Edited: the new PC also found the problem - which makes more sense)

We will continue to check if it is due to the ""if (column.isMarkedForDelete())"" is not working on the PC with have the null encountered. Since we checked that, both PCs have the same number of columns returned in ""scan"" method at line ""ColumnFamily indexRow = indexCFS.getColumnFamily(indexFilter);"", where ""indexRow.getColumnCount()"" both giving 1996, with some rows already deleted as tombstones. 
;;;","05/Apr/11 09:47;karshiang;Some information i missed in update:
In the PC with NULL exception, I do a continue when found ""data"" is null, and ignore that. I will get 1040 columns returned. On the 2nd (new) PC, without the NULL exception nor additional code to bypass null data, it is getting 1040 records as well. From here, we will study more our DB to find out where it went wrong/different.;;;","05/Apr/11 11:33;rjtg;Sounds As if the Index is still pointing to deleted entriss. ;;;","05/Apr/11 14:23;karshiang;hi, yes. it seems to me so. Here, we create a table ""FileMap"", in which we store columns e.g. ""content"", ""authorID"", ""Version"", ""Modified Time"", ""File Type"", etc. Among them, sorted indices are ""authorID"" (as UserIndex), ""File Type"", ""Modified Time"", and ""CassType""; where CassType means generally 'file type' here in our case. It is not used though.

{{03/30/2011  09:34 AM        11,366,878 FileMap-f-53-Data.db}}
{{03/30/2011  09:34 AM            78,496 FileMap-f-53-Filter.db}}
{{03/30/2011  09:34 AM           735,930 FileMap-f-53-Index.db}}
{{03/30/2011  09:34 AM             4,264 FileMap-f-53-Statistics.db}}
{{03/30/2011  05:37 PM             4,055 FileMap-f-54-Data.db}}
{{03/30/2011  05:37 PM                40 FileMap-f-54-Filter.db}}
{{03/30/2011  05:37 PM               270 FileMap-f-54-Index.db}}
{{03/30/2011  05:37 PM             4,264 FileMap-f-54-Statistics.db}}
{{04/04/2011  04:07 PM            24,068 FileMap-f-55-Data.db}}
{{04/04/2011  04:07 PM               200 FileMap-f-55-Filter.db}}
{{04/04/2011  04:07 PM             1,746 FileMap-f-55-Index.db}}
{{04/04/2011  04:07 PM             4,264 FileMap-f-55-Statistics.db}}
{{04/04/2011  04:07 PM           961,808 FileMap.CassTypeIndex-f-53-Data.db}}
{{04/04/2011  04:07 PM             1,936 FileMap.CassTypeIndex-f-53-Filter.db}}
{{04/04/2011  04:07 PM                11 FileMap.CassTypeIndex-f-53-Index.db}}
{{04/04/2011  04:07 PM             4,264 FileMap.CassTypeIndex-f-53-Statistics.db}}
{{03/29/2011  02:52 PM           961,386 FileMap.FileTypeIndex-f-50-Data.db}}
{{03/29/2011  02:52 PM             1,936 FileMap.FileTypeIndex-f-50-Filter.db}}
{{03/29/2011  02:52 PM                11 FileMap.FileTypeIndex-f-50-Index.db}}
{{03/29/2011  02:52 PM             4,264 FileMap.FileTypeIndex-f-50-Statistics.db}}
{{03/30/2011  05:37 PM               404 FileMap.FileTypeIndex-f-51-Data.db}}
{{03/30/2011  05:37 PM                16 FileMap.FileTypeIndex-f-51-Filter.db}}
{{03/30/2011  05:37 PM                11 FileMap.FileTypeIndex-f-51-Index.db}}
{{03/30/2011  05:37 PM             4,264 FileMap.FileTypeIndex-f-51-Statistics.db}}
{{04/04/2011  04:07 PM             2,358 FileMap.FileTypeIndex-f-52-Data.db}}
{{04/04/2011  04:07 PM                16 FileMap.FileTypeIndex-f-52-Filter.db}}
{{04/04/2011  04:07 PM                11 FileMap.FileTypeIndex-f-52-Index.db}}
{{04/04/2011  04:07 PM             4,264 FileMap.FileTypeIndex-f-52-Statistics.db}}
{{03/29/2011  02:52 PM         3,298,947 FileMap.ModifiedIndex-f-50-Data.db}}
{{03/29/2011  02:52 PM            78,016 FileMap.ModifiedIndex-f-50-Filter.db}}
{{03/29/2011  02:52 PM           731,106 FileMap.ModifiedIndex-f-50-Index.db}}
{{03/29/2011  02:52 PM             4,264 FileMap.ModifiedIndex-f-50-Statistics.db}}
{{03/30/2011  05:37 PM             2,065 FileMap.ModifiedIndex-f-51-Data.db}}
{{03/30/2011  05:37 PM                64 FileMap.ModifiedIndex-f-51-Filter.db}}
{{03/30/2011  05:37 PM               450 FileMap.ModifiedIndex-f-51-Index.db}}
{{03/30/2011  05:37 PM             4,264 FileMap.ModifiedIndex-f-51-Statistics.db}}
{{04/04/2011  04:07 PM            13,835 FileMap.ModifiedIndex-f-52-Data.db}}
{{04/04/2011  04:07 PM               328 FileMap.ModifiedIndex-f-52-Filter.db}}
{{04/04/2011  04:07 PM             3,006 FileMap.ModifiedIndex-f-52-Index.db}}
{{04/04/2011  04:07 PM             4,264 FileMap.ModifiedIndex-f-52-Statistics.db}}
{{04/04/2011  04:07 PM           962,874 FileMap.UserIndex-f-53-Data.db}}
{{04/04/2011  04:07 PM             1,936 FileMap.UserIndex-f-53-Filter.db}}
{{04/04/2011  04:07 PM               420 FileMap.UserIndex-f-53-Index.db}}
{{04/04/2011  04:07 PM             4,264 FileMap.UserIndex-f-53-Statistics.db}}

In the search, we are using IndexClause as:
		ByteBuffer field_author = ByteBuffer.wrap(new byte[]{'a'});
		ByteBuffer author_1 = IntegerSerializer.get().toByteBuffer(1);
		
		ByteBuffer file_type = ByteBuffer.wrap(new byte[]{'t'});
		ByteBuffer filetype_3 = ByteBuffer.wrap(new byte[]{3}); //file type 3
		
		IndexClause indexClause = new IndexClause();
		indexClause.setCount(3000);
		ArrayList<IndexExpression> expressions = new ArrayList();
		expressions.add(new IndexExpression(field_author, IndexOperator.EQ, author_1)); //user ID = 1
		expressions.add(new IndexExpression(file_type, IndexOperator.EQ, filetype_3)); //file type = 3
		
		indexClause.setExpressions(expressions);
		indexClause.setStart_key(new byte[]{});

}}
In the search, it scans all the indices from ""FileMap.UserIndex"", within which there seems having a key (index) which is not found in the table ""FileMap""; and I roughly get that it breaks at data retrieval with ""FileMap-f-53-Data"", when the position for the key is not found / available in ""FileMap-f-53-Data"".;;;","19/Apr/11 02:46;jbellis;So when you created the data, you did not use any expiring columns (TTL), correct?;;;","19/Apr/11 02:52;jbellis;The more I think about it, the more I think that there is a rare race condition here -- we do a kind of row lock during updates of indexed data, but we do not lock during reads. So it's possible for an index read to say ""row X has this value"" and then have that value deleted (by another client's request) before we can read row X.

BUT that does not look like what you are seeing because if I understand correctly you are seeing that the index has permanently missed a delete operation.;;;","19/Apr/11 03:03;karshiang;hi Jon,

sorry for less updates for past few days as we were busy on other tasks. We are thinking to stress-test with 0.7.5 when it is out.

In the test, we have all operations e.g. ""insert, replace, and delete"". If not wrong, we have simulated 20-users to run concurrently, however, they likely not able to delete key of different user. I think there is no such a case when 1 user is modifying his record, when another user deleting the record.

There is no expiring columns (TTL) in this test.

Same data on another PC will able to give the same exception, though we found the index position (n variable) can be shifted by 1 or 2.

Thanks, Jon and your team for the gd work!;;;","19/Apr/11 03:11;karshiang;Hi Jon,

Allow me to add more information:

Each simulated user thread will do the following in repeatitive manner:

loop = 0;
while( running )
{
    if( loop % 5 ==0 ) { list all files in folder; }

    create around 4~10 files but cap the total files around 2000 files only.
    modified around 20 files;
    delete 1~4 files;

    loop ++;
}

The ""list all files in folder"" is the scan action, where it will later for 1 or 2 users giving us ""no file"" in return after the next few days when restarted the same test, without resetting data. Found out it is due to the issue above. ;;;","19/Apr/11 13:36;rjtg;i just looked a little closer at your index expressions again.
If i understand them correctly they are subject to https://issues.apache.org/jira/browse/CASSANDRA-2347
Although i don't really think it is the issue you are describing it would be nice if you could apply the patch and see if the error still occurs.

You are creating the bytebuffers for author_id and file_type in a different way. Is this a mistake? ;;;","20/Apr/11 03:46;karshiang;Hi Roland,

The 'mistake' is intended by reusing some Hector API code.

Hector has a Integer Serializer, which will generate 4-byte[] from given integer. The file_type is a 1-byte array. It is to produce exact effected client call into a test case, solely running cassandra. ;;;","20/Apr/11 04:24;karshiang;for issue: https://issues.apache.org/jira/browse/CASSANDRA-2347, I suspect we encountered that in another case. It has a validation failure at times.

I applied the change. As expected, the error is still there, data is missing or indexed key extra then throw NULL exception out.;;;","04/May/11 17:47;jbellis;I found *a* bug that could cause this: Cassandra will re-create a deleted index entry if it gets a write with an obsolete timestamp, but the data row tombstone will correctly suppress an update there. (So when you do an index query for value=X, and the index says ""row K has that value,"" then you get an error trying to read row K that doesn't exist.)

I don't think this is the bug Tey Kar is hitting, though, because unless I'm mistaken you won't get this NPE until after the data row tombstone is removed by compaction after gc_grace_seconds.  4 days isn't enough to see that unless you've tweaked gc_g_s.

Still, it's worth fixing.  Patch attached.  (Also adds an assert w/ more information if/when another way of triggering this is found.);;;","04/May/11 22:07;cywjackson;I have an existing data that was resulting similar NPE  before the patch. After applying the patch, the following observed:

{noformat}
DEBUG [ReadStage:82] 2011-05-04 21:23:27,114 ColumnFamilyStore.java (line 1514) fetched data row ColumnFamily(inode -deleted at 1304363600008- [70617468:false:49@1304363600219,])
DEBUG [ReadStage:82] 2011-05-04 21:23:27,114 ColumnFamilyStore.java (line 1532) row ColumnFamily(inode -deleted at 1304363600008- [70617468:false:49@1304363600219,]) satisfies all clauses
DEBUG [ReadStage:82] 2011-05-04 21:23:27,115 ColumnFamilyStore.java (line 1514) fetched data row ColumnFamily(inode [70617468:false:10@1304353355296,])
ERROR [ReadStage:82] 2011-05-04 21:23:27,115 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[ReadStage:82,5,main]
java.lang.AssertionError: No data found for NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17]) in DecoratedKey(29842926756667498147838693957802723793, 3134346637326336393966396130336561376538623330316566383561616131):QueryPath(columnFamilyName='inode', superColumnName='null', columnName='null') (original filter NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17])) from expression 73656e74696e656cEQ78
    at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1512)
    at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:42)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}

was the fix intend to avoid future problem, as such existing problem would need a workaround solution?;;;","04/May/11 22:48;jbellis;bq. was the fix intend to avoid future problem

yes.  as discussed above, once you corrupt your index this way the corruption is recorded permanently and you need to drop the index and recreate it.;;;","05/May/11 15:19;slebresne;Comments on the patch:
  * ignoreObsoleteMutation() now forgot to actually remove the obsolete mutation from cf.
  * not sure why mutatedIndexColumns need to be concurrent. There is no concurrency in ignoreObsoleteMutation, is there ?
  * really minor: change to debug log ""Scanning index row %s ..."" seems misleading since the first argument is not a row name.

Other than that, I do agree with you that there is quite probably a race between reads and concurrent writes. But also agree that it doesn't seem to be the problem here;;;","05/May/11 15:54;jbellis;bq. ignoreObsoleteMutation() now forgot to actually remove the obsolete mutation from cf

this isn't actually necessary, though, since if it's taken out of the list of mutated index columns the obsolete columns will only be applied to the ""main"" data row, and including obsolete columns there is harmless.

bq. not sure why mutatedIndexColumns need to be concurrent

because we might remove from the collection while iterating over it.  treeset will throw concurrentmodificationexception.  but maybe iterator.remove would work, now that you mention it?;;;","05/May/11 16:35;slebresne;bq. this isn't actually necessary, though, since if it's taken out of the list of mutated index columns the obsolete columns will only be applied to the ""main"" data row, and including obsolete columns there is harmless.

Very true.

bq. but maybe iterator.remove would work

I think it will;;;","05/May/11 17:12;jbellis;v2 attached w/ iterator/Set change.

bq. change to debug log ""Scanning index row %s ..."" seems misleading since the first argument is not a row name

it actually is the same CF+row as before, I just encapsulated it in getExpressionString so I can re-use the method in case of assertion failure later. Tweaked format a bit in v2, here's an example debug output:

{noformat}
Scanning index 'world2 EQ 15' starting with
{noformat};;;","05/May/11 17:20;slebresne;+1 v2;;;","05/May/11 17:35;jbellis;Oops, that's actually column + value, not CF.

For the record, v3 adds CF:
{noformat}
Scanning index 'CF1.world2 EQ 15' starting with
{noformat}

Will commit based on v2 +1.;;;","05/May/11 18:14;hudson;Integrated in Cassandra-0.7 #470 (See [https://builds.apache.org/hudson/job/Cassandra-0.7/470/])
    improve ignoring of obsoletemutations in index maintenance
patch by jbellis; reviewed by slebresne for CASSANDRA-2401
;;;","07/May/11 23:40;cywjackson;Here is 1 way that i could 100% reproduce the issue with data being null:

Need 2 nodes, 1 is gonna to autobootstrap to the other. Also assuming completely clean start (blow up the /var/lib/cassandra/ or where ever data are stored

i am also using brisk beta to test

to start:
node-A:
1) get brisk
2) start brisk  with -t (jobtracker)
3) run a simple hive query : 
 3a) bin/brisk hive 
 3b) create table foo (bar INT);
 3c) select count(*) from foo;
 3d) exit;
4) every thing should be so far so good, let the brisk node continue to be up

node-B:
1) get brisk
2) modify the resources/cassandra/conf/cassandra.yaml:
 2a) to enable autobootstrap. 
 2b) point seeds to node-A

3) put a sleep or break point in o.a.c.service.StorageService.joinTokenRing method, right after ""Map<InetAddress, Double> loadinfo = StorageLoadBalancer.instance.getLoadInfo();"" (personal preference: log a sleep line, add a thread.sleep(a_long_time))
4) start brisk with -t on node-B 
5) wait till the log line ""Joining: getting bootstrap token"" , it should now reaches your break point (or zz)
6) crash the jvm (personal preference: kill -9 <pid>)

back to node-A
1) exit the jvm (BriskDaemon) ""normally"" (kill <pid>)
2) start the brisk node again (with -t):

log from node-A: 
{noformat}
 INFO 23:25:00,213 Logging initialized
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/riptano/work/brisk/resources/cassandra/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/riptano/work/brisk/resources/hadoop/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
 INFO 23:25:00,235 Heap size: 510263296/511311872
 INFO 23:25:00,237 JNA not found. Native methods will be disabled.
 INFO 23:25:00,263 Loading settings from file:/home/riptano/work/brisk/resources/cassandra/conf/cassandra.yaml
 INFO 23:25:00,470 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 23:25:00,496 Detected Hadoop trackers are enabled, setting my DC to Brisk
 INFO 23:25:00,696 Global memtable threshold is enabled at 162MB
 INFO 23:25:00,846 Opening /var/lib/cassandra/data/system/IndexInfo-f-1
 INFO 23:25:00,912 Opening /var/lib/cassandra/data/system/Schema-f-2
 INFO 23:25:00,926 Opening /var/lib/cassandra/data/system/Schema-f-1
 INFO 23:25:00,951 Opening /var/lib/cassandra/data/system/Migrations-f-2
 INFO 23:25:00,954 Opening /var/lib/cassandra/data/system/Migrations-f-1
 INFO 23:25:00,970 Opening /var/lib/cassandra/data/system/LocationInfo-f-2
 INFO 23:25:00,989 Opening /var/lib/cassandra/data/system/LocationInfo-f-1
 INFO 23:25:01,089 Loading schema version c4fd2440-7900-11e0-0000-ba846f9adcf7
 INFO 23:25:01,499 Creating new commitlog segment /var/lib/cassandra/commitlog/CommitLog-1304810701499.log
 INFO 23:25:01,530 Replaying /var/lib/cassandra/commitlog/CommitLog-1304810455288.log
 INFO 23:25:01,675 Finished reading /var/lib/cassandra/commitlog/CommitLog-1304810455288.log
 INFO 23:25:01,730 Enqueuing flush of Memtable-MetaStore@102170028(869/1086 serialized/live bytes, 3 ops)
 INFO 23:25:01,735 Writing Memtable-MetaStore@102170028(869/1086 serialized/live bytes, 3 ops)
 INFO 23:25:01,743 Enqueuing flush of Memtable-sblocks@1075051425(3044096/3805120 serialized/live bytes, 17 ops)
 INFO 23:25:01,747 Enqueuing flush of Memtable-inode.path@780298059(2848/3560 serialized/live bytes, 59 ops)
 INFO 23:25:01,748 Enqueuing flush of Memtable-inode.sentinel@1934329031(2848/3560 serialized/live bytes, 59 ops)
 INFO 23:25:01,748 Enqueuing flush of Memtable-inode@1660575731(6393/7991 serialized/live bytes, 134 ops)
 INFO 23:25:01,821 Completed flushing /var/lib/cassandra/data/HiveMetaStore/MetaStore-f-1-Data.db (989 bytes)
 INFO 23:25:01,832 Writing Memtable-sblocks@1075051425(3044096/3805120 serialized/live bytes, 17 ops)
 INFO 23:25:01,927 Completed flushing /var/lib/cassandra/data/cfs/sblocks-f-1-Data.db (3045448 bytes)
 INFO 23:25:01,928 Writing Memtable-inode.path@780298059(2848/3560 serialized/live bytes, 59 ops)
 INFO 23:25:01,968 Completed flushing /var/lib/cassandra/data/cfs/inode.path-f-1-Data.db (5346 bytes)
 INFO 23:25:01,969 Writing Memtable-inode.sentinel@1934329031(2848/3560 serialized/live bytes, 59 ops)
 INFO 23:25:02,035 Completed flushing /var/lib/cassandra/data/cfs/inode.sentinel-f-1-Data.db (1735 bytes)
 INFO 23:25:02,036 Writing Memtable-inode@1660575731(6393/7991 serialized/live bytes, 134 ops)
 INFO 23:25:02,085 Completed flushing /var/lib/cassandra/data/cfs/inode-f-1-Data.db (8582 bytes)
 INFO 23:25:02,087 Log replay complete
 INFO 23:25:02,092 Cassandra version: 0.8.0-beta2-SNAPSHOT
 INFO 23:25:02,092 Thrift API version: 19.10.0
 INFO 23:25:02,092 Loading persisted ring state
 INFO 23:25:02,092 load token size: 0
 INFO 23:25:02,093 Starting up server gossip
 INFO 23:25:02,104 Enqueuing flush of Memtable-LocationInfo@22262475(29/36 serialized/live bytes, 1 ops)
 INFO 23:25:02,105 Writing Memtable-LocationInfo@22262475(29/36 serialized/live bytes, 1 ops)
 INFO 23:25:02,127 Completed flushing /var/lib/cassandra/data/system/LocationInfo-f-3-Data.db (80 bytes)
 INFO 23:25:02,149 Starting Messaging Service on port 7000
 INFO 23:25:02,172 Using saved token 152036150612811635197207268153837644139
 INFO 23:25:02,173 Enqueuing flush of Memtable-LocationInfo@1977026981(53/66 serialized/live bytes, 2 ops)
 INFO 23:25:02,174 Writing Memtable-LocationInfo@1977026981(53/66 serialized/live bytes, 2 ops)
 INFO 23:25:02,190 Completed flushing /var/lib/cassandra/data/system/LocationInfo-f-4-Data.db (163 bytes)
 INFO 23:25:02,193 Compacting Major: [SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-f-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-f-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-f-3-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-f-4-Data.db')]
 INFO 23:25:02,196 Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO 23:25:02,196 Starting up Hadoop trackers
 INFO 23:25:02,197 Waiting for gossip to start
 INFO 23:25:02,225 Major@1830423861(system, LocationInfo, 438/741) now compacting at 16777 bytes/ms.
 INFO 23:25:02,257 Compacted to /var/lib/cassandra/data/system/LocationInfo-tmp-f-5-Data.db.  741 to 447 (~60% of original) bytes for 3 keys.  Time: 64ms.
 INFO 23:25:07,272 Chose seed 10.179.96.212 as jobtracker
 WARN 23:25:09,331 Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-jobtracker.properties, hadoop-metrics2.properties
 INFO 23:25:09,994 Chose seed 10.179.96.212 as jobtracker
 INFO 23:25:10,139 Updating the current master key for generating delegation tokens
 INFO 23:25:10,143 Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
 INFO 23:25:10,143 Scheduler configured with (memSizeForMapSlotOnJT, memSizeForReduceSlotOnJT, limitMaxMemForMapTasks, limitMaxMemForReduceTasks) (-1, -1, -1, -1)
 INFO 23:25:10,144 Updating the current master key for generating delegation tokens
 INFO 23:25:10,145 Refreshing hosts (include/exclude) list
 INFO 23:25:10,223 Starting jobtracker with owner as riptano
 INFO 23:25:10,245 Starting SocketReader
 INFO 23:25:10,374 Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
 INFO 23:25:10,623 Added global filtersafety (class=org.apache.hadoop.http.HttpServer$QuotingInputFilter)
 INFO 23:25:10,673 Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 50030
 INFO 23:25:10,673 listener.getLocalPort() returned 50030 webServer.getConnectors()[0].getLocalPort() returned 50030
 INFO 23:25:10,674 Jetty bound to port 50030
 INFO 23:25:10,674 jetty-6.1.21
 INFO 23:25:11,140 Started SelectChannelConnector@0.0.0.0:50030
 INFO 23:25:11,147 JobTracker up at: 8012
 INFO 23:25:11,147 JobTracker webserver: 50030
 WARN 23:25:11,276 Incorrect permissions on cassandra://localhost:9160/tmp/hadoop-riptano/mapred/system. Setting it to rwx------
ERROR 23:25:11,321 Fatal exception in thread Thread[ReadStage:4,5,main]
java.lang.AssertionError: No data found for NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17]) in DecoratedKey(55249227080490826413412398468829851220, 3165333533353736613164333836353061346636333465656437326131353939):QueryPath(columnFamilyName='inode', superColumnName='null', columnName='null') (original filter NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17])) from expression 'inode.73656e74696e656c EQ 78'
        at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1513)
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:46)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 23:25:20,059 Deleted /var/lib/cassandra/data/system/LocationInfo-f-3
 INFO 23:25:20,060 Deleted /var/lib/cassandra/data/system/LocationInfo-f-4
 INFO 23:25:20,576 Deleted /var/lib/cassandra/data/system/LocationInfo-f-1
 INFO 23:25:20,577 Deleted /var/lib/cassandra/data/system/LocationInfo-f-2
 INFO 23:25:21,297 problem cleaning system directory: cassandra://localhost:9160/tmp/hadoop-riptano/mapred/system
java.io.IOException: TimedOutException()
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listDeepSubPaths(CassandraFileSystemThriftStore.java:523)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listSubPaths(CassandraFileSystemThriftStore.java:529)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystem.listStatus(CassandraFileSystem.java:171)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:2374)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:2174)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:303)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:294)
        at org.apache.cassandra.hadoop.trackers.TrackerInitializer$1.run(TrackerInitializer.java:93)
        at java.lang.Thread.run(Thread.java:662)
Caused by: TimedOutException()
        at org.apache.cassandra.thrift.CassandraServer.get_indexed_slices(CassandraServer.java:673)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listDeepSubPaths(CassandraFileSystemThriftStore.java:506)
        ... 8 more
 WARN 23:25:31,300 Incorrect permissions on cassandra://localhost:9160/tmp/hadoop-riptano/mapred/system. Setting it to rwx------
ERROR 23:25:31,315 Fatal exception in thread Thread[ReadStage:6,5,main]
java.lang.AssertionError: No data found for NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17]) in DecoratedKey(55249227080490826413412398468829851220, 3165333533353736613164333836353061346636333465656437326131353939):QueryPath(columnFamilyName='inode', superColumnName='null', columnName='null') (original filter NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17])) from expression 'inode.73656e74696e656c EQ 78'
        at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1513)
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:46)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 23:25:41,303 problem cleaning system directory: cassandra://localhost:9160/tmp/hadoop-riptano/mapred/system
java.io.IOException: TimedOutException()
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listDeepSubPaths(CassandraFileSystemThriftStore.java:523)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listSubPaths(CassandraFileSystemThriftStore.java:529)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystem.listStatus(CassandraFileSystem.java:171)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:2374)
        at org.apache.hadoop.mapred.JobTracker.<init>(JobTracker.java:2174)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:303)
        at org.apache.hadoop.mapred.JobTracker.startTracker(JobTracker.java:294)
        at org.apache.cassandra.hadoop.trackers.TrackerInitializer$1.run(TrackerInitializer.java:93)
        at java.lang.Thread.run(Thread.java:662)
Caused by: TimedOutException()
        at org.apache.cassandra.thrift.CassandraServer.get_indexed_slices(CassandraServer.java:673)
        at org.apache.cassandra.hadoop.fs.CassandraFileSystemThriftStore.listDeepSubPaths(CassandraFileSystemThriftStore.java:506)
        ... 8 more
 WARN 23:25:51,308 Incorrect permissions on cassandra://localhost:9160/tmp/hadoop-riptano/mapred/system. Setting it to rwx------
ERROR 23:25:51,321 Fatal exception in thread Thread[ReadStage:8,5,main]
java.lang.AssertionError: No data found for NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17]) in DecoratedKey(55249227080490826413412398468829851220, 3165333533353736613164333836353061346636333465656437326131353939):QueryPath(columnFamilyName='inode', superColumnName='null', columnName='null') (original filter NamesQueryFilter(columns=java.nio.HeapByteBuffer[pos=12 lim=16 cap=17])) from expression 'inode.73656e74696e656c EQ 78'
        at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1513)
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:46)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

{noformat}

;;;","08/May/11 02:36;jbellis;kill -9 w/o the bootstrap is not sufficient to cause the problem?

If you allow the bootstrap to finish does it work correctly if you kill -9 node A?

Bootstrap shouldn't cause anything to be written to node A (except the presence of a new node, to system table) so I'm inclined to think the kill -9 of A is the important part.;;;","08/May/11 02:41;jbellis;bq. Bootstrap shouldn't cause anything to be written to node A

Hmm, but it does cause A to flush. I wonder if that's the connection.

Can you try with invoking nodetool flush against A, instead of doing a bootstrap?;;;","08/May/11 03:15;jbellis;Another thing to try: after kill -9 of A but before restarting it, remove the commitlog *header* files (just the header ones). This should force full CL replay on restart.;;;","12/May/11 17:54;slebresne;From irc:
{noformat}
pcmanus : jbellis: do you know what's up with #2401 ?
jbellis : jackson can't reproduce anymore either, but he wants to test more before calling it fixed
{noformat}
So I'm going to mark this resolved as this fixed a legit bug and I don't want to push it 0.7.7.
If there is still related problems, let's open another ticket.;;;","13/May/11 21:53;thobbs;With these changes, using a count of 0 in the SlicePredicate produces the following AssertionError (and a TimedOutExc for the client):

{noformat}
ERROR 16:13:38,864 Fatal exception in thread Thread[ReadStage:16,5,main]
java.lang.AssertionError: No data found for SliceQueryFilter(start=java.nio.HeapByteBuffer[pos=10 lim=10 cap=30], finish=java.nio.HeapByteBuffer[pos=17 lim=17 cap=30], reversed=false, count=0] in DecoratedKey(81509516161424251288255223397843705139, 6b657931):QueryPath(columnFamilyName='cf', superColumnName='null', columnName='null') (original filter SliceQueryFilter(start=java.nio.HeapByteBuffer[pos=10 lim=10 cap=30], finish=java.nio.HeapByteBuffer[pos=17 lim=17 cap=30], reversed=false, count=0]) from expression 'cf.626972746864617465 EQ 1'
	at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1517)
	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:42)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

This was during a get_indexed_slices().;;;","14/May/11 15:03;jbellis;Created CASSANDRA-2653 to address this, since it will probably be in a different release than the original 2401 fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MarshalException is thrown when cassandra-cli creates the example Keyspace specified by conf/schema-sample.txt,CASSANDRA-2390,12502469,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,yaojingguo,yaojingguo,26/Mar/11 16:31,16/Apr/19 09:33,14/Jul/23 05:52,27/Mar/11 05:28,0.7.5,,,,,,0,,,,"Use the following steps to recreate the bug:

1. Checkout the source code from trunk. For my case, revision is 1085753.
2. Run ""ant"" to build cassandra.
3. Run ""bin/cassandra -f"" to start cassandra.
4. Run ""bin/cassandra-cli -host localhost --file conf/schema-sample.txt"".

Then there is the following message:
{quote}
... schemas agree across the cluster
Line 9 => org.apache.cassandra.db.marshal.MarshalException: cannot parse 'birthdate' as hex bytes
{quote}
The root cause is BytesType's fromString method. FBUtilities's hexToBytes method is invoked with ""birthdate"". NumberFormatException is thrown since ""birthdate"" is not a hex string.

{code:title=BytesType.java|borderStyle=solid}

    public ByteBuffer fromString(String source)
    {
        try
        {
            return ByteBuffer.wrap(FBUtilities.hexToBytes(source));
        }
        catch (NumberFormatException e)
        {
            throw new MarshalException(String.format(""cannot parse '%s' as hex bytes"", source), e);
        }
    }
{code}",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20598,,,Sun Mar 27 05:44:34 UTC 2011,,,,,,,,,,"0|i0gb3j:",93228,,,,,Low,,,,,,,,,,,,,,,,,"26/Mar/11 16:42;yaojingguo;[CASSANDRA-2262|https://issues.apache.org/jira/browse/CASSANDRA-2262] makes BytesType's fromString method only accept hex strings.;;;","27/Mar/11 05:28;jbellis;fixed in r1085877 by adding comparator=UTF8Type to the CF definition.  Thanks for catching that!;;;","27/Mar/11 05:44;hudson;Integrated in Cassandra-0.7 #409 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/409/])
    specify UTF8Type comparator to fix regression found by Jingguo Yao
patch by jbellis for CASSANDRA-2390
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ColumnFamilyRecordReader fails for a given split because a host is down, even if records could reasonably be read from other replica.",CASSANDRA-2388,12502417,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,pauloricardomg,eldondev,eldondev,25/Mar/11 20:40,16/Apr/19 09:33,14/Jul/23 05:52,20/Nov/15 13:54,2.1.12,2.2.4,,Legacy/Tools,,,6,hadoop,inputformat,,ColumnFamilyRecordReader only tries the first location for a given split. We should try multiple locations for a given split.,,alexliu68,bontempi,brandon.williams,eldondev,jeromatron,lannyripple,mck,mdennis,patrik.modesto,pauloricardomg,pkolaczk,scottfines,slebresne,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4886,,,,,,,,"26/Mar/11 01:32;eldondev;0002_On_TException_try_next_split.patch;https://issues.apache.org/jira/secure/attachment/12474683/0002_On_TException_try_next_split.patch","25/Jun/14 19:22;pauloricardomg;1.2-CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12652488/1.2-CASSANDRA-2388.patch","25/Jun/14 19:22;pauloricardomg;2.0-CASSANDRA-2388-v2.patch;https://issues.apache.org/jira/secure/attachment/12652489/2.0-CASSANDRA-2388-v2.patch","09/Jun/14 14:11;pauloricardomg;2.0-CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12649379/2.0-CASSANDRA-2388.patch","28/Jun/11 15:02;mck;CASSANDRA-2388-addition1.patch;https://issues.apache.org/jira/secure/attachment/12484458/CASSANDRA-2388-addition1.patch","04/Jul/11 14:01;mck;CASSANDRA-2388-extended.patch;https://issues.apache.org/jira/secure/attachment/12485146/CASSANDRA-2388-extended.patch","02/Jul/11 22:14;mck;CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12485071/CASSANDRA-2388.patch","22/Jun/11 13:46;mck;CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12483441/CASSANDRA-2388.patch","11/Jun/11 09:15;mck;CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12482140/CASSANDRA-2388.patch","09/Jun/11 06:29;mck;CASSANDRA-2388.patch;https://issues.apache.org/jira/secure/attachment/12481890/CASSANDRA-2388.patch",,,,,10.0,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20597,,,Fri Nov 20 13:54:28 UTC 2015,,,,,,,,,,"0|i07hqv:",41642,,pkolaczk,,pkolaczk,Low,,,,,,,,,,,,,,,,,"25/Mar/11 22:45;brandon.williams;I'm not sure special casing NoRouteToHostException to be blacklisted is the best thing to do.  I don't think connections are being setup so often that maintaining a blacklist for any reason is needed.;;;","26/Mar/11 18:24;brandon.williams;Unfortunately, I thought of another problem here.  If we go over the entire replica set, we're potentially going outside of the DC, which is bad since a lot of installations have a DC dedicated to analytics so it doesn't affect their app. It seems that the local address is preferred though, are your task trackers not on the same machines as Cassandra?;;;","17/May/11 21:07;jbellis;Eldon, are you planning to take another stab at this?;;;","17/May/11 21:09;tjake;We need to return the list of replicas in the same DC;;;","23/May/11 14:45;jbellis;Mck, do you want to take a stab at this?;;;","23/May/11 16:27;mck;I'm having a go currently at CASSANDRA-1125 so i might as well look at this too. (but you've caught me on a holiday-week...);;;","07/Jun/11 14:11;mck;How do i obtain the DataCenter name for a given address?

IEndpointSnitch.getDataCenter(inetAddress) would work nicely for me but how do i get the snitch client-side?;;;","07/Jun/11 20:41;mck;Initial attempt at solution. Although I'm a little apprehensive to the additions to cassandra.thrift
(describe_rack(..) isn't used anywhere, it just made sense to add describe_datacenter(..) and describe_rack(..) at the same time).

I've tested that existing hadoop jobs work but the new functionality hasn't been tested (as i currently don't have any RF=2 data setup).

This patch does not include the required re-generated Cassandra.java;;;","09/Jun/11 06:29;mck;Second attempt. (god only knows what i was trying to test last patch ;)
this patch:
 - adds describe_datacenter and describe_rack to cassandra.thrift
 - adds locations in ColumnFamilyRecordReader from the split's alternative endpoints if dc is the same

This patch does not include the required re-generated Cassandra.java
;;;","09/Jun/11 10:22;mck;I have tested this now on data w/ RF=2.
Seems to work ~ok as far as i can see.

One side-effect of this patch is where once one could configure ConfigHelper.setInitialAddress(conf, ""localhost"") this will no longer work for tasks trying to run on the down node.
ColumnFamilyRecordReader.getLocations() will ConnectException trying to call describe_datacenter(..). This will lead to the task failing. Hadoop re-runs the task then on another node and eventually the job will complete. But the fall back to replica never is used.

If the initialAddress is hardcoded to one node then we no longer have a decentralised job.

I would like to allow a comma-separated in initialAddress, for example it could be ""localhost, node01, node02, node03"". This would give preference to localhost and avoid any centralisation.

I would also like to make ColumnFamilyRecordReader.getLocations() return an iterator instead of an array.
The createConnection(..) and client.describe_datacenter(..) calls are an unnecessary overhead when all nodes (or first endpoint location) are up, and could be avoided by lazy-loading the list.;;;","11/Jun/11 09:15;mck;New patch. I think i'm at last happy with it.

getLocations() returns an iterator so client.describe_datacenter() is only called when necessary.

Rather than provide a list in initialAddress it was possible to use either the initialAddress OR the endpoint. This gave the benefit in not listing a location that can't actually be connected to.

The ""only use replica from same DC"" is an option now in ConfigHelper. By default it is true.

Again the re-generated Cassandra.java is not included in the patch.

I have tested this on normal jobs, and RF=2 jobs with a node down.;;;","13/Jun/11 15:23;tjake;The get_rack seems unused so it should be removed.

Also, it might be better to pass all locations in the get_datacenter thrift call since you can get the results in one shot and sort them by the dynamic snitch, filtering out the dead nodes:

{noformat}
  DatabaseDescriptor.getEndpointSnitch().sortByProximity(FBUtilities.getLocalAddress(), endpoints); 
{noformat}

{noformat}
  FailureDetector.instance.isAlive(endpoint)
{noformat}

;;;","13/Jun/11 16:15;mck;Then (if i understand you correctly) i would need in cassandra.thrift
{noformat}
      /** returns alive endpoints, sorted by proximity, that belong in the same datacenter as the given endpoint */
  list<string> get_endpoints_in_same_datacenter(1: string endpoint, 2: required list<string> endpoints)
    throws (1:InvalidRequestException ire)
{noformat}

Then the API becomes quite specific to this usecase. Is the performance gain worth it? What's the cost of each client.describe_datacenter(..) call, and probably more important what is the lost performance of writing to the furthest node that's within the same datacenter?;;;","13/Jun/11 16:40;mck;Just make sure i understand you T Jake, you would rather something like this in CassandraServer.java?
(I've renamed from the previous comment get_endpoints_in_same_datacenter(..) to sort_endpoints_by_proximity(..))
{noformat}
    public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC) 
        throws TException, InvalidRequestException
    {
        try
        {
            List<String> results = new ArrayList<String>();
            InetAddress address = InetAddress.getByName(endpoint);
            String datacenter = DatabaseDescriptor.getEndpointSnitch().getDatacenter(address);
            List<InetAddress> addresses = new ArrayList<InetAddress>();
            for(String ep : endpoints)
            {
                addresses.add(InetAddress.getByName(ep));
            }
            DatabaseDescriptor.getEndpointSnitch().sortByProximity(address, addresses);
            for(InetAddress ep : addresses)
            {
                String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(ep);
                if(FailureDetector.instance.isAlive(ep) && (!restrictToSameDC || datacenter.equals(dc)))
                {
                    results.add(ep.getHostName());
                }
            }
            return results.toArray(new String[results.size()]);
        }
        catch (UnknownHostException e)
        {
            throw new InvalidRequestException(e.getMessage());
        }
    }
{noformat};;;","13/Jun/11 16:59;tjake;bq. what is the lost performance of writing to the furthest node that's within the same datacenter?

The benefit is really the DynamicSnitch. if a node it slow due to compaction then this would avoid sending requests there... 

bq. public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC)

I don't think it makes sense to send the client endpoint to this call since the endpoint might not be a cassandra node.  It's a reasonable assumption that the endpoint it's talking to is local enough to the client to use that.

;;;","13/Jun/11 17:52;mck;{quote}
bq.   public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC)
I don't think it makes sense to send the client endpoint to this call since the endpoint might not be a cassandra node. It's a reasonable assumption that the endpoint it's talking to is local enough to the client to use that.
{quote}
For the test set i was running against, RF=2, each split's has two endpoints always in different datacenters.

If the ""local"" endpoint is down then getLocations() will then call client.sort_endpoints_by_proximity(..) and this will fail (being the same endpoint).
It then makes a client connection through the ""other"" endpoint. \[see CFRR.describeDatacenter(..)].
This will presume the wrong datacenter and return itself as a valid endpoint. 
I need some way to know what the original datacenter is, even when it is down.;;;","13/Jun/11 18:19;tjake;ok but why not change the response to map<string,list<string>>  where key is DC and value are proximity sorted endpoints?;;;","13/Jun/11 19:19;mck;Won't the sorting still be wrong?
For the use-case above it will solve restricting to the correct datacenter, but the sorting will still be based on proximity to the wrong node?

bq. I don't think it makes sense to send the client endpoint to this call since the endpoint might not be a cassandra node. 
It might not be an alive cassandra node, but it should be a cassandra node. It comes from the split's list of endpoints. At least in this use-case, or are you referring to general usage for this new api?
bq. It's a reasonable assumption that the endpoint it's talking to is local enough to the client to use that.
I don't think so... The endpoint that it talks to is a completely random (just the next endpoint listed in the split's list). This is why i think that such sorting won't just be wrong but not even close. Does this make sense?;;;","14/Jun/11 13:07;tjake;I think the core issue is you can't assume the hadoop node is running on a cassandra node...

If it is then the logic is straight forward, if not then it's possible the connection could cross DC boundaries. One possibility is to use the ip octets like the RackInferringSnitch.  

How's this proposal then?  keep the sort_endpoints_by_proximity signature as is and pass the client endpoint along with the list of data endpoints and add the following logic:

1) sort the endpoints using the endpoint_snitch.
2) if client endpoint *is* a valid cassandra node get the nodes DC and prune nodes outside of this DC
3) if client endpoint *is not* a valid cassandra node try to infer the DC from its ip and prune dataendpoint nodes in a different DC. If no cassandra nodes are in the DC list goto 3).
4) all else fails return the sorted endpoint list
;;;","15/Jun/11 11:23;mck;bq. [snip] One possibility is to use the ip octets like the RackInferringSnitch. 

In our usecase we have three nodes defined via PropertyFileSnitch:{noformat}152.90.241.22=DC1:RAC1 #node1
152.90.241.23=DC2:RAC1 #node2
152.90.241.24=DC1:RAC1 #node3{noformat}
The only way to infer here is even addresses belong to one dc, odd to the other. This is not how RackInferringSnithc works.

When we make the connection through the ""other"" (node2) endpoint taking the rack inferring approach ""152.90."" will say it's in DC2. (again) this is the wrong DC and will return itself as a valid endpoint....

Step (3) seems to me to be too specific to be included here.
If i go only with steps (1),(2),and (4) we get this code:{noformat}    public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC) 
            throws TException, InvalidRequestException
    {
        try
        {
            List<String> results = new ArrayList<String>();
            InetAddress address = InetAddress.getByName(endpoint);
            boolean endpointValid = null != Gossiper.instance.getEndpointStateForEndpoint(address);
            String datacenter = DatabaseDescriptor
                    .getEndpointSnitch().getDatacenter(endpointValid ? address : FBUtilities.getLocalAddress());
            List<InetAddress> addresses = new ArrayList<InetAddress>();
            for(String ep : endpoints)
            {
                addresses.add(InetAddress.getByName(endpoint));
            }
            DatabaseDescriptor.getEndpointSnitch().sortByProximity(address, addresses);
            for(InetAddress ep : addresses)
            {
                String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(ep);
                if(FailureDetector.instance.isAlive(ep) && (!restrictToSameDC || datacenter.equals(dc)))
                {
                    results.add(ep.getHostName());
                }
            }
            return results.toArray(new String[results.size()]);
        }
        catch (UnknownHostException e)
        {
            throw new InvalidRequestException(e.getMessage());
        }
    }{noformat}

I'm happy with this (except that {{Gossiper.instance.getEndpointStateForEndpoint(address)}} is only my guess on how to tell if an endpoint is valid as such).;;;","22/Jun/11 13:04;mck;Problem with the suggested approach is that sortByProximity(..) *only* works when address is the local address. See assert statement DynamicEndpointSnitch:134

I could hack this and rewrite the line to
{noformat}IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
snitch = snitch instanceof DynamicEndpointSnitch ? ((DynamicEndpointSnitch)snitch).subsnitch : snitch;
snitch.sortByProximity(address, addresses);{noformat}
But this of course means that we always bypass DynamicEndpointSnitch's ""scores"".;;;","22/Jun/11 13:46;mck;Up to date patch.
Follows T Jake's points (1),(2), and (4).
And bypasses DynamicEndpointSnitch when sorting by proximity.;;;","24/Jun/11 15:43;tjake;committed with a change to use the dynamic snitch id the passed endpoint is valid.;;;","24/Jun/11 16:43;hudson;Integrated in Cassandra-0.8 #191 (See [https://builds.apache.org/job/Cassandra-0.8/191/])
    Change ColumnFamilyRecordReader to read split from replicas if primary is down

Patch by Mck SembWever; reviewed by tjake for CASSANDRA-2388

jake : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1139358
Files : 
* /cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ConfigHelper.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ColumnFamilyInputFormat.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
* /cassandra/branches/cassandra-0.8/interface/cassandra.thrift
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java
;;;","25/Jun/11 00:50;jeromatron;This patch applies to the current 0.7-branch with minimal problems - just some imports on CassandraServer that it couldn't resolve properly.  Can this be committed against 0.7-branch for inclusion in 0.7.7?;;;","25/Jun/11 01:05;jeromatron;I've done basic testing with the word count and pig examples to make sure that the basic hadoop integration isn't negatively affected by this.  I'll also try it against our dev cluster before and after the patch - killing one node to see if it fails over to another replica - to make sure it does what it should that way.;;;","25/Jun/11 01:15;jeromatron;Reopening for testing against 0.7.6.;;;","25/Jun/11 02:44;jbellis;Took a look at this belatedly.  I don't understand the contortions at all.  It looks like there's a ton of effort put in to avoiding making sortByProximity work w/ non-local nodes.  Why not just make that work instead?
;;;","25/Jun/11 02:48;jbellis;also: running hadoop on a non-cassandra node is dumb.  i don't see a point in supporting that really.  (yes, my fault it was written that way to begin with, mea culpa.);;;","25/Jun/11 02:58;jeromatron;Jonathan - is it possible to attach an updated patch based on your changes to 0.8 branch?  Not sure if that would be simple to extract.;;;","25/Jun/11 03:07;jbellis;with svn: svn diff -r 1139323:1139483 and hack out the OutboundTcpConnection change in the middle manually from the output.

with git: create a branch, rip out the offending OTC change and squash the other two;;;","25/Jun/11 03:07;jbellis;I think there's deep surgery to be done here still though.  Backporting is probably premature.;;;","25/Jun/11 03:12;jbellis;bq. It looks like there's a ton of effort put in to avoiding making sortByProximity work w/ non-local nodes

Wait, why do we even care?  ""local node"" IS the right host to sort against -- we want the split that is closest to the node running the job, this is not the same as some other C* node we contact.;;;","28/Jun/11 14:13;mck;bq. It looks like there's a ton of effort put in to avoiding making sortByProximity work w/ non-local nodes
Because it's only when that local node is down that we actually need to sort...
When/if DynamicEndpointSnitch's limitation is fixed (and it can sort by non-local nodes) then CassandraServer.java need not bypass it. But this won't simplify the code in CFRR. Now that CFIF supports multiple initialAddresses the method CFRR.sortEndpointsByProximity(..) can be rewritten (ie any connection to any initialAddress is all we need, no need to mess around with trying to connect through replica's to find information about replicas...)
bq. Wait, why do we even care? ""local node"" IS the right host to sort against
Depends on this is CFRR's ""local node"" or CassandraServer's ""local node""... 
CFRR's local node is the right and only node worth sorting against, it being the ""task tracker node"". 
But when c* on the ""task tracker node"" is down, then we randomly connect to another c* node so to find out of the replica we know about which are 1) up, 2) closest, and 3) in the same dc. Then it is a random c* node that becomes the ""local node"" and the call needs to be {{snitch.sortByProximity(initialAddress, addresses)}}.
But yes... the CFRR code is contorted. In many ways i prefer the simplicity of the first patch (both in api and in implementation) despite it not being ""as correct"". i thought of this ""fallback to replica"" as a last resort to keep the m/r job running, rather than an actively used feature where DynamicEndpointSnitch's scores will maximise performance. But then i'm only thinking in terms of a small c* cluster and i certainly am naive about what performance gains these scores can give...;;;","28/Jun/11 14:51;jbellis;bq. CFRR's local node is the right and only node worth sorting against, it being the ""task tracker node"". 

Right.

bq. Then it is a random c* node that becomes the ""local node""

We still want to sort by proxmity-to-TT, because CFRR connects directly to the split owner to do the reads.  initialAddress isn't involved post-split-discovery.

Again, all the complexity goes away if we just embed the snitch into CFIF/TT.

One wrinkle: ec2snitch requires gossip, so TT would need a separate local ip to participate in the gossip ring.  We could make that optional (and fall back to old ""recognize local data, otherwise you get a random replica"" behavior otherwise).
;;;","28/Jun/11 15:01;jbellis;Taking a step back: aren't we optimizing for (1) a corner case with (2) the wrong solution?

Here's what I mean:

1) CFRR already prioritizes the local replica.  So if you have >= one TT for each replica, this only helps if the local C* node dies, BUT the TT does not.  This doesn't happen often.

2) If we ARE in that situation, the ""right"" solution would be to send the job to a TT whose local replica IS live, not to read the data from a nonlocal replica.  How can we signal that?  ;;;","28/Jun/11 15:02;mck;CASSANDRA-2388-addition1.patch: Simplify CFRR now that multiple initialAddresses are supported.;;;","28/Jun/11 21:11;brandon.williams;bq. If we ARE in that situation, the ""right"" solution would be to send the job to a TT whose local replica IS live, not to read the data from a nonlocal replica. How can we signal that?

ISTM the right thing to do in that situation is just fail and let the JT reschedule somewhere else.;;;","29/Jun/11 04:58;mck; - This does happen already (i've seen it while testing initial patches that were no good).
Problem is that the TT is blacklisted, reducing hadoop's throughput for all jobs running.
I bet too that a fallback to a replica is faster than a fallback to another TT.

 - There is no guarantee that any given TT will have its split accessible via a local c* node - this is only a preference in CFRR. A failed task may just as likely go to a random c* node. At least now we can actually properly limit to the one DC and sort by proximity. 

 - One thing we're not doing here is applying this same DC limit and sort by proximity in the case when there isn't a localhost preference. See CFRR.initialize(..)
It would make sense to rewrite CFRR.getLocations(..) to
{noformat}    private Iterator<String> getLocations(final Configuration conf) throws IOException
    {
        return new SplitEndpointIterator(conf);
    }{noformat} and then to move the finding-a-preference-to-localhost code into SplitEndpointIterator...

 - A bug i can see in the patch that did get accepted already is in CassandraServer.java:763 when endpointValid is false and restrictToSameDC is true we end up restricting to a random DC. I could fix this so restrictToSameDC is disabled in such situations but this actually invalidates the previous point: we can't restrict to DC anymore and we can only sortByProximity to a random node... I think this supports Jonathan's point that it's overall a poor approach. I'm more and more in preference of my original approach using just client.getDatacenter(..) and not worrying about proximity within the datacenter.

 - Another bug is that, contray to my patch, the code committed
bq. committed with a change to use the dynamic snitch id the passed endpoint is valid.
 can call {{DynamicEndpointSnitch.sortByProximity(..)}} with an address that is not localhost and this breaks the assertion in the method. ;;;","29/Jun/11 18:40;brandon.williams;{quote}
This does happen already (i've seen it while testing initial patches that were no good).
Problem is that the TT is blacklisted, reducing hadoop's throughput for all jobs running.
{quote}

If the cassandra node where the TT resides isn't working, then throughput is reduced regardless.


bq. I bet too that a fallback to a replica is faster than a fallback to another TT.

I doubt that for any significant job.  Locality is important.  Move the job to the data, not the data to the job.

{quote}
There is no guarantee that any given TT will have its split accessible via a local c* node - this is only a preference in CFRR. A failed task may just as likely go to a random c* node. At least now we can actually properly limit to the one DC and sort by proximity.
{quote}

This sounds like the thing we need to fix, then.  Ensuring that the TT assigned to the map has a local replica.;;;","29/Jun/11 19:25;jbellis;bq. If the cassandra node where the TT resides isn't working, then throughput is reduced regardless.

Right: we _want_ it to be blacklisted in that scenario.;;;","29/Jun/11 19:45;jbellis;bq. This sounds like the thing we need to fix, then. Ensuring that the TT assigned to the map has a local replica.

reverted 1139358, 1139483 to make a fresh start for this.

how do we ""ensure"" this?  isn't that the JT's job, to send jobs to the splits we gave it from CFIF?  (which does make sure that only nodes with the data, are included in the split source list.);;;","29/Jun/11 19:49;mck;{quote}If the cassandra node where the TT resides isn't working, then throughput is reduced regardless.
bq. Right: we want it to be blacklisted in that scenario.{quote}
This is making the presumption that the hadoop cluster is only used with CFIF.
The TT could still be useful for other jobs submitted.
Furthermore a blacklisted TT does't automatically come back - it needs to be manually restarted. Isn't this creating more headache for operations?;;;","29/Jun/11 19:59;tjake;I dont think we should require the TT to be running locally. The whole idea is to support access to Cassandra data from hadoop even if it's just an import. 

This patch does spend a lot of time dealing with non local data for that reason. ;;;","29/Jun/11 20:47;brandon.williams;{quote}
This is making the presumption that the hadoop cluster is only used with CFIF.
The TT could still be useful for other jobs submitted.
{quote}

I'm fine with that assumption.  If you want to run other jobs, use a different cluster.  Cassandra's JVM is eating wasteful memory at that point.

{quote}
Furthermore a blacklisted TT does't automatically come back - it needs to be manually restarted. Isn't this creating more headache for operations?
{quote}

I don't think this is actually the case, see HADOOP-4305


{quote}
I dont think we should require the TT to be running locally. The whole idea is to support access to Cassandra data from hadoop even if it's just an import.

This patch does spend a lot of time dealing with non local data for that reason.
{quote}

I'm fine with dropping support for non-colocated TTs, or at least saying there's no DC-specific support.  Because frankly, that is a very suboptimal thing to do, transfer the data across the network all the time, and flies in the face of Hadoop's core principles.;;;","29/Jun/11 21:00;jbellis;bq. a blacklisted TT does't automatically come back

tlipcon says it comes back after 24h, fwiw.  In any case it's still the case that we DO want to blacklist it while it's down.  (Brisk could perhaps add a ""clear my tasktracker on restart"" operation as a further enhancement.)

bq. I'm fine with dropping support for non-colocated TTs

+1, it was a bad idea and I'm sorry I wrote it. :);;;","29/Jun/11 21:16;mck;bq. tlipcon says it comes back after 24h
just to be clear about my concerns. 
this means a dead c* node will bring down a TT. In a hadoop cluster with 3 nodes this means for 24hrs you're lost 33% throughput. (If less than 10% of hadoop jobs used CFIF i could well imagine some pissed users). (What if you have a temporarily problem with flapping c* nodes and you end up with a handful of blacklisted TTs? etc etc etc).

All this when using a replica, any replica, could have kept things going smoothly, the only slowdown being some of the data into CFIF had to go over the network instead...
;;;","30/Jun/11 00:32;jbellis;bq. this means a dead c* node will bring down a TT

Again: _this is what you want to happen_.  As long as the C* process on the same node is down, you want the TT to be blacklisted and the jobs to go elsewhere.

bq. In a hadoop cluster with 3 nodes this means for 24hrs you're lost 33% throughput

Right, but the real cause is because the C* process is dead, not b/c the TT is blacklisted.  Making the TT read from other nodes will only hurt your network, not fix the throughput problem, b/c i/o is the bottleneck.;;;","30/Jun/11 07:50;mck;Then i would hope for two separate InputFormats. One optimised for local node connection, where cassandra is deemed the more important system over hadoop, and another where data can be read in from anywhere. I think the latter should be supported in some manner  since users may not always have the possibility to install hadoop and cassandra on the same servers, or they might not think it to be so critical part (eg if CFIF is reading using a IndexClause the input data set might be quite small and the remaining code in the m/r be the bulk of the processing...);;;","30/Jun/11 13:17;jbellis;bq. another where data can be read in from anywhere

This is totally antithetical to how hadoop is designed to work.  I don't think it's worth supporting in-tree.;;;","30/Jun/11 21:43;mck;Is CASSANDRA-2388-local-nodes-only-rough-sketch the direction we want then?

This is very initial code, i can't get {{new JobClient(JobTracker.getAddress(conf), conf).getClusterStatus().getActiveTrackerNames()}} to work, need a little help here.
(Also CFRR.getLocations() can be drastically reduced).;;;","01/Jul/11 20:05;jbellis;+1 to CFRR changes

wasn't immediately clear to me what CFIF changes are doing, can you elaborate?;;;","02/Jul/11 22:07;mck;The idea is to setup splits to have only endpoints that are valid trackers. But now i see this is just a brainfart :-) Ofc the jobTracker will apply this match for us. And that CFIF was always 'restricted' to running on endpoints. Although the documentation on inputSplit.getLocations() is a little thin as to whether this restricts which trackers it should run on or whether is just a preference... I guess it doesn't matter, as you point out Jonathan all that's required here is the one line changed in CFRR.

;;;","02/Jul/11 22:16;mck;the new ""one-liner"" CASSANDRA-2388 attached. i'll ""submit patch"" once i've tested it some...;;;","02/Jul/11 22:53;jbellis;Sounds good, thanks!;;;","04/Jul/11 06:39;mck;{quote}2) If we ARE in that situation, the ""right"" solution would be to send the job to a TT whose local replica IS live, not to read the data from a nonlocal replica. How can we signal that?{quote}To /really/ solve this issue can we do the following? 
In CFIF.getRangeMap() take out of each range any endpoints that are not alive. A client connection already exists in this method. This filtering out of dead endpoints wouldn't be difficult, and would move tasks *to* the data making use of replica. This approach does need a new method in cassandra.thrift, eg {{list<string> describe_alive_nodes()}};;;","15/Aug/11 18:47;jbellis;Does that really fix things though?  Because you could have a data node be reachable from the coordinator answering describe_alive_nodes, but unreachable from the client.  So the client still needs to be able to skip unreachable endpoints itself, so describe_alive seems like gratuitous complexity.;;;","19/Aug/11 19:39;patrik.modesto;I'd like to point out the situation in which no node for a given range of keys is available. It can happen for example with keyspace set to RF=1 and a node goes down. I created a patch that gives a user a chance to ignore missing range/node and continue runnig the MapReduce job. The patch is here: http://pastebin.com/hhrr8m9P

Jonathan already replied to the ML with ""ignoring unavailable ranges is a misfeature, imo"".

In our case it's very usefull, although there may be another/smarter solution. We have a keyspace with RF=1 and the nature of our data allows us to ignore temporarily missing node. The current ColumnFamilyInputFormat fails with RuntimeException and AFAIK there is no way around.;;;","19/Aug/11 20:59;brandon.williams;bq. Does that really fix things though? Because you could have a data node be reachable from the coordinator answering describe_alive_nodes, but unreachable from the client. So the client still needs to be able to skip unreachable endpoints itself, so describe_alive seems like gratuitous complexity.

I agree, since the view is from the coordinator, describe_alive_nodes isn't very helpful, and also has to wait on the failure detector to mark the node down anyway.;;;","19/Aug/11 22:44;jbellis;bq. I agree, since the view is from the coordinator, describe_alive_nodes isn't very helpful

Committed Mck's most recent patch.

bq. We have a keyspace with RF=1 and the nature of our data allows us to ignore temporarily missing node

The ""right"" fix is to increase RF.  Ignoring missing data is not a scenario we want to support.;;;","22/Aug/11 08:27;hudson;Integrated in Cassandra-0.7 #539 (See [https://builds.apache.org/job/Cassandra-0.7/539/])
    fail jobs when Cassandra node has failed but TaskTracker has not
patch by Mck SembWever; reviewed by jbellis and brandonwilliams for CASSANDRA-2388

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1159807
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
;;;","30/Aug/11 20:55;mck;This approach isn't really working for me and was committed too quickly i believe.

bq. Although the documentation on inputSplit.getLocations() is a little thin as to whether this restricts which trackers it should run on or whether is just a preference

Tasks are still being evenly distributed around the ring regardless of what the ColumnFamilySplit.locations is.

The chance of a task actually working is RF/N. Therefore the chances of a blacklisted node are high. Worse is that the whole ring can quickly become blacklisted.

http://abel-perez.com/hadoop-task-assignment has an interesting section in it explaining how the task assignment is supposed to work (and that data locality is preferred but not a requirement). Could ColumnFamilySplit.locations be in the wrong format? (eg they should ip not hostname?).;;;","31/Aug/11 16:23;mck;see last comment. (say if this should be a separate bug...)

Maybe hadoop's task allocation isn't working properly because i've an unbalanced ring (i'm working in parallel to fix that).
If this is the case i think it's an unfortunate limitation (the ring must be balanced to get any decent hadoop performance).
It's also probably likely when using {{ConfigHelper.setInputRange(..)}} that the number of nodes involved is small (approaching RF).
With the default hadoop scheduler your hadoop cluster is occupied while just a few taskTrackers are busy. Of course switching to FairScheduler will help some here.

I'll take a look into hadoop's task allocation code as well...;;;","08/Sep/11 06:01;mck;In the meantime could we make this behavior configurable.
eg replace CFRR:176 with something like
{noformat}
    if(ConfigHelper.isDataLocalityDisabled())
    {
        return split.getLocations()[0];
    }
    else
    {
        throw new UnsupportedOperationException(""no local connection available"");
    }{noformat}
;;;","08/Sep/11 12:55;jbellis;Should we just revert the change for now?;;;","08/Sep/11 21:36;mck;Well that would work for me, was only thinking you want to push a ""default behavior"" (especially for those using a RP). 
But I think a better understanding (at least from me) of hadoop's task scheduling is required before enforcing data locality, as as-is it certainly doesn't work for all.;;;","14/Sep/11 02:25;tjake;I just want to confirm what this ticket is about.

The JT has a list of endpoints for a given split.
When a task runs it may or may not be on one of those nodes 
If other tasks are running on all those replicas the JT may put them on a remote node.

So we need to decide which endpoint to connect to given the chance that nodes are down.

1. Check if the node running CFRR is one of the replicas (we have this) this means JT has assigned a data-local task (good)
2. If none of these nodes are local then pick another.
3. If connection fails try the one other nodes.
4. Try to avoid endpoints in a different DC.

The biggest problem is 4.  Maybe the way todo this is change getSplits logic to never return replicas in another DC.  I think this would require adding DC info to the describe_ring call.  Then we only need to worry about 1-3.






;;;","14/Sep/11 14:49;hudson;Integrated in Cassandra-0.7 #552 (See [https://builds.apache.org/job/Cassandra-0.7/552/])
    revert CASSANDRA-2388 (again)

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1170333
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
;;;","08/Mar/12 22:38;jbellis;Marking as minor since the job should get re-submitted, and it's very difficult to reproduce when the tasktrackers are colocated with cassandra nodes (the recommended configuration).;;;","31/Oct/12 16:33;lannyripple;Would very much like a fix to this.  We have a 40 node ring running 2x hadoop clusters on 20 nodes each.  One cluster is on systems that are more flaky than the other (bad batch of memory).  When building a split on the first cluster if a ring node is down in the area of the second cluster we get timeouts with no way to blacklist the offending node even though we have replicas local to the first cluster.

The ring is partitioned into DC1:2, DC2:2 with a hadoop cluster over each DC.;;;","21/Nov/12 11:06;jbellis;Jake's plan above seems like a reasonable approach, but let me back up a step.  I'm just not convinced that the problem we're trying to solve is a real one.  Why do we want to suck a split's worth of data off-node?  If it's because you don't have TackTrackers running on your Cassandra nodes, well, go fix that.

If it's because Hadoop has created too many tasks and all the local replicas have their task queue full, won't assigning it to a non-local TT just cause more contention, than waiting for a local slot to free up?;;;","21/Nov/12 14:42;scottfines;I have two distinct use-cases where running TaskTrackers alongside Cassandra nodes does not accomplish our goals:

1. Joining data. We have a large data set in cassandra, true, but we have a *much* larger data set held in Hadoop itself (around 4 orders of magnitude larger in hadoop than in cassandra). We need to join the two datasets together, and use the output from that join to feed multiple systems, none of which are cassandra. Since the data in Hadoop is so much larger than that in Cassandra, we have to bring the Cassandra data to hadoop, not the other way around. Because of security concerns, we can't spread our hadoop data onto our cassandra nodes (even if that didn't screw with our capacity planning), so we have no other choice but to move the Cassandra data (in small chunks) onto Hadoop. Why not use HBase, you say? We needed Cassandra for its write performance for other problems than this one. 

1. Offline, incremental backups. We have a large volume of time-series data held in Cassandra, and taking nightly snapshots and moving them to our archival center is prohibitively slow--it turns out that moving RF copies of our entire dataset over a leased line every night is a pretty bad idea. Instead, I use MapReduce to take an incremental backup of a much smaller subset of the data, then move that. That way, we not only are not moving the entire data set, but we are also using Cassandra's consistency mechanisms to resolve all the replicas. The only efficient way I've found to do this is via MapReduce (we use the Random Partitioner), and since it's an offline backup, we need to move it over the network anyway--may as well use the optimized network connecting Hadoop and Cassandra instead of the tiny pipe connecting cassandra to our archival center. 

Both of these reasons dictate that we *not* run a TT alongside our Cassandra nodes, no matter what the *recommended* approach is. In this case, we need a strong, fault-tolerant CFIF to serve our purposes.

;;;","18/May/13 21:50;mck;Jonathan,
 I can't say i'm in favour of enforcing data locality.
Because  data locality in hadoop doesn't work this way… when a tasktracker through the next heartbeat announces that it has a task slot free the jobtracker will do its best to assign a task with data locality to it but failing this will assign it a random task. the number of these random tasks can be quite high, just like i mentioned above
{quote} Tasks are still being evenly distributed around the ring regardless of what the ColumnFamilySplit.locations is. {quote}

This can be almost solved by upgrading to hadoop-0.21+, using the fair scheduler and setting the property {code}<property>
        <name>mapred.fairscheduler.locality.delay</name>
        <value>360000000</value>
<property>{code}.

At the end of the day while hadoop encourages data locality it does not enforce it.
The ideal approach would be to sort all locations by proximity.
The feasible approach hopefully is still [~tjake]'s above. In addition i'd be in favour of a setting in the job's configuration as to whether a location from another datacenter can be used.

references:
 - http://www.infoq.com/articles/HadoopInputFormat
 - http://www.mentby.com/matei-zaharia/running-only-node-local-jobs.html
 - https://groups.google.com/a/cloudera.org/forum/?fromgroups#!topic/cdh-user/3ggnE5hV0PY
 - http://www.cs.berkeley.edu/~matei/papers/2010/eurosys_delay_scheduling.pdf;;;","27/May/13 16:02;jbellis;bq. The feasible approach hopefully is still T Jake Luciani's above

Okay.  Referring back to Jake's comments,

bq. The biggest problem is [avoiding endpoints in a different DC]. Maybe the way todo this is change getSplits logic to never return replicas in another DC. I think this would require adding DC info to the describe_ring call

I note that we expose node snitch location in system.peers.  So at worst we could ""join"" against that manually.;;;","27/May/13 18:41;mck;{quote}The biggest problem is [avoiding endpoints in a different DC]. Maybe the way todo this is change getSplits logic to never return replicas in another DC. I think this would require adding DC info to the describe_ring call{quote}

Tasktrackers may have access to a set of datacenters, so this DC info needs contain a list of DCs.

For example, our setup separates datacenters by physical datacenter and hadoop-usage, like:{noformat}DC1 ""Production + Hadoop""
  c*01 c*03
DC2 ""Production + Hadoop""
  c*02 c*04
DC3 ""Production""
  c*05
DC4 ""Production""
  c*06{noformat}

So here we'd pass to getSplits() a DC info like ""DC1,DC2"".
But the problem remain, given a task executing on c*01 that fails to connect to localhost, although we can now prevent a connection to DC3 or DC4, we can't favour a connection to any other split in DC1 over anything in DC2. Is this solvable? ;;;","09/Jun/14 14:19;pauloricardomg;Attached [2.0-CASSANDRA-2388.patch|https://issues.apache.org/jira/secure/attachment/12649379/2.0-CASSANDRA-2388.patch] (against 2.0), where CASSANDRA-6302 is ported to ColumnFamilyRecordReader, so the next replicas are tried when the first one fails.

Reading from a non-local-DC replica is not a problem anymore due to the introduction of describe_local_ring (CASSANDRA-6268), that limits the input splits to the local DC.

I would like to acknowledge my colleague Danilo Penna Queiroz who paired with me on this patch. ;;;","09/Jun/14 14:41;jbellis;[~pkolaczk] to review;;;","22/Jun/14 21:05;pauloricardomg;[~pkolaczk] any update on this? cheers! :);;;","23/Jun/14 07:44;pkolaczk;[~pauloricardomg] I can't promise, but I try to do that at the end of this week. ;;;","25/Jun/14 19:22;pauloricardomg;Attaching patch based on 1.2.16 and fixed patch (v2) for 2.0.

Maybe the 1.2 patch can still make it to 1.2.17...;;;","07/Aug/14 19:19;pauloricardomg;[~pkolaczk] any update on this? this review has been roaming for quite some time now...
sorry for bothering but would be nice to see this integrated. cheers!;;;","06/Oct/14 20:27;pauloricardomg;http://mail-archives.apache.org/mod_mbox/cassandra-dev/201410.mbox/%3CCALdd-zjmvp7JOtguZ_k951RQHDtFt1cthX=RnHQ332C=gAZbjw@mail.gmail.com%3E;;;","10/Oct/14 11:16;mck; [~pkolaczk] + [~pauloricardomg],  AFAIK everything thrift related is frozen, so i presume the patch isn't going to be applied to master.
Otherwise it's +1 on the patch from me.;;;","01/Dec/14 14:02;pkolaczk;+1;;;","24/Aug/15 16:18;jbellis;Paulo, can you rebase to 2.1?;;;","25/Aug/15 13:30;pauloricardomg;Rebased 2.1 patch available [here|https://github.com/pauloricardomg/cassandra/tree/2388-2.1].

2.1 tests:
* [testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.1-testall/lastCompletedBuild/testReport/]
* [dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.1-dtest/lastCompletedBuild/testReport/]

2.2 tests (don't know if {{ColumnFamilyInputFormat}} and {{ColumnFamilyRecordReader}} should be deprecated by then, but the classes are still present there):
* [testall|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.2-testall/lastCompletedBuild/testReport/]
* [dtest|http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.2-dtest/lastCompletedBuild/testReport/];;;","20/Nov/15 13:54;slebresne;Committed, thanks.;;;"
log4j unable to load properties file from classpath,CASSANDRA-2383,12502347,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dallsopp,iecanfly,iecanfly,25/Mar/11 04:07,16/Apr/19 09:33,14/Jul/23 05:52,13/Jul/11 03:19,0.7.8,,,Legacy/Tools,,,0,,,,"when cassandra home folder is placed inside a folder which has space characters in its name,
log4j settings are not properly loaded and warning messages are shown.","OS : windows
java : 1.6.0.23",dallsopp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/11 11:05;dallsopp;cassandra-0.7.6-2-2383.diff;https://issues.apache.org/jira/secure/attachment/12484961/cassandra-0.7.6-2-2383.diff",,,,,,,,,,,,,,1.0,dallsopp,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20595,,,Wed Jul 13 03:30:35 UTC 2011,,,,,,,,,,"0|i0gb2f:",93223,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"27/Jun/11 20:16;dallsopp;In cassandra.bat, change:

 -Dlog4j.configuration=log4j-server.properties^

to:

 -Dlog4j.configuration=""file:///%CASSANDRA_HOME%/conf/log4j-server.properties""^

I'm hopeful there's a less ugly way, but this seems to work.;;;","27/Jun/11 20:28;dallsopp;OK, simpler solution is:

 -Dlog4j.configuration=file:conf/log4j-server.properties^

But note that this only works if the current directory is CASSANDRA_HOME, so double-clicking on the batch file won't work, whereas the previous solution will work from the 'bin' directory, so double-clicking is OK.;;;","27/Jun/11 21:42;jbellis;log4j should already find it because of

set CLASSPATH=""%CASSANDRA_HOME%\conf""
;;;","27/Jun/11 21:45;dallsopp;Another solution that seems to work regardless of working directory is leave the original log4j.configuration line, but remove the line:

-Dlog4j.defaultInitOverride=true^

This gets the classloader to find the config file as a resource, rather than supplying a file reference directly.

However, I'm unsure why the defaultInitOverride was there in the first place...;;;","27/Jun/11 21:55;dallsopp;@Jonathan - yes, I thought so too, but it doesn't. The default init process for log4j is described at http://logging.apache.org/log4j/1.2/manual.html, but it doesn't really explain what happens if defaultInitOverride is set!

With  -Dlog4j.debug=true and the original batch file, i.e.

{quote}
 -Dlog4j.configuration=log4j-server.properties^
 -Dlog4j.defaultInitOverride=true^}}
{quote}
I see:
{quote}
Starting Cassandra Server
log4j: [/C:/Users/David/Key%20Value/apache-cassandra-0.7.6-2/conf/log4j-server.properties] does not exist.
log4j: Default initialization of overridden by log4j.defaultInitOverrideproperty.
log4j:WARN No appenders could be found for logger (org.apache.cassandra.service.AbstractCassandraDaemon).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{quote}
If I remove the defaultInitOverride, I get:
{quote}
Starting Cassandra Server
log4j: [/C:/Users/David/Key%20Value/apache-cassandra-0.7.6-2/conf/log4j-server.properties] does not exist.
log4j: Trying to find [log4j-server.properties] using context classloader sun.misc.Launcher$AppClassLoader@1a45a877.
log4j: Using URL [file:/C:/Users/David/Key%20Value/apache-cassandra-0.7.6-2/conf/log4j-server.properties] for automatic log4j configuration.
log4j: Reading configuration from URL file:/C:/Users/David/Key%20Value/apache-cassandra-0.7.6-2/conf/log4j-server.properties
[etc...]
{quote}
Finally, with:
{quote}
 -Dlog4j.configuration=file:conf/log4j-server.properties^
 -Dlog4j.defaultInitOverride=true^
{quote}
I get this, if current directory is CASSANDRA_HOME:
{quote}
Starting Cassandra Server
log4j: Default initialization of overridden by log4j.defaultInitOverrideproperty.
[etc...]
{quote}
But this, if current directory is CASSANDRA_HOME/bin (e.g. if double-clicking the batch file):
{quote}
Starting Cassandra Server
log4j: [conf/log4j-server.properties] does not exist.
log4j: Default initialization of overridden by log4j.defaultInitOverrideproperty.
log4j:WARN No appenders could be found for logger (org.apache.cassandra.service.AbstractCassandraDaemon).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{quote};;;","27/Jun/11 22:04;jbellis;Weird.  Could you dig into the code in AbstractCassandraDaemon a little?  Here's what's trying to do load by classpath:

{code}
            // load from the classpath.
            configLocation = AbstractCassandraDaemon.class.getClassLoader().getResource(config);
            if (configLocation == null)
                throw new RuntimeException(""Couldn't figure out log4j configuration."");
{code}
;;;","27/Jun/11 22:10;dallsopp;http://www.vipan.com/htdocs/log4jhelp.html has the following:

log4j.configuration=app_config.properties: 

First call to Category.getRoot() or Category.getInstance(...) method makes Log4j go through an initialization process. (You can watch that happening by setting ""log4j.debug=true"".) 

During this process, Log4j looks in the application's classpath for a ""log4j.properties"" file *or the properties file you specify* via the this property key. 

However, you need to set this as a system property, for example by running your program with java -Dlog4j.configuration=app_config.properties .... This is because, if you set it in the configuration file, it is too late. Log4j would have already started to read the log4j.properties file by default, if available!;;;","27/Jun/11 22:22;dallsopp;@Jonathan - will try to take a look soon. The getResource() stuff might perhaps be affected by the classes being within a jarfile.;;;","28/Jun/11 21:22;dallsopp;OK, have gone around in circles a bit on this!

-Dlog4j.defaultInitOverride enables AbstractCassandraDaemon to take charge of the log4j configuration in order to make it dynamic (you can change the log4j config file, and it should be updated using the log4j PropertyConfigurator every 10 seconds).

The default value of log4j.configuration in the code and in the batch file is ""log4j-server.properties"", which is not a valid URL, so we drop into:

{noformat} configLocation = AbstractCassandraDaemon.class.getClassLoader().getResource(config);{noformat} 

as you said before. This *does* detect the correct file from CASSANDRA_HOME/conf, since log4j logs the *full path* even though we only supply the filename ""log4j-server.properties"":

{noformat} log4j: [/C:/Users/David/Key%20Value/apache-cassandra-0.7.6-2/conf/log4j-server.properties] does not exist.{noformat} 

getResource() returns a URL. Converting this to a file using getFile() works fine when there are no spaces; one can verify that the file exists (File.exists() == true). If there are spaces, then this conversion produces a filename that includes the %20 encoding for spaces - this is an incorrect filename.

We need instead to convert using:

{noformat} new File(configLocation.toURI());{noformat} 

(with appropriate exception handling for URISyntaxException) which produces a filename with spaces rather than %20.
;;;","28/Jun/11 22:31;dallsopp;Suggested fix for AbstractCassandraDaemon static initializer (apologies - haven't got a suitable version of diff on this windows box yet). Untested on linux as yet.

{noformat}
    //Initialize logging in such a way that it checks for config changes every 10 seconds.
    static
    {
        String config = System.getProperty(""log4j.configuration"", ""log4j-server.properties"");
        URL configLocation = null;
        try 
        {
            // try loading from a physical location first.
            configLocation = new URL(config);
        }
        catch (MalformedURLException ex) 
        {
            // then try loading from the classpath.
            configLocation = AbstractCassandraDaemon.class.getClassLoader().getResource(config);
        }
        
        if (configLocation == null)
            throw new RuntimeException(""Couldn't figure out log4j configuration: ""+config);
        
        // Now convert URL to a filename
        String configFileName = null;
		try
		{
			// first try URL.getFile() which works for opaque URLs (file:foo) and paths without spaces
			configFileName = configLocation.getFile();
			File configFile = new File(configFileName);
			// then try alternative approach which works for all hierarchical URLs with or without spaces
			if(!configFile.exists())
			{
				configFileName = new File(configLocation.toURI()).getCanonicalPath();
			}
		} 
		catch (Exception e)
		{
			throw new RuntimeException(""Couldn't convert log4j configuration location to a valid file."", e);
		} 
        PropertyConfigurator.configureAndWatch(configFileName, 10000);
        org.apache.log4j.Logger.getLogger(AbstractCassandraDaemon.class).info(""Logging initialized"");
    }
{noformat};;;","29/Jun/11 01:09;jbellis;Looks like this breaks ""configuration parameter is a well-formed url?""  configLocation is never used if ""new URL"" succeeds.;;;","29/Jun/11 13:34;dallsopp;Yes, that'll teach me to post code late at night :-(;;;","02/Jul/11 09:39;dallsopp;OK, have edited the code snippet above to hopefully fix the obvious broken-ness! 

Still struggling to get Cassandra building properly on Windows/Eclipse so haven't yet been able to test properly though (need to work through http://wiki.apache.org/cassandra/RunningCassandraInEclipse again from scratch as it didn't seem to work first time round...);;;","02/Jul/11 10:06;dallsopp;The above code seems to work for full hierarchical URIs:

-Dlog4j.configuration=""file:///C:/conf%20space/log4j-server.properties""

and for classpath locations:

-Dlog4j.configuration=log4j-server.properties

(on windows, with a space in the file path)

It does not work for opaque URIs such as file:log4j-server.properties because you can't construct a File directly from these (you get java.lang.IllegalArgumentException: URI is not hierarchical);;;","02/Jul/11 10:26;dallsopp;OK, third time lucky I hope. The edited version above now tries the original getFile() approach, then falls back on the new File(url.toURI()) approach if the file doesn't exist.

This seems to work with classpath names, opaque URIs *and* hierarchical URIs;;;","02/Jul/11 13:11;dallsopp;Tries to get URL of log4j configuration as before (as a URL or from the classpath). 

Attempts two different methods of converting the URL to a valid File in order to cope with spaces in file paths on Windows, as well as various flavours of URL (opaque or hierarchical).;;;","13/Jul/11 03:19;jbellis;looks good to me.  committed, thanks!;;;","13/Jul/11 03:30;hudson;Integrated in Cassandra-0.7 #527 (See [https://builds.apache.org/job/Cassandra-0.7/527/])
    support spaces in path to log4j configuration
patch by David Allsopp; reviewed by jbellis for CASSANDRA-2383

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1145849
Files : 
* /cassandra/branches/cassandra-0.7/CHANGES.txt
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
* /cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/service/AbstractCassandraDaemon.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
statistics component not fsynced,CASSANDRA-2382,12502344,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,25/Mar/11 03:28,16/Apr/19 09:33,14/Jul/23 05:52,07/Apr/11 15:36,0.7.5,,,,,,0,,,,The statistics file is prone to getting lost during a hard reset since it is not fsynced like the other sstable components.,,cburroughs,cscotta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/11 03:33;jbellis;2382.txt;https://issues.apache.org/jira/secure/attachment/12474587/2382.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20594,,,Sun Mar 27 05:44:34 UTC 2011,,,,,,,,,,"0|i0gb27:",93222,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"25/Mar/11 03:33;jbellis;Patch switches to BRAF (which fsyncs on close) and also enables fadvise to avoid caching the written stats.;;;","25/Mar/11 04:05;brandon.williams;+1;;;","25/Mar/11 04:47;jbellis;Not sure what to do about CompactSerializerTest -- it doesn't seem to know about ICompactSerializer2.  (I'm kind of surprised we still have so many old-style serializers...);;;","25/Mar/11 04:50;brandon.williams;We should probably get rid of one of them if we can, I've never really understood why there are two.;;;","25/Mar/11 04:57;jbellis;Some serializers did (do?) depend on Stream-specific methods.  Those may be gone now, it's been a while since I checked.;;;","25/Mar/11 13:58;jbellis;bq. Not sure what to do about CompactSerializerTest

Gary says the point of CST was to test message serialization rather than on disk and EHS got lumped in because it was using ICS instead of ISC2 like the other disk serializers (ColumnFamily and childeren).  So removing it from the CST list is the right fix here.

committed w/ that change.;;;","27/Mar/11 05:44;hudson;Integrated in Cassandra-0.7 #409 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/409/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
orphaned data files may be created during migration race,CASSANDRA-2381,12502323,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,24/Mar/11 20:55,16/Apr/19 09:33,14/Jul/23 05:52,28/Mar/11 19:07,0.7.5,,,,,,0,,,,"We try to prevent creating orphans by locking Table.flusherLock in maybeSwitchMemtable and the Migration process, but since the actual writing is done asynchronously in Memtable.writeSortedContents there is a race window, where we acquire lock in maybeSwitch, we're not dropped so we queue the flush and release the lock, Migration does the drop, then Memtable writes itself out.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/11 20:59;jbellis;2381-0.8.txt;https://issues.apache.org/jira/secure/attachment/12474557/2381-0.8.txt","28/Mar/11 16:01;jbellis;2381-v3.txt;https://issues.apache.org/jira/secure/attachment/12474782/2381-v3.txt","28/Mar/11 14:10;jbellis;2831-v2.txt;https://issues.apache.org/jira/secure/attachment/12474778/2831-v2.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20593,,,Mon Mar 28 20:16:26 UTC 2011,,,,,,,,,,"0|i0gb1z:",93221,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"24/Mar/11 20:59;jbellis;Patch against 0.8:

- moves Table.flusherLock to Memtable.flushLock; acquire during memtable writing
- replaces flushLock use in Table.maybeSwitchMemtables with a synchronized block
- removes beforeApplyModel; snapshotting is moved into applyModel for drops and removed entirely for updates;;;","28/Mar/11 13:21;gdusbabek;Is it necessary to make the MT lock static?  I do not think there would be a problem with concurrently flushing two memtables of different column families (the CL serialization is still preserved by the synchronization in CFS).;;;","28/Mar/11 14:10;jbellis;You're right. v2 attached and rebased. Had to make lock/unlock responsibility of individual Migration classes since lock is no longer global.;;;","28/Mar/11 14:54;gdusbabek;I'm seeing unit test failures in CliTest and DefsTest with v2 applied (never ran them on v1).;;;","28/Mar/11 16:01;jbellis;v3 grabs CF reference before removing it from CFMetadata map.  tests pass.;;;","28/Mar/11 17:49;gdusbabek;+1.;;;","28/Mar/11 19:07;jbellis;committed;;;","28/Mar/11 20:16;hudson;Integrated in Cassandra-0.7 #411 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/411/])
    r/m unnecessary isDropped check from maybeSwitchMemtable (the one in Memtable.writeSortedContents it the important one)
patch by jbellis for CASSANDRA-2381
add actual dropped check to Memtable.flush for CASSANDRA-2381
patch by jbellis
fix migration race vs flush
patch by jbellis; reviewed by gdusbabek for CASSANDRA-2381
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE During Repair In StreamReplyVerbHandler,CASSANDRA-2377,12502261,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,bcoverston,bcoverston,24/Mar/11 06:57,16/Apr/19 09:33,14/Jul/23 05:52,24/Mar/11 18:59,0.7.5,,,,,,0,,,,"ERROR [MiscStage:4] 2011-03-24 02:45:05,172 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutorjava.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:62)        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
ERROR [MiscStage:4] 2011-03-24 02:45:05,172 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[MiscStage:4,5,main]java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:62)        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)",CentOS,mishravivek,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/11 18:30;brandon.williams;2377.txt;https://issues.apache.org/jira/secure/attachment/12474537/2377.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20590,,,Fri Mar 25 17:49:14 UTC 2011,,,,,,,,,,"0|i0gb13:",93217,,,,,Normal,,,,,,,,,,,,,,,,,"24/Mar/11 09:22;mishravivek;As per my analysis,
I can see that it is happening because StreamOutSession is not instantiated.
Which is done via ""create"" call in StreamOutSession. 

StreamReplyVerbHandler should create a new session, in case StreamOutSession instance is null?

Will this work as a solution?;;;","24/Mar/11 09:27;mishravivek;Can you please provide more log or details on it?;;;","24/Mar/11 18:30;brandon.williams;Patch to just log a warning when a stream command is received for an unknown session.;;;","24/Mar/11 18:36;jbellis;What could cause unexpected stream commands?  Is there a deeper bug we should be looking for?;;;","24/Mar/11 18:42;bcoverston;In the case I was looking at there were also some failures in compaction.

This was seen after running a full repair on the node.

I'm not sure that they are related:

ERROR [CompactionExecutor:1] 2011-03-24 01:56:48,515 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.IndexOutOfBoundsException
        at java.nio.Buffer.checkIndex(Unknown Source)
        at java.nio.HeapByteBuffer.getInt(Unknown Source)
        at org.apache.cassandra.db.DeletedColumn.getLocalDeletionTime(DeletedColumn.java:57)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedStandard(ColumnFamilyStore.java:879)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedColumnsOnly(ColumnFamilyStore.java:866)
        at org.apache.cassandra.db.ColumnFamilyStore.removeDeleted(ColumnFamilyStore.java:857)
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:94)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:147)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doValidationCompaction(CompactionManager.java:822)
        at org.apache.cassandra.db.CompactionManager.access$800(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$6.call(CompactionManager.java:358)
        at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
;;;","24/Mar/11 18:42;brandon.williams;In this specific case, the machine was performing a repair and then restarted.  The other nodes then sent it session close commands, so really this is just a cosmetic problem.;;;","24/Mar/11 18:46;jbellis;I think I'd rather see it at debug level if it's expected.  +1 otherwise;;;","24/Mar/11 18:59;brandon.williams;Committed with the message moved to debug.;;;","24/Mar/11 19:15;hudson;Integrated in Cassandra-0.7 #406 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/406/])
    Log a message when a streaming action for an unkown session is received
instead of NPE.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-2377
;;;","25/Mar/11 09:55;mishravivek;I was going through the changes made. I wonder what should be the value of reply.getSessionID for:


 StreamReply reply = StreamReply.serializer.deserialize(new DataInputStream(bufIn), message.getVersion());

Should we check for this, instead of 

if (session == null)
            {
                logger.warn(""Received stream action "" + reply.action + "" for an unknown session from "" + message.getFrom());
                return;
         }
 
As per log it happens for :

 case SESSION_FINISHED:


So changes should be something like this:

if (SESSION_FINISHED.equals(reply.action.))
            {
                logger.warn(""Received stream action "" + reply.action + "" for an unknown session from "" + message.getFrom());
                return;
         }


Idea is to save any additional static call on StreamOutSession.get(message.getFrom(), reply.sessionId), which results in object instantiation for new Pair<InetAddress, Long>(host, sessionId).


;;;","25/Mar/11 17:49;brandon.williams;An unknown stream action could be received for any of them.  It's not hard to imagine a FILE_RETRY happening here, and all of them require a non-null session.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Both name an index iterators cast block offset to int,CASSANDRA-2376,12502254,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,24/Mar/11 04:55,16/Apr/19 09:33,14/Jul/23 05:52,28/Mar/11 16:15,0.7.5,,,,,,0,,,,This means that performing random access to the end of a large row will not work.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/11 05:22;jbellis;2376-v2.txt;https://issues.apache.org/jira/secure/attachment/12474720/2376-v2.txt","24/Mar/11 04:56;jbellis;2376.txt;https://issues.apache.org/jira/secure/attachment/12474468/2376.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20589,,,Mon Mar 28 18:16:39 UTC 2011,,,,,,,,,,"0|i0gb0v:",93216,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"24/Mar/11 04:56;jbellis;Patch also unifies skipBytes checking into skipBytesFully even where we really are only skipping an int's worth of bytes.;;;","24/Mar/11 13:36;slebresne;Small nitpick: it could be nice to add a message to the eof exception saying how many bytes out of how many have been skipped before reaching eof, but that'd require to do a catch and re-throw in the 'long' version.

+1;;;","27/Mar/11 05:22;jbellis;v2 adds length information to EOFException;;;","28/Mar/11 16:15;slebresne;Committed as rev1086290.;;;","28/Mar/11 18:16;hudson;Integrated in Cassandra-0.7 #410 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/410/])
    Don't cast block offsets to int
patch by jbellis; reviewed by slebresne for CASSANDRA-2376
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Removed/Dead Node keeps reappearing,CASSANDRA-2371,12502232,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,acctech,acctech,23/Mar/11 22:03,16/Apr/19 09:33,14/Jul/23 05:52,18/Apr/11 18:59,0.7.5,,,Legacy/Tools,,,2,,,,"The removetoken option does not seem to work. The original node 10.240.50.63 comes back into the ring, even after the EC2 instance is no longer in existence. Originally I tried to add a new node 10.214.103.224 with the same token, but there were some complications with that. I have pasted below all the INFO log entries found with greping the system log files.

Seems to be a similar issue seen with http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Ghost-node-showing-up-in-the-ring-td6198180.html 

INFO [GossipStage:1] 2011-03-16 00:54:31,590 StorageService.java (line 745) Nodes /10.214.103.224 and /10.240.50.63 have the same token 95704415696513900000000000000000000000.  /10.214.103.224 is the new owner
 INFO [GossipStage:1] 2011-03-16 17:26:51,083 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:27:24,767 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:29:30,191 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:31:35,609 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:33:39,440 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-23 17:22:55,520 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.240.50.63


 INFO [GossipStage:1] 2011-03-10 03:52:37,299 Gossiper.java (line 608) Node /10.240.50.63 is now part of the cluster
 INFO [GossipStage:1] 2011-03-10 03:52:37,545 Gossiper.java (line 600) InetAddress /10.240.50.63 is now UP
 INFO [HintedHandoff:1] 2011-03-10 03:53:36,168 HintedHandOffManager.java (line 304) Started hinted handoff for endpoint /10.240.50.63
 INFO [HintedHandoff:1] 2011-03-10 03:53:36,169 HintedHandOffManager.java (line 360) Finished hinted handoff of 0 rows to endpoint /10.240.50.63
 INFO [GossipStage:1] 2011-03-15 23:23:43,770 Gossiper.java (line 623) Node /10.240.50.63 has restarted, now UP again
 INFO [GossipStage:1] 2011-03-15 23:23:43,771 StorageService.java (line 726) Node /10.240.50.63 state jump to normal
 INFO [HintedHandoff:1] 2011-03-15 23:28:48,957 HintedHandOffManager.java (line 304) Started hinted handoff for endpoint /10.240.50.63
 INFO [HintedHandoff:1] 2011-03-15 23:28:48,958 HintedHandOffManager.java (line 360) Finished hinted handoff of 0 rows to endpoint /10.240.50.63
 INFO [ScheduledTasks:1] 2011-03-15 23:37:25,071 Gossiper.java (line 226) InetAddress /10.240.50.63 is now dead.
 INFO [GossipStage:1] 2011-03-16 00:54:31,590 StorageService.java (line 745) Nodes /10.214.103.224 and /10.240.50.63 have the same token 95704415696513900000000000000000000000.  /10.214.103.224 is the new owner
 WARN [GossipStage:1] 2011-03-16 00:54:31,590 TokenMetadata.java (line 115) Token 95704415696513900000000000000000000000 changing ownership from /10.240.50.63 to /10.214.103.224
 INFO [GossipStage:1] 2011-03-18 23:37:09,158 Gossiper.java (line 610) Node /10.240.50.63 is now part of the cluster
 INFO [GossipStage:1] 2011-03-21 23:37:10,421 Gossiper.java (line 610) Node /10.240.50.63 is now part of the cluster
 INFO [GossipStage:1] 2011-03-21 23:37:10,421 StorageService.java (line 726) Node /10.240.50.63 state jump to normal
 INFO [GossipStage:1] 2011-03-23 17:22:55,520 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.240.50.63
 INFO [ScheduledTasks:1] 2011-03-23 17:22:55,521 HintedHandOffManager.java (line 210) Deleting any stored hints for 10.240.50.63
",Large Amazon EC2 instances. Ubuntu 10.04.2 ,haruska,jborgstrom,jeromatron,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/11 22:43;brandon.williams;2371.txt;https://issues.apache.org/jira/secure/attachment/12474449/2371.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20586,,,Mon Apr 18 19:43:59 UTC 2011,,,,,,,,,,"0|i0gazr:",93211,,,,,Low,,,,,,,,,,,,,,,,,"23/Mar/11 22:43;brandon.williams;My guess is what is happening is that after aVeryLongTime when the state is evicted, another gossip round occurs and the state is repopulated.  Patch to re-quarantine on eviction to avoid this.;;;","23/Mar/11 23:30;acctech;This looks like it's changing the source code. Can we deploy this on a live cluster? ;;;","01/Apr/11 04:09;jbellis;Did you get a chance to try a patched build?;;;","01/Apr/11 16:48;acctech;Actually the issue was resolved when we did a restart of the cluster, and ran the nodetool removetoken command when only a portion of the nodes had started up. The main reason I didn't apply the patch yet, is merely because I need to learn how to patch the code and integrate the new build first... Thank you!;;;","07/Apr/11 00:05;brandon.williams;There is a second problem here.  We don't populate the gossiper's application state to LEFT for removetoken, only for decommission.  The problem with adding it, however, is that SS.handleStateLeft gets called every gossip round and runs through the hint removal process.  One option may be to check if the node is locally persisted and if not, just ignore the message since we never knew about it anyway.  Another is to just not remove hints when we see the LEFT state, because they'll expire anyway and large unaccessed rows aren't a problem, so this seems like a throwback from the <=0.6 days.;;;","10/Apr/11 09:03;jeromatron;I applied the patch and within a few days even with no restarting of any of the Cassandra nodes, the removed token came back. Just FYI.;;;","11/Apr/11 16:11;jbellis;bq. The problem with adding it, however, is that SS.handleStateLeft gets called every gossip round and runs through the hint removal process

But it only runs hint removal once per node, right?  Or is ""onChange"" not an accurate method name?

bq. One option may be to check if the node is locally persisted and if not, just ignore the message since we never knew about it anyway.

Locally persisted... with a token in SystemTable?  Ignore the ... hint message?

bq. Another is to just not remove hints when we see the LEFT state

We should issue a delete to the hints row, but we should not force a major compaction. The rule of thumb is, avoiding inflicting a performance hit on the cluster trumps immediate disk space cleanup.;;;","14/Apr/11 22:55;brandon.williams;It looks like the bad news here is that gossip makes many assumptions about a node being alive when it hears about it, and has no provisions for keeping removed node state.  This is bad, but highly exacerbated by keeping the removed node state for 3 days instead of 30s.  I think the best solution right now is to revert CASSANDRA-2115 until we can overhaul gossip to handle this.  The bug will still exist as it always has, but the window to trigger it is far shorter.;;;","15/Apr/11 00:50;jbellis;Sounds reasonable.;;;","18/Apr/11 18:59;brandon.williams;Reverted CASSANDRA-2115 and created CASSANDRA-2496 to fix this correctly.;;;","18/Apr/11 19:43;hudson;Integrated in Cassandra-0.7 #440 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/440/])
    Revert ""Keep endpoint state until aVeryLongTime when not a fat client""

This reverts commit db9164ffd96ebc7752fc5789c90c7211ba323ad2.

For CASSANDRA-2371.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unstable repo has disappeared from http://www.apache.org/dist/cassandra/debian/dists/,CASSANDRA-2370,12502220,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,brandon.williams,brandon.williams,23/Mar/11 19:39,16/Apr/19 09:33,14/Jul/23 05:52,28/Mar/11 15:14,,,,Packaging,,,2,,,,,The entire internet,diederik,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20585,,,Mon Mar 28 15:14:35 UTC 2011,,,,,,,,,,"0|i0gazj:",93210,,,,,Normal,,,,,,,,,,,,,,,,,"28/Mar/11 15:14;jeromatron;Asked in #asfinfra - sounds like something happened with the ownership of the dirs in there.  It's fixed now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup conversions between bytes and strings,CASSANDRA-2367,12502184,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,23/Mar/11 14:22,16/Apr/19 09:33,14/Jul/23 05:52,28/Mar/11 21:06,0.7.5,,,,,,0,,,,"There is a bit of inconsistency in our conversions between ByteBuffers and Strings.
For instance, ByteBufferUtil.string() uses as a default the java default charset, while ByteBufferUtil.bytes(String) assumes UTF8. Moreover, a number of places in the code don't use those functions and uses getBytes() directly. There again, we often encode with the default charset but decode in UTF8 or the contrary.

Using the default charset is probably a bad idea anyway, since this depends on the actual system the node is running on and could lead to a stupid bug when running in heterogeneous systems.

This ticket proposes to always assume UTF8 all over the place (and tries to use the ByteBufferUtil as much as possible to help with that).",,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"23/Mar/11 14:23;slebresne;0001-Cleanup-bytes-string-conversions.patch;https://issues.apache.org/jira/secure/attachment/12474397/0001-Cleanup-bytes-string-conversions.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20583,,,Mon Mar 28 21:06:22 UTC 2011,,,,,,,,,,"0|i0gayv:",93207,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"23/Mar/11 14:23;slebresne;Attaching patch against 0.7.;;;","23/Mar/11 17:11;jbellis;I see two places this fixes bugs:

- HintedHandOffManger: post-delivery hint deletion is now done w/ UTF8 encoding, which matches old and new encoding of ip-address-as-string.
- SystemTable now encodes cluster name as UTF8; before it encoded as system encoding, decoded as UTF8.

Is that accurate?;;;","23/Mar/11 17:27;slebresne;There is also:
  * the avro schema (DEFINITION_SCHEMA_COLUMN_NAME) for mutation. I was encoded in UTF8 (in Migration.java), but decoded using system encoding (in DefsTable.loadFromStorage(), since decoded by ByteBufferUtil.string() with default charset).
  * In HintedHandOffManager, the combined table and cfName is encoded as UTF8 but decoded with system encoding (once again through the use of BBUtil.string() with no specific charset.
;;;","23/Mar/11 18:13;jbellis;committed to 0.7 w/ some build fixes for contrib/ (so you will want to base port to trunk on r1084660);;;","23/Mar/11 18:34;hudson;Integrated in Cassandra-0.7 #401 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/401/])
    fix encoding bugs in HintedHandoffManager, SystemTable when default charset is not UTF8
patch by slebresne; reviewed by jbellis for CASSANDRA-2367
;;;","28/Mar/11 20:38;hudson;Integrated in Cassandra #812 (See [https://hudson.apache.org/hudson/job/Cassandra/812/])
    ;;;","28/Mar/11 21:06;jbellis;(merged to trunk by Sylvain);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ByteBufferUtil.read(byte[]) returns 0 when the end of the stream has been reached.,CASSANDRA-2365,12502073,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,julie.zhang10,julie.zhang10,22/Mar/11 18:23,16/Apr/19 09:33,14/Jul/23 05:52,23/Mar/11 02:29,0.7.5,,,,,,0,,,,"read(byte[], int, int) doesn't return -1 when the end of the stream is reached. Instead, it returns 0. 

len = Math.min(len, copy.remaining());
copy.get(bytes, off, len);

return len;

copy.remaining() returns 0 when the end of the stream is reached. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/11 18:32;jbellis;2365.txt;https://issues.apache.org/jira/secure/attachment/12474319/2365.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20582,,,Wed Mar 23 02:58:21 UTC 2011,,,,,,,,,,"0|i0gayf:",93205,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"22/Mar/11 18:32;jbellis;patch to return -1 if no more data;;;","22/Mar/11 18:56;slebresne;+1;;;","23/Mar/11 02:29;jbellis;committed;;;","23/Mar/11 02:58;hudson;Integrated in Cassandra-0.7 #399 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/399/])
    fix potential infinite loop in ByteBufferUtil.inputStream
patch by jbellis; reviewed by slebresne for CASSANDRA-2365
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
help in schema-sample uses wrong file name,CASSANDRA-2360,12501914,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,21/Mar/11 02:42,16/Apr/19 09:33,14/Jul/23 05:52,21/Mar/11 14:21,0.7.5,,,,,,0,,,,"As described in CASSANDRA-2007 

Wasn't sure about re-opening a resolved issue and wanted to make sure it was not lost. 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/11 02:43;amorton;0001-change-help-to-use-correct-file-name-conf-sample-sch.patch;https://issues.apache.org/jira/secure/attachment/12474145/0001-change-help-to-use-correct-file-name-conf-sample-sch.patch",,,,,,,,,,,,,,1.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20580,,,Mon Mar 21 14:21:01 UTC 2011,,,,,,,,,,"0|i0gaxb:",93200,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"21/Mar/11 02:42;amorton;Modifies the help to use the correct name for the sample schema file. ;;;","21/Mar/11 14:21;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI doesn't handle inserting negative integers,CASSANDRA-2358,12501895,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,thobbs,thobbs,20/Mar/11 06:36,16/Apr/19 09:33,14/Jul/23 05:52,31/Mar/11 18:41,0.7.5,,,Legacy/Tools,,,0,,,,"The CLI raises a syntax error when trying to insert negative integers:

{noformat}
[default@Keyspace1] set StandardInteger['key'][-12] = 'val';
Syntax error at position 28: mismatched character '1' expecting '-'
{noformat}",,,,,,,,,,,,,,,,,,,,,,,1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,"29/Mar/11 10:51;xedin;CASSANDRA-2358-trunk.patch;https://issues.apache.org/jira/secure/attachment/12474866/CASSANDRA-2358-trunk.patch","20/Mar/11 15:12;xedin;CASSANDRA-2358.patch;https://issues.apache.org/jira/secure/attachment/12474116/CASSANDRA-2358.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20578,,,Thu Mar 31 19:56:40 UTC 2011,,,,,,,,,,"0|i0gawv:",93198,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"20/Mar/11 14:37;xedin;I will wait until CASSANDRA-2341 is committed, it brings notation of the positive/negative integers into CLI grammar.;;;","20/Mar/11 14:42;jbellis;We should fix negative ints in 0.7 unless it's a huge pain.  (Counters will stay 0.8 only.);;;","20/Mar/11 14:43;xedin;Gotcha! No, this won't be a pain at all.;;;","23/Mar/11 18:56;jbellis;committed;;;","23/Mar/11 19:32;hudson;Integrated in Cassandra-0.7 #402 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/402/])
    allow negative numbers in the cli
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-2358
;;;","28/Mar/11 21:21;jbellis;merged to trunk but CliTest fails.  Can you fix?;;;","28/Mar/11 21:29;xedin;Can you remove it from trunk for now? I will create a separate version of this patch for the trunk.;;;","28/Mar/11 21:58;jbellis;removed;;;","28/Mar/11 22:01;xedin;Thanks! ;;;","29/Mar/11 10:51;xedin;branch: trunk (latest commit e6c5a28da940a086d0e786f1ad0288c0b0efa27d) ;;;","31/Mar/11 18:41;jbellis;committed trunk patch;;;","31/Mar/11 19:56;hudson;Integrated in Cassandra #822 (See [https://hudson.apache.org/hudson/job/Cassandra/822/])
    add negative number support to cli, trunk version
patch by Pavel Yaskevich for CASSANDRA-2358
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
zero-length strings should result in zero-length ByteBuffers,CASSANDRA-2352,12501733,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,17/Mar/11 20:55,16/Apr/19 09:33,14/Jul/23 05:52,17/Mar/11 21:38,0.8 beta 1,,,Legacy/CQL,,,0,cql,,,"The {{o.a.c.db.marshal.AbstractType.fromString()}} methods should return an empty {{ByteBuffer}} when passed a zero-length string, (empty bytes do {{validate()}} properly).",,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/11 20:56;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2352-AT.fromString-should-return-empty-BB-fo.txt;https://issues.apache.org/jira/secure/attachment/12473943/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2352-AT.fromString-should-return-empty-BB-fo.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20575,,,Wed Mar 23 01:28:10 UTC 2011,,,,,,,,,,"0|i0gavr:",93193,,gdusbabek,,gdusbabek,Normal,,,,,,,,,,,,,,,,,"17/Mar/11 21:23;gdusbabek;+1;;;","17/Mar/11 21:38;urandom;committed;;;","17/Mar/11 21:55;jbellis;we actually keep BBUtil.EMPTY_BYTE_BUFFER to avoid allocating lots of zero-length BBs, can you update post-commit?;;;","18/Mar/11 02:38;jbellis;went ahead and did this.;;;","18/Mar/11 04:13;urandom;Thanks Jonathan!;;;","18/Mar/11 07:20;stuhood;Does this mean empty strings will validate as column names?;;;","18/Mar/11 13:16;jbellis;We have separate checks in ThriftValidation for name.remaining() != 0.

I'm guessing this is to make it easier to do slicing, where empty start is valid?;;;","18/Mar/11 15:08;urandom;bq. Does this mean empty strings will validate as column names?

The {{AbstractType.validate()}} methods already allow this.  As Jonathan says, wherever that's not acceptable, {{ThriftValidation}} has tests to ensure name.remaining() != 0.;;;","23/Mar/11 01:28;hudson;Integrated in Cassandra #797 (See [https://hudson.apache.org/hudson/job/Cassandra/797/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null CF comments should be allowed,CASSANDRA-2351,12501730,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jhermes,gdusbabek,gdusbabek,17/Mar/11 20:29,16/Apr/19 09:33,14/Jul/23 05:52,17/Mar/11 21:14,0.8 beta 1,,,,,,0,,,,"Prior to 1906, cassandra tolerated null CF comments.  They were converted to empty quotes when the CFM was created.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/11 20:57;jhermes;2351.txt;https://issues.apache.org/jira/secure/attachment/12473944/2351.txt","17/Mar/11 20:31;gdusbabek;null_cf_comment_test_case.patch;https://issues.apache.org/jira/secure/attachment/12473942/null_cf_comment_test_case.patch",,,,,,,,,,,,,2.0,jhermes,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20574,,,Wed Mar 23 01:28:10 UTC 2011,,,,,,,,,,"0|i0gavj:",93192,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"17/Mar/11 20:31;gdusbabek;Attached a test case that demonstrates the regression.;;;","17/Mar/11 20:57;jhermes;Enforces comment never null, also includes this test for further regression-catching.;;;","17/Mar/11 21:14;gdusbabek;+1 committed.;;;","23/Mar/11 01:28;hudson;Integrated in Cassandra #797 (See [https://hudson.apache.org/hudson/job/Cassandra/797/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Races between schema changes and StorageService operations,CASSANDRA-2350,12501718,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,paladin8,paladin8,17/Mar/11 18:30,16/Apr/19 09:33,14/Jul/23 05:52,18/Mar/11 13:20,0.7.5,,,,,,0,,,,"I only tested this on 0.7.0, but it judging by the 0.7.3 code (latest I've looked at) the same thing should happen.

The case in particular that I ran into is this: I force a compaction for all CFs in a keyspace, and while the compaction is happening I add another CF to the keyspace. I get the following exception because the underlying set of CFs has changed while being iterated over.

{noformat}
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(Unknown Source)
        at java.util.HashMap$ValueIterator.next(Unknown Source)
        at java.util.Collections$UnmodifiableCollection$1.next(Unknown Source)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1140)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(Unknown Source)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor84.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
        at sun.rmi.transport.Transport$1.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source) 
{noformat}

The problem is a little more fundamental than that, though, as I believe any schema change of CFs in the keyspace during one of these operations (e.g. flush, compaction, etc) has the potential to cause a race. I'm not sure what would happen if the set of CFs to compact was acquired and one of them was dropped before it had been compacted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/11 04:15;jbellis;2350.txt;https://issues.apache.org/jira/secure/attachment/12473969/2350.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20573,,,Mon Mar 21 22:15:38 UTC 2011,,,,,,,,,,"0|i0gavb:",93191,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"18/Mar/11 04:15;jbellis;make Table.columnFamilyStores a ConcurrentHashMap (and make it private);;;","18/Mar/11 12:43;gdusbabek;+1;;;","18/Mar/11 13:20;jbellis;committed;;;","21/Mar/11 22:15;hudson;Integrated in Cassandra-0.7 #397 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/397/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expring columns can expire between the two phase of LazilyCompactedRow.,CASSANDRA-2349,12501688,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,slebresne,slebresne,17/Mar/11 14:37,16/Apr/19 09:33,14/Jul/23 05:52,18/Mar/11 04:31,0.7.5,,,,,,0,,,,"LazilyCompactedRow reads the columns to compact twice. First to create the index, bloom filter and calculate the data size, and then another phase to actually write the columns. But a column can expire between those two phase, which will result in a bad data size in the sstable (and a possibly corrupted row index).",,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"17/Mar/11 17:25;slebresne;0001-Introduce-expireBefore-and-keep-it-constant-all-thro.patch;https://issues.apache.org/jira/secure/attachment/12473924/0001-Introduce-expireBefore-and-keep-it-constant-all-thro.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20572,,,Mon Mar 21 22:15:38 UTC 2011,,,,,,,,,,"0|i0gav3:",93190,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"17/Mar/11 14:40;slebresne;This could be fixed by making LazilyCompactedRow do only one phase. We could write a place holder header, write the column, and seek back at the end to write the header. The tricky part is that this won't fit well with AbstractCompactedRow, since it is assume that the column count is known before write is called.;;;","17/Mar/11 15:07;slebresne;Actually, this won't work because we cannot anticipate the index size.;;;","17/Mar/11 15:56;slebresne;Attaching simpler solution consisting in not transforming expired columns into tombstones, so that the second phase of LazilyCompactedRow sees the same columns than the first phase.

I would be happy to come up with a unit test for this, but this is a subtle race condition and I'm really not sure how to write a test that capture it.;;;","17/Mar/11 16:11;jbellis;don't we need forceKeepExpired for both passes or we have the opposite problem?  If so, would prefer fKE a constructor arg for the iterator instead of setter.;;;","17/Mar/11 16:14;jbellis;Or: should we have an expireBefore parameter like gcBefore so it can stay constant during the compaction?;;;","17/Mar/11 16:27;slebresne;bq. don't we need forceKeepExpired for both passes or we have the opposite problem? If so, would prefer fKE a constructor arg for the iterator instead of setter.

There is no opposite problem, since you can't have a column expired for the first phase but not for the second one.

Note that the point of transforming expired columns to tombstone is to re-gain quickly the disk space used by the column value (without having to wait for gcGrace). So using fKE in phase 1 would prevent that for LazilyCompactedRow, which would be a pity.;;;","17/Mar/11 16:47;jbellis;bq. There is no opposite problem, since you can't have a column expired for the first phase but not for the second one.

But you can have an expired column counted as a tombstone for the first but not in the second since you have enabled forceKE.  No?

bq. the point of transforming expired columns to tombstone is to re-gain quickly the disk space used by the column value

Right, which is why using a constant expireBefore value that we pass to the serializer instead of computing it locally each time seems like a better solution. ;;;","17/Mar/11 16:58;slebresne;bq. But you can have an expired column counted as a tombstone for the first but not in the second since you have enabled forceKE. No?

You are right. Oups.

You are right, having an expireBefore is a better solution. I'll work out the patch.;;;","17/Mar/11 17:25;slebresne;Attached patch using the expireBefore idea.;;;","18/Mar/11 04:31;jbellis;committed;;;","21/Mar/11 22:15;hudson;Integrated in Cassandra-0.7 #397 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/397/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
index scan uses incorrect comparator on non-indexed expressions,CASSANDRA-2347,12501647,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,rjtg,jbellis,jbellis,17/Mar/11 03:35,16/Apr/19 09:33,14/Jul/23 05:52,18/Mar/11 02:55,0.7.5,,,,,,0,,,,"When multiple index expressions are specified, the column name comparator is used when evaluating secondary (non-indexed) expressions after an indexed expression match.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/11 03:51;jbellis;2347.txt;https://issues.apache.org/jira/secure/attachment/12473878/2347.txt",,,,,,,,,,,,,,1.0,rjtg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20571,,,Wed Mar 30 18:54:25 UTC 2011,,,,,,,,,,"0|i0gaun:",93188,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"17/Mar/11 03:51;jbellis;patch w/ failing test + fix;;;","18/Mar/11 02:55;jbellis;committed (original fix by Roland Gude, not sure if he has a Jira account);;;","18/Mar/11 04:06;hudson;Integrated in Cassandra-0.7 #391 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/391/])
    fix comparator used for non-indexed secondary expressions inindex scan
patch by Roland Gude and jbellis for CASSANDRA-2347
;;;","30/Mar/11 08:57;rjtg;i just created an account. just in case it is needed and maybe i can contribute more in the future.;;;","30/Mar/11 18:54;jbellis;Great!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows: CliTest broken because of /r/n,CASSANDRA-2337,12501536,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bcoverston,bcoverston,bcoverston,16/Mar/11 01:53,16/Apr/19 09:33,14/Jul/23 05:52,17/Mar/11 02:38,0.7.5,,,,,,0,windows,,,Somebody thought that windows should emulate a telex machine and we ended up with /r/n.,Windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/11 01:55;bcoverston;2337.patch;https://issues.apache.org/jira/secure/attachment/12473759/2337.patch",,,,,,,,,,,,,,1.0,bcoverston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20568,,,Fri Mar 18 04:06:55 UTC 2011,,,,,,,,,,"0|i0gasf:",93178,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"16/Mar/11 01:55;bcoverston;Fixes newline substituting environment variable.;;;","17/Mar/11 02:38;jbellis;fixed;;;","18/Mar/11 04:06;hudson;Integrated in Cassandra-0.7 #391 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/391/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up thread pool and queue sizes,CASSANDRA-2333,12501491,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,15/Mar/11 18:11,16/Apr/19 09:33,14/Jul/23 05:52,23/Mar/11 16:22,0.7.5,,,,,,0,,,,"Most of Cassandra assumes that ThreadPoolExecutor handles tasks by starting with Core threads, adding threads up to Max as tasks arrive, then queuing any additional.  This is not correct:

{noformat}
    If fewer than corePoolSize threads are running, the Executor always prefers adding a new thread rather than queuing.
    If corePoolSize or more threads are running, the Executor always prefers queuing a request rather than adding a new thread.
    If a request cannot be queued, a new thread is created unless this would exceed maximumPoolSize, in which case, the task will be rejected.
{noformat}

CASSANDRA-2178 fixed this in one place but made it worse by default since most people run with a single data dir, meaning as soon as you have multiple CFs flushing (or a single one with indexes) then you will start blocking writes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/11 18:22;jbellis;2333.txt;https://issues.apache.org/jira/secure/attachment/12473712/2333.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20566,,,Tue Mar 15 20:31:32 UTC 2011,,,,,,,,,,"0|i0garj:",93174,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"15/Mar/11 18:22;jbellis;Patch to remove use of maximumpoolsize in favor of core size + timing out core threads.  Also adds memtable_flush_queue_size configuration directive, defaulting to 4.;;;","15/Mar/11 19:04;brandon.williams;+1, increased flush queue size is enough to prevent blocking writes at all with stress and an index.;;;","15/Mar/11 20:31;hudson;Integrated in Cassandra-0.7 #386 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/386/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index predicate values used in get_indexed_slice() are not validated,CASSANDRA-2328,12501429,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,amorton,amorton,amorton,15/Mar/11 06:53,16/Apr/19 09:33,14/Jul/23 05:52,17/Mar/11 02:55,0.7.5,,,,,,0,,,,"If a client makes a get_indexed_slice() request with malformed predicate values we get an assertion failing rather than InvalidRequestException.

{noformat}
ERROR 14:47:56,842 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: 6
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVer
bHandler.java:51)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.
java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IndexOutOfBoundsException: 6
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:121)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compareTimestampBytes(Ti
meUUIDType.java:56)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.jav
a:45)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.jav
a:29)
        at org.apache.cassandra.db.ColumnFamilyStore.satisfies(ColumnFamilyStore
.java:1608)
        at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java
:1552)
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVer
bHandler.java:42)
        ... 4 more
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/11 00:12;amorton;0001-validate-index-predicate-name-and-value.patch;https://issues.apache.org/jira/secure/attachment/12473753/0001-validate-index-predicate-name-and-value.patch",,,,,,,,,,,,,,1.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20565,,,Fri Mar 18 04:06:55 UTC 2011,,,,,,,,,,"0|i0gaqf:",93169,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Mar/11 00:12;amorton;Attached patch validates the expression column name and value for get_indexed_slice(). 

Also adds a regression test in the (thrift) system tests.;;;","17/Mar/11 02:55;jbellis;committed, thanks!;;;","18/Mar/11 04:06;hudson;Integrated in Cassandra-0.7 #391 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/391/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java indexed range slicing is broken,CASSANDRA-2326,12501404,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,14/Mar/11 21:26,16/Apr/19 09:33,14/Jul/23 05:52,13/Apr/11 20:37,0.7.5,0.8 beta 1,,,,,0,,,,"I probably broke it when I fixed the build that CASSANDRA-2312 broke.  Now it compiles, but never works.",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/11 18:23;xedin;CASSANDRA-2326-trunk.patch;https://issues.apache.org/jira/secure/attachment/12476147/CASSANDRA-2326-trunk.patch","12/Apr/11 18:23;xedin;CASSANDRA-2326.patch;https://issues.apache.org/jira/secure/attachment/12476146/CASSANDRA-2326.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20563,,,Tue Apr 19 00:04:18 UTC 2011,,,,,,,,,,"0|i0gapz:",93167,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"14/Mar/11 22:11;jbellis;do you get back incomplete data or no data at all?;;;","14/Mar/11 22:15;brandon.williams;{noformat}
contrib/stress/bin/stress -n 300000 -d cassandra-1,cassandra-2,cassandra-3 -i 1 -t 300 -x KEYS -l2 -o INDEXED_RANGE_SLICE
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
Operation [77] retried 10 times - error on calling get_indexed_slices for offset 0 
{noformat}

Not much further with keep-going:

{noformat}
contrib/stress/bin/stress -n 300000 -d cassandra-1,cassandra-2,cassandra-3 -i 1 -t 300 -x KEYS -l2 -o INDEXED_RANGE_SLICE -k
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
Operation [99] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [178] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [173] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [54] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [190] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [35] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [9] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [104] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [139] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
Operation [195] retried 1 times - error on calling get_indexed_slices for offset 0 

Index: 0, Size: 0
{noformat};;;","14/Mar/11 22:21;jbellis;looks like stress.java could stand to print out InvalidRequestException.why;;;","27/Mar/11 21:29;xedin;It does print an exception message when it's not null. 

It seems to be imposible to currently implement IndexedRangeSlice operation because Operation.generateValues() now generates strings using randomizer (IndexedRangeSlice implies that we know an exact value we are looking keys for) so each of the runs can potentially give 0 results and we can loop infinitely...;;;","11/Apr/11 10:45;xedin;We can maybe offer users to provide list of the values for indexes range slices, don't have any other solution right now...;;;","12/Apr/11 15:49;jbellis;a --values [list of values] option sounds like a good solution to me.  doesn't have to be specific to index queries, but it's most useful there obviously.;;;","12/Apr/11 15:52;xedin;I agree, I was also thinking about flag which will use old values generator instead of random as I possible solution.;;;","12/Apr/11 16:07;brandon.williams;I like the idea of adding a flag for the old behavior, that Just Worked and didn't require more command line mess.  I'd even be onboard with making it the default and having random as an option, since random hasn't bought us much, if anything, yet.;;;","12/Apr/11 16:09;xedin;I agree with Brandon on this, what do you think Jonathan?;;;","12/Apr/11 16:12;jbellis;sgtm;;;","12/Apr/11 18:23;xedin;-V to generate randomized average size values, old behaviour by default;;;","13/Apr/11 20:37;brandon.williams;Committed;;;","19/Apr/11 00:04;jbellis;(rebased + committed trunk version);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair transfers more data than necessary,CASSANDRA-2324,12501390,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,14/Mar/11 19:43,16/Apr/19 09:33,14/Jul/23 05:52,10/Apr/11 18:16,0.8 beta 1,,,,,,2,,,,"To repro: 3 node cluster, stress.java 1M rows with -x KEYS and -l 2.  The index is enough to make some mutations drop (about 20-30k total in my tests).  Repair afterwards will repair a large amount of ranges the first time.  However, each subsequent run will repair the same set of small ranges every time.  INDEXED_RANGE_SLICE in stress never fully works.  Counting rows with sstablekeys shows there are 2M rows total as expected, however when trying to count the indexed keys, I get exceptions like:
{noformat}
Exception in thread ""main"" java.io.IOException: Key out of order! DecoratedKey(101571366040797913119296586470838356016, 0707ab782c5b5029d28a5e6d508ef72f0222528b5e28da3b7787492679dc51b96f868e0746073e54bc173be927049d0f51e25a6a95b3268213b8969abf40cea7d7) > DecoratedKey(12639574763031545147067490818595764132, 0bc414be3093348a2ad389ed28f18f0cc9a044b2e98587848a0d289dae13ed0ad479c74654900eeffc6236)
        at org.apache.cassandra.tools.SSTableExport.enumeratekeys(SSTableExport.java:206)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:388)
{noformat}",,dkuebric,jborgstrom,jeromatron,skamio,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2316,,,,,,,,"08/Apr/11 01:29;slebresne;0001-Make-repair-operate-over-a-node-token-range-v2.patch;https://issues.apache.org/jira/secure/attachment/12475769/0001-Make-repair-operate-over-a-node-token-range-v2.patch","01/Apr/11 15:00;slebresne;0001-Make-repair-operate-over-a-node-token-range.patch;https://issues.apache.org/jira/secure/attachment/12475228/0001-Make-repair-operate-over-a-node-token-range.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20562,,,Sun Apr 10 18:37:30 UTC 2011,,,,,,,,,,"0|i0gapj:",93165,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"14/Mar/11 19:44;jbellis;Key out of order is because sstableexport doesn't know that index sstables use LocalPartitioner instead of the cluster partitioner RP or BOPP.;;;","14/Mar/11 21:24;brandon.williams;It looks like INDEXED_RANGE_SLICE is broken in stress.java, so the only problem here is repair doing superfluous work.;;;","28/Mar/11 12:57;slebresne;The problem is, the ranges repair hashes are not actual node ranges.

Let's consider the following ring (RF=2), where I consider token being in [0..12] to simplify, and where everything is consistent:
{noformat}
                  _.-""""""""-._
 C (token: 11)  .'          `.
 [11,3][3,7]   /              \
              |                |
              |                | A(token: 3)
              |                | [3,7],[7,11]
               \              /
                `._        _.'
       B (token: 7)`-....-'
       [7,11],[11,3]
{noformat}
Now say I run a repair on node A. The problem is that the Merkle tree ranges are built by dividing the full range by 2 recursively. This means that in this example, the ranges in the tree will for instance be [0,2], [2, 4], [4, 6], [6, 8], [8,10] and [10,12].

If you compare the hashes for A and B on those ranges, changes are you'll find mismatches for [6,8] and [10,12] (because A don't have anyone on [11, 12] while B have, and B don't have anyone on [6, 7] while A have). As a consequence, the range [7,8] and [10,11] will be repaired, even though there is no inconsistencies.

What that means in practice is that it will be very rare for anti-antropy to actually consider the nodes in sync, it will almost surely ""repair"" something, even if the nodes are perfectly consistent. It's Very easy to check btw: with a cluster right the one above (3 nodes, RF=2), with as few as 5 keys for the whole cluster I'm able to have a repair do repairs over and over again.

Now the good question is: how bad is it ? I'm not sure, I depends a bit.

On a 3 nodes cluster (RF=2), I tried inserting 1M keys with stress (stress -l 2) and triggered repair afterwards. The amount of (unnecessarily) repaired keys was around 150 keys for a given node (it varies slightly for run to run because there is some randomness in the creation of the Merkle tree), corresponding to ~44KB streamed (that is the amount transfered to the node where repair has been ran, so for the total operation its twice this, since we stream in both ways). That's ~0.02% of keys (a given node have ~666 666 keys).  It's bad to do useless work, but not a really big deal.

However, the less keys we'll have, the worst it gets (and the bigger our rows are, the more useless transfer we do). With the same experiment inserting only 10K keys, there is 190 keys uselessly repaired. That's now close to 3% of the load. It also gets worst with increasing replication factor.


To fix this, we would need for the range in the Merkle tree to ""share"" the node range boundaries. An interesting way to do this would be to have the coordinating node give a list a range for which to calculate Merkle trees, and the node would compute one tree by range (for the coordinating node, that would be #RF's tree). A nice think with this is that it would leave room to optimizing repair since a node would need to do a validation compaction only on the range asked for, which means that only the coordinator node would validate all its data. The neighbors would do less work.
;;;","28/Mar/11 13:44;jbellis;bq. To fix this, we would need for the range in the Merkle tree to ""share"" the node range boundaries

couldn't we just take the interesection of the computed ranges w/ the range actually being repaired? ;;;","28/Mar/11 13:58;slebresne;bq. couldn't we just take the interesection of the computed ranges w/ the range actually being repaired?

We do that. But the problem is: you're node A and you receive a merkle tree from B that in particular says that for the range [0..10] the hash is x. And on [0..10] your has is x'. The problem is when [0..10] is partly one of your range, partly not. For instance it can be that you're a replica for [8..10] but not at all for [0..8].
This is due to the fact that the ranges for which the hashes are computed are computed without concern for actual node ranges. So now you know there is some inconsistency on [0..10] but it may just be that B is responsible for [0..8] and have data for it (and we don't since we are not in charge of that).
In that case, the code do take the intersection of [0..10] with the local range and will stream only [8..10]. But it's still useless.;;;","28/Mar/11 14:14;jbellis;I thought repair is per-token-range, i.e., if I say ""nodetool repair A"" then range (11, 3] and (3, 7] will be repaired independently.;;;","28/Mar/11 14:34;slebresne;No, not if I read this code correctly (but I think it should, that's roughly what I'm proposing to do).

Actually thinking about it, there is probably no need to construct multiple merkle trees, it will be enough for neighbors to only add to the tree the keys that are in the range of the node asking for the tree.;;;","28/Mar/11 14:47;jbellis;So what about this:

- change the atom of repair (in nodetool + StorageService) to be a single token range, so it's unambiguous what we're repairing.  This has the side benefit of making it enormously easier to repair an entire cluster w/o doing redundant work.
- provide backwards compatibility w/ existing repair command by splitting it into RF repair ranges and waiting on each of those futures in StorageService mbean
;;;","28/Mar/11 15:00;slebresne;Sounds good, will do.;;;","01/Apr/11 15:00;slebresne;Attached patch modify repair to operate on one token range at a time. Nodetool repair schedule as many repair session than the node have ranges to perform a full node repair. Note that this is more efficient than previously, since the neighbors of the node will only do a validation compaction on the range they have in common with the node coordinating the repair (instead of validating everything).

This moreover makes it trivial to add an option to nodetool so that the node only repair it's primary range. That way, you can repair a full cluster by calling this operation on every node and there is no duplication of work. The patch doesn't add this option yet though.

The patch is against trunk. Because the way we construct the merkleTree is fundamentally different, the trees created by 0.7 cannot be compared to the ones created with this patch. The strategy this patch adopts with respect to talking to 0.7 nodes is this:
  * If a 0.7 node asks for a merkleTree, since we are still able to do a full compaction validation, we do it and answer with that.
  * Since a 0.7 node cannot do a merkleTree that would be ok for us, we simply exclude 0.7 nodes from the endpoints we ask merkleTree from.

I don't feel this is a trivially enough patch to go to the 0.7 branch.;;;","01/Apr/11 22:24;stuhood;This change definitely makes sense: thanks for tackling it. The original implementation was intended to take advantage of naturally occurring compactions: I would still like to get in a position where that is possible, but living with the existing implementation until then isn't worth it.

From a quick skim: forceTableRepair incorrectly reports that the session has failed if the client thread dies: the repair will continue in the background (or used to).;;;","01/Apr/11 23:03;stuhood;I'll give this a more complete review over the weekend.;;;","07/Apr/11 08:22;stuhood;* SSTableBoundScanner might be much simpler if it iterates within a list of file offsets, as returned by SSTableReader.getPositionsForRanges
* SSTableReader.getKeySamples could perform two binary searches for min and max rather than doing sequential comparisons to the keys

Thanks again Sylvain: this is great!;;;","08/Apr/11 01:29;slebresne;bq. SSTableBoundScanner might be much simpler if it iterates within a list of file offsets, as returned by SSTableReader.getPositionsForRanges

Good call, that's much simpler. Thanks.

bq. SSTableReader.getKeySamples could perform two binary searches for min and max rather than doing sequential comparisons to the keys

Yeah, realized getKeySamples was buggy anyway since it wasn't handling wrapping ranges correctly.

Attaching patch that simplify the bounded scanner and fixes getKeySamples.
;;;","08/Apr/11 04:45;stuhood;+1
Thanks!;;;","10/Apr/11 18:16;slebresne;Committed as r1090840. Thanks.;;;","10/Apr/11 18:37;hudson;Integrated in Cassandra #844 (See [https://hudson.apache.org/hudson/job/Cassandra/844/])
    Make repair work on a token range instead of the full ring
patch by slebresne; reviewed by stuhood for CASSANDRA-2324
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java should not allow arbitrary arguments,CASSANDRA-2323,12501378,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,14/Mar/11 17:01,16/Apr/19 09:33,14/Jul/23 05:52,30/Mar/11 16:56,0.7.5,,,,,,0,,,,"This doesn't seem like a big deal, until you accidentally insert a space between a dash and it's flag, and it's at the point where the line wraps in your terminal.",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/11 16:28;xedin;CASSANDRA-2323.patch;https://issues.apache.org/jira/secure/attachment/12474787/CASSANDRA-2323.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20561,,,Wed Mar 30 18:02:25 UTC 2011,,,,,,,,,,"0|i0gapb:",93164,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"30/Mar/11 16:56;brandon.williams;Committed;;;","30/Mar/11 18:02;hudson;Integrated in Cassandra-0.7 #414 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/414/])
    stress.java rejects arbitrary arguments.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2323
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disallow to querying a counter CF with non-counter operation,CASSANDRA-2321,12501289,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,mubarak.seyed,mubarak.seyed,13/Mar/11 05:27,16/Apr/19 09:33,14/Jul/23 05:52,29/Mar/11 20:39,0.8 beta 1,,,,,,0,,,,"CounterColumnType.getString() returns hexString.

{code}
public String getString(ByteBuffer bytes)
{ 
       return ByteBufferUtil.bytesToHex(bytes);
}
{code}
and python stress.py reader returns


[ColumnOrSuperColumn(column=None, super_column=SuperColumn(name='19', columns=[Column(timestamp=1299984960277, name='56', value='\x7f\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00,', ttl=None), Column(timestamp=1299985019923, name='57', value='\x7f\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00;\x00\x00\x00\x00\x00\x00\x08\xfd', ttl=None))]",Linux,cburroughs,johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/11 12:58;slebresne;0001-Don-t-allow-normal-query-on-counter-CF.patch;https://issues.apache.org/jira/secure/attachment/12474612/0001-Don-t-allow-normal-query-on-counter-CF.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20560,,,Tue Mar 29 23:53:54 UTC 2011,,,,,,,,,,"0|i0gaov:",93162,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"15/Mar/11 16:08;slebresne;Actually, having getString() return hexString is the right thing to do, because internally counters are bytes and getString is used by sstable2json in particular.

The problem however is that it is not disallowed to query a counter CF with non-counter operation. Attaching a patch to correct this. This is a bigger patch that one would hope because ThriftValidation doesn't help. So the patch does a bunch of refactoring to allow what it must do. As a side node, the refactoring makes it more efficient (We don't revalidate the column family for each mutation of a batch_mutate).

Btw, stress.py has no support for counters, but stress.java has it. ;;;","25/Mar/11 12:58;slebresne;Rebased patch attached;;;","25/Mar/11 17:37;jbellis;I'm not sure this refactor is much of an improvement.  Seems like the code moves around a lot but volume and complexity are not really reduced.

Can't we just add a ""validateCounterCF"" to counter calls and ""validateNonCounterCF"" otherwise?

(I'd prefer to say ""counter/noncounter"" vs ""noncommutative/commutative"" but if you really prefer the other that's okay too.);;;","25/Mar/11 19:00;slebresne;bq. I'm not sure this refactor is much of an improvement. Seems like the code moves around a lot but volume and complexity are not really reduced.

I do believe the refactor is an improvement (granted, maybe not a huge one). During validation we do a bunch of queries to DatabaseDescriptor to check the cf exists, then to get its type (super or standard), then to get its value validator, etc... Granted those are just hashMaps gets, but they are just unnecessary. I do think that the method used by the refactor, that is getting the metadata once and giving it to all other method is cleaner. There also the fact that we revalidate the column family for each Mutation of a batch_mutate, even if all of them are on the same cf, which this refactor fixes too.
I don't care so much about those and won't fight over it, but I think it would be a pity to leave ThriftValidation in that state (and the refactoring is really trivial). Note that I'll be perfectly ok with moving the refactor in another ticket if that makes it better.

bq. Can't we just add a ""validateCounterCF"" to counter calls and ""validateNonCounterCF"" otherwise?

Yes we could, up to the fact that the refactor would still make sense I think for the reason above :)

bq. (I'd prefer to say ""counter/noncounter"" vs ""noncommutative/commutative"" but if you really prefer the other that's okay too.)

When I have a few free cycles I plan to remove the 'commutative' wording completely as I think this was premature generalization and make the code less readable right now. So I'm on your side, I just went for consistency with the rest of the code for now.;;;","29/Mar/11 20:39;jbellis;committed;;;","29/Mar/11 23:53;hudson;Integrated in Cassandra #817 (See [https://hudson.apache.org/hudson/job/Cassandra/817/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dropping an index leaves index in Built state in system table,CASSANDRA-2320,12501288,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,13/Mar/11 04:43,16/Apr/19 09:33,14/Jul/23 05:52,14/Mar/11 19:58,0.7.5,,,,,,0,,,,,,mdennis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/11 04:58;jbellis;2320.txt;https://issues.apache.org/jira/secure/attachment/12473490/2320.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20559,,,Mon Mar 14 20:16:20 UTC 2011,,,,,,,,,,"0|i0gaon:",93161,,mdennis,,mdennis,Low,,,,,,,,,,,,,,,,,"13/Mar/11 04:58;jbellis;setIndexRemoved was called with the wrong columnfamily name, so the old index name remained in the built state.

this is harmless unless you want to recreate the index afterwards, and then it is fatal (because addIndex says ""never mind, it's already built).  Workaround is to give a different index name when re-creating.

patch attached with failing test + fix.;;;","14/Mar/11 19:53;mdennis;I'd rather see the index build scheduled at the end of addIndex scheduled via an executor of some sort instead of creating a new thread each time, but otherwise patch looks good.
;;;","14/Mar/11 19:58;jbellis;committed;;;","14/Mar/11 20:16;hudson;Integrated in Cassandra-0.7 #379 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/379/])
    clear Built flag in system table when dropping an index
patch by jbellis; reviewed by mdennis for CASSANDRA-2320
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column family deletion time is not always reseted after gc_grace,CASSANDRA-2317,12501241,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,12/Mar/11 11:19,16/Apr/19 09:33,14/Jul/23 05:52,04/Jul/11 14:36,1.0.0,,,,,,0,,,,"Follow up of CASSANDRA-2305.
Reproducible (thanks to Jeffrey Wang) by: 

Create a CF with gc_grace_seconds = 0 and no row cache.
Insert row X, col A with timestamp 0.
Insert row X, col B with timestamp 2.
Remove row X with timestamp 1 (expect col A to disappear, col B to stay).
Wait 1 second.
Force flush and compaction.
Insert row X, col A with timestamp 0.
Read row X, col A (see nothing).",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"14/Mar/11 13:47;slebresne;0001-Add-AbstractColumnContainer-to-factor-common-parts-o.patch;https://issues.apache.org/jira/secure/attachment/12473561/0001-Add-AbstractColumnContainer-to-factor-common-parts-o.patch","14/Mar/11 13:47;slebresne;0002-Add-unit-test.patch;https://issues.apache.org/jira/secure/attachment/12473562/0002-Add-unit-test.patch","14/Mar/11 13:47;slebresne;0003-Reset-CF-and-SC-deletion-time-after-compaction.patch;https://issues.apache.org/jira/secure/attachment/12473563/0003-Reset-CF-and-SC-deletion-time-after-compaction.patch",,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20558,,,Mon Jul 04 15:20:32 UTC 2011,,,,,,,,,,"0|i0ganz:",93158,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"12/Mar/11 11:20;slebresne;Adding patch against 0.7.

First patch is a unit test to reproduce the failure, patch 2 is the fix.;;;","12/Mar/11 11:54;slebresne;I realize this fix don't take the cache into account. I'll attach an updated patch asap.;;;","12/Mar/11 13:12;jbellis;How do you have CF objects around at all post-purge?;;;","14/Mar/11 13:47;slebresne;bq. How do you have CF objects around at all post-purge?

The problem is actually with cf objects that don't get fully purged. Those still retain their markedForDeleteAt and localDeletionTime after compaction even though it could be way past gc_grace. That is, a deletion can easily live way past gc_grace + compaction.

As it turns out, super columns also suffers for the same problem.

Because it felt a bit annoying to have to fix the problem in 2 places, and because that's not the first time that happens, the first attached is a refactoring one, that introduces an AbstractColumnContainer class that factor common code to ColumnFamily and SuperColumn.

The second patch introduces unit tests for column family and super columns and third patch is the fix. It introduces a new structure to hold both markedDeletedAt and localDeletionTime so that we are able to set both of those together atomically. This is necessary for the second part of CASSANDRA-2305.  I think that anyhow it was not fully correct to update them non atomically.

Note that the third patch depends on the patch for CASSANDRA-2279.

All patches are against 0.7.
;;;","14/Mar/11 14:27;jbellis;Oh, so the problem is that we're not actually losing information (when a CF contains non-deleted data) that we may lose otherwise?

Is that really worth adding a bunch of complexity to ""fix?"";;;","14/Mar/11 15:01;slebresne;This is clearly not of big importance.

I however think that we should fix this for coherence sake. Tombstones have side effects on client queries and tombstone collection too (it's not just an internal detail). As such, we should make the behavior as coherent as possible. That a deletion always has effect until the first compaction after gc_grace would be more coherent that the current status quo. The fact that Jeffrey was surprised and that we assume right away that it was a bug proves that, I think.

And the fix is actually fairly simple, even though my patch doesn't do it justice. The first patch is really just a refactoring that I think should have been done a long time ago. I'm happy to create a ticket for this specifically if we prefer, but I just think it is stupid to not factor that code.

The second part of the patch is to put markedDeletedAt and localDeletionTime in a common structure that we CAS for changes. Again, this is because right now I think we have a race condition when updating those two values. That is, we could end up with the markedDeleteAt of a given operation but the localDeletionTime of another one. This could also be put on another ticket (I just tend to be lazy so I've put everything here, sorry).

Once those are done, the fix for this specific ticket is really just the maybeResetDeletionTime() function.;;;","13/Jun/11 19:16;jbellis;doesn't this mean that for a CF w/ no tombstone, we create a new deletioninfo every call to maybeReset?

bq. if (current.localDeletionTime > gcBefore || deletionInfo.compareAndSet(current, new DeletionInfo()))

otherwise, +1 for trunk.;;;","22/Jun/11 22:33;paladin8;Does anyone know whether this is fixed in 0.8? We are thinking of upgrading soon, but I don't want to try to apply the 0.7 patch to 0.8...;;;","22/Jun/11 23:19;jbellis;""Unresolved"" means not fixed anywhere yet.;;;","04/Jul/11 14:37;slebresne;Committed to trunk (as I agree this should really go there).

bq. doesn't this mean that for a CF w/ no tombstone, we create a new deletioninfo every call to maybeReset?

You're right, I've included a current.localDeletionTime == Integer.MIN_VALUE in the condition to escape early in that case.;;;","04/Jul/11 15:20;hudson;Integrated in Cassandra #948 (See [https://builds.apache.org/job/Cassandra/948/])
    Reset CF and SC deletion time after gc_grace
patch by slebresne; reviewed by jbellis for CASSANDRA-2317

slebresne : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1142690
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamily.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/filter/QueryFilter.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/RowMutation.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/SuperColumn.java
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/test/unit/org/apache/cassandra/service/RowResolverTest.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/RowTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/IColumnContainer.java
* /cassandra/trunk/test/unit/org/apache/cassandra/db/compaction/CompactionsPurgeTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/AbstractColumnContainer.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilySerializer.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchElement exception on node which is streaming a repair,CASSANDRA-2316,12501231,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,alienth,alienth,12/Mar/11 07:12,16/Apr/19 09:33,14/Jul/23 05:52,18/Apr/11 23:04,0.7.5,,,,,,0,repair,,,"Running latest SVN snapshot of 0.7.

When I ran a repair on a node, that node's neighbor threw the following exception. Let me know what other info could be helpful.

{code}
 INFO 23:43:44,358 Streaming to /10.251.166.15
ERROR 23:50:21,321 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.util.NoSuchElementException
        at com.google.common.collect.AbstractIterator.next(AbstractIterator.java:146)
        at org.apache.cassandra.service.AntiEntropyService$Validator.add(AntiEntropyService.java:366)
        at org.apache.cassandra.db.CompactionManager.doValidationCompaction(CompactionManager.java:825)
        at org.apache.cassandra.db.CompactionManager.access$800(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$6.call(CompactionManager.java:358)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2324,,,,,,"18/Mar/11 19:55;jbellis;2316-assert.txt;https://issues.apache.org/jira/secure/attachment/12474026/2316-assert.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20557,,,Tue Apr 19 14:04:20 UTC 2011,,,,,,,,,,"0|i0ganr:",93157,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"14/Mar/11 16:45;jbellis;The loop in validator.add apparently assumes that _some_ range will contain any given row. But

- ranges is supposed to be ""invalid"" ranges not all ranges
- comments in validator.add say it is called for each row in the CF
- so any rows that are not part of an ""invalid"" range will cause this exception

So either my superficial understanding of what ""invalid"" ranges are is broken, or the comments are wrong, or I'm surprised we're not hitting this a lot more frequently.;;;","14/Mar/11 16:47;jbellis;Looks like this dates back to 0.6.;;;","17/Mar/11 19:36;stuhood;""Invalid"" ranges in the tree are ranges that need to be hashed. The idea was that the tree could be persisted between repair sessions, and ranges would be invalidated as writes arrived: then the validation compaction would only need to compact invalid ranges of the tree.

In the current implementation, the tree will only contain invalid ranges, since it is being created from scratch for every repair.;;;","17/Mar/11 19:39;stuhood;I wonder if this is a keys-out-of-order problem?;;;","18/Mar/11 02:39;jbellis;Since we iterate over each key in the CF, order shouldn't actually matter should it?;;;","18/Mar/11 08:04;stuhood;Order matters, because there will be up to 2^16 invalid ranges. If keys arrive out of order we will consume ranges that should have contained keys, possibly leading us to consume all invalid ranges.

Either way, an assert that keys are arriving in order would be handy here.;;;","18/Mar/11 19:55;jbellis;proposed assert attached;;;","18/Apr/11 22:12;stuhood;+1 For the assert.;;;","19/Apr/11 14:04;hudson;Integrated in Cassandra-0.7 #447 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/447/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommutativeRowIndexer always read full row in memory,CASSANDRA-2313,12501164,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,11/Mar/11 16:37,16/Apr/19 09:33,14/Jul/23 05:52,07/Apr/11 20:32,0.8 beta 1,,,,,,1,,,,"CommutativeRowIndexer use CFSerializer.deserializeColumns() that read the full row in memory. We should use PreCompactedRow/LazilyCompactedRow instead to avoid this on huge row.

As an added benefit, using PreCompactedRow will avoid a current seek back to write the row size.",,alanliang,cburroughs,johanoskarsson,stuhood,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,"05/Apr/11 18:01;slebresne;0001-Introduce-CompactionController-to-handler-compaction-v2.patch;https://issues.apache.org/jira/secure/attachment/12475515/0001-Introduce-CompactionController-to-handler-compaction-v2.patch","16/Mar/11 10:48;slebresne;0001-Introduce-CompactionController-to-handler-compaction.patch;https://issues.apache.org/jira/secure/attachment/12473776/0001-Introduce-CompactionController-to-handler-compaction.patch","05/Apr/11 18:01;slebresne;0002-Make-CommutativeRowIndexer-uses-AbstractCompactionRo-v2.patch;https://issues.apache.org/jira/secure/attachment/12475516/0002-Make-CommutativeRowIndexer-uses-AbstractCompactionRo-v2.patch","16/Mar/11 10:48;slebresne;0002-Make-CommutativeRowIndexer-uses-AbstractCompactionRo.patch;https://issues.apache.org/jira/secure/attachment/12473777/0002-Make-CommutativeRowIndexer-uses-AbstractCompactionRo.patch",,,,,,,,,,,4.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20555,,,Thu Apr 07 21:50:06 UTC 2011,,,,,,,,,,"0|i0gan3:",93154,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"16/Mar/11 10:48;slebresne;Attaching patch against trunk.

This turns out to be slightly harder than expected because (PreCompacted|LazilyCompacted)Row and SSTableIdentityIterator were relying on a SSTableReader, but while rebuilding the index, we don't have one yet (and faking one would probably be fragile).

Instead, a first patch introduces CompactionController, that is used to manage the compaction options. This remove parts of the dependency mentioned above.  I also think that it cleans code and slightly optimize it in that it avoid recreating a HashSet of the sstables for each given row.

The second patch modify CommutativeRowIndexer to use (PreCompacted|LazilyCompacted)Row, which actually greatly simply the code there.
;;;","05/Apr/11 18:01;slebresne;Attaching rebased patch set.;;;","07/Apr/11 06:06;stuhood;+1
This looks good, thanks Sylvain.;;;","07/Apr/11 20:32;slebresne;Committed as r1089993;;;","07/Apr/11 21:50;hudson;Integrated in Cassandra #835 (See [https://hudson.apache.org/hudson/job/Cassandra/835/])
    Use {Lazy|Pre}CompactedRow for CommutativeRowIndexer
patch by slebresne; reviewed by stuhood for CASSANDRA-2313
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stress.java columns are bigger than advertised,CASSANDRA-2312,12501067,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,10/Mar/11 21:33,16/Apr/19 09:33,14/Jul/23 05:52,11/Mar/11 18:24,0.7.4,,,Legacy/Tools,,,0,,,,"Converting from bytes to hex makes the columns 4x larger than they should be.  (2x for conversion to hex, then another 2x for converting to UTF-16 which is the default String encoding.)
",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"10/Mar/11 21:35;jbellis;2312.txt;https://issues.apache.org/jira/secure/attachment/12473333/2312.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20554,,,Fri Mar 11 23:07:02 UTC 2011,,,,,,,,,,"0|i0gamv:",93153,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"10/Mar/11 21:35;jbellis;Patch to leave column values as raw bytes. Also adds comparator metadata for column names.;;;","11/Mar/11 18:09;xedin;+1;;;","11/Mar/11 18:24;jbellis;committed;;;","11/Mar/11 23:07;hudson;Integrated in Cassandra-0.7 #375 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/375/])
    fix stress.java column sizes
patch by jbellis; reviewed by Pavel Yaskevich for CASSANDRA-2312
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CassandraStorage for pig checks for environment variable on mappers/reducers, but it should only need to be set on the machine launching pig.",CASSANDRA-2310,12501053,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,eldondev,eldondev,10/Mar/11 19:14,16/Apr/19 09:33,14/Jul/23 05:52,10/Mar/11 20:47,0.7.4,,,,,,0,,,,"Only error out if necessary pig settings have not previously been set in job config. CassandraStorage checks for environment variables on mappers/reducers, but it should only need to be set on the machine launching the pig jobs.",,eldondev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/11 19:16;eldondev;0001-Dont-fail-if-configs-already-set.patch;https://issues.apache.org/jira/secure/attachment/12473312/0001-Dont-fail-if-configs-already-set.patch",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20553,,,Fri Mar 11 03:47:16 UTC 2011,,,,,,,,,,"0|i0gamf:",93151,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"10/Mar/11 19:15;eldondev;If the environment variable is set, it wins. If not, and it's already stored in the job configuration, do nothing. If it doesn't exist at all, fail.;;;","10/Mar/11 19:30;jeromatron;+1 - works where before it would error out in mapreduce mode (post storefunc). I did suggest maybe unifying the conditions and using an else or something to make it more concise, but that's a trivial thing.;;;","10/Mar/11 20:47;brandon.williams;Committed, thanks!;;;","11/Mar/11 03:47;hudson;Integrated in Cassandra-0.7 #373 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/373/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tombstoned rows not purged from cache after gcgraceseconds,CASSANDRA-2305,12500984,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,paladin8,paladin8,10/Mar/11 04:39,16/Apr/19 09:33,14/Jul/23 05:52,10/Apr/11 18:50,0.7.5,,,,,,0,,,,"From email to list:

I was wondering if this is the expected behavior of deletes (0.7.0). Let's say I have a 1-node cluster with a single CF which has gc_grace_seconds = 0. The following sequence of operations happens (in the given order):

insert row X with timestamp T
delete row X with timestamp T+1
force flush + compaction
insert row X with timestamp T

My understanding is that the tombstone created by the delete (and row X) will disappear with the flush + compaction which means the last insertion should show up. My experimentation, however, suggests otherwise (the last insertion does not show up).

I believe I have traced this to the fact that the markedForDeleteAt field on the ColumnFamily does not get reset after a compaction (after gc_grace_seconds has passed); is this desirable? I think it introduces an inconsistency in how tombstoned columns work versus tombstoned CFs. Thanks.",,slebresne,,,,,,,,,,,,,,,,,,,,";11/Mar/11 10:26;slebresne;7200",7200,0,7200,100%,7200,0,7200,,,,,,,,,,,,,,,"11/Mar/11 10:24;slebresne;0001-Compaction-test.patch;https://issues.apache.org/jira/secure/attachment/12473375/0001-Compaction-test.patch","11/Mar/11 10:24;slebresne;0002-Invalidate-row-cache-on-compaction-purge.patch;https://issues.apache.org/jira/secure/attachment/12473376/0002-Invalidate-row-cache-on-compaction-purge.patch","14/Mar/11 14:07;slebresne;2305_2nd_patch.patch;https://issues.apache.org/jira/secure/attachment/12473565/2305_2nd_patch.patch",,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20550,,,Sun Apr 10 23:17:06 UTC 2011,,,,,,,,,,"0|i0galb:",93146,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"10/Mar/11 04:41;paladin8;For a little more info, I think this only happens when you remove an entire row. If you delete specific columns, the tombstones are handled appropriately.;;;","10/Mar/11 18:01;slebresne;This will also happen if you remove all the columns inside the row (even though you didn't issued a row deletion command).

The problem is that when you flush + compact and gcGrace has elapsed, if the row is empty (i.e. all tombstone have been collected), the row itself is collected. This means that when you issue the second wave of inserts, there is no trace whatsoever of the row.

That's why you are not supposed to have a gcGrace too low and why it is highly advised to use the current time as a timestamp. If so, the scenario above will never happen.

Best thing we can do is probably to edit http://wiki.apache.org/cassandra/DistributedDeletes to add that gcGrace should be such that no insert with a timestamp lower that a delete could reach any given node after gcGrace has elapsed.
;;;","10/Mar/11 18:32;jbellis;bq. The problem is that when you flush + compact and gcGrace has elapsed, if the row is empty (i.e. all tombstone have been collected), the row itself is collected. This means that when you issue the second wave of inserts, there is no trace whatsoever of the row.

That's what's supposed to happen, but Jeffrey is saying that is NOT what he observes.;;;","10/Mar/11 18:49;slebresne;Oups, my mistake. I somehow confused myself. I was not able to reproduce though, but I'll try harder tomorrow.;;;","11/Mar/11 10:24;slebresne;I think this is due to row cache. We do not invalidate the row cache when a row is fully collected by compaction.

Jeffrey, can you confirm that you had some row cache enabled when doing your experiments ?

Attaching 2 patch against 0.7. The first one is a unit test showing the failure, the second one is the fix.;;;","11/Mar/11 14:26;jbellis;committed;;;","11/Mar/11 20:57;paladin8;I actually don't have row cache enabled (I just checked cfstats to make sure), so I don't think that's the cause of my problem in particular. Here's some more info that may or may not be correct:

- When I run the compaction, in ColumnFamilyStore.removeDeletedStandard() I see that columns are being removed because of the c.timestamp() <= cf.getMarkedForDeleteAt() condition, which makes sense since I issued a delete on the entire row.
- However, after the compaction, I do the insert, and if I flush/compact again, I still see the columns being removed because of that condition. It seems like the markedForDeleteAt field on the ColumnFamily is persisting across the major compaction which I believe is hiding the newly inserted column.

Also, my initial steps to repro were not correct, which made it hard to figure out the root cause. Here is a proper repro:

- Create a CF with gc_grace_seconds = 0 and no row cache.
- Insert row X, col A with timestamp 0.
- Insert row X, col B with timestamp 2.
- Remove row X with timestamp 1 (expect col A to disappear, col B to stay).
- Wait 1 second.
- Force flush and compaction.
- Insert row X, col A with timestamp 0.
- Read row X, col A (see nothing).

Inserting row X, col B is necessary for this to repro because if all the columns in a row disappear, the ColumnFamily object goes away and the markedForDeleteAt field is reset. Only when a column still exists does the field persist across the compaction. Hope this helps!;;;","11/Mar/11 20:57;paladin8;I believe the cause to be something else (see latest comment).;;;","11/Mar/11 23:07;hudson;Integrated in Cassandra-0.7 #375 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/375/])
    ;;;","12/Mar/11 11:22;slebresne;Ok, I understand what you meant. I've created CASSANDRA-2317 with the fix since we have already committed a patch here. Thanks a lot for the report.;;;","12/Mar/11 11:23;slebresne;Remarking this resolved, the follow up is in CASSANDRA-2317 instead.;;;","14/Mar/11 14:00;slebresne;I'm reopening because the committed patch, while ok, is only a partial fix.;;;","14/Mar/11 14:07;slebresne;The first patch was purging the cache when the full row is expired. But we don't remove expired tombstone from the cache.

Attaching a second patch to purge the cache. This has two purposes: 
  # avoid surprise for client getting tombstones back from query (either sstable2json or rangeSlice) even well after gc_grace and compaction has occured
  # reclaim some memory for row sitting in the cache for a very long time

Note that this patch introduces concurrent deletes, and as such SHOULDN'T be applied to a branch that do not have CASSANDRA-1559 (0.7 and 0.8 have it).

Patch is against 0.7;;;","08/Apr/11 21:47;jbellis;+1;;;","10/Apr/11 18:50;slebresne;Committed to 0.7 (r1090867) and a rebased version to trunk (r1090866);;;","10/Apr/11 19:07;hudson;Integrated in Cassandra-0.7 #429 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/429/])
    Purge tombstone from row cache (0.7 version)
patch by slebresne; reviewed by jbellis for CASSANDRA-2305
;;;","10/Apr/11 23:17;hudson;Integrated in Cassandra #846 (See [https://hudson.apache.org/hudson/job/Cassandra/846/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"sstable2json dies with ""Too many open files"", regardless of ulimit",CASSANDRA-2304,12500973,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,alienth,alienth,10/Mar/11 00:54,16/Apr/19 09:33,14/Jul/23 05:52,10/Mar/11 17:13,0.7.4,,,Legacy/Tools,,,0,,,,"Running sstable2json on the attached sstable eventually results in the following:

{code}
Exception in thread ""main"" java.io.IOError: java.io.FileNotFoundException: /var/lib/cassandra/data/reddit/CommentSortsCache-f-9764-Data.db (Too many open files)
        at org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:68)
        at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:567)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:49)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:68)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
        at org.apache.cassandra.tools.SSTableExport.serializeRow(SSTableExport.java:187)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:355)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:377)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:390)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:448)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/reddit/CommentSortsCache-f-9764-Data.db (Too many open files)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:111)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:106)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:91)
        at org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:62)
{code}

Set my ulimit -n to 60000 and got the same result. Leaking file descriptors?",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"10/Mar/11 16:38;jbellis;2304.txt;https://issues.apache.org/jira/secure/attachment/12473282/2304.txt","10/Mar/11 16:56;xedin;CASSANDRA-2304-v2.patch;https://issues.apache.org/jira/secure/attachment/12473288/CASSANDRA-2304-v2.patch","10/Mar/11 01:06;alienth;sstable.tar.bz2;https://issues.apache.org/jira/secure/attachment/12473227/sstable.tar.bz2",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20549,,,Thu Mar 10 17:40:56 UTC 2011,,,,,,,,,,"0|i0gal3:",93145,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"10/Mar/11 01:06;alienth;Sstable which causes sstable2json to die. Grabbed from an 0.7.3 node.;;;","10/Mar/11 01:07;alienth;Output from lsof. Thousands of lines of the following:

{code}
java      1766       root 1080r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1081r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1082r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1083r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1084r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1085r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1086r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1087r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1088r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1089r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1090r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1091r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1092r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1093r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1094r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
java      1766       root 1095r      REG              251,0 11809597   32374948 /var/lib/cassandra/data/reddit/Hide-f-734-Data.db
{code};;;","10/Mar/11 16:38;jbellis;This is a bug with non-mmap'd I/O.;;;","10/Mar/11 16:38;jbellis;patch to close column iterator and only do one pass per row;;;","10/Mar/11 16:56;xedin;with number of the exported columns properly incremented. LGTM.;;;","10/Mar/11 17:13;jbellis;committed v2;;;","10/Mar/11 17:40;hudson;Integrated in Cassandra-0.7 #370 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/370/])
    fix fd leak in sstable2json with non-mmap'd i/o
patch by jbellis; reviewed by Pavel Yaskevich for CASSANDRA-2304
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM on repair with many inconsistent ranges,CASSANDRA-2301,12500936,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,j.casares,j.casares,j.casares,09/Mar/11 19:46,16/Apr/19 09:33,14/Jul/23 05:52,11/Mar/11 02:57,0.7.4,,,,,,0,,,,"Repair can OOM when lots of ranges are inconsistent, causing many sstables to be streamed.

I replicated the error by making 1264 3MB sstables on one node, added a second node, changed the replication factor to 2, and ran a repair.

Looking at the heap dump of the original failure, there were 2.4GB of FutureTasks, each taking up 8MB of space. I tracked down the BufferedRandomAccessFile and made sure that it was cleared every time it was closed inside of src/java/org/apache/cassandra/io/sstable/SSTableWriter.java.

Attached is the patch I used which stopped the error when I was trying to replicate it.",,mdennis,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,"09/Mar/11 20:09;jbellis;2301-v2.txt;https://issues.apache.org/jira/secure/attachment/12473193/2301-v2.txt","09/Mar/11 20:14;jbellis;2301-v3.txt;https://issues.apache.org/jira/secure/attachment/12473194/2301-v3.txt","09/Mar/11 19:47;j.casares;2301.diff;https://issues.apache.org/jira/secure/attachment/12473187/2301.diff",,,,,,,,,,,,3.0,j.casares,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20548,,,Fri Mar 11 03:47:16 UTC 2011,,,,,,,,,,"0|i0gakf:",93142,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"09/Mar/11 20:09;jbellis;I'm actually not sure how this could help -- dfile.close() already un-references the buffer, so there's no need to set dfile itself to be null.  Also, the file pointer changes as we read through the file, so changing bytescomplete to init-once is broken. (This is what nodetool compactionstats uses.)

My guess is that the second time around you just got lucky and index build was able to keep up w/ streaming enough to avoid OOMing.

But there is a bug here with the file/buffer handling -- we should lazy-init this once we're ready to build the index, rather than when we enqueue the task.  Patch attached w/ this approach.;;;","09/Mar/11 20:14;jbellis;v2 overcomplicated the problem.  v3 attached w/o extra Builder fields.;;;","10/Mar/11 21:23;mdennis;+1 on v3;;;","10/Mar/11 23:54;j.casares;+1 on v3.

Ran the patched code twice and my cluster of 2 didn't OOM.;;;","11/Mar/11 02:57;jbellis;committed;;;","11/Mar/11 03:47;hudson;Integrated in Cassandra-0.7 #373 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/373/])
    reduce memory use during streaming of multiple sstables
patch by jbellis; reviewed by mdennis and tested by Joaquin Casares for CASSANDRA-2301
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsupportedOperationException: Overflow in bytesPastMark(..),CASSANDRA-2297,12500876,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,muga_nishizawa,muga_nishizawa,09/Mar/11 11:41,16/Apr/19 09:33,14/Jul/23 05:52,11/Mar/11 18:32,0.7.4,,,,,,0,,,,"I hit the following exception on a row that was more than 60GB.  
The row has column families of super column type.

This problem is discussed by the following thread.  
http://www.mail-archive.com/dev@cassandra.apache.org/msg01881.html

{code}
ERROR [HintedHandoff:1] 2011-02-26 18:49:35,708 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.UnsupportedOperationException: Overflow: 2147484294
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.UnsupportedOperationException: Overflow: 2147484294
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.bytesPastMark(BufferedRandomAccessFile.java:477)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.getNextBlock(IndexedSliceReader.java:179)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:120)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:1)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:118)
        at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:142)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1290)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1167)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1095)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:138)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$1(HintedHandOffManager.java:262)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
{code}","Java 1.6.0_23, CentOS 5.5 (64bit)",,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"10/Mar/11 17:07;jbellis;2297.txt;https://issues.apache.org/jira/secure/attachment/12473291/2297.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20547,,,Fri Mar 11 23:07:02 UTC 2011,,,,,,,,,,"0|i0gajj:",93138,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"10/Mar/11 17:07;jbellis;fix bytesPastMark to return long.

Note that if your row index is > 2GB then you should probably increase column_index_size_in_kb to 256 or higher.  I've created CASSANDRA-2308 to expose row index size so this can be tuned more accurately.;;;","11/Mar/11 18:16;slebresne;It breaks BufferedRandomAccessFileTest.
But +1 on the patch otherwise.;;;","11/Mar/11 18:32;jbellis;committed w/ removal of obsolete BRAFTest code;;;","11/Mar/11 23:07;hudson;Integrated in Cassandra-0.7 #375 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/375/])
    fix HH delivery when column index is larger than 2GB
patch by jbellis; reviewed by slebresne for CASSANDRA-2297
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Scrub resulting in ""bloom filter claims to be longer than entire row size"" error",CASSANDRA-2296,12500831,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,alienth,alienth,09/Mar/11 01:40,16/Apr/19 09:33,14/Jul/23 05:52,09/Mar/11 14:37,0.7.4,,,Legacy/Tools,,,0,,,,"Doing a scrub on a node which I upgraded from 0.7.1 (was previously 0.6.8) to 0.7.3. Getting this error multiple times:
{code}
 WARN [CompactionExecutor:1] 2011-03-08 18:33:52,513 CompactionManager.java (line 625) Row is unreadable; skipping to next
 WARN [CompactionExecutor:1] 2011-03-08 18:33:52,514 CompactionManager.java (line 599) Non-fatal error reading row (stacktrace follows)
java.io.IOError: java.io.EOFException: bloom filter claims to be longer than entire row size
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:117)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:590)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.EOFException: bloom filter claims to be longer than entire row size
        at org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:113)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:87)
        ... 8 more
 WARN [CompactionExecutor:1] 2011-03-08 18:33:52,515 CompactionManager.java (line 625) Row is unreadable; skipping to next
 INFO [CompactionExecutor:1] 2011-03-08 18:33:53,777 CompactionManager.java (line 637) Scrub of SSTableReader(path='/cassandra/data/reddit/Hide-f-671-Data.db') complete: 254709 rows in new sstable
 WARN [CompactionExecutor:1] 2011-03-08 18:33:53,777 CompactionManager.java (line 639) Unable to recover 1630 that were skipped.  You can attempt manual recovery from the pre-scrub snapshot.  You can also run nodetool repair to transfer the data from a healthy replica, if any
{code}",,mdennis,stuhood,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"09/Mar/11 03:33;jbellis;2296.txt;https://issues.apache.org/jira/secure/attachment/12473092/2296.txt","09/Mar/11 01:58;alienth;sstable_part1.tar.bz2;https://issues.apache.org/jira/secure/attachment/12473088/sstable_part1.tar.bz2","09/Mar/11 01:58;alienth;sstable_part2.tar.bz2;https://issues.apache.org/jira/secure/attachment/12473089/sstable_part2.tar.bz2",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20546,,,Thu Mar 10 19:08:11 UTC 2011,,,,,,,,,,"0|i0gajb:",93137,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"09/Mar/11 02:40;jbellis;With debug logging turned on it looks like this:

{noformat}
[lots of rows around 119 bytes long]
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 559) row 337a306f615f666c38756a is 119 bytes
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 588) Index doublecheck: row 337a306f615f666c38756a is 119 bytes
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 550) Reading row at 44385
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,251 CompactionManager.java (line 559) row 34306536785f666f666b65 is 0 bytes
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,252 CompactionManager.java (line 588) Index doublecheck: row 34306536785f666f666b65 is 0 bytes
 WARN [CompactionExecutor:1] 2011-03-08 20:34:12,253 CompactionManager.java (line 606) Non-fatal error reading row (stacktrace follows)
java.io.IOError: java.io.EOFException: bloom filter claims to be 734305 bytes, longer than entire row size 0
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:125)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:597)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:57)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:196)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
Caused by: java.io.EOFException: bloom filter claims to be 734305 bytes, longer than entire row size 0
        at org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:113)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:95)
        ... 8 more
 WARN [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 632) Row is unreadable; skipping to next
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 550) Reading row at 44406
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 559) row 34616465655f66707a6178 is 119 bytes
DEBUG [CompactionExecutor:1] 2011-03-08 20:34:12,255 CompactionManager.java (line 588) Index doublecheck: row 34616465655f66707a6178 is 119 bytes
[lots more rows around 119 bytes]
{noformat}

In other words: there's an row that's empty except for the key, which is causing the problem because we're not supposed to write rows like that.  I checked with a hex editor and that's what it looks like.

The good news is that scrub is correctly skipping it and recovering everything else fine.

The bad news is we have (or possibly, had) a bug that was causing those empty rows to be written.;;;","09/Mar/11 02:59;jbellis;added asserts to catch zero-length rows in r1079650;;;","09/Mar/11 03:07;alienth;Got the following error while restarting *after* I ran the scrub on that same node:

{code}
ERROR [CompactionExecutor:1] 2011-03-08 19:54:48,023 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOError: java.io.EOFException
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:117)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:67)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:179)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:144)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:136)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:39)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:284)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:449)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:124)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:94)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code};;;","09/Mar/11 03:12;jbellis;The first scrub ended with

{noformat}
 INFO [CompactionExecutor:1] 2011-03-08 20:45:39,174 CompactionManager.java (line 644) Scrub of SST\
ableReader(path='/var/lib/cassandra/data/KS1/Hide-f-671-Data.db') complete: 254709 rows in new ssta\
ble
 WARN [CompactionExecutor:1] 2011-03-08 20:45:39,174 CompactionManager.java (line 646) Unable to re\
cover 1630 rows that were skipped.  You can attempt manual recovery from the pre-scrub snapshot.  Y\
ou can also run nodetool repair to transfer the data from a healthy replica, if any
{noformat}

Scrubbing the scrubbed version again, ended with

{noformat}
 INFO 21:11:01,349 Scrub of SSTableReader(path='/var/lib/cassandra/data/KS1/Hide-f-672-Data.db') complete: 253308 rows in new sstable
 WARN 21:11:01,349 Unable to recover 1401 rows that were skipped.  You can attempt manual recovery from the pre-scrub snapshot.  You can also run nodetool repair to transfer the data from a healthy replica, if any
{noformat}

Scrub is eating rows.;;;","09/Mar/11 03:21;jbellis;Scrub writes a zero-length row when tombstones expire and there is nothing left, instead of writing no row at all.  So, as the clock rolls forwards and more tombstones expire, you will usually get a few more zero-length rows written, that will be cleaned out by the next scrub.;;;","09/Mar/11 03:22;hudson;Integrated in Cassandra-0.7 #362 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/362/])
    add asserts to make sure we don't write zero-length rows; see CASSANDRA-2296
;;;","09/Mar/11 03:33;jbellis;fix attached.  now skips tombstoned rows properly w/o leaving stubs in the new sstable.;;;","09/Mar/11 09:48;slebresne;In the retry part, the goodRows++ after the if should be removed to avoid counting rows twice.

Other than this, +1;;;","09/Mar/11 14:37;jbellis;committed w/ ++ fix;;;","09/Mar/11 15:02;hudson;Integrated in Cassandra-0.7 #365 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/365/])
    avoid writing empty rows when scrubbing tombstoned rows
patch by jbellis; reviewed by slebresne for CASSANDRA-2296
;;;","09/Mar/11 16:58;jbellis;also added test in r1079882;;;","10/Mar/11 00:32;alienth;I applied the patch and retried. Getting a new exception. Thousands of this:

{code}
 WARN [CompactionExecutor:1] 2011-03-09 17:29:59,752 CompactionManager.java (line 641) Row at 517805025 is unreadable; skipping to next
 INFO [CompactionExecutor:1] 2011-03-09 17:29:59,752 SSTableWriter.java (line 108) Last written key : DecoratedKey(125686934811414729670440675125192621396, 627975726c2833626333626339353363353762313133373331336461303233396438303534312c66692e676f73757065726d6f64656c2e636f6d2f70726f66696c65732f2f6170706c65747265713d3132373333393332313937363529)
 INFO [CompactionExecutor:1] 2011-03-09 17:29:59,752 SSTableWriter.java (line 109) Current key : DecoratedKey(11081980355438931816706032048128862258, 30303063623061323633313463653465376663333561303531326333653737363333663065646134)
 INFO [CompactionExecutor:1] 2011-03-09 17:29:59,752 SSTableWriter.java (line 110) Writing into file /var/lib/cassandra/data/reddit/permacache-tmp-f-168615-Data.db
 WARN [CompactionExecutor:1] 2011-03-09 17:29:59,752 CompactionManager.java (line 607) Non-fatal error reading row (stacktrace follows)
java.io.IOException: Keys must be written in ascending order.
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:111)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:128)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:598)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}

Keys are getting written back improperly?;;;","10/Mar/11 01:36;jbellis;Looks like a different problem. What is the context at debug level?;;;","10/Mar/11 01:51;alienth;I'll check the debug output on it tomorrow. I should note that I ran a scrub on this same set of data yesterday on 0.7.3. I got two errors regarding another CF, but nothing for the CF which is now complaining.;;;","10/Mar/11 18:52;alienth;Here is the debug output. Going to get a comparison on the unpatched 0.7.3 to see if there is any difference.

{code}
DEBUG 11:50:52,510 Reading row at 504216964
DEBUG 11:50:52,510 row 636f6d6d656e74735f706172656e74735f3233383135363235 is 66 bytes
DEBUG 11:50:52,510 Index doublecheck: row 636f6d6d656e74735f706172656e74735f3233383135363235 is 66 bytes
 INFO 11:50:52,511 Last written key : DecoratedKey(125686934811414729670440675125192621396, 627975726c2833626333626339353363353762313133373331336461303233396438303534312c66692e676f73757065726d6f64656c2e636f6d2f70726f66696c65732f2f6170706c65747265713d3132373333393332313937363529)
 INFO 11:50:52,511 Current key : DecoratedKey(11047858886149374835950241979723972473, 636f6d6d656e74735f706172656e74735f3233383135363235)
 INFO 11:50:52,511 Writing into file /var/lib/cassandra/data/reddit/permacache-tmp-f-168492-Data.db
 WARN 11:50:52,511 Non-fatal error reading row (stacktrace follows)
java.io.IOException: Keys must be written in ascending order.
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:111)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:128)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:598)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}

;;;","10/Mar/11 19:08;alienth;Disregard. Getting the same thing on unpatched 0.7.3. I'll create a separate bug report.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connections are not reset if a node is restarted but we had not marked it down,CASSANDRA-2292,12500808,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,08/Mar/11 19:49,16/Apr/19 09:33,14/Jul/23 05:52,08/Mar/11 20:40,0.7.4,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"08/Mar/11 20:32;slebresne;0001-Reset-connections-when-a-node-is-restarted-but-we-di-v2.patch;https://issues.apache.org/jira/secure/attachment/12473041/0001-Reset-connections-when-a-node-is-restarted-but-we-di-v2.patch","08/Mar/11 19:50;slebresne;0001-Reset-connections-when-a-node-is-restarted-but-we-di.patch;https://issues.apache.org/jira/secure/attachment/12473033/0001-Reset-connections-when-a-node-is-restarted-but-we-di.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20545,,,Tue Mar 08 21:08:11 UTC 2011,,,,,,,,,,"0|i0gaif:",93133,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"08/Mar/11 20:07;jbellis;any reason not to put the convict in markAlive?;;;","08/Mar/11 20:32;slebresne;Attach v2 that call onDead for all subscriber instead of simply convict().;;;","08/Mar/11 20:40;brandon.williams;Committed with comments clarified.;;;","08/Mar/11 21:08;hudson;Integrated in Cassandra-0.7 #358 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/358/])
    Notify subscribers that the node was dead if it restarts before we mark
it down.
Patch slebresne, reviewed by brandonwilliams for CASSANDRA-2292
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ant build script in contrib/stress fails.,CASSANDRA-2291,12500802,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jahangir,jahangir,08/Mar/11 19:12,16/Apr/19 09:33,14/Jul/23 05:52,08/Mar/11 19:17,0.8 beta 1,,,,,,0,,,,"Build fails in contrib/stress with following message:
/mnt/hgfs/workspace/cassandra-source/contrib/stress/build.xml:38: javac doesn't support the nested ""path"" element.

Fix:
Needs to move <path refid=""cassandra.classes"" /> inside path element.

SVN copy:
            <path refid=""cassandra.classes"" />
            <classpath>
                <path>
                    <fileset dir=""${cassandra.lib}"">
                        <include name=""**/*.jar"" />
                    </fileset>
                </path>
            </classpath>

To be changed to:
            <classpath>
                <path>
                    <path refid=""cassandra.classes"" />
                    <fileset dir=""${cassandra.lib}"">
                        <include name=""**/*.jar"" />
                    </fileset>
                </path>
            </classpath>",Apache Ant version 1.8.0.,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20544,,,Tue Mar 08 19:17:56 UTC 2011,,,,,,,,,,"0|i0gai7:",93132,,,,,Low,,,,,,,,,,,,,,,,,"08/Mar/11 19:17;jbellis;fixed in r1079494.  thanks for the report!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair hangs if one of the neighbor is dead,CASSANDRA-2290,12500799,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,08/Mar/11 19:09,16/Apr/19 09:33,14/Jul/23 05:52,19/Apr/11 12:54,0.7.5,0.8 beta 1,,,,,0,,,,"Repair don't cope well with dead/dying neighbors. There is 2 problems:

  # Repair don't check if a node is dead before sending a TreeRequest; this is easily fixable.
  # If a neighbor dies mid-repair, the repair will also hang forever.

The second point is not easy to deal with. The best approach is probably CASSANDRA-1740 however. That is, if we add a way to query the state of a repair, and that this query correctly check all neighbors and also add a way to cancel a repair, this would probably be enough.
",,jborgstrom,stuhood,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,CASSANDRA-1740,,"09/Mar/11 13:16;slebresne;0001-Don-t-start-repair-if-a-neighbor-is-dead.patch;https://issues.apache.org/jira/secure/attachment/12473125/0001-Don-t-start-repair-if-a-neighbor-is-dead.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20543,,,Tue Apr 19 14:04:21 UTC 2011,,,,,,,,,,"0|i0gahz:",93131,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"09/Mar/11 13:16;slebresne;Attaching patch for the first problem above. It checks that all neighbors are alive before attempting the repair. If not, it don't start the repair. Another option would be to still do the repair with whomever neighbor are alive (if any). But I think that refusing to repair is a saner default and I'm fine waiting that someone needs the second option before considering adding it.
;;;","09/Mar/11 18:32;amorton;Not sure if this helps. I found a place where AES was hanging while testing failure during streaming transfer for CASSANDRA-2088 (against 0.7). I broke the FileStresmTask to only send one range and close the sending channel. 

The  IncomingStreamReader.readFile() got stuck in an infinite loop because it does not check the return from FileChannel.transferFrom(). It was returning 0 bytes read. Also the FileStreamTask does not check the bytes sent by transferTo()

While stuck in the loop the socket it was reading from was (127.0.0.1 was in the loop, .0.2 was sending) 
java      25371 aaron   73u  IPv4 0xffffff8010742ff8      0t0  TCP 127.0.0.1:7000->127.0.0.2:52759 (CLOSE_WAIT)

When I was debugging the socketChannel was still reporting it was open. 

Update: Modified FileStresmTask to call System.exit() after sending the first section and got the same result.;;;","18/Apr/11 22:11;jbellis;+1 the check-neighbors patch;;;","18/Apr/11 22:23;slebresne;Committed (to 0.7 and 0.8) the check-neighbor patch. I'm closing this as the rest of this issue is a duplicate CASSANDRA-2433 and could go there.;;;","19/Apr/11 00:25;jbellis;oops, need to fix AESTest now;;;","19/Apr/11 10:28;hudson;Integrated in Cassandra #856 (See [https://hudson.apache.org/hudson/job/Cassandra/856/])
    ;;;","19/Apr/11 10:39;slebresne;Tests fixed, sorry about that.;;;","19/Apr/11 10:46;hudson;Integrated in Cassandra-0.8 #19 (See [https://hudson.apache.org/hudson/job/Cassandra-0.8/19/])
    Fix unit tests for CASSANDRA-2290
;;;","19/Apr/11 14:04;hudson;Integrated in Cassandra-0.7 #447 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/447/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replicate on write NPE for empty row,CASSANDRA-2289,12500798,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,08/Mar/11 19:00,16/Apr/19 09:33,14/Jul/23 05:52,09/Mar/11 15:24,0.8 beta 1,,,,,,0,,,,Replicate on write will throw a NPE for the first write to a row.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/11 01:24;stuhood;0001-Appendix-to-CASSANDRA-2289.txt;https://issues.apache.org/jira/secure/attachment/12473084/0001-Appendix-to-CASSANDRA-2289.txt","08/Mar/11 19:00;stuhood;0001-don-t-replicate-empty-row.txt;https://issues.apache.org/jira/secure/attachment/12473027/0001-don-t-replicate-empty-row.txt",,,,,,,,,,,,,2.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20542,,,Wed Mar 09 18:23:03 UTC 2011,,,,,,,,,,"0|i0gahr:",93130,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"08/Mar/11 20:03;slebresne;+1;;;","08/Mar/11 20:09;jbellis;committed;;;","08/Mar/11 21:26;hudson;Integrated in Cassandra #759 (See [https://hudson.apache.org/hudson/job/Cassandra/759/])
    don't replicate rows that don't exist yet
patch by Stu Hood; reviewed by slebresne for CASSANDRA-2289
;;;","09/Mar/11 01:24;stuhood;Very sorry... the original patch was not enough to fix this. Attaching an appendix. EDIT: errata?;;;","09/Mar/11 07:59;slebresne;+1 to the errata too. ;;;","09/Mar/11 15:24;jbellis;committed;;;","09/Mar/11 18:23;hudson;Integrated in Cassandra #765 (See [https://hudson.apache.org/hudson/job/Cassandra/765/])
    fix #2 for counter replication NPE
patch by Stu Hood; reviewed by slebresne for CASSANDRA-2289
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
range queries don't respect snitch for local replicas,CASSANDRA-2286,12500783,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,08/Mar/11 16:37,16/Apr/19 09:33,14/Jul/23 05:52,11/Mar/11 18:28,0.7.4,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"08/Mar/11 16:39;jbellis;2286.txt;https://issues.apache.org/jira/secure/attachment/12472994/2286.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20541,,,Fri Mar 11 23:07:02 UTC 2011,,,,,,,,,,"0|i0gah3:",93127,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"11/Mar/11 18:20;slebresne;+1;;;","11/Mar/11 18:28;jbellis;committed [w/ addition of ""&& !liveEndpoints.isEmpty()"" to local condition];;;","11/Mar/11 23:07;hudson;Integrated in Cassandra-0.7 #375 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/375/])
    makerange queries respect snitch for local replicas
patch by jbellis; reviewed by slebresne for CASSANDRA-2286
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading an empty commit log throw an exception,CASSANDRA-2285,12500768,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,08/Mar/11 14:47,16/Apr/19 09:33,14/Jul/23 05:52,09/Mar/11 20:52,0.7.4,,,,,,0,,,,"Start a one node cluster, shutdown within 10 seconds but after the node is started and the location infos has been flushed. Restart node, you'll get a 'EOFException: unable to seek past the end of the file in read-only mode.'",,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"09/Mar/11 12:49;slebresne;0001-skip-CL-recover-when-fully-data-was-fully-flushed-wi.patch;https://issues.apache.org/jira/secure/attachment/12473123/0001-skip-CL-recover-when-fully-data-was-fully-flushed-wi.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20540,,,Wed Mar 09 20:52:47 UTC 2011,,,,,,,,,,"0|i0gagv:",93126,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"08/Mar/11 14:48;slebresne;Attached patch against 0.7;;;","08/Mar/11 16:11;jbellis;I can't reproduce, probably because Cassandra always does a few writes to system tables post-startup.  Can you make a failing test?;;;","08/Mar/11 16:22;slebresne;Yeah, when I say stop, that may involve killing the node mid-start.;;;","09/Mar/11 12:49;slebresne;
I had of wrong understanding of what was going on. The problem is that we can have a commit log header with a replay position greater than the size of the commit log.

This happens if some data gets flushed before it had time to hit the actual log (thus only in periodic mode). Which in turn can happen because we use a FileOutputStream for the header, which will get sync to disk even if cassandra dies/is killed shortly afterwards (unless this is a system failure).

It's fairly unlikely to happen in real use, but it is fairly easy to reproduce (see description).

Attaching a patch using the correct condition as well as a test unit.
;;;","09/Mar/11 20:52;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming Old Format Data Fails in 0.7.3 after upgrade from 0.6.8,CASSANDRA-2283,12500677,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,eonnen,eonnen,07/Mar/11 21:00,16/Apr/19 09:33,14/Jul/23 05:52,18/Apr/11 21:58,0.7.5,,,,,,0,,,,"After successfully upgrading a 0.6.8 ring to 0.7.3, we needed to bootstrap in a new node relatively quickly. When starting the new node with an assigned token in auto bootstrap mode, we see the following exceptions on the new node:

INFO [main] 2011-03-07 10:37:32,671 StorageService.java (line 505) Joining: sleeping 30000 ms for pending range setup
 INFO [main] 2011-03-07 10:38:02,679 StorageService.java (line 505) Bootstrapping
 INFO [HintedHandoff:1] 2011-03-07 10:38:02,899 HintedHandOffManager.java (line 304) Started hinted handoff for endpoint /10.211.14.200
 INFO [HintedHandoff:1] 2011-03-07 10:38:02,900 HintedHandOffManager.java (line 360) Finished hinted handoff of 0 rows to endpoint /10.211.14.200
 INFO [CompactionExecutor:1] 2011-03-07 10:38:04,924 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuff-f-1
 INFO [CompactionExecutor:1] 2011-03-07 10:38:05,390 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuff-f-2
 INFO [CompactionExecutor:1] 2011-03-07 10:38:05,768 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-1
 INFO [CompactionExecutor:1] 2011-03-07 10:38:06,389 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-2
 INFO [CompactionExecutor:1] 2011-03-07 10:38:06,581 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-3
ERROR [CompactionExecutor:1] 2011-03-07 10:38:07,056 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at org.apache.cassandra.io.sstable.IndexHelper.skipIndex(IndexHelper.java:65)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:303)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:923)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:916)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [CompactionExecutor:1] 2011-03-07 10:38:08,480 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-5
 INFO [CompactionExecutor:1] 2011-03-07 10:38:08,582 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid_reg_idx-f-1
ERROR [CompactionExecutor:1] 2011-03-07 10:38:08,635 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at org.apache.cassandra.io.sstable.IndexHelper.skipIndex(IndexHelper.java:65)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:303)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:923)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:916)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [CompactionExecutor:1] 2011-03-07 10:38:08,666 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at org.apache.cassandra.io.sstable.IndexHelper.skipIndex(IndexHelper.java:65)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:303)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:923)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:916)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [CompactionExecutor:1] 2011-03-07 10:38:08,855 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid_reg_idx-f-4

Two attempts to bootstrap in the new node both exhibited this behavior. On the node owning the tokens being migrated, stream activity is visible but doesn't update any progress I think due to the issues on the receiving host.







Lastly, just case it's relevant, we had an EC2 node die underneath us during the upgrade so not all nodes were drained. This didn't affect the upgrade but I wanted to note it her to be thorough.","0.7.3 upgraded from 0.6.8, Linux Java HotSpot(TM) 64-Bit Server VM (build 17.1-b03, mixed mode)",eonnen,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/11 14:36;jbellis;2283.txt;https://issues.apache.org/jira/secure/attachment/12474779/2283.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20539,,,Mon Apr 18 22:21:41 UTC 2011,,,,,,,,,,"0|i0gagf:",93124,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"09/Mar/11 14:52;jbellis;CASSANDRA-2296 is going to cause streaming trouble too. In other words: scrub won't fix this unless run w/ a build that has 2296 applied.;;;","09/Mar/11 17:10;jbellis;bq. scrub won't fix this

that is, for sstables containing expired tombstones;;;","12/Mar/11 20:31;stuhood;While not ideal, this is actually supposed to throw an exception in SSTableWriter.createBuilder, but that is dependent on a correctly versioned Descriptor being created on the destination side. I expect that streaming is dropping the source version when it creates the destination descriptor.;;;","24/Mar/11 20:54;eonnen;Ok, I can confirm that after upgrading to 0.7.4 where 2296 was applied, and after performing a scrub, we were able to bootstrap in new nodes again.;;;","25/Mar/11 19:03;jbellis;IMO the right thing to do here is to deserialize enough of the data sent during stream to (a) rewrite it to latest format and (b) write bloom filter and row index -- currently this is done in a second pass post-stream.;;;","26/Mar/11 09:51;stuhood;> (a) rewrite it to latest format and (b) write bloom filter and row index
I was hoping that we could get away with just doing (b) in order to avoid having to re-write the data file, but it certainly simplifies things to re-write in the current format.

EDIT: CASSANDRA-2336 took a step toward allowing rebuilding and index writing to be version specific, in order to implement (b). I'm most of the way through an implementation of CASSANDRA-2319 on top of it, but I don't see a clear answer for a/b/a+b.;;;","28/Mar/11 14:34;jbellis;You're right, making it one-pass isn't feasible without writing the streamed row out to a temporary file first, since we don't have a way to rebuild the row-level bloom filter + block index.  In other words, not really any more one-pass than the existing approach.;;;","28/Mar/11 14:36;jbellis;Patch to preserve version across streams. Also removes obsolete component field from PendingFile (we only stream Component.DATA).;;;","12/Apr/11 22:13;stuhood;Looks good: only comment is that BootstrapTest should probably purposely use an old version and check that it is preserved.;;;","18/Apr/11 21:58;jbellis;added version check to BootstrapTest.

committed w/ just the version changes -- left PendingFile alone. (CASSANDRA-2438 suggests we might want to stream fully-formed sstables for bulk load.);;;","18/Apr/11 22:21;hudson;Integrated in Cassandra-0.7 #442 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/442/])
    preserve version when streaming data from old sstables
patch by jbellis; reviewed by Stu Hood for CASSANDRA-2283
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadCallback AssertionError: resolver.getMessageCount() <= endpoints.size(),CASSANDRA-2282,12500665,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,thobbs,thobbs,07/Mar/11 19:34,16/Apr/19 09:33,14/Jul/23 05:52,09/Mar/11 14:58,0.7.4,,,,,,0,,,,"In a three node cluster with RF=2, when trying to page through all rows with get_range_slices() at CL.ONE, I get timeouts on the client side.  Looking at the Cassandra logs, all of the nodes show the following AssertionError repeatedly:

{noformat}
ERROR [RequestResponseStage:2] 2011-03-07 19:10:27,527 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.AssertionError
        at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
ERROR [RequestResponseStage:2] 2011-03-07 19:10:27,529 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[RequestResponseStage:2,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{noformat}

The nodes are all running 0.7.3.  The cluster was at size 3 before any data was inserted, and everything else appears perfectly healthy.",,cburroughs,jeromatron,skamio,tapter,terjem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/11 18:02;jbellis;2282-v2.txt;https://issues.apache.org/jira/secure/attachment/12473017/2282-v2.txt","08/Mar/11 16:34;jbellis;2282.txt;https://issues.apache.org/jira/secure/attachment/12472984/2282.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20538,,,Fri Apr 22 05:59:19 UTC 2011,,,,,,,,,,"0|i0gag7:",93123,,thobbs,,thobbs,Normal,,,,,,,,,,,,,,,,,"08/Mar/11 04:26;skamio;In our case, the same AssertionError occurs on multi node cluster with replication factor = 3 (0.7.3 release version).
Feeding data into cassandra looks ok (consistency level = QUORUM). Though, UnavailableException was received via hector 0.7.0-28 several times. It warns about number of replica (see the following stack trace). It might relate to this problem. But not sure. 

When querying data, AssertionError occurs in cassandra and client gets timedout exception.
Our client queries in several query types in different column families. This timeout occurs quite often in secondary index query.
The error is only logged on the host which client connects via thrift (according to timestamp in log).

Another experience is when I tried to retrieve data via CLI.
Query like ""list Standard1 limit 10"" returns results normally. But the cassandra logs the AssertionError on the host. Other node doesn't have the log.
(When query returns ""null"" (I guess there is not enough replica), this exception is very likely to be logged.)

-------------------
* Unavailable exception stack trace received via hector 0.7.0-28 when feeding data into cassandra:

me.prettyprint.hector.api.exceptions.HUnavailableException: : May not be enough replicas present to handle consistency level.
        at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:52)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$1.execute(KeyspaceServiceImpl.java:95)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$1.execute(KeyspaceServiceImpl.java:88)
        at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:101)
        at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:221)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:129)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.batchMutate(KeyspaceServiceImpl.java:100)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.batchMutate(KeyspaceServiceImpl.java:106)
        at me.prettyprint.cassandra.model.MutatorImpl$2.doInKeyspace(MutatorImpl.java:203)
        at me.prettyprint.cassandra.model.MutatorImpl$2.doInKeyspace(MutatorImpl.java:200)
        at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)
        at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:85)
        at me.prettyprint.cassandra.model.MutatorImpl.execute(MutatorImpl.java:200)
        at jp.co.rakuten.gsp.cassandra_connector.feeder.CassandraFeeder.batchInsert(CassandraFeeder.java:506)
        at jp.co.rakuten.gsp.purchase_history.cassandra_connector.PHCassandraFeeder.consume(PHCassandraFeeder.java:240)
        at jp.co.rakuten.gsp.cassandra_connector.feeder.CassandraFeeder.process(CassandraFeeder.java:330)
        at jp.co.rakuten.gsp.cassandra_connector.feeder.Feeder.run(Feeder.java:164)
        at java.lang.Thread.run(Thread.java:662)
Caused by: UnavailableException()
        at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:16485)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:916)
        at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:890)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$1.execute(KeyspaceServiceImpl.java:93)
        ... 16 more

;;;","08/Mar/11 14:04;terjem;From ReadCallback.java


        this.endpoints = repair || resolver instanceof RowRepairResolver
                       ? endpoints
                       : endpoints.subList(0, Math.min(endpoints.size(), blockfor)); // min so as to not throw exception until assureSufficient is called

This will cut the list of endpoints to whatever is the consistency requirement (in the case where repair is false, which for instance happens all the time for a rangescan).

Later:
assert resolver.getMessageCount() <= endpoints.size()

which will cause an assert if all nodes answers on a range request (or if you have a random readrepair).

I am actually not 100% sure what the fix is right now as I have not had time to scan the rest of the code and need to leave office for today, but that is the problem anyway :)
To be honest, the logic here may look a bit broken.

The assert happening here may good either. The error condition is not returned to the client, so it will hang around waiting for a timeout to occur.
Maybe the code should throw some exception before the assert?

Something like
assert resolver.getMessageCount() <= endpoints.size() :  ""Got "" + resolver.getMessageCount() + "" replies but only know "" + endpoints.size() + "" endpoints"";
may also be good?;;;","08/Mar/11 14:06;terjem;Just to make this very clear.
The list of endpoints is cut to the consistency requirement so if:
- The request requires gossip with more than one node
- all nodes answers
- you do not use consistencylevel ALL,
then the assert will trigger.;;;","08/Mar/11 16:34;jbellis;The bug is that range queries were not restricting the set of endpoints it queries to handler.endpoints the way single-row queries do. Fix attached with a couple extra comments.;;;","08/Mar/11 17:57;skamio;Hi Jonathan,

I tried your patch. But still the assertion happens in our cluster. Query is IndexedSlicesQuery with QUORUM.


ERROR [RequestResponseStage:23] 2011-03-09 02:47:23,343 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[RequestResponseStage:23,5,main]
java.lang.AssertionError: Got 3 replies but requests were only sent to 2 endpoints
        at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:129)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [RequestResponseStage:10] 2011-03-09 02:48:48,463 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[RequestResponseStage:10,5,main]
java.lang.AssertionError: Got 3 replies but requests were only sent to 2 endpoints
        at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:129)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
;;;","08/Mar/11 18:02;jbellis;v2 adds same fix for indexed queries;;;","08/Mar/11 21:40;jbellis;Workaround: remove this line from cassandra-env.sh

{noformat}
JVM_OPTS=""$JVM_OPTS -ea""
{noformat};;;","08/Mar/11 21:50;thobbs;v2 patch fixes the issue for me.;;;","09/Mar/11 03:18;skamio;v2 patch works for me too. Server side errors are gone.

My client still gets TimedOut exception. But maybe this is a different problem.;;;","09/Mar/11 14:55;jbellis;bq. My client still gets TimedOut exception. But maybe this is a different problem.

real timeouts can still happen. :)

i'd suggest 1) checking for dropped message warnings in the log; if there aren't any, 2) enabling debug logging to see where the commands are going and not coming back from in time.;;;","09/Mar/11 14:58;jbellis;committed;;;","09/Mar/11 15:08;skamio;I'm trying to find out when the timeout happens. So, I put logging to CassandraServer.java and  ReadCallback.java just before timeout exception.
I saw Timeout occurs even when 2 responses are received for query with quorum. 2 responses should be enough for quorum. Is it correct behavior?

------
 INFO [pool-1-thread-44] 2011-03-09 23:34:47,685 ReadCallback.java (line 125) Operation timed out - received only 2 responses from /xx.xx.xx.67, /xx.xx.xx.68, command class = org.apache.cassandra.db.SliceFromReadCommand
 INFO [pool-1-thread-44] 2011-03-09 23:34:47,686 CassandraServer.java (line 110) --- timed out: Operation timed out - received only 2 responses from /xx.xx.xx.67, /xx.xx.xx.68,  .
-----
;;;","09/Mar/11 15:16;hudson;Integrated in Cassandra-0.7 #366 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/366/])
    update CHANGES for CASSANDRA-2282 that got committed already (in r1079812 I think)
;;;","09/Mar/11 15:21;jbellis;And what does the Blockfor/repair is %s/%s; setting up requests to %s debug line show?;;;","09/Mar/11 15:39;skamio;Timeout setting in cassandra.yaml is 15sec. So, the following must be the line. IP address of cassandra node which has the log is xx.xx.xx.70.

----
DEBUG [pool-1-thread-121] 2011-03-09 23:34:32,726 ReadCallback.java (line 88) Blockfor/repair is 2/true; setting up requests to /xx.xx.xx.66,/xx.xx.xx.67,/xx.xx.xx.68
----
;;;","09/Mar/11 17:04;jbellis;Sounds like you're getting replies back from digest request but not data.  In StorageProxy:

{code}
            // The data-request message is sent to dataPoint, the node that will actually get
            // the data for us. The other replicas are only sent a digest query.
{code}

Note that debug logs show which node gets the data request.;;;","22/Apr/11 05:59;stuhood;> Sounds like you're getting replies back from digest request but not data.
Created CASSANDRA-2540;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tombstones not collected post-repair,CASSANDRA-2279,12500654,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,j.casares,j.casares,07/Mar/11 17:42,16/Apr/19 09:33,14/Jul/23 05:52,08/Apr/11 22:23,0.7.5,,,Legacy/Tools,,,0,,,,"The keys would only show up in sstables2json and look like this:

(root@aps4):/opt/cassandra/storage/queue/data/Panama Wed Feb 23 07:24:34am 
===> /opt/cassandra/bin/sstable2json Queue-2527-Data.db -k waq:publicMessageIndexingWorkArea:PUM8a65ce95-9d35-4941-928c-dd5965e8b29b 
2011-02-23 07:24:43,710 INFO [org.apache.cassandra.config.DatabaseDescriptor] - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap 
2011-02-23 07:24:43,972 INFO [org.apache.cassandra.io.SSTableReader] - Opening /opt/cassandra/storage/queue/data/Panama/Queue-2527-Data.db 
{ 
""waq:publicMessageIndexingWorkArea:PUM8a65ce95-9d35-4941-928c-dd5965e8b29b"": [] 
} 
(root@aps4):/opt/cassandra/storage/queue/data/Panama Wed Feb 23 07:24:44am 
===>

The steps that I took to reproduce it were:
Create a keyspace, column family, and a key
Delete the key on Node 1 using the cli (del cf['key'];)
Flush 
Repair on a cluster with more than 1 node
Wait GCSeconds 
Compact
And the empty row would appear on Node 2

However, when I was able to get rid of the empty rows, I was following these steps on a single machine: 
Create a keyspace, column family, and a key
Delete the key
Flush
Sample write (writing to some temporary key)
Deleting the attribute to that temporary key (not the entire key)
Flush
Compact

or these steps:
Create a keyspace, column family, and a key
Delete the key
Flush 
Wait GCseconds
Compact

",,cburroughs,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/11 18:18;slebresne;RowIteration-unit-tests.patch;https://issues.apache.org/jira/secure/attachment/12473595/RowIteration-unit-tests.patch","14/Mar/11 18:18;slebresne;fix-RowIteratorFactory.patch;https://issues.apache.org/jira/secure/attachment/12473596/fix-RowIteratorFactory.patch","14/Mar/11 22:27;j.casares;nodeA.txt;https://issues.apache.org/jira/secure/attachment/12473620/nodeA.txt","14/Mar/11 22:27;j.casares;nodeB.txt;https://issues.apache.org/jira/secure/attachment/12473621/nodeB.txt",,,,,,,,,,,4.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20537,,,Fri Apr 08 22:23:58 UTC 2011,,,,,,,,,,"0|i0gafj:",93120,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"09/Mar/11 15:35;slebresne;I'm not able to reproduce. And I don't see why repair would screw up with tombstones collection. Only compaction should remove them and repair shouldn't have anything to do with this. If sstable2json is the only one to show it, it's even weirder (that is, listing with the cli should also show an empty row if there was problem with tombstone collection).

Are you sure you compacted node 2 (on which the empty row showed up) ? Are you sure there wasn't a Queue-2527-Compacted file alongside Queue-2527-Data.db ? Are you sure node 2 local time wasn't highly out of sync with node 1 ?;;;","11/Mar/11 10:30;slebresne;Ok, strongest hypothesis: this is a sign of CASSANDRA-2305.
Joaquim: was some row cache enabled on the incriminated CF ?;;;","11/Mar/11 17:35;j.casares;The entire thing was reproduced on a fresh install and I was just using the standard Keyspace that already exists, I believe. I'll go through and follow my steps again and give a more detailed account.;;;","14/Mar/11 13:44;slebresne;It's a bit of a shot in the dark since I'm not sure exactly how was produced and how to reproduce this, but I found a place in RowIteratorFactor where we don't handle a CF localDeletionTime correctly. Since RowIterator is used for repair and for sstable2json, it sounds like a promising candidate for this (and it's a legit problem even if not the one at hand here).

Patch is against 0.7.;;;","14/Mar/11 14:28;jbellis;how hard would it be to create a unit test that catches this?;;;","14/Mar/11 14:38;slebresne;Should be doable. I'll do that.;;;","14/Mar/11 16:50;slebresne;Attaching the unit test.;;;","14/Mar/11 17:09;jbellis;what's the significance of gc_grace_seconds: 2 in the test CF?  I don't see any sleeps in the test.;;;","14/Mar/11 17:19;slebresne;Sorry, that was me trying stuff and forgetting to remove it. I update the test to use an existing column with standard gc_grace;;;","14/Mar/11 18:18;slebresne;There was actually 2 related bugs in RowIteratorFactory. Re-attaching new patch with the 2 unit tests and the fix (fixing both problem and simplifying, I think, RowIteratorFactory).;;;","14/Mar/11 19:32;jbellis;that's a big improvement over the existing factory code!  committed.

we'll need a separate patch for 0.6 -- RowIteratorFactory doesn't exist (the analogous code is inline in getRangeSlice).;;;","14/Mar/11 19:48;hudson;Integrated in Cassandra-0.7 #378 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/378/])
    fix tombstone handling in repair andsstable2json
patch by slebresne; reviewed by jbellis for CASSANDRA-2279
;;;","14/Mar/11 22:27;j.casares;Here are all the commands that were run. I was messing around with T2 references and the bug didn't show up the first time, so I tried it again.

The second time it worked following the exact steps that I listed. I set the GCSeconds to be 30 minutes to wait for the repair to finish 100%.

This time however, the SSTables actually have the old values as well.;;;","18/Mar/11 14:30;slebresne;bq. we'll need a separate patch for 0.6

I don't think 0.6 suffers from the problem fixed by the attached patch. It uses CFStore.getFamily() for range slices to do its bidding which handles correctly the column family deletion times as far as I can tell.

Which make me think that there could be something else at hand here. I'll have a look at Joaquin instructions to try to reproduce what he is seeing. ;;;","30/Mar/11 22:11;j.casares;I just read this and was wondering if this may be the case:
http://wiki.apache.org/cassandra/Operations#Dealing_with_the_consequences_of_nodetool_repair_not_running_within_GCGraceSeconds

GCGraceSeconds was set to 30 minutes to allow the repair to finish and I waited 35 before running the compaction on the above steps.;;;","08/Apr/11 22:06;jbellis;compaction should be runnable any time, as long as you do repair before gcgrace expires.
;;;","08/Apr/11 22:15;slebresne;For the record, we did try to reproduce with Joaquin and weren't able to find anything wrong in there. In particular, in one instance when we though we had a column coming back from the dead, we were actually looking at a compacted sstable (which sstable2json won't avoid).

So I do not know if there is an actual problem here.;;;","08/Apr/11 22:23;jbellis;Sounds good, thanks for looking into that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool info NPE when node isn't fully booted,CASSANDRA-2270,12500386,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,wajam,wajam,04/Mar/11 01:16,16/Apr/19 09:33,14/Jul/23 05:52,04/Mar/11 20:43,0.7.4,,,,,,0,,,,"Running ""nodetool -h 127.0.0.1 info"" when the node is not yet ready throw a NPE.

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.gms.Gossiper.getCurrentGenerationNumber(Gossiper.java:313)
        at org.apache.cassandra.service.StorageService.getCurrentGenerationNumber(StorageService.java:1239)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:205)
",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"04/Mar/11 19:08;brandon.williams;2270.txt;https://issues.apache.org/jira/secure/attachment/12472697/2270.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20534,,,Fri Mar 04 23:34:27 UTC 2011,,,,,,,,,,"0|i0gadj:",93111,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"04/Mar/11 14:57;jbellis;By ""not yet ready"" do you mean early in normal startup, or started up with explicitly not joining ring?;;;","04/Mar/11 15:01;wajam;Early in normal startup, just run ""nodetool -h localhost info"" when the node its still opening index.;;;","04/Mar/11 19:08;brandon.williams;Patch to not check the generation when gossip is not initialized.;;;","04/Mar/11 19:12;jbellis;+1;;;","04/Mar/11 20:43;brandon.williams;Committed.;;;","04/Mar/11 23:34;hudson;Integrated in Cassandra-0.7 #349 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/349/])
    Fix NPE in nodetool when gossip isn't initialized.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-2270
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM in the Thrift thread doesn't shut down server,CASSANDRA-2269,12500381,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,03/Mar/11 23:46,16/Apr/19 09:33,14/Jul/23 05:52,15/Mar/11 19:48,0.6.13,0.7.5,,Legacy/CQL,,,0,,,,"Example:

{noformat}
java.lang.OutOfMemoryError: Java heap space
        at org.cliffc.high_scale_lib.NonBlockingHashMap$CHM.resize(NonBlockingHashMap.java:849)
        at org.cliffc.high_scale_lib.NonBlockingHashMap$CHM.access$200(NonBlockingHashMap.java:699)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:634)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:339)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.put(NonBlockingHashMap.java:302)
        at org.apache.cassandra.utils.ExpiringMap.put(ExpiringMap.java:112)
        at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:237)
        at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:305)
        at org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:386)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:347)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:92)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:175)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:254)
        at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:215)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:1272)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1166)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,"04/Mar/11 00:15;jbellis;2269-0.6.txt;https://issues.apache.org/jira/secure/attachment/12472634/2269-0.6.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19344,,,Fri Mar 18 19:14:11 UTC 2011,,,,,,,,,,"0|i0gadb:",93110,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"04/Mar/11 00:15;jbellis;patch to extract exception logging from DTPE and call from the Thrift executor.

(the actual shutdown is done by the default exception hook we set up -- it's not normally called on executor threads because both Future and executor afterExecute machinery swallow exceptions.);;;","04/Mar/11 04:24;tjake;Makes sense... Is there any known way to reproduce it to verify the fix infact works?;;;","04/Mar/11 04:32;jbellis;Adding ByteBuffer.allocateDirect(256 * 1024 * 1024) to any of the CassandraServer methods should do it;;;","15/Mar/11 19:21;tjake;+1;;;","15/Mar/11 19:48;jbellis;committed;;;","18/Mar/11 19:14;hudson;Integrated in Cassandra-0.6 #63 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/63/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql driver jar,CASSANDRA-2263,12500226,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,02/Mar/11 20:59,16/Apr/19 09:33,14/Jul/23 05:52,30/May/11 17:55,1.0.0,,,Legacy/CQL,,,1,cql,,,"Work was done in CASSANDRA-1848  to create a jar for the CQL Java driver.  The generated Thrfit code was broken out into it's own jar as well, since that is a dependency for both servers and clients. However, based on the work currently happening in CASSANDRA-2262 and CASSANDRA-2124, it seems that additional dependencies will exist, and new jar(s) will need to be created.

The easiest way to fix this will probably be to put copies of all of {{o.a.c.db.marshal}} and {{o.a.c.utils}}, and a copy of {{o.a.c.config.ConfigurationException}} into the CQL driver jar (a split along those lines to create another jar doesn't make sense IMO).",,,,,,,,,,,,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,"30/May/11 15:53;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2263-don-t-ship-jdbc-in-main-jar.txt;https://issues.apache.org/jira/secure/attachment/12480856/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2263-don-t-ship-jdbc-in-main-jar.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20533,,,Tue May 31 22:43:04 UTC 2011,,,,,,,,,,"0|i0gabz:",93104,,,,,Low,,,,,,,,,,,,,,,,,"28/May/11 19:45;larswunderlich;I don't know whether this issue should be documented in a separate bug for 0.8.0-rc1, but since this one deals with separation of jar file, I add the comment here.

The following code fails on my machine:

Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
Connection c = DriverManager.getConnection(""jdbc:cassandra:/@localhost:9160/testspace"");

As of apache-cassandra-0.8.0-rc1 together with apache-cassandra-cql-1.0.2.jar the connection to local host couldn't be established, even though it was running and the keyspace was fine, because of the following reasons:

1.) cql jar requires direct classpath relationship to apache-cassandra-0.8.0-rc1.jar, without it cannot run at all, what contradicts server implementation encapsulation/secret in my mind to attach core jar file: 

Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/cassandra/db/marshal/AbstractType
	at org.apache.cassandra.cql.jdbc.Connection.execute(Connection.java:142)
	at org.apache.cassandra.cql.jdbc.Connection.execute(Connection.java:124)
	at org.apache.cassandra.cql.jdbc.CassandraConnection.<init>(CassandraConnection.java:83)
	at org.apache.cassandra.cql.jdbc.CassandraDriver.connect(CassandraDriver.java:86)
	at java.sql.DriverManager.getConnection(Unknown Source)
	at java.sql.DriverManager.getConnection(Unknown Source)
	at TestConnection.main(TestConnection.java:18)

2.) apache-cassandra-0.8.0-rc1.jar internally contains org.apache.cassandra.cql.jdbc package a second time, which might conflict with the cql standalone jar version of the driver in terms of class compatibility.

3.) Using CassandraDriver from core apache-cassandra-0.8.0-rc1.jar results in an error message that cassandra.yaml couldn't be found, but cassandra.yaml is not required from my point of view for clients anyway.

Cannot locate cassandra.yaml
Fatal configuration error; unable to start server.  See log for stacktrace.;;;","29/May/11 18:56;larswunderlich;Connection problem could be solved by passing parameter -Dcassandra.config=cassandra.yaml when starting test case and adding cassandra.yaml file as copy to classpath. However, cql driver couldn't be started out of the box and I haven't found documentation including code examples how to successfully setup a working JDBC connection as of now.  ;;;","30/May/11 15:55;urandom;Ultimately, we don't want to have to depend on the main jar, but the fact that this is the case now, is known.  I think the problem here (as you already suggested), is that there is also a copy in the cassandra jar.  The attached patch remedies that.;;;","30/May/11 16:16;jbellis;+1;;;","30/May/11 17:55;urandom;committed;;;","31/May/11 03:56;jbellis;I think this broke the jdbc tests post-clean:

{noformat}
build-test:
    [javac] /Users/jonathan/projects/cassandra/svn-0.8.0/build.xml:973: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
    [javac] Compiling 124 source files to /Users/jonathan/projects/cassandra/svn-0.8.0/build/test/classes
    [javac] /Users/jonathan/projects/cassandra/svn-0.8.0/drivers/java/test/org/apache/cassandra/cql/jdbc/PreparedStatementTest.java:357: cannot find symbol
    [javac] symbol  : class CassandraPreparedStatement
    [javac] location: class org.apache.cassandra.cql.jdbc.PreparedStatementTest
    [javac]         CassandraPreparedStatement stmt = (CassandraPreparedStatement)con.prepareStatement(q);
    [javac]         ^
{noformat};;;","31/May/11 22:43;urandom;Gah, I'd fixed this locally but forgot to commit.  It's there now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
column values are only being validated in insert(),CASSANDRA-2259,12500108,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mishravivek,urandom,urandom,01/Mar/11 22:06,16/Apr/19 09:33,14/Jul/23 05:52,08/Mar/11 17:31,0.7.4,,,Legacy/CQL,,,0,,,,"insert() is the only code path that currently results in validate() being called for column values; it is possible to write invalid column values using batch_mutate()",,mishravivek,stuhood,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"07/Mar/11 16:42;jbellis;2259-v2.txt;https://issues.apache.org/jira/secure/attachment/12472833/2259-v2.txt","07/Mar/11 08:17;mishravivek;CASSANDRA-2259_v1.0;https://issues.apache.org/jira/secure/attachment/12472806/CASSANDRA-2259_v1.0",,,,,,,,,,,,,2.0,mishravivek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20532,,,Tue Mar 08 17:28:46 UTC 2011,,,,,,,,,,"0|i0gab3:",93100,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"07/Mar/11 08:19;mishravivek;@Eric:

Attached code change.
Changes are:
1) introduced a new method validateColumnValue().
2) On call to ThriftValidation.validateMutation() is modified to invoke validateColumnValue().

Patch attached for review.

Let me know if this fine then i can pick up this issue.;;;","07/Mar/11 16:42;jbellis;v2 attached:

- adds test that fails pre-fix
- uses existing validateColumn instead of creating a new identical method
- moves validateColumns call out of validateColumn, and renames to validateColumnNames
- renames validateColumn to validateColumnData
- adds comments to main methods in TV;;;","07/Mar/11 18:47;mishravivek;Jonathan,
Possible to add a svn patch file?;;;","07/Mar/11 18:54;jbellis;That's what I attached.;;;","07/Mar/11 19:06;mishravivek;+1 for change.Modifying and reusing method is good thing.


Only thing is if we only column Family in validateColumnData() method. Then we can avoid : 

validateColumnData(keyspace, new ColumnParent(cfName), cosc.column); (creating ColumnParent instance). 
in validateColumnOrSuperColumn() method.;;;","08/Mar/11 16:48;jbellis;committed w/ your improvement, thanks!;;;","08/Mar/11 17:06;hudson;Integrated in Cassandra-0.7 #357 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/357/])
    validate column values in batches aswell assingle-Column inserts
patch by Vivek Mishra and jbellis for CASSANDRA-2259
;;;","08/Mar/11 17:28;mishravivek;Should i mark it as resolve?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
service.SerializationsTest failes under cobertura,CASSANDRA-2258,12500103,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,jbellis,jbellis,01/Mar/11 21:39,16/Apr/19 09:33,14/Jul/23 05:52,15/Mar/11 13:07,0.7.5,,,Legacy/Testing,,,0,,,,"ant codecoverage -Dtest.name=SerializationsTest gives

{noformat}
    [junit] Testcase: testTreeResponseRead(org.apache.cassandra.service.SerializationsTest):	Caused an ERROR
    [junit] java.io.InvalidClassException: org.apache.cassandra.dht.BigIntegerToken; local class incompatible: stream classdesc serialVersionUID = -5833589141319293006, local class serialVersionUID = 2280189098581028124
    [junit] java.lang.RuntimeException: java.io.InvalidClassException: org.apache.cassandra.dht.BigIntegerToken; local class incompatible: stream classdesc serialVersionUID = -5833589141319293006, local class serialVersionUID = 2280189098581028124
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.deserialize(AntiEntropyService.java:634)
    [junit] 	at org.apache.cassandra.service.SerializationsTest.testTreeResponseRead(SerializationsTest.java:90)
    [junit] Caused by: java.io.InvalidClassException: org.apache.cassandra.dht.BigIntegerToken; local class incompatible: stream classdesc serialVersionUID = -5833589141319293006, local class serialVersionUID = 2280189098581028124
    [junit] 	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:562)
    [junit] 	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1582)
    [junit] 	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1495)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1731)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.deserialize(AntiEntropyService.java:630)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/11 21:02;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-do-not-instrument-the-Token-classes.txt;https://issues.apache.org/jira/secure/attachment/12473614/ASF.LICENSE.NOT.GRANTED--v1-0001-do-not-instrument-the-Token-classes.txt","14/Mar/11 20:53;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-fix-cobertura-brokenness-by-forcing-serialVersionUIDs.txt;https://issues.apache.org/jira/secure/attachment/12473610/ASF.LICENSE.NOT.GRANTED--v1-0001-fix-cobertura-brokenness-by-forcing-serialVersionUIDs.txt",,,,,,,,,,,,,2.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20531,,,Wed Mar 16 00:31:14 UTC 2011,,,,,,,,,,"0|i0gaav:",93099,,,,,Low,,,,,,,,,,,,,,,,,"14/Mar/11 21:04;gdusbabek;Several problems here:
1.  AntiEntropyService is using serialized java objects.
2.  Cobertura doesn't preserve serialVersionUID during instrumentation.

I've attached two possible fixes, neither of them very impressive.

The first forces the serialVersionUID for all Token descendants to 1L.  This is bad because it breaks wire compatibility between $THIS_VERSION and 0.7.x (see CASSANDRA-1015 for why we shouldn't allow this).

The second disables instrumenting *Token.class during the instrumentation phase.  Upshot is that we don't get code coverage reports for those classes.;;;","14/Mar/11 22:10;jbellis;bq. The second disables instrumenting *Token.class during the instrumentation phase. Upshot is that we don't get code coverage reports for those classes.

+1 this approach if you add a comment to build.xml for posterity :);;;","15/Mar/11 13:07;gdusbabek;committed;;;","15/Mar/11 16:46;hudson;Integrated in Cassandra-0.7 #382 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/382/])
    ;;;","16/Mar/11 00:31;hudson;Integrated in Cassandra #781 (See [https://hudson.apache.org/hudson/job/Cassandra/781/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BRAF assertion error,CASSANDRA-2256,12499978,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,jbellis,jbellis,28/Feb/11 21:29,16/Apr/19 09:33,14/Jul/23 05:52,03/Mar/11 16:44,0.7.4,,,,,,0,,,,"While investigating CASSANDRA-2240 I ran into this:

{noformat}
java.lang.AssertionError
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.read(BufferedRandomAccessFile.java\
:230)
        at java.io.RandomAccessFile.readByte(RandomAccessFile.java:589)
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:273)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:539)
{noformat}",,stuhood,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,"02/Mar/11 22:09;xedin;CASSANDRA-2256-v2.patch;https://issues.apache.org/jira/secure/attachment/12472476/CASSANDRA-2256-v2.patch","02/Mar/11 19:30;xedin;CASSANDRA-2256.patch;https://issues.apache.org/jira/secure/attachment/12472452/CASSANDRA-2256.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20529,,,Thu Mar 03 16:27:39 UTC 2011,,,,,,,,,,"0|i0gaaf:",93097,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"02/Mar/11 17:52;xedin;Test for the problem:
{code}
public void testAssertOnRead() throws IOException
    {
        BufferedRandomAccessFile file = createTempFile(""braf"");
        file.write(new byte[10]);
        file.sync();

        BufferedRandomAccessFile copy = new BufferedRandomAccessFile(file.getPath(), ""r"");

        copy.seek(15);
        copy.read();

        file.close();
        copy.close();
    }
{code}

This happens when you are trying to seek to position > file length on read-only file and then read (because fileLength is cached, method isEOF() does not work properly). I don't think that we should allow such seeks.;;;","02/Mar/11 19:02;jbellis;Technically RAF.seek allows seeking beyond EOF and writing there (presumably the intervening space would be filled with 0?) but Cassandra doesn't use this and it's kind of a weird corner case.  There's a pretty strong assumption in BRAF that current <= EOF.

So, I would be okay with throwing EOFException if you try to seek past EOF.;;;","02/Mar/11 19:36;jbellis;Is there a way to do this w/o calling channel.size() for each seek?  We seek twice for every row written, and channel.size() is fairly expensive.;;;","02/Mar/11 19:41;xedin;There is no way to skip this unless we will check this only for read-only files.;;;","02/Mar/11 19:50;tjake;throwing EOF is good.

Regarding the check of length on every seek for writable files, I think you could change it to see if the seek is < bufferOffset + buffer.length before calling length()

;;;","02/Mar/11 20:11;xedin;It is also half-solving, I think we can check that for read-only files and for writable we can leave this as is (makes perfect sense), any counter-argument?;;;","02/Mar/11 20:19;tjake;The counter argument is we call seek() 2 times for every row in SSTableWriter.

So we need to make sure we don't call stat() unless we have no other choice.
Another approach would be to set fileLength based on the total number of bytes from the starting size, or 0 for a new file...;;;","02/Mar/11 20:36;xedin;if we will be doing this for read-only condition will be 
{code}
if (fileLength != -1 && newPosition > fileLength)
    throw new EOFException();
{code}

this won't call channel.size() or do any expensive calculations like length();;;","02/Mar/11 20:45;tjake;Oh so you mean let the it throw an AssertionError if you try to seek beyond the end of a file in ""rw"" mode?;;;","02/Mar/11 20:48;jbellis;Not a fan of relying on assert when we really want EOFException.;;;","02/Mar/11 20:55;xedin;Seems that we don't understand each other here :)

seek method will look like this:

{code}
    public void seek(long newPosition) throws IOException
    {
        if (newPosition < 0)
            throw new IllegalArgumentException(""new position should not be negative"");

        if (fileLength != -1 && newPosition > fileLength)
            throw new EOFException(""unable to seek past the end of the file in read-only mode."");

        current = newPosition;

        if (newPosition >= bufferOffset + validBufferBytes || newPosition < bufferOffset)
            reBuffer(); // this will set bufferEnd for us
    }
{code}

We set fileLength = -1 for ""rw"" mode and caching fileLength = channel.size() for ""r"" mode files, so condition ""fileLength != -1 && newPosition > fileLength"" will allow us to block seeking past the end of the file in ""r"" mode leaving ""rw"" untouched (which makes a good sense even if it's unused right now and lets us avoid calling length() for every seek()).;;;","02/Mar/11 21:00;tjake;Right, but what happens if you try to seek past the end of a file in ""rw"" mode?;;;","02/Mar/11 21:04;xedin;That will be allowed as it is right now but that won't create any problems because length of the file is dynamic in that case and isEOF() method will be working properly.;;;","02/Mar/11 21:23;tjake;If that's tested and proven then I don't see an issue with using the code snippet above.;;;","02/Mar/11 21:27;xedin;Yes, it is. I will attach v2 patch asap.;;;","03/Mar/11 16:06;jbellis;committed with additional test that writing past EOF actually works;;;","03/Mar/11 16:27;hudson;Integrated in Cassandra-0.7 #341 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/341/])
    throw EOFException when seeking past EOF in read-only mode
patch by Pavel Yaskevich; reviewed by tjake and jbellis for CASSANDRA-2256
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyOutputFormat drops mutations when batches fill up.,CASSANDRA-2255,12499973,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jeromatron,eldondev,eldondev,28/Feb/11 21:06,16/Apr/19 09:33,14/Jul/23 05:52,01/Mar/11 02:52,0.7.3,,,,,,1,,,,"queue.poll() takes a mutation,
but then the batch is already full,
so the while loop exits, ant the mutation we just got is dropped.",,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/11 21:22;eldondev;0001_Stop_dropping_mutations.txt;https://issues.apache.org/jira/secure/attachment/12472233/0001_Stop_dropping_mutations.txt","01/Mar/11 00:35;jeromatron;2255-patch-2.txt;https://issues.apache.org/jira/secure/attachment/12472260/2255-patch-2.txt",,,,,,,,,,,,,2.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20528,,,Tue Mar 01 04:15:26 UTC 2011,,,,,,,,,,"0|i0gaa7:",93096,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"28/Feb/11 21:07;eldondev;This should help.;;;","28/Feb/11 21:22;eldondev;Should apply cleanly to 0.7.0 tag;;;","28/Feb/11 21:32;brandon.williams;Committed, thanks!;;;","28/Feb/11 21:56;hudson;Integrated in Cassandra-0.7 #332 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/332/])
    CFRW no longer loses mutations.
Patch by Eldon Stegall, reviewed by Stu Hood and brandonwilliams for
CASSANDRA-2255
;;;","28/Feb/11 22:55;jbellis;Doesn't this mean batch sizes can be arbitrarily large?  Not only will this cause latency spikes on the server but you could OOM building the monster batch.;;;","28/Feb/11 22:56;jbellis;also: the mutation = null; line is dead code.;;;","28/Feb/11 23:42;jbellis;reverted;;;","01/Mar/11 00:34;jeromatron;another go at the patch;;;","01/Mar/11 02:52;jbellis;lgtm, committed (we'll try to get it in to 0.7.3 re-spin);;;","01/Mar/11 04:15;hudson;Integrated in Cassandra-0.7 #334 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/334/])
    fix Hadoop ColumnFamilyOutputFormat droppingof mutations
patch by Eldon Stegall and Jeremy Hanna; reviewed by jbellis for CASSANDRA-2255
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assert when using CL.EACH_QUORUM,CASSANDRA-2254,12499882,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,segy,segy,segy,28/Feb/11 00:42,16/Apr/19 09:33,14/Jul/23 05:52,28/Feb/11 01:50,0.7.4,,,,,,0,,,,"When using the NetworkTopology strategy, I am able to write using CL.LOCAL_QUORUM. When I attempt to write using CL.EACH_QUORUM, an assert is hit in DatacenterSyncWriteResponseHandler. Tracing the call through to the NetworkTopology code, it seems that this particular handler is only used when CL = EACH_QUORUM, yet the code asserts. ",RH Linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/11 00:55;segy;0001-Fixing-CASSANDRA-2254-DatacenterSyncWriteResponseHea.patch;https://issues.apache.org/jira/secure/attachment/12472132/0001-Fixing-CASSANDRA-2254-DatacenterSyncWriteResponseHea.patch",,,,,,,,,,,,,,1.0,segy,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20527,,,Mon Feb 28 02:06:24 UTC 2011,,,,,,,,,,"0|i0ga9z:",93095,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"28/Feb/11 00:55;segy;Patch that changes the assert to match what I believe is the intent of the check, basically s/LOCAL_QUORUM/EACH_QUORUM/.;;;","28/Feb/11 01:50;jbellis;committed, thanks!;;;","28/Feb/11 02:06;hudson;Integrated in Cassandra-0.7 #326 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/326/])
    fix assert in DatacenterSyncWriteResponseHandler
patch by Mark Guzman; reviewed by jbellis for CASSANDRA-2254
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossiper Starvation,CASSANDRA-2253,12499862,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mikaels,mikaels,mikaels,27/Feb/11 16:03,16/Apr/19 09:33,14/Jul/23 05:52,28/Feb/11 23:53,0.7.3,,,,,,0,,,,"Gossiper periodic task will get into starvation in case large sstable files need to be deleted.
Indeed the SSTableDeletingReference uses the same scheduledTasks pool (from StorageService) as the Gossiper and other periodic tasks, but the gossiper tasks should run each second to assure correct cluster status (liveness of nodes). In case of large sstable files to be deleted (several GB) the delete operation can take more than 30 sec, thus making the whole cluster going into a wrong state where nodes are marked as not living while they are!
This will lead to unneeded additional load like hinted hand off, wrong cluster state, increase in latency.

One of the possible solution is to use a separate pool for periodic and non periodic tasks. 
I've implemented such change and it resolves the problem. 
I can provide a patch ","linux, windows",mikaels,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"28/Feb/11 23:02;mikaels;CASSANDRA-0.7-2253.txt;https://issues.apache.org/jira/secure/attachment/12472246/CASSANDRA-0.7-2253.txt",,,,,,,,,,,,,,1.0,mikaels,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20526,,,Tue Mar 01 00:04:48 UTC 2011,,,,,,,,,,"0|i0ga9r:",93094,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"27/Feb/11 16:57;jbellis;bq. use a separate pool for periodic and non periodic tasks

that's reasonable; so might splitting Gossiper off to its own executor;;;","27/Feb/11 18:05;mikaels;I also though having gossiper in its own executor, nevertheless it means that other periodic tasks may come to starvation because of the heavy non periodic tasks. Therefore i chose to use a separate pool for the heavy one instead.
;;;","27/Feb/11 22:44;jbellis;Sounds good to me.  Were you going to submit a patch?;;;","28/Feb/11 03:43;mikaels;yes;;;","28/Feb/11 23:01;mikaels;Add a new pool for non periodic heavyweight tasks, to eliminate the starvation of periodic short time execution task like Gossiper.
Additionally add debug statement for periodic and non periodic task ;;;","28/Feb/11 23:02;mikaels;patch for bug 2253, Gossip starvation;;;","28/Feb/11 23:53;jbellis;committed the executor change

omitted the debug log statements; the ""right"" way to do that is to create an executor subclass that logs that for us instead of relying on boilerplate code in the caller;;;","01/Mar/11 00:04;hudson;Integrated in Cassandra-0.7 #333 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/333/])
    movefile deletions off of scheduledtasks executor
patch by Mikael Sitruk; reviewed by jbellis for CASSANDRA-2253
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unhelpful exception when failing to set keyspace,CASSANDRA-2251,12499749,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,25/Feb/11 19:27,16/Apr/19 09:33,14/Jul/23 05:52,25/Feb/11 22:06,,,,,,,0,,,,"If you fail to set the keyspace, {{ThriftValidation.validateColumnFamily()}} raises an {{AssertionError}}, which remotely results in a {{TApplicationException}}. ",,,,,,,,,,,,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,"25/Feb/11 21:35;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2251-raise-IRE-for-null-keyspace.txt;https://issues.apache.org/jira/secure/attachment/12471976/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2251-raise-IRE-for-null-keyspace.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20525,,,Fri Feb 25 22:53:30 UTC 2011,,,,,,,,,,"0|i0ga9b:",93092,,,,,Normal,,,,,,,,,,,,,,,,,"25/Feb/11 19:59;jbellis;maybe we should move the check into getKeyspace?  there's a bunch of callers that don't call vCF afterwards.;;;","25/Feb/11 21:36;urandom;bq. maybe we should move the check into getKeyspace? there's a bunch of callers that don't call vCF afterwards.

That'll work too.  New patch attached.;;;","25/Feb/11 21:42;jbellis;+1;;;","25/Feb/11 22:06;urandom;committed.;;;","25/Feb/11 22:53;hudson;Integrated in Cassandra #745 (See [https://hudson.apache.org/hudson/job/Cassandra/745/])
    raise IRE for null keyspace

Patch by eevans; reviewed by jbellis for CASSANDRA-2251
raise IRE for null keyspace argument

Patch by eevans; reviewed by jbellis for CASSANDRA-2251
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant javadoc fails on windows,CASSANDRA-2248,12499705,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,norman,norman,norman,25/Feb/11 12:47,16/Apr/19 09:33,14/Jul/23 05:52,25/Feb/11 15:06,0.7.3,,,Packaging,,,0,,,,"When try to run ""ant javadoc"" (or any task that include javadoc) on windows it fails with the error:

Javadoc failed: java.io.IOException: Cannot run program ""c:\Program Files\Java\jdk1.6.0_17\bin\javadoc.exe"": CreateProcess error=87, The parameter is incorrect","windows 7, ant 1.8.2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/11 12:49;norman;CASSANDRA-2248.diff;https://issues.apache.org/jira/secure/attachment/12471927/CASSANDRA-2248.diff",,,,,,,,,,,,,,1.0,norman,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20524,,,Fri Feb 25 15:37:46 UTC 2011,,,,,,,,,,"0|i0ga8n:",93089,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"25/Feb/11 12:49;norman;This fix the build on windows.;;;","25/Feb/11 13:01;norman;Related to this: https://issues.apache.org/bugzilla/show_bug.cgi?id=41958;;;","25/Feb/11 15:06;jbellis;committed, thanks!;;;","25/Feb/11 15:37;hudson;Integrated in Cassandra-0.7 #322 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/322/])
    fix ant javadoc on Windows
patch by Norman Maurer; reviewed by jbellis for CASSANDRA-2248
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
secondary indexes aren't created on pre-existing or streamed data,CASSANDRA-2244,12499629,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,24/Feb/11 19:29,16/Apr/19 09:33,14/Jul/23 05:52,03/Mar/11 17:42,0.7.4,,,Feature/2i Index,,,0,,,,"The repaired node neither receives indexes from the replicas, nor does it generate them afterwards.  The same bug prevents generation of new indexes against existing data.",,jborgstrom,skamio,stuhood,,,,,,,,,,,,,,,,,,,57600,57600,,0%,57600,57600,,,,,,,,,,,,,,,,"03/Mar/11 17:04;jbellis;2244-v2.txt;https://issues.apache.org/jira/secure/attachment/12472561/2244-v2.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20523,,,Thu Mar 03 17:55:22 UTC 2011,,,,,,,,,,"0|i0ga7r:",93085,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"28/Feb/11 07:31;jbellis;Doesn't repair use normal Streaming?  That should take care of generating the index CFs.;;;","01/Mar/11 23:33;brandon.williams;I added some logging, it does do something after streaming:

{noformat}
 INFO 23:27:32,050 Opening /var/lib/cassandra/data/Keyspace1/Standard1-f-1
 INFO 23:27:34,843 Opening /var/lib/cassandra/data/Keyspace1/Standard1-f-2
 INFO 23:27:35,232 Opening /var/lib/cassandra/data/Keyspace1/Standard1-f-3
 INFO 23:27:35,246 Building index for ColumnFamilyStore(table='Keyspace1', columnFamily='Standard1') [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-f-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-f-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-f-3-Data.db')] [java.nio.HeapByteBuffer[pos=0 lim=2 cap=2]]
 INFO 23:29:53,448 Finished streaming session 19283352579402862 from /10.179.65.102
{noformat}

During the ~2.5 minutes between the last 2 lines it appeared to generate the index, however flushing afterwards shows no index was generated:

{noformat}
 INFO 23:32:06,591 switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/var/lib/cassandra/commitlog/CommitLog-1299022015229.log', position=12763)
 INFO 23:32:06,592 Enqueuing flush of Memtable-LocationInfo@1459852990(35 bytes, 1 operations)
 INFO 23:32:06,593 Writing Memtable-LocationInfo@1459852990(35 bytes, 1 operations)
 INFO 23:32:06,623 Completed flushing /var/lib/cassandra/data/system/LocationInfo-f-3-Data.db (89 bytes)
{noformat}


;;;","03/Mar/11 16:34;jbellis;The ""don't bother flushing dropped CFs"" check was unaware that index CFs are never part of the global CF metadata.

(I think that check is race-prone to begin with, so we should probably drop it and do something more robust, but for now it's better than nothing -- it will *usually* prevent creating new sstables for dropped CFs.);;;","03/Mar/11 17:04;jbellis;Gary points out that migration does acquire the flushlock.;;;","03/Mar/11 17:04;jbellis;patch adds test that catches the problem, and adds fix to CFS.;;;","03/Mar/11 17:42;brandon.williams;+1, committed;;;","03/Mar/11 17:55;hudson;Integrated in Cassandra-0.7 #342 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/342/])
    CFS correctly flushes index CFs.
Patch by jbellis, reviewed by brandonwilliams for CASSANDRA-2244
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"fix ""ant codecoverage""",CASSANDRA-2243,12499624,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stephenc,jbellis,jbellis,24/Feb/11 19:05,16/Apr/19 09:33,14/Jul/23 05:52,27/Feb/11 02:42,0.7.3,,,,,,0,build,,,,,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/11 01:41;stephenc;CASSANDRA-2243.patch;https://issues.apache.org/jira/secure/attachment/12472082/CASSANDRA-2243.patch",,,,,,,,,,,,,,1.0,stephenc,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20522,,,Sun Feb 27 03:06:26 UTC 2011,,,,,,,,,,"0|i0ga7j:",93084,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"27/Feb/11 01:43;stephenc;attached patch fixes cobertura code coverage and leverages the maven local repo to avoid having to specify cobertura jar location via a system property and also avoid shipping the GPL artifact (i.e. cobertura.jar) in the Apache distribution;;;","27/Feb/11 02:10;stephenc;hmmm the whole way cobertura instrumentation had been implemented is a little messy and with this fix I have concerns about GPL leaking as cobertura is always on the classpath even for non-instrumented tests. I think the testmacro needs to be tweaked;;;","27/Feb/11 02:23;stephenc;OK, here is my concern.

Prior to this fix, the codecoverage target would only work if you ran something like

ant codecoverage -Dcobertura.dir=/path/to/cobertura.jar

if you just ran 

ant codecoverage

then it would fail.

The reasoning behind the above is to ensure that cobertura.jar is never bundled in the -bin.tar.gz or -src.tar.gz that we ship (due to cobertura.jar having GPL code)

With this patch, we never have the risk of bundling cobertura.jar as it remains safely in ~/.m2/repository/net/sourceforge/cobertura/... and never comes near the build directory...

However there is a side-effect, namely now the ant target ""test"" will always run with cobertura.jar on the classpath.

This should be minimal risk, as you are not bundling the test classpath, but it is up to the Cassandra PMC to decide on that risk as the Cassandra PMC has to approve the releases;;;","27/Feb/11 02:42;jbellis;committed, thanks!;;;","27/Feb/11 03:06;hudson;Integrated in Cassandra-0.7 #325 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/325/])
    fix ""ant codecoverage""
patch by Stephen Connolly; tested by jbellis for CASSANDRA-2243
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BRAF read can loop infinitely instead of detecting EOF,CASSANDRA-2241,12499622,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,24/Feb/11 18:53,16/Apr/19 09:33,14/Jul/23 05:52,25/Feb/11 15:00,0.7.3,,,,,,0,,,,"(marking this Minor since normally we never try to read past the end of an SSTable, but CASSANDRA-2240 is running into it.)",,,,,,,,,,,,,,,,,,,,,,,21600,21600,,0%,21600,21600,,,,,,,,,,,,,,,,"24/Feb/11 18:59;jbellis;2241.txt;https://issues.apache.org/jira/secure/attachment/12471862/2241.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19345,,,Fri Feb 25 15:15:13 UTC 2011,,,,,,,,,,"0|i0ga73:",93082,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"24/Feb/11 18:59;jbellis;Adds tests for this problem and similar ones around EOF behavior.

Fixes bug and makes some changes to make BRAF more straightforward:

- ByteBuffer buffer -> byte[] (no more confusion between ""current"" and buffer's internal markers)

- Instead of bufferEnd (easy to confuse with bufferOffset + buffer.length), use validBufferBytes

- removes hitEOF

- gives read() the behavior of readAtMost(), which was the behavior read() is *supposed* to have

- avoid allocating new byte[1] for each call to write(int)

- adds asserts for expected internal state

- fixes length() when we seek to an earlier position that is still inside the current buffer;;;","24/Feb/11 19:00;jbellis;(At least some of these are regressions introduced by CASSANDRA-1470.);;;","24/Feb/11 20:32;tjake;I don't see anything here that is a problem, but I'll run it through some workloads...;;;","25/Feb/11 01:27;tjake;Ok, I ran this through some more tests and it looks good to me +1;;;","25/Feb/11 15:00;jbellis;committed;;;","25/Feb/11 15:15;hudson;Integrated in Cassandra-0.7 #321 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/321/])
    fix BufferedRandomAccessFile bugs
patch by jbellis; reviewed by tjake for CASSANDRA-2241
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool scrub hangs or throws an exception,CASSANDRA-2240,12499601,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,kunda,kunda,24/Feb/11 15:32,16/Apr/19 09:33,14/Jul/23 05:52,01/Mar/11 16:10,0.7.3,,,Tool/nodetool,,,0,,,,"trying to run nodetool scrub hung or (only happened one time) threw the following exception:

ERROR [CompactionExecutor:1] 2011-02-28 10:26:26,620 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.dht.RandomPartitioner.convertFromDiskFormat(RandomPartitioner.java:62)
        at org.apache.cassandra.io.sstable.SSTableReader.decodeKey(SSTableReader.java:627)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:538)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
",using build #314 from hudson,kunda,wajam,,,,,,,,,,,,,,,,,,,,57600,57600,,0%,57600,57600,,,,,,,,CASSANDRA-2217,,,,,,,,"01/Mar/11 02:49;jbellis;2240-v2.txt;https://issues.apache.org/jira/secure/attachment/12472270/2240-v2.txt","01/Mar/11 04:45;jbellis;2240-v3.txt;https://issues.apache.org/jira/secure/attachment/12472278/2240-v3.txt","01/Mar/11 13:22;slebresne;2240-v5.patch;https://issues.apache.org/jira/secure/attachment/12472305/2240-v5.patch","01/Mar/11 14:46;jbellis;2240-v6.txt;https://issues.apache.org/jira/secure/attachment/12472313/2240-v6.txt","01/Mar/11 05:30;jbellis;2240.txt;https://issues.apache.org/jira/secure/attachment/12472279/2240.txt","28/Feb/11 23:16;jbellis;2240.txt;https://issues.apache.org/jira/secure/attachment/12472250/2240.txt","27/Feb/11 10:44;kunda;exception2.txt;https://issues.apache.org/jira/secure/attachment/12472102/exception2.txt","27/Feb/11 10:34;kunda;jstack1.txt;https://issues.apache.org/jira/secure/attachment/12472100/jstack1.txt","27/Feb/11 10:34;kunda;signatureBuckets-f-104.tar.gz;https://issues.apache.org/jira/secure/attachment/12472101/signatureBuckets-f-104.tar.gz","28/Feb/11 08:07;kunda;system.log.2.gz;https://issues.apache.org/jira/secure/attachment/12472152/system.log.2.gz","28/Feb/11 08:01;kunda;system.log.gz;https://issues.apache.org/jira/secure/attachment/12472151/system.log.gz","24/Feb/11 15:41;kunda;test-0.6.x-tables.tar.gz;https://issues.apache.org/jira/secure/attachment/12471844/test-0.6.x-tables.tar.gz","27/Feb/11 10:44;kunda;userChannelFilter-f-210.tar.gz;https://issues.apache.org/jira/secure/attachment/12472103/userChannelFilter-f-210.tar.gz",,13.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20521,,,Tue Mar 01 20:15:08 UTC 2011,,,,,,,,,,"0|i0ga6v:",93081,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"24/Feb/11 15:37;jbellis;what is going on in the compactionmanager when it's ""hung?""  (use jstack);;;","24/Feb/11 15:41;kunda;attached the tables that can be used to reproduce the hang;;;","24/Feb/11 16:03;jbellis;This is not a valid sstable.  It claims (from its lack of version string) that it contains encoded row keys, meaning <token>:<key>, but it actually does not.  scrub can't help you with that.;;;","24/Feb/11 16:07;jbellis;... on closer inspection it does have colon-delimited keys, but scrub doesn't see them.  ;;;","24/Feb/11 16:08;kunda;Here's the trace I got (narrowed down):

""CompactionExecutor:1"" prio=10 tid=0x000000001eb67800 nid=0x7fdb runnable [0x0000000040dc4000]
   java.lang.Thread.State: RUNNABLE
        at java.io.DataInputStream.readFully(DataInputStream.java:195)
        at java.io.DataInputStream.readLong(DataInputStream.java:416)
        at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:51)
        at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:30)
        at org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:108)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:86)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:549)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

   Locked ownable synchronizers:
        - <0x00002aaabd5f71c8> (a java.util.concurrent.ThreadPoolExecutor$Worker)
        - <0x00002aaabd6cd008> (a java.util.concurrent.locks.ReentrantLock$NonfairSync);;;","24/Feb/11 19:37;jbellis;What do you see when you apply the patch for CASSANDRA-2241?

What create column family statement should I give the cli, to create this userActionUtilsKey CF?;;;","24/Feb/11 19:46;jbellis;also, can you attach the system.log from when you started cassandra?  we're trying to figure out why it's using the new-version BloomFilterSerializer, when it should be using LegacyBloomFilterSerializer.;;;","24/Feb/11 19:50;kunda;I didn't have time to apply the patch yet - but I will next week.
Regarding the example CF - as [~slebresne] commented in CASSANDRA-2217, it was indeed created in Cassandra 0.6.5 -
so the cli cannot be used to create it ;)

However, here is KS xml element that defined it in storage-conf.xml:
{code:xml} 
    <Keyspace Name=""generalUtils"">
      <ColumnFamily Name=""userActionUtilsKey"" CompareWith=""UTF8Type"" />
      <ColumnFamily Name=""facebookShowIds"" CompareWith=""UTF8Type"" />

      <ReplicaPlacementStrategy>org.apache.cassandra.locator.RackUnawareStrategy</ReplicaPlacementStrategy>
      <ReplicationFactor>1</ReplicationFactor>
      <EndPointSnitch>org.apache.cassandra.locator.EndPointSnitch</EndPointSnitch>
    </Keyspace>
{code};;;","24/Feb/11 19:50;jbellis;When I create a keyspace and CF w/ default settings and scrub it, I get

{noformat}
 INFO 13:49:17,594 Scrubbing SSTableReader(path='/var/lib/cassandra/data/Keyspace1/userActionUtilsKey-9-Data.db')
 INFO 13:49:17,856 Scrub of SSTableReader(path='/var/lib/cassandra/data/Keyspace1/userActionUtilsKey-9-Data.db') complete
{noformat};;;","24/Feb/11 20:14;kunda;I had to go back 26 x 20MiB logs consisting mostly of lines such as:
{code} INFO [CompactionExecutor:1] 2011-02-24 11:18:10,262 SSTableIdentityIterator.java (line 90) Invalid bloom filter in SSTableReader(path='/vm1/cassandraDB/data/Keyspace2/ruleGroup-f-243-Data.db'); will rebuild it {code}
which were preceded by a bunch of NegativeArraySizeExceptions, which were after the initialization log:
{code}
INFO [main] 2011-02-24 11:08:43,595 AbstractCassandraDaemon.java (line 77) Logging initialized
 INFO [main] 2011-02-24 11:08:43,605 AbstractCassandraDaemon.java (line 97) Heap size: 8330936320/8331984896
 INFO [main] 2011-02-24 11:08:43,606 CLibrary.java (line 61) JNA not found. Native methods will be disabled.
 INFO [main] 2011-02-24 11:08:43,613 DatabaseDescriptor.java (line 121) Loading settings from file:/usr/local/apache-cassandra-2011-02-24_02-21-51/conf/cassandra.yaml
 INFO [main] 2011-02-24 11:08:43,821 DatabaseDescriptor.java (line 181) DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO [main] 2011-02-24 11:08:43,893 SSTable.java (line 147) Deleted /vm1/cassandraDB/data/system/LocationInfo-f-210
 INFO [main] 2011-02-24 11:08:43,894 SSTable.java (line 147) Deleted /vm1/cassandraDB/data/system/LocationInfo-f-212
 INFO [main] 2011-02-24 11:08:43,894 SSTable.java (line 147) Deleted /vm1/cassandraDB/data/system/LocationInfo-f-211
 INFO [main] 2011-02-24 11:08:43,894 SSTable.java (line 147) Deleted /vm1/cassandraDB/data/system/LocationInfo-f-209
 INFO [main] 2011-02-24 11:08:43,919 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/IndexInfo-f-5
 INFO [main] 2011-02-24 11:08:43,937 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/Schema-f-89
 INFO [main] 2011-02-24 11:08:43,945 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/Migrations-f-85
 INFO [main] 2011-02-24 11:08:43,947 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/Migrations-f-87
 INFO [main] 2011-02-24 11:08:43,948 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/Migrations-f-86
 INFO [main] 2011-02-24 11:08:43,951 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/LocationInfo-f-213
 INFO [main] 2011-02-24 11:08:43,953 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/system/LocationInfo-f-214
 INFO [main] 2011-02-24 11:08:43,982 DatabaseDescriptor.java (line 461) Loading schema version b6fdb590-3e9e-11e0-8d0e-34b74a661156
 WARN [main] 2011-02-24 11:08:44,128 DatabaseDescriptor.java (line 493) Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
 INFO [main] 2011-02-24 11:08:44,188 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommenders-f-176
 INFO [main] 2011-02-24 11:08:44,191 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommenders-f-177
 INFO [main] 2011-02-24 11:08:44,192 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommenders-f-178
 INFO [main] 2011-02-24 11:08:44,197 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/businessRule-f-200
 INFO [main] 2011-02-24 11:08:44,204 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommendTo-f-92
 INFO [main] 2011-02-24 11:08:44,205 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommendTo-f-93
 INFO [main] 2011-02-24 11:08:44,209 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/followers-f-175
 INFO [main] 2011-02-24 11:08:44,213 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/names-f-165
 INFO [main] 2011-02-24 11:08:44,218 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/ruleGroup-f-244
 INFO [main] 2011-02-24 11:08:44,218 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/ruleGroup-f-243
 INFO [main] 2011-02-24 11:08:44,225 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommendFrom-f-109
 INFO [main] 2011-02-24 11:08:44,233 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/channels-f-2866
 INFO [main] 2011-02-24 11:08:44,262 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/channels-f-2867
 INFO [main] 2011-02-24 11:08:44,286 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace2/recommendAgain-f-1425
 INFO [main] 2011-02-24 11:08:44,293 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userProfile-f-816
 INFO [main] 2011-02-24 11:08:44,294 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userProfile-f-815
 INFO [main] 2011-02-24 11:08:44,301 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6153
 INFO [main] 2011-02-24 11:08:44,302 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6039
 INFO [main] 2011-02-24 11:08:44,443 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6130
 INFO [main] 2011-02-24 11:08:44,498 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6040
 INFO [main] 2011-02-24 11:08:44,591 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6143
 INFO [main] 2011-02-24 11:08:44,608 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6036
 INFO [main] 2011-02-24 11:08:44,676 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6035
 INFO [main] 2011-02-24 11:08:44,852 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6038
 INFO [main] 2011-02-24 11:08:44,953 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6134
 INFO [main] 2011-02-24 11:08:44,960 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6114
 INFO [main] 2011-02-24 11:08:44,976 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6096
 INFO [main] 2011-02-24 11:08:45,040 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6147
 INFO [main] 2011-02-24 11:08:45,048 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/showData-f-6152
 INFO [main] 2011-02-24 11:08:45,062 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/cloudData-f-5990
 INFO [main] 2011-02-24 11:08:45,087 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/cloudData-f-6063
 INFO [main] 2011-02-24 11:08:45,109 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/cloudData-f-6109
 INFO [main] 2011-02-24 11:08:45,110 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/cloudData-f-5991
 INFO [main] 2011-02-24 11:08:45,136 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/cloudData-f-6108
 INFO [main] 2011-02-24 11:08:45,154 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userChannelFilter-f-207
 INFO [main] 2011-02-24 11:08:45,154 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userChannelFilter-f-208
 INFO [main] 2011-02-24 11:08:45,162 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/popularCloudIds-f-203
 INFO [main] 2011-02-24 11:08:45,166 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userActions-f-1160
 INFO [main] 2011-02-24 11:08:45,177 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/headends-f-5793
 INFO [main] 2011-02-24 11:08:45,195 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/headends-f-5801
 INFO [main] 2011-02-24 11:08:45,196 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/headends-f-5761
 INFO [main] 2011-02-24 11:08:45,202 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/headends-f-5800
 INFO [main] 2011-02-24 11:08:45,213 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userActionLikes-f-10
 INFO [main] 2011-02-24 11:08:45,218 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userAttributes-f-2517
 INFO [main] 2011-02-24 11:08:45,219 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userAttributes-f-2533
 INFO [main] 2011-02-24 11:08:45,221 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/Keyspace1/userAttributes-f-2532
 INFO [main] 2011-02-24 11:08:45,230 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-102
 INFO [main] 2011-02-24 11:08:45,607 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-103
 INFO [main] 2011-02-24 11:08:45,799 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-88
 INFO [main] 2011-02-24 11:08:46,319 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-93
 INFO [main] 2011-02-24 11:08:46,846 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-63
 INFO [main] 2011-02-24 11:08:49,879 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureBuckets-83
 INFO [main] 2011-02-24 11:08:51,340 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/userSignatures-22
 INFO [main] 2011-02-24 11:08:51,351 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/userSignatures-23
 INFO [main] 2011-02-24 11:08:51,355 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/userSignatures-17
 INFO [main] 2011-02-24 11:08:51,385 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/userSignatures-24
 INFO [main] 2011-02-24 11:08:51,390 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity/signatureFunctions-5
 INFO [main] 2011-02-24 11:08:51,394 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/generalUtils/userActionUtilsKey-9
 INFO [main] 2011-02-24 11:08:51,396 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/generalUtils/facebookShowIds-f-37
 INFO [main] 2011-02-24 11:08:51,398 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/signatureBuckets-6
 INFO [main] 2011-02-24 11:08:51,527 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/signatureBuckets-5
 INFO [main] 2011-02-24 11:08:51,903 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/userSignatures-1
 INFO [main] 2011-02-24 11:08:52,006 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/userSignatures-e-3
 INFO [main] 2011-02-24 11:08:52,009 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/userSignatures-2
 INFO [main] 2011-02-24 11:08:52,013 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/signatureFunctions-2
 INFO [main] 2011-02-24 11:08:52,013 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/signatureFunctions-1
 INFO [main] 2011-02-24 11:08:52,014 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/userSimilarity2/signatureFunctions-e-3
 INFO [main] 2011-02-24 11:08:52,019 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/facebookActions-f-186
 INFO [main] 2011-02-24 11:08:52,019 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/facebookActions-f-185
 INFO [main] 2011-02-24 11:08:52,022 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/watchingUsers-f-477
 INFO [main] 2011-02-24 11:08:52,038 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/showState-e-35
 INFO [main] 2011-02-24 11:08:52,039 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/showState-e-37
 INFO [main] 2011-02-24 11:08:52,039 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/showState-e-36
 INFO [main] 2011-02-24 11:08:52,042 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/watchingUserHistory-f-462
 INFO [main] 2011-02-24 11:08:52,042 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/watchingUserHistory-f-461
 INFO [main] 2011-02-24 11:08:52,043 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/realTimeSocial/watchingUserHistory-f-463
 INFO [main] 2011-02-24 11:08:52,045 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/chats/chatUsers-f-129
 INFO [main] 2011-02-24 11:08:52,046 SSTableReader.java (line 154) Opening /vm1/cassandraDB/data/chats/chatHistory-f-129
 INFO [main] 2011-02-24 11:08:52,055 CommitLogSegment.java (line 50) Creating new commitlog segment /vm1/cassandraDB/commitlog/CommitLog-1298538532055.log
 INFO [main] 2011-02-24 11:08:52,061 CommitLog.java (line 155) Replaying /vm1/cassandraDB/commitlog/CommitLog-1298537386811.log
 INFO [main] 2011-02-24 11:08:52,073 CommitLog.java (line 311) Finished reading /vm1/cassandraDB/commitlog/CommitLog-1298537386811.log
 INFO [main] 2011-02-24 11:08:52,074 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for recommendAgain at CommitLogContext(file='/vm1/cassandraDB/commitlog/CommitLog-1298538532055.log', position=0)
 INFO [main] 2011-02-24 11:08:52,076 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-recommendAgain@205025794(636 bytes, 7 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,077 Memtable.java (line 157) Writing Memtable-recommendAgain@205025794(636 bytes, 7 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,113 Memtable.java (line 164) Completed flushing /vm1/cassandraDB/data/Keyspace2/recommendAgain-f-1426-Data.db (859 bytes)
 INFO [main] 2011-02-24 11:08:52,118 CommitLog.java (line 163) Log replay complete
 INFO [main] 2011-02-24 11:08:52,131 StorageService.java (line 354) Cassandra version: 2011-02-24_02-21-51
 INFO [main] 2011-02-24 11:08:52,131 StorageService.java (line 355) Thrift API version: 19.4.0
 INFO [main] 2011-02-24 11:08:52,132 StorageService.java (line 368) Loading persisted ring state
 INFO [main] 2011-02-24 11:08:52,132 StorageService.java (line 404) Starting up server gossip
 INFO [main] 2011-02-24 11:08:52,137 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/vm1/cassandraDB/commitlog/CommitLog-1298538532055.log', position=148)
 INFO [main] 2011-02-24 11:08:52,137 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-LocationInfo@1068086436(29 bytes, 1 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,138 Memtable.java (line 157) Writing Memtable-LocationInfo@1068086436(29 bytes, 1 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,178 Memtable.java (line 164) Completed flushing /vm1/cassandraDB/data/system/LocationInfo-f-215-Data.db (80 bytes)
 INFO [main] 2011-02-24 11:08:52,198 StorageService.java (line 468) Using saved token 51653040247566871911249877869558549493
 INFO [main] 2011-02-24 11:08:52,199 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for LocationInfo at CommitLogContext(file='/vm1/cassandraDB/commitlog/CommitLog-1298538532055.log', position=444)
 INFO [main] 2011-02-24 11:08:52,199 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-LocationInfo@832074392(53 bytes, 2 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,199 Memtable.java (line 157) Writing Memtable-LocationInfo@832074392(53 bytes, 2 operations)
 INFO [FlushWriter:1] 2011-02-24 11:08:52,230 Memtable.java (line 164) Completed flushing /vm1/cassandraDB/data/system/LocationInfo-f-216-Data.db (163 bytes)
 INFO [CompactionExecutor:1] 2011-02-24 11:08:52,231 CompactionManager.java (line 395) Compacting [SSTableReader(path='/vm1/cassandraDB/data/system/LocationInfo-f-213-Data.db'),SSTableReader(path='/vm1/cassandraDB/data/system/LocationInfo-f-214-Data.db'),SSTableReader(path='/vm1/cassandraDB/data/system/LocationInfo-f-215-Data.db'),SSTableReader(path='/vm1/cassandraDB/data/system/LocationInfo-f-216-Data.db')]
 INFO [main] 2011-02-24 11:08:52,233 Mx4jTool.java (line 72) Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO [CompactionExecutor:1] 2011-02-24 11:08:52,292 CompactionManager.java (line 482) Compacted to /vm1/cassandraDB/data/system/LocationInfo-tmp-f-217-Data.db.  851 to 445 (~52% of original) bytes for 3 keys.  Time: 61ms.
 INFO [main] 2011-02-24 11:08:52,296 CassandraDaemon.java (line 112) Binding thrift service to /0.0.0.0:9160
 INFO [main] 2011-02-24 11:08:52,298 CassandraDaemon.java (line 126) Using TFastFramedTransport with a max frame size of 15728640 bytes.
 INFO [Thread-3] 2011-02-24 11:08:52,300 CassandraDaemon.java (line 153) Listening for thrift clients...
{code};;;","24/Feb/11 20:17;kunda;The previously mentioned tons of ""Invalid bloom filter in SSTableReader"" logs ended with a single stacktrace:
{code}
ERROR [CompactionExecutor:1] 2011-02-24 11:36:19,268 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
	at org.apache.cassandra.dht.RandomPartitioner.convertFromDiskFormat(RandomPartitioner.java:62)
	at org.apache.cassandra.io.sstable.SSTableReader.decodeKey(SSTableReader.java:627)
	at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:541)
	at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
	at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{code}
followed by what appears to be standard flushing/switching logs.;;;","24/Feb/11 20:20;kunda;I now restarted the server again, and two things happened:
1) performing a scrub on the example CF and a bunch of other CFs succeeded without hanging
2) performing a scrub on a different CF resulting in the following stack trace (on different retries):
{code}
ERROR [CompactionExecutor:1] 2011-02-24 22:02:30,329 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOError: java.io.EOFException
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:113)
	at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:549)
	at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
	at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readInt(RandomAccessFile.java:776)
	at org.apache.cassandra.io.sstable.IndexHelper.skipBloomFilter(IndexHelper.java:47)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:104)
	... 8 more
{code}
3) and on another CF:
{code}
ERROR [CompactionExecutor:1] 2011-02-24 22:28:28,002 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.dht.RandomPartitioner.convertFromDiskFormat(RandomPartitioner.java:62)
        at org.apache.cassandra.io.sstable.SSTableReader.decodeKey(SSTableReader.java:627)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:541)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code};;;","24/Feb/11 20:55;jbellis;I really can't help with just a stacktrace and none of the logs leading up to it.

Remember that scrub snapshots before it does its thing, so it's easy to restore the pre-scrubbed versions and try again.;;;","25/Feb/11 16:23;jbellis;Couldn't reproduce by scrubbing sstables produced by 0.6 stress.py;;;","27/Feb/11 10:28;kunda;After performing more tests, I realized why the problem could not be reproduced - the scrub process hang on a different CF, and afterwards any scrub operation would hang, until the server is restarted.
I was able to narrow down the problem to a specific sstable - I will soon post it along the stack trace.;;;","27/Feb/11 10:34;kunda;Here is the sstable that hangs the scrub process - ran on 0.7 build #325;;;","27/Feb/11 10:44;kunda;And here is another sstable that doesn't hang but throws an exception;;;","27/Feb/11 11:23;kunda;Update: restarted server and rerun scrub on the attached signatureBuckets CF, this time did not hung but gave the following error:
 INFO [CompactionExecutor:1] 2011-02-27 13:21:27,928 CompactionManager.java (line 511) Scrubbing SSTableReader(path='/vm1/cassandraDB/data/userSimilarity/signatureBuckets-f-104-Data.db')
 INFO [CompactionExecutor:1] 2011-02-27 13:21:28,430 SSTableIdentityIterator.java (line 90) Invalid bloom filter in SSTableReader(path='/vm1/cassandraDB/data/userSimilarity/signatureBuckets-f-104-Data.db'); will rebuild it
 INFO [CompactionExecutor:1] 2011-02-27 13:21:28,430 SSTableIdentityIterator.java (line 99) Invalid row summary in SSTableReader(path='/vm1/cassandraDB/data/userSimilarity/signatureBuckets-f-104-Data.db'); will rebuild it
ERROR [CompactionExecutor:1] 2011-02-27 13:21:28,434 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOError: java.io.EOFException: attempted to skip 758940931 bytes but only skipped 1400349
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:113)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:548)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException: attempted to skip 758940931 bytes but only skipped 1400349
        at org.apache.cassandra.io.sstable.IndexHelper.skipBloomFilter(IndexHelper.java:51)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:104)
        ... 8 more

;;;","27/Feb/11 12:05;kunda;Repeating the process now results in a different failure on a different and very big (>500MB) sstable of the same signatureBuckets CF:

ERROR [CompactionExecutor:1] 2011-02-27 13:26:01,307 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.dht.RandomPartitioner.convertFromDiskFormat(RandomPartitioner.java:62)
        at org.apache.cassandra.io.sstable.SSTableReader.decodeKey(SSTableReader.java:627)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:538)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

I think it has something to do with [~jbellis]'s comment about missing colon delimiter -
if scrub cannot solve this, is there any way to fix this problem?
;;;","27/Feb/11 21:53;wajam;We are running into the same issue using #325 from Hudson. Exception everytime.

Everything started when we upgraded from 0.6.8 to 0.7.2. One of the node (17 nodes total) started to be slow, we realized one CF wasn't compacting and throw exception (Error in ThreadPoolExecutor java.io.IOError: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0).

So I gave the new scrub function a try and but looks like it doesnt work. We really need to have this fixed!

Thank you!;;;","27/Feb/11 22:45;jbellis;can someone post a log file from a failed scrub attempt?  ideally with log level debug but I'll take info if that's what you have.;;;","27/Feb/11 23:14;wajam;I tried to activate debug log but since this is a live node, it seems there is WAY too much going on. Here is the INFO log:

ERROR 23:10:35,105 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.IOError: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:246)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:262)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:223)
        at java.util.concurrent.ConcurrentSkipListMap.buildFromSorted(ConcurrentSkipListMap.java:1521)
        at java.util.concurrent.ConcurrentSkipListMap.<init>(ConcurrentSkipListMap.java:1471)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:366)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:314)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumns(ColumnFamilySerializer.java:129)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:172)
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:78)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:139)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:448)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:123)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:93)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:68)
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:242)
        ... 25 more
 INFO 23:10:35,106 Scrubbing SSTableReader(path='/var/lib/cassandra/data/Wajam/Comment-f-710-Data.db')
ERROR 23:10:37,489 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:268)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:540)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

And the exception that is output by nodetool scrub:

Error occured while scrubbing keyspace <keyspacename>
java.util.concurrent.ExecutionException: java.io.EOFException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.CompactionManager.performScrub(CompactionManager.java:203)
        at org.apache.cassandra.db.ColumnFamilyStore.scrub(ColumnFamilyStore.java:963)
        at org.apache.cassandra.service.StorageService.scrub(StorageService.java:1247)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:268)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:540)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        ... 3 more

;;;","28/Feb/11 08:01;kunda;Attached a system log leading up to a ""java.io.EOFException: attempted to skip X bytes but only skipped Y"" error during a scrub of a very big (probably uncompactable) sstable;;;","28/Feb/11 08:07;kunda;another system log, this time leading to a vanilla java.io.EOFException;;;","28/Feb/11 21:25;jbellis;Looked at userChannelFilter-f-210.tar.gz.  Data file does not match index even a little.  Scrub can't help there.
;;;","28/Feb/11 21:41;wajam;I have a 71MB SS table that scrub fail to fix so maybe you would be interested in having it ? There is private data in there so I could upload it somewhere so only you can download it. Let me know how we can work together.

Thank you!;;;","28/Feb/11 23:16;jbellis;Patch to make scrub less crashy.  It still can't magically fix severely corrupted files like the exhibits here, though.;;;","28/Feb/11 23:19;jbellis;Sebastien, go ahead and try scrub after rebuilding w/ the newest patch.

But if your example is like the others, you're seeing something other than the CASSANDRA-2211/CASSANDRA-2216 corruption that scrub is intended to deal with.

So far the only suggestion I have to track that down is to start over and run with snapshot_before_compaction turned on in cassandra.yaml, so when a corrupt sstable is generated we will know where it came from.;;;","01/Mar/11 00:57;wajam;Jonathan,

I'm pretty positive I'm running into CASSANDRA-2216.

Applied the patch and still having the same exception, maybe this is related to CASSANDRA-2256 ?

ERROR 00:51:55,214 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:268)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:541)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)

;;;","01/Mar/11 02:49;jbellis;2211/2216 would leave valid row keys written, which is where yours is corrupt.  That is why I think this is something else.

Here is a v2 though that will catch the error and try the next row from the index.;;;","01/Mar/11 03:58;wajam;
Now it seems to hang for a while on a 1.4GB sstable. Eventually I get this exception...

ERROR 03:56:29,413 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:569)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
;;;","01/Mar/11 04:18;wajam;Looking at the data directory, it looks like it was working as excepted before the exception happened. The tmp SStable is 1.0GB... Looks like we are getting close!;;;","01/Mar/11 04:45;jbellis;v3 adds support for the index file ending before the data file;;;","01/Mar/11 04:57;wajam;I will try v3 soon and let you know. While looking at the patch, I found a typo:

logger.warn(""No valid rows found while scrubbing "" + sstable + ""; it is marked for deltion now. If you want to attempt manual recovery, you can find a copy in the pre-scrub snapshot"");

Should be ""deletion"" instead of ""deltion""... no big deal :);;;","01/Mar/11 05:02;wajam;Now I'm getting this:

ERROR 04:58:53,338 Error reading index file.  Scrub does not (yet) know how to recover from corrupt index files; you can try rebuilding it offline.  See http://www.mail-archive.com/user@cassandra.apache.org/msg03325.html
ERROR 04:58:53,338 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.io.EOFException
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:565)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:268)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:559)
        ... 7 more

And

root@WajamCassandra12:/usr/share/cassandra# nodetool -h 127.0.0.1 scrub Wajam Wajam
Error occured while scrubbing keyspace Wajam
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.EOFException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.CompactionManager.performScrub(CompactionManager.java:204)
        at org.apache.cassandra.db.ColumnFamilyStore.scrub(ColumnFamilyStore.java:963)
        at org.apache.cassandra.service.StorageService.scrub(StorageService.java:1247)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:251)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:857)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:795)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1450)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:90)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1285)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1383)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:807)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: java.io.EOFException
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:565)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        ... 3 more
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:416)
        at java.io.RandomAccessFile.readFully(RandomAccessFile.java:394)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:268)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:310)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:559)
        ... 7 more
;;;","01/Mar/11 05:30;jbellis;v4 attached.  (might be my last for the night; getting late here.  but I'll check back first thing in the morning);;;","01/Mar/11 05:51;wajam;Scrub is running with v4 right now, we will see what happen, no exception so far. There is these things being displayed about fifty millions time in the log tho:) :

 INFO 05:33:29,521 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:29,548 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:29,859 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:29,882 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:31,223 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:31,249 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:31,644 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:31,678 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,102 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,129 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,393 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,425 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,713 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:32,734 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,004 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,031 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,305 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,331 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,599 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,623 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,897 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:33,917 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:34,330 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:34,375 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:35,504 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:35,525 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it
 INFO 05:33:35,933 Invalid row summary in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-146-Data.db'); will rebuild it

Either way, as long as it fix my ss table I will be happy :) I will let you know in the morning if there is any more issue

Thank you!
;;;","01/Mar/11 13:22;slebresne;Stepping in while Jonathan takes some well deserved rest and attaching v5 (based on last version attached). This makes the following changes:

* In doScrub(), move the first indexFile.readLong() out of the assert.
* Fix computation of dataStartFromIndex (was missing th 4 or 8 bytes for the data size).
* IndexHelper.defreezeBloomFilter don't leave the file pointer after the bloomFilter for new BF since it reads directly from the file (instead of reading the bytes at once and deserializing from that). Correct this.
* Log if a row has been correctly read the first time but index start and size are different from data start and size (since index should then be manually rebuilt).
* Do a retry if dataStart == dataStartFromIndex but dataSize != dataSizeFromIndex (in case the row size only has been corrupted).

Lastly, a minor remarks: the patch removes a flush in BF.serialize(). Maybe this belongs to another ticket ?
;;;","01/Mar/11 13:42;wajam;Thank you Sylvain

Unfortunatly, i'm unable to apply v5 patch for some reason, is it a svn or git patch ?;;;","01/Mar/11 13:57;slebresne;It's a git patch, git apply or 'patch -p1 -i 2240-v5.patch' should do the trick (it does here). It's based on current cassandra-0.7 branch. ;;;","01/Mar/11 14:46;jbellis;v6 applies Sylvain's fixes to v4 (which mysteriously disappeared from jira);;;","01/Mar/11 14:46;jbellis;I also backed out the read-directly-from-file change in IndexHelper.  Will create a new ticket for that.;;;","01/Mar/11 14:47;wajam;Yep, that worked. So it's currently running with your patch, and so far I get no output from scrub after running for like 5 mins, I usually get the ""Scrubbing SSTABLE"" in the first min so I'm not sure if that's normal. There is the ""tmp"" files in the data directory but looks like they are stuck at 0 byte.

Oh, looks like I will try v6 now then!

Jonathan, v4 is there, you named it 2240.txt :);;;","01/Mar/11 15:00;jbellis;bq. v4 is there, you named it 2240.txt

Oops -- that was the original patch, not actually v4.  Must have been tired.;;;","01/Mar/11 15:28;slebresne;Alright, v6 looks good, +1. Though, we may want to wait to see if it works alright for Sebastien too.;;;","01/Mar/11 15:36;wajam;I'm testing it as we speak!

Log are flooded with this:

 INFO 15:31:15,783 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-164-Data.db'); will rebuild it
 INFO 15:31:15,783 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-164-Data.db'); will rebuild it
 INFO 15:31:15,783 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-164-Data.db'); will rebuild it
 INFO 15:31:15,783 Invalid bloom filter in SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-164-Data.db'); will rebuild it

I'm not sure why it's scrubbing ""WebsiteWajams"" CF as I asked for ""Wajam"" (nodetool -h 127.0.0.1 scrub Wajam Wajam) ... Either way, this sstable is messed up too so that's fine.

Hopefully it won't just do like v4 that ran all night and was still echoing invalid bloom filter for the same sstable in the log this morning, Looks like it was stuck in a infinite loop or something.

It looks like it's scrubbing another sstable now so that's already an improvment.

Will keep you guys updated!


;;;","01/Mar/11 15:48;wajam; INFO 15:42:52,690 Scrub of SSTableReader(path='/var/lib/cassandra/data/Wajam/WebsiteWajams-f-155-Data.db') complete: 3271113 in new sstable

""3271113 in new sstable""... I'm not sure it's 3271113 what... Looks like ""good rows"" according to the patch, might want to specify! :)

I think it's the first time I see this line tho so it's good news. Is there any way the ""invalid bloom filter, will rebuild it"" line can only appear once per sstable ? I'm guessing its generating useless IO writing about 1000 of those lines/second in the log!

This should be definitly included in 0.7.3 which hopefully will be released soon!

Good work guys, very appreciated :);;;","01/Mar/11 16:10;wajam;I did see a few exceptions every now and then. Not sure if you need to do anything about this. Sorry for all these posts, hopefully it help :):

224574- INFO 16:06:40,744 Retrying from row index; data is 245642090 bytes starting at 42
224657- WARN 16:06:40,744 Retry failed too.  Skipping to next row (retry's stacktrace follows)
224745-java.io.IOError: java.io.EOFException
224783- at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:117)
224884: at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:610)
224966- at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
225050- at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
225131- at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
225202- at java.util.concurrent.FutureTask.run(FutureTask.java:166)
225263- at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
225347- at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
225431- at java.lang.Thread.run(Thread.java:636)
225473-Caused by: java.io.EOFException
225505- at java.io.DataInputStream.readInt(DataInputStream.java:392)
225567- at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:47);;;","01/Mar/11 16:10;jbellis;committed w/ update to goodRows message and moving the BF/row header messages to debug level

thanks!;;;","01/Mar/11 16:14;jbellis;bq. I did see a few exceptions every now and then

Did you see any ""Error reading index file"" lines before that?  That's the only way I can think that it would come up with such a strange row size.
;;;","01/Mar/11 16:22;wajam;Yep I did see error reading index file before that.

Any chance to include this in 0.7.3 release ? I'm sure it would help a ton of people :);;;","01/Mar/11 16:30;jbellis;Yes, we'll get this into 0.7.3.;;;","01/Mar/11 16:53;hudson;Integrated in Cassandra-0.7 #336 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/336/])
    make nodetool scrub more robust
patch by jbellis and slebresne; tested by Sébastien Giroux for CASSANDRA-2240
;;;","01/Mar/11 20:15;kunda;Thank you so much, my sstables are finally clean!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra.bat does fail when CASSANDRA_HOME contains a whitespace,CASSANDRA-2237,12499597,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,norman,norman,norman,24/Feb/11 14:24,16/Apr/19 09:33,14/Jul/23 05:52,24/Feb/11 19:56,0.7.3,,,Packaging,,,0,,,,"If you try to start cassandra from a directory with whitespaces you will see a stacktrace similar to this:

Starting Cassandra Server
Exception in thread ""main"" java.lang.NoClassDefFoundError: and
Caused by: java.lang.ClassNotFoundException: and
        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:303)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:316)
Could not find the main class: and.  Program will exit.
",windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/11 14:26;norman;CASSANDRA-2237.diff;https://issues.apache.org/jira/secure/attachment/12471837/CASSANDRA-2237.diff",,,,,,,,,,,,,,1.0,norman,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20520,,,Fri Feb 25 15:15:13 UTC 2011,,,,,,,,,,"0|i0ga67:",93078,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"24/Feb/11 14:26;norman;Here is the batch which fix this;;;","24/Feb/11 14:29;norman;the patch must get applied in the bin/ folder.;;;","24/Feb/11 19:56;jbellis;committed, thanks!;;;","25/Feb/11 15:15;hudson;Integrated in Cassandra-0.7 #321 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/321/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cli does not support updating replicate_on_write,CASSANDRA-2236,12499565,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,24/Feb/11 08:46,16/Apr/19 09:33,14/Jul/23 05:52,24/Feb/11 18:11,0.8 beta 1,,,,,,0,,,,Add support for updating a column families replicate on write setting.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/11 08:47;lenn0x;0001-cli-support-replicate_on_write-via-update-CF.patch;https://issues.apache.org/jira/secure/attachment/12471815/0001-cli-support-replicate_on_write-via-update-CF.patch",,,,,,,,,,,,,,1.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20519,,,Thu Feb 24 20:14:28 UTC 2011,,,,,,,,,,"0|i0ga5z:",93077,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"24/Feb/11 08:47;lenn0x;Patch by Kelvin Kakugawa.;;;","24/Feb/11 18:04;xedin;+1;;;","24/Feb/11 20:14;hudson;Integrated in Cassandra #742 (See [https://hudson.apache.org/hudson/job/Cassandra/742/])
    Cli does not support updating replicate_on_write. patch Kelvin Kakugawa; reviewed by Pavel Yaskevich for CASSANDRA-2236
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counter AES performance issue,CASSANDRA-2235,12499563,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,lenn0x,lenn0x,lenn0x,24/Feb/11 08:08,16/Apr/19 09:33,14/Jul/23 05:52,16/Mar/11 15:12,0.8 beta 1,,,,,,0,,,,"We noticed tonight when trying out AES for Counters in trunk, there is a serious performance issue when inlining the SSTables. We found that the way we are seeking in the file, BRAF keeps flushing out its buffer of 8MB, and we call dfile.sync() on every row. We are finalizing a patch to write a new SSTable on rebuild, instead of inlining. ",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20518,,,Wed Mar 16 15:12:38 UTC 2011,,,,,,,,,,"0|i0ga5r:",93076,,,,,Normal,,,,,,,,,,,,,,,,,"24/Feb/11 14:47;jbellis;bq. We found that the way we are seeking in the file, BRAF keeps flushing out its buffer of 8MB, and we call dfile.sync() on every row

fixed in CASSANDRA-2218;;;","16/Mar/11 15:12;slebresne;Marking resolved since either CASSANDRA-2218 or CASSANDRA-2288 fixed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race conditions when reinitialisating nodes (OOM + Nullpointer),CASSANDRA-2228,12499491,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,tbritz,tbritz,23/Feb/11 15:50,16/Apr/19 09:33,14/Jul/23 05:52,03/Mar/11 22:59,0.7.4,,,,,,0,,,,"I had a corrupt system table which wouldn't compact anymore and I deleted the files and restarted cassandra and let it take the same token/ip address.

I experienced the same errors when I'm adding a newly installed node under the same token/ip address before calling repair.

1)
After a few seconds/minutes, I get a OOM error:


 INFO [FlushWriter:1] 2011-02-23 16:40:28,958 Memtable.java (line 164) Completed flushing /cassandra/data/system/Schema-f-15-Data.db (8037 bytes)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,965 Migration.java (line 133) Applying migration 3e30e76b-1e3f-11e0-8369-5a9c1faed4ae Add keyspace: table_userentriesrep factor:3rep strategy:SimpleStrategy{org.apache.cassandra.config.CFMetaData@58925d9[cfId=1024,tableName=table_userentries,cfName=table_userentries,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType@b44dff0,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=200000.0,readRepairChance=0.0,gcGraceSeconds=86400,defaultValidator=org.apache.cassandra.db.marshal.BytesType@b44dff0,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=3600,memtableFlushAfterMins=60,memtableThroughputInMb=64,memtableOperationsInMillions=10.0,column_metadata={}], org.apache.cassandra.config.CFMetaData@11ab7246[cfId=1025,tableName=table_userentries,cfName=table_userentries_meta,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType@b44dff0,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=200000.0,readRepairChance=0.0,gcGraceSeconds=86400,defaultValidator=org.apache.cassandra.db.marshal.BytesType@b44dff0,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=3600,memtableFlushAfterMins=60,memtableThroughputInMb=64,memtableOperationsInMillions=10.0,column_metadata={}]}
 INFO [MigrationStage:1] 2011-02-23 16:40:28,965 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for Migrations at CommitLogContext(file='/cassandra/commitlog/CommitLog-1298475572022.log', position=226075)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,966 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-Migrations@2121008793(12529 bytes, 1 operations)
 INFO [FlushWriter:1] 2011-02-23 16:40:28,966 Memtable.java (line 157) Writing Memtable-Migrations@2121008793(12529 bytes, 1 operations)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,966 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for Schema at CommitLogContext(file='/cassandra/commitlog/CommitLog-1298475572022.log', position=226075)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,967 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-Schema@139610466(8370 bytes, 15 operations)
 INFO [ScheduledTasks:1] 2011-02-23 16:40:28,972 StatusLogger.java (line 89) table_sourcedetection.table_sourcedetection                 0,0                 0/0            0/200000
ERROR [FlushWriter:1] 2011-02-23 16:41:01,240 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[FlushWriter:1,5,main]
java.lang.OutOfMemoryError: Java heap space
        at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:39)
        at java.nio.ByteBuffer.allocate(ByteBuffer.java:312)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:126)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:75)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:158)
        at org.apache.cassandra.db.Memtable.access$000(Memtable.java:51)
        at org.apache.cassandra.db.Memtable$1.runMayThrow(Memtable.java:176)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)





2) If I restart then, I'm getting an Nullpointer exception. The OOM error will only appear once.

ERROR [main] 2011-02-23 16:42:32,782 AbstractCassandraDaemon.java (line 333) Exception encountered during startup.
java.lang.NullPointerException
        at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:768)
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:925)
        at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:105)
        at org.apache.cassandra.service.MigrationManager.applyMigrations(MigrationManager.java:161)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:185)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)


Killing and restarting the node multiple times will eventually ""fix"" these errors.


Steps to reproduce. Remove complete data directory and restart node with same token/ip.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/11 21:32;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-initialize-localEndpoint_.txt;https://issues.apache.org/jira/secure/attachment/12472612/ASF.LICENSE.NOT.GRANTED--v1-0001-initialize-localEndpoint_.txt",,,,,,,,,,,,,,1.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20516,,,Thu Mar 03 23:32:31 UTC 2011,,,,,,,,,,"0|i0ga47:",93069,,,,,Normal,,,,,,,,,,,,,,,,,"23/Feb/11 15:56;tbritz;The normal heap usage of the node is about half of the allocated heap:

Gossip active    : true
Load             : 30.99 GB
Generation No    : 1298476238
Uptime (seconds) : 308
Heap Memory (MB) : 1544.41 / 2493.38
;;;","23/Feb/11 16:58;jbellis;The OOM is because flushing allocates a buffer the size of in_memory_compaction_limit.  Sounds like you need to lower that.

The NPE looks like a bug.;;;","02/Mar/11 20:58;gdusbabek;What is listen_address in cassandra.yaml?  I'm having a hard time conceiving of a way for FBUtilities.getLocalAddress() to return null.;;;","02/Mar/11 20:59;thibaut.britz@trendiction.com;Hi,

I'm on vacation until March 14th.

For urgent matters, please contact:
Christophe Folschette (christophe.folschette@trendiction.com)
+352 20 33 35 32
;;;","03/Mar/11 16:09;jbellis;It looks like we're doing Migration stuff before Gossiper.start is called.

One possible solution: initialize Gossiper.localEndpoint in constructor instead of in start.;;;","03/Mar/11 17:14;gdusbabek;CHM throws NPE when the key is null.  To me, then, the bug is that FBU.getLocalAddress() is somehow returning null.  

I checked yesterday: even if local_address is unspecified, it ends up returning localhost.;;;","03/Mar/11 17:18;jbellis;My point was that we can call put before initializing localEndpoint_ to FBU.gLA, so it's naturally going to be null.;;;","03/Mar/11 20:46;gdusbabek;aha. I was looking in the trunk code (no chance of null).  I see the problem in the 0.7 branch.;;;","03/Mar/11 22:03;jbellis;+1;;;","03/Mar/11 22:59;gdusbabek;committed;;;","03/Mar/11 23:32;hudson;Integrated in Cassandra-0.7 #344 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/344/])
    initialize localendpoint in gossiper earlier. patch by gdusbabek, reviewed by jbellis. CASSANDRA-2228
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClientOnly mode is creating directories,CASSANDRA-2223,12499415,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,22/Feb/11 22:36,16/Apr/19 09:33,14/Jul/23 05:52,23/Feb/11 19:47,0.7.3,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/11 22:42;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-client-only-mode-shouldn-t-create-any-local-data.txt;https://issues.apache.org/jira/secure/attachment/12471666/ASF.LICENSE.NOT.GRANTED--v1-0001-client-only-mode-shouldn-t-create-any-local-data.txt","23/Feb/11 16:45;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0001-ClientOnlyExample-waits-for-schema-to-arrive-tries-to-.txt;https://issues.apache.org/jira/secure/attachment/12471748/ASF.LICENSE.NOT.GRANTED--v2-0001-ClientOnlyExample-waits-for-schema-to-arrive-tries-to-.txt","23/Feb/11 16:45;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0002-avoid-creating-local-files-when-in-fat-client-mode.txt;https://issues.apache.org/jira/secure/attachment/12471749/ASF.LICENSE.NOT.GRANTED--v2-0002-avoid-creating-local-files-when-in-fat-client-mode.txt","23/Feb/11 16:45;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0003-make-sure-fat-clients-gossip-their-schema.txt;https://issues.apache.org/jira/secure/attachment/12471750/ASF.LICENSE.NOT.GRANTED--v2-0003-make-sure-fat-clients-gossip-their-schema.txt","23/Feb/11 16:45;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0004-update-contrib_client-only-meta.txt;https://issues.apache.org/jira/secure/attachment/12471751/ASF.LICENSE.NOT.GRANTED--v2-0004-update-contrib_client-only-meta.txt",,,,,,,,,,5.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19346,,,Wed Feb 23 20:11:13 UTC 2011,,,,,,,,,,"0|i0ga33:",93064,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"23/Feb/11 15:53;tjake;The client_only example no longer works with this patch:

jake@Jake-Lucianis-MacBook-Pro(client_only)$ ./bin/client_only write
11/02/23 10:53:01 INFO config.DatabaseDescriptor: Loading settings from jar:file:/Users/jake/workspace/cassandra-git/contrib/client_only/build/client_only.jar!/cassandra.yaml
11/02/23 10:53:01 INFO config.DatabaseDescriptor: DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
11/02/23 10:53:01 INFO service.StorageService: Starting up client gossip
11/02/23 10:53:01 INFO gms.Gossiper: Node /127.0.0.1 has restarted, now UP again
11/02/23 10:53:02 INFO gms.Gossiper: InetAddress /127.0.0.1 is now dead.
11/02/23 10:53:02 INFO gms.Gossiper: InetAddress /127.0.0.1 is now UP
Exception in thread ""main"" java.lang.AssertionError
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:66)
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:61)
        at org.apache.cassandra.db.RowMutation.add(RowMutation.java:144)
        at org.apache.cassandra.db.RowMutation.add(RowMutation.java:152)
        at ClientOnlyExample.testWriting(ClientOnlyExample.java:68)
        at ClientOnlyExample.main(ClientOnlyExample.java:127);;;","23/Feb/11 17:07;tjake;v2 works great.  verified no dirs are created. +1

Only thing I've been seeing with client mode in general is this on startup (randomly)

11/02/23 12:03:46 INFO service.StorageService: Starting up client gossip
Exception in thread ""Thread-2"" java.io.IOError: java.io.EOFException
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:73)
Caused by: java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:375)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:61)

Seems to cause no issues though...;;;","23/Feb/11 19:47;gdusbabek;committed!;;;","23/Feb/11 20:11;hudson;Integrated in Cassandra-0.7 #312 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/312/])
    fat clients were creating local data. patch by gdusbabek, reviewed by tjake. CASSANDRA-2223
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression caused by cache-avoiding code in BRAF,CASSANDRA-2218,12499383,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,22/Feb/11 18:19,16/Apr/19 09:33,14/Jul/23 05:52,22/Feb/11 18:41,0.7.3,,,,,,0,,,,"As reported by Ivan Georgiev on the mailing list, BRAF.reBuffer unnecessarily does extra read + fadvise when seeking to the end of the file.",,stuhood,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"22/Feb/11 18:21;jbellis;2218.txt;https://issues.apache.org/jira/secure/attachment/12471646/2218.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19347,,,Tue Feb 22 18:57:55 UTC 2011,,,,,,,,,,"0|i0ga1z:",93059,,tjake,,tjake,Normal,,,,,,,,,,,,,,,,,"22/Feb/11 18:31;tjake;good find +1;;;","22/Feb/11 18:41;jbellis;committed;;;","22/Feb/11 18:57;hudson;Integrated in Cassandra-0.7 #304 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/304/])
    fix BRAF performancewhen seeking toEOF
patch by Ivan Georgiev; reviewed by tjake for CASSANDRA-2218
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction can echo data which breaks upon sstable format changes,CASSANDRA-2216,12499332,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,slebresne,slebresne,22/Feb/11 12:17,16/Apr/19 09:33,14/Jul/23 05:52,22/Feb/11 17:32,0.7.3,,,,,,0,compaction,,,"While compaction, if for a row we have only 1 sstable holding data, we echo this data. This breaks when we change the data format, creating mixed (corrupted) sstable.

(I suspect this is the cause of CASSANDRA-2195, but opening a new ticket until we can confirm that hunch)",,cburroughs,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"22/Feb/11 12:24;slebresne;0001-Don-t-echo-data-during-compaction.patch;https://issues.apache.org/jira/secure/attachment/12471609/0001-Don-t-echo-data-during-compaction.patch","22/Feb/11 15:35;slebresne;2216_v2.patch;https://issues.apache.org/jira/secure/attachment/12471625/2216_v2.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20512,,,Tue Feb 22 18:57:55 UTC 2011,,,,,,,,,,"0|i0ga13:",93055,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"22/Feb/11 12:24;slebresne;Attached patch completely remove the echoing of data when we have only one row. We could easily, as in CASSANDRA-2211, echo data if the sstable we are echoing from is at the last version.

However, not doing so will allow potentially corrupted sstable to get repaired by compaction (in the case of corruption from the bloom filter change).

We could add back the echoing optimisation later on.;;;","22/Feb/11 15:15;slebresne;Attaching v2 that does the optimisation of checking for last version. Implies we repair the inconsistencies introduced outside of compaction.;;;","22/Feb/11 16:40;jbellis;bq. We could easily echo data if the sstable we are echoing from is at the last version

Let's do that, and introduce a separate command to force deserialization.  (Telling people ""compact to fix it"" is not something we want to do since that leaves you with One Big SSTable and all the problems associated w/ that.);;;","22/Feb/11 17:32;jbellis;committed v2;;;","22/Feb/11 18:57;hudson;Integrated in Cassandra-0.7 #304 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/304/])
    fix compaction echoing old-style data into new sstable version
patch by slebresne; reviewed by jbellis for CASSANDRA-2216
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A bug in BufferedRandomAccessFile,CASSANDRA-2213,12499317,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,leojay,leojay,leojay,22/Feb/11 09:15,16/Apr/19 09:33,14/Jul/23 05:52,22/Feb/11 18:04,0.7.3,,,,,,0,,,,"The first line of BufferedRandomAccessFile.readAtMost is
{code}if (length >= bufferEnd && hitEOF){code}

I think It should be "">"" instead of "">="",
Here is a test for this:{code}
    @Test
    public void testRead() throws IOException {
        File tmpFile = File.createTempFile(""readtest"", ""bin"");
        tmpFile.deleteOnExit();

        // Create the BRAF by filename instead of by file.
        BufferedRandomAccessFile rw = new BufferedRandomAccessFile(tmpFile.getPath(), ""rw"");
        rw.write(new byte[] {1});

        rw.seek(0);
        byte[] buffer = new byte[1];
        assert rw.read(buffer) == 1;
        assert buffer[0] == 1;
}
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,leojay,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20509,,,Tue Feb 22 18:57:54 UTC 2011,,,,,,,,,,"0|i0ga0f:",93052,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"22/Feb/11 18:04;jbellis;committed, thanks!;;;","22/Feb/11 18:57;hudson;Integrated in Cassandra-0.7 #304 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/304/])
    avoid EOFing on requests for the last bytes in a file
patch by Leo Jay; reviewed by jbellis for CASSANDRA-2213
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot get range slice of super columns in reversed order,CASSANDRA-2212,12499291,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,muga_nishizawa,muga_nishizawa,22/Feb/11 02:36,16/Apr/19 09:33,14/Jul/23 05:52,23/Feb/11 17:55,0.6.13,0.7.3,,,,,1,,,,"I cannot get range slice of super columns in reversed order.  These data are stored in Cassandra in advance.  On the other hand, range slice of these data in normal order can be acquired.

You can reproduce the bug by executing attached programs.  
- 1. Start Cassandra daemon on localhost (number of thrift port is 9160)
- 2. Create keyspace and column family, according to ""create_table.cli"", 
- 3. Execute ""cassandra_sample_insert.py"", storing pairs of row keys and super columns
- 4. Execute ""cassandra_sample_rangeslice.py"" and get range slice of stored super columns
""cassandra_sample_insert.py"" and ""cassandra_sample_rangeslice.py"" require pycassa.  

You will need to execute 4.""cassandra_sample_rangeslice.py"" with following options so that you get range slice of super columns in reversed order.  

 % python cassandra_sample_rangeslice.py -r 00082 00083

On the other hand, to get range slice in normal order, you will need to use following options.  

 % python cassandra_sample_rangeslice.py -f 00082 00083

00082 and 00083 are the specified key range.  Range slice can be acquired in normal order but, I cannot get it in reversed order.  

I assume that there may be a bug within the code for acquiring the index block of specified range.  In fact, 00083 is included in gap between lastName of index block and firstName of next index block.   ","Fedore 11, Intel Core i5",paulrbrown,skamio,terjem,,,,,,,,,,,,,,,,,,,21600,21600,,0%,21600,21600,,,,,,,,,,,,,,,,"23/Feb/11 14:53;slebresne;0001-Fix-IndexHelp.indexFor-for-reverse-query.patch;https://issues.apache.org/jira/secure/attachment/12471739/0001-Fix-IndexHelp.indexFor-for-reverse-query.patch","18/Mar/11 14:17;slebresne;2212_0.6.patch;https://issues.apache.org/jira/secure/attachment/12473997/2212_0.6.patch","22/Feb/11 02:39;muga_nishizawa;cassandra_sample_insert.py;https://issues.apache.org/jira/secure/attachment/12471596/cassandra_sample_insert.py","22/Feb/11 02:40;muga_nishizawa;cassandra_sample_rangeslice.py;https://issues.apache.org/jira/secure/attachment/12471597/cassandra_sample_rangeslice.py","22/Feb/11 02:39;muga_nishizawa;create_table.cli;https://issues.apache.org/jira/secure/attachment/12471595/create_table.cli",,,,,,,,,,5.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20508,,,Fri Mar 18 18:11:34 UTC 2011,,,,,,,,,,"0|i0ga07:",93051,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"22/Feb/11 05:11;thobbs;I am able to reproduce this with revision r1072164 (from Feb. 18, a couple days after 0.7.2) on a single node.  Other column ranges work as expected for reversed slices.;;;","23/Feb/11 14:53;slebresne;Thanks a lot Muga for the test script, and you are right, this a bug in getting the index block during reverse queries.

Patch attached with unit tests for IndexHelper.;;;","23/Feb/11 17:15;jbellis;Sylvain, can you summarize the bug and how this patch fixes it?;;;","23/Feb/11 17:32;slebresne;Sure.

The problem is that we were not picking the right index slot for reverse query. Let's take the example from the unit test, and say your index look like this:
  [0..5][10..15][20..25]

And say you look for the slice [13..17]. When doing forward slice, we we doing a binary search comparing 13 (the start of the query) to the lastName part of the index slot, which is fine. You'll end up with the ""first"" slot, going from left to right, that may contain the start.

When doing a reverse query, we were doing the same thing, only using as a start column the end of the query, aka 17 in my example. However, comparing 17 with the lastName of each index slot, you end up selecting the last slot, which is wrong (the slice exit early since 17 is not in the range).

What you want to do is pick the ""first"" slot, but now going from right to left, that may contain start. So you want to find the slot where firstName > start and take the slot just before.

I hope I'm clear. Anyway, that's what the patch does. ;;;","23/Feb/11 17:55;jbellis;committed, with your explanation added as a comment to indexFor :);;;","23/Feb/11 18:54;hudson;Integrated in Cassandra-0.7 #311 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/311/])
    fix reversed slice queries on large rows
patch by slebresne; reviewed by jbellis for CASSANDRA-2212
;;;","18/Mar/11 08:13;paulrbrown;I think that I have a situation where this occurs against 0.6.8 as well.  Is this fix suitable for backporting onto 0.6?;;;","18/Mar/11 14:17;slebresne;Attaching patch against 0.6;;;","18/Mar/11 18:11;jbellis;committed for 0.6.13;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup can create sstables whose contents do not match their advertised version,CASSANDRA-2211,12499284,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,21/Feb/11 23:28,16/Apr/19 09:33,14/Jul/23 05:52,22/Feb/11 14:53,0.7.3,,,,,,0,,,,"Since cleanup switched to per-sstable operation (CASSANDRA-1916), the main loop looks like this:

{code}
                    if (Range.isTokenInRanges(row.getKey().token, ranges))
                    {
                        writer = maybeCreateWriter(sstable, compactionFileLocation, expectedBloomFilterSize, writer);
                        writer.append(new EchoedRow(row));
                        totalkeysWritten++;
                    }
                    else
                    {
                        while (row.hasNext())
                        {
                            IColumn column = row.next();
                            if (indexedColumns.contains(column.name()))
                                Table.cleanupIndexEntry(cfs, row.getKey().key, column);
                        }
                    }
{code}

... that is, rows that haven't changed we copy to the new sstable without deserializing, with EchoedRow.  But, the new sstable is created with CURRENT_VERSION which may not be what the old data consisted of.

(This could cause symptoms similar to CASSANDRA-2195 but I do not think it is the cause of that bug; IIRC the cluster in question there was not upgraded from an older Cassandra.)",,cburroughs,stuhood,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,"22/Feb/11 12:40;slebresne;0001-2211-v3.patch;https://issues.apache.org/jira/secure/attachment/12471610/0001-2211-v3.patch","22/Feb/11 05:43;jbellis;2211-v2.txt;https://issues.apache.org/jira/secure/attachment/12471599/2211-v2.txt","21/Feb/11 23:49;jbellis;2211.txt;https://issues.apache.org/jira/secure/attachment/12471584/2211.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20507,,,Tue Feb 22 15:20:55 UTC 2011,,,,,,,,,,"0|i0g9zz:",93050,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"21/Feb/11 23:44;jbellis;This bug was introduced in 0.7.0 but in practice was not a problem until we changed sstable formats again in 0.7.1 for CASSANDRA-1555.;;;","22/Feb/11 02:41;jbellis;A better fix would be to have it echo if the data is on the current version, otherwise rewrite.  This would (a) be a better fit with our policy of not having to keep code around to write old versions and (b) allow a better upgrade path to version N + 1 (that doesn't support the old version sstables) than major compaction. I'll see if I can do that tonight.;;;","22/Feb/11 05:43;jbellis;v2 as described above.;;;","22/Feb/11 12:40;slebresne;+1 on the patch. I'm just attaching a v3 that simply use getDefaultGcBefore() throughout CompactionManager (to make things cleaner)

Sadly, this is not the only place where we echo data wrongfully, cf. CASSANDRA-2216;;;","22/Feb/11 14:53;jbellis;committed;;;","22/Feb/11 15:20;hudson;Integrated in Cassandra-0.7 #303 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/303/])
    fix for cleanup writing old-format data into new-version sstable
patch by jbellis; reviewed by slebresne for CASSANDRA-2211
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error saving cache on Windows,CASSANDRA-2207,12499223,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,tantra,tantra,21/Feb/11 12:14,16/Apr/19 09:33,14/Jul/23 05:52,23/Feb/11 14:10,0.7.3,,,,,,0,,,,"I launch clean cassandra 7.2 instalation, and after few days i look at system.log follow error (more then 10 times):


ERROR [CompactionExecutor:1] 2011-02-19 02:56:17,965 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.io.IOException: Unable to rename cache to F:\Cassandra\7.2\saved_caches\system-LocationInfo-KeyCache
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Unable to rename cache to F:\Cassandra\7.2\saved_caches\system-LocationInfo-KeyCache
    at org.apache.cassandra.io.sstable.CacheWriter.saveCache(CacheWriter.java:85)
    at org.apache.cassandra.db.CompactionManager$9.runMayThrow(CompactionManager.java:746)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    ... 6 more
",WindowsXP(SP3) 32 bit,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"21/Feb/11 21:27;jbellis;2207.txt;https://issues.apache.org/jira/secure/attachment/12471579/2207.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20504,,,Wed Feb 23 15:18:09 UTC 2011,,,,,,,,,,"0|i0g9z3:",93046,,mdennis,,mdennis,Low,,,,,,,,,,,,,,,,,"21/Feb/11 21:27;jbellis;Apparently Windows is won't let you rename over an existing file. Patch attached to explicitly delete the old one first.;;;","23/Feb/11 07:31;mdennis;+1;;;","23/Feb/11 14:10;jbellis;committed;;;","23/Feb/11 15:18;hudson;Integrated in Cassandra-0.7 #309 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/309/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Startup fails due to cassandra trying to delete nonexisting file,CASSANDRA-2206,12499221,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,tbritz,tbritz,21/Feb/11 11:57,16/Apr/19 09:33,14/Jul/23 05:52,24/Feb/11 15:14,0.7.3,,,,,,0,,,,"Hi,

On one of our nodes, startup fails due to cassandra trying to delete a nonexistant ""Data"" file (see below).

Why that file is missing is another mistery... The log file entries don't show any ERROR messages before cassandra restarted (for reasons I don't know) and this error occured.

Directory listing:

total 109M
-rw-r--r-- 1 root root  51M 2011-02-21 05:25 table_task-f-1666-Data.db
-rw-r--r-- 1 root root 243K 2011-02-21 05:25 table_task-f-1666-Filter.db
-rw-r--r-- 1 root root 6.1M 2011-02-21 05:25 table_task-f-1666-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 05:25 table_task-f-1666-Statistics.db
-rw-r--r-- 1 root root 9.8M 2011-02-21 11:36 table_task-f-1703-Data.db
-rw-r--r-- 1 root root  57K 2011-02-21 11:36 table_task-f-1703-Filter.db
-rw-r--r-- 1 root root 1.3M 2011-02-21 11:36 table_task-f-1703-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:36 table_task-f-1703-Statistics.db
-rw-r--r-- 1 root root 292K 2011-02-21 11:42 table_task-f-1704-Data.db
-rw-r--r-- 1 root root 1.7K 2011-02-21 11:42 table_task-f-1704-Filter.db
-rw-r--r-- 1 root root  42K 2011-02-21 11:42 table_task-f-1704-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:42 table_task-f-1704-Statistics.db
-rw-r--r-- 1 root root 364K 2011-02-21 11:52 table_task-f-1705-Data.db
-rw-r--r-- 1 root root 2.0K 2011-02-21 11:52 table_task-f-1705-Filter.db
-rw-r--r-- 1 root root  50K 2011-02-21 11:52 table_task-f-1705-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:52 table_task-f-1705-Statistics.db
-rw-r--r-- 1 root root 535K 2011-02-21 12:10 table_task-f-1706-Data.db
-rw-r--r-- 1 root root 2.8K 2011-02-21 12:10 table_task-f-1706-Filter.db
-rw-r--r-- 1 root root  70K 2011-02-21 12:10 table_task-f-1706-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 12:10 table_task-f-1706-Statistics.db
-rw-r--r-- 1 root root  11M 2011-02-21 12:11 table_task-f-1707-Data.db
-rw-r--r-- 1 root root  18M 2011-02-21 09:47 table_task_meta-f-417-Data.db
-rw-r--r-- 1 root root 271K 2011-02-21 09:47 table_task_meta-f-417-Filter.db
-rw-r--r-- 1 root root 6.7M 2011-02-21 09:47 table_task_meta-f-417-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 09:47 table_task_meta-f-417-Statistics.db
-rw-r--r-- 1 root root 1.2M 2011-02-21 10:47 table_task_meta-f-418-Data.db
-rw-r--r-- 1 root root  18K 2011-02-21 10:47 table_task_meta-f-418-Filter.db
-rw-r--r-- 1 root root 460K 2011-02-21 10:47 table_task_meta-f-418-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 10:47 table_task_meta-f-418-Statistics.db
-rw-r--r-- 1 root root 791K 2011-02-21 11:47 table_task_meta-f-419-Data.db
-rw-r--r-- 1 root root  13K 2011-02-21 11:47 table_task_meta-f-419-Filter.db
-rw-r--r-- 1 root root 311K 2011-02-21 11:47 table_task_meta-f-419-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:47 table_task_meta-f-419-Statistics.db
-rw-r--r-- 1 root root  57K 2011-02-21 12:11 table_task-tmp-f-1707-Filter.db
-rw-r--r-- 1 root root 1.4M 2011-02-21 12:11 table_task-tmp-f-1707-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 12:11 table_task-tmp-f-1707-Statistics.db


Cassandra log:

/software/cassandra/bin/cassandra
rm: cannot remove `/software/cassandra/lib/jna.jar': No such file or directory
root@intr1n3:/cassandra/data/table_task#  INFO 12:47:29,020 Logging initialized
 INFO 12:47:29,030 Heap size: 2614493184/2614493184
 INFO 12:47:29,031 JNA not found. Native methods will be disabled.
 INFO 12:47:29,038 Loading settings from file:/software/cassandra/conf/cassandra.yaml
 INFO 12:47:29,320 DiskAccessMode is standard, indexAccessMode is mmap
 INFO 12:47:29,332 Creating new commitlog segment /hd1/cassandra_md5/commitlog/CommitLog-1298288849332.log
 INFO 12:47:29,422 Opening /cassandra/data/system/Schema-f-244
 INFO 12:47:29,434 Opening /cassandra/data/system/Migrations-f-244
 INFO 12:47:29,437 Opening /cassandra/data/system/LocationInfo-f-137
 INFO 12:47:29,440 Opening /cassandra/data/system/HintsColumnFamily-f-352
 INFO 12:47:29,441 Opening /cassandra/data/system/HintsColumnFamily-f-353
 INFO 12:47:29,465 Loading schema version 54bc134e-2229-11e0-9159-fdf0b6b4b562
 WARN 12:47:29,623 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
ERROR 12:47:29,638 Exception encountered during startup.
java.io.IOError: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:145)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:468)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:153)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
Caused by: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:51)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:41)
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:133)
        ... 4 more
Exception encountered during startup.
java.io.IOError: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:145)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:468)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:153)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
Caused by: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:51)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:41)
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:133)
        ... 4 more
",linux,mdennis,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"23/Feb/11 23:44;jbellis;2206-v2.txt;https://issues.apache.org/jira/secure/attachment/12471786/2206-v2.txt","21/Feb/11 14:52;jbellis;2206.txt;https://issues.apache.org/jira/secure/attachment/12471565/2206.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20503,,,Thu Feb 24 15:39:39 UTC 2011,,,,,,,,,,"0|i0g9yv:",93045,,gdusbabek,,gdusbabek,Normal,,,,,,,,,,,,,,,,,"21/Feb/11 14:37;jbellis;You can manually fix this by removing the ""tmp"" from all the tmp-f-1707 files.;;;","21/Feb/11 14:51;jbellis;Patch to fix this for new files by making sure we rename -Data last.;;;","23/Feb/11 07:57;mdennis;renaming -data files last doesn't actually fix the problem.

You can test this by running

{noformat}
cd /var/lib/cassandra/data/system
touch Schema-tmp-f-763-Data.db  Schema-f-763-Filter.db
{noformat}

before starting cassandra.

This would simulate a crash between renaming all the the components besides -data and renaming -data (though really a crash any time between when the first component is renamed and before -data is renamed will exhibit the issue).
;;;","23/Feb/11 23:44;jbellis;v2 also fixes a bug where tmp and non-tmp Descriptors were considered equal, which is what confused the file scanning for scrub in your example;;;","24/Feb/11 15:12;gdusbabek;+1;;;","24/Feb/11 15:14;jbellis;committed;;;","24/Feb/11 15:39;hudson;Integrated in Cassandra-0.7 #316 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/316/])
    improve detection and cleanup ofpartially-written sstables
patch by jbellis; reviewed by gdusbabek for CASSANDRA-2206
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java doesn't insert the correct amount of rows,CASSANDRA-2200,12499092,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,18/Feb/11 22:29,16/Apr/19 09:33,14/Jul/23 05:52,23/Feb/11 19:34,0.7.3,,,,,,0,,,,"For example, if you pass -n 2000000 you only get 1999800 (with 300 threads at least, didn't check if it was related)",,cburroughs,,,,,,,,,,,,,,,,,,,,,28800,28800,,0%,28800,28800,,,,,,,,,,,,,,CASSANDRA-2020,,,,,,,,,,,,,,,,0.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20500,,,Wed Feb 23 19:34:23 UTC 2011,,,,,,,,,,"0|i0g9xj:",93039,,,,,Low,,,,,,,,,,,,,,,,,"18/Feb/11 22:36;xedin;this is related to gaussian function and (when -r is used) pseudo-random generator which generate the same keys in some circumstances. We either should acknowledge this is an known issue or retry key generation (which could take infinite time).;;;","18/Feb/11 22:38;jbellis;_generation_ should be deterministic.  key distribution functions should only be applied to reads.;;;","18/Feb/11 22:52;xedin;oh sorry for misleading comment, of course this is about reads. for write however - each thread was a portion of range of keys to generate and that range is calculated in the following way:

(keysPerThread * (idx + keysToSkip), keysPerThread * (idx + 1)), where _idx_ - index of the current thread, _keysPerThread_ - totalKeys / threadCount, _keysToSkip_ - determined by user in params.

This is ported from python code (line 203). For some numbers of the total keys and threadCount it won't generate precise ranges. Seems like we'll need to range one more thread at the end to generate those missing rows sometimes.;;;","18/Feb/11 23:03;brandon.williams;Maybe we should just move forward with CASSANDRA-2020 then.;;;","18/Feb/11 23:16;xedin;I concur. It seems to me that porting from py_stress wasn't such a good idea, stress.java needs re-design.;;;","23/Feb/11 19:34;brandon.williams;Solved by CASSANDRA-2020;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hyphenated index names cause problems,CASSANDRA-2196,12499061,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,sgpope,sgpope,18/Feb/11 16:16,16/Apr/19 09:33,14/Jul/23 05:52,26/Feb/11 01:19,0.7.3,,,,,,0,,,,"When inserting a large number of entries with batch_insert (100000) using thrift compiled into C# there's a NumberFormatException that occurs.

The first logged entry that tipped me off was this:
 INFO 10:53:52,171 Writing Memtable-TransactionLogs.client-hostname@350930888(1171371 bytes, 32787 o
perations)
ERROR 10:53:52,171 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NumberFormatException: For input string: ""tmp""
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NumberFormatException: For input string: ""tmp""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
        at java.lang.Integer.parseInt(Integer.java:449)
        at java.lang.Integer.parseInt(Integer.java:499)
        at org.apache.cassandra.io.sstable.Descriptor.fromFilename(Descriptor.java:154)
        at org.apache.cassandra.io.sstable.Descriptor.fromFilename(Descriptor.java:119)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:67)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:156)
        at org.apache.cassandra.db.Memtable.access$000(Memtable.java:49)
        at org.apache.cassandra.db.Memtable$1.runMayThrow(Memtable.java:174)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more

 Which points to the suspect piece of code in Descriptor.java:154 (browse at https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/Descriptor.java)

 The file I believe it's trying to parse is mentioned in my logs as:

INFO 10:51:31,231 Compacted to C:\cassandra\apache-cassandra-0.7.2\bin\..\Storage\data\system\Index
Info-tmp-f-6-Data.db.  384 to 225 (~58% of original) bytes for 1 keys.  Time: 281ms.

 I'm new here, so I'm not sure what needs fixing here (the filename, or the parsing of it).","Cassandra 0.7.2

Windows 7 64-bit
java version ""1.6.0_23""
Java(TM) SE Runtime Environment (build 1.6.0_23-b05)
Java HotSpot(TM) 64-Bit Server VM (build 19.0-b09, mixed mode)
",,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"18/Feb/11 19:12;jbellis;2196.txt;https://issues.apache.org/jira/secure/attachment/12471434/2196.txt","25/Feb/11 23:32;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2196-invoke-toString-instead-of-casting.txt;https://issues.apache.org/jira/secure/attachment/12471994/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2196-invoke-toString-instead-of-casting.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20496,,,Sat Feb 26 02:40:31 UTC 2011,,,,,,,,,,"0|i0g9wn:",93035,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"18/Feb/11 16:49;jbellis;The bug is that it shouldn't be trying to open a file with ""tmp"" in the name.

I wonder if this is another one of those bugs where we have unix-oriented assumptions about opened files, that don't hold under Windows.  Can you doublecheck that there are no earlier ERROR or WARN lines in the log?;;;","18/Feb/11 16:56;sgpope;No errors before that one, and the only warning before that is:
 WARN 10:50:25,748 Generated random token Token(bytes[c010b410364388921ed82a633849a3cc]). Random tok
ens will result in an unbalanced ring; see http://wiki.apache.org/cassandra/Operations
;;;","18/Feb/11 18:32;jbellis;I see the problem. Your CF is named client-hostname, but - is an illegal character in columnfamily names.  Apparently the code that checks that got broken at some point.  How did you create the CF, through the CLI?;;;","18/Feb/11 18:43;sgpope;I'm confused. My CFs are TransactionLogs and Terms. The CF in the log entry is the system-generated one. I created my CFs in code.;;;","18/Feb/11 18:52;jbellis;Ah, I thought ""TransactionLogs.client-hostname"" was KS.CF but it must be CF.indexname. ;;;","18/Feb/11 19:00;sgpope;Sorry, yeah. I should've mentioned that client-hostname is one of my indexes.;;;","18/Feb/11 19:12;jbellis;patch to keep invalid characters out of index names.

if you can afford to lose the data the easiest fix for this CF is to drop and recreate it.;;;","18/Feb/11 19:15;sgpope;I can afford to lose it. Thanks!;;;","18/Feb/11 19:28;gdusbabek;+1;;;","18/Feb/11 20:56;hudson;Integrated in Cassandra-0.7 #293 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/293/])
    validate index namesfor \w+
patch by jbellis; reviewed by gdusbabek for CASSANDRA-2196
;;;","21/Feb/11 21:15;jbellis;committed;;;","25/Feb/11 23:31;urandom;r1072123 broke the CQL system tests with the following logged exception:

{noformat}
java.lang.ClassCastException: org.apache.avro.util.Utf8 cannot be cast to java.lang.String
        at org.apache.cassandra.db.migration.UpdateColumnFamily.<init>(UpdateColumnFamily.java:52)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:638)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1209)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4576)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:3235)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:188)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{noformat}

(Trivial )patch attached.;;;","25/Feb/11 23:37;gdusbabek;+1;;;","26/Feb/11 02:40;hudson;Integrated in Cassandra #746 (See [https://hudson.apache.org/hudson/job/Cassandra/746/])
    invoke toString() instead of casting

Patch by eevans; reviewed by gdusbabek for CASSANDRA-2196
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
json2sstable fails due to OutOfMemory,CASSANDRA-2189,12498991,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,skamio,skamio,18/Feb/11 02:53,16/Apr/19 09:33,14/Jul/23 05:52,08/Dec/11 16:39,0.8.9,1.0.6,,Legacy/Tools,,,0,,,,"I have a json file created with sstable2json for a column family of super column type. Its size is about 1.9GB. (It's a dump of all keys because I cannot find out how to specify keys to dump in sstable2json.)
When I tried to create sstable from the json file, it failed with OutOfMemoryError as follows.

 WARN 00:31:58,595 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
Exception in thread ""main"" java.lang.OutOfMemoryError: PermGen space
        at java.lang.String.intern(Native Method)
        at org.codehaus.jackson.util.InternCache.intern(InternCache.java:40)
        at org.codehaus.jackson.sym.BytesToNameCanonicalizer.addName(BytesToNameCanonicalizer.java:471)
        at org.codehaus.jackson.impl.Utf8StreamParser.addName(Utf8StreamParser.java:893)
        at org.codehaus.jackson.impl.Utf8StreamParser.findName(Utf8StreamParser.java:773)
        at org.codehaus.jackson.impl.Utf8StreamParser.parseLongFieldName(Utf8StreamParser.java:379)
        at org.codehaus.jackson.impl.Utf8StreamParser.parseMediumFieldName(Utf8StreamParser.java:347)
        at org.codehaus.jackson.impl.Utf8StreamParser._parseFieldName(Utf8StreamParser.java:304)
        at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:140)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.mapObject(UntypedObjectDeserializer.java:93)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.deserialize(UntypedObjectDeserializer.java:65)
        at org.codehaus.jackson.map.deser.MapDeserializer._readAndBind(MapDeserializer.java:197)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:145)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:23)
        at org.codehaus.jackson.map.ObjectMapper._readValue(ObjectMapper.java:1261)
        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:517)
        at org.codehaus.jackson.JsonParser.readValueAs(JsonParser.java:897)
        at org.apache.cassandra.tools.SSTableImport.importUnsorted(SSTableImport.java:208)
        at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:197)
        at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:421)

So, what I had to is that split the json file with ""split"" command and modify them to be correct json file. Create sstable for each small json files.

Could you change json2sstable to avoid OutOfMemory?",linux,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"07/Dec/11 23:54;jbellis;2189-2.txt;https://issues.apache.org/jira/secure/attachment/12506542/2189-2.txt","18/Feb/11 06:58;jbellis;2189.txt;https://issues.apache.org/jira/secure/attachment/12471370/2189.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20491,,,Thu Dec 08 17:37:14 UTC 2011,,,,,,,,,,"0|i0g9v3:",93028,,,,,Low,,,,,,,,,,,,,,,,,"18/Feb/11 03:00;jbellis;That kind of looks like a jackson bug to me.  I'll ask Tatu.;;;","18/Feb/11 06:58;jbellis;patch to disable interning.  (Thanks to Tatu for pointing me at the right Feature.);;;","21/Feb/11 21:13;jbellis;Shotaro, can you test the patch?;;;","23/Feb/11 23:28;jbellis;committed;;;","23/Feb/11 23:46;hudson;Integrated in Cassandra-0.7 #313 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/313/])
    turnoff string interning in json2sstable
patch by jbellis for CASSANDRA-2189
;;;","07/Dec/11 23:46;jbellis;This didn't actually fix the problem.  Tatu again:

""This calls configure on parser; but at that point the symbol table has already been created. So configure must be called on the factory first (and only need to be called once, really), and then settings will be passed as expected."";;;","07/Dec/11 23:54;jbellis;Patch attached to move configuration to the factory.;;;","08/Dec/11 16:39;jbellis;Customer testing indicates that this is a big improvement, especially when combined with an upgrade to Jackson 1.9.2.  I'll commit this patch to 0.8.9 and 1.0.6, and upgrade Jackson in trunk.;;;","08/Dec/11 17:37;hudson;Integrated in Cassandra-0.8 #413 (See [https://builds.apache.org/job/Cassandra-0.8/413/])
    turn off string interning in json2sstable, take 2
patch by jbellis; tested by George Ciubotaru for CASSANDRA-2189

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1211976
Files : 
* /cassandra/branches/cassandra-0.8
* /cassandra/branches/cassandra-0.8/CHANGES.txt
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/tools/SSTableImport.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"sstable2json generates invalid json for ""paged"" rows",CASSANDRA-2188,12498989,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,skamio,skamio,18/Feb/11 02:43,16/Apr/19 09:33,14/Jul/23 05:52,22/Feb/11 17:38,0.7.3,0.8 beta 1,,Legacy/Tools,,,0,,,,"I have a json file created with sstable2json for a column family of super column type. But json2sstable failed to create sstable from the file. It's because file format is wrong. 

 WARN 11:41:55,141 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
org.codehaus.jackson.JsonParseException: Unexpected character ('""' (code 34)): was expecting comma to separate OBJECT entries
 at [Source: dump.json; line: 2, column: 739439661]
        at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:929)
        at org.codehaus.jackson.impl.JsonParserBase._reportError(JsonParserBase.java:632)
        at org.codehaus.jackson.impl.JsonParserBase._reportUnexpectedChar(JsonParserBase.java:565)
        at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:128)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.mapObject(UntypedObjectDeserializer.java:93)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.deserialize(UntypedObjectDeserializer.java:65)
        at org.codehaus.jackson.map.deser.MapDeserializer._readAndBind(MapDeserializer.java:197)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:145)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:23)
        at org.codehaus.jackson.map.ObjectMapper._readValue(ObjectMapper.java:1261)
        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:517)
        at org.codehaus.jackson.JsonParser.readValueAs(JsonParser.java:897)
        at org.apache.cassandra.tools.SSTableImport.importUnsorted(SSTableImport.java:208)
        at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:197)
        at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:421)
ERROR: Unexpected character ('""' (code 34)): was expecting comma to separate OBJECT entries
 at [Source: dump.json; line: 2, column: 739439661]

When I looked at the file, I found that a comma is missing between super columns. The part of data is like this: 

[""756e697473"",
 ""32"",
 1297926692097000, false]]}""32303036303830373135303030302f313030303030303030302d32303036313030322d303030303030303639382d612f30"": {
""deletedAt"": -9223372036854775808,
 ""subColumns"": [[""5f64656c"",
 """",
 1297926692097000,
 false],

You'll see no comma between } and "". 

",linux,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"22/Feb/11 12:14;xedin;CASSANDRA-2188.patch;https://issues.apache.org/jira/secure/attachment/12471607/CASSANDRA-2188.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20490,,,Tue Feb 22 18:57:54 UTC 2011,,,,,,,,,,"0|i0g9uv:",93027,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"18/Feb/11 10:03;xedin;This is a problem when sstable2json then, can you please tell me the version of the cassandra from which you were running sstable2json? can you regenerate json file using the lastest version of the cassandra and check if it is corrent using for example http://www.jsonlint.com/? Because I can't reproduce a problem which broken JSON for super column families on my side and need a bit more details on this...;;;","22/Feb/11 04:47;muga_nishizawa;Hi Pavel, 

I was able to reproduce the problem above with my sstable file.  I explained the detail of how to generate problematic sstable on CASSANDRA-2212.  Please check it.  
;;;","22/Feb/11 05:01;jbellis;I'm not sure what CASSANDRA-2212 has to do with json. Do you mean we should close this ticket?;;;","22/Feb/11 12:14;xedin;Fixed problem in SSTable2JSON which was causing this error - no delimiter was set after each of the row paged part.;;;","22/Feb/11 17:38;jbellis;committed;;;","22/Feb/11 18:57;hudson;Integrated in Cassandra-0.7 #304 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/304/])
    fix sstable2json large-row pagination
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-2188
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra Cli hangs forever if schema does not settle within timeout window,CASSANDRA-2187,12498988,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,18/Feb/11 02:24,16/Apr/19 09:33,14/Jul/23 05:52,22/Mar/11 22:18,0.7.3,,,,,,0,,,,"validateSchemaIsSettled will hang in the while loop since we never update start if migrations never settle.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/11 02:30;lenn0x;0001-Fix-Cassandra-cli-to-respect-timeout-if-schema-does-.patch;https://issues.apache.org/jira/secure/attachment/12471359/0001-Fix-Cassandra-cli-to-respect-timeout-if-schema-does-.patch",,,,,,,,,,,,,,1.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20489,,,Wed Feb 23 15:18:09 UTC 2011,,,,,,,,,,"0|i0g9un:",93026,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Feb/11 02:40;jbellis;kind of confusing flow to have a start variable that we keep mutating.  simpler to just eliminate it and use System.cTM in the loop condition?;;;","18/Feb/11 02:56;hudson;Integrated in Cassandra-0.7 #291 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/291/])
    Fix Cassandra cli to respect timeout if schema does not settle patch by goffinet; reviewed by jbellis for CASSANDRA-2187
;;;","23/Feb/11 15:18;hudson;Integrated in Cassandra-0.7 #309 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/309/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Returning split length of 0 confuses Pig,CASSANDRA-2184,12498930,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,jbellis,jbellis,17/Feb/11 18:35,16/Apr/19 09:33,14/Jul/23 05:52,18/Feb/11 22:49,0.7.3,,,,,,1,,,,"Matt Kennedy reports on the user list,

bq. There is a new feature in Pig 0.8 that will try to reduce the number of splits used to speed up the whole job.  Since the ColumnFamilyInputFormat lists the input size as zero, this feature eliminates all of the splits except for one. 
bq. The workaround is to disable this feature for jobs that use CassandraStorage by setting -Dpig.splitCombination=false in the pig_cassandra script.
{noformat}

bq. However, we wanted to keep splitCombination on because it is a useful optimization for a lot of our use cases, so I went digging for the least intrusive way to keep the split combiner on, but also prevent it from combining splits that read from Cassandra.  My solution, which you are welcome to critique, is to change line 65 of http://svn.apache.org/viewvc/cassandra/trunk/src/java/org/apache/cassandra/hadoop/ColumnFamilySplit.java such that it returns Long.MAX_VALUE instead of zero.

I looked into actually returning the number of keys in the split but Hadoop javadoc says ""Get the size of the split, so that the input splits can be sorted by size"" so since our splits should be very very close in size this doesn't sound like it's worth doing an extra round trip to the host servers to get super accurate numbers on.  Returning MAX_VALUE seems like it's good enough.",,jeromatron,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20487,,,Fri Feb 18 22:51:53 UTC 2011,,,,,,,,,,"0|i0g9tz:",93023,,,,,Low,,,,,,,,,,,,,,,,,"18/Feb/11 22:21;brandon.williams;Committed.;;;","18/Feb/11 22:51;hudson;Integrated in Cassandra-0.7 #296 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/296/])
    Change split length from 0 to Long.MAX_VALUE
Patch by Matt Kennedy, reviewed by brandonwilliams for CASSANDRA-2184
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memtable_flush_after_mins setting not working,CASSANDRA-2183,12498922,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,chenyy,chenyy,17/Feb/11 16:51,16/Apr/19 09:33,14/Jul/23 05:52,23/Feb/11 04:57,0.7.3,,,,,,0,,,,"We have observed the behavior that memtable_flush_after_mins setting not working occasionally.   After some testing and code digging, we finally figured out what going on.
The memtable_flush_after_mins won't work on certain condition with current implementation in Cassandra.

In org.apache.cassandra.db.Table,  the scheduled flush task is setup by the following code during construction.

------------------------------------------------------------------------------------------------------------------
int minCheckMs = Integer.MAX_VALUE;
       
for (ColumnFamilyStore cfs : columnFamilyStores.values())  
{
    minCheckMs = Math.min(minCheckMs, cfs.getMemtableFlushAfterMins() * 60 * 1000);
}

Runnable runnable = new Runnable()
{
   public void run()
   {
       for (ColumnFamilyStore cfs : columnFamilyStores.values())
       {
           cfs.forceFlushIfExpired();
       }
   }
};
flushTask = StorageService.scheduledTasks.scheduleWithFixedDelay(runnable, minCheckMs, minCheckMs, TimeUnit.MILLISECONDS);
------------------------------------------------------------------------------------------------------------------------------

Now for our application, we will create a keyspacewithout without any columnfamily first.  And only add needed columnfamily later depends on request.

However, when keyspacegot created (without any columnfamily ), the above code will actually schedule a fixed delay flush check task with Integer.MAX_VALUE ms
since there is no columnfamily yet.

Later when you add columnfamily to this empty keyspace, the initCf() method in Table.java doesn't check whether the scheduled flush check task interval need
to be updated or not.   To fix this, we'd need to restart the Cassandra after columnfamily added into the keyspace. 

I would suggest that add additional logic in initCf() method to recreate a scheduled flush check task if needed.
",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"17/Feb/11 16:57;jbellis;2183.txt;https://issues.apache.org/jira/secure/attachment/12471294/2183.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20486,,,Wed Feb 23 15:18:09 UTC 2011,,,,,,,,,,"0|i0g9tr:",93022,,scode,,scode,Low,,,,,,,,,,,,,,,,,"17/Feb/11 16:57;jbellis;The original approach of checking barely more often than we think is necessary is overengineering the problem; even with 1000s of CFs, checking every 10s would have no affect whatsoever on performance.  Patch attached.;;;","21/Feb/11 22:25;scode;(was asked to review) I agree on the overengineering. Patch looks good to me. The obvious caveat of being limited to 10 second resolution should be utterly irrelevant for the purposes for which the time based flushing is intended.
;;;","23/Feb/11 04:57;jbellis;committed;;;","23/Feb/11 15:18;hudson;Integrated in Cassandra-0.7 #309 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/309/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra doesn't startup on single core boxes.,CASSANDRA-2182,12498911,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,tjake,tjake,tjake,17/Feb/11 16:13,16/Apr/19 09:33,14/Jul/23 05:52,17/Feb/11 16:31,0.7.3,,,,,,0,,,,"I happened to run cassandra in a VM and got the following error, caused by the single core:

ERROR 10:47:30,304 Exception encountered during startup.
java.lang.AssertionError: multi-threaded stages must have at least 2 threads
        at org.apache.cassandra.concurrent.StageManager.multiThreadedStage(StageManager.java:60)
        at org.apache.cassandra.concurrent.StageManager.<clinit>(StageManager.java:53)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:303)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:159)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:175)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"17/Feb/11 16:15;tjake;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-2182-rr-pool-needs-at-least-two-threads.txt;https://issues.apache.org/jira/secure/attachment/12471285/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-2182-rr-pool-needs-at-least-two-threads.txt",,,,,,,,,,,,,,1.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20485,,,Thu Feb 17 17:23:07 UTC 2011,,,,,,,,,,"0|i0g9tj:",93021,,,,,Low,,,,,,,,,,,,,,,,,"17/Feb/11 16:18;jbellis;+1 (this was from CASSANDRA-2069 which was committed after 0.7.2);;;","17/Feb/11 17:23;hudson;Integrated in Cassandra-0.7 #287 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/287/])
    read repair stage requires a minimum of 2 threads
patch by tjake for CASSANDRA-2182
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memtable Flush writers doesn't actually flush in parallel,CASSANDRA-2178,12498818,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,17/Feb/11 01:45,16/Apr/19 09:33,14/Jul/23 05:52,17/Feb/11 02:44,0.7.3,0.8 beta 1,,,,,0,,,,"The flushWriter JMXEnabledThreadPoolExecutor sets the core pool min to 1, and sets the LBQ to DatabaseDescriptor.getFlushWriters(). Increasing memtable_flush_writers should allow us to flush more in parallel. The pool will not grow until LBQ fills up to DatabaseDescriptor.getFlushWriters(). ",,mdennis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/11 02:35;lenn0x;0001-Set-the-core-min-pool-size-to-memtable_flush_writers-v2.patch;https://issues.apache.org/jira/secure/attachment/12471245/0001-Set-the-core-min-pool-size-to-memtable_flush_writers-v2.patch","17/Feb/11 01:47;lenn0x;0001-Set-the-core-min-pool-size-to-memtable_flush_writers.patch;https://issues.apache.org/jira/secure/attachment/12471239/0001-Set-the-core-min-pool-size-to-memtable_flush_writers.patch",,,,,,,,,,,,,2.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20482,,,Tue Mar 15 18:12:00 UTC 2011,,,,,,,,,,"0|i0g9sn:",93017,,,,,Low,,,,,,,,,,,,,,,,,"17/Feb/11 02:08;jbellis;Wouldn't setting {core=1, max=flush_writers, queue=SynchronousQueue) be better since it allows the pool size to adjust as needed?;;;","17/Feb/11 02:35;lenn0x;Use a SynchronousQueue instead;;;","17/Feb/11 02:41;jbellis;+1;;;","17/Feb/11 02:44;lenn0x;commited to 0.7 and merged into trunk;;;","17/Feb/11 05:40;hudson;Integrated in Cassandra-0.7 #283 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/283/])
    Use a SynchronousQueue instead of LBQ so when memtable_flush_writers is > 1, it will allow actual parallel flushes. 
patch by goffinet reviewed by jbellis for CASSANDRA-2178
;;;","15/Mar/11 18:12;jbellis;see CASSANDRA-2333 for related work.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
saved caches written with BufferedRandomAccessFile cannot be read by ObjectInputStream,CASSANDRA-2174,12498718,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mdennis,mdennis,mdennis,16/Feb/11 04:35,16/Apr/19 09:33,14/Jul/23 05:52,16/Feb/11 21:38,0.7.3,,,,,,0,,,,The CacheWriter is currently writing with BufferedRandomAccessFile which is incompatible with ObjectInputStream resulting in stack traces about corrupted stream headers when loading a saved cache.,,cburroughs,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"16/Feb/11 20:21;mdennis;2174-cassandra-0.7.txt;https://issues.apache.org/jira/secure/attachment/12471220/2174-cassandra-0.7.txt",,,,,,,,,,,,,,1.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20480,,,Wed Feb 16 22:52:17 UTC 2011,,,,,,,,,,"0|i0g9rr:",93013,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"16/Feb/11 04:38;mdennis;patch is on top of CASSANDRA-2172;;;","16/Feb/11 06:05;jbellis;We don't use ObjectInputStream for cache reads anymore, and we need to use BRAF to get the non-buffer-cache-clobbering effect on cache writes.

We don't care about 0.7.0 reading 0.7.1/2 caches, since we made the choice to allow 0.7.1 to write new-format data files (CASSANDRA-1555) which is a much bigger incompatibility.

If 0.7.1/2 can't read 0.7.0, then we goofed, but changing it back (breaking 0.7.2 -> 0.7.3) seems like too late.;;;","16/Feb/11 06:17;jbellis;Oops, I applied the patch and _then_ looked for uses of OIS.  Those operations are not commutative. :);;;","16/Feb/11 06:19;jbellis;So...  the important fix is the CFS change, and the CacheWriter change is gratuitous?;;;","16/Feb/11 20:21;mdennis;I neglected to see the skipCache flag and was intending to make the read/write code symmetric.

attached patch contains only CFS change from OIS to DIS. ;;;","16/Feb/11 21:38;jbellis;committed;;;","16/Feb/11 22:52;hudson;Integrated in Cassandra-0.7 #282 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/282/])
    read saved caches with DataInputStream
patch by mdennis; reviewed by jbellis for CASSANDRA-2174
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Saved-cache files are created for empty caches,CASSANDRA-2172,12498711,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mdennis,jbellis,jbellis,16/Feb/11 02:10,16/Apr/19 09:33,14/Jul/23 05:52,16/Feb/11 05:58,0.7.3,,,,,,0,,,,This results in a harmless EOFException on startup.,,cburroughs,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"16/Feb/11 04:30;mdennis;2172-cassandra-0.7-v2.txt;https://issues.apache.org/jira/secure/attachment/12471149/2172-cassandra-0.7-v2.txt","16/Feb/11 02:18;jbellis;2172.txt;https://issues.apache.org/jira/secure/attachment/12471143/2172.txt",,,,,,,,,,,,,2.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20479,,,Wed Feb 16 06:16:58 UTC 2011,,,,,,,,,,"0|i0g9rb:",93011,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Feb/11 02:18;jbellis;patch to avoid creating file for empty cache;;;","16/Feb/11 04:30;mdennis;it's not safe to return null as some callers wait on the future.

v2 patch centralizes check and delete logic in one place and returns a valid future.;;;","16/Feb/11 04:43;mdennis;noticed CASSANDRA-2174 while reviewing this;;;","16/Feb/11 05:58;jbellis;committed v2;;;","16/Feb/11 06:16;hudson;Integrated in Cassandra-0.7 #281 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/281/])
    don't save empty caches
patch by mdennis; reviewed by jbellis for CASSANDRA-2172
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load spikes,CASSANDRA-2170,12498681,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,jbellis,jbellis,15/Feb/11 19:46,16/Apr/19 09:33,14/Jul/23 05:52,04/Oct/11 17:28,,,,,,,0,,,,"as reported on CASSANDRA-2058, some users are still seeing load spikes on 0.6.11, even with fairly low-volume read workloads.",,bcoverston,kzadorozhny,scode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2357,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,18533,,,Wed Oct 12 17:46:46 UTC 2011,,,,,,,,,,"0|i0g9qv:",93009,,,,,Normal,,,,,,,,,,,,,,,,,"15/Feb/11 19:48;jbellis;AFAIK nobody has seen this on 0.7.1.;;;","09/Mar/11 17:16;jbellis;I wonder if this could have been CASSANDRA-2175.;;;","09/Mar/11 17:16;jbellis;(I.e. memory pressure caused by key cache preheating being too aggressive.);;;","14/Mar/11 13:45;jbellis;No new reports of similar behavior on 0.6.11 or 0.6.12;;;","18/May/11 22:40;brandon.williams;I never saw anyone reliably report this on any platform except ec2, so I strongly the suspect the cause was what is covered in this link:

https://silverline.librato.com/blog/main/EC2_Users_Should_be_Cautious_When_Booting_Ubuntu_10_04_AMIs;;;","10/Aug/11 05:08;alienth;Re-opening per request of driftx.

So, still seeing this problem ever since our upgrade from 0.6.7.

It is 100% consistent on 0.7.1, 0.7.2, 0.7.3, 0.7.4, 0.8.0, 0.8.1. I've tried Sun JRE and OpenJDK. Tried with JNA and without. Tried Ubuntu 08.04/10.04/10.10/11.04, as well as RHEL5.1. It *only* happens on coordinator nodes.

For the 0.8 ring, I created a brand new ring and added data from our app one CF at a time. As soon as I added a busy CF, the problem popped up again. The load on the boxes in the new ring is under 1 all the time, except for when the load spike occurs.;;;","15/Aug/11 22:23;brandon.williams;From the comment [here|https://issues.apache.org/jira/browse/CASSANDRA-2845?focusedCommentId=13083455&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13083455] I would suggest try running without jsvc (if you are now and have JNA enabled);;;","15/Sep/11 19:34;alienth;Tried without jsvc, same result.

This problem is also consistent on 0.8.5 on Debian Squeeze. I replicated the issue on a coordinator which only owned 1 token.

The node I'm replicating it on now is at a load of 0 almost all of the time, except during the spikes.

The symptoms continue to appear identical to #2054;;;","16/Sep/11 21:27;alienth;I did a couple more thread dumps on spiking nodes. One interesting thing I'm seeing is that there are a high number of NBHM threads in the runnable state during the load spikes. One the dumps I analyzed, there were often 200-300 of these threads in RUNNABLE.

Here is an example of the threads: https://gist.github.com/ef215227b85bdff5f033;;;","17/Sep/11 20:13;scode;Wow, interesting. Are you sure it's 0.8.5 though? The stack trace is not matching what I see in the 0.8.5 tag (mismatched line number for MessagingService.addCallback()).

We've been seeing load spikes on 0.7, but havent reported it because it's such an old version. However we were never able to grab stacks because no JMX query would ever succeed during this condition.

The stack trace indicates it's stuck doing resize operations on the NBHM where each thread is trying to help the resizing operation along by performing potentially duplicate (for forward progress producing) work.

Do you have a list of all stacks? Do you find any thread (should be 0 or 1) that are executing in ExpiringMap.CacheMonitor.run() at the time of the load spikes?

I guess we're seeing some kind of fallen-and-cant-get-up senario having to do with the resize. Maybe dogpiling the resize is making it overall slow enough that it never gets unstuck without a temporary stop in incoming requests. Or some such. That's gut feely speculation without having actually looked at it carefully, so take it with a grain of salt :)
;;;","27/Sep/11 23:29;alienth;The stack trace in that example was from a 0.8.1 node. The same problem has occurred on my 0.8.5 nodes.

I checked the stack trace that I had, and I couldn't find any threads executing in any ExpiringMap stuff.

We switched to HSHA about a week ago and it appears to have resolved the loads pikes. Is the stack trace I gave interesting enough to pursue further investigation into the issue, or should I just leave the answer as 'HSHA' ?;;;","04/Oct/11 17:28;brandon.williams;bq. Is the stack trace I gave interesting enough to pursue further investigation into the issue, or should I just leave the answer as 'HSHA' ?

I'm content with calling this 'ec2 sucks when there are tons of threads.' (Jason told me he's opening 2000+ connections);;;","12/Oct/11 17:46;brandon.williams;FWIW, I was able to finally reproduce this by throwing a ton of connections at a cold cassandra instance.  Increasing the rpc_min_threads initially seems to help avoid locking the machine up briefly.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
user created with debian packaging is unable to increase memlock,CASSANDRA-2169,12498679,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,15/Feb/11 19:36,16/Apr/19 09:33,14/Jul/23 05:52,19/Feb/11 22:54,0.7.3,,,Packaging,,,0,,,,"To reproduce:
- Install a fresh copy of ubuntu 10.04.
- Install sun's java6 jdk.
- Install libjna-java 3.2.7 into /usr/share/java.
- Install cassandra 0.7.0 from the apache debian packages.
- Start cassandra using /etc/init.d/cassandra
In the output.log there will be the following error:
{quote}
Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK or run Cassandra as root.
{quote}
This shouldn't be as the debian package creates /etc/security/limits.d/cassandra.conf and sets the cassandra user's memlock limit to 'unlimited'.

I tried a variety of things including making the memlock unlimited for all users in /etc/security/limits.conf.  I was able to run cassandra using root with jna symbolically linked into /usr/share/cassandra from /usr/share/java, but I could never get the init.d script to work and get beyond that error.

Based on all the trial and error, I think it might have to do with the cassandra user itself, but my debian/ubuntu fu isn't as good as others'.",,brandon.williams,thepaul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/11 21:14;jeromatron;2169-0_7.txt;https://issues.apache.org/jira/secure/attachment/12471439/2169-0_7.txt",,,,,,,,,,,,,,1.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20478,,,Sat Feb 19 23:16:39 UTC 2011,,,,,,,,,,"0|i0g9qn:",93008,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,"15/Feb/11 19:46;jeromatron;Upgraded to 0.7.1 and still appears to have the same issue.;;;","16/Feb/11 18:39;brandon.williams;What happens if you su to the cassandra user and check ulimit?;;;","16/Feb/11 18:50;jeromatron;""ulimit -l"" always returns 64.

I gave the cassandra user a shell so I can su to that user.  When I do, the memlock doesn't ever take - whether it's defined in /etc/security/limits.conf for the cassandra user, for all users '*', or in the /etc/security/limits.d/cassandra.conf.;;;","16/Feb/11 18:53;brandon.williams;Just to be certain limits.conf is taking effect, have you tried rebooting the box since changing it?;;;","16/Feb/11 18:56;jeromatron;I made sure it took effect for the user I originally logged in as - ubuntu for my ec2 instance and jeremy for my local vm.  It took effect for those users whenever I changed it.  I also tried rebooting.  That was the way it took effect for the ec2 server's ubuntu user iirc.  But it didn't take effect for the cassandra user.;;;","16/Feb/11 21:17;jeromatron;Based on a suggestion from jake and brandon, I added {quote}session required pam_limits.so{quote} to /etc/pam.d/common-session and rebooted.  That made it so the memlock value was set correctly.  However, for some reason, using /etc/init.d/cassandra still gives the memory locking error.  I su to the cassandra user and run {quote}/usr/sbin/cassandra -f{quote} and it is able to lock the memory.

For the pam setting, see http://posidev.com/blog/2009/06/04/set-ulimit-parameters-on-ubuntu/;;;","18/Feb/11 05:10;thepaul;I grepped through jsvc's source, and I don't see any references to pam at all. So unless the jre has some special ""switch users and set up a pam session"" functionality I don't know about, jsvc isn't setting up a pam session when switching users.

This means limits.conf (and limits.d/cassandra.conf) are useless, except in what they define for root's resource limits.

If we want to use both limits.conf and jsvc, then hrm. Maybe we could switch users in the initscript using /bin/su, but afaik default debian and ubuntu systems all comment out pam_limits.so from /etc/pam.d/su , so that wouldn't work without monkeying with users' conffiles.

We could switch users in the initscript with sudo, but it's pretty hard to be sure the user hasn't done something funky with their sudoers file which would break our startup.

I can only come up with 2 halfway-decent options: both involve ditching limits.d/cassandra.conf.

1: just /bin/su in the initscript and do a 'ulimit -l unlimited' in the child before exec'ing jsvc.

2: implement limit setting and user switching in cassandra itself. is there any good way to do setrlimit() and setuid() in java?;;;","18/Feb/11 05:19;jbellis;bq. 2: implement limit setting and user switching in cassandra itself. is there any good way to do setrlimit() and setuid() in java?

No, you'd have to write a JNI wrapper or use JNA.  (Which I guess is technically feasible since JNA is packaged for both deb and rpm but eww. :);;;","18/Feb/11 05:23;thepaul;Actually, durr, you could just do 'ulimit -l unlimited' in sh before switching users. It should stick. Sorry, I'm slow tonight.

Still might be a good idea to keep limits.d/cassandra.conf around, in case people want to su or sudo to the cassandra user for testing stuff.;;;","18/Feb/11 16:21;jeromatron;Paul: so is that something that would go in the /debian configuration as a change?  Would you like me to try that in my test server and see if that resolves the problem?;;;","18/Feb/11 18:49;jeromatron;so adding that manually to the /etc/init.d/cassandra script does the trick.  I will submit a patch that adds that.  Thanks guys!;;;","18/Feb/11 21:14;jeromatron;One line patch to fix the ulimit stuff.;;;","19/Feb/11 22:54;urandom;committed; thanks!;;;","19/Feb/11 23:16;hudson;Integrated in Cassandra-0.7 #299 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/299/])
    increase memlock at daemon startup

Patch by Jeremy Hanna; reviewed by eevans for CASSANDRA-2169
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable2Json tool returns a different key when a querying for a specific key in an SSTable that does not exist,CASSANDRA-2168,12498678,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bcoverston,bcoverston,bcoverston,15/Feb/11 19:19,16/Apr/19 09:33,14/Jul/23 05:52,05/Mar/11 05:34,0.7.4,,,,,,0,,,,"bin/sstable2json storage/core/data/Foo/BAR-1-Data.db -k NonExistantKey

returns

{ ""ExistantKey"" } ",,bcoverston,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/11 05:05;bcoverston;2168.txt;https://issues.apache.org/jira/secure/attachment/12472736/2168.txt",,,,,,,,,,,,,,1.0,bcoverston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20477,,,Sat Mar 05 05:34:45 UTC 2011,,,,,,,,,,"0|i0g9qf:",93007,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Feb/11 22:00;jbellis;I think what we want is in the export overload that loops through the toExport list, we need to add a SSTableScanner.seekTo method that calls getPosition with Operator.EQ instead of GE (maybe we want to move the Operator into a parameter of SeekTo? there is only one other caller).;;;","05/Mar/11 05:02;bcoverston;I made a simpler change. All I did was make a sanity check to see if after seeking the key matched the key we were looking for. If seek reached the end of the list it would only deserialize the row if it is the one we are looking for, otherwise it will continue to the next key in the for loop.;;;","05/Mar/11 05:05;bcoverston;Patch 2168 attached for review.;;;","05/Mar/11 05:34;jbellis;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EOFException during name query,CASSANDRA-2165,12498670,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,slebresne,slebresne,15/Feb/11 18:11,16/Apr/19 09:33,14/Jul/23 05:52,15/Feb/11 20:42,0.7.2,,,,,,0,EOF,,,"As reported by Jonas Borgstrom on the mailing list:

{quote}
While testing the new 0.7.1 release I got the following exception:

ERROR [ReadStage:11] 2011-02-15 16:39:18,105
DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.io.IOError: java.io.EOFException
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:75)
       at
org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:59)
       at
org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
       at
org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1274)
       at
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1166)
       at
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1095)
       at org.apache.cassandra.db.Table.getRow(Table.java:384)
       at
org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:60)
       at
org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:473)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
       at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
       at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException
       at java.io.DataInputStream.readInt(DataInputStream.java:392)
       at
org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:48)
       at
org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:30)
       at
org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:108)
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:106)
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:71)
       ... 12 more

{quote}",,cburroughs,lenn0x,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,CASSANDRA-2234,,,,,,"15/Feb/11 18:13;slebresne;0001-Fix-bad-signed-conversion-from-byte-to-int.patch;https://issues.apache.org/jira/secure/attachment/12471093/0001-Fix-bad-signed-conversion-from-byte-to-int.patch","15/Feb/11 19:35;jbellis;2165-1.txt;https://issues.apache.org/jira/secure/attachment/12471101/2165-1.txt","15/Feb/11 19:38;jbellis;2165-2.txt;https://issues.apache.org/jira/secure/attachment/12471102/2165-2.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20476,,,Fri Feb 18 20:03:01 UTC 2011,,,,,,,,,,"0|i0g9pr:",93004,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"15/Feb/11 18:13;slebresne;The problem is when creating an inputStream from a ByteBuffer. We directly return the result of BB.get(), but this can be negative which breaks the method contract.

The fix is the return of the get(), the addition of available() is more of an improvement.;;;","15/Feb/11 19:35;jbellis;patch 1 provides a long-test that exposes the bug.  this requires turning off row caching in the test keyspaces to keep them from covering up the error.  for good measure, key caching is also tured off except in KeyCacheSpace / KeyCacheTest.;;;","15/Feb/11 19:38;jbellis;patch 2 fixes the bug;;;","15/Feb/11 20:42;jbellis;committed;;;","15/Feb/11 21:43;hudson;Integrated in Cassandra-0.7 #280 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/280/])
    fix column bloom filter deserialization
patch by jbellis and slebresne for CASSANDRA-2165
;;;","18/Feb/11 20:03;jbellis;For those not following the mailing list: this is a read-time error, upgrading to 0.7.2 fixes the problem with no data loss.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
debian build dep on ant-optional is missing,CASSANDRA-2164,12498643,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,15/Feb/11 14:46,16/Apr/19 09:33,14/Jul/23 05:52,21/Jun/11 17:24,0.7.7,0.8.2,,,,,0,,,,"Without the ant-optional package installed in Debian, builds fail (on lenny) with:

    Could not create type regexpmapper due to No supported regular expression matcher found: java.lang.ClassNotFoundException: org.apache.tools.ant.util.regexp.Jdk14RegexpMatcher

The attached patch makes it build. Tested on lenny and squeeze.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/11 14:47;scode;2164.txt;https://issues.apache.org/jira/secure/attachment/12471077/2164.txt",,,,,,,,,,,,,,1.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20475,,,Wed Jun 22 07:47:05 UTC 2011,,,,,,,,,,"0|i0g9pj:",93003,,thepaul,,thepaul,Low,,,,,,,,,,,,,,,,,"15/Feb/11 14:48;scode;(Ok, I hope the patch got uploaded alright. Not yet used to the new JIRA. I attached it separately as a file, as the 'submit patch' action didn't seem to offer an actual file upload facility.);;;","21/Jun/11 17:15;thepaul;+1 this from me, fwiw;;;","21/Jun/11 17:24;jbellis;committed, thanks!;;;","21/Jun/11 17:28;thepaul;+1 again, but this time With Authority!  The extra dependency is both necessary and satisfiable on all debian/ubuntu releases since 2008 (Lenny/Hardy or later), and it's doubtful that Cassandra builds, runs correctly, and is properly supportable on older releases anyway.;;;","22/Jun/11 07:47;hudson;Integrated in Cassandra-0.7 #507 (See [https://builds.apache.org/job/Cassandra-0.7/507/])
    add ant-optional debian Build-Depends
patch by Peter Schuller; reviewed by Paul Cannon for CASSANDRA-2164

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1138102
Files : 
* /cassandra/branches/cassandra-0.7/debian/control
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnavailableException after bootstrapping a previously decommissioned node,CASSANDRA-2163,12498566,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,slebresne,slebresne,14/Feb/11 19:31,16/Apr/19 09:33,14/Jul/23 05:52,15/Feb/11 08:16,0.8 beta 1,,,,,,0,,,,"In trunk (not in 0.7), if I boostrap a node, decommission it and boostrap it back (after having clean all data directory but on same ip), I get UnavailableException on read. 
I've bisected the regression to r1067508.",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20474,,,Tue Feb 15 08:15:52 UTC 2011,,,,,,,,,,"0|i0g9pb:",93002,,,,,Normal,,,,,,,,,,,,,,,,,"14/Feb/11 21:58;brandon.williams;Unable to repro with r1070628, but I did make some gossiper changes there forward-porting CASSANDRA-2115 so maybe I struck gold.;;;","15/Feb/11 08:15;slebresne;I confirm I cannot repro now either. The forward port of CASSANDRA-2115 apparently fixed whatever was wrong.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli --keyspace option doesn't work properly when used with authentication,CASSANDRA-2162,12498565,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jancona,jancona,jancona,14/Feb/11 19:30,16/Apr/19 09:33,14/Jul/23 05:52,17/Feb/11 21:22,0.7.3,,,Legacy/Tools,,,0,authentication,cli,,"The logic to select the keyspace is applied before authentication credentials are processed in cassandra-cli. This leads to a ""Keyspace FOO not found"" message at login for a keyspace that exists.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/11 19:38;jancona;0001-Swap-order-of-authentication-and-keyspace-processing.patch;https://issues.apache.org/jira/secure/attachment/12471011/0001-Swap-order-of-authentication-and-keyspace-processing.patch",,,,,,,,,,,,,,1.0,jancona,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20473,,,Thu Feb 17 21:38:45 UTC 2011,,,,,,,,,,"0|i0g9p3:",93001,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"14/Feb/11 19:36;jancona;Swap the order of processing the authentication and keyspace parameters.;;;","17/Feb/11 20:32;xedin;LGTM;;;","17/Feb/11 21:22;jbellis;committed;;;","17/Feb/11 21:38;hudson;Integrated in Cassandra-0.7 #288 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/288/])
    Swap the order of processing the authentication and keyspace CLI arguments
patch by Jim Ancona; reviewed by Pavel Yaskevich for CASSANDRA-2162
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix build of stress.java in trunk,CASSANDRA-2159,12498528,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,slebresne,slebresne,14/Feb/11 13:19,16/Apr/19 09:33,14/Jul/23 05:52,14/Feb/11 14:55,0.8 beta 1,,,,,,0,,,,Some lines in stress (java) seems to have missed a merge,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/11 13:19;slebresne;0001-Fix-stress.java-build.patch;https://issues.apache.org/jira/secure/attachment/12470994/0001-Fix-stress.java-build.patch",,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20472,,,Mon Feb 14 14:55:34 UTC 2011,,,,,,,,,,"0|i0g9of:",92998,,,,,Low,,,,,,,,,,,,,,,,,"14/Feb/11 14:55;jbellis;fixed by merging contrib/ from 0.7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
memtable_throughput_in_mb can not support sizes over 2.2 gigs because of an integer overflow.,CASSANDRA-2158,12498478,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,trisk,trisk,13/Feb/11 06:54,16/Apr/19 09:33,14/Jul/23 05:52,18/Feb/11 22:10,0.7.4,,,,,,0,,,,"If memtable_throughput_in_mb is set past 2.2 gigs, no errors are thrown.  However, as soon as data starts being written it is almost immediately being flushed.  Several hundred SSTables are created in minutes.  I am almost positive that the problem is that when memtable_throughput_in_mb is being converted into bytes the result is stored in an integer, which is overflowing.

From memtable.java:

    private final int THRESHOLD;
    private final int THRESHOLD_COUNT;

...
this.THRESHOLD = cfs.getMemtableThroughputInMB() * 1024 * 1024;
this.THRESHOLD_COUNT = (int) (cfs.getMemtableOperationsInMillions() * 1024 * 1024);


NOTE:
I also think currentThroughput also needs to be changed from an int to a long.  I'm not sure if it is as simple as this or if this also is used in other places.",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"14/Feb/11 22:12;jbellis;2158.txt;https://issues.apache.org/jira/secure/attachment/12471026/2158.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20471,,,Sat Mar 12 00:23:38 UTC 2011,,,,,,,,,,"0|i0g9o7:",92997,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"13/Feb/11 14:33;jbellis;While you are correct, you almost certainly shouldn't have throughput set that high, because if you are tuning things correctly you will hit your operations count limit first for 99.9% of workloads.;;;","13/Feb/11 17:46;trisk;I think I have a usecase where larger memtables would help a lot.  I have a combination of fat columns that can update frequently, and I have lots of memory (currently 96 gb).  I know I could also handle this by putting more boxes in the cluster, but I think I can get a lot more out of the boxes I have.  I am experimenting with breaking up my cf into multiple ones to get the same effect as the bigger sstable.  So far it seems to perform well, but feels hacky.

Even if you decide to have a hard limit on the memtable size, it should probably fail loudly instead of generating hundreds of sstables.  With my understanding of the current defaults, any default install of cassandra with more than 32 gb of memory will default to this state and will be hard for new users to understand (32/2 -> 16 gig heap | 16/8 -> 2gb default CF memtable throughput).  I would much prefer the option than the hard limit though :).;;;","14/Feb/11 22:12;jbellis;patch to make ints into longs and validate input;;;","16/Feb/11 18:16;brandon.williams;+1;;;","18/Feb/11 22:10;jbellis;committed;;;","18/Feb/11 22:51;hudson;Integrated in Cassandra-0.7 #296 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/296/])
    update memtable_throughput to be a long
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-2158
;;;","12/Mar/11 00:11;jbellis;Erik Forkalsrud commented on the mailing list,

{noformat}
It looks like the fix isn't entirely correct.  The bug is still in 0.7.3.   In Memtable.java, the line:
  THRESHOLD = cfs.getMemtableThroughputInMB() * 1024 * 1024;

should be changed to:
  THRESHOLD = cfs.getMemtableThroughputInMB() * 1024L * 1024L;

Here's some code that illustrates the difference:

   public void testMultiplication() {
       int memtableThroughputInMB = 2300;
       long thresholdA = memtableThroughputInMB * 1024 * 1024;
       long thresholdB = memtableThroughputInMB * 1024L * 1024L;
       System.out.println(""a="" + thresholdA + "" b="" + thresholdB);
   }
{noformat}

Made this change for 0.7.4;;;","12/Mar/11 00:23;hudson;Integrated in Cassandra-0.7 #376 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/376/])
    fix memtable thresholds better
patch by Erik Foralsrud; reviewed by jbellis for CASSANDRA-2158
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix counter bug (regression from svn commit r1068504),CASSANDRA-2155,12498391,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,11/Feb/11 15:45,16/Apr/19 09:33,14/Jul/23 05:52,11/Feb/11 16:43,0.8 beta 1,,,,,,0,,,,A line was mistakenly removed by the merge from 0.7 at r1068504,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"11/Feb/11 15:45;slebresne;0001-Fix-regression-from-svn-commit-1068504.patch;https://issues.apache.org/jira/secure/attachment/12470863/0001-Fix-regression-from-svn-commit-1068504.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20469,,,Fri Feb 11 17:49:48 UTC 2011,,,,,,,,,,"0|i0g9nj:",92994,,,,,Low,,,,,,,,,,,,,,,,,"11/Feb/11 16:43;jbellis;committed;;;","11/Feb/11 17:49;hudson;Integrated in Cassandra #724 (See [https://hudson.apache.org/hudson/job/Cassandra/724/])
    fix merge
patch by slebresne; reviewed by jbellis for CASSANDRA-2155
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Encryption options are not validated correctly in DatabaseDescriptor,CASSANDRA-2152,12498269,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,zznate,zznate,zznate,10/Feb/11 17:24,16/Apr/19 09:33,14/Jul/23 05:52,10/Feb/11 18:43,0.8 beta 1,,,,,,0,,,,Missing configuration for encryption_options introduced via CASSANDRA-1567 result in an obtuse NPE from MessagingService,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/11 17:36;zznate;2152.txt;https://issues.apache.org/jira/secure/attachment/12470786/2152.txt",,,,,,,,,,,,,,1.0,zznate,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20467,,,Thu Feb 10 22:43:17 UTC 2011,,,,,,,,,,"0|i0g9mv:",92991,,,,,Normal,,,,,,,,,,,,,,,,,"10/Feb/11 17:36;zznate;checks for null before reference;;;","10/Feb/11 18:43;gdusbabek;+1 committed.;;;","10/Feb/11 22:43;hudson;Integrated in Cassandra #721 (See [https://hudson.apache.org/hudson/job/Cassandra/721/])
    check for null encryption in MessagingService. patch by Nate McCall, reviewed by gdusbabek. CASSANDRA-2152
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstablekeys silently ignores extra arguments,CASSANDRA-2150,12498191,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,10/Feb/11 04:47,16/Apr/19 09:33,14/Jul/23 05:52,10/Feb/11 05:35,0.7.1,,,Legacy/Tools,,,0,,,,"sstablekeys only passes arg $1 to SSTableExporter instead of passing all arguments, like sstable2json.  Only one SSTable is allowed as an argument, but this is normally detected in SSTableExporter.java.  By only passing the one argument, we end up silently ignoring the remaining arguments.",,,,,,,,,,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,"10/Feb/11 05:03;thobbs;2150.txt;https://issues.apache.org/jira/secure/attachment/12470753/2150.txt",,,,,,,,,,,,,,1.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20466,,,Thu Feb 10 06:47:11 UTC 2011,,,,,,,,,,"0|i0g9mf:",92989,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"10/Feb/11 05:03;thobbs;Attached patch passes all args.;;;","10/Feb/11 05:36;jbellis;committed;;;","10/Feb/11 06:47;hudson;Integrated in Cassandra-0.7 #274 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/274/])
    forward all arguments to sstablekeys
patch by thobbs; reviewed by jbellis for CASSANDRA-2150
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system CFs default to large memtable throughputs on large heaps,CASSANDRA-2148,12498177,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mdennis,mdennis,mdennis,09/Feb/11 21:09,16/Apr/19 09:33,14/Jul/23 05:52,09/Feb/11 23:45,0.7.1,,,,,,0,,,,"The default memtableThroughputInMB is calculated now based on the heap size.  Most people running with a large heap in production explicitly set it for their memtable(s).  However, the the CFs in the system keyspace still default to the calculated value which on a large heap can be quite large.  HintsColumnFamily is really the only problematic one though as the others are flushed afters changes to them.

we should:

1) set the throughput on the hints CF to a reasonable max and min value - min(256, max(32, normalDefault/2))
2) set the throughput on the other system CFs to some small constant value (just as a safety); 8M sounds good",,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"09/Feb/11 23:32;mdennis;2148-cassandra-0.7.txt;https://issues.apache.org/jira/secure/attachment/12470740/2148-cassandra-0.7.txt",,,,,,,,,,,,,,1.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20465,,,Thu Feb 10 00:44:39 UTC 2011,,,,,,,,,,"0|i0g9lz:",92987,,,,,Normal,,,,,,,,,,,,,,,,,"09/Feb/11 21:14;mdennis;attached patch bounds Hints CF throughput between 32M and 256M and fixes the throughput of other system CFs to 8M;;;","09/Feb/11 23:32;mdennis;rebased;;;","09/Feb/11 23:45;brandon.williams;Committed.;;;","10/Feb/11 00:44;hudson;Integrated in Cassandra-0.7 #272 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/272/])
    bound hints CF memtable throughput between 32M and 256M
Patch by Matthew Dennis, reviewed by brandonwilliams for CASSANDRA-2148
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java doesn't read more unique rows than 2x the number of threads,CASSANDRA-2147,12498161,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,09/Feb/11 19:02,16/Apr/19 09:33,14/Jul/23 05:52,18/Feb/11 22:48,0.7.3,,,,,,0,,,,"This can be observed by watching how much the row/key cache grows on each run.  I'm not sure when this started or if it was always the case, but it's actually useful behavior when you want to benchmark just the cache, so it'd be nice to preserve as an option.",,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"18/Feb/11 19:58;xedin;CASSANDRA-2147.patch;https://issues.apache.org/jira/secure/attachment/12471437/CASSANDRA-2147.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20464,,,Sat Feb 19 01:42:19 UTC 2011,,,,,,,,,,"0|i0g9lr:",92986,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"10/Feb/11 02:55;jbellis;can't we use stdev instead of adding another option?;;;","18/Feb/11 19:58;xedin;there was a small mistake in the guassian distribution algorithm which is fixed now (also -r flag wasn't parsed correctly, which is fixed too).;;;","18/Feb/11 22:48;brandon.williams;Committed.;;;","19/Feb/11 01:42;hudson;Integrated in Cassandra-0.7 #297 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/297/])
    Fix key distribution in stress.java.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-2147
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cli read_repair_chance input not validated,CASSANDRA-2146,12498154,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,cburroughs,cburroughs,09/Feb/11 18:28,16/Apr/19 09:33,14/Jul/23 05:52,10/Feb/11 18:51,0.7.2,,,,,,0,,,,"{noformat}
put(ColumnFamilyArgument.READ_REPAIR_CHANCE, ""Probability (0.0-1.0) with which to perform read repairs on CL.ONE reads"");
{noformat}

The input range is not enforced so 
{noformat}
create column family ... with ... read_repair_chance = 25;
{noformat}

Will result in
{noformat}
Keyspace: ks1:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
    Replication Factor: 3
  Column Families:
    ColumnFamily: cf1
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
...
      Read repair chance: 25.0
{noformat}


I am unsure if in practice this means RR chance 100%, or something surprising.  (I ran into this because read_repair_chance requires a leading 0 and ommiting it results in an unhelpful ""Command not found:"" message).
",,,,,,,,,,,,,,,,,,,,,,";10/Feb/11 16:54;xedin;1800",1800,0,1800,100%,1800,0,1800,,,,,,,,,,,,,,,"10/Feb/11 16:53;xedin;CASSANDRA-2146.patch;https://issues.apache.org/jira/secure/attachment/12470782/CASSANDRA-2146.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20463,,,Thu Feb 10 22:11:36 UTC 2011,,,,,,,,,,"0|i0g9lj:",92985,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"10/Feb/11 18:51;jbellis;committed;;;","10/Feb/11 22:11;hudson;Integrated in Cassandra-0.7 #276 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/276/])
    validate read_repair_chance
patch by xedin; reviewed by jbellis for CASSANDRA-2146
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
versioning isn't going to work when forwarding messages,CASSANDRA-2140,12498014,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gdusbabek,gdusbabek,gdusbabek,08/Feb/11 18:06,16/Apr/19 09:33,14/Jul/23 05:52,09/Feb/11 19:11,0.8 beta 1,,,,,,0,,,,SP.sendToHintedEndpoints needs to take care to create properly versioned messages that get forwarded to other nodes.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/11 18:41;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0001-cache-versioned-messages-in-StorageProxy.txt;https://issues.apache.org/jira/secure/attachment/12470699/ASF.LICENSE.NOT.GRANTED--v2-0001-cache-versioned-messages-in-StorageProxy.txt","09/Feb/11 18:41;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0002-fix-misspeeling.txt;https://issues.apache.org/jira/secure/attachment/12470700/ASF.LICENSE.NOT.GRANTED--v2-0002-fix-misspeeling.txt","09/Feb/11 18:41;gdusbabek;ASF.LICENSE.NOT.GRANTED--v2-0003-MessageProducer-int-to-Integer.txt;https://issues.apache.org/jira/secure/attachment/12470701/ASF.LICENSE.NOT.GRANTED--v2-0003-MessageProducer-int-to-Integer.txt",,,,,,,,,,,,3.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20462,,,Wed Feb 09 20:30:16 UTC 2011,,,,,,,,,,"0|i0g9k7:",92979,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"09/Feb/11 17:04;jbellis;+1 to the approach.  a couple possible improvements:

- I think all the calls to sendRR(prod.getMessage(Gossiper..), endpoint, handler) can be replaced with sendRR(prod, endpoint, handler)?
- Let's make CMP less wasteful by giving initialcapacity of 2 (common case: everyone is on same versio -- it gets multipled by load factor, so 2 gets rounded down to 1) and making getMessage take an Integer (so we auto[un]box zero times instead of 4)
;;;","09/Feb/11 18:41;gdusbabek;v2 makes suggested improvements.;;;","09/Feb/11 19:03;jbellis;+1;;;","09/Feb/11 19:11;gdusbabek;committed.;;;","09/Feb/11 20:30;hudson;Integrated in Cassandra #719 (See [https://hudson.apache.org/hudson/job/Cassandra/719/])
    MessageProducer int to Integer. patch by gdusbabek, reviewed by jbellis. CASSANDRA-2140
fix misspeeling. patch by gdusbabek, reviewed by jbellis. CASSANDRA-2140
cache versioned messages in StorageProxy. patch by gdusbabek, reviewed by jbellis. CASSANDRA-2140
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI does not use sub-comparator on Super CF `get`.,CASSANDRA-2136,12497984,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,xedin,xedin,08/Feb/11 14:55,16/Apr/19 09:33,14/Jul/23 05:52,10/Feb/11 18:48,0.7.2,,,Legacy/Tools,,,0,,,,"[default@foo] get foo[page-field];
=> (super_column=20110208,
    (column=82f4c650-2d53-11e0-a08b-58b035f3f60d, value=msg1,
timestamp=1297159430471000)
    (column=82f4c650-2d53-11e0-a08b-58b035f3f60e, value=msg2,
timestamp=1297159437423000)
    (column=82f4c650-2d53-11e0-a08b-58b035f3f60f, value=msg3,
timestamp=1297159439855000))
Returned 1 results.

[default@foo] get foo[page-field][20110208];
, value=msg1, timestamp=1297159430471000)
=> (column=???P-S???X?5??, value=msg2, timestamp=1297159437423000)
=> (column=???P-S???X?5??, value=msg3, timestamp=1297159439855000)
Returned 3 results.

[default@foo] get
foo[page-field][20110208][82f4c650-2d53-11e0-a08b-58b035f3f60e];
=> (column=???P-S???X?5??, value=msg2, timestamp=1297159437423000)","Column family settings:
       - name: foo
         column_type: Super
         compare_with: AsciiType
         compare_subcolumns_with: TimeUUIDType
         default_validation_class: AsciiType",,,,,,,,,,,,,,,,,,,,,";10/Feb/11 17:42;xedin;3600",3600,0,3600,100%,3600,0,3600,,,,,,,,,,,,,,,"10/Feb/11 17:42;xedin;CASSANDRA-2136.patch;https://issues.apache.org/jira/secure/attachment/12470787/CASSANDRA-2136.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20461,,,Thu Feb 10 22:11:36 UTC 2011,,,,,,,,,,"0|i0g9jb:",92975,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"10/Feb/11 18:48;jbellis;committed;;;","10/Feb/11 22:11;hudson;Integrated in Cassandra-0.7 #276 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/276/])
    format subcolumn names with subcomparator
patch by xedin; reviewed by jbellis for CASSANDRA-2136
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error in ThreadPoolExecutor,CASSANDRA-2134,12497954,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,patrik.modesto,patrik.modesto,08/Feb/11 06:57,16/Apr/19 09:33,14/Jul/23 05:52,09/Feb/11 14:23,0.7.1,,,,,,0,,,,"On my two-node test setup I get repeatedly following error:

The 10.0.18.129 server log:
{noformat} 
 INFO 14:10:37,707 Node /10.0.18.99 has restarted, now UP again
 INFO 14:10:37,708 Checking remote schema before delivering hints
 INFO 14:10:37,708 Sleeping 45506ms to stagger hint delivery
 INFO 14:10:37,709 Node /10.0.18.99 state jump to normal
 INFO 14:11:23,215 Started hinted handoff for endpoint /10.0.18.99
ERROR 14:11:23,884 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
ERROR 14:11:23,885 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
{noformat} 

The 10.0.18.99 server log:
{noformat} 
 INFO 14:10:37,691 Binding thrift service to /0.0.0.0:9160
 INFO 14:10:37,693 Using TFastFramedTransport with a max frame size of
15728640 bytes.
 INFO 14:10:37,695 Listening for thrift clients...
 INFO 14:10:38,337 GC for ParNew: 954 ms, 658827608 reclaimed leaving
966732432 used; max is 4265607168
 INFO 14:11:27,142 Started hinted handoff for endpoint /10.0.18.129
ERROR 14:11:27,370 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
ERROR 14:11:27,371 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
{noformat}

It happen durring batch_mutate test or after restart, when there are commitlogs to replay. Using current 0.7.1 from cassandra-0.7 branch.",Linux 2.6.32-bpo.4edois1-openvz-amd64 #1 SMP x86_64 GNU/Linux,cburroughs,patrik.modesto,,,,,,,,,,,,,,,,,,,,60,60,,0%,60,60,,,,,,,,,,,,,,,,"08/Feb/11 14:57;slebresne;0001-Fix-BBUtil.string-offset-related-to-position-instead.patch;https://issues.apache.org/jira/secure/attachment/12470594/0001-Fix-BBUtil.string-offset-related-to-position-instead.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20460,,,Wed Feb 09 14:23:18 UTC 2011,,,,,,,,,,"0|i0g9in:",92972,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"08/Feb/11 14:57;slebresne;Attached patch should fix this.;;;","08/Feb/11 15:29;hudson;Integrated in Cassandra-0.7 #258 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/258/])
    fix ByteBufferUtil.string position
patch by slebresne; reviewed by jbellis for CASSANDRA-2134
;;;","08/Feb/11 15:54;patrik.modesto;I can still get exception in ThreadPoolExecutor.

1st server:

{noformat}
ERROR 16:50:34,349 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:318)
        at org.apache.cassandra.db.Table.<init>(Table.java:258)
        at org.apache.cassandra.db.Table.open(Table.java:107)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR 16:50:34,351 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:318)
        at org.apache.cassandra.db.Table.<init>(Table.java:258)
        at org.apache.cassandra.db.Table.open(Table.java:107)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
{noformat}

2nd server:

{noformat}
ERROR 16:50:37,532 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:318)
        at org.apache.cassandra.db.Table.<init>(Table.java:258)
        at org.apache.cassandra.db.Table.open(Table.java:107)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR 16:50:37,547 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:318)
        at org.apache.cassandra.db.Table.<init>(Table.java:258)
        at org.apache.cassandra.db.Table.open(Table.java:107)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
{noformat};;;","08/Feb/11 16:52;jbellis;If you haven't removed the corrupt HintsColumnFamily files, you need to do so.;;;","09/Feb/11 12:12;patrik.modesto;Deleting the old HintColumnFamily files helped. Thanks.;;;","09/Feb/11 14:23;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Illegal file mode when saving caches,CASSANDRA-2131,12497914,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,amorton,amorton,amorton,07/Feb/11 22:01,16/Apr/19 09:33,14/Jul/23 05:52,07/Feb/11 23:29,0.7.1,,,,,,0,,,,"The following error is logged when trying to save caches


DEBUG [CompactionExecutor:1] 2011-02-08 07:30:03,647 CacheWriter.java (line 45) Saving /var/lib/cassandra/saved_caches/Keyspace1-ascii-KeyCache
ERROR [CompactionExecutor:1] 2011-02-08 07:30:03,725 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: Illegal mode ""w"" must be one of ""r"", ""rw"", ""rws"", or ""rwd""
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.IllegalArgumentException: Illegal mode ""w"" must be one of ""r"", ""rw"", ""rws"", or ""rwd""
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:197)
	at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:116)
	at org.apache.cassandra.io.sstable.CacheWriter.saveCache(CacheWriter.java:48)
	at org.apache.cassandra.db.CompactionManager$9.runMayThrow(CompactionManager.java:746)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 6 more
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20458,,,Mon Feb 07 23:29:36 UTC 2011,,,,,,,,,,"0|i0g9hz:",92969,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"07/Feb/11 23:29;jbellis;fixed in r1068220;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
removetoken after removetoken rf error fails to work,CASSANDRA-2129,12497902,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,mbulman,mbulman,07/Feb/11 20:56,16/Apr/19 09:33,14/Jul/23 05:52,14/Jul/11 21:26,0.8.2,,,,,,0,,,,"2 node cluster, a keyspace existed with rf=2.  Tried removetoken and got:

mbulman@ripcord-maverick1:/usr/src/cassandra/tags/cassandra-0.7.0$ bin/nodetool -h localhost removetoken 159559397954378837828954138596956659794
Exception in thread ""main"" java.lang.IllegalStateException: replication factor (2) exceeds number of endpoints (1)

Deleted the keyspace, and tried again:

mbulman@ripcord-maverick1:/usr/src/cassandra/tags/cassandra-0.7.0$ bin/nodetool -h localhost removetoken 159559397954378837828954138596956659794
Exception in thread ""main"" java.lang.UnsupportedOperationException: This node is already processing a removal. Wait for it to complete.",,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,"27/May/11 20:28;brandon.williams;2129-v2.txt;https://issues.apache.org/jira/secure/attachment/12480689/2129-v2.txt","16/Mar/11 19:33;brandon.williams;2129.txt;https://issues.apache.org/jira/secure/attachment/12473835/2129.txt",,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20457,,,Thu Jul 14 22:14:00 UTC 2011,,,,,,,,,,"0|i0g9hj:",92967,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"07/Feb/11 21:55;jbellis;Does removetoken force clear up the second problem?

We should allow reducing the nodes in the cluster below the RF count, just as we allow creating a keyspace with RF greater than the node count.  (In both cases, writes will be rejected until more nodes are added or RF is reduced.);;;","08/Feb/11 16:26;mbulman;No.  force and status both report no token removals in process.

tags/cassandra-0.7.0# bin/nodetool -h localhost removetoken status                                 
RemovalStatus: No token removals in process.

Restarting the node that was processing the removal clears up the confusion/issue.;;;","16/Mar/11 19:33;brandon.williams;Patch to allow removing the token, and throw UE instead of an internal error when trying to insert and the number of endpoints is less than the RF.;;;","17/Mar/11 02:14;jbellis;SP.mutate should be throwing UAE already (assureSufficientLiveNodes).  Why isn't that working?;;;","17/Mar/11 20:13;brandon.williams;Because it calls rs.getNaturalEndpoints first, which throws ISE and SP.mutate only catches IOException.  I'm a little uneasy with catching both IOE and ISE, or modifying the RS to throw IOE, but I'm not sure what the best solution is.  The patch as-is still has ISE problems, at least on describe_keyspace.;;;","18/Mar/11 02:39;jbellis;what purpose does leaving the ISE in serve, at this point?  should we just remove it?;;;","06/Apr/11 16:59;nickmbailey;Note: a similar error occurs when trying to do describe_ring on a cluster where rf < N.;;;","29/Apr/11 19:14;jbellis;Possibly the same bug was reported on the user list: http://permalink.gmane.org/gmane.comp.db.cassandra.user/15803;;;","27/May/11 20:28;brandon.williams;Removing ISE almost got us all the way there, but there was a subtle bug in WRH.determineBlockFor being relative to the amount of endpoints, instead of the RF.  v2 removes ISE and addresses this problem.;;;","14/Jul/11 21:20;xedin;+1;;;","14/Jul/11 21:26;brandon.williams;Committed;;;","14/Jul/11 22:14;hudson;Integrated in Cassandra-0.8 #215 (See [https://builds.apache.org/job/Cassandra-0.8/215/])
    Allow RF to exceed the number of nodes (but disallow writes)
Patch by brandonwilliams, reviewed by Pavel Yaskevich for CASSANDRA-2129

brandonwilliams : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1146900
Files : 
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/OldNetworkTopologyStrategy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/SimpleStrategy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/NetworkTopologyStrategy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/locator/AbstractReplicationStrategy.java
* /cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/WriteResponseHandler.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corrupted Commit logs,CASSANDRA-2128,12497895,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,pquerna,pquerna,07/Feb/11 19:41,16/Apr/19 09:33,14/Jul/23 05:52,09/Feb/11 17:07,0.6.12,0.7.2,,,,,0,,,,"Two of our nodes had a hard failure.

They both came up with a corrupted commit log.

On startup we get this:
{quote}
011-02-07_19:34:03.95124 INFO - Finished reading /var/lib/cassandra/commitlog/CommitLog-1297099954252.log
2011-02-07_19:34:03.95400 ERROR - Exception encountered during startup.
2011-02-07_19:34:03.95403 java.io.EOFException
2011-02-07_19:34:03.95403 	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
2011-02-07_19:34:03.95404 	at java.io.DataInputStream.readUTF(DataInputStream.java:572)
2011-02-07_19:34:03.95405 	at java.io.DataInputStream.readUTF(DataInputStream.java:547)
2011-02-07_19:34:03.95406 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:363)
2011-02-07_19:34:03.95407 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:318)
2011-02-07_19:34:03.95408 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:240)
2011-02-07_19:34:03.95409 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
2011-02-07_19:34:03.95409 	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:115)
2011-02-07_19:34:03.95410 	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
2011-02-07_19:34:03.95422 Exception encountered during startup.
2011-02-07_19:34:03.95436 java.io.EOFException
2011-02-07_19:34:03.95447 	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
2011-02-07_19:34:03.95458 	at java.io.DataInputStream.readUTF(DataInputStream.java:572)
2011-02-07_19:34:03.95468 	at java.io.DataInputStream.readUTF(DataInputStream.java:547)
2011-02-07_19:34:03.95478 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:363)
2011-02-07_19:34:03.95489 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:318)
2011-02-07_19:34:03.95499 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:240)
2011-02-07_19:34:03.95510 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
2011-02-07_19:34:03.95521 	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:115)
2011-02-07_19:34:03.95531 	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
{quote}

On node A, the commit log in question is 100mb.

On node B, the commit log in question is 60mb.

An ideal resolution would be if EOF is hit early, log something, but don't stop the startup.  Instead process everything that we have done so far, and keep going.
","cassandra-0.6 @ r1064246 (0.6.11)
Ubuntu 9.10
Rackspace Cloud
",cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/11 16:36;jbellis;2128.txt;https://issues.apache.org/jira/secure/attachment/12470686/2128.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20456,,,Wed Feb 09 18:54:54 UTC 2011,,,,,,,,,,"0|i0g9hb:",92966,,gdusbabek,,gdusbabek,Normal,,,,,,,,,,,,,,,,,"07/Feb/11 19:43;gdusbabek;0.7 handles this gracefully by using checksums when the RowMutations are read from the CL. 0.6 could handle this better with a try{}catch{} where the RowMutation is deserialized.;;;","07/Feb/11 19:49;gdusbabek;Actually, this is kind of bizarre. 0.6 does use a checksum.  The failure is happening after the RM has been read (in full) while we are attempting to deserialize it.;;;","07/Feb/11 19:58;jbellis;0.6 uses checksums too, for this area.  (The one place it doesn't is the Header, which I posted a patch for in CASSANDRA-1815, but that doesn't seem to actually matter in practice.)

Here's the code in question:

{code}
                    long claimedCRC32;
                    byte[] bytes;
                    try
                    {
                        bytes = new byte[(int) reader.readLong()]; // readlong can throw EOFException too
                        reader.readFully(bytes);
                        claimedCRC32 = reader.readLong();
                    }
                    catch (EOFException e)
                    {
                        // last CL entry didn't get completely written.  that's ok.
                        break;
                    }

                    ByteArrayInputStream bufIn = new ByteArrayInputStream(bytes);
                    Checksum checksum = new CRC32();
                    checksum.update(bytes, 0, bytes.length);
                    if (claimedCRC32 != checksum.getValue())
                    {
                        // this part of the log must not have been fsynced.  probably the rest is bad too,
                        // but just in case there is no harm in trying them.
                        continue;
                    }

                    /* deserialize the commit log entry */
                    final RowMutation rm = RowMutation.serializer().deserialize(new DataInputStream(bufIn));
{code}

In other words, first we read the mutation into a buffer and checksum it.  If it passes, we try to deserialize that into a mutation.

It's expected that read-into-buffer can throw an EOF (which we expect and catch) but once it passes checksum it's not expected that mutation deserialize should fail.

(Yes, checksums aren't perfect, especially not 32bit ones, but for the checksum to generate a false positive on the last entry in the commitlog, on two different machines, is not a coincidence I'm buying.)

At first glance, the most likely explanation is a bug in RowMutation serializer.  But it's also possible that I'm wrong and it really is erroring out due to some unflushed data somehow.  If you turn on debug logging, it will include the offset in the commitlog of the mutation being replayed -- if it's very very close to the end of the file, then that would increase that likelihood.

bq. An ideal resolution would be if EOF is hit early, log something, but don't stop the startup. Instead process everything that we have done so far, and keep going.

Disagreed.  This is a serious enough bug that we should require human intervention before ignoring it and starting up anyway.;;;","09/Feb/11 15:07;jbellis;CASSANDRA-2055 reports the exact same stacktrace: RM.deserialize EOFing on the very first field it tries to read.  Suspicious!;;;","09/Feb/11 16:20;jbellis;Paul sent me one of the CommitLog files exhibiting this problem.  It is 99614720 bytes long.

I added one more log entry when replaying it:

{code}
                    logger.debug(""checksum of "" + bytes.length + "" successful: "" + claimedCRC32);
                    /* deserialize the commit log entry */
{code}

The last entries in the log before dying are

{noformat}
DEBUG [main] 2011-02-09 09:39:15,577 CommitLog.java (line 213) Reading mutation at 97750254
DEBUG [main] 2011-02-09 09:39:15,577 CommitLog.java (line 240) checksum of 1209 successful: 2281213824
DEBUG [main] 2011-02-09 09:39:15,578 CommitLog.java (line 244) replaying mutation for MonitorApp...
DEBUG [main] 2011-02-09 09:39:15,578 CommitLog.java (line 213) Reading mutation at 97751479
DEBUG [main] 2011-02-09 09:39:15,580 CommitLog.java (line 240) checksum of 0 successful: 0
 INFO [main] 2011-02-09 09:39:15,604 CommitLog.java (line 253) Finished reading /var/lib/cassandra/commitlog/CommitLog-1297099954252.log
ERROR [main] 2011-02-09 09:39:15,652 CassandraDaemon.java (line 242) Exception encountered during startup.
java.io.EOFException
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
{noformat}

This is a good 2MB before the end of the file.  So I looked in a hex editor to see what was there, starting at offset 97751479.  There's about 8KB of pure 00 bytes, followed by nonsense that occasionally looks like a RowMutation (some appearances of column names cnt, avg, dev that are present in the early, intact part of the commitlog) but mostly does not: there are no more appearances of the keyspace name, MonitorApp, which is the first part of any real RowMutation in the 0.6 serialization format, and in one place there is even the string ""java.util.BitSet"" visible, which should only appear in the commitlog segment header at the start of the file (that we rewrite at each flush).

To me it looks like ""EC2 can write a bunch of nonsense in your commitlog during a hard failure,"" and the fix is to make the checksum process more robust against nonsense that happens to conform to the first couple bytes we read for a RowMutation.;;;","09/Feb/11 16:36;jbellis;Patch adds a check to the length of the row mutation read to make sure it is sane.  (Sane lengths will make sure the CRC can do its job.);;;","09/Feb/11 16:39;jbellis;0.7 already has a weaker fix for this (checking that serializedSize > 0, which isn't quite as strong as >= 10, but probably adequate in practice, and adding a separate checksum for the size itself.);;;","09/Feb/11 17:00;gdusbabek;+1, except don't modify the log4j config!;;;","09/Feb/11 17:07;jbellis;committed, without the log4j changes;;;","09/Feb/11 18:54;hudson;Integrated in Cassandra-0.6 #58 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/58/])
    update commitlog replay to catch bogus RowMutation lengths
patch by jbellis; reviewed by gdusbabek for CASSANDRA-2128
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool cfhistograms write/read latency columns are reversed,CASSANDRA-2123,12497861,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,cburroughs,cburroughs,07/Feb/11 15:53,16/Apr/19 09:33,14/Jul/23 05:52,09/Feb/11 16:48,0.7.1,,,Tool/nodetool,,,0,,,,"As first reported by Oleg Proudnikov in the thread http://www.mail-archive.com/user@cassandra.apache.org/msg09607.html the columns for read and write latency are reversed in the output of cfhistograms.  The Mbean values are correct.

Example output during stress.java insert test.
{noformat}
Keyspace1/Standard1 histograms
Offset      SSTables     Write Latency      Read Latency          Row Size      Column Count
1                  0                 0                 0                 0                 0
2                  0                 0                 1                 0                 0
3                  0                 0               998                 0                 0
4                  0                 0              7729                 0                 0
5                  0                 0             22844                 0                 0
6                  0                 0             44439                 0           6524792
7                  0                 0             64576                 0                 0
8                  0                 0             79000                 0                 0
10                 0                 0            139338                 0                 0
12                 0                 0             84675                 0                 0
14                 0                 0             36928                 0                 0
17                 0                 0             16547                 0                 0
20                 0                 0              3926                 0                 0
24                 0                 0              1681                 0                 0
29                 0                 0               776                 0                 0
35                 0                 0               357                 0                 0
42                 0                 0               172                 0                 0
50                 0                 0                51                 0                 0
60                 0                 0                15                 0                 0
72                 0                 0                10                 0                 0
86                 0                 0                 4                 0                 0
103                0                 0                 6                 0                 0
124                0                 0                 3                 0                 0
149                0                 0                 1                 0                 0
179                0                 0                 0                 0                 0
215                0                 0                 1                 0                 0
258                0                 0                 1                 0                 0
310                0                 0                 0                 0                 0
372                0                 0                 1           6524792                 0
446                0                 0                 2                 0                 0
535                0                 0                 0                 0                 0
642                0                 0                 0                 0                 0
770                0                 0                 0                 0                 0
924                0                 0                 0                 0                 0
1109               0                 0                 1                 0                 0
1331               0                 0                 0                 0                 0
{noformat}",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20454,,,Wed Feb 09 18:01:03 UTC 2011,,,,,,,,,,"0|i0g9g7:",92961,,,,,Low,,,,,,,,,,,,,,,,,"09/Feb/11 16:48;jbellis;fixed in r1068967;;;","09/Feb/11 18:01;hudson;Integrated in Cassandra-0.7 #267 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/267/])
    put read/write latencies in the right columns for nodetool cfhistograms
patch by jbellis for CASSANDRA-2123
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java cardinality option parsing typo,CASSANDRA-2121,12497851,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cburroughs,cburroughs,cburroughs,07/Feb/11 14:49,16/Apr/19 09:33,14/Jul/23 05:52,09/Feb/11 14:32,0.7.1,,,,,,0,,,,"Session.java
{noformat} 
            if (cmd.hasOption(""C""))
                cardinality = Integer.parseInt(cmd.getOptionValue(""t""));
{noformat} 

",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,cburroughs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20453,,,Wed Feb 09 14:57:58 UTC 2011,,,,,,,,,,"0|i0g9fr:",92959,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"09/Feb/11 14:32;jbellis;committed, thanks!;;;","09/Feb/11 14:57;hudson;Integrated in Cassandra-0.7 #265 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/265/])
    fix parsing of cardinality option
patch by Chris Burroughs; reviewed by jbellis for CASSANDRA-2121
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli 'use Keyspace user pass' breaks with SimpleAuth,CASSANDRA-2111,12497714,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,thobbs,thobbs,04/Feb/11 20:41,16/Apr/19 09:33,14/Jul/23 05:52,10/Feb/11 02:43,0.7.1,,,Legacy/Tools,,,0,,,,"If SimpleAuth is used and the -Daccess.properties... JVM options are passed in, the CLI's ""use Keyspace user 'password'"" command breaks.  However, if the --username and --password options are used, you can still authenticate.",,,,,,,,,,,,,,,,,,,,,,";05/Feb/11 16:03;xedin;7200",7200,0,7200,100%,7200,0,7200,,,,,,,,,,,,,,,"10/Feb/11 00:34;xedin;CASSANDRA-2111-v2.patch;https://issues.apache.org/jira/secure/attachment/12470744/CASSANDRA-2111-v2.patch","05/Feb/11 16:03;xedin;CASSANDRA-2111.patch;https://issues.apache.org/jira/secure/attachment/12470370/CASSANDRA-2111.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20448,,,Thu Feb 10 04:34:53 UTC 2011,,,,,,,,,,"0|i0g9dr:",92950,,thobbs,,thobbs,Low,,,,,,,,,,,,,,,,,"04/Feb/11 22:32;xedin;Looks like that right now without a valid user/password given by --username/--password you won't be able to connect the cassandra instance, that means that if you will try to use `use Keyspace <username> '<password>';` it will tell you that `Keyspace is not found` which is a wrong error message in this case. 

I will make following changes: 

a). add support for setting username/password at `connect host/port;` command; 
b). change error to show that you are not connected to node instead of '<keyspace> not found' for `use <keyspace>` (other commands show the right error);;;;","05/Feb/11 16:03;xedin;`connect` command enhancement:
`connect host/port <username> '<password>';`

and changes from previous comment.

;;;","10/Feb/11 00:22;thobbs;The normal 'help' output does not show that you can specify a user/pass combination with connect.  The 'help connect' output does, though.

Other than that, it looks good.;;;","10/Feb/11 00:34;xedin;v2 fixes missing note about user/pass for connect command when just `help` is used without `connect` attribute. This patch could be applied to both trunk and cassandra-0.7 branches.;;;","10/Feb/11 00:41;thobbs;+1;;;","10/Feb/11 02:43;jbellis;committed;;;","10/Feb/11 04:34;hudson;Integrated in Cassandra-0.7 #273 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/273/])
    add cli support for setting username/password at 'connect' command
patch by Pavel Yaskevich; reviewed by thobbs for CASSANDRA-2111
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the read race condition in CFStore for counters ,CASSANDRA-2105,12497550,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,03/Feb/11 10:51,16/Apr/19 09:33,14/Jul/23 05:52,28/Mar/11 14:45,0.8 beta 1,,,,,,1,counters,,,"There is a (known) race condition during counter read. Indeed, for standard
column family there is a small time during which a memtable is both active and
pending flush and similarly a small time during which a 'memtable' is both
pending flush and an active sstable. For counters that would imply sometime
reconciling twice during a read the same counterColumn and thus over-counting.

Current code changes this slightly by trading the possibility to count twice a
given counterColumn by the possibility to miss a counterColumn. Thus it trades
over-counts for under-counts.

But this is no fix and there is no hope to offer clients any kind of guarantee
on reads unless we fix this.
",,cburroughs,stuhood,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,"04/Feb/11 16:56;slebresne;2115_option1_withLock.patch;https://issues.apache.org/jira/secure/attachment/12470250/2115_option1_withLock.patch","04/Feb/11 16:56;slebresne;2115_option2_nolock.patch;https://issues.apache.org/jira/secure/attachment/12470251/2115_option2_nolock.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20446,,,Mon Mar 28 14:45:59 UTC 2011,,,,,,,,,,"0|i0g9cf:",92944,,,,,Normal,,,,,,,,,,,,,,,,,"03/Feb/11 10:51;slebresne;I plan to reuse the fix I had made for #1546 that uses a ReadWriteLock to fix
this (unless I find a better idea in the meantime). Unless proven otherwise I
don't think this will have a huge impact on counter read performance, but if
someone finds a better idea, I'm listening.;;;","04/Feb/11 16:56;slebresne;Attached not 1 but 2 options for this patch. I'm not sure with which version to go so I'm asking for opinions.

Version 1 is the one extracted from #1546. It uses a ReadWriteLock to protect from the race condition.

Version 2 don't use a lock. So less chances of lock contention which is always good. Only problem is, it still suffers in theory of a race condition. But I think this race condition is borderline impossible.
Basically, given a memtable m being flushed, let's call s(m) the sstable initially produced by its flushing and let's denote by s'(m) any sstable resulting of the compaction of s(m). The race is if a read thread sees m when grabbing the references to the memtable being flushed and sees s'(m) (not s(m), that is the initial race condition and this is not impossible at all) when grabing the reference to the sstables.
If it's unclear, the code has a comment explaining this that may be more clear.

So not sure which version to go with. I may slightly lean towards Version 1 because I usually side with correction before anything else, but since this is in a critical path it feels slightly wasteful to use a lock for this given how remote the race condition of version 2 seems.
;;;","08/Mar/11 14:52;slebresne;I've opened CASSANDRA-2284 that provides what I think is a better solution than the one I have attached previously to this problem (I've opened it separately because it's a more generic solution, not just a counter related fix).

;;;","28/Mar/11 14:36;hudson;Integrated in Cassandra #810 (See [https://hudson.apache.org/hudson/job/Cassandra/810/])
    Atomically switch cfstore memtables and sstables
patch by slebresne; reviewed by jbellis for CASSANDRA-2284 (and CASSANDRA-2105)
;;;","28/Mar/11 14:45;slebresne;Fixed by CASSANDRA-2284;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBoundsException during lazy row compaction of supercolumns,CASSANDRA-2104,12497548,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,dln,dln,03/Feb/11 10:08,16/Apr/19 09:33,14/Jul/23 05:52,24/Feb/11 19:09,0.7.3,,,,,,0,,,,"I ran into an exception when lazily compacting wide rows of TimeUUID columns.
It seems to trigger when a row is larger than {{in_memory_compaction_limit_in_mb}}.

Traceback:
{noformat}
 INFO [CompactionExecutor:1] 2011-02-03 10:59:59,262 CompactionIterator.java (line 135) Compacting large row XXXXXXXXXXXXX (76999384 bytes) incrementally
 ERROR [CompactionExecutor:1] 2011-02-03 10:59:59,266 AbstractCassandraDaemon.java (line 114) Fatal exception in thread T
 hread[CompactionExecutor:1,1,main]
 java.lang.IndexOutOfBoundsException
         at java.nio.Buffer.checkIndex(Buffer.java:514)
         at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:121)
         at org.apache.cassandra.db.marshal.TimeUUIDType.compareTimestampBytes(TimeUUIDType.java:56)
         at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:45)
         at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:29)
         at java.util.concurrent.ConcurrentSkipListMap$ComparableUsingComparator.compareTo(ConcurrentSkipListMap.java:606
 )
         at java.util.concurrent.ConcurrentSkipListMap.findPredecessor(ConcurrentSkipListMap.java:685)
         at java.util.concurrent.ConcurrentSkipListMap.doPut(ConcurrentSkipListMap.java:864)
         at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(ConcurrentSkipListMap.java:1893)
         at org.apache.cassandra.db.SuperColumn.addColumn(SuperColumn.java:170)
         at org.apache.cassandra.db.SuperColumn.putColumn(SuperColumn.java:195)
         at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:221)
         at org.apache.cassandra.io.LazilyCompactedRow$LazyColumnIterator.reduce(LazilyCompactedRow.java:204)
         at org.apache.cassandra.io.LazilyCompactedRow$LazyColumnIterator.reduce(LazilyCompactedRow.java:185)
         at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:62)
         at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
         at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
         at com.google.common.collect.Iterators$7.computeNext(Iterators.java:604)
         at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
         at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
         at org.apache.cassandra.db.ColumnIndexer.serializeInternal(ColumnIndexer.java:76)
         at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:50)
         at org.apache.cassandra.io.LazilyCompactedRow.<init>(LazilyCompactedRow.java:88)
         at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:137)
         at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
         at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
         at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
         at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
         at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
         at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
         at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
         at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:426)
         at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:122)
         at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:92)
         at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
         at java.util.concurrent.FutureTask.run(FutureTask.java:138)
         at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
         at java.lang.Thread.run(Thread.java:662)

{noformat}",,jborgstrom,,,,,,,,,,,,,,,,,,,,,28800,28800,,0%,28800,28800,,,,,,,,,,,,,,,,"24/Feb/11 16:32;slebresne;0001-Use-the-right-comparator-when-deserializing-superCol.patch;https://issues.apache.org/jira/secure/attachment/12471850/0001-Use-the-right-comparator-when-deserializing-superCol.patch","24/Feb/11 18:16;slebresne;Unit-test.patch;https://issues.apache.org/jira/secure/attachment/12471857/Unit-test.patch",,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20445,,,Thu Mar 10 10:04:36 UTC 2011,,,,,,,,,,"0|i0g9c7:",92943,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"03/Feb/11 16:41;jbellis;I believe you mentioned on the ML that you have data that compacts fine if you increase the in_memory limit, but not with lazy mode?

If so, can you get me a copy of that sstable to test with?  I can set up a place for you to upload it privately if you can't make it public.;;;","04/Feb/11 11:22;dln;Unfortunately, I'm not at liberty to share that data, sorry. But I'm working on seeing if I can reproduce it with non-sensitive data...

These are timeuuid cols, wide rows (100k or so cols, with a few multi-mb columns but mostly <1k sizes). No validators.

The instance in question had small memtable thresholds (32MB), a pretty low heap (6GB) but nothing out of the ordinary.;;;","16/Feb/11 02:51;jbellis;Any luck reproducing?;;;","23/Feb/11 14:30;dln;Nope, never managed to trigger it again.
Closing this issue for now.;;;","23/Feb/11 14:56;jborgstrom;Hi Jonathan,

I now have a python script that seems to be able to reproduce this after a few runs (it does not seem to always happen)

http://jonas.borgstrom.se/cassandra/CASSANDRA-2104.txt

I've tested this on an empty single node 0.7.2 cluster and default cassandra.yaml, except for in_memory_compaction_limit_in_mb=32 hoping that would make it easier to reproduce.

A complete copy of the data directory (and config) after running the script a couple of times can be downloaded here:
http://jonas.borgstrom.se/cassandra/CASSANDRA-2104.tar.gz

And a copy of system.log from booting cassandra using this data directory:
http://jonas.borgstrom.se/cassandra/CASSANDRA-2104-system.log

Let me know if you need any more details.;;;","23/Feb/11 15:14;jbellis;Jonas, are you also seeing the error only on rows larger than in_memory_compaction_limit then?;;;","23/Feb/11 16:14;jborgstrom;Jonathan, yes that seems to be the case. For me key 3139 (19) seems to be the one triggering this. So increasing in_memory_compaction_limit enough to cover that key seems to do the trick. Even if some other keys are compacted incrementally.;;;","24/Feb/11 11:42;jborgstrom;I've done some more testing now and I'm ONLY able to reproduce this when using both super columns and the TimeUUIDType column comparator.

After looking at the code TimeUUIDType seems to be the only marshaller that would actually notice (throw an exception) if its compare() method was called with a partial value (1-15 bytes in the case of UUIDs).

So to me it looks like the incremental compactor sometimes sends corrupted/partial data to the marshaller, at least for super column families. This corrupted/partial data is silently ignored unless the TimeUUIDType marshaller is used.;;;","24/Feb/11 13:46;slebresne;I have a hard time reproducing (using your script). Do you use TimeUUIDType for the column or the super column names ? (I've tried both though).

I'll continue trying. However, if you happen to have some SSTables that directly triggers it and that is smaller that the ones you added above, that would be awesome (I'm struggling making enough room on my laptop for the ones you attached above :)).

Anyway, thanks for taking time on this, I'll continue to try.;;;","24/Feb/11 14:47;jborgstrom;Hi Sylvain,

The last time I reproduce it I used ""create column family bar with column_type=Super and comparator=TimeUUIDType and subcomparator=UTF8Type;"" and had to run the test case twice.

Anyway here's a new data directory with only a subset of the sstables from the last dump. I get an exception in the log within seconds after issuing a ""compact"" command.

http://jonas.borgstrom.se/cassandra/CASSANDRA-2104-v2.tar.gz (153kB compressed and 127MB uncompressed);;;","24/Feb/11 16:36;slebresne;Thanks a lot Jonas, those new sstables were most useful.

Turns out the problem was that we were using the wrong comparator when deserializing, so we were comparing the column using TimeUUIDType (instead of UTF8Type).

Sadly, it could be that we were generating wrongly sorted on-disk superColumns. However any compaction with the attached patch should fix that anyway.;;;","24/Feb/11 18:16;slebresne;Attaching a unit test that exposes the bug (and confirm the fix);;;","24/Feb/11 19:09;jbellis;committed;;;","25/Feb/11 15:15;hudson;Integrated in Cassandra-0.7 #321 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/321/])
    ;;;","10/Mar/11 00:47;akaris;Hi,

I'm also getting an IndexOutOfBounds exception when compacting.

Here's the detailed error I get on screen when running ""nodetool -h 10.3.133.10 compact"":

Error occured while compacting keyspace test
java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException
    at java.util.concurrent.FutureTask$Sync.innerGet(Unknown Source)
    at java.util.concurrent.FutureTask.get(Unknown Source)
    at org.apache.cassandra.db.CompactionManager.performMajor(CompactionManager.java:186)
    at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1678)
    at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1248)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
    at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
    at com.sun.jmx.mbeanserver.PerInterface.invoke(Unknown Source)
    at com.sun.jmx.mbeanserver.MBeanSupport.invoke(Unknown Source)
    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Unknown Source)
    at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl.access$200(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(Unknown Source)
    at javax.management.remote.rmi.RMIConnectionImpl.invoke(Unknown Source)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
    at sun.rmi.transport.Transport$1.run(Unknown Source)
    at java.security.AccessController.doPrivileged(Native Method)
    at sun.rmi.transport.Transport.serviceCall(Unknown Source)
    at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IndexOutOfBoundsException
    at java.nio.Buffer.checkIndex(Unknown Source)
    at java.nio.HeapByteBuffer.getInt(Unknown Source)
    at org.apache.cassandra.db.DeletedColumn.getLocalDeletionTime(DeletedColumn.java:57)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedStandard(ColumnFamilyStore.java:822)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedColumnsOnly(ColumnFamilyStore.java:809)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeleted(ColumnFamilyStore.java:800)
    at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:94)
    at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:139)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
    at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
    at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
    at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
    at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:427)
    at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:217)
    at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
    at java.util.concurrent.FutureTask.run(Unknown Source)
    ... 3 more

And here's the error I'm getting in my log file:

ERROR [CompactionExecutor:1] 2011-03-09 19:16:52,299 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.IndexOutOfBoundsException
    at java.nio.Buffer.checkIndex(Unknown Source)
    at java.nio.HeapByteBuffer.getInt(Unknown Source)
    at org.apache.cassandra.db.DeletedColumn.getLocalDeletionTime(DeletedColumn.java:57)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedStandard(ColumnFamilyStore.java:822)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeletedColumnsOnly(ColumnFamilyStore.java:809)
    at org.apache.cassandra.db.ColumnFamilyStore.removeDeleted(ColumnFamilyStore.java:800)
    at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:94)
    at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:139)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
    at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
    at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
    at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
    at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
    at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:427)
    at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:217)
    at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
    at java.util.concurrent.FutureTask.run(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)

I run Cassandra 0.7.2. We have 8 machines in the cluster, the error happens only on one machine. I'm not sure it's the same issue than this ticket but it's the only reference I found about compacting and IndexOutOfBounds. We're not inserting any SuperColumn in that database.

Thanks for the help.;;;","10/Mar/11 01:09;akaris;We upgraded to 0.7.3 and we still have the same error, so I guess it's a different problem :(;;;","10/Mar/11 10:04;slebresne;Can you open a new ticket with the description of the problem then. If you're at liberty to attach an bad sstable to reproduce the problem, that would clearly help.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
saved row cache doesn't save the cache,CASSANDRA-2102,12497520,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mdennis,mdennis,mdennis,02/Feb/11 22:42,16/Apr/19 09:33,14/Jul/23 05:52,10/Feb/11 17:09,0.7.1,,,,,,0,,,,"saving row caches works by periodically iterating of the keySet() on the caches and writing the keys for the cached contents to disk.  The cache keys are DecoratedKeys.  DecoratedKeys contain a Token token and a ByteBuffer key.  The underlying buffer on the key gets reused so the contents change.  This means that all the cache entries have distinct tokens but only a handful of distinct key values.  This means that when the cache is loaded you only end up loading a handful of keys instead of the ones actually in your cache.

",,cburroughs,scode,,,,,,,,,,,,,,,,,,,,240,240,,0%,240,240,,,,,,,,,,,,,,,,"03/Feb/11 18:41;mdennis;2102-cassandra-0.7-v2.txt;https://issues.apache.org/jira/secure/attachment/12470173/2102-cassandra-0.7-v2.txt","02/Feb/11 22:51;mdennis;2102-cassandra-0.7.txt;https://issues.apache.org/jira/secure/attachment/12470082/2102-cassandra-0.7.txt",,,,,,,,,,,,,2.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20444,,,Thu Feb 10 22:11:36 UTC 2011,,,,,,,,,,"0|i0g9br:",92941,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"02/Feb/11 22:51;mdennis;attached patch switches to saving tokens to disk instead of keys, removes DecoratedKey.key from the entries in caches (fixing the bug and reducing memory consumption) and versions (kind of) the saved cache files.;;;","02/Feb/11 23:09;jbellis;bq. The underlying buffer on the key gets reused

Where does this reuse happen?  RowMutation.deepCopy should be taking care of it on the local side, and sent-over-the-network buffers are not reused.;;;","03/Feb/11 16:45;jbellis;duplicate of CASSANDRA-2076;;;","03/Feb/11 18:39;mdennis;not really a duplicate.

This deals with writing the cache correctly.

CASSANDRA-2076 should better handle in general when a cache file is corrupt.;;;","03/Feb/11 18:41;mdennis;per IRC discussion v2 patch just copies the key when inserting into the cache.

Since CASSANDRA-1034 looks likely in the somewhat near future, it's pointless to make the changes in the original patch.;;;","03/Feb/11 20:16;jbellis;Let's clone in storageproxy since it's not necessary for keys read over MessagingService;;;","09/Feb/11 00:01;mdennis;I really think safety is the way to go here (e.g. clone right before we put it into the cache).  Not to mention cloning early would be generating memcopies for things we don't need to if they never actually end up in the cache (e.g. things already in the cache, rejected by bloom filters).;;;","10/Feb/11 16:56;jbellis;I think you're right, if your cache is doing its job (high hit rate) then copy-before-insert will be less copies than copy-for-local-operation.;;;","10/Feb/11 17:09;jbellis;committed.  (even without TFFT enabled, it's still a good idea for the same reason as CASSANDRA-1801;;;","10/Feb/11 22:11;hudson;Integrated in Cassandra-0.7 #276 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/276/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support deletes in counters,CASSANDRA-2101,12497514,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,kelvin,kelvin,kelvin,02/Feb/11 22:05,16/Apr/19 09:33,14/Jul/23 05:52,05/Feb/11 05:13,0.8 beta 1,,,,,,0,,,,Obey timestampOfLastDelete during reconciliation.,,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1936,CASSANDRA-1072,"03/Feb/11 20:55;kelvin;0001-CASSANDRA-2101-obey-tsOfLastDelete-remove-irrelevant.patch;https://issues.apache.org/jira/secure/attachment/12470183/0001-CASSANDRA-2101-obey-tsOfLastDelete-remove-irrelevant.patch",,,,,,,,,,,,,,1.0,kelvin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20443,,,Sat Feb 05 06:42:05 UTC 2011,,,,,,,,,,"0|i0g9bj:",92940,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"02/Feb/11 22:06;kelvin;timestampOfLastDelete was added and 90% implemented.  Missed this last logic.;;;","02/Feb/11 22:07;kelvin;add logic to reconcile; test logic.;;;","02/Feb/11 22:22;kelvin;modified patch w/o dependencies on expiring counter column code.;;;","03/Feb/11 13:08;slebresne;Before commenting on the patch itself, I want to use this ticket to recall that counter deletes are intrinsically broken. It has been said already but I'll use this comment to explain in more depth how so and keep a trace of this for the record.

First, I'll use the following notation:
{noformat}
  c(x, 3)@[4, 2] - for a counter column of name x, value 3, timestamp 4 and timestampOfLastDelete 2 (I'll use -1 as the min timestampOfLastDelete).
{noformat}
and
{noformat}
  d(x)@[5] - for a tombstone of name x and timestamp 5
{noformat}

And now suppose that the following inserts are done (in that order):
{noformat}
   c(x, 1)@[1, -1]
   d(x)@[2]
   c(x, 1)@[3, -1]
{noformat}

If these inserts are resolved in that order, everything is fine:
{noformat}
   c(x, 1)@[1, -1]
 + d(x)@[2]
=> d(x)@[2]
 + c(x, 1)@[3, -1]
=> c(x, 1)@[3, 2]
{noformat}

However, some reordering don't work. Namely, if you merge the two counts together, before you merge one of the count with the delete:
{noformat}
   c(x, 1)@[1, -1]
 + c(x, 1)@[3, -1]
=> c(x, 2)@[3, -1]
 + d(x)@[2]
=> c(x, 2)@[3, 2]
{noformat}

The problem is, the resolve operation is not commutative when you consider counter columns and tombstones. But Cassandra rely heavily on resolve being commutative (as a side note, I never understood the reason of the CommutativeType terminology in the code. It suggest that regular columns are not commutative, while they are as far as resolve is concerned. Resolve is not idempotent on counters however).

Not only is there no guarantee on which order the insert will be received by each node, but even if they are in the right order, there is no guarantee that (minor) compaction won't screw up this.

Hence I think that there is not much guarantee we can give on deletes. The only one I can think of is that when on issue a delete, you must wait to issue any following update that the delete have reach all the nodes and all of them have been fully compacted.

That being said, we can keep counter deletes. It's at least useful for cases where you know that you won't reuse a counter ever and want to get rid of the disk space. But I would add a very strong warning to its documentation.

Lastly, the deletion of full counter rows or super columns suffers the same problem for the same reason.
;;;","03/Feb/11 13:08;slebresne;+1 on the patch in the spirit of ""let's make delete work as often as we can"".

However, I realized that the first 'if' of CounterColumn.reconcile() is dead-code since a CounterColumn is never markedForDelete(). It would be nice to remove it while submitting this
;;;","03/Feb/11 20:55;kelvin;Good point, Sylvain!

Attached a new patch w/ the irrelevant code removed.;;;","04/Feb/11 12:10;slebresne;So +1 on the new patch;;;","05/Feb/11 05:13;jbellis;committed;;;","05/Feb/11 06:42;hudson;Integrated in Cassandra #709 (See [https://hudson.apache.org/hudson/job/Cassandra/709/])
    add delete support for counters
patch by Kelvin Kakugawa; reviewed by slebresne for CASSANDRA-2101
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restart required to change cache_save_period,CASSANDRA-2100,12497510,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jhermes,nickmbailey,nickmbailey,02/Feb/11 21:46,16/Apr/19 09:33,14/Jul/23 05:52,25/Feb/11 20:31,0.7.3,,,,,,0,,,,"The cache_save_period is set in the schema for each column family.  However this value is only checked when a node starts up so changing this value isn't really dynamic.

We should actually change this when the schema changes instead of having to restart.",,cburroughs,mdennis,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"21/Feb/11 23:22;jhermes;2100-2.txt;https://issues.apache.org/jira/secure/attachment/12471582/2100-2.txt","24/Feb/11 00:31;jhermes;2100-3.txt;https://issues.apache.org/jira/secure/attachment/12471792/2100-3.txt","10/Feb/11 20:28;jhermes;2100.txt;https://issues.apache.org/jira/secure/attachment/12470802/2100.txt",,,,,,,,,,,,3.0,jhermes,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20442,,,Fri Feb 25 21:02:43 UTC 2011,,,,,,,,,,"0|i0g9bb:",92939,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"10/Feb/11 20:28;jhermes;row/key cache save period in seconds now config-able at runtime via JMX (not just migrations).

On either migration or JMX-change path, the scheduling has been broken out. It first cancels the previous scheduled task (but will not interrupt currently saving caches) then schedules a new task with the correct frequency.;;;","21/Feb/11 20:22;jbellis;can you rebase?;;;","21/Feb/11 23:22;jhermes;Rebased.;;;","23/Feb/11 04:50;jbellis;how does this handle updating when schema changes?;;;","23/Feb/11 23:57;jhermes;I thought that UpdateColumnFamily.applyModels() called CFS.reload() in all cases, but that only happens when the server is not in client mode.

CFMetaData.apply(avro CfDef) will need to call the update logic. Patch forthcoming.;;;","24/Feb/11 00:31;jhermes;Slight surprise in testing due to 2172 changes, but tests correctly (save period is updated at runtime for both JMX and schema migrations (both client and non-client mode)).;;;","25/Feb/11 19:00;jbellis;I'm not a fan of having side effects in CFMetaData like that.

I checked w/ Gary and his take was, ""The saving probably belongs in UpdateColumnFamily.  CFMetaData.apply() should only affect a single object—-nothing persistent."";;;","25/Feb/11 19:29;jhermes;2100-2.txt works fine when the schema changes.

`if (!clientMode) { CFS.reload() }` will reset the config appropriately, and it's good we don't do anything when clientMode because we don't apply() from migration in that case as well (and there would be nothing to reset).

From your comment I thought that something didn't work. Don't scare me like that. :P;;;","25/Feb/11 20:31;jbellis;committed;;;","25/Feb/11 21:02;hudson;Integrated in Cassandra-0.7 #324 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/324/])
    add [get|set][row|key]cacheSavePeriod to JMX
patch by Jon Hermes; reviewed by jbellis for CASSANDRA-2100
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UUID generation when specifying date-times is broken,CASSANDRA-2099,12497499,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,urandom,urandom,02/Feb/11 20:06,16/Apr/19 09:33,14/Jul/23 05:52,03/Feb/11 17:43,0.8 beta 1,,,,,,0,cql,,,"o.a.c.utils.UUIDGen properly guards against a clock moving backward, but this makes the creation of UUIDs based on arbitrary date-times problematic.",,,,,,,,,,,,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,"02/Feb/11 21:02;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2099-fix-UUIDv1-generation-for-non-default-d.txt;https://issues.apache.org/jira/secure/attachment/12470069/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2099-fix-UUIDv1-generation-for-non-default-d.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20441,,,Thu Feb 03 17:43:34 UTC 2011,,,,,,,,,,"0|i0g9b3:",92938,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"03/Feb/11 03:34;gdusbabek;+1;;;","03/Feb/11 04:23;hudson;Integrated in Cassandra #705 (See [https://hudson.apache.org/hudson/job/Cassandra/705/])
    fix UUIDv1 generation for non-default date-times

Patch by eevans; reviewed by gdusbabek for CASSANDRA-2099
;;;","03/Feb/11 17:43;urandom;committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InternalException on system_update_column_family if column_metadata is not assigned,CASSANDRA-2096,12497417,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lenn0x,lenn0x,lenn0x,02/Feb/11 03:44,16/Apr/19 09:33,14/Jul/23 05:52,08/Feb/11 05:06,0.7.1,,,,,,0,,,,"Steps to reproduce:

Execute system_update_column_family without passing in column_metadata in CfDef object.

Error:


java.lang.NullPointerException
	at org.apache.cassandra.config.CFMetaData.convertToAvro(CFMetaData.java:827)
	at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:882)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.process(Cassandra.java:4518)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:3227)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
",,gdusbabek,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/11 03:48;lenn0x;0001-Fix-internal-exception-when-not-passing-in-column_me.patch;https://issues.apache.org/jira/secure/attachment/12470015/0001-Fix-internal-exception-when-not-passing-in-column_me.patch",,,,,,,,,,,,,,1.0,lenn0x,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20440,,,Tue Feb 08 04:41:17 UTC 2011,,,,,,,,,,"0|i0g9af:",92935,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"03/Feb/11 14:21;gdusbabek;I think this needs a rebase. The patch doesn't apply to trunk or 0.7.;;;","03/Feb/11 17:54;gdusbabek;+1;;;","07/Feb/11 18:21;hudson;Integrated in Cassandra-0.7 #254 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/254/])
    Fix internal exception when not passing in column_metadata over Thrift patch by goffinet; reviewed by gdusbabek for CASSANDRA-2096
;;;","08/Feb/11 04:30;jbellis;needs to be merged to trunk [there are conflicts]: svn merge https://svn.apache.org/repos/asf/cassandra/branches/cassandra-0.7 -c r1068028;;;","08/Feb/11 04:41;lenn0x;Merged from 0.7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SST counter repair,CASSANDRA-2095,12497410,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,kelvin,kelvin,kelvin,02/Feb/11 01:24,16/Apr/19 09:33,14/Jul/23 05:52,02/Feb/11 14:47,0.8 beta 1,,,,,,0,,,,"When creating an SST for AES of a commutative/counter CF, do not ""clean"" non-commutative/counter columns.  i.e. deleted columns.",,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/11 01:25;kelvin;0001-CASSANDRA-2095-do-not-clean-nodes-of-deleted-columns.patch;https://issues.apache.org/jira/secure/attachment/12470008/0001-CASSANDRA-2095-do-not-clean-nodes-of-deleted-columns.patch",,,,,,,,,,,,,,1.0,kelvin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20439,,,Wed Feb 02 15:18:43 UTC 2011,,,,,,,,,,"0|i0g9a7:",92934,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,"02/Feb/11 01:25;kelvin;do not clean nodes of deleted columns;;;","02/Feb/11 09:03;slebresne;+1;;;","02/Feb/11 14:47;jbellis;committed;;;","02/Feb/11 15:18;hudson;Integrated in Cassandra #704 (See [https://hudson.apache.org/hudson/job/Cassandra/704/])
    When creating an SST for AES of a commutative/counter CF, do not clean non-commutative/counter columns. i.e. deleted columns
patch by Kelvin Kakugawa; reviewed by slebresne for CASSANDRA-2095
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix regression in CL.ALL read,CASSANDRA-2094,12497408,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,kelvin,kelvin,kelvin,02/Feb/11 01:14,16/Apr/19 09:33,14/Jul/23 05:52,02/Feb/11 14:33,0.7.1,,,,,,0,,,,"regression:
- digest message object re-used across multiple hosts.

problem:
- shared message id, so the first digest response received will remove the callback for all others.",,cburroughs,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2038,,,,,,"02/Feb/11 01:16;kelvin;0001-fix-bug-in-2038-via-4826e8c8.patch;https://issues.apache.org/jira/secure/attachment/12470007/0001-fix-bug-in-2038-via-4826e8c8.patch",,,,,,,,,,,,,,1.0,kelvin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20438,,,Wed Feb 02 14:52:10 UTC 2011,,,,,,,,,,"0|i0g99z:",92933,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"02/Feb/11 01:15;kelvin;caught by distributed testing.;;;","02/Feb/11 01:16;kelvin;do not re-use digest message object.;;;","02/Feb/11 01:31;kelvin;future reference: please test w/ RF=3, which is a more interesting case for CL.ALL.;;;","02/Feb/11 14:33;jbellis;Thanks for catching that, Kelvin!  committed.

(FWIW this is a regression from CASSANDRA-1959.);;;","02/Feb/11 14:52;hudson;Integrated in Cassandra-0.7 #234 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/234/])
    remove digestMessage reuse to fix regression from #1959
patch by Kelvin Kakugawa; reviewed by jbellis for CASSANDRA-2094
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up after failed (repair) streaming operation,CASSANDRA-2088,12497286,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,stuhood,stuhood,01/Feb/11 05:49,16/Apr/19 09:33,14/Jul/23 05:52,13/Apr/11 21:06,0.7.5,,,,,,1,,,,,,christianmovi,dln,skamio,slebresne,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/11 17:45;slebresne;0001-Better-detect-failures-from-the-other-side-in-Incomi.patch;https://issues.apache.org/jira/secure/attachment/12476140/0001-Better-detect-failures-from-the-other-side-in-Incomi.patch","11/Apr/11 05:50;amorton;0001-detect-streaming-failures-and-cleanup-temp-files.patch;https://issues.apache.org/jira/secure/attachment/12475965/0001-detect-streaming-failures-and-cleanup-temp-files.patch","11/Apr/11 05:50;amorton;0002-delete-partial-sstable-if-compaction-error.patch;https://issues.apache.org/jira/secure/attachment/12475966/0002-delete-partial-sstable-if-compaction-error.patch",,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20437,,,Wed Apr 13 21:06:38 UTC 2011,,,,,,,,,,"0|i0g98n:",92927,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"01/Feb/11 05:53;stuhood;Regarding repair: http://www.mail-archive.com/user@cassandra.apache.org/msg09259.html
And compaction: CASSANDRA-2084;;;","07/Mar/11 05:23;amorton;I'm keen to try this ticket (to learn more about compaction and repair) if it's not already been worked on. Also if it's ok for me to take a couple of days while I dig into this.

For compaction I'm looking in
- CompactionManager.doCompaction where it creates a new SSTableWriter via cfs.createCompactionWriter() 
- CompactionManager.doCleanupCompaction() also uses an SSTableWriter

Are the sorts of failures we're considering for compaction ones that come from the CompactionIterator or SSTableScanner ?

For repair I'm looking in:
- IncomingStreamReader appears to clean up the temporary pending file in some error situations. Do we have any more info on the sorts of failures here? e.g. If there is an IOException sending the re-stream message, or a non checked exception it will fail to cleaup the file. 
- I'm looking into what happens in StreamInSession.finished() closeIfFinished()
- Are we considering failures during the streaming or when processing the data after the stream has finished?

Any guidance welcome. ;;;","07/Mar/11 16:59;jbellis;bq. Are the sorts of failures we're considering for compaction ones that come from the CompactionIterator or SSTableScanner ?

Both. Also I suppose it's possible for the writer to error out from lack of disk space since it only checks at the beginning for space and doesn't ""reserve"" it vs flushes.

bq. Are we considering failures during the streaming or when processing the data after the stream has finished?

The former is much more common (I've never seen the latter reported), so I'd start with that.;;;","11/Apr/11 05:50;amorton;patch 0001 tracks failures during AES streaming, files for failed Stream sessions are cleaned up and repair is allowed to continue. Failed files are logged at the StreamSession, TreeRequest, and RepairSession level. 

patch 0002 handle exceptions when doing a (normal) compaction and deletes the temp SSTable. The SSTableWriter components are closed before deletion so that windows will delete correctly. ;;;","12/Apr/11 17:45;slebresne;I think there is a few different things here and I think we should separate them somehow.

Fixing the fact that streaming leave tmp files around when it fails is a 2 lines fix and I think this is simple enough that it could go to 0.7. I'm attaching a patch against 0.7. It's extracted from Aaron first patch, although rebased on 0.7 (and fix a bug).

Making repair aware that there has been some failures is actually more complicated so that should go in 0.8.1 or something (and should go to CASSANDRA-2433 or another ticket that describe the problem better). ;;;","12/Apr/11 17:51;jbellis;bq. I'm attaching a patch against 0.7

Is that 0001-Better-detect-failures-from-the-other-side-in-Incomi.patch?  I don't see the connection to .tmp files.  (Also: have you verified that the channel will actually infinite-loop returning 0?  Kind of odd behavior, although I guess it's technically within-spec.);;;","12/Apr/11 18:21;slebresne;bq. Is that 0001-Better-detect-failures-from-the-other-side-in-Incomi.patch? I don't see the connection to .tmp files. (Also: have you verified that the channel will actually infinite-loop returning 0? Kind of odd behavior, although I guess it's technically within-spec.)

Yes. IncomingStreamReader does clean the tmp file when there is an expection (there's an enclosing 'try catch'). The problem is that no exception is raised if the other side of the connection dies. What will happen then is the read will infinitely read 0 bytes. So this actually avoid the infinite loop returning 0 (and so I think answered your second question, so it wasn't very clear).

Note that without this patch, there is an infinite loop that will hold a socket open forever (and consume cpu, though very few probably in that case). So this is not just merely a fix of deleting the tmp files. But it does as a consequence of correctly raising an exception when should be.;;;","12/Apr/11 18:25;jbellis;+1, and can you move some of that explanation inline as a comment?;;;","12/Apr/11 19:39;slebresne;Committed that first part. I think we should keep that open to fix the tmp files for failed compaction and move the rest to another ticket (like CASSANDRA-2433 for instance).

About the attached patch on cleaning up failed compaction:
  * We should also handle cleanup and scrub
  * We should handle SSTableWriter.Builder as it is yet another place where we could miss to cleanup a tmp file on error.
  * In theory a failed flush could leave a tmp file behind. If that happens having a tmp file would be the least of your problem but for completeness sake we could handle it.
  * The logging when failing to close iwriter and dataFile in SSTableWriter could probably go at error (we should not be failing there, if we do something is wrong)
  * That's nitpick but I'm not a huge fan of catching RuntimeException in this case as this pollute the code for something that would be a programming error (that's probably debatable though). Maybe another solution would be to have this in the final block. It means making sure closeAndDelete() is ok with the file being already closed and/or deleted and having this final block *after* the closeAndOpenReader call.
;;;","13/Apr/11 03:08;amorton;Thanks will take another look at the cleanup for compaction. 
;;;","13/Apr/11 21:06;jbellis;Created CASSANDRA-2468 for compaction cleanup. Will close this one for streaming.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
digest latencies are not included in snitch calculations,CASSANDRA-2085,12497283,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,01/Feb/11 04:19,16/Apr/19 09:33,14/Jul/23 05:52,01/Feb/11 12:38,0.6.12,,,,,,0,,,,"ResponseVerbHandler calls

        MessagingService.instance.maybeAddLatency(cb, message.getFrom(), age);

but maybeAddLatency needs to include DigestResponseHandler (it was ported from 0.7 where that no longer exists)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/11 04:21;jbellis;2085.txt;https://issues.apache.org/jira/secure/attachment/12469888/2085.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20435,,,Tue Feb 01 14:43:52 UTC 2011,,,,,,,,,,"0|i0g97z:",92924,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"01/Feb/11 05:19;stuhood;+1

Tested manually with RF=2 on 3 nodes with ONE and RF=3 on 3 nodes with QUORUM. I'm going to go find and vote for any tickets that mention testing nodes in degraded states.;;;","01/Feb/11 10:00;stuhood;Couldn't find any, but created CASSANDRA-2089;;;","01/Feb/11 12:38;jbellis;committed;;;","01/Feb/11 14:43;hudson;Integrated in Cassandra-0.6 #55 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/55/])
    include digest responses in dynamic snitch latencies
patch by jbellis; reviewed by stuhood for CASSANDRA-2085
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted Handoff and schema race,CASSANDRA-2083,12497250,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,31/Jan/11 19:57,16/Apr/19 09:33,14/Jul/23 05:52,03/Feb/11 23:34,0.7.1,,,,,,0,,,,"If a node is down while a keyspace/cf is created and then data is inserted into the CF causing other nodes to hint, when the down node recovers it will lose some hints until the schema propagates:

{noformat}
ERROR 19:59:28,264 Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1000
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:117)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:377)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:50)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:70)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 19:59:28,356 Applying migration 28e2e7a4-2d74-11e0-9b6b-cdc89135952c
{noformat}",,gdusbabek,,,,,,,,,,,,,,,,,,,,,28800,28800,,0%,28800,28800,,,,,,,,,,,,,,,,"03/Feb/11 17:36;jbellis;2083-v3.txt;https://issues.apache.org/jira/secure/attachment/12470163/2083-v3.txt","03/Feb/11 20:07;brandon.williams;2083-v4.txt;https://issues.apache.org/jira/secure/attachment/12470180/2083-v4.txt","01/Feb/11 22:46;brandon.williams;2083.txt;https://issues.apache.org/jira/secure/attachment/12469992/2083.txt","02/Feb/11 21:10;brandon.williams;2083v2.txt;https://issues.apache.org/jira/secure/attachment/12470070/2083v2.txt",,,,,,,,,,,4.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20433,,,Fri Feb 04 00:04:40 UTC 2011,,,,,,,,,,"0|i0g97j:",92922,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"01/Feb/11 22:46;brandon.williams;Patch to wait up to RING_DELAY for schema alignment before delivering hints.  Also ensures that we keep schema up to date in gossip.;;;","02/Feb/11 14:25;jbellis;gossip propagation is laggy enough that we've traditionally used the special version rpc call for schema version checks instead.  we should probably keep doing that here.;;;","02/Feb/11 21:10;brandon.williams;v2 builds on v1, in that it keeps schema up to date in gossip.  In HHOM we first check if the schema matches in gossip so we can avoid the rpc in the common case of no schema change, then fall back to waiting up to RING_DELAY for agreement via rpc before delivering hints.;;;","02/Feb/11 21:18;jbellis;So...  now that I think about it more, maybe gossip lag is a GOOD thing here: w/ the RPC, everyone will start hammering the recovered nodes w/ replay at the same time.  W/ gossip it will likely be staggered.;;;","02/Feb/11 21:22;brandon.williams;Interesting point, although schema will probably match 99% of the time and they'll replay at the same time anyway.;;;","03/Feb/11 17:34;jbellis;v3 only uses gossip to check for agreement, and adds a random sleep from 0 to 60 seconds if schemas agree to stagger delivery a little.;;;","03/Feb/11 17:59;brandon.williams;Committed with the debug line indicating the check is in progress bumped to info.;;;","03/Feb/11 18:31;brandon.williams;Reverted.  This has problems, the least of which is passing tests.;;;","03/Feb/11 20:07;brandon.williams;v4 is similar to v3, but fixes migration announcement in gossip correctly by splitting the active announcement (via rpc) from the passive announcement (via gossip), increases logging to indicate it is sleeping to stagger the hints, and makes sure the host is still alive after the sleep before beginning delivery.;;;","03/Feb/11 20:16;hudson;Integrated in Cassandra-0.7 #243 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/243/])
    ;;;","03/Feb/11 23:28;gdusbabek;+1;;;","03/Feb/11 23:34;brandon.williams;Committed.;;;","04/Feb/11 00:04;hudson;Integrated in Cassandra-0.7 #245 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/245/])
    Fix race between HH and schema changes.
Patch by brandonwilliams, reviewed by gdusbabek for CASSANDRA-2083
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Saved row cache continues to be read past max cache size,CASSANDRA-2082,12497246,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cburroughs,cburroughs,cburroughs,31/Jan/11 19:46,16/Apr/19 09:33,14/Jul/23 05:52,26/Jul/11 18:58,1.0.0,,,,,,0,,,,"Scenario:
 - Node has a saved row cache of size n
 - node OOMs
 - Make row cache size = .5n to prevent OOM while we debug, restart node.
 - n items are still read from the row cache, making startup take twice as long as needed.


(This is intended as a straightforward bug, not as a hackish CASSANDRA-1966.)",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/11 19:00;cburroughs;0001-v1-Only-load-up-to-capacity-items-into-the-cache.patch;https://issues.apache.org/jira/secure/attachment/12487465/0001-v1-Only-load-up-to-capacity-items-into-the-cache.patch","22/Jul/11 19:00;cburroughs;0002-v1-unit-test-for-CASSANDRA-2082.patch;https://issues.apache.org/jira/secure/attachment/12487466/0002-v1-unit-test-for-CASSANDRA-2082.patch",,,,,,,,,,,,,2.0,cburroughs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20432,,,Tue Jul 26 19:19:38 UTC 2011,,,,,,,,,,"0|i0g97b:",92921,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"22/Jul/11 19:00;cburroughs;Note that the second patch applies on top of CASSANDRA-1966.;;;","26/Jul/11 18:58;jbellis;Committed since the code involved is small, but I note that the applicability is limited because you can't change the cache setting via schema or JMX until the node has started successfully.  So it's only useful if you notice memory pressure and change the setting before the node OOMs.
;;;","26/Jul/11 19:19;hudson;Integrated in Cassandra #975 (See [https://builds.apache.org/job/Cassandra/975/])
    stop reading cache after max size-to-save is reached
patch by Chris Burroughs; reviewed by jbellis for CASSANDRA-2082

jbellis : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1151211
Files : 
* /cassandra/trunk/test/unit/org/apache/cassandra/db/RowCacheTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consistency QUORUM does not work anymore (hector:Could not fullfill request on this host),CASSANDRA-2081,12497245,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,amorton,tbritz,tbritz,31/Jan/11 19:40,16/Apr/19 09:33,14/Jul/23 05:52,07/Feb/11 16:06,0.6.12,0.7.1,,,,,0,,,,"I'm using apache-cassandra-2011-01-28_20-06-01.jar and hector 7.0.25.

Using consistency level Quorum won't work anymore (tested it on read). Consisteny level ONE still works though

I have tried this with one dead node in my cluster.

If I restart cassandra with an older svn revision (apache-cassandra-2011-01-28_20-06-01.jar), I can access the cluster with consistency level QUORUM again, while still using apache-cassandra-2011-01-28_20-06-01.jar and hector 7.0.25 in my application.


11/01/31 19:54:38 ERROR connection.CassandraHostRetryService: Downed intr1n18(192.168.0.18):9160 host still appears to be down: Unable to open transport to intr1n18(192.168.0.18):9160 , java.net.NoRouteToHostException: No route to host
11/01/31 19:54:38 INFO connection.CassandraHostRetryService: Downed Host retry status false with host: intr1n18(192.168.0.18):9160
11/01/31 19:54:45 ERROR connection.HConnectionManager: Could not fullfill request on this host CassandraClient<intr1n11:9160-483>

intr1n11 is marked as up however and I can also access the node through the cassandra cli.


192.168.0.1     Up     Normal  8.02 GB         5.00%   0cc
192.168.0.2     Up     Normal  7.96 GB         5.00%   199
192.168.0.3     Up     Normal  8.24 GB         5.00%   266
192.168.0.4     Up     Normal  4.94 GB         5.00%   333
192.168.0.5     Up     Normal  5.02 GB         5.00%   400
192.168.0.6     Up     Normal  5 GB            5.00%   4cc
192.168.0.7     Up     Normal  5.1 GB          5.00%   599
192.168.0.8     Up     Normal  5.07 GB         5.00%   666
192.168.0.9     Up     Normal  4.78 GB         5.00%   733
192.168.0.10    Up     Normal  4.34 GB         5.00%   7ff
192.168.0.11    Up     Normal  5.01 GB         5.00%   8cc
192.168.0.12    Up     Normal  5.31 GB         5.00%   999
192.168.0.13    Up     Normal  5.56 GB         5.00%   a66
192.168.0.14    Up     Normal  5.82 GB         5.00%   b33
192.168.0.15    Up     Normal  5.57 GB         5.00%   c00
192.168.0.16    Up     Normal  5.03 GB         5.00%   ccc
192.168.0.17    Up     Normal  4.77 GB         5.00%   d99
192.168.0.18    Down   Normal  ?               5.00%   e66
192.168.0.19    Up     Normal  4.78 GB         5.00%   f33
192.168.0.20    Up     Normal  4.83 GB         5.00%   ffffffffffffffff




","linux, hector + cassandra",cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/11 13:18;amorton;2081-2.txt;https://issues.apache.org/jira/secure/attachment/12470443/2081-2.txt","02/Feb/11 21:26;amorton;2081-logging.patch;https://issues.apache.org/jira/secure/attachment/12470072/2081-logging.patch","02/Feb/11 22:29;jbellis;2081.txt;https://issues.apache.org/jira/secure/attachment/12470078/2081.txt",,,,,,,,,,,,3.0,amorton,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20431,,,Mon Feb 07 16:06:12 UTC 2011,,,,,,,,,,"0|i0g973:",92920,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"31/Jan/11 19:55;jbellis;What kind of ""doesn't work"" are you seeing?;;;","31/Jan/11 20:01;tbritz;My application hangs/blocks forever as I catch all the Hector exceptions and retry when there was an error.

Above log file messages will repeat itself again and again.

There are also no error messages in the cassandra log file.

Also ""Could not fullfill request on this host CassandraClient"" is an error message I have never seen before. ;;;","31/Jan/11 20:38;jbellis;Is this RF=3?

What do you see in the Cassandra log when you set log level to debug, for the queries that Hector gives up on?

What are the versions you tried that works/doesn't work?  (In description above both versions are given as apache-cassandra-2011-01-28_20-06-01.jar.);;;","31/Jan/11 21:00;tbritz;RF=3

I will enable the debug log level tomorrow for cassandra, switch back to apache-cassandra-2011-01-28_20-06-01.jar and post you the results.

The last version that I tried that worked was apache-cassandra-2011-01-24_06-01-26.jar. apache-cassandra-2011-01-28_20-06-01.jar doesn't work anymore.
;;;","31/Jan/11 21:22;brandon.williams;I'm not able to reproduce with contrib/stress, can you try that?;;;","01/Feb/11 04:46;amorton;I've sort of stumbled onto something similar with an 0.7 install. I need to go home now so cannot dig any deeper and rule out human error, but this is what I have.

5 node 0.7.0 install

1) Load data in using

python stress.py -d jb-cass1,jb-cass2,jb-cass3,jb-cass4,jb-cass5 -o insert -n 1000000 -e QUORUM -t 10 -i 1 -l 3
(use all 5 nodes, insert 1,000,000 rows with RF 3 and QUORUM and 10 threads, report progress every second)

2) Read back using 

python stress.py -d jb-cass2,jb-cass3,jb-cass4,jb-cass5 -o read -n 1000000 -e QUORUM -t 10 -i 1
(note that jb-cass1 is removed from the list)

3) make big bang

Once the read has run a few seconds I ran ""reboot -f"" on node 1. I expect the read operations to complete, output was 

11270,1315,1315,0.00839671943578,9
11631,361,361,0.00746133188792,11
11631,0,0,NaN,12
11631,0,0,NaN,13
11631,0,0,NaN,14
11631,0,0,NaN,15
11631,0,0,NaN,16
11631,0,0,NaN,17
11631,0,0,NaN,18
11631,0,0,NaN,19
Process Reader-10:
Traceback (most recent call last):
  File ""/vol/apps/python-2.6.4_64/lib/python2.6/multiprocessing/process.py"", line 232, in _bootstrap
    self.run()
  File ""stress.py"", line 279, in run
    r = self.cclient.get_slice(key, parent, p, consistency)
  File ""/local1/frameworks/cassandra/apache-cassandra-0.7.0-src/contrib/py_stress/cassandra/Cassandra.py"", line 432, in get_slice
    return self.recv_get_slice()
  File ""/local1/frameworks/cassandra/apache-cassandra-0.7.0-src/contrib/py_stress/cassandra/Cassandra.py"", line 462, in recv_get_slice
    raise result.te

All clients died. stress.py is not setting a timeout on the thrift socket, so am guessing this is server side.

I was running DEBUG on all the nodes (but had turned off the line numbers), this is from one. the 114.63 machine is obviously the one I killed. 


DEBUG [pool-1-thread-2] 2011-02-01 17:14:08,186 StorageService.java (line org.apache.cassandra.service.StorageService) Sorted endpoints are /192.168.114.63,jb08.wetafx.co.nz/192.168.114.67,/192.168.114.64
DEBUG [pool-1-thread-2] 2011-02-01 17:14:08,186 QuorumResponseHandler.java (line org.apache.cassandra.service.QuorumResponseHandler) QuorumResponseHandler blocking for 2 responses
DEBUG [pool-1-thread-2] 2011-02-01 17:14:08,186 StorageProxy.java (line org.apache.cassandra.service.StorageProxy) strongread reading digest for SliceFromReadCommand(table='Keyspace1', key='30323334343534', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) from 6624@jb08.wetafx.co.nz/192.168.114.67
DEBUG [pool-1-thread-2] 2011-02-01 17:14:08,187 StorageProxy.java (line org.apache.cassandra.service.StorageProxy) strongread reading data for SliceFromReadCommand(table='Keyspace1', key='30323334343534', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) from 6623@/192.168.114.63
DEBUG [pool-1-thread-2] 2011-02-01 17:14:08,187 StorageProxy.java (line org.apache.cassandra.service.StorageProxy) strongread reading digest for SliceFromReadCommand(table='Keyspace1', key='30323334343534', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) from 6624@/192.168.114.64
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 SliceQueryFilter.java (line org.apache.cassandra.db.filter.SliceQueryFilter) collecting 0 of 5: 4330:false:34@1296532428248604
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 SliceQueryFilter.java (line org.apache.cassandra.db.filter.SliceQueryFilter) collecting 1 of 5: 4331:false:34@1296532428248637
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 SliceQueryFilter.java (line org.apache.cassandra.db.filter.SliceQueryFilter) collecting 2 of 5: 4332:false:34@1296532428248640
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 SliceQueryFilter.java (line org.apache.cassandra.db.filter.SliceQueryFilter) collecting 3 of 5: 4333:false:34@1296532428248642
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 SliceQueryFilter.java (line org.apache.cassandra.db.filter.SliceQueryFilter) collecting 4 of 5: 4334:false:34@1296532428248656
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 ReadVerbHandler.java (line org.apache.cassandra.db.ReadVerbHandler) digest is 220b82e28c2bb4be869c168243d75f01
DEBUG [ReadStage:19] 2011-02-01 17:14:08,187 ReadVerbHandler.java (line org.apache.cassandra.db.ReadVerbHandler) Read key 30323334343534; sending response to 7D8FA1FD-A2FE-6A54-7BB0-3B129206D1E1@jb08.wetafx.co.nz/192.168.114.67
DEBUG [RequestResponseStage:13] 2011-02-01 17:14:08,188 ResponseVerbHandler.java (line org.apache.cassandra.net.ResponseVerbHandler) Processing response on a callback from 7D8FA1FD-A2FE-6A54-7BB0-3B129206D1E1@jb08.wetafx.co.nz/192.168.114.67
DEBUG [RequestResponseStage:13] 2011-02-01 17:14:08,188 ReadResponseResolver.java (line org.apache.cassandra.service.ReadResponseResolver) Preprocessed digest response
DEBUG [RequestResponseStage:16] 2011-02-01 17:14:08,188 ResponseVerbHandler.java (line org.apache.cassandra.net.ResponseVerbHandler) Processing response on a callback from 7D8FA1FD-A2FE-6A54-7BB0-3B129206D1E1@/192.168.114.64
DEBUG [RequestResponseStage:16] 2011-02-01 17:14:08,188 ReadResponseResolver.java (line org.apache.cassandra.service.ReadResponseResolver) Preprocessed digest response
 INFO [ScheduledTasks:1] 2011-02-01 17:14:15,438 Gossiper.java (line org.apache.cassandra.gms.Gossiper) InetAddress /192.168.114.63 is now dead.
DEBUG [ScheduledTasks:1] 2011-02-01 17:14:15,440 MessagingService.java (line org.apache.cassandra.net.MessagingService) Resetting pool for /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:14:17,442 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [pool-1-thread-1] 2011-02-01 17:14:18,183 CassandraServer.java (line org.apache.cassandra.thrift.CassandraServer) ... timed out
DEBUG [pool-1-thread-2] 2011-02-01 17:14:18,189 CassandraServer.java (line org.apache.cassandra.thrift.CassandraServer) ... timed out
DEBUG [pool-1-thread-1] 2011-02-01 17:14:18,232 ClientState.java (line org.apache.cassandra.service.ClientState) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-2] 2011-02-01 17:14:18,232 ClientState.java (line org.apache.cassandra.service.ClientState) logged out: #<User allow_all groups=[]>
DEBUG [ScheduledTasks:1] 2011-02-01 17:14:29,811 StorageLoadBalancer.java (line org.apache.cassandra.service.StorageLoadBalancer) Disseminating load info ...
DEBUG [ScheduledTasks:1] 2011-02-01 17:15:29,814 StorageLoadBalancer.java (line org.apache.cassandra.service.StorageLoadBalancer) Disseminating load info ...
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:15:53,637 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:16:08,667 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:16:23,697 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [ScheduledTasks:1] 2011-02-01 17:16:29,816 StorageLoadBalancer.java (line org.apache.cassandra.service.StorageLoadBalancer) Disseminating load info ...
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:16:36,723 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:16:50,751 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:17:03,775 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:17:19,807 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [ScheduledTasks:1] 2011-02-01 17:17:29,818 StorageLoadBalancer.java (line org.apache.cassandra.service.StorageLoadBalancer) Disseminating load info ...
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:17:32,834 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:17:42,852 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63
DEBUG [WRITE-/192.168.114.63] 2011-02-01 17:17:56,880 OutboundTcpConnection.java (line org.apache.cassandra.net.OutboundTcpConnection) attempting to connect to /192.168.114.63


need to leave work now so may not be able to get further into this until tomorrow. ;;;","01/Feb/11 08:48;amorton;Had a read through the code for think I have an idea what my problem was, not sure if it applies to the previous issue and not sure if its a real bug. 

o.a.c.service.ReadCallback.response() will only signal the o.a.c.utils.SimpleCondition if the data request has been received. If the signal is not set after rpc_timeout then ReadCallback.get() will raise a j.u.c.TimeoutException() which comes out of the StorageProxy and is caught in CassandraServer and turned into a o.a.c.thrift.TimedOutException. 

So if the node that is asked for the data fails to return, the entire request will timeout even if there are enough nodes to serve the request. I think I've seen this discussed before as by design, and the client should just retry in response to the timeout. Is that correct ? ;;;","01/Feb/11 11:04;tbritz;Brandon, I haven't yet run stress test. I can reproduce this error every single time with a single thread accessing my idle cluster.

I also reverted to an older version of hector, but this won't help. As noted before, this error doesn't occur running apache-cassandra-2011-01-24_06-01-26.jar.

Here is the debug output of one of the nodes timing out in my application and not returning an answer:

I only try to access (read/iterator) one single table  ""table_usersources"".

The application runs on node intr1n5 (192.168.0.5) and I added the debug output of intr1n19 (192.168.0.19). The node intr1n18(192.168.0.18) is down and not responding.

Please let me know if you need more information in order to fix this bug. Thanks!


Intr1n19:

 INFO [HintedHandoff:1] 2011-02-01 11:47:39,772 HintedHandOffManager.java (line 249) Finished hinted handoff of 0 rows to endpoint /192.168.0.15
 INFO [ScheduledTasks:1] 2011-02-01 11:47:40,773 Gossiper.java (line 205) InetAddress /192.168.0.10 is now dead.
DEBUG [ScheduledTasks:1] 2011-02-01 11:47:40,773 MessagingService.java (line 176) Resetting pool for /192.168.0.10
 INFO [HintedHandoff:1] 2011-02-01 11:47:40,775 HintedHandOffManager.java (line 192) Started hinted handoff for endpoint /192.168.0.10
 INFO [GossipStage:1] 2011-02-01 11:47:40,775 Gossiper.java (line 579) InetAddress /192.168.0.10 is now UP
 INFO [HintedHandoff:1] 2011-02-01 11:47:40,775 HintedHandOffManager.java (line 249) Finished hinted handoff of 0 rows to endpoint /192.168.0.10
DEBUG [WRITE-intr1n4/192.168.0.4] 2011-02-01 11:47:41,479 OutboundTcpConnection.java (line 159) attempting to connect to intr1n4/192.168.0.4
DEBUG [WRITE-intr1n20/192.168.0.20] 2011-02-01 11:47:42,729 OutboundTcpConnection.java (line 159) attempting to connect to intr1n20/192.168.0.20
DEBUG [WRITE-intr1n11/192.168.0.11] 2011-02-01 11:47:42,776 OutboundTcpConnection.java (line 159) attempting to connect to intr1n11/192.168.0.11
DEBUG [WRITE-intr1n18/192.168.0.18] 2011-02-01 11:47:46,781 OutboundTcpConnection.java (line 159) attempting to connect to intr1n18/192.168.0.18
DEBUG [WRITE-intr1n15/192.168.0.15] 2011-02-01 11:47:50,786 OutboundTcpConnection.java (line 159) attempting to connect to intr1n15/192.168.0.15
DEBUG [WRITE-intr1n10/192.168.0.10] 2011-02-01 11:47:53,698 OutboundTcpConnection.java (line 159) attempting to connect to intr1n10/192.168.0.10

DEBUG [ScheduledTasks:1] 2011-02-01 11:48:15,413 GCInspector.java (line 135) GC for ParNew: 32 ms, 124741344 reclaimed leaving 880060312 used; max is 3289776128
DEBUG [ScheduledTasks:1] 2011-02-01 11:48:24,853 FileUtils.java (line 48) Deleting LocationInfo-f-79-Index.db
DEBUG [ScheduledTasks:1] 2011-02-01 11:48:24,853 FileUtils.java (line 48) Deleting LocationInfo-f-79-Filter.db
DEBUG [ScheduledTasks:1] 2011-02-01 11:48:24,854 FileUtils.java (line 48) Deleting LocationInfo-f-79-Statistics.db
 INFO [ScheduledTasks:1] 2011-02-01 11:48:24,854 SSTable.java (line 147) Deleted /hd2/cassandra_md5/data/system/LocationInfo-f-79
DEBUG [WRITE-intr1n18/192.168.0.18] 2011-02-01 11:48:28,820 OutboundTcpConnection.java (line 159) attempting to connect to intr1n18/192.168.0.18
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,936 CassandraServer.java (line 445) range_slice
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,943 StorageProxy.java (line 514) RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,], max_keys=250}
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,943 StorageProxy.java (line 705) restricted ranges for query [,] are [[,0cc], (0cc,199], (199,266], (266,333], (333,400], (400,4cc], (4cc,599], (599,666], (666,733], (733,7ff], (7ff,8cc], (8cc,999], (999,a66], (a66,b33], (b33,c00], (c00,ccc], (ccc,d99], (d99,e66], (e66,f33], (f33,ffffffffffffffff], (ffffffffffffffff,]]
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,949 ReadCallback.java (line 58) ReadCallback blocking for 2 responses
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,949 StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,0cc], max_keys=250} from 269@/192.168.0.1
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,949 StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,0cc], max_keys=250} from 269@/192.168.0.2
DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,950 StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,0cc], max_keys=250} from 269@/192.168.0.3
DEBUG [RequestResponseStage:1] 2011-02-01 11:48:28,954 ResponseVerbHandler.java (line 48) Processing response on a callback from 269@/192.168.0.1
DEBUG [ScheduledTasks:1] 2011-02-01 11:48:38,771 StorageLoadBalancer.java (line 349) Disseminating load info ...
DEBUG [pool-1-thread-1] 2011-02-01 11:48:38,950 CassandraServer.java (line 483) ... timed out
DEBUG [pool-1-thread-1] 2011-02-01 11:48:38,957 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [WRITE-intr1n18/192.168.0.18] 2011-02-01 11:48:44,835 OutboundTcpConnection.java (line 159) attempting to connect to intr1n18/192.168.0.18
DEBUG [pool-1-thread-3] 2011-02-01 11:48:56,275 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-6] 2011-02-01 11:48:56,275 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-7] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-11] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-5] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-2] 2011-02-01 11:48:56,277 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-8] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-10] 2011-02-01 11:48:56,277 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-12] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-15] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-14] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-9] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-16] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-13] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [pool-1-thread-4] 2011-02-01 11:48:56,278 ClientState.java (line 91) logged out: #<User allow_all groups=[]>
DEBUG [WRITE-intr1n18/192.168.0.18] 2011-02-01 11:48:57,845 OutboundTcpConnection.java (line 159) attempting to connect to intr1n18/192.168.0.18


Application:

11:48:25,616 INFO  ~ Registering JMX me.prettyprint.cassandra.service:ServiceType=hector,MonitorType=hector
11:48:25,652 INFO  ~ get connection for table_lists: consistency: ONE
11:48:25,695 INFO  ~ get connection for table_lists: consistency: ONE
11:48:25,825 INFO  ~ Downed Host Retry service started with queue size -1 and retry delay 10s
11:48:28,887 ERROR ~ Unable to open transport to intr1n18(192.168.0.18):9160
org.apache.thrift.transport.TTransportException: java.net.NoRouteToHostException: No route to host
        at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
        at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
        at me.prettyprint.cassandra.connection.HThriftClient.open(HThriftClient.java:111)
        at me.prettyprint.cassandra.connection.ConcurrentHClientPool.<init>(ConcurrentHClientPool.java:44)
        at me.prettyprint.cassandra.connection.HConnectionManager.<init>(HConnectionManager.java:63)
        at me.prettyprint.cassandra.service.AbstractCluster.<init>(AbstractCluster.java:62)
        at me.prettyprint.cassandra.service.AbstractCluster.<init>(AbstractCluster.java:58)
        at me.prettyprint.cassandra.service.ThriftCluster.<init>(ThriftCluster.java:17)
        at me.prettyprint.hector.api.factory.HFactory.createCluster(HFactory.java:157)
        at me.prettyprint.hector.api.factory.HFactory.getOrCreateCluster(HFactory.java:136)
     ....
Caused by: java.net.NoRouteToHostException: No route to host
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
        ... 23 more
11:48:28,889 ERROR ~ Could not start connection pool for host intr1n18(192.168.0.18):9160
11:48:28,889 INFO  ~ Host detected as down was added to retry queue: intr1n18(192.168.0.18):9160
11:48:28,897 INFO  ~ get connection for table_usersources: consistency: QUORUM
11:48:38,014 ERROR ~ Unable to open transport to intr1n18(192.168.0.18):9160
org.apache.thrift.transport.TTransportException: java.net.NoRouteToHostException: No route to host
        at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
        at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
        at me.prettyprint.cassandra.connection.HThriftClient.open(HThriftClient.java:111)
        at me.prettyprint.cassandra.connection.CassandraHostRetryService$RetryRunner.verifyConnection(CassandraHostRetryService.java:116)
        at me.prettyprint.cassandra.connection.CassandraHostRetryService$RetryRunner.run(CassandraHostRetryService.java:96)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.NoRouteToHostException: No route to host
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
        ... 13 more
11:48:38,019 ERROR ~ Downed intr1n18(192.168.0.18):9160 host still appears to be down: Unable to open transport to intr1n18(192.168.0.18):9160 , java.net.NoRouteToHostException: No route to host
11:48:38,020 INFO  ~ Downed Host retry status false with host: intr1n18(192.168.0.18):9160
11:48:38,956 ERROR ~ Could not fullfill request on this host CassandraClient<intr1n19:9160-594>
11:48:38,956 ERROR ~ Exception:
me.prettyprint.hector.api.exceptions.HTimedOutException: TimedOutException()
        at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:32)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:161)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:143)
        at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:101)
        at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:159)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:129)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.getRangeSlices(KeyspaceServiceImpl.java:165)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:67)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:63)
        at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)
        at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:85)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery.execute(ThriftRangeSlicesQuery.java:62)
    ...
Caused by: TimedOutException()
        at org.apache.cassandra.thrift.Cassandra$get_range_slices_result.read(Cassandra.java:12104)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_range_slices(Cassandra.java:732)
        at org.apache.cassandra.thrift.Cassandra$Client.get_range_slices(Cassandra.java:704)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:149)
        ... 23 more
11:48:48,961 ERROR ~ Could not fullfill request on this host CassandraClient<intr1n17:9160-577>
11:48:48,961 ERROR ~ Exception:
me.prettyprint.hector.api.exceptions.HTimedOutException: TimedOutException()
        at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:32)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:161)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:143)
        at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:101)
        at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:159)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:129)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.getRangeSlices(KeyspaceServiceImpl.java:165)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:67)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery$1.doInKeyspace(ThriftRangeSlicesQuery.java:63)
        at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)
        at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:85)
        at me.prettyprint.cassandra.model.thrift.ThriftRangeSlicesQuery.execute(ThriftRangeSlicesQuery.java:62)
       ...
Caused by: TimedOutException()
        at org.apache.cassandra.thrift.Cassandra$get_range_slices_result.read(Cassandra.java:12104)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_range_slices(Cassandra.java:732)
        at org.apache.cassandra.thrift.Cassandra$Client.get_range_slices(Cassandra.java:704)
        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$3.execute(KeyspaceServiceImpl.java:149)
        ... 23 more
11:48:51,025 ERROR ~ Unable to open transport to intr1n18(192.168.0.18):9160
org.apache.thrift.transport.TTransportException: java.net.NoRouteToHostException: No route to host
        at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
        at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
        at me.prettyprint.cassandra.connection.HThriftClient.open(HThriftClient.java:111)
        at me.prettyprint.cassandra.connection.CassandraHostRetryService$RetryRunner.verifyConnection(CassandraHostRetryService.java:116)
        at me.prettyprint.cassandra.connection.CassandraHostRetryService$RetryRunner.run(CassandraHostRetryService.java:96)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.NoRouteToHostException: No route to host
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
        ... 13 more
11:48:51,025 ERROR ~ Downed intr1n18(192.168.0.18):9160 host still appears to be down: Unable to open transport to intr1n18(192.168.0.18):9160 , java.net.NoRouteToHostException: No route to host
11:48:51,025 INFO  ~ Downed Host retry status false with host: intr1n18(192.168.0.18):9160



Address         Status State   Load            Owns    Token
                                                       ffffffffffffffff
192.168.0.1     Up     Normal  11.26 GB        5.00%   0cc
192.168.0.2     Up     Normal  11.23 GB        5.00%   199
192.168.0.3     Up     Normal  11.58 GB        5.00%   266
192.168.0.4     Up     Normal  6.77 GB         5.00%   333
192.168.0.5     Up     Normal  6.86 GB         5.00%   400
192.168.0.6     Up     Normal  6.81 GB         5.00%   4cc
192.168.0.7     Up     Normal  6.88 GB         5.00%   599
192.168.0.8     Up     Normal  6.84 GB         5.00%   666
192.168.0.9     Up     Normal  6.52 GB         5.00%   733
192.168.0.10    Up     Normal  5.17 GB         5.00%   7ff
192.168.0.11    Up     Normal  6.75 GB         5.00%   8cc
192.168.0.12    Up     Normal  7.06 GB         5.00%   999
192.168.0.13    Up     Normal  7.27 GB         5.00%   a66
192.168.0.14    Up     Normal  7.71 GB         5.00%   b33
192.168.0.15    Up     Normal  7.46 GB         5.00%   c00
192.168.0.16    Up     Normal  6.94 GB         5.00%   ccc
192.168.0.17    Up     Normal  6.45 GB         5.00%   d99
192.168.0.18    Down   Normal  ?               5.00%   e66
192.168.0.19    Up     Normal  6.26 GB         5.00%   f33
192.168.0.20    Up     Normal  6.33 GB         5.00%   ffffffffffffffff

;;;","01/Feb/11 20:35;amorton;My understanding here is the 0.19 node is sending read requests to the 0.1, 0.2 and 0.3 nodes and only getting a reply from the 0.1 node before timing out. The 0.1 node is the first node the request is sent to, so this is the data request the others are digest. 

The timeout is the rpc_timeout, and can be seen here...

DEBUG [pool-1-thread-1] 2011-02-01 11:48:28,949 ReadCallback.java (line 58) ReadCallback blocking for 2 responses
...10 seconds... 
DEBUG [pool-1-thread-1] 2011-02-01 11:48:38,950 CassandraServer.java (line 483) ... timed out

Whats happening on the 0.2 and 0.3 nodes at this point? Are they logging errors or WARN messages about dropped messages ? Can you see any logs about processing messages from the 0.19 node? I'm not sure the down 0.18 node is a factor here.

The client should be retrying when it gets a timeout, which I think you said Hector was doing. 

 ;;;","01/Feb/11 21:46;tbritz;There is absolutely no load on the cluster, so it's strange that the timeout is being triggered. I will put the other nodes into debug mode tomorrow as well and post the DEBUG output of 0.1, 0.2, and 0.3

;;;","02/Feb/11 16:08;tbritz;Debug Output:



========================================
192.168.0.1
--------------------
2011-02-02 12:54:29,117 DEBUG [ScheduledTasks:1] StorageLoadBalancer.java (line 349) Disseminating load info ...

2011-02-02 12:54:35,016 DEBUG [ReadStage:8] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 567@/192.168.0.7
2011-02-02 12:54:45,015 DEBUG [ReadStage:9] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 600@/192.168.0.7

2011-02-02 12:54:49,679 DEBUG [pool-1-thread-5] ClientState.java (line 91) logged out: #<User allow_all groups=[]>

========================================
192.168.0.2
--------------------
2011-02-02 12:54:30,147 DEBUG [ScheduledTasks:1] StorageLoadBalancer.java (line 349) Disseminating load info ...

2011-02-02 12:54:34,789 DEBUG [MutationStage:5] RowMutationVerbHandler.java (line 52) Applying RowMutation(keyspace='table_lists', key='32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674390000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674391000!2419199,])])
2011-02-02 12:54:34,790 DEBUG [MutationStage:5] Table.java (line 397) applying mutation of row 32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334
2011-02-02 12:54:34,792 DEBUG [MutationStage:5] RowMutationVerbHandler.java (line 81) RowMutation(keyspace='table_lists', key='32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674390000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674391000!2419199,])]) applied.  Sending response to 566@/192.168.0.7

2011-02-02 12:54:34,953 DEBUG [ReadStage:8] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 567@/192.168.0.7
2011-02-02 12:54:44,965 DEBUG [ReadStage:9] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 600@/192.168.0.7

2011-02-02 12:54:45,057 DEBUG [ScheduledTasks:1] GCInspector.java (line 135) GC for ParNew: 14 ms, 13086296 reclaimed leaving 2087311488 used; max is 4856348672
2011-02-02 12:54:49,613 DEBUG [pool-1-thread-11] ClientState.java (line 91) logged out: #<User allow_all groups=[]>

========================================
192.168.0.3
--------------------
2011-02-02 12:54:30,920 DEBUG [ScheduledTasks:1] StorageLoadBalancer.java (line 349) Disseminating load info ...

2011-02-02 12:54:34,368 DEBUG [MutationStage:8] RowMutationVerbHandler.java (line 52) Applying RowMutation(keyspace='table_lists', key='36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674314000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674322000!2419199,])])
2011-02-02 12:54:34,369 DEBUG [MutationStage:8] Table.java (line 397) applying mutation of row 36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165
2011-02-02 12:54:34,371 DEBUG [MutationStage:8] RowMutationVerbHandler.java (line 81) RowMutation(keyspace='table_lists', key='36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674314000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674322000!2419199,])]) applied.  Sending response to 562@/192.168.0.7
2011-02-02 12:54:34,381 DEBUG [MutationStage:9] RowMutationVerbHandler.java (line 52) Applying RowMutation(keyspace='table_lists', key='32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674390000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674391000!2419199,])])
2011-02-02 12:54:34,382 DEBUG [MutationStage:9] Table.java (line 397) applying mutation of row 32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334
2011-02-02 12:54:34,383 DEBUG [MutationStage:9] RowMutationVerbHandler.java (line 81) RowMutation(keyspace='table_lists', key='32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334', modifications=[ColumnFamily(table_lists [7461626c655f6c69737473:false:116@1296647674390000!2419199,]), ColumnFamily(table_lists_meta [6e6578745f72657175657374:false:8@1296647674391000!2419199,])]) applied.  Sending response to 564@/192.168.0.7

2011-02-02 12:54:34,539 DEBUG [ReadStage:8] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 567@/192.168.0.7
2011-02-02 12:54:44,540 DEBUG [ReadStage:9] RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=} to 600@/192.168.0.7

2011-02-02 12:54:49,202 DEBUG [pool-1-thread-13] ClientState.java (line 91) logged out: #<User allow_all groups=[]>

========================================
192.168.0.7
--------------------
2011-02-02 12:54:34,059 DEBUG [ScheduledTasks:1] StorageLoadBalancer.java (line 349) Disseminating load info ...

2011-02-02 12:54:34,356 DEBUG [pool-1-thread-1] CassandraServer.java (line 355) batch_mutate
2011-02-02 12:54:34,375 DEBUG [pool-1-thread-1] StorageProxy.java (line 154) insert writing key 36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165 to 561@/192.168.0.5
2011-02-02 12:54:34,375 DEBUG [pool-1-thread-1] StorageProxy.java (line 154) insert writing key 36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165 to 562@/192.168.0.3
2011-02-02 12:54:34,381 DEBUG [pool-1-thread-1] StorageProxy.java (line 154) insert writing key 36306463373934666139396136366665303539636130373063333366323066615f2e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313636345f65626263303033372d356639612d343066382d383833382d356436336433616233366165 to 563@/94.242.198.13
2011-02-02 12:54:34,384 DEBUG [RequestResponseStage:3] ResponseVerbHandler.java (line 48) Processing response on a callback from 561@/192.168.0.5
2011-02-02 12:54:34,386 DEBUG [RequestResponseStage:5] ResponseVerbHandler.java (line 48) Processing response on a callback from 563@/192.168.0.4
2011-02-02 12:54:34,386 DEBUG [RequestResponseStage:4] ResponseVerbHandler.java (line 48) Processing response on a callback from 562@/192.168.0.3

2011-02-02 12:54:34,391 DEBUG [pool-1-thread-2] CassandraServer.java (line 355) batch_mutate
2011-02-02 12:54:34,393 DEBUG [pool-1-thread-2] StorageProxy.java (line 154) insert writing key 32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334 to 564@/192.168.0.3
2011-02-02 12:54:34,394 DEBUG [pool-1-thread-2] StorageProxy.java (line 154) insert writing key 32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334 to 565@/192.168.0.4
2011-02-02 12:54:34,394 DEBUG [pool-1-thread-2] StorageProxy.java (line 154) insert writing key 32383663623561363162363164306231333162326264656232303730303866625f7777772e7768617469736d79697076362e6e65745f686f7374726571756573746c6973745f393232333337303734303230373130313432315f36346334613666322d333737392d343331642d623334382d663636383533316233656334 to 566@/192.168.0.2
2011-02-02 12:54:34,398 DEBUG [RequestResponseStage:6] ResponseVerbHandler.java (line 48) Processing response on a callback from 564@/192.168.0.3
2011-02-02 12:54:34,398 DEBUG [RequestResponseStage:7] ResponseVerbHandler.java (line 48) Processing response on a callback from 565@/192.168.0.4
2011-02-02 12:54:34,399 DEBUG [RequestResponseStage:8] ResponseVerbHandler.java (line 48) Processing response on a callback from 566@/192.168.0.2

2011-02-02 12:54:34,531 DEBUG [pool-1-thread-3] CassandraServer.java (line 445) range_slice
2011-02-02 12:54:34,533 DEBUG [pool-1-thread-3] StorageProxy.java (line 514) RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,], max_keys=250}
2011-02-02 12:54:34,534 DEBUG [pool-1-thread-3] StorageProxy.java (line 705) restricted ranges for query [,] are [[,24], (24,49], (49,6d], (6d,92], (92,b6], (b6,db], (db,ffffffffffffffff], (ffffffffffffffff,]]
2011-02-02 12:54:34,536 DEBUG [pool-1-thread-3] ReadCallback.java (line 58) ReadCallback blocking for 2 responses
2011-02-02 12:54:34,536 DEBUG [pool-1-thread-3] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 567@/192.168.0.1
2011-02-02 12:54:34,537 DEBUG [pool-1-thread-3] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 567@/192.168.0.2
2011-02-02 12:54:34,537 DEBUG [pool-1-thread-3] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 567@/192.168.0.3
2011-02-02 12:54:34,554 DEBUG [RequestResponseStage:1] ResponseVerbHandler.java (line 48) Processing response on a callback from 567@/192.168.0.3
2011-02-02 12:54:44,537 DEBUG [pool-1-thread-3] CassandraServer.java (line 483) ... timed out
2011-02-02 12:54:44,548 DEBUG [pool-1-thread-3] ClientState.java (line 91) logged out: #<User allow_all groups=[]>

2011-02-02 12:54:44,549 DEBUG [pool-1-thread-4] CassandraServer.java (line 445) range_slice
2011-02-02 12:54:44,550 DEBUG [pool-1-thread-4] StorageProxy.java (line 514) RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,], max_keys=250}
2011-02-02 12:54:44,550 DEBUG [pool-1-thread-4] StorageProxy.java (line 705) restricted ranges for query [,] are [[,24], (24,49], (49,6d], (6d,92], (92,b6], (b6,db], (db,ffffffffffffffff], (ffffffffffffffff,]]
2011-02-02 12:54:44,550 DEBUG [pool-1-thread-4] ReadCallback.java (line 58) ReadCallback blocking for 2 responses
2011-02-02 12:54:44,552 DEBUG [pool-1-thread-4] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 600@/192.168.0.1
2011-02-02 12:54:44,552 DEBUG [pool-1-thread-4] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 600@/192.168.0.2
2011-02-02 12:54:44,552 DEBUG [pool-1-thread-4] StorageProxy.java (line 562) reading RangeSliceCommand{keyspace='table_usersources', column_family='table_usersources_meta', super_column=null, predicate=SlicePredicate(column_names:[java.nio.HeapByteBuffer[pos=76 lim=88 cap=65536]]), range=[,24], max_keys=250} from 600@/192.168.0.3
2011-02-02 12:54:44,554 DEBUG [RequestResponseStage:5] ResponseVerbHandler.java (line 48) Processing response on a callback from 600@/192.168.0.1
2011-02-02 12:54:54,550 DEBUG [pool-1-thread-4] CassandraServer.java (line 483) ... timed out
2011-02-02 12:54:54,552 DEBUG [pool-1-thread-4] ClientState.java (line 91) logged out: #<User allow_all groups=[]>


;;;","02/Feb/11 21:25;amorton;There is something odd about the way the messages are been described in the logs, e.g. yours say

2011-02-02 12:54:44,554 DEBUG [RequestResponseStage:5] ResponseVerbHandler.java (line 48) Processing response on a callback from 600@/192.168.0.1

mine say

DEBUG [RequestResponseStage:16] 2011-02-01 17:14:08,188 ResponseVerbHandler.java (line org.apache.cassandra.net.ResponseVerbHandler) Processing response on a callback from 7D8FA1FD-A2FE-6A54-7BB0-3B129206D1E1@/192.168.114.6 

The MessagingService.sendRR() overload that takes an array (and is called when there are multiple messages) should be updating the message ID for all messages to a shared GUID before sending. This happens the log message about ""StorageProxy.java (line 562) reading RangeSliceCommand..."" It should be present in the ResponseVerbHandler log message.

Perhaps this means the callback for the message cannot be found. To test the theory can you apply the 2081-logging.patch to the node your are running the request on? It updates ResponseVernHandler to log when a message is lost.


;;;","02/Feb/11 21:26;amorton;adds logging to ResponseVerbHandler to log is the message is received but there is no callback;;;","02/Feb/11 21:31;jbellis;Thibaut is using a 0.7.1 prerelease version where the uuid messages from sendRR have been replaced with unique per-host messages (which are standard int IDs like the old non-shared messages were).;;;","02/Feb/11 21:58;tbritz;Is there an archive of older builds from hudson? I could try out a few versions between 24th january and the 28th to pinpoint the revision that is causing this. This won't take long.;;;","02/Feb/11 22:17;amorton;Looking at the right code now. 

In StorageProxy.fetchRows() line 395 the digestMessage is shared for all non local digest requests. 

And I just realised the log messages above are coming from StorageProxy.getRangeSlice() where the message object is shared for all endpoints. The log messages from Thibaut show the same message ID on nodes 0.1 0.2 and 0.3.

My reading of the MessagingService and ResponseVerbHandler is that once a message with ID X has been received, if others are received with the same ID they will be ignored. Is that correct? If so this looks like a bug in the two methods above. ;;;","02/Feb/11 22:28;jbellis;bq. In StorageProxy.fetchRows() line 395 the digestMessage is shared for all non local digest requests. 

This is fixed in CASSANDRA-2094, but shouldn't break quorum for RF=3 (since a single digest is all we need)

bq. the log messages above are coming from StorageProxy.getRangeSlice() where the message object is shared for all endpoints

Aha, I didn't notice that.  I think this is our bug.  Patch attached.;;;","02/Feb/11 22:39;amorton;Out of interest, how about making the ExpiringMap check the return of NonBlockingHashMap.put() and assert it was the value passed in. May catch future problems. 
;;;","03/Feb/11 10:11;mconrad;Jonathan, I tried your patch and it fixes the problem. Thanks.;;;","03/Feb/11 12:59;tbritz;As Michel noted, this fixes the problem. Thanks :-);;;","03/Feb/11 16:02;jbellis;Thanks for helping track that down everyone!;;;","03/Feb/11 16:16;hudson;Integrated in Cassandra-0.7 #237 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/237/])
    fix range slice ConsistencyLevel > ONE
patch by jbellis; tested by Michel Conrad and Thibaut for CASSANDRA-2081
;;;","03/Feb/11 16:24;hudson;Integrated in Cassandra-0.6 #56 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/56/])
    fix range slice ConsistencyLevel > ONE
patch by jbellis; tested by Michel Conrad and Thibaut for CASSANDRA-2081
;;;","03/Feb/11 16:37;hudson;Integrated in Cassandra-0.7 #238 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/238/])
    add debug log message for missing callback
patch by Aaron Morton and jbellis for CASSANDRA-2081
;;;","07/Feb/11 12:48;amorton;StorageProxy.scan() is also reusing Message objects, with the same result of requests at QUORUM timing out for get_indexed_slice()

Log messages below show a get_indexed_slice() in a 3 node cluster @ QUORUM. The new log message shows the callback has already been removed. 



DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,893 CassandraServer.java (line 509) scan
DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,894 StorageProxy.java (line 703) restricted ranges for query [-1,-1] are [[-1,85070591730234615865843651857942052864], (85070591730234615865843651857942052864,95070591730234615865843651857942052864], (95070591730234615865843651857942052864,108074891939685041992920030907211891412], (108074891939685041992920030907211891412,-1]]
DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,919 StorageProxy.java (line 796) scan ranges are [-1,85070591730234615865843651857942052864],(85070591730234615865843651857942052864,95070591730234615865843651857942052864],(95070591730234615865843651857942052864,108074891939685041992920030907211891412],(108074891939685041992920030907211891412,-1]
DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,919 ReadCallback.java (line 58) ReadCallback blocking for 2 responses
DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,923 StorageProxy.java (line 819) reading org.apache.cassandra.db.IndexScanCommand@26a1b248 from 6289@/127.0.0.3
DEBUG [pool-1-thread-12] 2011-02-08 01:46:32,923 StorageProxy.java (line 819) reading org.apache.cassandra.db.IndexScanCommand@26a1b248 from 6289@/127.0.0.2
DEBUG [RequestResponseStage:1] 2011-02-08 01:46:32,926 ResponseVerbHandler.java (line 51) Processing response on a callback from 6289@/127.0.0.3
DEBUG [RequestResponseStage:2] 2011-02-08 01:46:32,928 ResponseVerbHandler.java (line 41) Callback already removed for 6289
;;;","07/Feb/11 12:48;amorton;Am trying to fix this now. ;;;","07/Feb/11 13:17;amorton;patch 2081-2.txt is against the current 0.7 branch. I've added an assert in the MessageService that looks like this when a callback is added for an existing message ID. 

RROR [pool-1-thread-1] 2011-02-08 02:07:46,763 Cassandra.java (line 2918) Internal error processing get_indexed_slices
java.lang.AssertionError
	at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:262)
	at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:278)
	at org.apache.cassandra.service.StorageProxy.scan(StorageProxy.java:817)
	at org.apache.cassandra.thrift.CassandraServer.get_indexed_slices(CassandraServer.java:520)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_indexed_slices.process(Cassandra.java:2910)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)


Have also modified StorageProxy.scan() to create a message for each endpoint. 

I've read through StorageProxy and think their might be an problem in .sendMessages() line 236, it looks like it sends the same message to all endpoints in the local DC. It's late/early and I'm not sure so can someone else take a look please. 
;;;","07/Feb/11 14:47;hudson;Integrated in Cassandra-0.7 #252 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/252/])
    avoid re-using Message object in index queries
patch by Aaron Morton; reviewed by jbellis for CASSANDRA-2081
;;;","07/Feb/11 14:48;jbellis;bq. it looks like it sends the same message to all endpoints in the local DC

It sends the same message to each entry in the list...  which is always going to be a list of size one.  So the good news is I don't think it's incorrect, but the bad news is we broke CASSANDRA-1530.;;;","07/Feb/11 16:06;jbellis;will re-open 1530;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not restarting due to Invalid saved cache,CASSANDRA-2076,12497183,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,mdennis,tbritz,tbritz,31/Jan/11 09:15,16/Apr/19 09:33,14/Jul/23 05:52,10/Feb/11 03:29,0.7.1,,,,,,0,,,,"This occured on two nodes on me (running 0.7.1 from svn)

One node was killed by the kernel due to a OOM and the other node was haning and I had to kill it manually with kill -9 (kill didn't work). (maybe these were faulty hardware nodes, I don't know)

The saved_cache was corrupt afterwards and I couldn't start the nodes. 

After deleting the saved_caches directory I could start the nodes again. 

Instead of not starting when an error occurs, cassandra could simply delete the errornous file and continue to start?




 INFO 22:31:11,570 reading saved cache
/hd1/cassandra_md5/saved_caches/table_attributes-table_attributes-KeyCache
ERROR 22:31:11,595 Exception encountered during startup.
java.lang.RuntimeException: The provided key was not UTF8 encoded.
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
       at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:281)
       at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:218)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:458)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:440)
       at org.apache.cassandra.db.Table.initCf(Table.java:360)
       at org.apache.cassandra.db.Table.<init>(Table.java:290)
       at org.apache.cassandra.db.Table.open(Table.java:107)
       at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:167)
       at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:312)
       at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:81)
Caused by: java.nio.charset.MalformedInputException: Input length = 1
       at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
       at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
       at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
       ... 11 more
Exception encountered during startup.
java.lang.RuntimeException: The provided key was not UTF8 encoded.
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
       at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:281)
       at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:218)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:458)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:440)
       at org.apache.cassandra.db.Table.initCf(Table.java:360)
       at org.apache.cassandra.db.Table.<init>(Table.java:290)
       at org.apache.cassandra.db.Table.open(Table.java:107)
       at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:167)
       at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:312)
       at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:81)
Caused by: java.nio.charset.MalformedInputException: Input length = 1
       at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
       at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
       at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
       ... 11 more",linux,cburroughs,tbritz,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"08/Feb/11 23:19;mdennis;2076-cassandra-0.7.txt;https://issues.apache.org/jira/secure/attachment/12470637/2076-cassandra-0.7.txt","05/Feb/11 13:56;jbellis;2076-v2.txt;https://issues.apache.org/jira/secure/attachment/12470361/2076-v2.txt",,,,,,,,,,,,,2.0,mdennis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20430,,,Thu Feb 10 04:34:54 UTC 2011,,,,,,,,,,"0|i0g96f:",92917,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,"31/Jan/11 15:01;tbritz;This might be related:

Two other nodes (still running) also show up the ""The provided key was not UTF8 encoded."" error in the log.

I have never seen this error in 0.7.0


ERROR [MutationStage:19] 2011-01-30 21:36:16,951 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
        at org.apache.cassandra.db.Table.apply(Table.java:406)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:190)
        at org.apache.cassandra.service.StorageProxy$2.runMayThrow(StorageProxy.java:288)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
        ... 8 more
ERROR [MutationStage:19] 2011-01-30 21:36:16,991 AbstractCassandraDaemon.java (line 119) Fatal exception in thread Thread[MutationStage:19,5,main]
java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
        at org.apache.cassandra.db.Table.apply(Table.java:406)
        at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:190)
        at org.apache.cassandra.service.StorageProxy$2.runMayThrow(StorageProxy.java:288)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
        ... 8 more
 WARN [ScheduledTasks:1] 2011-01-30 21:36:21,450 MessagingService.java (line 506) Dropped 8 MUTATION messages in the last 5000ms



;;;","31/Jan/11 15:22;tbritz;I brought down and restarted the entire cluster. (100 nodes, 5x20 nodes)

Every single node complains of an invalid file in the saved_cache directory.;;;","01/Feb/11 11:20;tbritz;This just happens if you kill the node with -9;;;","03/Feb/11 16:46;jbellis;The problem is that we're not cloning the buffer backing the key read from thrift, so we are hitting a problem similar to CASSANDRA-1743.  Matt is working on a fix.;;;","03/Feb/11 23:35;mdennis;attached patch (depends on CASSANDRA-2102) allows C* to start when the saved caches are invalid/corrupt.

CASSANDRA-2102 prevents the saved caches from becoming corrupt in the first place.

;;;","04/Feb/11 09:22;tbritz;I was running yesterday's version with the Consitency fix and getting a similar error messages while reading the commitlog at start. I had to delete the commitlog (data loss) to restart cassandra.

Is this also related to CASSANDRA-2102 or shall I open a new bug report?


 INFO 23:50:00,922 Replaying /hd1/cassandra_md5/commitlog/CommitLog-1296731219671.log, /hd1/cassandra_md5/commitlog/CommitLog-1296759540149.log, /hd1/cassandra_md5/commitlog/CommitLog-1296760444366.log, /hd1/cassandra_md5/commitlog/CommitLog-1296761120546.log, /hd1/cassandra_md5/commitlog/CommitLog-1296762054192.log, /hd1/cassandra_md5/commitlog/CommitLog-1296773137101.log, /hd1/cassandra_md5/commitlog/CommitLog-1296773242671.log
 INFO 23:50:02,192 Finished reading /hd1/cassandra_md5/commitlog/CommitLog-1296731219671.log
ERROR 23:50:02,235 Fatal exception in thread Thread[MutationStage:7,5,main]
java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
        at org.apache.cassandra.db.Table.apply(Table.java:406)
        at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:294)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
        ... 10 more
 INFO 23:50:02,245 Finished reading /hd1/cassandra_md5/commitlog/CommitLog-1296759540149.log
ERROR 23:50:02,245 Exception encountered during startup.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:455)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:301)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:159)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:166)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:307)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:81)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:451)
        ... 5 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
        at org.apache.cassandra.db.Table.apply(Table.java:406)
        at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:294)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
        ... 10 more
Exception encountered during startup.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:455)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:301)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:159)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:166)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:307)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:81)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:451)
        ... 5 more
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException: The provided key was not UTF8 encoded.
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
        at org.apache.cassandra.db.Table.apply(Table.java:406)
        at org.apache.cassandra.db.commitlog.CommitLog$2.runMayThrow(CommitLog.java:294)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
Caused by: java.nio.charset.MalformedInputException: Input length = 1
        at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
        at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
        at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
        at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
        ... 10 more


;;;","04/Feb/11 15:23;jbellis;It's a different bug.

Did you have any nodes down for part of the time (any hinted handoff going on?);;;","04/Feb/11 15:32;jbellis;I reverted CASSANDRA-1743 which is what caused CASSANDRA-2102.  I suspect it's causing this other problem too for reasons not yet understood.;;;","04/Feb/11 17:41;tbritz;I can't say for sure. I guess so.

Can you trigger a hudson build with the most recent fixes as well. (Don't know for sure which revision to take) So I can let it run over the weekend and check if any of the exceptions and bugs (these ones, cpu spike, consistency...) occurs again or not.


;;;","04/Feb/11 19:41;jbellis;build scheduled;;;","05/Feb/11 03:03;mdennis;the commit logs have checksums in them to attempt to catch corrupted files.  C* is supposed to log that it is corrupted, skip over it and then continue.  The fact that *any* value inside a commit log can cause C* not to start is a bug of it's own.  I've opened CASSANDRA-2113 to track this.;;;","05/Feb/11 13:56;jbellis;v2 is a less-invasive fix to the original problem.;;;","08/Feb/11 23:19;mdennis;I like the first patch better (recently rebased) as it adds a version field at the top and operates on an actual count instead of using in.available() which is not actually reliable in all cases.

In addition the rebase fixes a recently introduced bug that prevented the caches from getting saved where BufferedRandomAccessFile doesn't accept ""w"" as a mode and adds some comments in CacheWriter about how the totalBytes reported via ICompactionInfo is just an approximation.

If we don't want these other changes, +1 on the v2 patch.;;;","10/Feb/11 03:29;jbellis;I'm not excited about the enumerated cache format change, because the benefit is nominal (if available() is wrong, then the cache is corrupt and we'll find out soon enough; knowing how many their SHOULD have been is of purely theoretical interest at that point) while the cost is making the cache useless for upgrades, which is the reason it was added in the first place.

Committed w/o the format change, but with the change to getTotalBytes to take the max of estimated/written.;;;","10/Feb/11 04:34;hudson;Integrated in Cassandra-0.7 #273 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/273/])
    continue starting when invalid savedcache entries are encountered
patch by mdennis and jbellis for CASSANDRA-2076
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming occasionally makes gossip back up,CASSANDRA-2073,12497092,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,brandon.williams,brandon.williams,28/Jan/11 20:47,16/Apr/19 09:33,14/Jul/23 05:52,31/Jan/11 19:04,0.7.1,,,,,,0,,,,"Streaming occasionally makes gossip back up, causing nodes to mark each other as down even though the network is ok.  This appears to happen just after streaming has finished.  I noticed this in the course of working on CASSANDRA-2072, so decommission is one way to reproduce.  It seems to happen maybe one of fifteen or twenty tries, so it's fairly rare.",,gdusbabek,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"29/Jan/11 00:55;jbellis;2073.txt;https://issues.apache.org/jira/secure/attachment/12469720/2073.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20428,,,Mon Jan 31 19:04:16 UTC 2011,,,,,,,,,,"0|i0g95r:",92914,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"28/Jan/11 21:21;gdusbabek;I see this regularly during repair operations.;;;","28/Jan/11 21:24;brandon.williams;I'm fairly certain this is the true cause of CASSANDRA-1730 and other ""gossip took longer than RING_DELAY"" bugs we've had.;;;","28/Jan/11 21:28;jbellis;bq. Streaming occasionally makes gossip back up

On the StreamOut [source] side, the StreamIn [receiver] side, or both?;;;","28/Jan/11 21:40;brandon.williams;Appears to be the receiver.  I just repro'd it with decom.  Nodes A, B, and C.  Decom B, streams to A and C complete, and afterwards A and C cannot gossip to each other for approximately 40s or so.  B did get the usual exception:

{noformat}
ERROR [Thread-6] 2011-01-28 21:23:08,720 AbstractCassandraDaemon.java (line 119) Fatal exception in thread Thread[Thread-6,5,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:62)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:387)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:91)
{noformat}

(I had CASSANDRA-2072 applied to avoid other problems) and I don't see any message about streaming completing on it, though A and C show StreamInSessions finishing right before the gossip outage.;;;","28/Jan/11 21:41;brandon.williams;tpstats on both nodes showed one active gossip task and a bunch pending.;;;","28/Jan/11 21:51;jbellis;i wonder if we have a Thread.sleep(RING_DELAY) in code that gets called on the gossip path.

jstack during the pause?;;;","28/Jan/11 22:09;brandon.williams;It looks like in my case, both nodes are stuck in the same spot:

{noformat}

""GossipStage:1"" prio=10 tid=0x0000000041644000 nid=0xece waiting on condition [0x00007fdbe861f000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  <0x00007fdc5ecf8828> (a java.util.concurrent.FutureTask$Sync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)
    at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:218)
    at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    at org.apache.cassandra.db.HintedHandOffManager.deleteHintsForEndPoint(HintedHandOffManager.java:156)
    at org.apache.cassandra.service.StorageService.excise(StorageService.java:831)
    at org.apache.cassandra.service.StorageService.handleStateLeft(StorageService.java:787)
    at org.apache.cassandra.service.StorageService.onChange(StorageService.java:645)
    at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:742)
    at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:732)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:649)
    at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:68)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:70)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}

They are waiting on deleting hints, and then likely for the hints CF to compact.;;;","28/Jan/11 22:15;jbellis;Bingo.  I don't see any reason why we'd want that to be blocking, do you?

If not we can just remove the .get().;;;","28/Jan/11 22:16;jbellis;... however, we DO want the flush before compaction to be blocking.

possibly kicking the whole operation out to the StorageService task queue is the best move.

(in which case the .get() is a no-op and can still be removed.);;;","28/Jan/11 23:56;brandon.williams;I don't see any reason why it should block, no.  Also I can't repro with repair on a real cluster, so I think this is the only bug.;;;","29/Jan/11 00:55;jbellis;attached;;;","31/Jan/11 15:26;gdusbabek;+1;;;","31/Jan/11 19:04;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition during decommission,CASSANDRA-2072,12497003,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,28/Jan/11 00:06,16/Apr/19 09:33,14/Jul/23 05:52,03/Feb/11 20:17,0.7.1,,,,,,0,,,,"Occasionally when decommissioning a node, there is a race condition that occurs where another node will never remove the token and thus propagate it again with a state of down.  With CASSANDRA-1900 we can solve this, but it shouldn't occur in the first place.

Given nodes A, B, and C, if you decommission B it will stream to A and C.  When complete, B will decommission and receive this stacktrace:

ERROR 00:02:40,282 Fatal exception in thread Thread[Thread-5,5,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:62)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:387)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:91

At this point A will show it is removing B's token, but C will not and instead its failure detector will report that B is dead, and nodetool ring on C shows B in a leaving/down state.  In another gossip round, C will propagate this state back to A.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/11 20:28;brandon.williams;0001-announce-having-left-the-ring-for-RING_DELAY-on-deco.patch;https://issues.apache.org/jira/secure/attachment/12469696/0001-announce-having-left-the-ring-for-RING_DELAY-on-deco.patch","28/Jan/11 20:28;brandon.williams;0002-Improve-TRACE-logging-for-Gossiper.patch;https://issues.apache.org/jira/secure/attachment/12469697/0002-Improve-TRACE-logging-for-Gossiper.patch","31/Jan/11 19:44;brandon.williams;0003-Remove-endpoint-state-when-expiring-justRemovedEndpo.patch;https://issues.apache.org/jira/secure/attachment/12469851/0003-Remove-endpoint-state-when-expiring-justRemovedEndpo.patch",,,,,,,,,,,,3.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20427,,,Thu Feb 03 20:47:49 UTC 2011,,,,,,,,,,"0|i0g95j:",92913,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"28/Jan/11 20:28;brandon.williams;Here is what is happening:

B sends LEFT to C, C calls removeEndpoint and drops the endpoint state.  B never gets to send to A (because it only waits 2s to announce, which can be just one round) and A still thinks it's LEAVING.  C sees B in a gossip digest from A, and  not knowing anything about it, calls requestAll, but A refuses to tell C anything about it because A has B in justRemovedEndpoints.  Eventually, QUARANTINE_DELAY expires and A unhelpfully propagates the LEAVING state back to C.

The obvious solution here is that B should announce LEFT for RING_DELAY, simply because it's the right thing to do as opposed to a one-off delay of 2 seconds.

However, this exposes a more subtle problem.  When removeEndpoint is called, we drop the state right away and track the endpoint in justRemovedEndpoints.  Instead, we should hold on to the state so it is still propagated in further gossip digests, and expire it when we expire justRemovedEndpoints.

Either of these changes is technically enough to solve this issue, but both together add an extra safeguard.  Changing where we expire the endpoint state is the more impacting of the two, however the gossip generation and version checks always prevent any negative consequences from doing this.;;;","03/Feb/11 18:15;gdusbabek;+1;;;","03/Feb/11 20:17;brandon.williams;Committed.;;;","03/Feb/11 20:47;hudson;Integrated in Cassandra-0.7 #244 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/244/])
    Fix race condition during decommission by announcing for RING_DELAY and
not removing endpoint state until removing the ep from
justRemovedEndpoints.
Patch by brandonwilliams, reviewed by gdusbabek for CASSANDRA-2072
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RP.describeOwnership() does some bad math,CASSANDRA-2071,12496999,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jhermes,jhermes,jhermes,27/Jan/11 23:23,16/Apr/19 09:33,14/Jul/23 05:52,27/Jan/11 23:36,0.7.1,,,,,,0,,,,"If the input isn't sorted correctly for some reason, then describeOwnership() fails to calculate the ownership %ages correctly.

Repro is 2 nodes with these tokens, you get these fractions:
49000620740128447720217646403197156812 : 0.7615167
770141183460469231731687303715884105727 : 4.2384834

423% ownership is obviously broken.",,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"27/Jan/11 23:29;jhermes;2071.txt;https://issues.apache.org/jira/secure/attachment/12469610/2071.txt",,,,,,,,,,,,,,1.0,jhermes,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20426,,,Thu Jan 27 23:54:35 UTC 2011,,,,,,,,,,"0|i0g95b:",92912,,,,,Low,,,,,,,,,,,,,,,,,"27/Jan/11 23:26;jhermes;Fixes the math by using a closed form in all cases.

Now returns:
49000620740128447720217646403197156812 : 0.7615167
770141183460469231731687303715884105727 : 0.2384833
;;;","27/Jan/11 23:29;jhermes;Whoops, wrong file.;;;","27/Jan/11 23:36;brandon.williams;Committed.;;;","27/Jan/11 23:54;hudson;Integrated in Cassandra-0.7 #223 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/223/])
    Fix RP.describeOwnership()
Patch by Jon Hermes, reviewed by brandonwilliams for CASSANDRA-2071
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read repair causes tremendous GC pressure,CASSANDRA-2069,12496983,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,27/Jan/11 21:14,16/Apr/19 09:33,14/Jul/23 05:52,15/Feb/11 17:15,0.7.3,,,,,,0,,,,"To reproduce: start a three node cluster, insert 1M rows with stress.java and rf=2.  Take one down, delete its data, then bring it back up and issue 1M reads against it.  After the run is done you will see at least 1 STW long enough to mark the node as dead, often 4 or 5.",,mdennis,,,,,,,,,,,,,,,,,,,,,57600,57600,,0%,57600,57600,,,,,,,,,,,,,,,,"14/Feb/11 22:44;jbellis;2069-v10.txt;https://issues.apache.org/jira/secure/attachment/12471030/2069-v10.txt","04/Feb/11 06:04;jbellis;2069-v2.txt;https://issues.apache.org/jira/secure/attachment/12470216/2069-v2.txt","04/Feb/11 15:38;jbellis;2069-v3.txt;https://issues.apache.org/jira/secure/attachment/12470239/2069-v3.txt","04/Feb/11 19:37;jbellis;2069-v4.txt;https://issues.apache.org/jira/secure/attachment/12470271/2069-v4.txt","04/Feb/11 21:22;jbellis;2069-v5.txt;https://issues.apache.org/jira/secure/attachment/12470280/2069-v5.txt","07/Feb/11 17:09;jbellis;2069-v6.txt;https://issues.apache.org/jira/secure/attachment/12470472/2069-v6.txt","07/Feb/11 23:48;jbellis;2069-v7.txt;https://issues.apache.org/jira/secure/attachment/12470525/2069-v7.txt","08/Feb/11 05:02;jbellis;2069-v8.txt;https://issues.apache.org/jira/secure/attachment/12470544/2069-v8.txt","08/Feb/11 19:11;jbellis;2069-v9.txt;https://issues.apache.org/jira/secure/attachment/12470615/2069-v9.txt","04/Feb/11 02:14;jbellis;2069.txt;https://issues.apache.org/jira/secure/attachment/12470212/2069.txt",,,,,10.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20425,,,Tue Feb 15 21:43:50 UTC 2011,,,,,,,,,,"0|i0g94v:",92910,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"27/Jan/11 22:08;jbellis;is this also true for 0.6?;;;","27/Jan/11 22:31;brandon.williams;No, only 0.7;;;","28/Jan/11 16:16;jbellis;In the log snippets I saw the heap was legitimately nearly-full during the repair process.  What does MAT show is using all the memory?;;;","03/Feb/11 22:35;brandon.williams;MAT indicates there are millions of pending futuretasks in StorageProxy.repairExecutor.;;;","04/Feb/11 00:05;brandon.williams;Bisecting this indicates two problems.  First, the tremendous GC pressure doesn't exhibit until we increase the newgen size.  If that is ignored and we only observe whether stress.java moves forward (it has the 'keep trying' rather than 'keep going' behavior) then the culprit is r1053245.

Both cases show that RR requests can grow unbounded, which is probably ok, but if it's the same request over and over we still queue it as many times as it's requested.;;;","04/Feb/11 00:44;mdennis;I've observed similar issues with large a large newgen, largish memtables and writing at RF>1 at CL.Q/CL.ALL though it's not as immediate as it is with RR.

We should look into a way at bounding the number of outstanding requests (RR or ""send to replica""), or more aggressively timing out the older requests once we hit some configurable threshold of outstanding requests.
;;;","04/Feb/11 02:14;jbellis;Attached patch does two things:

- allows the repair executor to use threads = # of cores
- stops sending repair requests when the executor queue gets too large.  (""dropped"" requests will be logged by MessagingService along with the other overload-scenario dropping.);;;","04/Feb/11 02:36;jbellis;Working on another approach that should let us throw away repair handlers in the expected case that everyone responds quickly.  This will be kinder to new gen gc since we won't have nearly as many survivors.;;;","04/Feb/11 06:04;jbellis;v2 adds a READ_REPAIR stage and	does resolve of digests that were not checked for the client result on that stage as soon as response() collects all the replies.	If there is a mismatch and we do a re-read of full	resultset, we also check those results on the RR stage based on response() (in AsyncRepairRunner, now in ReadCallback.)

I preserved the	feature	from v1	of not doing repairs if	the RR stage is	full.

Most of the code changes are about getting the right information into ReadCallback (e.g. endpoints) and some ceremony to make static typing happy (IReadCallback).

A side benefit is that StorageProxy.fetchRows is significantly cleaner (no more commandEndpoints or  repairs collections).
;;;","04/Feb/11 15:38;jbellis;bq. I preserved the feature from v1 of not doing repairs if the RR stage is full.

Removed this for v3. It's complexity we don't need to solve a non-problem, when we're not hanging on to each callback until RPC_TIMEOUT (as demonstrated by 0.6).;;;","04/Feb/11 19:37;jbellis;v4 fixes build;;;","04/Feb/11 21:22;jbellis;v5 fixes a confusion of endpoints and handler.endpoints;;;","04/Feb/11 23:20;brandon.williams;No GC pressure now, however RR does not appear to actually occur.;;;","05/Feb/11 01:20;jbellis;what does debug log show?;;;","05/Feb/11 01:27;brandon.williams;Nothing interesting, afaict.  Just a lot of this:
{noformat}
DEBUG 23:19:42,094 get_slice
DEBUG 23:19:42,094 get_slice
DEBUG 23:19:42,240 ReadCallback blocking for 1 responses
DEBUG 23:19:42,240 ReadCallback blocking for 1 responses
DEBUG 23:19:42,093 ReadCallback blocking for 1 responses
DEBUG 23:19:42,092 reading data for SliceFromReadCommand(table='Keyspace1', key='30343939323034', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,091 Read: 21 ms.
DEBUG 23:19:42,091 LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='30353032333935', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG 23:19:42,091 Read: 17 ms.
DEBUG 23:19:42,241 Read: 159 ms.
DEBUG 23:19:42,091 Read: 12 ms.
DEBUG 23:19:42,091 reading data for SliceFromReadCommand(table='Keyspace1', key='30333636313035', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,091 LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='30343831383238', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG 23:19:42,091 get_slice
DEBUG 23:19:42,242 Read: 161 ms.
DEBUG 23:19:42,242 ReadCallback blocking for 1 responses
DEBUG 23:19:42,091 get_slice
DEBUG 23:19:42,090 Read: 17 ms.
DEBUG 23:19:42,243 ReadCallback blocking for 1 responses
DEBUG 23:19:42,090 Read: 19 ms.
DEBUG 23:19:42,090 reading data for SliceFromReadCommand(table='Keyspace1', key='30353131363631', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,089 ReadCallback blocking for 1 responses
DEBUG 23:19:42,243 reading data for SliceFromReadCommand(table='Keyspace1', key='30353532343833', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,244 LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='30353532343833', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG 23:19:42,244 Read: 1 ms.
DEBUG 23:19:42,243 reading data for SliceFromReadCommand(table='Keyspace1', key='30363137363033', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,242 reading data for SliceFromReadCommand(table='Keyspace1', key='30363234303332', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5) locally
DEBUG 23:19:42,242 LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='30333636313035', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG 23:19:42,241 LocalReadRunnable reading SliceFromReadCommand(table='Keyspace1', key='30343939323034', column_parent='QueryPath(columnFamilyName='Standard1', superColumnName='null', columnName='null')', start='', finish='', reversed=false, count=5)
DEBUG 23:19:42,245 Read: 3 ms.
{noformat};;;","07/Feb/11 17:09;jbellis;v6 adds the new repair callbacks to DES latency tracking.;;;","07/Feb/11 23:48;jbellis;v7 rebases post-1530.;;;","08/Feb/11 05:02;jbellis;bq. RR does not appear to actually occur

No digest reads are being sent but I don't know why.  v8 adds better debug logging around ReadCallback.endpoints creation which governs that.;;;","08/Feb/11 19:11;jbellis;v9 fixes ReadCallback construction and adds more asserts around AsyncRepairRunner.;;;","10/Feb/11 20:35;jbellis;v10 fixes a race where a quick repair would remove the data needed for the response to client.  it also splits repair and digest-processing resolvers into different classes.;;;","14/Feb/11 22:44;jbellis;rebased v10;;;","14/Feb/11 23:45;brandon.williams;+1, RR works, GC pressure is gone.;;;","15/Feb/11 17:15;jbellis;committed;;;","15/Feb/11 21:43;hudson;Integrated in Cassandra-0.7 #280 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/280/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
refactor o.a.c.utils.UUIDGen to allow creating type 1 UUIDs for a given time,CASSANDRA-2067,12496972,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,urandom,urandom,27/Jan/11 19:16,16/Apr/19 09:33,14/Jul/23 05:52,29/Jan/11 17:27,0.8 beta 1,,,,,,0,,,,"CASSANDRA-2027 creates the need to generate type 1 UUIDs using arbitrary date/times.  IMO, this would be a good opportunity to replace o.a.c.utils.UUIDGen with the class that Gary Dusbabek wrote for Flewton (https://github.com/flewton/flewton/blob/master/src/com/rackspace/flewton/util/UUIDGen.java), which is better/more comprehensive.  We can even eliminate the dependency on JUG.

Patches to follow.",,,,,,,,,,,,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,"27/Jan/11 19:27;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2067-o.a.c.utils.UUIDGen-adapted-from-flewto.txt;https://issues.apache.org/jira/secure/attachment/12469581/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2067-o.a.c.utils.UUIDGen-adapted-from-flewto.txt","27/Jan/11 19:27;urandom;ASF.LICENSE.NOT.GRANTED--v1-0002-eliminate-usage-of-JUG-for-UUIDs.txt;https://issues.apache.org/jira/secure/attachment/12469582/ASF.LICENSE.NOT.GRANTED--v1-0002-eliminate-usage-of-JUG-for-UUIDs.txt","27/Jan/11 19:27;urandom;ASF.LICENSE.NOT.GRANTED--v1-0003-remove-JUG-jar-and-references.txt;https://issues.apache.org/jira/secure/attachment/12469583/ASF.LICENSE.NOT.GRANTED--v1-0003-remove-JUG-jar-and-references.txt","29/Jan/11 05:05;urandom;ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2067-o.a.c.utils.UUIDGen-adapted-from-flewto.txt;https://issues.apache.org/jira/secure/attachment/12469732/ASF.LICENSE.NOT.GRANTED--v2-0001-CASSANDRA-2067-o.a.c.utils.UUIDGen-adapted-from-flewto.txt","29/Jan/11 05:05;urandom;ASF.LICENSE.NOT.GRANTED--v2-0002-eliminate-usage-of-JUG-for-UUIDs.txt;https://issues.apache.org/jira/secure/attachment/12469733/ASF.LICENSE.NOT.GRANTED--v2-0002-eliminate-usage-of-JUG-for-UUIDs.txt","29/Jan/11 05:05;urandom;ASF.LICENSE.NOT.GRANTED--v2-0003-remove-JUG-jar-and-license-files.txt;https://issues.apache.org/jira/secure/attachment/12469734/ASF.LICENSE.NOT.GRANTED--v2-0003-remove-JUG-jar-and-license-files.txt",,,,,,,,,6.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20424,,,Mon Jan 31 22:20:48 UTC 2011,,,,,,,,,,"0|i0g94f:",92908,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"27/Jan/11 19:23;urandom;The patches have followed.;;;","28/Jan/11 16:04;jbellis;- message digest instantiation is expensive and synchronized, so we made a threadlocal for that in FBUtilities.  Should probably use that.
- I don't understand the purpose of clockOffsetTicker.  it's only used during the constructor, but constructor is only used once so clockOffsetTicker has no effect.
- i'd be happier if we used an AtomicLong instead of synchronization to protect lastNanos.  (look at java Random class for an example of using AtomicLong for a similar purpose);;;","29/Jan/11 05:26;urandom;bq. message digest instantiation is expensive and synchronized, so we made a threadlocal for that in FBUtilities. Should probably use that.

done

bq. I don't understand the purpose of clockOffsetTicker. it's only used during the constructor, but constructor is only used once so clockOffsetTicker has no effect.

it was a throw-back from an earlier iteration. removed.

bq. i'd be happier if we used an AtomicLong instead of synchronization to protect lastNanos. (look at java Random class for an example of using AtomicLong for a similar purpose)

I don't think this will work.  Each UUID is supposed to have a higher value than the previous (even with identical timestamps), and this would allow racing threads to get out of order.  For what it's worth, JUG's UUIDGenerator was synchronized in the same way here.

----

Also included in this most recent patchset is caching to makeNode() so that addresses only need to be hashed once.;;;","29/Jan/11 16:46;jbellis;nits:

- nodecache can be final
- makeNode is only called from instance methods and should be non-static (dropping references to static ""instance"" field)

otherwise, +1;;;","29/Jan/11 17:27;urandom;committed (free of nits).;;;","29/Jan/11 18:22;hudson;Integrated in Cassandra #700 (See [https://hudson.apache.org/hudson/job/Cassandra/700/])
    remove JUG jar and license files

Patch by eevans for CASSANDRA-2067
eliminate usage of JUG for UUIDs

Patch by eevans for CASSANDRA-2067
CASSANDRA-2067 o.a.c.utils.UUIDGen adapted from flewton

Patch by eevans for CASSANDRA-2067
;;;","31/Jan/11 22:20;messi;You could also use the Preferences system to store a random permanent node ID.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2 (more) Misuses of ByteBuffer relative gets,CASSANDRA-2066,12496957,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,27/Jan/11 16:28,16/Apr/19 09:33,14/Jul/23 05:52,27/Jan/11 16:37,0.7.1,,,,,,0,,,,In RandomPartitioner and SerDeUtils,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"27/Jan/11 16:31;slebresne;0001-Fix-2-misuses-of-ByteBuffer-relative-gets.patch;https://issues.apache.org/jira/secure/attachment/12469560/0001-Fix-2-misuses-of-ByteBuffer-relative-gets.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20423,,,Thu Jan 27 16:54:33 UTC 2011,,,,,,,,,,"0|i0g947:",92907,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"27/Jan/11 16:31;slebresne;Patch attached agains 0.7 branch.
I include a slight optimisation of ByteBufferUtil.getArray() too, that
will avoid a copy at least in some current use of 
RandomPartitionner.factory().fromByteArray() (and possibly other places);;;","27/Jan/11 16:37;jbellis;committed;;;","27/Jan/11 16:54;hudson;Integrated in Cassandra-0.7 #220 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/220/])
    fix possibleByteBuffer race conditions
patch by slebresne; reviewed by jbellis for CASSANDRA-2066
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bug with test,CASSANDRA-2063,12496928,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,amit71,amit71,27/Jan/11 11:34,16/Apr/19 09:33,14/Jul/23 05:52,27/Jan/11 22:44,0.7.1,,,,,,0,,,,"when executing nosetests (e.g: nosetests test/system/test_avro_system.py), you get the following error:

    mod = load_module(part_fqname, fh, filename, desc)
  File ""/tmp/apache-cassandra-0.7.0-src/test/system/test_avro_system.py"", line 19
    from . import AvroTester
         ^
SyntaxError: invalid syntax

All *.py scripts should be changed to be ""from __init__ import (AvroTester)""    instead of ""from . import AvroTester""


",RHL. Python 2.4.3,,,,,,,,,,,,,,,,,,,,,,600,600,,0%,600,600,,,,,,,,,,,,,,,,"27/Jan/11 21:27;urandom;ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2063-Python2.4-friendly-imports.txt;https://issues.apache.org/jira/secure/attachment/12469599/ASF.LICENSE.NOT.GRANTED--v1-0001-CASSANDRA-2063-Python2.4-friendly-imports.txt",,,,,,,,,,,,,,1.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20422,,,Fri Jan 28 08:49:46 UTC 2011,,,,,,,,,,"0|i0g93j:",92904,,,,,Low,,,,,,,,,,,,,,,,,"27/Jan/11 22:06;jbellis;i'm kind of surprised that import is the only thing keeping 2.4 from running, but +1;;;","27/Jan/11 22:44;urandom;Yeah, same here.  Maybe OP didn't make it past this (I don't have 2.4 handy to test with).  Anyway, committed.;;;","27/Jan/11 23:04;hudson;Integrated in Cassandra-0.7 #222 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/222/])
    CASSANDRA-2063 Python2.4-friendly imports

Patch by eevans for CASSANDRA-2063
;;;","28/Jan/11 08:49;amit71;why is it surprising you?
when you do the test, you will face one more issue. i dont remember where
exactly, but in one of the parameter declaration there is an extra 'b'
outside the value (instad of ""a='value'"" its written ""a= b'value' "".
anyway, i suggest also to add script that will be executed in the beginning
of each and every test that will do the following:
1)  delete the DB.
2) kill the existed instance of Cassandra in case there is a file of
existing Casandra in root diretory.
3) delete the existing file that prevent the test from being ran.

I can tell u that for me it was really confusing, especially that not much
documentation is written on it.
Cheers,
Amit


;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing logging for some exceptions,CASSANDRA-2061,12496895,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,stuhood,stuhood,27/Jan/11 04:29,16/Apr/19 09:33,14/Jul/23 05:52,16/Aug/11 20:53,1.0.0,,,,,,0,,,,"{quote}Since you are using ScheduledThreadPoolExecutor.schedule(), the exception was swallowed by the FutureTask.

You will have to perform a get() method on the ScheduledFuture, and you will get ExecutionException if there was any exception occured in run().{quote}",,cburroughs,,,,,,,,,,,,,,,,,,,,,28800,28800,,0%,28800,28800,,,,,,,,,,,,,,,,"28/Jan/11 15:32;jbellis;2061-0.7.txt;https://issues.apache.org/jira/secure/attachment/12469679/2061-0.7.txt","15/Aug/11 05:01;jbellis;2061-v3.txt;https://issues.apache.org/jira/secure/attachment/12490402/2061-v3.txt","28/Jan/11 15:33;jbellis;2061.txt;https://issues.apache.org/jira/secure/attachment/12469681/2061.txt",,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20421,,,Tue Aug 16 21:23:12 UTC 2011,,,,,,,,,,"0|i0g933:",92902,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"27/Jan/11 15:36;jbellis;As near as I can tell, an exception thrown on a scheduled task will never kill the executor, just like in TPE.  I don't remember why the author [me] wrote that code -- probably because it was replacing Timer and TimerTask, and an uncaught exception in a TimerTask _will_ kill the timer.

Patch removes RetryingSTPE and replaces with DebuggableSTPE that has an afterExecute copied from DTPE.;;;","27/Jan/11 15:44;jbellis;new patch also updates afterExecute in both classes to log error if default uncaught exception handler is null;;;","27/Jan/11 15:46;jbellis;patch for 0.7;;;","27/Jan/11 20:44;stuhood;* 2061.txt doesn't completely remove RetryingSTPE.java, and doesn't replace the usage in CFStore
* 2061-0.7.txt doesn't apply to the 0.7 branch

Also, will we need a separate patch for trunk?;;;","28/Jan/11 15:27;jbellis;bq. 2061.txt doesn't completely remove RetryingSTPE.java

that's just how svn diff works.

bq. and doesn't replace the usage in CFStore

fixed.
;;;","28/Jan/11 15:32;jbellis;bq. 2061-0.7.txt doesn't apply to the 0.7 branch

fixed.  also applies to trunk.;;;","31/Jan/11 05:03;stuhood;Based on anecdotal evidence (it exposed an exception I was expecting), this looks good. But it looks like we can probably merge Debuggable(Scheduled)ThreadPool... they are appear to be essentially identical now.;;;","31/Jan/11 05:09;jbellis;From the STPE javadoc, it sounds like STPE is more heavyweight than TPE and you don't want to use the former when all you need API-wise is the latter.  I have not done the code diving to confirm this though.;;;","31/Jan/11 05:12;jbellis;STPE also notes,

bq. While this class inherits from ThreadPoolExecutor, a few of the inherited tuning methods are not useful for it. In particular, because it acts as a fixed-sized pool using corePoolSize threads and an unbounded queue, adjustments to maximumPoolSize have no useful effect.

We've wanted bounded queues in the past, and we definitely still use growable pools in places, so that's another reason to keep both.;;;","03/Feb/11 17:42;jbellis;committed;;;","03/Feb/11 20:04;jbellis;reverted because of DynamicEndpointSnitchTest failure.  Not sure what is going on there -- I suspect some scheduled task is taking too long and keeping the DES update from happening, but why that should be affected by this patch is obscure to me.;;;","03/Feb/11 20:16;hudson;Integrated in Cassandra-0.7 #243 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/243/])
    ;;;","15/Aug/11 05:00;jbellis;Figured out the problem.  Here's the new version of logExceptionsAfterExecute that fixes it:

{code}
     public static void logExceptionsAfterExecute(Runnable r, Throwable t)
     {
-        // exceptions wrapped by FutureTask
-        if (r instanceof FutureTask<?>)
+        // Check for exceptions wrapped by FutureTask.  We do this by calling get(), which will
+        // cause it to throw any saved exception.
+        //
+        // Complicating things, calling get() on a ScheduledFutureTask will block until the task
+        // is cancelled.  Hence, the extra isDone check beforehand.
+        if ((r instanceof Future<?>) && ((Future<?>) r).isDone())
         {
             try
             {
-                ((FutureTask<?>) r).get();
+                ((Future<?>) r).get();
             }
{code};;;","15/Aug/11 05:01;jbellis;v3 attached for trunk.;;;","16/Aug/11 20:53;xedin;committed.;;;","16/Aug/11 21:23;hudson;Integrated in Cassandra #1027 (See [https://builds.apache.org/job/Cassandra/1027/])
    Fix missing logging for some exceptions
patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-2061

xedin : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1158439
Files : 
* /cassandra/trunk/src/java/org/apache/cassandra/service/StorageService.java
* /cassandra/trunk/src/java/org/apache/cassandra/concurrent/RetryingScheduledThreadPoolExecutor.java
* /cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableScheduledThreadPoolExecutor.java
* /cassandra/trunk/CHANGES.txt
* /cassandra/trunk/src/java/org/apache/cassandra/gms/Gossiper.java
* /cassandra/trunk/test/unit/org/apache/cassandra/locator/DynamicEndpointSnitchTest.java
* /cassandra/trunk/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableDeletingReference only deletes data files,CASSANDRA-2059,12496836,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,26/Jan/11 15:28,16/Apr/19 09:33,14/Jul/23 05:52,28/Jan/11 00:09,0.7.1,,,,,,0,,,,"Ching-Cheng Chen reports on the mailing list:
	

In SSTableDeletingReference, it try this operation

components.remove(Component.DATA);

before

STable.delete(desc, components);

However, the components was reference to the components object which was created inside SSTable by

this.components = Collections.unmodifiableSet(dataComponents);

As you can see, you can't try the remove operation on that components object.",,cburroughs,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"26/Jan/11 15:48;jbellis;2059.txt;https://issues.apache.org/jira/secure/attachment/12469428/2059.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20420,,,Fri Jan 28 00:46:26 UTC 2011,,,,,,,,,,"0|i0g92n:",92900,,stuhood,,stuhood,Normal,,,,,,,,,,,,,,,,,"26/Jan/11 15:48;jbellis;Patch to use Sets.difference instead of mutating components;;;","27/Jan/11 04:29;stuhood;+1

Do we have a separate issue to fix the lack-of-logged-exception problem? Opened CASSANDRA-2061... might be a dupe.;;;","28/Jan/11 00:09;jbellis;committed;;;","28/Jan/11 00:46;hudson;Integrated in Cassandra-0.7 #224 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/224/])
    fix deletionof sstable non-data components
patch by jbellis; reviewed by stuhood for CASSANDRA-2059
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load spikes due to MessagingService-generated garbage collection,CASSANDRA-2058,12496753,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,ketralnis,ketralnis,26/Jan/11 01:20,16/Apr/19 09:33,14/Jul/23 05:52,15/Feb/11 19:48,0.6.11,0.7.1,,,,,0,,,,"(Filing as a placeholder bug as I gather information.)

At ~10p 24 Jan, I upgraded our 20-node cluster from 0.6.8->0.6.10, turned on the DES, and moved some CFs from one KS into another (drain whole cluster, take it down, move files, change schema, put it back up). Since then, I've had four storms whereby a node's load will shoot to 700+ (400% CPU on a 4-cpu machine) and become totally unresponsive. After a moment or two like that, its neighbour dies too, and the failure cascades around the ring. Unfortunately because of the high load I'm not able to get into the machine to pull a thread dump to see wtf it's doing as it happens.

I've also had an issue where a single node spikes up to high load, but recovers. This may or may not be the same issue from which the nodes don't recover as above, but both are new behaviour","OpenJDK 64-Bit Server VM (build 1.6.0_0-b12, mixed mode)
Ubuntu 8.10
Linux pmc01 2.6.27-22-xen #1 SMP Fri Feb 20 23:58:13 UTC 2009 x86_64 GNU/Linux",bcoverston,brandon.williams,cburroughs,dkuebric,jhermes,mmalone,stuhood,,,,,,,,,,,,,,,1440,1440,,0%,1440,1440,,,,,,,,,,,,,,,,"27/Jan/11 18:30;brandon.williams;2058-0.7-v2.txt;https://issues.apache.org/jira/secure/attachment/12469574/2058-0.7-v2.txt","27/Jan/11 18:40;jbellis;2058-0.7-v3.txt;https://issues.apache.org/jira/secure/attachment/12469575/2058-0.7-v3.txt","27/Jan/11 04:06;jbellis;2058-0.7.txt;https://issues.apache.org/jira/secure/attachment/12469512/2058-0.7.txt","26/Jan/11 21:49;jbellis;2058.txt;https://issues.apache.org/jira/secure/attachment/12469482/2058.txt","26/Jan/11 01:29;ketralnis;cassandra.pmc01.log.bz2;https://issues.apache.org/jira/secure/attachment/12469368/cassandra.pmc01.log.bz2","26/Jan/11 03:38;ketralnis;cassandra.pmc14.log.bz2;https://issues.apache.org/jira/secure/attachment/12469380/cassandra.pmc14.log.bz2","26/Jan/11 03:30;ketralnis;graph a.png;https://issues.apache.org/jira/secure/attachment/12469378/graph+a.png","26/Jan/11 03:30;ketralnis;graph b.png;https://issues.apache.org/jira/secure/attachment/12469379/graph+b.png",,,,,,,8.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20419,,,Tue Feb 15 19:48:56 UTC 2011,,,,,,,,,,"0|i0g92f:",92899,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,"26/Jan/11 01:29;ketralnis;This is one of the affected nodes' logs from 7a..6p (uncompresses to ~33mb). Note that around 4p I added a job to pull a jstack every 120s. On this node around 5:46p I saw the version of the load spike where the node recovers (at around 17:53).;;;","26/Jan/11 01:30;ketralnis;It occurs to me that my timestamps may be in a different time zone than the logs themselves;;;","26/Jan/11 01:35;ketralnis;Also since then I've had notably worse performance, reading is maybe 30% slower than before.

My next step will be to hope that the jstacks in that log are the same as the ones causing the largest outages and to disable to dynamic snitch (as much as i'd like to get 100% reproduction, I'd also rather not take my site down) to see if that resolves the problem. If it doesn't, then I'll turn it back on and revert to 0.6.8 to see if that does it;;;","26/Jan/11 01:48;jbellis;I believe this is the same as CASSANDRA-2054 but will leave both open for now.;;;","26/Jan/11 03:30;ketralnis;Just had this happen again, attaching load/CPU graphs. Will have logs shortly.

I was in the middle of pushing out the change to turn off the DES. This is pmc14. As of when this happened, the nodes {pmc01 pmc04 pmc07 pmc10 pmc13 pmc16} had it turned off but the others have not been restarted;;;","26/Jan/11 05:10;jbellis;bq. If it doesn't, then I'll turn it back on and revert to 0.6.8 to see if that does it

You were running 0.6.8 + DS before?  Or is ""it"" not DynamicSnitch?;;;","26/Jan/11 06:48;ketralnis;bq. You were running 0.6.8 + DS before? Or is ""it"" not DynamicSnitch?

I was running 0.6.8 with no DES. Then I upgraded to 0.6.10 and turned it on. I had the aforementioned problems.

Now I'm running 0.6.10 with the DES turned off. (As of this writing, I'm still seeing the momentary spikes but thus far no sustained ones.)

If I continue to have the momentary or sustained spikes (I'll probably know by the morning), then I'll revert to 0.6.8, and turn *on* the DES.

If after that I continue to have problems I'll revert back to 0.6.8 with no DES, which is at least a configuration in which I didn't have any of these problems;;;","26/Jan/11 14:23;jbellis;DES in 0.6.8 is a no-op unless you're doing quorum reads.;;;","26/Jan/11 19:26;ketralnis;I am in fact still having both the momentary and the sustained failures and am rolling back to 0.6.8 with no DES (since you describe it as a no-op anyway);;;","26/Jan/11 21:22;ketralnis;I've rolled back to 0.6.8 with the DES disabled and not only has the load problem stopped, performance has also gone back up to previous levels;;;","26/Jan/11 21:49;jbellis;Brandon's testing has narrowed the culprit down to CASSANDRA-1959.  As discussed on CASSANDRA-2054, the main problem there is with the NonBlockingHashMap introduced to track timed out latencies.

This patch reverts that and takes a different approach, of tracking the latency in the callback map.  This means that we need a unique messageId for each target we send a message to.  The Right Way to do this would be to have Message objects only contain the data to send, not the From address and not the messageId.  Refactoring Message is outside our scope here though, so instead we create a new Message for each target.

This does let us clean up the callback map in ResponseVerbHandler instead of in each Callback.  (That is what is going on in the changes to QRH, WRH, and AR.);;;","27/Jan/11 03:49;tjake;This looks good overall, nothing major I can see.

The only niggles are:
 
1. ExpiringMap; we could do the same with MapMaker and may be more bulletproof. see EvictionListener http://guava-libraries.googlecode.com/svn/trunk/javadoc/com/google/common/collect/MapMaker.html

2. I also wonder what impact (if any) there will be for generating a message per endpoint rather than re-using the same one as was perviously done.

But as-is it's still +1;;;","27/Jan/11 03:57;jbellis;Thanks, Jake.

1. agreed, I'd like to upgrade at some point, but changing stuff i don't have to scares me at this point in 0.6.

2. we definitely saw a small speedup when i made that optimization the first time, but I'd rather have a working dynamic snitch.  (we can optimize later in 0.7 -- see The Right Way above.)  combined w/ the improved tcp performance in 0.6.10 we should still be ahead of 0.6.8 aka the last version that didn't have MessagingService bugs.;;;","27/Jan/11 04:06;jbellis;port to 0.7 attached.;;;","27/Jan/11 16:51;brandon.williams;0.6 version looks good, RR, HH, and DES work, no more CPU spikes under heavy load.;;;","27/Jan/11 16:56;jbellis;committed 0.6 version;;;","27/Jan/11 17:13;hudson;Integrated in Cassandra-0.6 #52 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/52/])
    reduce garbage generated by MessagingServiceto prevent loadspikes
patch by jbellis; reviewed by brandonwilliams and tjake for CASSANDRA-2058
;;;","27/Jan/11 18:30;brandon.williams;0.7 v2 fixes the DES by incorporating the approach from CASSANDRA-2004 and having it register with MS directly and removing ILP.  However, it does not receive timings for the local node.;;;","27/Jan/11 18:40;jbellis;v3 adds latency tracking to LocalReadRunnable;;;","27/Jan/11 22:25;brandon.williams;+1;;;","27/Jan/11 22:44;jbellis;committed to 0.7 and trunk;;;","27/Jan/11 23:48;mmalone;Jake/Jonathan,

FWIW, I re-implemented ExpiringMap with MapMaker using an eviction listener (but mostly maintaining the ExpiringMap API) a little while back while investigating some messaging service issues we were seeing. The patch is against 0.6.8, but here's the code if you wanna try it out: https://gist.github.com/a2f645c69ca8f44ccff3

It could definitely be simplified more by someone willing to make more widespread code changes. Actually, I think using MapMaker directly and getting rid of ExpiringMap would probably be best. *shrug*;;;","28/Jan/11 00:01;jbellis;bq. I think using MapMaker directly and getting rid of ExpiringMap would probably be best

Agreed, opened CASSANDRA-2070 for that;;;","31/Jan/11 21:14;ketralnis;I have upgraded to 0.6.11 and am definitely still seeing this problem (although I'm no longer seeing the 30% performance hit while the nodes are up);;;","31/Jan/11 21:51;jbellis;Please tell me you're at least seeing this less often than with .10 :);;;","01/Feb/11 00:07;ketralnis;It's hard to say. I lost 5 nodes in about an hour, but I don't know how many I lost last time;;;","01/Feb/11 15:58;pquerna;FYI, since upgrading to .10 we are also seeing this problem :(  Tried getting a jstack, but didn't work, tpstats etc all timed out.;;;","01/Feb/11 17:51;brandon.williams;Paul, I would expect to see it on .10 (I can repro there) but that is what this ticket was supposed to address.  Can you repro with .11?;;;","01/Feb/11 19:22;urandom;They're seeing it on r1064246 (one rev newer than 0.6.11).;;;","04/Feb/11 09:27;tbritz;I'm also seeing something similar on yesterday's svn version (the one with the Consistency level fix).

It only occurs if I enable JNA.

Nodes will experience enormous high kernel load (htop, red bar). Ssh sessions on these servers will lag extermely. Nodes won't take 100% cpu though, but the cluster is unusable.

(Just to note: it's a completely different pattern to the 100% cpu spike which occured before, and I can't reproduce it wihout JNA enabled)
;;;","04/Feb/11 18:02;ketralnis;I don't have JNA on these hosts, so at least in my case it's not JNA-related.;;;","04/Feb/11 20:20;brandon.williams;Could those who are seeing this issue please post the JVM flags they're using?;;;","04/Feb/11 21:08;ketralnis;JVM_OPTS="" \
        -ea \
        -Xms6656m \
        -Xmx6656m \
        -XX:+UseParNewGC \
        -XX:+UseConcMarkSweepGC \
        -XX:+CMSParallelRemarkEnabled \
        -XX:SurvivorRatio=8 \
        -XX:MaxTenuringThreshold=1 \
        -XX:CMSInitiatingOccupancyFraction=75 \
        -XX:+UseCMSInitiatingOccupancyOnly \
        -XX:+HeapDumpOnOutOfMemoryError \
        -XX:+UseThreadPriorities \
        -XX:ThreadPriorityPolicy=42 \
        -Dcassandra.compaction.priority=1 \
        -Dcom.sun.management.jmxremote.port=8080 \
        -Dcom.sun.management.jmxremote.ssl=false \
        -Dcom.sun.management.jmxremote.authenticate=false""


/usr/bin/java -ea -Xms6656m -Xmx6656m -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=1 -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Dcassandra.compaction.priority=1 -Dcom.sun.management.jmxremote.port=8080 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dstorage-config=bin/../conf -Dcassandra-foreground=yes -cp bin/../conf:bin/../build/classes:bin/../lib/antlr-3.1.3.jar:bin/../lib/clhm-production.jar:bin/../lib/commons-cli-1.1.jar:bin/../lib/commons-codec-1.2.jar:bin/../lib/commons-collections-3.2.1.jar:bin/../lib/commons-lang-2.4.jar:bin/../lib/google-collections-1.0.jar:bin/../lib/hadoop-core-0.20.1.jar:bin/../lib/high-scale-lib.jar:bin/../lib/jackson-core-asl-1.4.0.jar:bin/../lib/jackson-mapper-asl-1.4.0.jar:bin/../lib/jline-0.9.94.jar:bin/../lib/json-simple-1.1.jar:bin/../lib/libthrift-r917130.jar:bin/../lib/log4j-1.2.14.jar:bin/../lib/slf4j-api-1.5.8.jar:bin/../lib/slf4j-log4j12-1.5.8.jar org.apache.cassandra.thrift.CassandraDaemon


java version ""1.6.0_0""
IcedTea6 1.3.1 (6b12-0ubuntu6.7) Runtime Environment (build 1.6.0_0-b12)
OpenJDK 64-Bit Server VM (build 1.6.0_0-b12, mixed mode)


Linux pmc01 2.6.27-22-xen #1 SMP Fri Feb 20 23:58:13 UTC 2009 x86_64 GNU/Linux;;;","15/Feb/11 19:48;jbellis;closing this so it's clear that the excessive object creation problem introduced in CASSANDRA-1905 is fixed in 0.6.11 / 0.7.1.

opened CASSANDRA-2170 for other load spikes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
overflow in NodeCmd,CASSANDRA-2057,12496751,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,kingryan,kingryan,kingryan,26/Jan/11 00:54,16/Apr/19 09:33,14/Jul/23 05:52,26/Jan/11 01:52,0.7.1,,,Legacy/Tools,,,0,,,,We aggregate the long read/write counts across CFs into an int.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/11 00:56;kingryan;nodetool_overflow.patch;https://issues.apache.org/jira/secure/attachment/12469365/nodetool_overflow.patch",,,,,,,,,,,,,,1.0,kingryan,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20418,,,Wed Jan 26 02:28:41 UTC 2011,,,,,,,,,,"0|i0g927:",92898,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"26/Jan/11 01:52;jbellis;committed, thanks!;;;","26/Jan/11 02:28;hudson;Integrated in Cassandra-0.7 #215 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/215/])
    fix potential overflow in nodetool cfstats
patch by Ryan King; reviewed by jbellis for CASSANDRA-2057
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make cache saving less contentious,CASSANDRA-2053,12496713,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,nickmbailey,nickmbailey,25/Jan/11 19:15,16/Apr/19 09:33,14/Jul/23 05:52,05/Feb/11 21:18,0.7.1,,,,,,0,,,,"The current default for saving key caches is every hour.  Additionally the default timeout for flushing memtables is every hour.  I've seen situations where both of these occuring at the same time every hour causes enough pressure on the node to have it drop messages and other nodes mark it dead.  This happens across the cluster and results in flapping.

We should do something to spread this out. Perhaps staggering cache saves/flushes that occur due to timeouts.",,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,"01/Feb/11 17:48;jbellis;2053.txt;https://issues.apache.org/jira/secure/attachment/12469946/2053.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20415,,,Sat Feb 05 21:52:47 UTC 2011,,,,,,,,,,"0|i0g91b:",92894,,nickmbailey,,nickmbailey,Normal,,,,,,,,,,,,,,,,,"01/Feb/11 17:48;jbellis;Moves CacheWriter to a top-level class and runs it on the CompactionManager executor.  Reads and writes udpated to use BRAF cache-non-polluting mode.;;;","05/Feb/11 20:08;nickmbailey;+1;;;","05/Feb/11 21:18;jbellis;committed.

also increased default key cache save period from 1h to 4h.;;;","05/Feb/11 21:52;hudson;Integrated in Cassandra-0.7 #251 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/251/])
    cache writing moved to CompactionManager to reduce i/o contention and updated to use non-cache-polluting writes
patch by jbellis; reviewed by nickmbailey for CASSANDRA-2053
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixes for multi-datacenter writes,CASSANDRA-2051,12496702,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ivancso,jbellis,jbellis,25/Jan/11 16:57,16/Apr/19 09:33,14/Jul/23 05:52,25/Jan/11 20:14,0.7.1,,,,,,0,,,,"Copied from CASSANDRA-982:

    * Message::removeHeader
      message.setHeader(RowMutation.FORWARD_HEADER, null) throws NullPointerException

    * db/RowMutationVerbHandler::forwardToLocalNodes
      set correct destination address for sendOneWay

    * response(ReadResponse result) added to DatacenterReadCallback
      otherwise ReadCallback will process local results and condition will be never signaled in DatacenterReadCallback

    * FORWARD header removed in StorageProxy::sendMessages if dataCenter equals to localDataCenter
      (if a non local DC processed before local DC FORWARD header will be set when unhintedMessage used in sendToHintedEndpoints. one instance of Message used for unhintedMessage)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/11 17:17;jbellis;2051-2.txt;https://issues.apache.org/jira/secure/attachment/12469299/2051-2.txt","25/Jan/11 16:58;jbellis;rep_fix_02.patch;https://issues.apache.org/jira/secure/attachment/12469297/rep_fix_02.patch",,,,,,,,,,,,,2.0,ivancso,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20414,,,Tue Jan 25 20:29:38 UTC 2011,,,,,,,,,,"0|i0g90v:",92892,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"25/Jan/11 16:58;jbellis;Ivan's patch;;;","25/Jan/11 17:17;jbellis;patch 2 re-organizes the loops in SP::sendMessages to make ivan's fix a little more clear.  (with the side benefit that we now call String.equals once per DC instead of once per message.);;;","25/Jan/11 17:19;jbellis;committed everything else from ivan's patch in r1063361;;;","25/Jan/11 18:49;hudson;Integrated in Cassandra-0.7 #207 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/207/])
    ;;;","25/Jan/11 19:55;ivancso;patch 2 much better. thanks Jonathan. ;)

i pulled latest trunk and applied patch in 2051-2.txt.

it seems that communication between nodes and DCs works as expected.;;;","25/Jan/11 20:04;tjake;Looks good +1,  thanks for testing this ivan;;;","25/Jan/11 20:14;jbellis;committed.  thanks Ivan and Jake!;;;","25/Jan/11 20:29;hudson;Integrated in Cassandra-0.7 #210 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/210/])
    clean out forward headers from message between loops
patch by ivancso and jbellis; reviewed by tjake for CASSANDRA-2051
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"On the CLI, creating or updating a keyspace to use the NetworkTopologyStrategy breaks ""show keyspaces;""",CASSANDRA-2049,12496687,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,jeromatron,jeromatron,25/Jan/11 15:23,16/Apr/19 09:33,14/Jul/23 05:52,27/Jan/11 15:21,0.7.1,,,,,,0,,,,"To reproduce:
- Start fresh.
- Run ""show keyspaces;""
- Run ""create keyspace Keyspace1 with placement_strategy='org.apache.cassandra.locator.NetworkTopologyStrategy';""
- Run ""show keyspaces;""

Note how before it showed the system keyspace.  After it shows just:
Keyspace: Keyspace1:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
null

If you have multiple keyspaces, it will hide those as well.  Also, if you create the keyspace and then update it with NetworkTopologyStrategy, the same thing will happen.",,,,,,,,,,,,,,,,,,,,,,";26/Jan/11 22:02;xedin;1200",1200,0,1200,100%,1200,0,1200,,,,,,,,,,,,,,,"26/Jan/11 22:03;xedin;CASSANDRA-2049.patch;https://issues.apache.org/jira/secure/attachment/12469486/CASSANDRA-2049.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20413,,,Thu Jan 27 15:44:42 UTC 2011,,,,,,,,,,"0|i0g90f:",92890,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"27/Jan/11 15:21;jbellis;committed;;;","27/Jan/11 15:44;hudson;Integrated in Cassandra-0.7 #217 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/217/])
    fix CLI ""show keyspaces"" with null options on NTS
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-2049
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ivy.jar is included in the binary distribution,CASSANDRA-2046,12496610,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,stephenc,stephenc,24/Jan/11 22:15,16/Apr/19 09:33,14/Jul/23 05:52,24/Jan/11 22:21,0.7.1,,,,,,0,,,,"The build currently copys ivy.xml into the bin.tar.gz

according to Eric, this is a bug


-rw-r--r-- 0/0          910990 2011-01-06 16:46 apache-cassandra-0.7.0/lib/ivy-2.1.0.jar
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20412,,,Mon Jan 24 22:34:10 UTC 2011,,,,,,,,,,"0|i0g8zr:",92887,,,,,Normal,,,,,,,,,,,,,,,,,"24/Jan/11 22:21;urandom;committed.;;;","24/Jan/11 22:34;hudson;Integrated in Cassandra-0.7 #202 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/202/])
    do not install ivy jar to artifacts

Patch by eevans for CASSANDRA-2046
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI should loop on describe_schema until agreement or fatel exit with stacktrace/message if no agreement after X seconds,CASSANDRA-2044,12496600,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,mdennis,mdennis,24/Jan/11 20:36,16/Apr/19 09:33,14/Jul/23 05:52,25/Jan/11 19:57,0.7.1,,,,,,0,,,,"see CASSANDRA-2026 for brief background.

It's easy to enter statements into the CLI before the schema has settled, often causing problems where it is no longer possible to get the nodes in agreement about the schema without removing the system directory.

The alleviate the most common problems with this, the CLI should issue the modification statement and loop on describe_schema until all nodes agree or until X seconds has passed.  If the timeout has been exceeded, the CLI should exit with an error and inform the user that the schema has not settled and further migrations are ill-advised until it does.

number_of_nodes/2+1 seconds seems like a decent wait time for schema migrations to start with.

Bonus points for making the value configurable.",,,,,,,,,,,,,,,,,,,,,,";25/Jan/11 20:14;xedin;7200",14400,0,7200,50%,14400,0,7200,,,,,,,,,,,,,,,"25/Jan/11 01:01;xedin;CASSANDRA-2044.patch;https://issues.apache.org/jira/secure/attachment/12469224/CASSANDRA-2044.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20411,,,Tue Jan 25 20:14:33 UTC 2011,,,,,,,,,,"0|i0g8zb:",92885,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"25/Jan/11 01:01;xedin;I have added option ""--schema-mwt"" to set wait time for schema migration in seconds from command line, if it was not set system will use node_count/2 + 1 secs or 5 secs if couldn't connect to JMX.

CLI checks node agreement on every keyspace create/drop and if nodes were not able to agree in given wait time system will print error message ""The schema has not settled in %d seconds and further migrations are ill-advised until it does."" and exit with -1 code.;;;","25/Jan/11 19:57;jbellis;committed with a couple changes:

- default wait changed to flat 10s; schema propagation does not use gossip and is not expected to be proportional to number of nodes in cluster
- added validation to keyspace update and cf create/update/drop;;;","25/Jan/11 20:14;hudson;Integrated in Cassandra-0.7 #209 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/209/])
    CLI attemptsto block for new schemato propagate
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-2044
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LazilyCompactedRow doesn't add CFInfo to digest,CASSANDRA-2039,12496518,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,richardlow,richardlow,richardlow,24/Jan/11 10:59,16/Apr/19 09:33,14/Jul/23 05:52,08/Apr/11 21:39,0.8 beta 1,,,,,,0,,,,"LazilyCompactedRow.update doesn't add the CFInfo or columnCount to the digest, so the hash value in the Merkle tree does not include this data.  However, PrecompactedRow does include this.  Two consequences of this are:
* Row-level tombstones are not compared when using LazilyCompactedRow so could remain inconsistent
* LazilyCompactedRow and PrecompactedRow produce different hashes of the same row, so if two nodes have differing in_memory_compaction_limit_in_mb values, rows of size in between the two limits will have different hashes so will always be repaired even when they are the same.",,richardlow,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/11 12:29;richardlow;trunk-2038-LazilyCompactedRowTest.txt;https://issues.apache.org/jira/secure/attachment/12469416/trunk-2038-LazilyCompactedRowTest.txt","26/Jan/11 18:24;richardlow;trunk-2038-v2.txt;https://issues.apache.org/jira/secure/attachment/12469454/trunk-2038-v2.txt","24/Jan/11 11:03;richardlow;trunk-2038.txt;https://issues.apache.org/jira/secure/attachment/12469140/trunk-2038.txt",,,,,,,,,,,,3.0,richardlow,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20410,,,Sat Feb 05 21:37:53 UTC 2011,,,,,,,,,,"0|i0g8y7:",92880,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"25/Jan/11 20:05;jbellis;this looks okay eyeballing it, but can you add a check to LazilyCompactedRowTest similar to assertBytes to make sure this stays fixed?;;;","26/Jan/11 12:37;richardlow;I added assertDigest (see the patch trunk-2038-LazilyCompactedRowTest.txt), but it currently fails in testManyRows.  The reason is that the Bloom filters have different sizes, because getEstimatedColumnCount returns a value too large in LazilyCompactedRow.  There is probably no way round this without doing another pass on the data.

However, it isn't necessary to add the Bloom filter or indeed the index to the digest - they are determined by the data that comes later.  So could the header be excluded from the digest?;;;","26/Jan/11 14:41;jbellis;That sounds good.  Let's do that in trunk to avoid breaking compatibility in 0.7.;;;","26/Jan/11 18:39;richardlow;The v2 patch does this (and supersedes the other two patches).  Now the updated LazilyCompactedRowTest passes, as do the other tests.

It now might be possible to compute the digest for LazilyCompactedRow in just one pass through the data - the first pass is now just used to calculate columnCount.  columnCount could be excluded from the digest (I added it just because it was there in PrecompactedRow).  However, it's used by isEmpty - what would be the impact of using estimatedColumnCount here instead?  Is estimatedColumnCount zero if and only if columnCount is zero?  Or does it matter is isEmpty sometimes returns false when it's true?;;;","05/Feb/11 20:38;jbellis;Committed v2.  Thanks!

bq. what would be the impact of using estimatedColumnCount here instead

It would break the part of CompactionIterator that leaves out rows with no columns from the new SSTable.  ""estimated"" is the maximum possible number of columns in the new row, so it's ok to use it in the bloom filter, but not in the ""is this row empty post-merge"" check.;;;","05/Feb/11 21:37;hudson;Integrated in Cassandra #710 (See [https://hudson.apache.org/hudson/job/Cassandra/710/])
    make PreCompactedRow and LazyCompactedRow digest computations match
patch by Richard Low and jbellis for CASSANDRA-2039
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-topology.properties cannot reside inside jar file,CASSANDRA-2036,12496455,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mck,mck,mck,23/Jan/11 20:19,16/Apr/19 09:33,14/Jul/23 05:52,24/Jan/11 15:59,0.7.1,,,,,,0,,,,"PropertyFileSnitch cannot load the cassandra-topology.properties if it is located inside a jar file.

At startup cassandra will print and exit
[ERROR] 20:50:01  Fatal error: Unable to read cassandra-topology.properties
Bad configuration; unable to start server

It seems FBUtilities.resourceToFIle(..) can only be used for loading plain files.

The attached patch solves the problem. It uses the standard java approach for loading a resource stream...
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/11 07:45;mck;CASSANDRA-2036.patch;https://issues.apache.org/jira/secure/attachment/12469125/CASSANDRA-2036.patch","23/Jan/11 20:33;mck;CASSANDRA-2036.patch;https://issues.apache.org/jira/secure/attachment/12469101/CASSANDRA-2036.patch",,,,,,,,,,,,,2.0,mck,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20408,,,Mon Jan 24 16:38:54 UTC 2011,,,,,,,,,,"0|i0g8xj:",92877,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"24/Jan/11 05:40;jbellis;Should we also allow loading from a URL the way we do cassandra.yaml in DatabaseDescriptor?;;;","24/Jan/11 06:52;mck;There currently is no way of specifying the path/url to the cassandra-topology.properties, it is just presumed to be at the root of your classpath.

But the one cassandra-topology.properties file can serve not just every node in the cluster, but all nodes in all clusters (useful when test environments run multiple nodes/clusters per server, so it does make sense to allow loading from a URL. 

I can make a new patch for this (although i think this issue becomes an improvement then).;;;","24/Jan/11 07:45;mck;second revision. with improvements so ""-Dcassandra.propertyFileSnitch=<url>"" can be specified on the command line (in a similar manner to ""-Dcassadra.config"".;;;","24/Jan/11 15:59;jbellis;committed the first patch, and created CASSANDRA-2040 for a more general solution to the URL problem in 0.8;;;","24/Jan/11 16:38;hudson;Integrated in Cassandra-0.7 #194 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/194/])
    load PFS properties with ResourceAsStream
patch by Michael SembWever; reviewed by jbellis for CASSANDRA-2036
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI chokes on whitespace after semicolon when using -f,CASSANDRA-2031,12496345,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,mdennis,mdennis,21/Jan/11 22:06,16/Apr/19 09:33,14/Jul/23 05:52,24/Jan/11 05:39,0.7.1,,,Legacy/Tools,,,0,,,,"The CLI chokes on whitespace after the semicolon when a file is passed with -f

""... missing EOF at""",,,,,,,,,,,,,,,,,,,,,,";22/Jan/11 14:43;xedin;600",600,0,600,100%,600,0,600,,,,,,,,,,,,,,,"22/Jan/11 14:43;xedin;CASSANDRA-2031.patch;https://issues.apache.org/jira/secure/attachment/12469045/CASSANDRA-2031.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20406,,,Mon Jan 24 05:53:50 UTC 2011,,,,,,,,,,"0|i0g8wf:",92872,,mdennis,,mdennis,Low,,,,,,,,,,,,,,,,,"24/Jan/11 02:37;mdennis;+1;;;","24/Jan/11 05:39;jbellis;committed;;;","24/Jan/11 05:53;hudson;Integrated in Cassandra-0.7 #192 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/192/])
    trim cli input to avoid confusing parser
patch by Pavel Yaskevich; reviewed by mdennis for CASSANDRA-2031
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix regression in 1968 (young gen sizing logic),CASSANDRA-2023,12496207,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,scode,scode,scode,21/Jan/11 00:57,16/Apr/19 09:33,14/Jul/23 05:52,24/Jan/11 19:40,0.7.1,,,Packaging,,,0,,,,"1968 introduced a regression (there was still cleanup to do). In particular it broke when an explicit MAX_HEAP_SIZE was set. Attaching *draft* patch (needs more testing).

Allowing automatic newsize calculation in the face of a manually specified MAX_HEAP_SIZE was problematic. Either one has to duplicate JVM parsing of MAX_HEAP_SIZE or ask the user to set MAX_HEAP_SIZE_IN_MB (or similar) instead.

In this patch (consider it a draft) i opted for the latter + picking up MAX_HEAP_SIZE for backwards compatibility (but with the effect that it disables new size calculation). I tried to make it slightly more posixly correct, but as usual no guarantees given that I have no posix shell to test it on.

I'm not really happy about the shell acrobatics and my confidence that there is not some left-over issue is not high. Should we just not worry about MAX_HEAP_SIZE compatibility and remove all that compatibility cruft? Plenty of acrobatics left still, but it would remove the more hideous parts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/11 08:24;scode;2023-v2.txt;https://issues.apache.org/jira/secure/attachment/12468957/2023-v2.txt","21/Jan/11 00:57;scode;2023.txt;https://issues.apache.org/jira/secure/attachment/12468935/2023.txt",,,,,,,,,,,,,2.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20404,,,Mon Jan 24 20:31:50 UTC 2011,,,,,,,,,,"0|i0g8un:",92864,,urandom,,urandom,Normal,,,,,,,,,,,,,,,,,"21/Jan/11 03:56;jbellis;bq. automatic newsize calculation in the face of a manually specified MAX_HEAP_SIZE was problematic. Either one has to duplicate JVM parsing of MAX_HEAP_SIZE or ask the user to set MAX_HEAP_SIZE_IN_MB (or similar) instead.

I would rather say ""if you manually specify MAX_HEAP_SIZE, you must also specify HEAP_NEWSIZE."";;;","21/Jan/11 08:24;scode;v2 is significantly simpler and cleaner (IMO). Tested on MacOS and Linux.

FWIW, the reason I didn't do ""must set both"" initially is that I felt the new size was a lot more obscure/advanced than merely setting heap size. v2 tries to mitigate this by documenting what to do if unsure/don't care.

;;;","24/Jan/11 19:40;urandom;Please avoid making unrelated changes to whitespace/styling as it makes review more difficult.  Otherwise, it looks good.

Committed.;;;","24/Jan/11 19:59;hudson;Integrated in Cassandra-0.7 #199 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/199/])
    fix young gen sizing logic

Patch by Peter Shuller (w/ minor changes); review by eevans for CASSANDRA-2023
;;;","24/Jan/11 20:31;hudson;Integrated in Cassandra #687 (See [https://hudson.apache.org/hudson/job/Cassandra/687/])
    fix young gen sizing logic

Patch by Peter Shuller (w/ minor changes); review by eevans for CASSANDRA-2023
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Twisted driver for CQL,CASSANDRA-2022,12496185,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,urandom,urandom,20/Jan/11 21:58,16/Apr/19 09:33,14/Jul/23 05:52,16/Mar/11 03:06,0.8 beta 1,,,Legacy/CQL,,20/Jan/11 00:00,0,,,,"In-tree CQL drivers should be reasonably consistent with one another (wherever possible/practical), and implement a minimum of:

• Query compression
• Keyspace assignment on connection
• Connection pooling / load-balancing

The goal is not to supplant the idiomatic libraries, but to provide a consistent, stable base for them to build upon.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/11 22:27;brandon.williams;txcql.txt;https://issues.apache.org/jira/secure/attachment/12473742/txcql.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20403,,,Thu Mar 17 16:23:25 UTC 2011,,,,,,,,,,"0|i0g8uf:",92863,,,,,Low,,,,,,,,,,,,,,,,,"14/Mar/11 19:55;urandom;You need to change {{default_validation_class}} to {{default_validation}} to make {{example.py}} work, but otherwise, LGTM.;;;","15/Mar/11 22:27;brandon.williams;Update patch fixes validation_class, adds support for SchemaDisagreementException, allows a decoder to be passed to the conn pool, adds a setup.py and README.;;;","16/Mar/11 03:06;brandon.williams;Committed;;;","17/Mar/11 16:23;hudson;Integrated in Cassandra #789 (See [https://hudson.apache.org/hudson/job/Cassandra/789/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Files added with missing license headers,CASSANDRA-2016,12496139,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,stephenc,stephenc,stephenc,20/Jan/11 15:35,16/Apr/19 09:33,14/Jul/23 05:52,20/Jan/11 15:57,0.7.1,,,,,,0,,,,"       src/java/org/apache/cassandra/utils/BloomFilterSerializer.java
       src/java/org/apache/cassandra/utils/LegacyBloomFilterSerializer.java
       src/java/org/apache/cassandra/service/RepairCallback.java
       src/java/org/apache/cassandra/io/util/ColumnSortedMap.java

are all missing license headers... ASLv2 I assume",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/11 15:37;stephenc;CASSANDRA-2016-missing-license-headers.patch;https://issues.apache.org/jira/secure/attachment/12468855/CASSANDRA-2016-missing-license-headers.patch",,,,,,,,,,,,,,1.0,stephenc,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20401,,,Thu Jan 20 16:44:05 UTC 2011,,,,,,,,,,"0|i0g8sv:",92856,,,,,Critical,,,,,,,,,,,,,,,,,"20/Jan/11 15:37;stephenc;attached patch adds the ASL headers which I assume are missing by mistake and not intentionally;;;","20/Jan/11 15:57;urandom;There is an ant target, {{rat-write}}, which I use when creating release artifacts (as someone has usually forgotten).;;;","20/Jan/11 16:44;hudson;Integrated in Cassandra-0.7 #182 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/182/])
    added missing license headers

Patch eevans; reported by Stephen Connolly for CASSANDRA-2016
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't delete whole row from Hadoop MapReduce,CASSANDRA-2014,12496104,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,patrik.modesto,patrik.modesto,patrik.modesto,20/Jan/11 08:55,16/Apr/19 09:33,14/Jul/23 05:52,17/Feb/11 15:09,0.7.3,,,,,,0,,,,"ColumnFamilyRecordWriter.java doesn't support Mutation with Deletion without slice_predicat and super_column to delete whole row. The other way I tried is to specify SlicePredicate with empty start and finish and I got:

{code}
java.io.IOException: InvalidRequestException(why:Deletion does not yet support SliceRange predicates.)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter$RangeClient.run(ColumnFamilyRecordWriter.java:355)
{code}

I tryied to patch the ColumnFamilyRecordWriter.java like this:
{code}
--- a/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordWriter.java
+++ b/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordWriter.java
@@ -166,10 +166,17 @@ implements org.apache.hadoop.mapred.RecordWriter<ByteBuffer,List<org.apache.cass
             // deletion
             Deletion deletion = new Deletion(amut.deletion.timestamp);
             mutation.setDeletion(deletion);
+
             org.apache.cassandra.avro.SlicePredicate apred = amut.deletion.predicate;
-            if (amut.deletion.super_column != null)
+            if (apred == null && amut.deletion.super_column == null)
+            {
+                // epmty; delete whole row
+            }
+            else if (amut.deletion.super_column != null)
+            {
                 // super column
                 deletion.setSuper_column(copy(amut.deletion.super_column));
+            }
             else if (apred.column_names != null)
             {
                 // column names
{code}

but that didn't work as well.",Debian Linux 2.6.32 amd64,jeromatron,mck,patrik.modesto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/11 07:49;patrik.modesto;2014-mr-delete-whole-row.patch;https://issues.apache.org/jira/secure/attachment/12469256/2014-mr-delete-whole-row.patch",,,,,,,,,,,,,,1.0,patrik.modesto,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20399,,,Thu Feb 17 15:34:31 UTC 2011,,,,,,,,,,"0|i0g8sf:",92854,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"25/Jan/11 07:49;patrik.modesto;The patch actually worked, I just did misinterpret the data.;;;","17/Feb/11 08:51;patrik.modesto;Hi, can the fix be merged please?;;;","17/Feb/11 15:09;jbellis;committed, thanks!;;;","17/Feb/11 15:34;hudson;Integrated in Cassandra-0.7 #284 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/284/])
    Handle whole-row deletions in CFOutputFormat
patch by Patrik Modesto; reviewed by jbellis for CASSANDRA-2014
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when read repair is disabled,CASSANDRA-2010,12496038,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,19/Jan/11 18:54,16/Apr/19 09:33,14/Jul/23 05:52,20/Jan/11 02:34,0.7.1,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"19/Jan/11 18:56;jbellis;2010.txt;https://issues.apache.org/jira/secure/attachment/12468775/2010.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19350,,,Thu Jan 20 02:59:27 UTC 2011,,,,,,,,,,"0|i0g8rj:",92850,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"19/Jan/11 19:01;jbellis;the problem is that messages and endpoints in the sendRR need to be the same length.  fix attached.;;;","19/Jan/11 23:15;tjake;+1;;;","20/Jan/11 02:34;jbellis;committed;;;","20/Jan/11 02:59;hudson;Integrated in Cassandra-0.7 #181 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/181/])
    fix messages/endpoints mismatch when RR is disabled
patch by jbellis; reviewed by tjake for CASSANDRA-2010
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0.7 migrations/schema serializations are incompatible with trunk,CASSANDRA-2001,12495898,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,gdusbabek,gdusbabek,gdusbabek,18/Jan/11 14:03,16/Apr/19 09:33,14/Jul/23 05:52,18/Jan/11 18:25,0.8 beta 1,,,,,,0,,,,"Two problems:
1. inserting replicate_on_write into the middle of the CfDef members created a problem with serialization.  
2. merging the genavro files created a strange namespacing problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/11 21:11;stuhood;0004-Set-a-default-for-rep-on-write-and-revert-0001.txt;https://issues.apache.org/jira/secure/attachment/12468682/0004-Set-a-default-for-rep-on-write-and-revert-0001.txt","18/Jan/11 17:19;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0001-use-the-writer-schema-and-not-the-current-schema-when-.txt;https://issues.apache.org/jira/secure/attachment/12468653/ASF.LICENSE.NOT.GRANTED--v1-0001-use-the-writer-schema-and-not-the-current-schema-when-.txt","18/Jan/11 17:19;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0002-get-rid-of-the-avro-db.migrations-classes-that-were-du.txt;https://issues.apache.org/jira/secure/attachment/12468654/ASF.LICENSE.NOT.GRANTED--v1-0002-get-rid-of-the-avro-db.migrations-classes-that-were-du.txt","18/Jan/11 17:19;gdusbabek;ASF.LICENSE.NOT.GRANTED--v1-0003-fix-order-of-replicate_on_write-and-make-sure-it-s-not.txt;https://issues.apache.org/jira/secure/attachment/12468655/ASF.LICENSE.NOT.GRANTED--v1-0003-fix-order-of-replicate_on_write-and-make-sure-it-s-not.txt","18/Jan/11 20:09;gdusbabek;data.zip;https://issues.apache.org/jira/secure/attachment/12468676/data.zip",,,,,,,,,,5.0,gdusbabek,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20397,,,Tue Jan 18 23:16:43 UTC 2011,,,,,,,,,,"0|i0g8pj:",92841,,eevans,,eevans,Low,,,,,,,,,,,,,,,,,"18/Jan/11 14:03;gdusbabek;fwiw CASSANDRA-1923 contains unit tests that will catch these kinds of problems in the future.;;;","18/Jan/11 17:20;gdusbabek;also fixes the incorrect schema that was used when deserializing an object.;;;","18/Jan/11 17:54;urandom;Just for posterity sake: CASSANDRA-926 moved  the remaining Avro records from o.a.c.avro elsewhere, to packages that made it obvious which components were still using them (but obviously, created a bug in the process).

+1 on this patchset though (with or without the o.a.c.db.migration.avro -> o.a.c.db.avro move).;;;","18/Jan/11 18:24;hudson;Integrated in Cassandra #676 (See [https://hudson.apache.org/hudson/job/Cassandra/676/])
    fix order of replicate_on_write and make sure it's not null. patch by gdusbabek, reviewed by eevans. CASSANDRA-2001
get rid of the avro db.migrations classes that were duplicated elsewhere. patch by gdusbabek, reviewed by eevans. CASSANDRA-2001
use the writer schema and not the current schema when deserializing. patch by gdusbabek, reviewed by eevans. CASSANDRA-2001
;;;","18/Jan/11 18:25;gdusbabek;committed;;;","18/Jan/11 20:09;gdusbabek;schema from a vanilla 0.7 node.;;;","18/Jan/11 20:27;gdusbabek;If you bypass the ClassCastException that 926 introduced, you end up with an avro error:
org.apache.avro.AvroTypeException: Found {""type"":""record"",""name"":""CfDef"",""namespace"":""org.apache.cassandra.avro"",""fields"":[{""name"":""keyspace"",""type"":""string""},{""name"":""name"",""type"":""string""},{""name"":""column_type"",""type"":[""string"",""null""]},{""name"":""comparator_type"",""type"":[""string"",""null""]},{""name"":""subcomparator_type"",""type"":[""string"",""null""]},{""name"":""comment"",""type"":[""string"",""null""]},{""name"":""row_cache_size"",""type"":[""double"",""null""]},{""name"":""key_cache_size"",""type"":[""double"",""null""]},{""name"":""read_repair_chance"",""type"":[""double"",""null""]},{""name"":""gc_grace_seconds"",""type"":[""int"",""null""]},{""name"":""default_validation_class"",""type"":[""null"",""string""],""default"":null},{""name"":""min_compaction_threshold"",""type"":[""null"",""int""],""default"":null},{""name"":""max_compaction_threshold"",""type"":[""null"",""int""],""default"":null},{""name"":""row_cache_save_period_in_seconds"",""type"":[""int"",""null""],""default"":0},{""name"":""key_cache_save_period_in_seconds"",""type"":[""int"",""null""],""default"":3600},{""name"":""memtable_flush_after_mins"",""type"":[""int"",""null""],""default"":60},{""name"":""memtable_throughput_in_mb"",""type"":[""null"",""int""],""default"":null},{""name"":""memtable_operations_in_millions"",""type"":[""null"",""double""],""default"":null},{""name"":""id"",""type"":[""int"",""null""]},{""name"":""column_metadata"",""type"":[{""type"":""array"",""items"":{""type"":""record"",""name"":""ColumnDef"",""fields"":[{""name"":""name"",""type"":""bytes""},{""name"":""validation_class"",""type"":""string""},{""name"":""index_type"",""type"":[{""type"":""enum"",""name"":""IndexType"",""symbols"":[""KEYS""],""aliases"":[""org.apache.cassandra.config.avro.IndexType""]},""null""]},{""name"":""index_name"",""type"":[""string"",""null""]}]}},""null""]}]}, expecting {""type"":""record"",""name"":""CfDef"",""namespace"":""org.apache.cassandra.avro"",""fields"":[{""name"":""keyspace"",""type"":""string""},{""name"":""name"",""type"":""string""},{""name"":""column_type"",""type"":[""string"",""null""]},{""name"":""comparator_type"",""type"":[""string"",""null""]},{""name"":""subcomparator_type"",""type"":[""string"",""null""]},{""name"":""comment"",""type"":[""string"",""null""]},{""name"":""row_cache_size"",""type"":[""double"",""null""]},{""name"":""key_cache_size"",""type"":[""double"",""null""]},{""name"":""read_repair_chance"",""type"":[""double"",""null""]},{""name"":""gc_grace_seconds"",""type"":[""int"",""null""]},{""name"":""default_validation_class"",""type"":[""null"",""string""],""default"":null},{""name"":""min_compaction_threshold"",""type"":[""null"",""int""],""default"":null},{""name"":""max_compaction_threshold"",""type"":[""null"",""int""],""default"":null},{""name"":""row_cache_save_period_in_seconds"",""type"":[""int"",""null""],""default"":0},{""name"":""key_cache_save_period_in_seconds"",""type"":[""int"",""null""],""default"":3600},{""name"":""memtable_flush_after_mins"",""type"":[""int"",""null""],""default"":60},{""name"":""memtable_throughput_in_mb"",""type"":[""null"",""int""],""default"":null},{""name"":""memtable_operations_in_millions"",""type"":[""null"",""double""],""default"":null},{""name"":""id"",""type"":[""int"",""null""]},{""name"":""column_metadata"",""type"":[{""type"":""array"",""items"":{""type"":""record"",""name"":""ColumnDef"",""fields"":[{""name"":""name"",""type"":""bytes""},{""name"":""validation_class"",""type"":""string""},{""name"":""index_type"",""type"":[{""type"":""enum"",""name"":""IndexType"",""symbols"":[""KEYS""],""aliases"":[""org.apache.cassandra.config.avro.IndexType""]},""null""]},{""name"":""index_name"",""type"":[""string"",""null""]}],""aliases"":[""org.apache.cassandra.config.avro.ColumnDef""]}},""null""]},{""name"":""replicate_on_write"",""type"":[""boolean"",""null""]}],""aliases"":[""org.apache.cassandra.config.avro.CfDef""]}

basically, avro was expecting to see replicate_on_write.;;;","18/Jan/11 21:11;stuhood;Attaching the patch discussed in IRC.

In summary: it is necessary to add a default with every new field, independent of whether you are using a union to make the field optional. Defaults are used in places where the writer's schema doesn't contain a field, but the reader's schema does. Unions are used to make fields optional: you can use a union together with a default to add a new optional field.;;;","18/Jan/11 21:28;gdusbabek;backed out of the original fix and committed the one-liner that sets a default on CfDef.replicate_on_write.;;;","18/Jan/11 23:16;hudson;Integrated in Cassandra #677 (See [https://hudson.apache.org/hudson/job/Cassandra/677/])
    set a default for replicate_on_write. patch by stuhood and gdusbabek. CASSANDRA-2001
back out of 2001. patch by gdusbabek. CASSANDRA-2001
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Table comments indicates expiry checking happens 10x times per minimum interval, but doesn't",CASSANDRA-2000,12495843,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,scode,scode,scode,18/Jan/11 01:24,16/Apr/19 09:33,14/Jul/23 05:52,18/Jan/11 02:56,0.7.1,,,,,,0,,,,"Minor point, the there is a comment in the body of the Table constructor that claims it checks 10x as often to not miss the deadline by more than 10%. It seems to me that either the comment should be removed, or a change is necessary to make it true (trivial patch attached).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/11 01:25;scode;2000.txt;https://issues.apache.org/jira/secure/attachment/12468614/2000.txt",,,,,,,,,,,,,,1.0,scode,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20396,,,Tue Jan 18 08:33:39 UTC 2011,,,,,,,,,,"0|i0g8pb:",92840,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"18/Jan/11 02:56;jbellis;you totally just wanted to get issue 2000 :);;;","18/Jan/11 02:56;jbellis;committed;;;","18/Jan/11 08:33;hudson;Integrated in Cassandra-0.7 #170 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/170/])
    check more frequently for memtable expiration
patch by Peter Schuller; reviewed by jbellis for CASSANDRA-2000
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix misuses of ByteBufferUtil.string(),CASSANDRA-1999,12495809,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,17/Jan/11 17:48,16/Apr/19 09:33,14/Jul/23 05:52,17/Jan/11 18:42,0.7.1,0.8 beta 1,,,,,0,,,,"ByteBufferUtil.string() takes a start offset and a length. It is however used as if taking
a start and end offset.",,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"17/Jan/11 18:13;slebresne;0001-Fix-misuse-of-ByteBufferUtil.string.patch;https://issues.apache.org/jira/secure/attachment/12468582/0001-Fix-misuse-of-ByteBufferUtil.string.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20395,,,Mon Jan 17 19:48:04 UTC 2011,,,,,,,,,,"0|i0g8p3:",92839,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"17/Jan/11 17:49;slebresne;Attached patch against 0.7;;;","17/Jan/11 18:13;slebresne;Fixed a tiny mistake. Should be good now.;;;","17/Jan/11 18:42;jbellis;committed.  (looks like these were introduced in CASSANDRA-1714, fwiw, so i'm going to tag affects-version to 0.7.1);;;","17/Jan/11 19:48;hudson;Integrated in Cassandra-0.7 #168 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/168/])
    fix ByteBuffer regressions from #1714
patch by slebresne; reviewed by jbellis for CASSANDRA-1999
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli doesn't accept 'name' as a column name in column metadata when creating a column family,CASSANDRA-1995,12495723,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,thobbs,thobbs,16/Jan/11 20:03,16/Apr/19 09:33,14/Jul/23 05:52,17/Jan/11 16:00,0.7.1,,,Legacy/Tools,,,0,,,,"This fails:

create column family Countries with comparator=UTF8Type and column_metadata=[ {column_name: name, validation_class: UTF8Type} ];

This works:

create column family Countries with comparator=UTF8Type and column_metadata=[ {column_name: fooname, validation_class: UTF8Type} ];",,,,,,,,,,,,,,,,,,,,,,";17/Jan/11 13:34;xedin;10800",10800,0,10800,100%,10800,0,10800,,,,,,,,,,,,,,,"17/Jan/11 13:34;xedin;CASSANDRA-1995.patch;https://issues.apache.org/jira/secure/attachment/12468551/CASSANDRA-1995.patch",,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20394,,,Mon Jan 17 16:21:26 UTC 2011,,,,,,,,,,"0|i0g8o7:",92835,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"17/Jan/11 13:34;xedin;Work branch: cassandra-0.7, latest commit 2a3a497fd8b12160faf81edce1d7e2cbac953b95 (r/m unused code and improve formatting)

Also: tests added for this case and for help statements.;;;","17/Jan/11 16:00;jbellis;committed;;;","17/Jan/11 16:21;hudson;Integrated in Cassandra-0.7 #167 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/167/])
    fix CLI parsing of ""cluster name.""
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-1995
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Word count example doesn't output the words correctly to cassandra.  It outputs spurious data past the length of the byte array.,CASSANDRA-1993,12495691,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jesseshieh,jesseshieh,jesseshieh,16/Jan/11 01:38,16/Apr/19 09:33,14/Jul/23 05:52,16/Jan/11 22:55,0.7.1,,,,,,0,,,,"To reproduce:
# start a local cassandra server e.g. sudo bin/cassandra -f
cd contrib/word_count
ant
bin/word_count_setup
bin/word_count

# check the data in cassandra, all looks fine because the words are all of the same length.
# change the data in cassandra to real words, rerun the mapreduce and you'll see some words have spurious characters written past their length
# this is because the word bytes are not terminated at their length",All,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"16/Jan/11 04:03;jbellis;1993-v2.txt;https://issues.apache.org/jira/secure/attachment/12468473/1993-v2.txt","16/Jan/11 01:43;jesseshieh;trunk-1993.txt;https://issues.apache.org/jira/secure/attachment/12468470/trunk-1993.txt",,,,,,,,,,,,,2.0,jesseshieh,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20392,,,Sun Jan 16 23:14:51 UTC 2011,,,,,,,,,,"0|i0g8nr:",92833,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"16/Jan/11 04:03;jbellis;Thanks for the report!

v2 should provide a similar fix while avoiding an unnecessary intermediate copy.;;;","16/Jan/11 04:59;jesseshieh;nice improvement =)

you might also be interested to know that the latest version of hadoop adds a method copyBytes to the Text object that should be able to replace getBytes and take care of this automatically for us.

see: http://svn.apache.org/viewvc/hadoop/common/trunk/src/java/org/apache/hadoop/io/Text.java?r1=953881&r2=1050070;;;","16/Jan/11 22:55;jbellis;committed v2;;;","16/Jan/11 23:14;hudson;Integrated in Cassandra-0.7 #164 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/164/])
    fix copy bounds for word Text in wordcount demo
patch by Jesse Shieh and jbellis for CASSANDRA-1993
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Bootstrap breaks data stored (missing rows, extra rows, column values modified)",CASSANDRA-1992,12495685,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,matkor,matkor,15/Jan/11 21:47,16/Apr/19 09:33,14/Jul/23 05:52,19/Jan/11 19:32,0.7.1,,,,,,0,,,,"Scenario:
Two fresh (empty /data /commitog /saved_caches dirs) cassandra installs.
Start first one.
Run data inserting program [1],  run again in verify mode - all data intact.
Bootstrap 2nd node.
Run verification again, now it fails.

Issue is very strange to me as cassandra works perfectly for me when cluster nodes stay the same for days now but any bootstrap ( 1 -> 2 nodes, 2 -> 3 nodes, 2->3 nodes RF=2) breaks data.

I am running cassandra with 1GB heap size, 32bit userland on 64bit kernels, not sure what else could matter there.
Any hints ?
Thanks in advance, regards.

[1] simple program generating data and later verifying data.
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/test.py

[2] Logs from 1st node:
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/system-3.4.log

[3] Logs from 2nd (bootstraping node)
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/system-3.8.log

","Linux 2.6.36-1 #1 SMP Tue Nov 9 09:56:02 CET 2010 x86_64 Intel(R)_Core(TM)2_Quad_CPU____Q8300__@_2.50GHz PLD Linux
glibc-2.12-4.i686
java-sun-1.6.0.22-1.i686
",ivol,jdamick,johanoskarsson,marrs,matkor,,,,,,,,,,,,,,,,";19/Jan/11 08:34;brandon.williams;3600",28800,0,3600,12%,28800,0,3600,,,,,,,,,,,,,,,"19/Jan/11 08:21;brandon.williams;1992.txt;https://issues.apache.org/jira/secure/attachment/12468740/1992.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20391,,,Tue Feb 01 13:41:15 UTC 2011,,,,,,,,,,"0|i0g8nj:",92832,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"15/Jan/11 22:27;matkor;Not sure if it is important but:

After loading data ring looks like:
Address         Status State   Load            Owns    Token
192.168.3.4     Up     Normal  59.02 KB        100.00% 0

After bootstraping:
192.168.3.4     Up     Normal  199.28 KB       56.45%  0
192.168.3.8     Up     Normal  115.03 KB       43.55%  74091174110465149971373554442555361956

Load gets tripled on 1st node.;;;","16/Jan/11 03:21;stuhood;Are both nodes reporting the same ring (via nodetool ring) after the bootstrap? The last entry in 3.8.log indicates that it thinks 3.4 is dead, but this might just be because you stopped the nodes before collecting the logs.

Also, what exactly is the error you get from your script?;;;","16/Jan/11 10:51;matkor;Stu,
Yes both nodes show same via nodetool ring: 
192.168.3.4 Up Normal 199.28 KB 56.45% 0
192.168.3.8 Up Normal 115.03 KB 43.55% 74091174110465149971373554442555361956

You are right, I stopped cluster by stopping 3.4 first. (Anyway it would be good to store information of gracefull shutdown of node in system.log, IMHO) 

Error is:
[matkor@laptop-hp ~/src/caswife]$ python test.py
column_family: 'CF0'
row_key: 'row=0 '
row_key: 'row=1 \x00'
row_key: 'row=2 \x00\x01'
row_key: 'row=3 \x00\x01\x02'
row_key: 'row=4 \x00\x01\x02\x03'
row_key: 'row=5 \x00\x01\x02\x03\x04'
row_key: 'row=6 \x00\x01\x02\x03\x04\x05'
Traceback (most recent call last):
  File ""test.py"", line 36, in <module>
    loaded_cols_dict = current_cf.get(row_key)
  File ""/usr/share/python2.7/site-packages/pycassa/columnfamily.py"", line 362, in new_f
  File ""/usr/share/python2.7/site-packages/pycassa/columnfamily.py"", line 429, in get
pycassa.cassandra.ttypes.NotFoundException: NotFoundException()

So program found 7 rows but failed to find 8th as from my understanding of pycassa.;;;","16/Jan/11 11:31;matkor;One more thing:
I repeated setup today, also reaching missing row(s) but 2nd node got different token 61078635599166706937511052402724559481
Following
http://wiki.apache.org/cassandra/Operations#Load_balancing ,
having 1st node token set to 0 and using RandomPartitioner, I would expect  2nd node toke to be set always in middle of token space to
850705917302346158658436518579420528
?

;;;","16/Jan/11 12:23;matkor;Another thing: 
By luck, I discovered that restarting one or both nodes makes data to be served again intact.
To avoid picking token 2nd node  randomness I set it explicitly to 85070591730234615865843651857942052864
.
So scenario is now:
Clean pycassa installs with tokens set to 0 and 85070591730234615865843651857942052864.

Starting first node [1], uploading data, verifing ok, ring:
192.168.3.4     Up     Normal  59.02 KB        100.00% 0

Bootstraping 2nd node, waiting to finish streaming, data verification bad:
column_family: 'CF0'
row_key: 'row=0 '
Traceback (most recent call last):
  File ""test.py"", line 36, in <module>
    loaded_cols_dict = current_cf.get(row_key)
  File ""/usr/share/python2.7/site-packages/pycassa/columnfamily.py"", line 362, in new_f
  File ""/usr/share/python2.7/site-packages/pycassa/columnfamily.py"", line 429, in get
pycassa.cassandra.ttypes.NotFoundException: NotFoundException()
Final ring (same on both nodes):
192.168.3.4     Up     Normal  199.28 KB       50.00%  0
192.168.3.8     Up     Normal  135.7 KB        50.00%  85070591730234615865843651857942052864

Restaring 192.168.3.4, data _same_ _error_ as above, ring changes to:
192.168.3.4     Up     Normal  201.51 KB       50.00%  0
192.168.3.8     Up     Normal  135.7 KB        50.00%  85070591730234615865843651857942052864

Restarting 192.168.3.8, data _verified_ _ok_ , ring changes on (same on both nodes) to:
192.168.3.4     Up     Normal  201.51 KB       50.00%  0
192.168.3.8     Up     Normal  145.8 KB        50.00%  85070591730234615865843651857942052864

[1] Logs from 1st 192.168.3.4 node:
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/logs_with_restart/system-3.4.log

[2] Logs from 2nd 192.168.3.8 node:
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/logs_with_restart/system-3.8.log

HIH, regards;;;","16/Jan/11 18:25;brandon.williams;Since the issue appears to be a missing row, can you reproduce with contrib/py_stress?;;;","16/Jan/11 20:44;matkor;Brandon, yes and no ;).
Unable with original contrib/py_stress as it uses only one CF to do all tests. Most of my issues looks like missing row in 2nd CF or broken data in 2nd CF (like contents of 1st CF injected into 2nd CF).
I slightly modified contrib/py_stress so it created 3 standard CFs and 3 super CFs [1] and allows to select one wants to operate via --column_family_idx= switch and I can reproduce:

Starting 1st node.

$ python stress.py --nodes 192.168.3.8 --operation insert  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=1       
Created keyspaces.  Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00823852300644,0
$ python stress.py --nodes 192.168.3.8 --operation insert  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=2 
Keyspace already exists.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00132475852966,0
$ python stress.py --nodes 192.168.3.8 --operation insert  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=3
Keyspace already exists.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00138550519943,0

Verification of data in each CF:
$ python stress.py --nodes 192.168.3.8 --operation read  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=3
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00282711744308,0
$ python stress.py --nodes 192.168.3.8 --operation read  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=2
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00149053096771,0
$ python stress.py --nodes 192.168.3.8 --operation read  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=1
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00125009775162,0

Bootstrap 2nd node and now failure:
$ python stress.py --nodes 192.168.3.8 --operation read  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=1
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
100,20,20,0.00376108169556,0
$ python stress.py --nodes 192.168.3.8 --operation read  --num-keys 100 --progress-interval 5  --keep-going --column_family_idx=2
Key 074 not found
Key 061 not found
Key 047 not found
( cut 40 more Key 0xx not found)
Key 047 not found
Key 042 not found
Key 058 not found
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
Key 033 not found
100,20,20,0.00241538286209,0

Similar failure for 3rd CF.

[1]: Modified stress.py from 0.7.0 with --column_family_idx= added.
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/stress.py
;;;","16/Jan/11 20:51;matkor;And again, restarting first node, cuts number of missing row by more or less half, restarting 2nd node cures all missing rows.;;;","17/Jan/11 09:33;ivol;I have the exact same problem with an existing installation and was preparing to create an issue for it, but found this issue just before creating it. I'll describe the issue I have, maybe that provides some relevant information.

I ran into this issue with Cassandra 0.7 trying to add just one node to an existing one-node cluster. The existing node contains already some data when the second node is added to the cluster. This is what I did:

Setup
I have two nodes both running on Linux; a server called 'veers' on 172.16.2.203 and a 'r2d2' on 172.16.2.206. I use Cassandra 0.7 and only change the following settings in the cassandra.yaml and log4j-server.properties (I use the default values for all other entries):

In cassandra.yaml:

initial_token: 0
data_file_directories: /vol/users/ivol/cassandra_work/data
commitlog_directory: /vol/users/ivol/cassandra_work/commitlog
saved_caches_directory: /vol/users/ivol/cassandra_work/saved_caches
seeds: 172.16.2.203
listen_address: 172.16.2.203
rpc_address: 172.16.2.203

In log4j-server.properties:

log4j.appender.R.File=/vol/users/ivol/cassandra_work/system.log


Now I start the first node and connect it using cassandra-cli. I add the following keyspace, column families and rows:

create keyspace Default;
use Default;

create column family Role;
set Role['user_1']['name'] = 'User 1';
set Role['user_2']['name'] = 'User 2';
set Role['user_3']['name'] = 'User 3';

create column family Gadget;
set Gadget['gadget_1']['name'] = 'Gadget 1';
set Gadget['gadget_2']['name'] = 'Gadget 2';
set Gadget['gadget_3']['name'] = 'Gadget 3';

After this 'list Role' and 'list Gadget' return the proper rows.

Now I append a second node to the cluster, with this configuration:

In cassandra.yaml:

initial_token:
auto_bootstrap: true
data_file_directories: /vol/users/ivol/cassandra_work/data
commitlog_directory: /vol/users/ivol/cassandra_work/commitlog
saved_caches_directory: /vol/users/ivol/cassandra_work/saved_caches
seeds: 172.16.2.203
listen_address: 172.16.2.206
rpc_address: 172.16.2.206

In log4j-server.properties:

log4j.appender.R.File=/vol/users/ivol/cassandra_work/system.log


Now I start the second node. Bootstrapping takes some time, about 2 minutes in total but finishes without any warnings or errors:

...
INFO [main] 2011-01-17 09:58:09,170 StorageService.java (line 399) Joining: getting load information
INFO [main] 2011-01-17 09:58:09,171 StorageLoadBalancer.java (line 366) Sleeping 90000 ms to wait for load information...
INFO [GossipStage:1] 2011-01-17 09:58:10,447 Gossiper.java (line 577) Node /172.16.2.203 is now part of the cluster
INFO [HintedHandoff:1] 2011-01-17 09:58:11,451 HintedHandOffManager.java (line 192) Started hinted handoff for endpoint /172.16.2.203
INFO [GossipStage:1] 2011-01-17 09:58:11,451 Gossiper.java (line 569) InetAddress /172.16.2.203 is now UP
INFO [HintedHandoff:1] 2011-01-17 09:58:11,453 HintedHandOffManager.java (line 248) Finished hinted handoff of 0 rows to endpoint /172.16.2.203
INFO [main] 2011-01-17 09:59:39,189 StorageService.java (line 399) Joining: getting bootstrap token
INFO [main] 2011-01-17 09:59:39,203 BootStrapper.java (line 148) New token will be 110533280274756817580689726417060138498 to assume load from /172.16.2.203
INFO [main] 2011-01-17 09:59:39,265 StorageService.java (line 399) Joining: sleeping 30000 ms for pending range setup
INFO [main] 2011-01-17 10:00:09,272 StorageService.java (line 399) Bootstrapping
INFO [main] 2011-01-17 10:00:09,663 CassandraDaemon.java (line 77) Binding thrift service to /172.16.2.206:9160
INFO [main] 2011-01-17 10:00:09,666 CassandraDaemon.java (line 91) Using TFramedTransport with a max frame size of 15728640 bytes.
INFO [main] 2011-01-17 10:00:09,671 CassandraDaemon.java (line 119) Listening for thrift clients...

Although everything seemed to worked just fine, when node 2 is completely finished bootstrapping the rows in the 'Role' and 'Gadget' Column Families are messed up;

list Role;

-------------------
RowKey: user_3
=> (column=6e616d65, value=557365722033, timestamp=1295254678545000)

1 Row Returned.


list Gadget;

-------------------
RowKey: user_2
=> (column=6e616d65, value=557365722032, timestamp=1295254678514000)
-------------------
RowKey: gadget_2
=> (column=6e616d65, value=4761646765742032, timestamp=1295254678805000)
-------------------
RowKey: gadget_3
=> (column=6e616d65, value=4761646765742033, timestamp=1295254679429000)
-------------------
RowKey: gadget_1
=> (column=6e616d65, value=4761646765742031, timestamp=1295254678771000)
-------------------
RowKey: user_1
=> (column=6e616d65, value=557365722031, timestamp=1295254678449000)

5 Rows Returned.

So 2 rows have been moved from CF 'Role' to 'Gadget', just by adding a node to the cluster. The actual result differs each time I try, but always some rows have been moved to some other CF. The problem seems the same as the one described by Mateusz.

I also found out that restarting the nodes seems to 'fix' the issue. Also changing the replication factor from 1 to 2 most of the times 'resolves' the issue.;;;","17/Jan/11 15:57;jbellis;if restarting nodes fixes it, it sounds like the streamed data is not getting wired in correctly to the sstabletracker;;;","19/Jan/11 08:21;brandon.williams;There were two bugs here in StreamInSession.  First, it was adding all streamed sstables to the last CFS it saw.  Secondly, secondary index generation was being performed against all sstables seen.  This patch switches from a scalar CFS and a separate list of all sstables to a hash of lists where the CFS is the key and the value is the sstables that belong to it.;;;","19/Jan/11 14:29;jbellis;I think we need to get rid of StreamInSession.table and StreamHeader.table too, then.  (But leave it on the wire protocol as an empty string, for compatibility w/ 0.7.0);;;","19/Jan/11 19:24;nickmbailey;This would be a good test to have in the distributed test set up.;;;","19/Jan/11 19:32;jbellis;Brandon said on IRC: ""maybe the table is there for some kind of reason, and the sessions are separated by keyspace.""

This is correct, so I'm going to leave the table field even though it's currently not referenced.  So it's not dead weight. I added an assert that each sstable we're adding at the end of the session belongs to the table that the sender said it was for.

Also added an assert to CFS.addSSTable to verify that the SSTR being added does belong to the CF it is being added to.

Committed w/ those two additions.;;;","19/Jan/11 19:52;hudson;Integrated in Cassandra-0.7 #177 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/177/])
    fix streaming of multiple CFs during bootstrap
patch by brandonwilliams; reviewed by jbellis for CASSANDRA-1992
;;;","01/Feb/11 13:41;jdamick;is there is any way to repair the problem without deleting all of my data? (shutting down and bringing back up did not solve the problem);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot parse generation after restart,CASSANDRA-1989,12495657,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,stuhood,stuhood,15/Jan/11 05:43,16/Apr/19 09:33,14/Jul/23 05:52,15/Jan/11 15:20,0.8 beta 1,,,,,,0,,,,"Looks like CASSANDRA-1714 broke some parsing of the generation on some restarts: haven't tracked it down yet, but this is likely to be obvious for someone who worked on that issue.
{code}java.lang.UnsupportedOperationException
        at java.nio.ByteBuffer.array(ByteBuffer.java:940)
        at org.apache.cassandra.utils.FBUtilities.byteBufferToInt(FBUtilities.java:212)
        at org.apache.cassandra.db.SystemTable.incrementAndGetGeneration(SystemTable.java:286)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:356)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:184)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:54)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:240)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:133){code}
",,,,,,,,,,,,,,,,,,,,,,";15/Jan/11 15:33;xedin;7200",7200,0,7200,100%,7200,0,7200,,,,CASSANDRA-1964,,,,,,,,,,,,,,,,,,,,,,,,,0.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20390,,,Sat Jan 15 16:24:17 UTC 2011,,,,,,,,,,"0|i0g8mv:",92829,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"15/Jan/11 15:09;xedin;I have added patch (CASSANDRA-1714-additions-to-trunk.patch) to CASSANDRA-1714 which will fix this for trunk, cassandra-0.7 branch does not have this issue.;;;","15/Jan/11 15:20;jbellis;committed patch attached to CASSANDRA-1714;;;","15/Jan/11 16:24;hudson;Integrated in Cassandra #672 (See [https://hudson.apache.org/hudson/job/Cassandra/672/])
    fix use of direct buffers from CASSANDRA-1714
patch by Pavel Yaskevich; reviewed by jbellis for CASSANDRA-1989
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
write CL > ONE regression,CASSANDRA-1986,12495612,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,kelvin,kelvin,kelvin,14/Jan/11 18:16,16/Apr/19 09:33,14/Jul/23 05:52,15/Jan/11 02:02,0.7.1,,,,,,0,,,,write CL > ONE regression by the DC refactor.,,lenn0x,stuhood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1964,,,CASSANDRA-1931,CASSANDRA-1964,,,,,,,"14/Jan/11 18:21;kelvin;CASSANDRA-1986-0001-fix-write-CL-ONE-regression.patch;https://issues.apache.org/jira/secure/attachment/12468387/CASSANDRA-1986-0001-fix-write-CL-ONE-regression.patch",,,,,,,,,,,,,,1.0,kelvin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19351,,,Sat Jan 15 02:44:44 UTC 2011,,,,,,,,,,"0|i0g8m7:",92826,,tjake,,tjake,Normal,,,,,,,,,,,,,,,,,"14/Jan/11 18:21;kelvin;The multi-DC (local / remote) write path sends local DC messages only to the target node (the lead replica), instead of to each of the following replicas.;;;","14/Jan/11 18:21;kelvin;fix regression: send local DC messages to the follower nodes, as well.;;;","14/Jan/11 22:51;tjake;ah good catch, +1;;;","15/Jan/11 02:44;hudson;Integrated in Cassandra-0.7 #160 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/160/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
read repair on CL.ONE regression,CASSANDRA-1985,12495562,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,kelvin,kelvin,14/Jan/11 00:32,16/Apr/19 09:33,14/Jul/23 05:52,15/Jan/11 19:37,0.7.1,,,,,,0,,,,"read repair w/ CL.ONE had a regression.

The RepairCallback was dropped (in the background for CL.ONE), so ReadResponseResolver : resolve() was never called.",,johanoskarsson,stuhood,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,CASSANDRA-982,,,,,,"14/Jan/11 20:33;jbellis;1985-v2.txt;https://issues.apache.org/jira/secure/attachment/12468403/1985-v2.txt","14/Jan/11 00:45;kelvin;CASSANDRA-1985-0001-fix-CL.ONE-read-repair-regression.patch;https://issues.apache.org/jira/secure/attachment/12468326/CASSANDRA-1985-0001-fix-CL.ONE-read-repair-regression.patch",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19352,,,Sat Jan 15 19:53:24 UTC 2011,,,,,,,,,,"0|i0g8lz:",92825,,tjake,,tjake,Normal,,,,,,,,,,,,,,,,,"14/Jan/11 00:33;kelvin;This regression occurred, here.;;;","14/Jan/11 00:45;kelvin;ensure RR happens in the background.;;;","14/Jan/11 04:10;jbellis;It's possible that we missed something, but we we did test read repair post-982.  This is the part that does the resolve:

                if (repairs.contains(command))
                    repairExecutor.schedule(new RepairRunner(readCallback.resolver, command, endpoints), DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
;;;","14/Jan/11 18:00;kelvin;Yes, you're right, that does schedule it, once.

The process for CL.ONE is:
1) schedule RR for data+digest and watch for a DigestMismatchException,
2) catch DME and call repair() to do a RR for data-only.

However, the handler for the second RR (that repair() returns) is never used.  So, even though it's collecting all the data repair messages, the RRR's resolve() never gets called.
;;;","14/Jan/11 18:50;jbellis;The ""second RR"" (that is, the second read request, for performing repair when a mismatch was detected by the digest read) is this one:

{code}
                RepairCallback<Row> handler = repair(command, endpoints);
...
                repairResponseHandlers.add(handler);
...
            for (RepairCallback<Row> handler : repairResponseHandlers)
            {
                try
                {
                    Row row = handler.get();
{code};;;","14/Jan/11 19:44;kelvin;Yes, that's correct for read CL > ONE.  A quorum / all read goes through that path.  However, the CL.ONE case does not go through that code path.

The branch in the code is in fetchRows(...) when it checks for randomlyReadRepair(...).  If the targets > handler.blockfor, it does a background repair via RepairRunner in service.StorageProxy.  i.e. it won't go through the block of code you pasted, because a DigestMismatchException won't be thrown for CL.ONE.

Now, let's look at RepairRunner : runMayThrow.  It calls repair(command, endpoints), but the RepairCallback<row> that is returned by repair(...) is dropped on the floor.  So, resolve is never called on that RepairCallback's ReadResponseResolver.

The above error was found via my own set of distributed tests.;;;","14/Jan/11 19:48;jbellis;I get it now: the callback from the repair() call in RepairRunner is the one that we don't resolve.;;;","14/Jan/11 20:33;jbellis;v2 keeps the resolve off the response stage, which we want to keep very low latency.;;;","14/Jan/11 22:34;tjake;+1;;;","15/Jan/11 19:37;jbellis;committed;;;","15/Jan/11 19:53;hudson;Integrated in Cassandra-0.7 #162 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/162/])
    fix read repair on CL.ONE regression
patch by jbellis; reviewed by tjake for CASSANDRA-1985
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The writing of statistics in SSTableWrite.Builder has been mistakenly removed by #1072,CASSANDRA-1981,12495531,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,13/Jan/11 19:27,16/Apr/19 09:33,14/Jul/23 05:52,13/Jan/11 19:43,0.8 beta 1,,,,,,0,,,,Everything's in the summary,,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,"13/Jan/11 19:27;slebresne;0001-Add-back-writting-of-statistics-in-SSTableWriter.patch;https://issues.apache.org/jira/secure/attachment/12468283/0001-Add-back-writting-of-statistics-in-SSTableWriter.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,19353,,,Thu Jan 13 21:14:19 UTC 2011,,,,,,,,,,"0|i0g8l3:",92821,,tjake,,tjake,Low,,,,,,,,,,,,,,,,,"13/Jan/11 19:43;tjake;Committed!;;;","13/Jan/11 21:14;hudson;Integrated in Cassandra #667 (See [https://hudson.apache.org/hudson/job/Cassandra/667/])
    turn back on sstable statistics generation after it was removed by #1072 mistakenly.

Patch by Sylvain Lebresne; reviewed by Jake Luciani for CASSANDRA-1981
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraServiceDataCleaner.prepare() fails with IOException.,CASSANDRA-1979,12495504,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,zznate,cng1066,cng1066,13/Jan/11 16:09,16/Apr/19 09:33,14/Jul/23 05:52,17/Jan/11 17:07,0.7.1,,,,,,0,,,,"CassandraServiceDataCleaner.prepare() fails with an IOException if run in isolation.  It seems that initializing the DataDescriptor creates a new CommitLog file, and then the cleaner tries to delete this file and fails.

16:06:07.204 [main] INFO  o.a.c.config.DatabaseDescriptor - Loading settings from file:/C:/workspace/sandbox/target/classes/cassandra.yaml
16:06:07.282 [main] DEBUG o.a.c.config.DatabaseDescriptor - Syncing log with a period of 10000
16:06:07.282 [main] INFO  o.a.c.config.DatabaseDescriptor - DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
16:06:07.797 [main] DEBUG o.a.c.config.DatabaseDescriptor - setting auto_bootstrap to false
16:06:07.797 [main] INFO  o.a.c.db.commitlog.CommitLogSegment - Creating new commitlog segment target/var/lib/cassandra/commitlog\CommitLog-1294934767797.log
16:06:07.813 [main] DEBUG o.apache.cassandra.io.util.FileUtils - Deleting CommitLog-1294934767797.log
Exception in thread ""main"" java.io.IOException: Failed to delete C:\workspace\sandbox\target\var\lib\cassandra\commitlog\CommitLog-1294934767797.log
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:54)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:201)
	at org.apache.cassandra.contrib.utils.service.CassandraServiceDataCleaner.cleanDir(CassandraServiceDataCleaner.java:99)
	at org.apache.cassandra.contrib.utils.service.CassandraServiceDataCleaner.cleanupDataDirectories(CassandraServiceDataCleaner.java:53)
	at org.apache.cassandra.contrib.utils.service.CassandraServiceDataCleaner.prepare(CassandraServiceDataCleaner.java:44)
	at cng.sandbox.App.main(App.java:15)

This also seems to leave a bunch of threads running in the background, so the process has to be manually killed.

This was tested with the javautils in the 0.7.0 branch.",Windows XP,cng1066,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/11 16:56;zznate;1979.txt;https://issues.apache.org/jira/secure/attachment/12468574/1979.txt",,,,,,,,,,,,,,1.0,zznate,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20389,,,Mon Jan 17 19:48:03 UTC 2011,,,,,,,,,,"0|i0g8kn:",92819,,,,,Normal,,,,,,,,,,,,,,,,,"14/Jan/11 22:55;zznate;This code path had not been tested in a while. Test case had the following issues:
- not using bytebuffer
- storage-config param from 0.6
- non-framed transport

Patch brings everything up to date with current trunk.;;;","14/Jan/11 23:01;zznate;I'm not sure this belongs in contrib anymore as it will just get out of date again. I would prefer we remove it in favor of doing something smarter in refactoring out o.a.c.SchemaLoader and friends in the test/unit tree to a module of some sort and including that on the test classpath of the main build file. (In conjunction with CASSANDRA-1848 perhaps?);;;","15/Jan/11 19:34;jbellis;patch doesn't apply to 0.7 for me w/ p0 or p1 option;;;","17/Jan/11 16:56;zznate;Oops - diff'ed from wrong level. New patched diff'ed from top level.;;;","17/Jan/11 17:07;jbellis;committed

ultimately I think we should move this into Hector or a github project (CASSANDRA-1805) although it's possible we could make changes to core to make this easier.;;;","17/Jan/11 17:22;zznate;Per my comments on CASSANDRA-1805 I'm fine appropriating this. We have an issue open to break out some other testing utils into a non-hector-core dependent 'test-utils' module anyhoo. ;;;","17/Jan/11 19:48;hudson;Integrated in Cassandra-0.7 #168 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/168/])
    fixes for contrib/javautils
patch by Nate McCall for CASSANDRA-1979
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableWriter.writeStatistics is serializing incorrect data.,CASSANDRA-1976,12495423,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,gdusbabek,gdusbabek,12/Jan/11 21:45,16/Apr/19 09:33,14/Jul/23 05:52,18/Jan/11 22:58,0.7.1,,,,,,0,,,,it is serializing rowSizes twice instead of serializing rowSizes and columnCounts.,,,,,,,,,,,,,,,,,,,,,,";24/Jan/11 17:18;jbellis;600",,0,600,,,0,600,,,,,,,,,,,,,,,"18/Jan/11 22:45;jbellis;1976.txt;https://issues.apache.org/jira/secure/attachment/12468695/1976.txt",,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20388,,,Wed Jan 19 00:28:14 UTC 2011,,,,,,,,,,"0|i0g8jz:",92816,,gdusbabek,,gdusbabek,Low,,,,,,,,,,,,,,,,,"18/Jan/11 22:48;gdusbabek;+1;;;","18/Jan/11 22:58;jbellis;committed;;;","19/Jan/11 00:28;hudson;Integrated in Cassandra-0.7 #174 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/174/])
    fix writing SSTable column count statistics
patch by jbellis; reviewed by gdusbabek for CASSANDRA-1976
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress.java -k doesn't keep going,CASSANDRA-1973,12495398,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,brandon.williams,brandon.williams,12/Jan/11 18:27,16/Apr/19 09:33,14/Jul/23 05:52,18/Jan/11 20:22,0.7.1,,,,,,0,,,,"stress.java's -k option doesn't work correctly.  In the face of many errors, it ends up printing 'null' a bunch and then exiting.",,,,,,,,,,,,,,,,,,,,,,";16/Jan/11 13:25;xedin;3600",3600,0,3600,100%,3600,0,3600,,,,,,,,,,,,,,,"18/Jan/11 20:10;xedin;CASSANDRA-1973-v2.patch;https://issues.apache.org/jira/secure/attachment/12468677/CASSANDRA-1973-v2.patch","16/Jan/11 13:24;xedin;CASSANDRA-1973.patch;https://issues.apache.org/jira/secure/attachment/12468490/CASSANDRA-1973.patch",,,,,,,,,,,,,2.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20387,,,Tue Jan 18 21:28:25 UTC 2011,,,,,,,,,,"0|i0g8jb:",92813,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,"12/Jan/11 21:28;xedin;Can't reproduce issue on the latest stress build it just prints Key XXX not found. and then exits after all -n keys are done.

Tried on empty db to read 100000 keys - standard and super (on both Standard and Super CF).

Can you please provide a better description how to reproduce this issue?;;;","12/Jan/11 21:30;brandon.williams;I was trying to insert a node to death.  As death approached, stress.java died.;;;","12/Jan/11 21:33;xedin;Ok gotcha, I will take a look at this. Thank you!;;;","18/Jan/11 19:48;brandon.williams;Better, but it looks like the exception isn't trickling up:

Error while inserting key 4686646 - null
Error while inserting key 4697766 - null
Error while inserting key 4297775 - null
Error while inserting key 0609162 - null
Error while inserting key 0003898 - null
Error while inserting key 2719911 - null
Error while inserting key 4308726 - null
Error while inserting key 4453292 - null
Error while inserting key 4203275 - null
Error while inserting key 2625669 - null
Error while inserting key 1342402 - null
Error while inserting key 1247940 - null

I'm pretty sure these were TimeoutExceptions so it'd be nice to have that reflected.;;;","18/Jan/11 19:51;xedin;I can make it write exception class too e.g, ""Error while inserting key 1247940 - (ExceptionClass): null"", what do you think?;;;","18/Jan/11 20:22;brandon.williams;v2 looks good, committed.  Thanks!;;;","18/Jan/11 21:28;hudson;Integrated in Cassandra-0.7 #172 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/172/])
    Improve stress.java error handling.
Patch by Pavel Yaskevich, reviewed by brandonwilliams for CASSANDRA-1973
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default concurrency values are improperly proportioned,CASSANDRA-1972,12495338,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,12/Jan/11 07:04,16/Apr/19 09:33,14/Jul/23 05:52,14/Jan/11 19:14,0.7.1,,,,,,0,,,,"The ""default""/""suggested"" {{concurrent_reads}} value is much too low. It assumes that CPU will be the bottleneck, rather than IO, and for most deployments, this will not be the case. Additionally it is better to be queued for IO in the kernel or on your device than in user space, because the former work to optimize queue order.

Additionally, reads are much cheaper than writes in terms of CPU time (since writes can experience contention due to retries), so while {{concurrent_writes}} should probably factor in the number of cores on the machine, {{concurrent_reads}} should probably be calculated purely by number of spindles.",,mdennis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/11 04:53;stuhood;0001-Recommendation-changes.txt;https://issues.apache.org/jira/secure/attachment/12468346/0001-Recommendation-changes.txt",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20386,,,Fri Jan 14 19:55:03 UTC 2011,,,,,,,,,,"0|i0g8j3:",92812,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"12/Jan/11 18:07;scode;I agree in general, unless the intent is to make out-of-the-box stress testing on memory-bound workloads optimized.

In particular I agree about being queued in the kernel, and even further down on devices or RAID controllers. Propagating outstanding I/O requests all the way down, within reason, is key to saturating the capacity of underlying spindles.;;;","12/Jan/11 18:24;jbellis;Yes, this sounds like a pretty straightforward win.  Have you done testing on better values, Stu?;;;","14/Jan/11 04:08;stuhood;Based on IO/seek bound tests on two machines with 4 and 12 drives in RAID-0 and RAID-10, we saw the best performance with around 20 threads per drive. Not everyone will be completely IO bound though (hopefully _something_ is in cache), so perhaps a better recommended value is 16 threads per drive. I'll write a new blurb for the config file.;;;","14/Jan/11 19:14;jbellis;committed, thanks!;;;","14/Jan/11 19:55;hudson;Integrated in Cassandra-0.7 #157 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/157/])
    update concurrent_reads default setting + comments
patch by Stu Hood; reviewed by jbellis for CASSANDRA-1972
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MutationTest of the distributed-test suite fails,CASSANDRA-1964,12495250,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,tawamudu,tawamudu,11/Jan/11 14:08,16/Apr/19 09:33,14/Jul/23 05:52,19/Jan/11 19:53,0.7.1,0.8 beta 1,,,,,0,,,,"MutationTest of the distributed-test test suite causes errors on trunk.

To reproduce, issue:

ant distributed-test -Dwhirr.config=<path_to_whirr_config_file>

from the project root.

relevant whirr configuration settings used:

whirr.service-name=cassandra
whirr.cluster-name=cassandra_test
whirr.instance-templates=4 cassandra
whirr.version=0.3.0-incubating-SNAPSHOT
whirr.location-id=us-west-1
whirr.image-id=us-west-1/ami-16f3a253
whirr.hardware-id=m1.large
whirr.blobstore.provider=s3
whirr.blobstore.container=tawamuducassandratests
whirr.provider=ec2
whirr.run-url-base=http://hoodidge.net/scripts/

Traceback:

distributed-test:   
     [echo] running distributed tests
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/Users/mallen/Desktop/cassandra-trunk/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
    [junit] Testsuite: org.apache.cassandra.MovementTest
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 446.65 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] SLF4J: Class path contains multiple SLF4J bindings.
    [junit] SLF4J: Found binding in [jar:file:/Users/mallen/Desktop/cassandra-trunk/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    [junit] SLF4J: Found binding in [jar:file:/Users/mallen/Desktop/cassandra-trunk/build/test/lib/jars/whirr-cli-0.3.0-incubating-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    [junit] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
    [junit]  WARN 12:12:46,654 over limit 471283/262144: wrote temp file
    [junit]  WARN 12:12:48,572 over limit 374979/262144: wrote temp file
    [junit]  WARN 12:12:50,701 over limit 892174/262144: wrote temp file
    [junit]  WARN 12:12:54,442 over limit 612358/262144: wrote temp file
    [junit] ------------- ---------------- ---------------
    [junit] Testsuite: org.apache.cassandra.MutationTest
    [junit] Tests run: 4, Failures: 0, Errors: 3, Time elapsed: 110.971 sec
    [junit] 
    [junit] Testcase: testInsert(org.apache.cassandra.MutationTest):    Caused an ERROR
    [junit] null
    [junit] NotFoundException()
    [junit]     at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:6900)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:568)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:541)
    [junit]     at org.apache.cassandra.MutationTest.getColumn(MutationTest.java:210)
    [junit]     at org.apache.cassandra.MutationTest.testInsert(MutationTest.java:66)
    [junit] 
    [junit] 
    [junit] Testcase: testWriteAllReadOne(org.apache.cassandra.MutationTest):   Caused an ERROR
    [junit] null
    [junit] NotFoundException()
    [junit]     at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:6900)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:568)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:541)
    [junit]     at org.apache.cassandra.MutationTest.getColumn(MutationTest.java:210)
    [junit]     at org.apache.cassandra.MutationTest.testWriteAllReadOne(MutationTest.java:87)
    [junit] 
    [junit] 
    [junit] Testcase: testWriteOneReadAll(org.apache.cassandra.MutationTest):   Caused an ERROR
    [junit] null
    [junit] TimedOutException()
    [junit]     at org.apache.cassandra.thrift.Cassandra$insert_result.read(Cassandra.java:15392)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.recv_insert(Cassandra.java:907)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.insert(Cassandra.java:879)
    [junit]     at org.apache.cassandra.MutationTest.insert(MutationTest.java:202)
    [junit]     at org.apache.cassandra.MutationTest.testWriteOneReadAll(MutationTest.java:185)
    [junit] 
    [junit] 
    [junit] TEST org.apache.cassandra.MutationTest FAILED
    [junit] Tests FAILED

BUILD FAILED
/Users/mallen/Desktop/cassandra-trunk/build.xml:557: The following error occurred while executing this line:
/Users/mallen/Desktop/cassandra-trunk/build.xml:540: Some distributed test(s) failed.

Total time: 10 minutes 15 seconds
","OS X Version 10.6.6
java version ""1.6.0_22""
Java(TM) SE Runtime Environment (build 1.6.0_22-b04-307-10M3261)
Java HotSpot(TM) 64-Bit Server VM (build 17.1-b03-307, mixed mode)
Apache Ant version 1.8.1 compiled on September 21 2010",johanoskarsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-1986,CASSANDRA-1989,CASSANDRA-2005,,,CASSANDRA-1965,CASSANDRA-1988,CASSANDRA-1986,,,,,,"15/Jan/11 04:49;stuhood;0001-Add-a-builder-for-a-retrying-get-attempt-and-use-to-im.txt;https://issues.apache.org/jira/secure/attachment/12468443/0001-Add-a-builder-for-a-retrying-get-attempt-and-use-to-im.txt",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20385,,,Wed Jan 19 20:15:44 UTC 2011,,,,,,,,,,"0|i0g8hb:",92804,,xedin,,xedin,Low,,,,,,,,,,,,,,,,,"11/Jan/11 19:27;stuhood;I think we have a race condition on our hands. Trunk tests just passed for me.

I'll see if we can add more blocking.;;;","14/Jan/11 04:59;stuhood;Attaching a fix for the first failing testcase: when doing a write at ONE and a read at ONE, we retry to ensure that the read eventually succeeds.

The other two failures are more interesting, and are not bugs in the testcase. Kelvin is investigating, but it appears that reads and writes at ALL are not blocking appropriately on the server side: he'll open a separate ticket for this tomorrow.;;;","14/Jan/11 21:29;kelvin;bug found.;;;","14/Jan/11 21:29;kelvin;may also be related.;;;","15/Jan/11 04:49;stuhood;Ready for review: CASSANDRA-1986 is closed, and we pass all tests consistently.

I was a little bit uncomfortable having to set such high retry timeouts in cases where we need to wait for gossip (in particular when waiting for the ""fail fast"" behaviour from CASSANDRA-1803 to kick in), but it is a bit of a necessary evil with cloud providers. I opened CASSANDRA-1988 to discuss ways to improve this behaviour.;;;","15/Jan/11 05:45;stuhood;Ack... after a rebase, it looks like we're blocked again: this time by CASSANDRA-1989. These tests restart the nodes multiple times.;;;","16/Jan/11 15:10;jbellis;With 1989 committed, is this patch good to go?;;;","16/Jan/11 22:37;stuhood;+1
This is ready.;;;","16/Jan/11 23:13;jbellis;Should have asked earlier -- for 0.7.1 or just trunk?  (I ask b/c CASSANDRA-1989 just affected trunk.);;;","17/Jan/11 00:37;stuhood;Both of 0.7.1 and 0.8 should receive this fix.;;;","19/Jan/11 11:38;xedin;Tested on both trunk and cassandra-0.7 branches, patch fixes TimedOutException and NotFoundException in both cases, code looks good but maybe move RetryingAction to the BaseTest to make it reusable?;;;","19/Jan/11 19:15;stuhood;> maybe move RetryingAction to the BaseTest to make it reusable
Good idea... I've actually done this in a separate issue: CASSANDRA-2005;;;","19/Jan/11 19:45;xedin;Great, I have nothing to add then! :);;;","19/Jan/11 19:53;jbellis;committed;;;","19/Jan/11 20:15;hudson;Integrated in Cassandra-0.7 #178 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/178/])
    fix distributed-test MutationTest
patch by stuhood; reviewed by Pavel Yaskevich for CASSANDRA-1964
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Equality problem in schema updates,CASSANDRA-1962,12495222,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,stuhood,stuhood,stuhood,11/Jan/11 09:53,16/Apr/19 09:33,14/Jul/23 05:52,11/Jan/11 14:24,0.7.1,,,,,,0,,,,"CFMetaData.apply uses equals to compare objects that are not the same class: this may work now, but we shouldn't rely on that behaviour.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/11 09:54;stuhood;0001-Strings-never-equal-UTFs.txt;https://issues.apache.org/jira/secure/attachment/12467990/0001-Strings-never-equal-UTFs.txt",,,,,,,,,,,,,,1.0,stuhood,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20384,,,Tue Jan 11 17:45:36 UTC 2011,,,,,,,,,,"0|i0g8gv:",92802,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,"11/Jan/11 10:21;slebresne;+1;;;","11/Jan/11 14:24;jbellis;committed;;;","11/Jan/11 17:45;hudson;Integrated in Cassandra-0.7 #151 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/151/])
    ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ArrayIndexOutOfBoundsException while executing repair on a freshly added node (0.7.0),CASSANDRA-1959,12495158,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,tbritz,tbritz,10/Jan/11 19:03,16/Apr/19 09:33,14/Jul/23 05:52,11/Jan/11 23:41,0.6.10,0.7.1,,,,,0,,,,"Hi,

I added a node to the cluster (20 nodes in total) and ran repair on it after a while.

The repair still runs, but there are errors in the log file (see below).

Some of the data in the clsuter has been filled with rc-4. The cluster runs on version 0.7.0 (the release linked on the main cassandra web site).

Any ideas what might cause this?
(PS. 0.7.0 turns up as unreleased version in Affects Version/s:) 


INFO [CompactionExecutor:1] 2011-01-10 19:55:20,684 SSTableReader.java (line 158) Opening /hd2/cassandra_md5/data/table_x/table_x-e-4
 INFO [CompactionExecutor:1] 2011-01-10 19:55:20,775 SSTableReader.java (line 158) Opening /hd2/cassandra_md5/data/table_x/table_x_meta-e-14
ERROR [RequestResponseStage:3] 2011-01-10 19:55:20,856 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.fastRemove(ArrayList.java:441)
        at java.util.ArrayList.remove(ArrayList.java:424)
        at com.google.common.collect.AbstractMultimap.remove(AbstractMultimap.java:219)
        at com.google.common.collect.ArrayListMultimap.remove(ArrayListMultimap.java:60)
        at org.apache.cassandra.net.MessagingService.responseReceivedFrom(MessagingService.java:436)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:40)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:63)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [RequestResponseStage:3] 2011-01-10 19:55:20,856 AbstractCassandraDaemon.java (line 91) Fatal exception in thread Thread[RequestResponseStage:3,5,main]
java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.fastRemove(ArrayList.java:441)
        at java.util.ArrayList.remove(ArrayList.java:424)
        at com.google.common.collect.AbstractMultimap.remove(AbstractMultimap.java:219)
        at com.google.common.collect.ArrayListMultimap.remove(ArrayListMultimap.java:60)
        at org.apache.cassandra.net.MessagingService.responseReceivedFrom(MessagingService.java:436)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:40)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:63)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
",,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,"11/Jan/11 17:24;jbellis;1959-v2.txt;https://issues.apache.org/jira/secure/attachment/12468027/1959-v2.txt","11/Jan/11 21:03;messi;post-1959-v2-cleanup.patch.txt;https://issues.apache.org/jira/secure/attachment/12468057/post-1959-v2-cleanup.patch.txt",,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20382,,,Wed Jan 12 00:38:08 UTC 2011,,,,,,,,,,"0|i0g8g7:",92799,,messi,,messi,Normal,,,,,,,,,,,,,,,,,"11/Jan/11 08:21;messi;Looks like MessagingService.targets needs to be synchronized.;;;","11/Jan/11 16:46;nickmbailey;Looks like this was caused by CASSANDRA-1905. Or maybe just exposed by it.

I think this may be particularly nasty due to the fact that droped responses will lead to inflated timings reported to the DynamicEndpointSnitch.  I'm not sure but I think this may be contributing to flapping in another case where I've seen this error.;;;","11/Jan/11 17:24;jbellis;Right, this is from 1905 and there's definitely a concurrency bug here.

I don't want to use the synchronized multimap option though since *every* message hits this so we need better concurrency.  v2 is a patch that replaces multimap with a manually constructed Map of Sets.;;;","11/Jan/11 21:03;messi;I saw that you added putTarget() as instance method although targets itself is a static field. This patch cleans up these inconsistencies and makes MessagingService a true singleton.;;;","12/Jan/11 00:37;hudson;Integrated in Cassandra-0.7 #152 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/152/])
    convert MessagingService into a true singleton
patch by Folke Behrens; reviewed by jbellis for CASSANDRA-1959
fix race condition in MessagingService.targets
patch by jbellis; reviewed by Folke Behrens for CASSANDRA-1959
;;;","12/Jan/11 00:38;hudson;Integrated in Cassandra-0.6 #49 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/49/])
    backport CASSANDRA-1959 from 0.7
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Addition of internode buffering broke Streaming,CASSANDRA-1943,12494805,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,,stuhood,stuhood,06/Jan/11 03:18,16/Apr/19 09:33,14/Jul/23 05:52,06/Jan/11 23:34,0.6.10,0.7.1,,,,,0,,,,Adding internode buffering broke StreamingTransferTest in the 0.7 branch. Bisected to r1055313,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/11 22:58;stuhood;0.6-0001-Don-t-begin-buffering-a-connection-until-we-ve-determi.txt;https://issues.apache.org/jira/secure/attachment/12467684/0.6-0001-Don-t-begin-buffering-a-connection-until-we-ve-determi.txt","06/Jan/11 04:33;stuhood;0001-Don-t-begin-buffering-a-connection-until-we-ve-determi.txt;https://issues.apache.org/jira/secure/attachment/12467613/0001-Don-t-begin-buffering-a-connection-until-we-ve-determi.txt",,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20378,,,Fri Jan 07 01:38:13 UTC 2011,,,,,,,,,,"0|i0g8cv:",92784,,gdusbabek,,gdusbabek,Critical,,,,,,,,,,,,,,,,,"06/Jan/11 04:33;stuhood;Streaming connections don't use the InputStream implementation to read their data: they bypass all buffering and use the SocketChannel directly. By buffering immediately after opening the connection, we were buffering in the beginning of the streamed file.;;;","06/Jan/11 04:36;stuhood;Also affects trunk.;;;","06/Jan/11 07:09;stuhood;Actually, the IOError we used to throw in the constructor should probably become a log statement like the other exceptions in run().;;;","06/Jan/11 21:45;gdusbabek;+1;;;","06/Jan/11 22:58;stuhood;And a version rebased for 0.6;;;","06/Jan/11 23:34;brandon.williams;Committed.;;;","07/Jan/11 01:38;hudson;Integrated in Cassandra-0.6 #48 (See [https://hudson.apache.org/hudson/job/Cassandra-0.6/48/])
    Don't begin buffering a connection until we've determined the type.
Patch by Stu Hood, reviewed by gdusbabek for CASSANDRA-1943
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misuses of ByteBuffer absolute get (wrongfully adding arrayOffset to the index),CASSANDRA-1939,12494734,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,05/Jan/11 16:21,16/Apr/19 09:33,14/Jul/23 05:52,07/Jan/11 08:34,0.7.0,,,,,,0,,,,ByteBuffer.arrayOffset() should not be added to the argument of an absolute get. ,,johanoskarsson,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,"05/Jan/11 16:22;slebresne;0001-Remove-addition-of-arrayOffset-in-ByteBuffer-absolut.patch;https://issues.apache.org/jira/secure/attachment/12467553/0001-Remove-addition-of-arrayOffset-in-ByteBuffer-absolut.patch",,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20377,,,Thu Jan 06 01:06:29 UTC 2011,,,,,,,,,,"0|i0g8c7:",92781,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,"06/Jan/11 01:06;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update token metadata for NORMAL state,CASSANDRA-1934,12494667,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,nickmbailey,nickmbailey,05/Jan/11 02:06,16/Apr/19 09:33,14/Jul/23 05:52,20/Jan/11 21:37,0.7.1,,,,,,0,,,,"The handleStateNormal() method in StorageService.java doesn't update the tokenmetadata. This means if you try to decommission a node but for some reason it fails, and then you bring the node back up, all other nodes will see it in a 'Leaving' state. When the state jumps back to normal they should update the token metadata to reflect that.

This also means you won't be able to call 'removetoken' on that node, unless you restart another node in the cluster in order to put it back in a 'normal' state.",,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,"10/Jan/11 18:10;brandon.williams;1934.txt;https://issues.apache.org/jira/secure/attachment/12467903/1934.txt",,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20376,,,Thu Jan 20 21:53:26 UTC 2011,,,,,,,,,,"0|i0g8b3:",92776,,nickmbailey,,nickmbailey,Low,,,,,,,,,,,,,,,,,"10/Jan/11 18:10;brandon.williams;Patch to allow recovery to normal if we knew about the endpoint previously and its IP has not changed.;;;","20/Jan/11 20:12;jbellis;How does updating when endpoint.equals(currentNode) solve the problem?

It looks to me like the relevant path is

{code}
            logger_.info(String.format(""Nodes %s and %s have the same token %s.  %s is the new owner"",
                                       endpoint, currentNode, token, endpoint));
            tokenMetadata_.updateNormalToken(token, endpoint);
{code}

which is already doing The Right Thing.;;;","20/Jan/11 20:16;brandon.williams;If you are comparing endpoint to currentNode and they are the same, the generation difference can never be > 0;;;","20/Jan/11 20:22;jbellis;my point is that currentNode should always have the right view of itself anyway, it's the other nodes we are worried about (according to the issue description anyway). ;;;","20/Jan/11 20:35;brandon.williams;currentNode (which failed decom and then restarted) does have the correct view of itself (normal) but
{code}
        else if (endpoint.equals(currentNode))
        {
            // nothing to do
        }
{code}

Prevents the other nodes from updating the state from 'Leaving' back to 'normal'.

We almost update the state here:
{code}
        else if (Gossiper.instance.compareEndpointStartup(endpoint, currentNode) > 0)
        {
            logger_.info(String.format(""Nodes %s and %s have the same token %s.  %s is the new owner"",
                                       endpoint, currentNode, token, endpoint));
            tokenMetadata_.updateNormalToken(token, endpoint);
            if (!isClientMode)
                SystemTable.updateToken(endpoint, token);
        }
{code}
But don't because the generations will always be equal (in other words, this code only handles a *different* node updating the state, not the same node returning)
;;;","20/Jan/11 21:24;jbellis;I still don't get it -- endpoint.equals(currentNode) will never be true on the other nodes.;;;","20/Jan/11 21:29;jbellis;bq. I still don't get it - endpoint.equals(currentNode) will never be true on the other nodes. 

I was thinking that currentNode == localAddress but that is not the case.  It makes more sense now! :);;;","20/Jan/11 21:30;jbellis;+1;;;","20/Jan/11 21:30;brandon.williams;Yes it will.  endpoint is the node entering the ring, currentNode is the node that currently has the token passed in (which in this case will be endpoint's token also), but does not mean 'my address' to the node making the evaluation.  Thus, if we see the same endpoint and token, we do nothing in the current code, causing the state to never get updated.  ;;;","20/Jan/11 21:32;jbellis;(renamed currentNode to currentOwner so I can remember the difference in the future.);;;","20/Jan/11 21:37;brandon.williams;committed.;;;","20/Jan/11 21:53;hudson;Integrated in Cassandra-0.7 #186 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/186/])
    Update token metadata for NORMAL state when endpoint has not changed.
Patch by brandonwilliams, reviewed by jbellis for CASSANDRA-1934
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internal error processing insert java.lang.AssertionError  at org.apache.cassandra.service.StorageProxy.sendMessages(StorageProxy.java:219),CASSANDRA-1931,12494562,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tjake,kmueller,kmueller,04/Jan/11 05:29,16/Apr/19 09:33,14/Jul/23 05:52,04/Jan/11 21:34,0.7.1,,,,,,0,,,,"ERROR [pool-1-thread-137] 2011-01-03 18:22:21,751 Cassandra.java (line 2960) Internal error processing insert
java.lang.AssertionError
        at org.apache.cassandra.service.StorageProxy.sendMessages(StorageProxy.java:219)
        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:174)
        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:412)
        at org.apache.cassandra.thrift.CassandraServer.insert(CassandraServer.java:349)
        at org.apache.cassandra.thrift.Cassandra$Processor$insert.process(Cassandra.java:2952)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)",Linux Fedora 12 x86_64,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,CASSANDRA-1986,,,,,,"04/Jan/11 16:49;tjake;ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-1931-change-sendMessages-to-handle-many-mess.txt;https://issues.apache.org/jira/secure/attachment/12467437/ASF.LICENSE.NOT.GRANTED--0001-CASSANDRA-1931-change-sendMessages-to-handle-many-mess.txt",,,,,,,,,,,,,,1.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20374,,,Tue Jan 04 21:54:50 UTC 2011,,,,,,,,,,"0|i0g8af:",92773,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"04/Jan/11 05:33;jbellis;Looks like this was introduced by CASSANDRA-1530.;;;","04/Jan/11 16:53;tjake;sendMessages was incorrectly assuming one message would be sent to a DC at a time, which isn't the case for batch mutations.

This patch groups the enpoints to a message so many messages can be sent.;;;","04/Jan/11 21:34;jbellis;committed;;;","04/Jan/11 21:54;hudson;Integrated in Cassandra-0.7 #144 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/144/])
    fix batch mutations post-#1530
patch by tjake; reviewed by jbellis for CASSANDRA-1931
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop Integration doesn't work when one node is down,CASSANDRA-1927,12494462,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mck,uctopcu,uctopcu,03/Jan/11 01:15,16/Apr/19 09:33,14/Jul/23 05:52,03/Jan/11 19:43,0.7.1,,,,,,1,,,,"using the same directives in the sample code:

When I start the CFInputFormat to read a CF in a keyspace of RF=3 on a 4-node cluster:
- If all the nodes are all up, everything works fine and I don't have any problems walking through the all data in the CF, however
- If there's a node down, the hadoop job does not even start, just dies without any errors or exceptions.

So I'm really sorry for not being able to post any errors or exceptions, though it's really easy to reproduce. Just startup a cluster and take one node down and you're there :)",,mck,stuhood,uctopcu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/11 17:41;mck;CASSANDRA-1927.patch;https://issues.apache.org/jira/secure/attachment/12467331/CASSANDRA-1927.patch",,,,,,,,,,,,,,1.0,mck,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,20373,,,Mon Jan 03 20:23:29 UTC 2011,,,,,,,,,,"0|i0g89j:",92769,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,"03/Jan/11 08:25;mck;Client side (hadoop job):

java.io.IOException: Could not get input splits
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getSplits(ColumnFamilyInputFormat.java:127)
	at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:885)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:779)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:432)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:447)
	at no.finntech.countstats.reduce.FakeAdCounterTableReduce.run(FakeAdCounterTableReduce.java:421)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at no.finntech.countstats.reduce.FakeAdCounterTableReduce.main(FakeAdCounterTableReduce.java:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Caused by: java.util.concurrent.ExecutionException: java.io.IOException: unable to connect to server
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getSplits(ColumnFamilyInputFormat.java:123)
	... 13 more
Caused by: java.io.IOException: unable to connect to server
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.createConnection(ColumnFamilyInputFormat.java:212)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getSubSplits(ColumnFamilyInputFormat.java:187)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.access$200(ColumnFamilyInputFormat.java:74)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat$SplitCallable.call(ColumnFamilyInputFormat.java:160)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat$SplitCallable.call(ColumnFamilyInputFormat.java:145)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
	at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.createConnection(ColumnFamilyInputFormat.java:208)
	... 9 more
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
	at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
	at java.net.Socket.connect(Socket.java:525)
	at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
	... 11 more

;;;","03/Jan/11 08:27;mck;There's a todo comment in ColumnFamilyInputFormat
 // TODO handle failure of range replicas & retry

line 198 only tries the first endpoint. a loop on the TException trying the next endpoint is needed.;;;","03/Jan/11 10:45;mck;Utku: are you able to test this patch?

( It didn't work for me because RF was never really set to 3. using cassandra-cli ""describe keyspace xxx"" reported ""Replication Factor: 1"" )  :-$

;;;","03/Jan/11 11:00;uctopcu;Mck: Right now I can't access to our compilation server. However I can replace the running binaries and test them if I have the patched rc4. Can you somehow provide me the compiled package?;;;","03/Jan/11 11:08;mck;Sent DM. If it doesn't work you should at minimum see the job's IOException stacktrace change from ""unable to connect to server"" to ""failed connecting to all endpoints"".;;;","03/Jan/11 13:58;uctopcu;I'll be testing it in a few hours. I'll write down the results. something urgent came up.;;;","03/Jan/11 15:53;mck;After fixing my local RF problem this patch works for me.;;;","03/Jan/11 17:25;jbellis;It looks like this patch includes the code from CASSANDRA-1921, which is causing conflicts b/c it's already applied on 0.7 and trunk.  Can you create a patch for 1927 only?;;;","03/Jan/11 17:27;mck;Putting Stu as reviewer since he was for CASSANDRA-342 (which the TODO comment in question was added under).;;;","03/Jan/11 17:34;mck;Yeah, the patch had a lot of crap in it. sorry. will re-apply.;;;","03/Jan/11 17:38;mck;correct patch & license grant;;;","03/Jan/11 17:41;mck;third time lucky. removed unnecessary import.;;;","03/Jan/11 17:54;uctopcu;I've tested against the rc4+patch and it works.;;;","03/Jan/11 19:43;jbellis;committed, thanks!;;;","03/Jan/11 20:23;hudson;Integrated in Cassandra-0.7 #142 (See [https://hudson.apache.org/hudson/job/Cassandra-0.7/142/])
    retry hadoop split requests on connection failure
patch by mck; reviewed by jbellis for CASSANDRA-1927
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
