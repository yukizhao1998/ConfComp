Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocked),Inward issue link (Blocker),Inward issue link (Child-Issue),Inward issue link (Cloners),Outward issue link (Completes),Inward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Supercedes),Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
REPAIR TABLE fails on table refreshing,SPARK-35935,13386561,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,29/Jun/21 16:31,30/Jun/21 08:47,13/Jul/23 08:47,30/Jun/21 06:45,3.0.4,3.1.3,3.2.0,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"MSCK REPAIR TABLE can fail while table recovering with the exception:
{code:java}
Error in SQL statement: AnalysisException: Incompatible format detected.
...
	at org.apache.spark.sql.execution.datasources.FindDataSourceTable.org$apache$spark$sql$execution$datasources$FindDataSourceTable$$verifyNonDeltaTable(DataSourceStrategy.scala:297)
	at org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply0$1.applyOrElse(DataSourceStrategy.scala:378)
	at org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anonfun$apply0$1.applyOrElse(DataSourceStrategy.scala:342)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$4(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1093)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1092)
	at org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:187)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:175)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:316)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:98)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:95)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:75)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:74)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:30)
	at org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply0(DataSourceStrategy.scala:342)
	at org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:336)
	at org.apache.spark.sql.execution.datasources.FindDataSourceTable.apply(DataSourceStrategy.scala:248)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$3(RuleExecutor.scala:221)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:221)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:218)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:210)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:210)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:251)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:245)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:207)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:188)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:109)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:188)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:228)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:323)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:227)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:96)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:178)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:178)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:86)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:94)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:92)
	at org.apache.spark.sql.SparkSession.table(SparkSession.scala:668)
	at org.apache.spark.sql.internal.CatalogImpl.refreshTable(CatalogImpl.scala:548)
	at org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand.run(ddl.scala:714)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:79)
{code}
The same command worked on previous Spark versions.
",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 30 07:09:05 UTC 2021,,,,,,,,,,"0|z0sf5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/21 16:59;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/33137;;;","30/Jun/21 06:45;maxgekk;Issue resolved by pull request 33137
[https://github.com/apache/spark/pull/33137];;;","30/Jun/21 07:08;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/33152;;;","30/Jun/21 07:09;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/33152;;;",,,,,,,,,,,,,,,,
Schema inference of nested structs defaults to map,SPARK-35929,13386450,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,itholic,itholic,29/Jun/21 07:49,12/Dec/22 18:10,13/Jul/23 08:47,07/Jul/21 06:14,3.2.0,,,,,,,,,,,,,,,3.3.0,,,,PySpark,,,,0,,,"Inferring schema for struct columns causing schema issues as below.
{code:java}
data = [{""inside_struct"": {""payment"": 100.5, ""name"": ""Lee""}}]
df = spark.createDataFrame(data)
df.show()
+--------------------+
|       inside_struct|
+--------------------+
|{name -> null, pa...|
+--------------------+
{code}
The ""inside_struct"" is a map, and the ""name"" column inside of it becomes null.

The schema inferring might decide on a map type with a value type of the first field of the struct, we should fix it.",,apachespark,itholic,,,,,,,,,,,,,,,,,,,,,,,,SPARK-38839,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 07 06:14:38 UTC 2021,,,,,,,,,,"0|z0segw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/21 07:51;itholic;I'm working on this;;;","05/Jul/21 08:14;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/33214;;;","07/Jul/21 06:14;gurwls223;Issue resolved by pull request 33214
[https://github.com/apache/spark/pull/33214];;;",,,,,,,,,,,,,,,,,
${spark.yarn.isHadoopProvided} in config.properties is not edited if build with SBT,SPARK-35921,13386389,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,29/Jun/21 02:29,29/Jun/21 21:26,13/Jul/23 08:47,29/Jun/21 21:25,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Build,,,,0,,,"yarn sub-module contains config.properties.
{code}
spark.yarn.isHadoopProvided = ${spark.yarn.isHadoopProvided}
{code}

The ${spark.yarn.isHadoopProvided} part is replaced with true or false in build depending on whether Hadoop is provided or not (specified by -Phadoop-provided).
The edited config.properties will be loaded at runtime to control how to populate Hadoop-related classpath.

If we build with Maven, these process works but doesn't with SBT.",,apachespark,dbtsai,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 29 21:25:59 UTC 2021,,,,,,,,,,"0|z0se3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/21 02:57;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33121;;;","29/Jun/21 21:25;dbtsai;Issue resolved by pull request 33121
[https://github.com/apache/spark/pull/33121];;;",,,,,,,,,,,,,,,,,,
[SQL] JSON read behavior is different depending on the cache setting when nullable is false.,SPARK-35912,13386145,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,fchen,Heedo,Heedo,28/Jun/21 02:58,12/Dec/22 18:10,13/Jul/23 08:47,22/Jul/21 02:14,3.1.1,,,,,,,,,,,,,,,3.3.0,,,,SQL,,,,0,,,"Below is the reproduced code.

 
{code:java}
import org.apache.spark.sql.Encoders
 
case class TestSchema(x: Int, y: Int)
case class BaseSchema(value: TestSchema)
 
val schema = Encoders.product[BaseSchema].schema
val testDS = Seq(""""""{""value"":{""x"":1}}"""""", """"""{""value"":{""x"":2}}"""""").toDS
val jsonDS = spark.read.schema(schema).json(testDS)

jsonDS.show
+---------+
|    value|
+---------+
|{1, null}|
|{2, null}|
+---------+

jsonDS.cache.show
+------+
| value|
+------+
|{1, 0}|
|{2, 0}|
+------+

{code}
 

The above result occurs when a schema is created with a nested StructType and nullable of StructField is false.

 ",,apachespark,dongjoon,fchen,Heedo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 06 15:43:15 UTC 2022,,,,,,,,,,"0|z0scl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/21 10:20;fchen;Working on this;;;","30/Jun/21 03:37;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/33146;;;","30/Jun/21 03:38;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/33146;;;","05/Jul/21 07:54;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/33212;;;","05/Jul/21 07:55;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/33212;;;","20/Jul/21 08:57;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/33436;;;","20/Jul/21 08:58;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/33436;;;","22/Jul/21 02:14;gurwls223;Issue resolved by pull request 33436
[https://github.com/apache/spark/pull/33436];;;","03/May/22 00:48;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36435;;;","06/May/22 15:43;dongjoon;I updated the fixed version 3.3.0 to 3.3.1 because this is not in RC1.;;;",,,,,,,,,,
Fix documentation error in Spark SQL Guide - Getting Started,SPARK-35909,13386085,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dhruvildave,dhruvildave,dhruvildave,27/Jun/21 12:19,27/Jun/21 18:37,13/Jul/23 08:47,27/Jun/21 18:35,3.1.2,,,,,,,,,,,,,,,3.0.4,3.1.3,3.2.0,,Documentation,,,,0,docs,,"The hyperlinks in Python code blocks in [https://spark.apache.org/docs/latest/sql-getting-started.html|https://spark.apache.org/docs/latest/sql-getting-started.html)] currently point to invalid addresses and return 404.",,apachespark,dhruvildave,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 27 18:35:41 UTC 2021,,,,,,,,,,"0|z0sc7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/21 16:53;apachespark;User 'dhruvildave' has created a pull request for this issue:
https://github.com/apache/spark/pull/33107;;;","27/Jun/21 18:35;dongjoon;Issue resolved by pull request 33107
[https://github.com/apache/spark/pull/33107];;;",,,,,,,,,,,,,,,,,,
Fix UT to clean up table/view in SQLQuerySuite,SPARK-35905,13385960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,26/Jun/21 05:42,26/Jun/21 16:56,13/Jul/23 08:47,26/Jun/21 16:56,2.4.8,3.0.2,3.1.0,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"Adding withTable in SQLQuerySuite about test(""SPARK-33338: GROUP BY using literal map should not fail"")",,angerszhuuu,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 26 16:56:11 UTC 2021,,,,,,,,,,"0|z0sbg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/21 05:49;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33092;;;","26/Jun/21 05:50;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33092;;;","26/Jun/21 16:56;dongjoon;Issue resolved by pull request 33092
[https://github.com/apache/spark/pull/33092];;;",,,,,,,,,,,,,,,,,
Converting arrays with RowToColumnConverter triggers assertion,SPARK-35898,13385896,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tomvanbussel,tomvanbussel,tomvanbussel,25/Jun/21 15:45,28/Jun/21 14:52,13/Jul/23 08:47,28/Jun/21 14:52,3.1.2,,,,,,,,,,,,,,,3.1.3,3.2.0,,,SQL,,,,0,,,"When trying to convert a row that contains an array to a ColumnVector with RowToColumnConverter the following error is thrown:
{code:java}
java.lang.AssertionError at org.apache.spark.sql.execution.vectorized.OffHeapColumnVector.putArray(OffHeapColumnVector.java:560) at org.apache.spark.sql.execution.vectorized.WritableColumnVector.appendArray(WritableColumnVector.java:622) at org.apache.spark.sql.execution.RowToColumnConverter$ArrayConverter.append(Columnar.scala:353) at org.apache.spark.sql.execution.RowToColumnConverter$BasicNullableTypeConverter.append(Columnar.scala:241) at org.apache.spark.sql.execution.RowToColumnConverter.convert(Columnar.scala:221)
{code}",,apachespark,tomvanbussel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 27 11:03:01 UTC 2021,,,,,,,,,,"0|z0sb1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/21 15:45;tomvanbussel;I will open a PR with the fix.;;;","27/Jun/21 11:03;apachespark;User 'tomvanbussel' has created a pull request for this issue:
https://github.com/apache/spark/pull/33108;;;",,,,,,,,,,,,,,,,,,
Find and set JAVA_HOME from javac location,SPARK-35887,13385759,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,25/Jun/21 03:19,25/Jun/21 04:09,13/Jul/23 08:47,25/Jun/21 04:09,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Build,,,,0,,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 25 04:09:37 UTC 2021,,,,,,,,,,"0|z0sa7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/21 03:45;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33075;;;","25/Jun/21 04:09;dongjoon;Issue resolved by pull request 33075
[https://github.com/apache/spark/pull/33075];;;",,,,,,,,,,,,,,,,,,
Codegen issue for decimal type,SPARK-35886,13385747,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,yumwang,yumwang,25/Jun/21 02:25,27/Jun/21 22:43,13/Jul/23 08:47,27/Jun/21 22:42,3.0.3,3.1.2,3.2.0,,,,,,,,,,,,,3.0.4,3.1.3,3.2.0,,SQL,,,,0,,,"How to reproduce this issue:
{code:scala}
spark.sql(
  """"""
    |CREATE TABLE t1 (
    |  c1 DECIMAL(18,6),
    |  c2 DECIMAL(18,6),
    |  c3 DECIMAL(18,6))
    |USING parquet;
    |"""""".stripMargin)

spark.sql(""SELECT sum(c1 * c3) + sum(c2 * c3) FROM t1"").show
{code}


{noformat}
20:23:36.272 ERROR org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 56, Column 6: Expression ""agg_exprIsNull_2_0"" is not an rvalue
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 56, Column 6: Expression ""agg_exprIsNull_2_0"" is not an rvalue
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12675)
	at org.codehaus.janino.UnitCompiler.toRvalueOrCompileException(UnitCompiler.java:7676)
{noformat}
",,angerszhu,apachespark,xkrogen,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 27 19:15:11 UTC 2021,,,,,,,,,,"0|z0sa4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/21 03:11;angerszhu;Working on this;;;","25/Jun/21 09:47;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33082;;;","25/Jun/21 09:48;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33082;;;","26/Jun/21 22:17;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/33103;;;","27/Jun/21 19:12;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/33111;;;","27/Jun/21 19:13;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/33111;;;","27/Jun/21 19:14;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/33112;;;","27/Jun/21 19:15;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/33112;;;",,,,,,,,,,,,
Use keyserver.ubuntu.com as a keyserver for CRAN,SPARK-35885,13385738,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,25/Jun/21 01:34,23/Nov/21 17:00,13/Jul/23 08:47,25/Jun/21 02:53,3.0.3,3.1.2,3.2.0,,,,,,,,,,,,,3.0.4,3.1.3,3.2.0,,Kubernetes,R,,,0,,,"This issue aims to use `keyserver.ubuntu.com` as a keyserver for CRAN.
K8s SparkR docker image build fails because both servers don't work correctly.
{code}
$ docker run -it --rm openjdk:11 /bin/bash
root@3e89a8d05378:/# echo ""deb http://cloud.r-project.org/bin/linux/debian buster-cran35/"" >> /etc/apt/sources.list
root@3e89a8d05378:/# (apt-key adv --keyserver keys.gnupg.net --recv-key 'E19F5F87128899B192B1A2C2AD5F960A256A04AF' || apt-key adv --keyserver keys.openpgp.org --recv-key 'E19F5F87128899B192B1A2C2AD5F960A256A04AF')
Executing: /tmp/apt-key-gpghome.8lNIiUuhoE/gpg.1.sh --keyserver keys.gnupg.net --recv-key E19F5F87128899B192B1A2C2AD5F960A256A04AF
gpg: keyserver receive failed: No name
Executing: /tmp/apt-key-gpghome.stxb8XUlx8/gpg.1.sh --keyserver keys.openpgp.org --recv-key E19F5F87128899B192B1A2C2AD5F960A256A04AF
gpg: key AD5F960A256A04AF: new key but contains no user ID - skipped
gpg: Total number processed: 1
gpg:           w/o user IDs: 1
root@3e89a8d05378:/# apt-get update
...
Err:3 http://cloud.r-project.org/bin/linux/debian buster-cran35/ InRelease
  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FCAE2A0E115C3D8A
...
W: GPG error: http://cloud.r-project.org/bin/linux/debian buster-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FCAE2A0E115C3D8A
E: The repository 'http://cloud.r-project.org/bin/linux/debian buster-cran35/ InRelease' is not signed.
N: Updating from such a repository can't be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.
{code}

`keyserver.ubuntu.com` is a recommended backup server in CRAN document.
- http://cloud.r-project.org/bin/linux/debian/

{code}
$ docker run -it --rm openjdk:11 /bin/bash
root@c9b183e45ffe:/# echo ""deb http://cloud.r-project.org/bin/linux/debian buster-cran35/"" >> /etc/apt/sources.list
root@c9b183e45ffe:/# apt-key adv --keyserver keyserver.ubuntu.com --recv-key 'E19F5F87128899B192B1A2C2AD5F960A256A04AF'
Executing: /tmp/apt-key-gpghome.P6cxYkOge7/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-key E19F5F87128899B192B1A2C2AD5F960A256A04AF
gpg: key AD5F960A256A04AF: public key ""Johannes Ranke (Wissenschaftlicher Berater) <johannes.ranke@jrwb.de>"" imported
gpg: Total number processed: 1
gpg:               imported: 1
root@c9b183e45ffe:/# apt-get update
Get:1 http://deb.debian.org/debian buster InRelease [122 kB]
Get:2 http://security.debian.org/debian-security buster/updates InRelease [65.4 kB]
Get:3 http://cloud.r-project.org/bin/linux/debian buster-cran35/ InRelease [4375 B]
Get:4 http://deb.debian.org/debian buster-updates InRelease [51.9 kB]
Get:5 http://cloud.r-project.org/bin/linux/debian buster-cran35/ Packages [53.3 kB]
Get:6 http://security.debian.org/debian-security buster/updates/main arm64 Packages [287 kB]
Get:7 http://deb.debian.org/debian buster/main arm64 Packages [7735 kB]
Get:8 http://deb.debian.org/debian buster-updates/main arm64 Packages [14.5 kB]
Fetched 8334 kB in 2s (4537 kB/s)
Reading package lists... Done
{code}
",,apachespark,dongjoon,mn-mikke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 23 17:00:31 UTC 2021,,,,,,,,,,"0|z0sa2o:",9223372036854775807,,,,,,,,,,,,,3.2.0,,,,,,,,,,"25/Jun/21 01:45;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33071;;;","25/Jun/21 02:53;dongjoon;Issue resolved by pull request 33071
[https://github.com/apache/spark/pull/33071];;;","22/Nov/21 13:11;mn-mikke;[~dongjoon] FYI, buster-cran35 is signed by a different key (fingerprint '95C0FAF38DB3CCAD0C080A7BDC78B2DDEABC47B7') since 17th November (see [http://cloud.r-project.org/bin/linux/debian/]). According to the current repository state, this change might affect the branch [branch-3.0|https://github.com/apache/spark/blob/branch-3.0/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/bindings/R/Dockerfile#L32].;;;","23/Nov/21 17:00;dongjoon;Thank you for reporting, [~mn-mikke].;;;",,,,,,,,,,,,,,,,
[SQL] AQE does not support columnar execution for the final query stage,SPARK-35881,13385664,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andygrove,andygrove,andygrove,24/Jun/21 15:28,11/Aug/21 07:01,13/Jul/23 08:47,30/Jul/21 18:24,3.0.3,3.1.2,3.2.0,,,,,,,,,,,,,3.2.0,3.3.0,,,SQL,,,,0,,,"In AdaptiveSparkPlanExec, a query is broken down into stages and these stages are executed until the entire query has been executed. These stages can be row-based or columnar. However, the final stage, produced by the private getFinalPhysicalPlan method is always assumed to be row-based. The only way to execute the final stage is by calling the various doExecute methods on AdaptiveSparkPlanExec, and doExecuteColumnar is not implemented. The supportsColumnar method also always returns false.

In the RAPIDS Accelerator for Apache Spark, we currently call the private getFinalPhysicalPlan method using reflection and then determine if that plan is columnar or not, and then call the appropriate doExecute method, bypassing the doExecute methods on AdaptiveSparkPlanExec. We would like a supported mechanism for executing a columnar AQE plan so that we do not need to use reflection.

 

 

 

 ",,andygrove,apachespark,code_kr_dev_s,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 11 07:01:43 UTC 2021,,,,,,,,,,"0|z0s9m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/21 14:14;code_kr_dev_s;[~andygrove]

 If the issue is still unresolved, Can I work upon this?;;;","29/Jun/21 18:58;apachespark;User 'andygrove' has created a pull request for this issue:
https://github.com/apache/spark/pull/33140;;;","03/Aug/21 17:59;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/33624;;;","11/Aug/21 07:01;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/33701;;;",,,,,,,,,,,,,,,,
Fix performance regression caused by collectFetchRequests,SPARK-35879,13385633,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,24/Jun/21 13:04,26/Jun/21 04:49,13/Jul/23 08:47,26/Jun/21 04:49,3.1.0,3.2.0,,,,,,,,,,,,,,3.1.3,3.2.0,,,Shuffle,Spark Core,,,0,,,"{code:java}
```sql
 SET spark.sql.adaptive.enabled=true;
 SET spark.sql.shuffle.partitions=3000;
 SELECT /*+ REPARTITION */ 1 as pid, id from range(1, 1000000, 1, 500);
 SELECT /*+ REPARTITION(pid, id) */ 1 as pid, id from range(1, 1000000, 1, 500);
 ```{code}
{code:java}
```log
 21/06/23 13:54:22 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
 21/06/23 13:54:38 DEBUG ShuffleBlockFetcherIterator: Creating fetch request of 2314708 at BlockManagerId(2, 10.1.3.114, 36423, None) with 86 blocks
 21/06/23 13:54:59 DEBUG ShuffleBlockFetcherIterator: Creating fetch request of 2636612 at BlockManagerId(3, 10.1.3.115, 34293, None) with 87 blocks
 21/06/23 13:55:18 DEBUG ShuffleBlockFetcherIterator: Creating fetch request of 2508706 at BlockManagerId(4, 10.1.3.116, 41869, None) with 90 blocks
 21/06/23 13:55:34 DEBUG ShuffleBlockFetcherIterator: Creating fetch request of 2350854 at BlockManagerId(5, 10.1.3.117, 45787, None) with 85 blocks
 21/06/23 13:55:34 INFO ShuffleBlockFetcherIterator: Getting 438 (11.8 MiB) non-empty blocks including 90 (2.5 MiB) local and 0 (0.0 B) host-local and 348 (9.4 MiB) remote blocks
 21/06/23 13:55:34 DEBUG ShuffleBlockFetcherIterator: Sending request for 87 blocks (2.5 MiB) from 10.1.3.115:34293
 21/06/23 13:55:34 INFO TransportClientFactory: Successfully created connection to /10.1.3.115:34293 after 1 ms (0 ms spent in bootstraps)
 21/06/23 13:55:34 DEBUG ShuffleBlockFetcherIterator: Sending request for 90 blocks (2.4 MiB) from 10.1.3.116:41869
 21/06/23 13:55:34 INFO TransportClientFactory: Successfully created connection to /10.1.3.116:41869 after 2 ms (0 ms spent in bootstraps)
 21/06/23 13:55:34 DEBUG ShuffleBlockFetcherIterator: Sending request for 85 blocks (2.2 MiB) from 10.1.3.117:45787
 ```{code}
{code:java}
```log
 21/06/23 14:00:45 INFO MapOutputTracker: Broadcast outputstatuses size = 411, actual size = 828997
 21/06/23 14:00:45 INFO MapOutputTrackerWorker: Got the map output locations
 21/06/23 14:00:45 DEBUG ShuffleBlockFetcherIterator: maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
 21/06/23 14:00:55 DEBUG ShuffleBlockFetcherIterator: Creating fetch request of 1894389 at BlockManagerId(2, 10.1.3.114, 36423, None) with 99 blocks
 21/06/23 14:01:04 DEBUG ShuffleBlockFetcherIterator: Creating fetch request of 1919993 at BlockManagerId(3, 10.1.3.115, 34293, None) with 100 blocks
 21/06/23 14:01:14 DEBUG ShuffleBlockFetcherIterator: Creating fetch request of 1977186 at BlockManagerId(5, 10.1.3.117, 45787, None) with 103 blocks
 21/06/23 14:01:23 DEBUG ShuffleBlockFetcherIterator: Creating fetch request of 1938336 at BlockManagerId(4, 10.1.3.116, 41869, None) with 101 blocks
 21/06/23 14:01:23 INFO ShuffleBlockFetcherIterator: Getting 500 (9.1 MiB) non-empty blocks including 97 (1820.3 KiB) local and 0 (0.0 B) host-local and 403 (7.4 MiB) remote blocks
 21/06/23 14:01:23 DEBUG ShuffleBlockFetcherIterator: Sending request for 101 blocks (1892.9 KiB) from 10.1.3.116:41869
 21/06/23 14:01:23 DEBUG ShuffleBlockFetcherIterator: Sending request for 103 blocks (1930.8 KiB) from 10.1.3.117:45787
 21/06/23 14:01:23 DEBUG ShuffleBlockFetcherIterator: Sending request for 99 blocks (1850.0 KiB) from 10.1.3.114:36423
 21/06/23 14:01:23 DEBUG ShuffleBlockFetcherIterator: Sending request for 100 blocks (1875.0 KiB) from 10.1.3.115:34293
 21/06/23 14:01:23 INFO ShuffleBlockFetcherIterator: Started 4 remote fetches in 37889 ms
 ```{code}",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 26 04:49:24 UTC 2021,,,,,,,,,,"0|z0s9fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/21 13:21;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/33063;;;","24/Jun/21 13:22;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/33063;;;","26/Jun/21 04:49;Qin Yao;Issue resolved by pull request 33063
[https://github.com/apache/spark/pull/33063];;;",,,,,,,,,,,,,,,,,
add fs.s3a.endpoint if unset and fs.s3a.endpoint.region is null,SPARK-35878,13385628,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,stevel@apache.org,stevel@apache.org,stevel@apache.org,24/Jun/21 12:45,25/Jun/21 12:25,13/Jul/23 08:47,25/Jun/21 12:25,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Spark Core,,,,0,,,"People working with S3A and hadoop 3.3.1 outside of EC2, and without the AWS CLI setup, are likely to hit: HADOOP-17771

It should be straightforward to fix up the config similar to SPARK-35868...this will be backwards (harmless) and forwards compatible (it's the recommended workaround)",,apachespark,dongjoon,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-17771,SPARK-29250,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 25 12:25:17 UTC 2021,,,,,,,,,,"0|z0s9e8:",9223372036854775807,,,,,,,,,,,,,3.2.0,,,,,,,,,,"24/Jun/21 15:55;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/33064;;;","25/Jun/21 12:25;dongjoon;Issue resolved by pull request 33064
[https://github.com/apache/spark/pull/33064];;;",,,,,,,,,,,,,,,,,,
array_zip unexpected column names,SPARK-35876,13385617,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,dcrezee,dcrezee,24/Jun/21 11:53,12/Dec/22 18:10,13/Jul/23 08:47,29/Jun/21 03:29,3.1.2,,,,,,,,,,,,,,,3.0.4,3.1.3,3.2.0,,Spark Core,,,,0,,,"{{When I'm using the array_zip function in combination with renamed columns, I get an unexpected schema written to disk.}}
{code:java}
// code placeholder
from pyspark.sql import * 
from pyspark.sql.functions import *

spark = SparkSession.builder.getOrCreate()

data = [
  Row(a1=[""a"", ""a""], b1=[""b"", ""b""]),
]
df = (
  spark.sparkContext.parallelize(data).toDF()
    .withColumnRenamed(""a1"", ""a2"")
    .withColumnRenamed(""b1"", ""b2"")
    .withColumn(""zipped"", arrays_zip(col(""a2""), col(""b2"")))
)
df.printSchema()
// root
//  |-- a2: array (nullable = true)
//  |    |-- element: string (containsNull = true)
//  |-- b2: array (nullable = true)
//  |    |-- element: string (containsNull = true)
//  |-- zipped: array (nullable = true)
//  |    |-- element: struct (containsNull = false)
//  |    |    |-- a2: string (nullable = true)
//  |    |    |-- b2: string (nullable = true)

df.write.save(""test.parquet"")
spark.read.load(""test.parquet"").printSchema()
// root
//  |-- a2: array (nullable = true)
//  |    |-- element: string (containsNull = true)
//  |-- b2: array (nullable = true)
//  |    |-- element: string (containsNull = true)
//  |-- zipped: array (nullable = true)
//  |    |-- element: struct (containsNull = true)
//  |    |    |-- a1: string (nullable = true)
//  |    |    |-- b1: string (nullable = true){code}
I would expect the schema of the DataFrame written to disk to be the same as that printed out. It seems that instead of using the renamed version of the column names, it uses the old column names.

 ",,apachespark,dcrezee,fchen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 23 09:48:07 UTC 2021,,,,,,,,,,"0|z0s9bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/21 10:52;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33106;;;","29/Jun/21 03:29;gurwls223;Issue resolved by pull request 33106
[https://github.com/apache/spark/pull/33106];;;","23/Aug/21 09:47;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33810;;;","23/Aug/21 09:48;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33810;;;",,,,,,,,,,,,,,,,
Upgrade Jetty to 9.4.42,SPARK-35870,13385560,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,24/Jun/21 06:38,04/Aug/21 15:37,13/Jul/23 08:47,24/Jun/21 18:33,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Build,,,,0,,,"Recently, CVE-2021-28169 was reported and 9.4.40 which Spark uses in the current master affects.
https://nvd.nist.gov/vuln/detail/CVE-2021-28169.",,apachespark,sarutak,this,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 04 15:37:36 UTC 2021,,,,,,,,,,"0|z0s8z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/21 06:50;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33053;;;","24/Jun/21 18:33;sarutak;Issue resolved inhttps://github.com/apache/spark/pull/33053.;;;","04/Aug/21 15:18;this;CVE-2021-28169 is also present in Spark v2.4.8 (See [https://github.com/apache/spark/blob/v2.4.8/pom.xml#L137).] [~sarutak] Can we open this issue and add 2.4.8 to the affected versions? I can open a PR to fix in [branch-2.4|https://github.com/apache/spark/blob/branch-2.4/pom.xml#L137].;;;","04/Aug/21 15:37;sarutak;Hi [~this]. v2.4.8 is the last release of brnch-2.4 (See https://spark.apache.org/versioning-policy.html).
So, we can't upgrade Jetty for branch-2.4.;;;",,,,,,,,,,,,,,,,
"Cannot run program ""python"" error when run do-release-docker.sh",SPARK-35869,13385542,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,24/Jun/21 03:41,12/Dec/22 18:10,13/Jul/23 08:47,24/Jun/21 03:48,3.0.0,3.0.2,3.0.3,3.1.0,3.1.1,3.1.2,,,,,,,,,,3.2.0,,,,Project Infra,,,,0,,,"Moving back into docs dir.
Moving to SQL directory and building docs.
Generating SQL API Markdown files.
21/06/15 09:45:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Exception in thread ""main"" java.io.IOException: Cannot run program ""python"": error=2, No such file or directory
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
        at org.apache.spark.deploy.PythonRunner$.main(PythonRunner.scala:97)
        at org.apache.spark.deploy.PythonRunner.main(PythonRunner.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:928)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.IOException: error=2, No such file or directory
        at java.lang.UNIXProcess.forkAndExec(Native Method)
        at java.lang.UNIXProcess.<init>(UNIXProcess.java:247)
        at java.lang.ProcessImpl.start(ProcessImpl.java:134)
        at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
        ... 14 more",,apachespark,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 24 03:48:11 UTC 2021,,,,,,,,,,"0|z0s8v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/21 03:47;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/33048;;;","24/Jun/21 03:47;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/33048;;;","24/Jun/21 03:48;gurwls223;Fixed in https://github.com/apache/spark/pull/33048;;;",,,,,,,,,,,,,,,,,
Add fs.s3a.downgrade.syncable.exceptions if not set,SPARK-35868,13385516,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,24/Jun/21 00:47,24/Jun/21 19:41,13/Jul/23 08:47,24/Jun/21 05:47,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Spark Core,,,,0,,,,,apachespark,dongjoon,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-17597,,,,,,,,SPARK-29250,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 24 19:41:44 UTC 2021,,,,,,,,,,"0|z0s8pc:",9223372036854775807,,,,,,,,,,,,,3.2.0,,,,,,,,,,"24/Jun/21 00:50;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33044;;;","24/Jun/21 01:04;dongjoon;I marked this as `Blocker` because this causes `SparkContext` initialize failures.;;;","24/Jun/21 05:47;dongjoon;Issue resolved by pull request 33044
[https://github.com/apache/spark/pull/33044];;;","24/Jun/21 12:42;stevel@apache.org;Which bits of code was this failing on? You got a stack trace?

I'm curious as 
* if something is being aggressive about using it then it can hurt performance (hdfs) or scale (ABFS)
* they may actually have unmet expectations that it works (ranger audit logs);;;","24/Jun/21 19:41;dongjoon;Thank you for the comment, [~stevel@apache.org]. It's a Spark code issue which is not ready for the exceptions. For now, we want to use Hadoop 3.3.1 in the same way like Hadoop 3.3.0/2.7.x.
{code}
21/06/23 22:54:22 INFO SingleEventLogFileWriter: Logging events to s3a://aci-spark/spark-events/spark-4282c7eec8b54a74a233da4e34a0aa88.zstd.inprogress
21/06/23 22:54:22 ERROR SparkContext: Error initializing SparkContext.
java.lang.UnsupportedOperationException: S3A streams are not Syncable. See HADOOP-17597.
	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.handleSyncableInvocation(S3ABlockOutputStream.java:656)
	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.hflush(S3ABlockOutputStream.java:643)
	at org.apache.hadoop.fs.FSDataOutputStream.hflush(FSDataOutputStream.java:136)
	at org.apache.spark.deploy.history.EventLogFileWriter.$anonfun$writeLine$3(EventLogFileWriters.scala:122)
{code}
;;;",,,,,,,,,,,,,,,
SparkPlan.makeCopy should not set the active session,SPARK-35858,13385227,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,apachespark,cloud_fan,cloud_fan,22/Jun/21 16:59,12/Dec/22 18:10,13/Jul/23 08:47,23/Jun/21 00:51,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 23 00:51:17 UTC 2021,,,,,,,,,,"0|z0s6xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/21 17:08;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/33029;;;","23/Jun/21 00:51;gurwls223;Issue resolved by pull request 33029
[https://github.com/apache/spark/pull/33029];;;",,,,,,,,,,,,,,,,,,
Wrong docs in function GraphGenerators.sampleLogNormal,SPARK-35851,13385054,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zengrui,zengrui,zengrui,22/Jun/21 02:41,23/Jun/21 23:21,13/Jul/23 08:47,23/Jun/21 23:20,3.1.2,,,,,,,,,,,,,,,3.2.0,,,,Documentation,GraphX,,,0,,,"In class GraphGenerators docs, function sampleLogNormal use wrong variable to compute X，m and s should not be used in the formula.",,apachespark,zengrui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 23 23:20:52 UTC 2021,,,,,,,,,,"0|z0s5uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/21 02:57;apachespark;User 'zengruios' has created a pull request for this issue:
https://github.com/apache/spark/pull/33010;;;","23/Jun/21 23:20;srowen;Issue resolved by pull request 33010
[https://github.com/apache/spark/pull/33010];;;",,,,,,,,,,,,,,,,,,
OuterReference resolution should reject ambiguous column names,SPARK-35845,13384991,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,21/Jun/21 18:27,23/Jun/21 06:32,13/Jul/23 08:47,23/Jun/21 06:32,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,,,apachespark,cloud_fan,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 23 06:32:56 UTC 2021,,,,,,,,,,"0|z0s5gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/21 18:36;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/33004;;;","23/Jun/21 06:32;Gengliang.Wang;Issue resolved by pull request 33004
[https://github.com/apache/spark/pull/33004];;;",,,,,,,,,,,,,,,,,,
"Ignore all "".idea"" directories ",SPARK-35842,13384829,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,21/Jun/21 09:32,21/Jun/21 14:07,13/Jul/23 08:47,21/Jun/21 14:07,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Project Infra,,,,0,,,"After https://github.com/apache/spark/pull/32337, all the `.idea/` in submodules are treated as git difference again.
For example, when I open the project `resource-managers/yarn/` with IntelliJ, the git status becomes

{code:java}
Untracked files:
  (use ""git add <file>..."" to include in what will be committed)
	resource-managers/yarn/.idea/
{code}
The same issue happens on opening `sql/hive-thriftserver/` with IntelliJ.
We should ignore all the "".idea"" directories instead of the one under the root path.
",,apachespark,Gengliang.Wang,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 21 14:07:37 UTC 2021,,,,,,,,,,"0|z0s4gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/21 09:37;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32998;;;","21/Jun/21 09:38;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32998;;;","21/Jun/21 14:07;Gengliang.Wang;Issue resolved by pull request 32998
[https://github.com/apache/spark/pull/32998];;;",,,,,,,,,,,,,,,,,
Casting string to decimal type doesn't work if the sum of the digits is greater than 38,SPARK-35841,13384822,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dc-heros,roberto.gelsi,roberto.gelsi,21/Jun/21 09:11,24/Jun/21 08:47,13/Jul/23 08:47,24/Jun/21 08:47,3.1.1,3.1.2,,,,,,,,,,,,,,3.1.3,3.2.0,,,SQL,,,,0,,,"Since Spark 3.1.1, NULL is returned when casting a string with many decimal places to a decimal type. If the sum of the digits before and after the decimal point is less than 39, a value is returned. From 39 digits, however, NULL is returned.
This worked until Spark 3.0.X.

Code to reproduce:

* A string with 2 decimal places in front of the decimal point and 37 decimal places after the decimal point returns null

{code:python}
data = ['28.9259999999999983799625624669715762138']
dfs = spark.createDataFrame(data, StringType())
dfd = dfs.withColumn('value', col('value').cast('decimal(10, 5)'))
dfd.show(truncate=False)
{code}

+-----+
|value|
+-----+
|null |
+-----+
 
* A string with 2 decimal places in front of the decimal point and 36 decimal places after the decimal point returns the number as decimal

{code:python}
data = ['28.925999999999998379962562466971576213']
dfs = spark.createDataFrame(data, StringType())
dfd = dfs.withColumn('value', col('value').cast('decimal(10, 5)'))
dfd.show(truncate=False)
{code}

+--------+
|value   |
+--------+
|28.92600|
+--------+

* A string with 1 decimal place in front of the decimal point and 37 decimal places after the decimal point returns the number as decimal

{code:python}
data = ['2.9259999999999983799625624669715762138']
dfs = spark.createDataFrame(data, StringType())
dfd = dfs.withColumn('value', col('value').cast('decimal(10, 5)'))
dfd.show(truncate=False)
{code}

+-------+
|value  |
+-------+
|2.92600|
+-------+
 ","Tested in a Kubernetes Cluster with Spark 3.1.1 and Spark 3.1.2 images

(Hadoop 3.2.1, Python 3.9, Scala 2.12.13)",apachespark,cloud_fan,dc-heros,roberto.gelsi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 24 08:47:33 UTC 2021,,,,,,,,,,"0|z0s4fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/21 01:31;dc-heros;I would like to work on this;;;","22/Jun/21 03:57;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33011;;;","24/Jun/21 08:47;cloud_fan;Issue resolved by pull request 33011
[https://github.com/apache/spark/pull/33011];;;",,,,,,,,,,,,,,,,,
Remove reference to spark.shuffle.push.based.enabled in ShuffleBlockPusherSuite,SPARK-35836,13384777,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,csingh,csingh,csingh,21/Jun/21 06:02,21/Jun/21 18:58,13/Jul/23 08:47,21/Jun/21 18:58,3.1.0,,,,,,,,,,,,,,,3.2.0,,,,Shuffle,Spark Core,,,0,,,"The test suite for ShuffleBlockPusherSuite was added with SPARK-32917 and in this suite, the configuration for push-based shuffle is incorrectly referenced as {{spark.shuffle.push.based.enabled}}. We need to remove this config from here.

{{ShuffleBlockPusher}} is created only when push based shuffle is enabled and this suite is for {{ShuffleBlockPusher}}, so no other change is required.",,apachespark,csingh,mridulm80,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 21 18:58:48 UTC 2021,,,,,,,,,,"0|z0s45c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/21 06:28;apachespark;User 'otterc' has created a pull request for this issue:
https://github.com/apache/spark/pull/32992;;;","21/Jun/21 18:58;mridulm80;Issue resolved by pull request 32992
[https://github.com/apache/spark/pull/32992];;;",,,,,,,,,,,,,,,,,,
Use the same cleanup logic as Py4J in inheritable thread API,SPARK-35834,13384747,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,21/Jun/21 01:07,12/Dec/22 18:11,13/Jul/23 08:47,14/Sep/21 00:09,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,PySpark,,,,0,,,"After https://github.com/apache/spark/commit/6d309914df422d9f0c96edfd37924ecb8f29e3a9, the test became flaky:

{code}
======================================================================
ERROR [71.813s]: test_save_load_pipeline_estimator (pyspark.ml.tests.test_tuning.CrossValidatorTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/__w/spark/spark/python/pyspark/ml/tests/test_tuning.py"", line 589, in test_save_load_pipeline_estimator
    self._run_test_save_load_pipeline_estimator(DummyLogisticRegression)
  File ""/__w/spark/spark/python/pyspark/ml/tests/test_tuning.py"", line 572, in _run_test_save_load_pipeline_estimator
    cvModel2 = crossval2.fit(training)
  File ""/__w/spark/spark/python/pyspark/ml/base.py"", line 161, in fit
    return self._fit(dataset)
  File ""/__w/spark/spark/python/pyspark/ml/tuning.py"", line 747, in _fit
    bestModel = est.fit(dataset, epm[bestIndex])
  File ""/__w/spark/spark/python/pyspark/ml/base.py"", line 159, in fit
    return self.copy(params)._fit(dataset)
  File ""/__w/spark/spark/python/pyspark/ml/pipeline.py"", line 114, in _fit
    model = stage.fit(dataset)
  File ""/__w/spark/spark/python/pyspark/ml/base.py"", line 161, in fit
    return self._fit(dataset)
  File ""/__w/spark/spark/python/pyspark/ml/pipeline.py"", line 114, in _fit
    model = stage.fit(dataset)
  File ""/__w/spark/spark/python/pyspark/ml/base.py"", line 161, in fit
    return self._fit(dataset)
  File ""/__w/spark/spark/python/pyspark/ml/classification.py"", line 2924, in _fit
    models = pool.map(inheritable_thread_target(trainSingleClass), range(numClasses))
  File ""/__t/Python/3.6.13/x64/lib/python3.6/multiprocessing/pool.py"", line 266, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File ""/__t/Python/3.6.13/x64/lib/python3.6/multiprocessing/pool.py"", line 644, in get
    raise self._value
  File ""/__t/Python/3.6.13/x64/lib/python3.6/multiprocessing/pool.py"", line 119, in worker
    result = (True, func(*args, **kwds))
  File ""/__t/Python/3.6.13/x64/lib/python3.6/multiprocessing/pool.py"", line 44, in mapstar
    return list(map(*args))
  File ""/__w/spark/spark/python/pyspark/util.py"", line 324, in wrapped
    InheritableThread._clean_py4j_conn_for_current_thread()
  File ""/__w/spark/spark/python/pyspark/util.py"", line 389, in _clean_py4j_conn_for_current_thread
    del connections[i]
IndexError: deque index out of range

----------------------------------------------------------------------
{code}",,apachespark,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 14 00:09:10 UTC 2021,,,,,,,,,,"0|z0s3yo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/21 01:20;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32989;;;","13/Sep/21 19:54;holden;Is this a test only issue or a regression for Python users?;;;","14/Sep/21 00:08;gurwls223;This is actually fixed too. I wonder why it wasn't resolved. It's not a regression, just a bug fix.;;;","14/Sep/21 00:09;gurwls223;Fixed in https://github.com/apache/spark/pull/32989;;;",,,,,,,,,,,,,,,,
Add LocalRootDirsTest trait,SPARK-35832,13384702,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,20/Jun/21 09:52,20/Jun/21 17:54,13/Jul/23 08:47,20/Jun/21 17:54,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Kubernetes,Spark Core,Tests,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 20 17:54:17 UTC 2021,,,,,,,,,,"0|z0s3oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/21 09:56;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32986;;;","20/Jun/21 17:54;dongjoon;Issue resolved by pull request 32986
[https://github.com/apache/spark/pull/32986];;;",,,,,,,,,,,,,,,,,,
Queries against wide Avro tables can be slow,SPARK-35817,13384592,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,18/Jun/21 20:41,25/Jun/21 04:15,13/Jul/23 08:47,25/Jun/21 04:15,3.1.1,3.1.2,3.2.0,,,,,,,,,,,,,3.1.3,3.2.0,,,SQL,,,,0,,,"A query against an Avro table can be quite slow when all are true:
- There are many columns in the Avro file
- The query contains a wide projection
- There are many splits in the input
- Some of the splits are read serially (e.g., less executors than there are tasks)

A write to an Avro table can be quite slow when all are true:
- There are many columns in the new rows
- The operation is creating many files

For example, a single-threaded query against a 6000 column Avro data set with 50K rows and 20 files takes less than a minute with Spark 3.0.1 but over 7 minutes with Spark 3.2.0-SNAPSHOT.

The culprit appears to be this line of code:
https://github.com/apache/spark/blob/3fb044e043a2feab01d79b30c25b93d4fd166b12/external/avro/src/main/scala/org/apache/spark/sql/avro/AvroUtils.scala#L226

For each split, AvroDeserializer will call this function once for each column in the projection, resulting in a potential n^2 lookup per split.

For each file, AvroSerializer will call this function once for each column, resulting in an n^2 lookup per file.
",,apachespark,bersprockets,Gengliang.Wang,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 25 04:15:50 UTC 2021,,,,,,,,,,"0|z0s308:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/21 20:44;bersprockets;The referenced line of code is meant to respect case sensitivity settings.

I think I have a fix that still respects case sensitivity. I will try to make a PR in the next day or so.;;;","18/Jun/21 22:04;xkrogen;Thanks for catching this [~bersprockets]! I will be happy to contribute as well, given I'm the original author of SPARK-34133. Let me know if there's anything I can do or provide input on. I guess we should create a map of one sides' fields (with lowercased names for case-insensitivity) and do lookups, to reduce the O(N^2) complexity to O(N) (N for map creation and N constant-time lookups during resolution).;;;","18/Jun/21 22:32;bersprockets;[~xkrogen]
Thanks!
{quote}I guess we should create a map of one sides' fields (with lowercased names for case-insensitivity) {quote}
That's what I did locally to test it out: a map (keyed by lowercased names) with sequences as the values (since the lower case name could map to multiple mixed case fields).;;;","19/Jun/21 02:07;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/32969;;;","19/Jun/21 02:07;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/32969;;;","25/Jun/21 01:48;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/33072;;;","25/Jun/21 04:15;Gengliang.Wang;Issue resolved by pull request 33072
[https://github.com/apache/spark/pull/33072];;;",,,,,,,,,,,,,
Spark SQL does not support creating views using DataSource v2 based data sources,SPARK-35803,13384431,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,planga82,davidrabinowitz,davidrabinowitz,18/Jun/21 01:24,12/Dec/22 18:11,13/Jul/23 08:47,08/Sep/21 04:17,2.4.8,3.1.2,,,,,,,,,,,,,,3.3.0,,,,SQL,,,,0,,,"When a temporary view is created in Spark SQL using an external data source, Spark then tries to create the relevant relation using DataSource.resolveRelation() method. Unlike DataFrameReader.load(), resolveRelation() does not check if the provided DataSource implements the DataSourceV2 interface and instead tries to use the RelationProvider trait in order to generate the Relation.

Furthermore, DataSourceV2Relation is not a subclass of BaseRelation, so it cannot be used in resolveRelation().

Last, I tried to implement the RelationProvider trait in my Java implementation of DataSourceV2, but the match inside resolveRelation() did not detect it as RelationProvider.



 ",,apachespark,cloud_fan,davidrabinowitz,planga82,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-41344,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 08 04:17:16 UTC 2021,,,,,,,,,,"0|z0s20o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/21 03:29;gurwls223;It would be great to have some reproducers here, [~davidrabinowitz].;;;","29/Jun/21 17:58;davidrabinowitz;Using regular spark-shell:
{code:java}
spark-shell --packages com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.21.1
scala> val df1 = spark.read.format(""bigquery"").load(""bigquery-public-data.samples.shakespeare"")
df1: org.apache.spark.sql.DataFrame = [word: string, word_count: bigint ... 2 more fields]

scala> df1.count
res0: Long = 164656                                                             

scala> val df2 = spark.read.format(""com.google.cloud.spark.bigquery.v2.BigQueryDataSourceV2"").load(""bigquery-public-data.samples.shakespeare"")
df2: org.apache.spark.sql.DataFrame = [word: string, word_count: bigint ... 2 more fields]

scala> df2.count
res1: Long = 164656
{code}
Using spark-sql:


{code:java}
spark-sql --packages com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.21.1
spark-sql> CREATE or REPLACE GLOBAL TEMPORARY VIEW s1 USING bigquery options (table 'bigquery-public-data.samples.shakespeare');
Time taken: 2.143 seconds
spark-sql> select count(*) from global_temp.s1;
21/06/29 17:51:34 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Querying table bigquery-public-data.samples.shakespeare, param
eters sent from Spark: requiredColumns=[], filters=[]
21/06/29 17:51:34 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Going to read from bigquery-public-data.samples.shakespeare co
lumns=[], filter=''
21/06/29 17:51:34 INFO com.google.cloud.spark.bigquery.direct.DirectBigQueryRelation: Used optimized BQ count(*) path. Count: 164656
164656
Time taken: 3.767 seconds, Fetched 1 row(s)
spark-sql> CREATE or REPLACE GLOBAL TEMPORARY VIEW s2 USING com.google.cloud.spark.bigquery.v2.BigQueryDataSourceV2 options (table 'bigquery-public-data.samples.shakespeare');
Error in query: com.google.cloud.spark.bigquery.v2.BigQueryDataSourceV2 is not a valid Spark SQL Data Source.;
{code}

Both runs used Spark 2.4.8 with Scala 2.12 (Dataproc image 1.5). The same code path exists in Spark 3 as well. The code for the connector is at https://github.com/GoogleCloudDataproc/spark-bigquery-connector

 ;;;","07/Sep/21 00:17;apachespark;User 'planga82' has created a pull request for this issue:
https://github.com/apache/spark/pull/33922;;;","08/Sep/21 04:17;cloud_fan;Issue resolved by pull request 33922
[https://github.com/apache/spark/pull/33922];;;",,,,,,,,,,,,,,,,
Fix SparkPlan.sqlContext usage,SPARK-35798,13384342,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,petertoth,petertoth,petertoth,17/Jun/21 11:51,17/Jun/21 13:49,13/Jul/23 08:47,17/Jun/21 13:49,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"There might be SparkPlan nodes where canonicalization on executor side can cause issues. 

Mode details here: https://github.com/apache/spark/pull/32885/files#r651019687",,apachespark,cloud_fan,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 17 13:49:55 UTC 2021,,,,,,,,,,"0|z0s1gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/21 12:00;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/32947;;;","17/Jun/21 12:00;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/32947;;;","17/Jun/21 13:49;cloud_fan;Issue resolved by pull request 32947
[https://github.com/apache/spark/pull/32947];;;",,,,,,,,,,,,,,,,,
Set the list of read columns in the task configuration to reduce reading of ORC data.,SPARK-35783,13384049,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weixiuli,weixiuli,weixiuli,16/Jun/21 05:11,24/Jun/21 05:09,13/Jul/23 08:47,17/Jun/21 05:06,3.0.3,3.1.2,3.2.0,,,,,,,,,,,,,3.0.4,3.1.3,3.2.0,,SQL,,,,0,,,"Now, the ORC reader will read all columns of the ORC table when the task configuration does not set the read column list. Therefore, we should set the list of read columns in the task configuration to reduce reading of ORC data.",,apachespark,dongjoon,weixiuli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 17 05:06:54 UTC 2021,,,,,,,,,,"0|z0rzo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/21 05:27;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/32923;;;","16/Jun/21 05:28;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/32923;;;","17/Jun/21 05:06;dongjoon;Issue resolved by pull request 32923
[https://github.com/apache/spark/pull/32923];;;",,,,,,,,,,,,,,,,,
CoalesceExec can execute child plan twice,SPARK-35767,13383974,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,andygrove,andygrove,andygrove,15/Jun/21 16:13,19/Jun/21 00:52,13/Jul/23 08:47,15/Jun/21 19:01,2.3.0,2.3.4,2.4.0,2.4.8,3.0.2,3.1.2,3.2.0,,,,,,,,,3.0.4,3.1.3,3.2.0,,SQL,,,,0,,,"CoalesceExec calls `child.execute()` in the if condition and throws away the results, then calls `child.execute()` again in the else condition. This could cause a section of the plan to be executed twice.
{code:java}
protected override def doExecute(): RDD[InternalRow] = {
  if (numPartitions == 1 && child.execute().getNumPartitions < 1) {
    // Make sure we don't output an RDD with 0 partitions, when claiming that we have a
    // `SinglePartition`.
    new CoalesceExec.EmptyRDDWithPartitions(sparkContext, numPartitions)
  } else {
    child.execute().coalesce(numPartitions, shuffle = false)
  }
} {code}",,andygrove,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 15 19:01:07 UTC 2021,,,,,,,,,,"0|z0rz7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/21 16:26;apachespark;User 'andygrove' has created a pull request for this issue:
https://github.com/apache/spark/pull/32920;;;","15/Jun/21 19:01;dongjoon;This is resolved via https://github.com/apache/spark/pull/32920;;;",,,,,,,,,,,,,,,,,,
Distinct aggs are not duplicate sensitive,SPARK-35765,13383929,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tanelk,tanelk,tanelk,15/Jun/21 11:37,12/Dec/22 18:11,13/Jul/23 08:47,15/Jun/21 13:25,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"Extended RemoveRedundantAggregates to remove deduplicating aggregations before aggregations that ignore duplicates.

",,apachespark,tanelk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33122,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 15 13:25:25 UTC 2021,,,,,,,,,,"0|z0ryxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/21 11:40;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/32904;;;","15/Jun/21 11:41;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/32904;;;","15/Jun/21 13:25;gurwls223;Issue resolved by pull request 32904
[https://github.com/apache/spark/pull/32904];;;",,,,,,,,,,,,,,,,,
Update the document about building Spark with Hadoop for Hadoop 2.x and 3.x,SPARK-35758,13383758,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,sarutak,ferranjr,ferranjr,14/Jun/21 15:37,12/Dec/22 18:11,13/Jul/23 08:47,15/Jun/21 11:20,3.1.1,3.1.2,3.2.0,,,,,,,,,,,,,3.1.3,3.2.0,,,Documentation,,,,1,,,"When trying to follow steps in documentation about building Spark with a specific Hadoop version, this seems to fail with the following compiler error.
{code:java}
 [INFO] Compiling 560 Scala sources and 99 Java sources to /Users/puigcalvachef/Documents/os/spark/core/target/scala-2.12/classes ...
[ERROR] [Error] /Users/puigcalvachef/Documents/os/spark/core/src/main/scala/org/apache/spark/ui/HttpSecurityFilter.scala:107: type mismatch;
 found   : K where type K
 required: String
[ERROR] [Error] /Users/puigcalvachef/Documents/os/spark/core/src/main/scala/org/apache/spark/ui/HttpSecurityFilter.scala:107: value map is not a member of V
[ERROR] [Error] /Users/puigcalvachef/Documents/os/spark/core/src/main/scala/org/apache/spark/ui/HttpSecurityFilter.scala:107: missing argument list for method stripXSS in class XssSafeRequest
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `stripXSS _` or `stripXSS(_)` instead of `stripXSS`.
[ERROR] [Error] /Users/puigcalvachef/Documents/os/spark/core/src/main/scala/org/apache/spark/ui/PagedTable.scala:307: value startsWith is not a member of K
[ERROR] four errors found
{code}
The minimal build to reproduce it would be just by building spark core module using:
{code:bash}
./build/mvn -Dhadoop.version=2.7.4 -pl :spark-core_2.12  -DskipTests clean install {code}
After some testing, it seems that something changed between 3.1.1-rc1 and 3.1.1-rc2 that made the feature start failing.

I tried a few versions of  Hadoop that fail: 2.7.4, 2.8.1, 2.8.5 
But it was successful when using Hadoop 3.0.0.",,apachespark,ferranjr,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 15 11:20:33 UTC 2021,,,,,,,,,,"0|z0rxvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/21 01:40;sarutak;[~ferranjr]
Could you build with -Phadoop-2.7 ?
I can build with the following command.
{code}
build/mvn -Phadoop-2.7 -Dhadoop.version=2.7.4 -pl :spark-core_2.12 -DskipTests compile
{code};;;","15/Jun/21 06:40;ferranjr;Thank you so much [~sarutak], I can build it using the -Phadoop-2.7 flag as you commented, there is no 2.8 profile, but I managed to build the core module for Hadoop versions 2.7.4 and 2.8.5 using this flag... 

It seems then that is only an issue with the [building spark docs where you can |https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn]find this as an example:
{code:java}
./build/mvn -Pyarn -Dhadoop.version=2.8.5 -DskipTests clean package
{code}
But we should probably require the -Phadoop-2.7 profile flag to be added.;;;","15/Jun/21 08:14;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32917;;;","15/Jun/21 11:20;gurwls223;Issue resolved by pull request 32917
[https://github.com/apache/spark/pull/32917];;;",,,,,,,,,,,,,,,,
Fix StreamingJoinHelper to be able to handle day-time interval,SPARK-35748,13383619,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,apachespark,sarutak,sarutak,13/Jun/21 10:49,14/Jun/21 12:45,13/Jul/23 08:47,14/Jun/21 12:45,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,Structured Streaming,,,0,,,"In the current master, StreamingJoinHelper.getStateValueWatermark can't handle conditions which contain day-time interval literals.",,apachespark,maxgekk,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 14 12:45:57 UTC 2021,,,,,,,,,,"0|z0rx0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/21 11:08;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32896;;;","13/Jun/21 11:08;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32896;;;","14/Jun/21 12:45;maxgekk;Issue resolved by pull request 32896
[https://github.com/apache/spark/pull/32896];;;",,,,,,,,,,,,,,,,,
Task id in the Stage page timeline is incorrect,SPARK-35746,13383526,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shahid,shahid,shahid,12/Jun/21 01:26,11/Jan/22 01:23,13/Jul/23 08:47,12/Jun/21 06:42,3.0.0,3.1.2,,,,,,,,,,,,,,,,,,Web UI,,,,0,,,!image-2021-06-12-07-03-09-808.png!,,apachespark,sarutak,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/21 01:33;shahid;image-2021-06-12-07-03-09-808.png;https://issues.apache.org/jira/secure/attachment/13026773/image-2021-06-12-07-03-09-808.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 11 01:23:46 UTC 2022,,,,,,,,,,"0|z0rwfs:",9223372036854775807,,,,,,,,,,,,,3.0.3,3.1.3,3.2.0,,,,,,,,"12/Jun/21 01:31;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/32888;;;","12/Jun/21 06:42;sarutak;Issue resolved in https://github.com/apache/spark/pull/32888.;;;","10/Jan/22 13:12;apachespark;User 'stczwd' has created a pull request for this issue:
https://github.com/apache/spark/pull/35155;;;","10/Jan/22 13:12;apachespark;User 'stczwd' has created a pull request for this issue:
https://github.com/apache/spark/pull/35155;;;","11/Jan/22 01:23;apachespark;User 'stczwd' has created a pull request for this issue:
https://github.com/apache/spark/pull/35159;;;",,,,,,,,,,,,,,,
Expression.semanticEquals should be symmetrical,SPARK-35742,13383459,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Jun/21 16:56,15/Jun/21 08:53,13/Jul/23 08:47,15/Jun/21 08:53,3.0.0,3.1.0,3.2.0,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,,,apachespark,cloud_fan,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 15 08:53:21 UTC 2021,,,,,,,,,,"0|z0rw0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/21 17:04;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/32885;;;","15/Jun/21 08:53;cloud_fan;Issue resolved by pull request 32885
[https://github.com/apache/spark/pull/32885];;;",,,,,,,,,,,,,,,,,,
spark.sql.orc.filterPushdown not working with Spark 3.1.1 for tables with varchar data type,SPARK-35700,13382956,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,arghya18,arghya18,09/Jun/21 13:39,12/Dec/22 18:11,13/Jul/23 08:47,22/Jun/21 02:23,3.1.1,,,,,,,,,,,,,,,3.1.3,3.2.0,,,Kubernetes,PySpark,Spark Core,,0,,,"We are not able to upgrade to Spark 3.1.1 from Spark 2.4.x as the join on varchar column is failing which is unexpected and works on Spark 3.0.0.  We are trying to run it on Spark 3.1.1 (MR 3.2) on K8s 

Below is my use case:

Tables are external hive table and files are stored as ORC. We do have varchar column and when we are trying to perform join on varchar column we are getting the exception.

As I understand Spark 3.1.1 have introduced varchar data type but seems its not well tested with ORC and does not have backward compatibility. I have even tried with below config without luck

*spark.sql.legacy.charVarcharAsString: ""true""*

We are not getting the error when *spark.sql.orc.filterPushdown=false*

Below is the code: Here col1 is of type varchar(32) in hive
{code:java}
df = spark.sql(""select col1, col2 from table1 a inner join table2 on b (a.col1=b.col1 and a.col2 > b.col2 )"") 
df.write.format(""orc"").option(""compression"", ""zlib"").mode(""Append"").save(""<s3_path>"")
{code}
Below is the error:

 
{code:java}
Job aborted due to stage failure: Task 43 in stage 5.0 failed 4 times, most recent failure: Lost task 43.3 in stage 5.0 (TID 524) (10.219.36.64 executor 5): java.lang.UnsupportedOperationException: DataType: varchar(32)
	at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.getPredicateLeafType(OrcFilters.scala:150)
	at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.getType$1(OrcFilters.scala:222)
	at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.buildLeafSearchArgument(OrcFilters.scala:266)
	at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.convertibleFiltersHelper$1(OrcFilters.scala:132)
	at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.$anonfun$convertibleFilters$4(OrcFilters.scala:135)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.convertibleFilters(OrcFilters.scala:134)
	at org.apache.spark.sql.execution.datasources.orc.OrcFilters$.createFilter(OrcFilters.scala:73)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.$anonfun$buildReaderWithPartitionValues$4(OrcFileFormat.scala:189)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.$anonfun$buildReaderWithPartitionValues$4$adapted(OrcFileFormat.scala:188)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.$anonfun$buildReaderWithPartitionValues$1(OrcFileFormat.scala:188)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:116)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:177)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:3
{code}
 

I can see there is no mapping of varchar in OrcFilters.scala:150

[https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcFilters.scala#L142]

 

 ",Spark 3.1.1 on K8S,apachespark,arghya18,dongjoon,saurabhc100,xkrogen,,,,,,,,,,,,,,,SPARK-38695,,,,,,SPARK-37096,SPARK-35762,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 22 18:16:42 UTC 2021,,,,,,,,,,"0|z0rsx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jun/21 05:04;gurwls223;cc [~Qin Yao] [~cloud_fan] [~dongjoon] FYI;;;","16/Jun/21 16:31;arghya18;Any update on this please? This is very common use case.;;;","18/Jun/21 13:05;saurabhc100;[~arghya18] - I tried to reproduce this scenario, but its not failing for me 

 

CREATE external TABLE tblvarchorc1 (x INTEGER, y VARCHAR(8)) stored as orc;

CREATE external TABLE tblvarchorc2 (x INTEGER, y VARCHAR(8)) stored as orc;

spark.sql(""select * from tblvarchorc1"").show
 +----+---+
|x|y|

+----+---+
|1|test|

+----+---+

spark.sql(""select * from tblvarchorc2"").show
 +----+---+
|x|y|

+----+---+
|1|test|

+----+---+

spark.sql(""select * from tblvarchorc1 a inner join tblvarchorc2 b on (a.y = b.y and a.x >= b.x ) "").show
 +----+---++-------+
|x|y|x|y|

+----+---++-------+
|1|test|1|test|

+----+---++-------+

This method converts the schema to the string type
{code:java}
def replaceCharVarcharWithString(dt: DataType): DataType = dt match {
 case ArrayType(et, nullable) =>
 ArrayType(replaceCharVarcharWithString(et), nullable)
 case MapType(kt, vt, nullable) =>
 MapType(replaceCharVarcharWithString(kt), replaceCharVarcharWithString(vt), nullable)
 case StructType(fields) =>
 StructType(fields.map { field =>
 field.copy(dataType = replaceCharVarcharWithString(field.dataType))
 })
 case _: CharType => StringType
 case _: VarcharType => StringType
 case _ => dt
}{code}
 

also i have validated we are getting the 

 
{code:java}
spark.sql(""select * from  tblvarchorc1"").schema
res1: org.apache.spark.sql.types.StructType = StructType(StructField(x,IntegerType,true), StructField(y,StringType,true))
{code}
 

 

Can you share the reproducible scenario to debug this further. 

 ;;;","20/Jun/21 03:50;gurwls223;[~Qin Yao] [~cloud_fan], there was a same JIRA reported in SPARK-35762.

It would be great if the issue persists with the reproducible step as SPARK-35762.;;;","20/Jun/21 09:03;saurabhc100;[~hyukjin.kwon]/ [~Qin Yao] / [~cloud_fan] - I was able to reproduce this issue in the master branch for the steps given in the SPARK-35762.

 Below are some of the scenario found while debugging this issue.

1) This is issue is reproducible for all the orc data created using the Hive, but when the insertion is done using the SPARK, we are not getting this exception.

2) We are getting the exception due this, when the file is created using hive  and while calling the readSchema the [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/orc/OrcUtils.scala#L63 |http://example.com/]

In the reader.getSchema we get the varchar datatype.

But on reading the orc file created by the Spark we are getting the string datatype.

3) I tried adding the validation for converting varchar to String 
{code:java}
readSchema(file, conf, ignoreCorruptFiles) match {
 case Some(schema) =>
 val orcSchema = CatalystSqlParser.parseDataType(
 schema.toString).asInstanceOf[StructType]
 val varCharExist = orcSchema.fields.exists(
 x => CharVarcharUtils.hasCharVarchar(x.dataType))
 if (varCharExist) {
 Some(CharVarcharUtils.replaceCharVarcharWithStringInSchema(orcSchema))
 } else {
 Some(orcSchema)
 }{code}
After adding this fix , we are converting the varchar to string and query is working fine.

 

4) Similar conversion of data type change is needed on the 

`def readSchema(file: Path, conf: Configuration, ignoreCorruptFiles: Boolean) `, this called by inferSchema ,

readOrcSchemasInParallel. 

 

If this approach is fine. Than I can go head and create the PR for the same, otherwise if we want to see some other approach we can discuss on this

 

 ;;;","21/Jun/21 12:12;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/33001;;;","22/Jun/21 02:23;dongjoon;Issue resolved by pull request 33001
[https://github.com/apache/spark/pull/33001];;;","22/Jun/21 18:16;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/33030;;;",,,,,,,,,,,,
Overflow on converting valid Timestamp to Microseconds,SPARK-35679,13382749,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,apachespark,Gengliang.Wang,Gengliang.Wang,08/Jun/21 15:45,10/Jun/21 05:24,13/Jul/23 08:47,10/Jun/21 05:24,3.2.0,,,,,,,,,,,,,,,3.0.3,3.1.3,3.2.0,,SQL,,,,0,,,"
{code:java}
scala> import org.apache.spark.sql.catalyst.util.DateTimeUtils._
import org.apache.spark.sql.catalyst.util.DateTimeUtils._

scala> instantToMicros(microsToInstant(Long.MinValue))
java.lang.ArithmeticException: long overflow
  at java.lang.Math.multiplyExact(Math.java:892)
  at org.apache.spark.sql.catalyst.util.DateTimeUtils$.instantToMicros(DateTimeUtils.scala:389)
  ... 49 elided


{code}
",,apachespark,dc-heros,Gengliang.Wang,maxgekk,pingsutw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 10 05:24:56 UTC 2021,,,,,,,,,,"0|z0rrn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/21 03:23;dc-heros;I will create a pull request soon!

 ;;;","09/Jun/21 07:50;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/32839;;;","10/Jun/21 05:24;maxgekk;Issue resolved by pull request 32839
[https://github.com/apache/spark/pull/32839];;;",,,,,,,,,,,,,,,,,
Spark fails on unrecognized hint in subquery,SPARK-35673,13382583,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fchen,rshkv,rshkv,08/Jun/21 00:29,20/Jun/21 03:31,13/Jul/23 08:47,10/Jun/21 07:50,3.0.2,3.1.1,3.1.2,3.2.0,,,,,,,,,,,,3.0.3,3.1.3,3.2.0,,SQL,,,,0,,,"Spark queries to fail on unrecognized hints in subqueries. An example to reproduce:
{code:sql}
SELECT /*+ use_hash */ 42;
-- 21/06/08 01:28:05 WARN HintErrorLogger: Unrecognized hint: use_hash()
-- 42

SELECT *
FROM (
    SELECT /*+ use_hash */ 42
);
-- 21/06/08 01:28:07 WARN HintErrorLogger: Unrecognized hint: use_hash()
-- Error in query: unresolved operator 'Project [*];
-- 'Project [*]
-- +- SubqueryAlias __auto_generated_subquery_name
--    +- Project [42 AS 42#2]
--       +- OneRowRelation
{code}",,angerszhuuu,apachespark,cloud_fan,fchen,maropu,rshkv,,,,,,,,,,,,,,,,,,,,SPARK-35795,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 09 10:19:50 UTC 2021,,,,,,,,,,"0|z0rqmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/21 06:09;maropu;Since 2.4.x does not have this issue, it is a kind of regressions. (I've checked that the master still has it). cc: [~angerszhuuu] Do you have time to look into this?;;;","08/Jun/21 18:48;fchen;Hi, [~maropu] 

This is because [Spark-28292|https://issues.apache.org/jira/browse/SPARK-28292] move rule RemoveAllHints after batch Resolution.
 The rule ResolveReferences is executed before rule RemoveAllHints. when the plan(Project[*]) contains an unresolved child(UnresolvedHint), means `plan.childrenResolved == false`, so ResolveReferences can't expand stars.

But we can't put the rule RemoveAllHints before batch Resolution, because the user-defined hint is injected in batch Resolution.

Any idea or suggestions to fix it?

 

 ;;;","08/Jun/21 23:56;maropu;We cannot move `RemoveAllHints` just after the `Hints` batch?;;;","09/Jun/21 02:15;fchen;No, we can't.  The user-defined hint is injected in batch `Post-Hoc Resolution`.;;;","09/Jun/21 02:40;fchen;Hi, [~maropu].

I think a better implement for injecting user-defined hint is [PR-25071|https://github.com/apache/spark/pull/25071/files], add a new public API `injectResolutionHint`, then we can inject user-defined hint in batch `Hints`, and add`RemoveAllHints`to the end of batch `Hints`.;;;","09/Jun/21 05:40;cloud_fan;Can we simply change `UnresolvedHint.resolved` to true?;;;","09/Jun/21 06:52;angerszhuuu;Just check the comment, problem is clear, we need to discuss more about this feature.;;;","09/Jun/21 07:34;fchen;{quote}Can we simply change `UnresolvedHint.resolved` to true?
{quote}
We can't simply change `UnresolvedHint.resolved = true`, but `UnresolvedHint.resolved = child.resolved`.

I will try and create the pull request later.;;;","09/Jun/21 10:19;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/32841;;;",,,,,,,,,,,
Spark fails to launch executors with very large user classpath lists on YARN,SPARK-35672,13382563,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xkrogen,xkrogen,xkrogen,07/Jun/21 20:10,12/Dec/22 18:10,13/Jul/23 08:47,16/Nov/21 17:09,3.1.2,,,,,,,,,,,,,,,3.3.0,,,,Spark Core,YARN,,,0,,,"When running Spark on YARN, the {{user-class-path}} argument to {{CoarseGrainedExecutorBackend}} is used to pass a list of user JAR URIs to executor processes. The argument is specified once for each JAR, and the URIs are fully-qualified, so the paths can be quite long. With large user JAR lists (say 1000+), this can result in system-level argument length limits being exceeded, typically manifesting as the error message:
{code}
/bin/bash: Argument list too long
{code}

A [Google search|https://www.google.com/search?q=spark%20%22%2Fbin%2Fbash%3A%20argument%20list%20too%20long%22&oq=spark%20%22%2Fbin%2Fbash%3A%20argument%20list%20too%20long%22] indicates that this is not a theoretical problem and afflicts real users, including ours. This issue was originally observed on Spark 2.3, but has been confirmed to exist in the master branch as well.","Linux RHEL7
Spark 3.1.1",apachespark,attilapiros,hamidelmaazouz,petertoth,tgraves,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 19 07:01:26 UTC 2021,,,,,,,,,,"0|z0rqi8:",9223372036854775807,,,,,,,,,,,,,3.1.3,3.2.0,,,,,,,,,"07/Jun/21 20:23;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32810;;;","25/Jun/21 16:02;xkrogen;#32810 went into master.

Put up #33090 for branch-3.1;;;","25/Jun/21 16:02;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/33090;;;","23/Sep/21 15:47;petertoth;[~xkrogen], [~tgraves], unfortunately, I think this is a breaking change and should be reverted.

On our clusters we use `{{spark.yarn.config.replacementPath={{HADOOP_COMMON_HOME}}/...}}` and before this change Yarn executor context looked like the following:
{noformat}
YARN executor launch context:
  env:
    ...

  command:
    ...
      {{JAVA_HOME}}/bin/java \
      -server \
      ...
      --user-class-path \
      file:{{HADOOP_COMMON_HOME}}/...jar \
      ...
{noformat}
and Yarn was able to substitute HADOOP_COMMON_HOME environment variable.

But after this change user classpath is distributed in {{SparkConf}} and we can't use environment variables any more.

cc [~Gengliang.Wang]
;;;","23/Sep/21 16:36;tgraves;Ok, sounds like we should revert then so this doesn't block 3.2 release;;;","23/Sep/21 18:25;petertoth;I put up a revert PR: https://github.com/apache/spark/pull/34082;;;","23/Sep/21 18:26;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/34082;;;","23/Sep/21 18:26;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/34082;;;","23/Sep/21 23:16;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/34084;;;","23/Sep/21 23:17;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/34084;;;","24/Sep/21 01:56;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/34088;;;","24/Sep/21 03:51;gurwls223;Reverted at:

https://github.com/apache/spark/commit/c2c4a48c783d96f6cc1590b84541f3e9a1cb2b45 (master)
https://github.com/apache/spark/commit/1b545804c47fc37b7d54ad8967bf788251dc10fc (branch-3.2)
https://github.com/apache/spark/commit/1b545804c47fc37b7d54ad8967bf788251dc10fc (branch-3.1);;;","24/Sep/21 15:00;xkrogen;Thanks [~petertoth] [~hyukjin.kwon] [~Gengliang.Wang] for reporting and dealing with the issue.

I'll work on submitting a new PR to master with the changes from PRs #31810 (original) and #34084 (environment variable fix) incorporated.;;;","27/Sep/21 20:40;xkrogen;Re-submitted at [PR #34120|https://github.com/apache/spark/pull/34120];;;","27/Sep/21 20:40;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/34120;;;","27/Sep/21 20:40;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/34120;;;","16/Nov/21 17:09;attilapiros;Issue resolved by pull request 34120
[https://github.com/apache/spark/pull/34120];;;","18/Nov/21 16:58;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34649;;;","19/Nov/21 07:01;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34664;;;",
fix special char in CSV header with filter pushdown,SPARK-35669,13382487,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,07/Jun/21 14:10,16/Jun/21 00:45,13/Jul/23 08:47,16/Jun/21 00:45,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"This is a followup of [#31964|https://github.com/apache/spark/pull/31964]

We should only quote the column name when nested column predicate pushdown is enabled, otherwise the data source side may not have the logic to parse the quoted column name and fail. This is not a problem before [#31964|https://github.com/apache/spark/pull/31964] , as we don't quote the column name if there is no dot in the name. But [#31964|https://github.com/apache/spark/pull/31964] changed it.",,apachespark,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 16 00:45:01 UTC 2021,,,,,,,,,,"0|z0rq1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/21 14:16;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/32807;;;","16/Jun/21 00:45;maropu;Resolved by https://github.com/apache/spark/pull/32807;;;",,,,,,,,,,,,,,,,,,
resolve UnresolvedAlias in CollectMetrics,SPARK-35665,13382406,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,07/Jun/21 08:57,12/Dec/22 18:10,13/Jul/23 08:47,07/Jun/21 12:07,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 07 12:07:39 UTC 2021,,,,,,,,,,"0|z0rpjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/21 09:03;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/32803;;;","07/Jun/21 12:07;gurwls223;Issue resolved by pull request 32803
[https://github.com/apache/spark/pull/32803];;;",,,,,,,,,,,,,,,,,,
Avoid write null to StateStore,SPARK-35659,13382358,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,06/Jun/21 23:57,08/Jun/21 23:02,13/Jul/23 08:47,08/Jun/21 16:54,3.0.2,3.1.2,3.2.0,,,,,,,,,,,,,3.0.3,3.1.3,3.2.0,,Structured Streaming,,,,0,,,"According to {{get}} method doc in StateStore API, it returns non-null row if the key exists. So basically we should avoid write null to StateStore. You cannot distinguish if the returned null row is because the key doesn't exist, or the value is actually null. And due to the defined behavior of {{get}}, it is quite easy to cause NPE error if the caller doesn't expect to get a null if the caller believes the key exists.",,apachespark,leastangle,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 08 23:02:12 UTC 2021,,,,,,,,,,"0|z0rp8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/21 00:04;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32796;;;","07/Jun/21 00:05;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32796;;;","08/Jun/21 16:41;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32823;;;","08/Jun/21 16:42;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32823;;;","08/Jun/21 16:55;viirya;The issue was resolved at https://github.com/apache/spark/pull/32796.;;;","08/Jun/21 23:02;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32828;;;",,,,,,,,,,,,,,
[SQL] CatalystToExternalMap interpreted path fails for Map with case classes as keys or values,SPARK-35653,13382162,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eejbyfeldt,eejbyfeldt,eejbyfeldt,04/Jun/21 14:20,10/Jun/21 16:40,13/Jul/23 08:47,10/Jun/21 16:40,3.0.2,3.1.2,3.2.0,,,,,,,,,,,,,3.0.3,3.1.3,3.2.0,,SQL,,,,0,,,"Interpreted path deserialization fails for Map with case classes as keys or values while the codegen path works correctly.

To reproduce the issue one can add test cases to the ExpressionEncoderSuite. For example adding the following
{noformat}
case class IntAndString(i: Int, s: String)
encodeDecodeTest(Map(1 -> IntAndString(1, ""a"")), ""map with case class as value"")
{noformat}
It will succeed for the code gen path while the interpreted path will fail with
{noformat}
[info] - encode/decode for map with case class as value: Map(1 -> IntAndString(1,a)) (interpreted path) *** FAILED *** (64 milliseconds)
[info] Encoded/Decoded data does not match input data
[info]
[info] in: Map(1 -> IntAndString(1,a))
[info] out: Map(1 -> [1,a])
[info] types: scala.collection.immutable.Map$Map1 [info]
[info] Encoded Data: [org.apache.spark.sql.catalyst.expressions.UnsafeMapData@5ecf5d9e]
[info] Schema: value#823
[info] root
[info] -- value: map (nullable = true)
[info] |-- key: integer
[info] |-- value: struct (valueContainsNull = true)
[info] | |-- i: integer (nullable = false)
[info] | |-- s: string (nullable = true)
[info]
[info]
[info] fromRow Expressions:
[info] catalysttoexternalmap(lambdavariable(CatalystToExternalMap_key, IntegerType, false, 178), lambdavariable(CatalystToExternalMap_key, IntegerType, false, 178), lambdavariable(CatalystToExternalMap_value, StructField(i,IntegerType,false), StructField(s,StringType,true), true, 179), if (isnull(lambdavariable(CatalystToExternalMap_value, StructField(i,IntegerType,false), StructField(s,StringType,true), true, 179))) null else newInstance(class org.apache.spark.sql.catalyst.encoders.IntAndString), input[0, map<int,struct<i:int,s:string>>, true], interface scala.collection.immutable.Map
[info] :- lambdavariable(CatalystToExternalMap_key, IntegerType, false, 178)
[info] :- lambdavariable(CatalystToExternalMap_key, IntegerType, false, 178)
[info] :- lambdavariable(CatalystToExternalMap_value, StructField(i,IntegerType,false), StructField(s,StringType,true), true, 179)
[info] :- if (isnull(lambdavariable(CatalystToExternalMap_value, StructField(i,IntegerType,false), StructField(s,StringType,true), true, 179))) null else newInstance(class org.apache.spark.sql.catalyst.encoders.IntAndString)
[info] : :- isnull(lambdavariable(CatalystToExternalMap_value, StructField(i,IntegerType,false), StructField(s,StringType,true), true, 179))
[info] : : +- lambdavariable(CatalystToExternalMap_value, StructField(i,IntegerType,false), StructField(s,StringType,true), true, 179)
[info] : :- null
[info] : +- newInstance(class org.apache.spark.sql.catalyst.encoders.IntAndString)
[info] : :- assertnotnull(lambdavariable(CatalystToExternalMap_value, StructField(i,IntegerType,false), StructField(s,StringType,true), true, 179).i)
[info] : : +- lambdavariable(CatalystToExternalMap_value, StructField(i,IntegerType,false), StructField(s,StringType,true), true, 179).i
[info] : : +- lambdavariable(CatalystToExternalMap_value, StructField(i,IntegerType,false), StructField(s,StringType,true), true, 179)
[info] : +- lambdavariable(CatalystToExternalMap_value, StructField(i,IntegerType,false), StructField(s,StringType,true), true, 179).s.toString
[info] : +- lambdavariable(CatalystToExternalMap_value, StructField(i,IntegerType,false), StructField(s,StringType,true), true, 179).s
[info] : +- lambdavariable(CatalystToExternalMap_value, StructField(i,IntegerType,false), StructField(s,StringType,true), true, 179)
[info] +- input[0, map<int,struct<i:int,s:string>>, true] (ExpressionEncoderSuite.scala:627)
{noformat}
So the value was not correctly deserialized in the interpreted path.

I have prepared a PR that I will submit for fixing this issue.",,apachespark,eejbyfeldt,viirya,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 10 16:40:43 UTC 2021,,,,,,,,,,"0|z0ro14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/21 14:43;apachespark;User 'eejbyfeldt' has created a pull request for this issue:
https://github.com/apache/spark/pull/32783;;;","10/Jun/21 16:40;viirya;Issue resolved by pull request 32783
[https://github.com/apache/spark/pull/32783];;;",,,,,,,,,,,,,,,,,,
Different Behaviour join vs joinWith in self joining,SPARK-35652,13382161,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dc-heros,walmaaoui,walmaaoui,04/Jun/21 14:19,19/Jun/21 00:52,13/Jul/23 08:47,11/Jun/21 12:42,3.1.2,,,,,,,,,,,,,,,3.0.4,3.1.3,3.2.0,,SQL,,,,0,,,"It seems like spark inner join is performing a cartesian join in self joining using `joinWith` and an inner join using `join` 

Snippet:

 
{code:java}
scala> val df = spark.range(0,5) 
df: org.apache.spark.sql.Dataset[Long] = [id: bigint]

scala> df.show 
+---+ 
| id| 
+---+ 
| 0|
| 1| 
| 2| 
| 3| 
| 4| 
+---+ 

scala> df.join(df, df(""id"") === df(""id"")).count 
21/06/04 16:01:39 WARN Column: Constructing trivially true equals predicate, 'id#1649L = id#1649L'. Perhaps you need to use aliases. 
res21: Long = 5

scala> df.joinWith(df, df(""id"") === df(""id"")).count
21/06/04 16:01:47 WARN Column: Constructing trivially true equals predicate, 'id#1649L = id#1649L'. Perhaps you need to use aliases. 
res22: Long = 25

{code}
According to the comment in code source, joinWith is expected to manage this case, right?
{code:java}
def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = {
    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,
    // etc.
{code}
I find it weird that join and joinWith haven't the same behaviour.","{color:#172b4d}Spark 3.1.2{color}

Scala 2.12",apachespark,dc-heros,walmaaoui,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 14 01:57:47 UTC 2021,,,,,,,,,,"0|z0ro0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/21 09:37;dc-heros;I will make a pull request soon;;;","10/Jun/21 09:39;dc-heros;I think we can fix this issues with the approach from [SPARK-6231|https://issues.apache.org/jira/browse/SPARK-6231];;;","10/Jun/21 11:00;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/32863;;;","14/Jun/21 01:57;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/32899;;;",,,,,,,,,,,,,,,,
Ambiguous variable reference in functions.py column(),SPARK-35643,13382036,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kvasist,kvasist,kvasist,04/Jun/21 02:50,12/Dec/22 18:10,13/Jul/23 08:47,05/Jun/21 03:41,3.1.2,,,,,,,,,,,,,,,3.1.3,3.2.0,,,PySpark,,,,0,,,"In functions.py, there is a function added {{def column(col)}}. There is also another method in the same file {{def col(col)}}. This leads to some ambiguity on whether the parameter is being referred to or the function. In pyspark 3.1.2, this leads to {{TypeError: 'str' object is not callable}} when the function {{column(col)}} is called - the highest preference is given to the string variable in scope as opposed to the function {{col }}in the file as intended.

This PR fixes that ambiguity by changing the variable name to {{col_like}}.

 

I have already raised a PR to fix the issue - https://github.com/apache/spark/pull/32771",,apachespark,kvasist,,,,,,,,,,43200,43200,,0%,43200,43200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 05 03:41:42 UTC 2021,,,,,,,,,,"0|z0rn94:",9223372036854775807,,,,,hyukjin.kwon,,,,,,,,,,,,,,,,,,"04/Jun/21 15:02;apachespark;User 'keerthanvasist' has created a pull request for this issue:
https://github.com/apache/spark/pull/32771;;;","04/Jun/21 15:03;apachespark;User 'keerthanvasist' has created a pull request for this issue:
https://github.com/apache/spark/pull/32771;;;","05/Jun/21 03:41;gurwls223;Fixed in https://github.com/apache/spark/pull/32771;;;",,,,,,,,,,,,,,,,,
"Cache commonly occurring strings from SQLMetrics, JsonProtocol and AccumulatorV2",SPARK-35613,13381771,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vsowrirajan,vsowrirajan,vsowrirajan,02/Jun/21 18:44,16/Jun/21 03:03,13/Jul/23 08:47,16/Jun/21 03:03,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Spark Core,SQL,,,0,,,"SQLMetrics allows creation of different metrics like sizing, timing metrics etc. some of those names for the metric can be duplicated and along with that the `Some` wrapper objects adds additional memory overhead. In our internal platform, this has caused huge memory usage on driver causing Full GC to kick in every so often.

cc [~mridulm80]",,apachespark,mridulm80,vsowrirajan,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 16 03:03:16 UTC 2021,,,,,,,,,,"0|z0rlm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/21 18:57;apachespark;User 'venkata91' has created a pull request for this issue:
https://github.com/apache/spark/pull/32754;;;","02/Jun/21 18:58;apachespark;User 'venkata91' has created a pull request for this issue:
https://github.com/apache/spark/pull/32754;;;","16/Jun/21 03:03;mridulm80;Issue resolved by pull request 32754
[https://github.com/apache/spark/pull/32754];;;",,,,,,,,,,,,,,,,,
Memory leak in Spark interpreter ,SPARK-35610,13381655,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,02/Jun/21 09:41,02/Jun/21 16:35,13/Jul/23 08:47,02/Jun/21 16:35,3.0.0,3.0.1,3.0.2,3.1.0,3.1.1,3.1.2,3.2.0,,,,,,,,,3.0.3,3.1.3,3.2.0,,Spark Core,,,,0,,,"I have identified this leak by running the Livy tests (I know it is close to the attic but this leak causes a constant OOM there) and it is in our Spark unit tests as well. 

This leak can be identified by checking the number of LeakyEntry in case of Scala 2.12.14 (and ZipEntry for Scala 2.12.10) instances which with its related data can take up a considerable amount of memory (as those are created from the jars which are on the classpath).

I have my own tool for instrumenting JVM code ([trace-agent|https://github.com/attilapiros/trace-agent]) and with that I am able to call JVM diagnostic commands at specific methods. Let me show how it in action.
 
It has a single text file embedded into the tool's jar called action.txt. 
In this case actions.txt content is:

{noformat}
$ unzip -q -c trace-agent-0.0.7.jar actions.txt
diagnostic_command org.apache.spark.repl.ReplSuite runInterpreter  cmd:gcClassHistogram,limit_output_lines:8,where:beforeAndAfter,with_gc:true
diagnostic_command org.apache.spark.repl.ReplSuite afterAll  cmd:gcClassHistogram,limit_output_lines:8,where:after,with_gc:true
{noformat}

Which creates a class histogram at the beginning and at the end of org.apache.spark.repl.ReplSuite#runInterpreter() (after triggering a GC which might not finish as GC is done in a separate thread..) and one histogram in the end of the afterAll() method.

And the histograms are the followings on master branch:

{noformat}
$ ./build/sbt "";project repl;set Test/javaOptions += \""-javaagent:/Users/attilazsoltpiros/git/attilapiros/memoryLeak/trace-agent-0.0.7.jar\""; testOnly"" |grep ""ZipEntry\|LeakyEntry""
   3:        197089        9460272  scala.reflect.io.FileZipArchive$LeakyEntry
   3:        197089        9460272  scala.reflect.io.FileZipArchive$LeakyEntry
   3:        197089        9460272  scala.reflect.io.FileZipArchive$LeakyEntry
   3:        197089        9460272  scala.reflect.io.FileZipArchive$LeakyEntry
   3:        197089        9460272  scala.reflect.io.FileZipArchive$LeakyEntry
   3:        197089        9460272  scala.reflect.io.FileZipArchive$LeakyEntry
   3:        394178       18920544  scala.reflect.io.FileZipArchive$LeakyEntry
   3:        394178       18920544  scala.reflect.io.FileZipArchive$LeakyEntry
   3:        591267       28380816  scala.reflect.io.FileZipArchive$LeakyEntry
   3:        591267       28380816  scala.reflect.io.FileZipArchive$LeakyEntry
   3:        788356       37841088  scala.reflect.io.FileZipArchive$LeakyEntry
   3:        788356       37841088  scala.reflect.io.FileZipArchive$LeakyEntry
   3:        985445       47301360  scala.reflect.io.FileZipArchive$LeakyEntry
   3:        985445       47301360  scala.reflect.io.FileZipArchive$LeakyEntry
   3:       1182534       56761632  scala.reflect.io.FileZipArchive$LeakyEntry
   3:       1182534       56761632  scala.reflect.io.FileZipArchive$LeakyEntry
   3:       1379623       66221904  scala.reflect.io.FileZipArchive$LeakyEntry
   3:       1379623       66221904  scala.reflect.io.FileZipArchive$LeakyEntry
   3:       1576712       75682176  scala.reflect.io.FileZipArchive$LeakyEntry
{noformat}

Where the header of the table is:
{noformat}
num     #instances         #bytes  class name
{noformat}

So the LeakyEntry in the end is about 75MB (173MB in case of Scala 2.12.10 and before for another class called ZipEntry) but the first item (a char/byte arrays) and the second item (strings) in the histogram also relates to this leak:


{noformat}
$ ./build/sbt "";project repl;set Test/javaOptions += \""-javaagent:/Users/attilazsoltpiros/git/attilapiros/memoryLeak/trace-agent-0.0.7.jar\""; testOnly"" |grep ""1:\|2:\|3:""
   1:          2701        3496112  [B
   2:         21855        2607192  [C
   3:          4885         537264  java.lang.Class
   1:        480323       55970208  [C
   2:        480499       11531976  java.lang.String
   3:        197089        9460272  scala.reflect.io.FileZipArchive$LeakyEntry
   1:        481825       56148024  [C
   2:        481998       11567952  java.lang.String
   3:        197089        9460272  scala.reflect.io.FileZipArchive$LeakyEntry
   1:        487056       57550344  [C
   2:        487179       11692296  java.lang.String
   3:        197089        9460272  scala.reflect.io.FileZipArchive$LeakyEntry
   1:        487054       57551008  [C
   2:        487176       11692224  java.lang.String
   3:        197089        9460272  scala.reflect.io.FileZipArchive$LeakyEntry
   1:        927823      107139160  [C
   2:        928072       22273728  java.lang.String
   3:        394178       18920544  scala.reflect.io.FileZipArchive$LeakyEntry
   1:        927793      107129328  [C
   2:        928041       22272984  java.lang.String
   3:        394178       18920544  scala.reflect.io.FileZipArchive$LeakyEntry
   1:       1361851      155555608  [C
   2:       1362261       32694264  java.lang.String
   3:        591267       28380816  scala.reflect.io.FileZipArchive$LeakyEntry
   1:       1361683      155493464  [C
   2:       1362092       32690208  java.lang.String
   3:        591267       28380816  scala.reflect.io.FileZipArchive$LeakyEntry
   1:       1803074      205157728  [C
   2:       1803268       43278432  java.lang.String
   3:        788356       37841088  scala.reflect.io.FileZipArchive$LeakyEntry
   1:       1802385      204938224  [C
   2:       1802579       43261896  java.lang.String
   3:        788356       37841088  scala.reflect.io.FileZipArchive$LeakyEntry
   1:       2236631      253636592  [C
   2:       2237029       53688696  java.lang.String
   3:        985445       47301360  scala.reflect.io.FileZipArchive$LeakyEntry
   1:       2236536      253603008  [C
   2:       2236933       53686392  java.lang.String
   3:        985445       47301360  scala.reflect.io.FileZipArchive$LeakyEntry
   1:       2668892      301893920  [C
   2:       2669510       64068240  java.lang.String
   3:       1182534       56761632  scala.reflect.io.FileZipArchive$LeakyEntry
   1:       2668759      301846376  [C
   2:       2669376       64065024  java.lang.String
   3:       1182534       56761632  scala.reflect.io.FileZipArchive$LeakyEntry
   1:       3101238      350101048  [C
   2:       3102073       74449752  java.lang.String
   3:       1379623       66221904  scala.reflect.io.FileZipArchive$LeakyEntry
   1:       3101240      350101104  [C
   2:       3102075       74449800  java.lang.String
   3:       1379623       66221904  scala.reflect.io.FileZipArchive$LeakyEntry
   1:       3533785      398371760  [C
   2:       3534835       84836040  java.lang.String
   3:       1576712       75682176  scala.reflect.io.FileZipArchive$LeakyEntry
   1:       3533759      398367088  [C
   2:       3534807       84835368  java.lang.String
   3:       1576712       75682176  scala.reflect.io.FileZipArchive$LeakyEntry
   1:       3967049      446893400  [C
   2:       3968314       95239536  java.lang.String
   3:       1773801       85142448  scala.reflect.io.FileZipArchive$LeakyEntry
[info] - SPARK-26633: ExecutorClassLoader.getResourceAsStream find REPL classes (8 seconds, 248 milliseconds)
Setting default log level to ""ERROR"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
   1:       3966423      446709584  [C
   2:       3967682       95224368  java.lang.String
   3:       1773801       85142448  scala.reflect.io.FileZipArchive$LeakyEntry
   1:       4399583      495097208  [C
   2:       4401050      105625200  java.lang.String
   3:       1970890       94602720  scala.reflect.io.FileZipArchive$LeakyEntry
   1:       4399578      495070064  [C
   2:       4401040      105624960  java.lang.String
   3:       1970890       94602720  scala.reflect.io.FileZipArchive$LeakyEntry
{noformat}

The last three is about 700MB altogether.
",,apachespark,attilapiros,dongjoon,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-29152,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 02 16:35:18 UTC 2021,,,,,,,,,,"0|z0rkwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/21 09:44;attilapiros;I am working on this, soon a PR will be opened.;;;","02/Jun/21 12:43;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/32748;;;","02/Jun/21 16:35;dongjoon;Issue resolved by pull request 32748
[https://github.com/apache/spark/pull/32748];;;",,,,,,,,,,,,,,,,,
Job crashes with java.io.UTFDataFormatException: encoded string too long,SPARK-35602,13381529,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,dejanm@gmail.com,dejanm@gmail.com,01/Jun/21 18:05,09/Jun/21 03:46,13/Jul/23 08:47,09/Jun/21 01:36,3.1.1,,,,,,,,,,,,,,,3.1.3,3.2.0,,,Structured Streaming,,,,0,,,"Running stafeful structured streaming app using java. When running on Spark 3.1.1 app is crashing with java.io.UTFDataFormatException: encoded string too long. I am not getting this problem when running on Spark 3.0.1

21/06/01 17:50:35 WARN DAGScheduler: Broadcasting large task binary with size 1986.3 KiB21/06/01 17:50:35 WARN DAGScheduler: Broadcasting large task binary with size 1986.3 KiB21/06/01 17:50:37 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 0) (ip-10-64-12-189.eu-west-1.compute.internal executor 1): java.io.UTFDataFormatException: encoded string too long: 156449 bytes at java.io.DataOutputStream.writeUTF(DataOutputStream.java:364) at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323) at org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityChecker.createSchemaFile(StateSchemaCompatibilityChecker.scala:102) at org.apache.spark.sql.execution.streaming.state.StateSchemaCompatibilityChecker.check(StateSchemaCompatibilityChecker.scala:67) at org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$getStateStoreProvider$2(StateStore.scala:487) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at scala.util.Try$.apply(Try.scala:213) at org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$getStateStoreProvider$1(StateStore.scala:487) at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86) at org.apache.spark.sql.execution.streaming.state.StateStore$.getStateStoreProvider(StateStore.scala:483) at org.apache.spark.sql.execution.streaming.state.StateStore$.get(StateStore.scala:468) at org.apache.spark.sql.execution.streaming.state.StateStoreRDD.compute(StateStoreRDD.scala:125) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) at org.apache.spark.rdd.RDD.iterator(RDD.scala:337) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) at org.apache.spark.rdd.RDD.iterator(RDD.scala:337) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) at org.apache.spark.rdd.RDD.iterator(RDD.scala:337) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:131) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)
21/06/01 17:50:40 ERROR TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job21/06/01 17:50:40 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@2745b41f is aborting.21/06/01 17:50:40 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@2745b41f aborted.21/06/01 17:50:40 ERROR MicroBatchExecution: Query [id = adcf4f93-8c51-4a14-9d9d-1e7a858c8a8c, runId = 86b6c41c-a32f-485d-bbf3-24b844c27739] terminated with errororg.apache.spark.SparkException: Writing job aborted. at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:388) at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336) at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:297) at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:304) at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40) at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40) at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:46) at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3733) at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3005) at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107) at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232) at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107) at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722) at org.apache.spark.sql.Dataset.collect(Dataset.scala:3005) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:589) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107) at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232) at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107) at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:584) at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357) at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355) at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:584) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:226) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:357) at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:355) at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:194) at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:188) at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:333) at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:244)",AWS emr-6.3.0,apachespark,dejanm@gmail.com,kabhwan,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 09 03:46:18 UTC 2021,,,,,,,,,,"0|z0rk4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/21 02:25;sarutak;Hi [~dejanm@gmail.com] , could you provide reproducible code?;;;","03/Jun/21 03:43;dejanm@gmail.com;Hi [Kousuke|https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sarutak],

Thanks for the quick response.

Not sure if I can create a simple example. This is a huge app with complex logic. The key thing is stateful part. State class is multi-level deep and can be quite big. My estimate is up to 30Mb. 

 

Note that code was running on Spark 2.4 and 3.0.1. So this is probably regression introduced by Spark 3.1.1

 

I will try to make an example that I can share.

Dejan

 

 ;;;","03/Jun/21 04:57;dejanm@gmail.com;The problem is in 
StateSchemaCompatibilityChecker.createSchemaFile 
which is calling 
DataOutputStream.writeUTF
for json schema string

wtiteUTF is imiting sring to 65535

{color:#000080}if {color}(utflen > {color:#0000ff}65535{color})
 {color:#000080}throw new {color}UTFDataFormatException(
 {color:#008000}""encoded string too long: "" {color}+ utflen + {color:#008000}"" bytes""{color});

*Basically code is limiting complexity of the schema that can be used in stateful  streaming by json* *schema* *string size.*

I think that DataOutputStream.writeUTF should not be used in StateSchemaCompatibilityChecker.createSchemaFile 

Best,

Dejan

{color:#0000ff} {color};;;","03/Jun/21 14:14;dejanm@gmail.com;StateSchemaCompatibilityChecker.scala is added to the project in

https://issues.apache.org/jira/browse/SPARK-27237

Best,

 

Dejan;;;","03/Jun/21 17:33;sarutak;[~dejanm@gmail.com]
Thank you for reporting. I understand the reason.
You can set spark.sql.streaming.stateStore.stateSchemaCheck to false for a workaround.;;;","05/Jun/21 05:23;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32788;;;","05/Jun/21 05:24;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32788;;;","09/Jun/21 01:36;kabhwan;Issue resolved by pull request 32788
[https://github.com/apache/spark/pull/32788];;;","09/Jun/21 01:49;kabhwan;Very interested that the json representation of the schema exceeds 64k.

[~dejanm@gmail.com]
Would you mind sharing the schema, with redaction on the field name? Like redacting the field name ""hello"" to ""xxxxx"" (same length). Never mind if the schema cannot be shared.;;;","09/Jun/21 03:46;dejanm@gmail.com;[~kabhwan],

Sorry can not share the schema. I am using class (encoded by Encoders.bean) that has many levels. Some time variable names are quite long. Application is using stateful streaming api and it is calculating user statistics for one network protocol. The protocol is quite rich.

Dejan;;;",,,,,,,,,,
BlockManagerMasterEndpoint should not ignore index-only shuffle file during updating,SPARK-35589,13381377,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,01/Jun/21 05:49,03/Jun/21 21:24,13/Jul/23 08:47,01/Jun/21 21:23,3.1.0,3.1.1,3.1.2,3.2.0,,,,,,,,,,,,3.1.3,3.2.0,,,Spark Core,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-20629,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 03 21:24:24 UTC 2021,,,,,,,,,,"0|z0rj74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/21 05:58;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32727;;;","01/Jun/21 05:58;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32727;;;","01/Jun/21 21:23;dongjoon;Issue resolved by pull request 32727
[https://github.com/apache/spark/pull/32727];;;","03/Jun/21 01:49;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32756;;;","03/Jun/21 21:23;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32774;;;","03/Jun/21 21:24;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32774;;;",,,,,,,,,,,,,,
Set a default value for spark.kubernetes.test.sparkTgz in pom.xml for Kubernetes integration tests,SPARK-35586,13381371,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,01/Jun/21 04:37,01/Jun/21 07:40,13/Jul/23 08:47,01/Jun/21 07:40,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Kubernetes,Tests,,,0,,,"In kubernetes/integration-tests/pom.xml, there is no default value for spark.kubernetes.test.sparkTgz so running tests with the following command will fail.

{code}
build/mvn -Dspark.kubernetes.test.namespace=default -Pkubernetes -Pkubernetes-integration-tests -Psparkr  -pl resource-managers/kubernetes/integration-tests integration-test
{code}
+ mkdir -p /home/kou/work/oss/spark/resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked
+ tar -xzvf --test-exclude-tags --strip-components=1 -C /home/kou/work/oss/spark/resource-managers/kubernetes/integration-tests/target/spark-dist-unpacked
tar (child): --test-exclude-tags: Cannot open: No such file or directory
tar (child): Error is not recoverable: exiting now
tar: Child returned status 2
tar: Error is not recoverable: exiting now
[ERROR] Command execution failed.
{code}
",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 01 07:40:21 UTC 2021,,,,,,,,,,"0|z0rj5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/21 04:44;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32722;;;","01/Jun/21 07:40;dongjoon;Issue resolved by pull request 32722
[https://github.com/apache/spark/pull/32722];;;",,,,,,,,,,,,,,,,,,
Casting special strings to DATE/TIMESTAMP returns inconsistent results,SPARK-35581,13381338,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,31/May/21 19:55,25/Aug/21 20:41,13/Jul/23 08:47,01/Jun/21 12:29,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"When casting the special values ""now"", ""today"", ""tomorrow"", and ""yesterday"" to DATE/TIMESTAMP, Spark may return inconsistent results.

Looks like Spark runs the expression on each executor, on every row independently. So the results could differ across executors if they have different system time, and across rows because of the resolution of ""now"".

https://github.com/databricks/runtime/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateTimeUtils.scala#L876",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,SPARK-36544,,,,,,,,,,,,SPARK-29012,SPARK-28141,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 25 20:41:06 UTC 2021,,,,,,,,,,"0|z0rizs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/May/21 20:04;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/32714;;;","01/Jun/21 12:29;maxgekk;Issue resolved by pull request 32714
[https://github.com/apache/spark/pull/32714];;;","25/Aug/21 20:41;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/33840;;;",,,,,,,,,,,,,,,,,
Update to janino 3.1.7 to fix a bug,SPARK-35579,13381307,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,singhpk234,cloud_fan,cloud_fan,31/May/21 15:13,18/Jul/22 13:12,13/Jul/23 08:47,18/Jul/22 13:11,3.3.0,,,,,,,,,,,,,,,3.4.0,,,,SQL,,,,0,,,See the test in SPARK-35578,,apachespark,cloud_fan,fchen,LuciferYang,maropu,singhpk234,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 18 13:11:47 UTC 2022,,,,,,,,,,"0|z0risw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/21 05:43;maropu;I'm working on this. See: https://github.com/janino-compiler/janino/pull/148;;;","10/Aug/21 05:04;cloud_fan;The janino upgrade was reverted. Let's retarget to 3.3;;;","11/Jul/22 05:56;singhpk234;should we bump janino to v3.1.7 considering : [https://github.com/janino-compiler/janino/pull/148#issuecomment-1103300681]

cc [~cloud_fan] ;;;","15/Jul/22 08:05;LuciferYang;[~singhpk234] Are you interested in submitting a pr to fix this?;;;","15/Jul/22 10:33;apachespark;User 'singhpk234' has created a pull request for this issue:
https://github.com/apache/spark/pull/37202;;;","15/Jul/22 10:41;singhpk234;sure [~LuciferYang] , added a pr for the same.;;;","18/Jul/22 13:11;srowen;Issue resolved by pull request 37202
[https://github.com/apache/spark/pull/37202];;;",,,,,,,,,,,,,
Redact the sensitive info in the result of Set command,SPARK-35576,13381260,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,31/May/21 10:51,01/Jun/21 07:33,13/Jul/23 08:47,31/May/21 22:00,1.6.3,2.0.2,2.1.3,2.2.3,2.3.4,2.4.8,3.0.2,3.1.2,3.2.0,,,,,,,3.1.3,3.2.0,,,Security,SQL,,,0,,,"Currently, the results of following SQL queries are not redacted:
```
SET [KEY];
SET;
```
For example:


{code:java}
scala> spark.sql(""set javax.jdo.option.ConnectionPassword=123456"").show()
+--------------------+------+
|                 key| value|
+--------------------+------+
|javax.jdo.option....|123456|
+--------------------+------+


scala> spark.sql(""set javax.jdo.option.ConnectionPassword"").show()
+--------------------+------+
|                 key| value|
+--------------------+------+
|javax.jdo.option....|123456|
+--------------------+------+


scala> spark.sql(""set"").show()
+--------------------+--------------------+
|                 key|               value|
+--------------------+--------------------+
|javax.jdo.option....|              123456|

{code}

We should hide the sensitive information and redact the query output.",,apachespark,dongjoon,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 01 04:07:40 UTC 2021,,,,,,,,,,"0|z0riig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/May/21 10:56;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32712;;;","31/May/21 10:57;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32712;;;","31/May/21 22:00;dongjoon;This is resolved via https://github.com/apache/spark/pull/32712;;;","01/Jun/21 04:07;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32720;;;","01/Jun/21 04:07;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32720;;;",,,,,,,,,,,,,,,
Recover updating build status in GitHub Actions,SPARK-35575,13381251,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,31/May/21 10:05,12/Dec/22 18:10,13/Jul/23 08:47,31/May/21 10:30,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Project Infra,,,,0,,,"https://github.com/apache/spark/runs/2709537998?check_suite_focus=true

Currently it fails to get the run id because apparently the author removed workflows. We should make it working even when they remove or disable GA in their forked repository back.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 31 10:30:15 UTC 2021,,,,,,,,,,"0|z0rigg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/May/21 10:28;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32711;;;","31/May/21 10:28;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32711;;;","31/May/21 10:30;gurwls223;Issue resolved by pull request 32711
[https://github.com/apache/spark/pull/32711];;;",,,,,,,,,,,,,,,,,
Make SparkR tests pass with R 4.1+,SPARK-35573,13381227,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,dongjoon,dongjoon,31/May/21 07:44,12/Dec/22 18:10,13/Jul/23 08:47,01/Jun/21 01:36,3.2.0,,,,,,,,,,,,,,,3.0.3,3.1.3,3.2.0,,R,,,,0,,,"Currently, there exists 6 SparkR UT failures in R 4.1.0.
Until R 4.0.5, there was no errors.
{code}
══ Failed ══════════════════════════════════════════════════════════════════════
── 1. Failure (test_sparkSQL_arrow.R:71:3): createDataFrame/collect Arrow optimi
collect(createDataFrame(rdf)) not equal to `expected`.
Component “g”: 'tzone' attributes are inconsistent ('UTC' and '')

── 2. Failure (test_sparkSQL_arrow.R:143:3): dapply() Arrow optimization - type
collect(ret) not equal to `rdf`.
Component “b”: 'tzone' attributes are inconsistent ('UTC' and '')

── 3. Failure (test_sparkSQL_arrow.R:229:3): gapply() Arrow optimization - type
collect(ret) not equal to `rdf`.
Component “b”: 'tzone' attributes are inconsistent ('UTC' and '')

── 4. Error (test_sparkSQL.R:1454:3): column functions ─────────────────────────
Error: (converted from warning) cannot xtfrm data frames
Backtrace:
  1. base::sort(collect(distinct(select(df, input_file_name())))) test_sparkSQL.R:1454:2
  2. base::sort.default(collect(distinct(select(df, input_file_name()))))
  5. base::order(x, na.last = na.last, decreasing = decreasing)
  6. base::lapply(z, function(x) if (is.object(x)) as.vector(xtfrm(x)) else x)
  7. base:::FUN(X[[i]], ...)
 10. base::xtfrm.data.frame(x)

── 5. Failure (test_utils.R:67:3): cleanClosure on R functions ─────────────────
`actual` not equal to `g`.
names for current but not for target
Length mismatch: comparison on first 0 components

── 6. Failure (test_utils.R:80:3): cleanClosure on R functions ─────────────────
`actual` not equal to `g`.
names for current but not for target
Length mismatch: comparison on first 0 components
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 01 01:36:20 UTC 2021,,,,,,,,,,"0|z0rib4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/May/21 07:47;gurwls223;will take a look;;;","31/May/21 07:57;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32709;;;","01/Jun/21 01:36;gurwls223;Issue resolved by pull request 32709
[https://github.com/apache/spark/pull/32709];;;",,,,,,,,,,,,,,,,,
Explain cost is not showing statistics for all the nodes,SPARK-35567,13381173,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shahid,shahid,shahid,30/May/21 23:33,31/May/21 16:56,13/Jul/23 08:47,31/May/21 16:56,3.0.0,3.1.2,,,,,,,,,,,,,,3.2.0,,,,Optimizer,SQL,,,0,,,"Explain cost command doesn't show statistics for all the nodes in most of the TPCDS queries

For eg: Query1

!image-2021-05-31-05-09-09-637.png!",,apachespark,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/21 23:39;shahid;image-2021-05-31-05-09-09-637.png;https://issues.apache.org/jira/secure/attachment/13026152/image-2021-05-31-05-09-09-637.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 30 23:37:43 UTC 2021,,,,,,,,,,"0|z0rhzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/May/21 23:36;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/32704;;;","30/May/21 23:37;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/32704;;;",,,,,,,,,,,,,,,,,,
Fix number of output rows for StateStoreRestoreExec,SPARK-35566,13381171,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,viirya,viirya,30/May/21 23:07,12/Dec/22 18:10,13/Jul/23 08:47,31/May/21 07:46,3.0.2,3.1.2,3.2.0,,,,,,,,,,,,,3.0.3,3.1.3,3.2.0,,Structured Streaming,,,,0,,,Currently the number of output rows of `StateStoreRestoreExec` only counts the each input row. But it actually outputs input rows + optional restored rows. We should provide correct number of output rows.,,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 31 07:46:50 UTC 2021,,,,,,,,,,"0|z0rhyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/May/21 23:13;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32703;;;","30/May/21 23:13;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32703;;;","31/May/21 07:46;gurwls223;Issue resolved by pull request 32703
[https://github.com/apache/spark/pull/32703];;;",,,,,,,,,,,,,,,,,
partition result is incorrect when insert into partition table with int datatype partition column,SPARK-35561,13381062,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yghu,yghu,yghu,29/May/21 02:00,23/Jul/21 17:42,13/Jul/23 08:47,23/Jul/21 17:42,3.0.0,3.0.1,3.0.2,3.1.1,3.1.2,,,,,,,,,,,3.3.0,,,,SQL,,,,0,,,"when inserting into partitioned table with int datatype partition column, if partition column value is starting with 0, like 001, get wrong partition result

 

*How to reproduce the problem:*

CREATE TABLE partitiontb04 (id INT, c_string STRING) STORED AS orc; 
 insert into table partitiontb04 values (10001,'test1');
 CREATE TABLE orc_part03(id INT, c_string STRING) partitioned by (p_int int) STORED AS orc;
 insert into table orc_part03 partition (p_int=001) select * from partitiontb04 where id < 10006;
 show partitions orc_part03;

expect result:

p_int=001

 

actural result:

p_int=1

 ",,apachespark,dc-heros,Stelyus,yghu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 23 17:42:29 UTC 2021,,,,,,,,,,"0|z0rhao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/May/21 17:09;Stelyus;I personally don't expect 001 because the type is an INT, if I wanted 001, I would have use the type STRING;;;","01/Jun/21 01:27;yghu;[~Stelyus] I know,but the amazing thing is, if I execute this statement `insert into table orc_part03 partition (p_int=002) select * from partitiontb04 where id > 10006`, the partition is 002. I think we should have same behavior.;;;","12/Jul/21 03:20;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33291;;;","12/Jul/21 03:21;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33291;;;","23/Jul/21 17:42;srowen;Issue resolved by pull request 33291
[https://github.com/apache/spark/pull/33291];;;",,,,,,,,,,,,,,,
Speed up one test in AdaptiveQueryExecSuite,SPARK-35559,13381038,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cloud_fan,dongjoon,dongjoon,28/May/21 19:37,28/May/21 19:40,13/Jul/23 08:47,28/May/21 19:40,3.1.2,3.2.0,,,,,,,,,,,,,,3.1.3,3.2.0,,,Tests,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 28 19:40:49 UTC 2021,,,,,,,,,,"0|z0rh5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/May/21 19:39;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/32695;;;","28/May/21 19:40;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/32695;;;","28/May/21 19:40;dongjoon;Issue resolved by pull request 32695
[https://github.com/apache/spark/pull/32695];;;",,,,,,,,,,,,,,,,,
Small memory leak in BlockManagerMasterEndpoint ,SPARK-35543,13380732,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,attilapiros,attilapiros,attilapiros,27/May/21 10:53,22/Jun/21 09:42,13/Jul/23 08:47,07/Jun/21 13:37,3.0.0,3.0.1,3.0.2,3.1.0,3.1.1,,,,,,,,,,,3.2.0,,,,Block Manager,,,,0,,,It is regarding _blockStatusByShuffleService_ when all the blocks are removed for a bmId the map entry can be cleaned too.,,apachespark,attilapiros,shahid,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 22 09:42:51 UTC 2021,,,,,,,,,,"0|z0rf9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/21 09:21;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/32790;;;","05/Jun/21 09:22;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/32790;;;","07/Jun/21 13:37;attilapiros;Issue resolved by pull request 32790
[https://github.com/apache/spark/pull/32790];;;","22/Jun/21 09:42;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/33020;;;",,,,,,,,,,,,,,,,
"Bucketizer created for multiple columns with parameters splitsArray,  inputCols and outputCols can not be loaded after saving it.",SPARK-35542,13380721,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,weichenxu123,srikanthpusarla,srikanthpusarla,27/May/21 10:11,19/Aug/22 04:28,13/Jul/23 08:47,19/Aug/22 04:28,3.1.1,,,,,,,,,,,,,,,3.1.4,3.2.3,3.3.1,3.4.0,PySpark,,,,0,,,"Bucketizer created for multiple columns with parameters *splitsArray*,  *inputCols* and *outputCols* can not be loaded after saving it.

The problem is not seen for Bucketizer created for single column.

*Code to reproduce* 

###################################

from pyspark.ml.feature import Bucketizer
df = spark.createDataFrame([(0.1,), (0.4,), (1.2,), (1.5,)], [""values""])
bucketizer = Bucketizer(*splitsArray*= [[-float(""inf""), 0.5, 1.4, float(""inf"")], [-float(""inf""), 0.1, 1.2, float(""inf"")]], *inputCols*=[""values"", ""values""], *outputCols*=[""b1"", ""b2""])
bucketed = bucketizer.transform(df).collect()
dfb = bucketizer.transform(df)
print(dfb.show())
bucketizerPath = ""dbfs:/mnt/S3-Bucket/"" + ""Bucketizer""
bucketizer.write().overwrite().save(bucketizerPath)
loadedBucketizer = {color:#FF0000}Bucketizer.load(bucketizerPath)    #### Failing here{color}
loadedBucketizer.getSplits() == bucketizer.getSplits()

############################################################

The error message is 

{color:#FF0000}*TypeError: array() argument 1 must be a unicode character, not bytes*{color}

 

*BackTrace:*

 
--------------------------------------------------------------------------
TypeError Traceback (most recent call last) <command-3999490> in <module> 15 16 bucketizer.write().overwrite().save(bucketizerPath)
---> 17 loadedBucketizer = Bucketizer.load(bucketizerPath)
18 loadedBucketizer.getSplits() == bucketizer.getSplits()
 
/databricks/spark/python/pyspark/ml/util.py in load(cls, path)
376 def load(cls, path):
377 """"""Reads an ML instance from the input path, a shortcut of `read().load(path)`.""""""
--> 378 return cls.read().load(path)
379
380
 
/databricks/spark/python/pyspark/ml/util.py in load(self, path)
330 raise NotImplementedError(""This Java ML type cannot be loaded into Python currently: %r""
331 % self._clazz)
--> 332 return self._clazz._from_java(java_obj)
333
334
 
def session(self, sparkSession): /databricks/spark/python/pyspark/ml/wrapper.py in _from_java(java_stage)
258
259 py_stage._resetUid(java_stage.uid())
--> 260 py_stage._transfer_params_from_java()
261 elif hasattr(py_type, ""_from_java""):
262 py_stage = py_type._from_java(java_stage)
 
/databricks/spark/python/pyspark/ml/wrapper.py in _transfer_params_from_java(self)
186 # SPARK-14931: Only check set params back to avoid default params mismatch.
187 if self._java_obj.isSet(java_param): --> 
188 value = _java2py(sc, self._java_obj.getOrDefault(java_param))
189 self._set(**{param.name: value})
190 # SPARK-10931: Temporary fix for params that have a default in Java
 
/databricks/spark/python/pyspark/ml/common.py in _java2py(sc, r, encoding)
107
108 if isinstance(r, (bytearray, bytes)):
--> 109 r = PickleSerializer().loads(bytes(r), encoding=encoding)
110  return r
111
 
/databricks/spark/python/pyspark/serializers.py in loads(self, obj, encoding)
467
468 def loads(self, obj, encoding=""bytes""):
--> 469 return pickle.loads(obj, encoding=encoding)
470
471
 
TypeError: array() argument 1 must be a unicode character, not bytes
 
 ",{color:#172b4d}DataBricks Spark 3.1.1{color},apachespark,srikanthpusarla,viirya,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/May/21 10:19;srikanthpusarla;Code-error.PNG;https://issues.apache.org/jira/secure/attachment/13026037/Code-error.PNG","27/May/21 10:19;srikanthpusarla;traceback.png;https://issues.apache.org/jira/secure/attachment/13026038/traceback.png",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 19 04:28:54 UTC 2022,,,,,,,,,,"0|z0rf6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/21 02:25;srowen;Huh, yeah I see that too. Weird, as I'd think this would have turned up elsewhere as Bucketizer shares code about inputCols with other classes.
This looks like https://issues.apache.org/jira/browse/SPARK-7379 which was never resolved. I don't suppose you have an idea about a fix? I am not sure how to address it.

[~viirya] I don't suppose you have any ideas? I think you touched the pickle code here last, but it may not be a problem with that part.;;;","23/Jun/21 02:51;viirya;So this is PySpark only issue and Scala Bucketizer is fine? Which pickle code you mean? cloudpickle.py? Or SPARK-24058?;;;","18/Aug/22 10:24;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/37568;;;","19/Aug/22 04:28;weichenxu123;Issue resolved by pull request 37568
[https://github.com/apache/spark/pull/37568];;;",,,,,,,,,,,,,,,,
Can not insert into hive bucket table if create table with upper case schema,SPARK-35531,13380485,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,opensky,opensky,26/May/21 07:39,23/Nov/22 06:52,13/Jul/23 08:47,11/Oct/21 10:02,3.0.0,3.1.1,3.2.0,,,,,,,,,,,,,3.1.4,3.3.0,,,SQL,,,,0,,," 

 

create table TEST1(
 V1 BIGINT,
 S1 INT)
 partitioned by (PK BIGINT)
 clustered by (V1)
 sorted by (S1)
 into 200 buckets
 STORED AS PARQUET;

 

insert into test1
 select
 * from values(1,1,1);

 

 

org.apache.hadoop.hive.ql.metadata.HiveException: Bucket columns V1 is not part of the table columns ([FieldSchema(name:v1, type:bigint, comment:null), FieldSchema(name:s1, type:int, comment:null)]
org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Bucket columns V1 is not part of the table columns ([FieldSchema(name:v1, type:bigint, comment:null), FieldSchema(name:s1, type:int, comment:null)]",,angerszhuuu,apachespark,cloud_fan,Gengliang.Wang,holden,opensky,tanvu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 23 06:52:04 UTC 2022,,,,,,,,,,"0|z0rdqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/May/21 07:52;apachespark;User 'opensky142857' has created a pull request for this issue:
https://github.com/apache/spark/pull/32675;;;","26/May/21 07:54;apachespark;User 'opensky142857' has created a pull request for this issue:
https://github.com/apache/spark/pull/32675;;;","13/Sep/21 19:56;holden;Did this use to work?;;;","08/Oct/21 03:31;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34218;;;","08/Oct/21 05:20;Gengliang.Wang;I can reproduce the issue on 3.0.0 and 3.1.1. 
It's a long-standing bug.;;;","11/Oct/21 10:02;cloud_fan;Issue resolved by pull request 34218
[https://github.com/apache/spark/pull/34218];;;","30/Jan/22 17:54;tanvu;Hi, JFYI, I tested the above commands on my local PC with {color:#172b4d}*spark-3.1.2-bin-hadoop3.2 distribution* and the same error happened.{color}

{color:#172b4d}So I think the issue is on 3.1.2 too.{color};;;","08/Feb/22 06:20;cloud_fan;[~angerszhuuu] can you help to backport it?;;;","08/Feb/22 06:23;angerszhuuu;Sure.;;;","10/Feb/22 08:37;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/35475;;;","10/Feb/22 08:38;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/35475;;;","03/Nov/22 09:58;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/38495;;;","23/Nov/22 06:51;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/38765;;;","23/Nov/22 06:52;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/38765;;;",,,,,,
Fix rounding error in DifferentiableLossAggregatorSuite with Java 11,SPARK-35530,13380482,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,26/May/21 07:16,12/Dec/22 18:10,13/Jul/23 08:47,28/May/21 02:39,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,ML,Tests,,,0,,,"

I'm personally checking whether all the tests pass with Java 11 for the current master and I found DifferentiableLossAggregatorSuite fails.
https://github.com/sarutak/spark/runs/2661859541?check_suite_focus=true#step:9:13895

The reason seems that the implementation of Blas.daxpy is different between for Java 8 and Java 11. For Java 11, Math.fma is used.

https://github.com/luhenry/netlib/blob/v2.2.0/blas/src/main/java/dev/ludovic/netlib/blas/Java8BLAS.java#L92
https://github.com/luhenry/netlib/blob/0053ea30b11686336cbdb8c7fceb41d59d268fa2/blas/src/main/java/dev/ludovic/netlib/blas/Java11BLAS.java#L40",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 28 02:39:55 UTC 2021,,,,,,,,,,"0|z0rdq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/May/21 07:22;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32673;;;","28/May/21 02:39;gurwls223;fixed in https://github.com/apache/spark/pull/32673;;;",,,,,,,,,,,,,,,,,,
Fix HiveExternalCatalogVersionsSuite to pass with Java 11,SPARK-35527,13380460,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,26/May/21 04:27,12/Dec/22 18:10,13/Jul/23 08:47,26/May/21 08:21,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,Tests,,,0,,,"I'm personally checking whether all the tests pass with Java 11 for the current master and I found HiveExternalCatalogVersionsSuite fails.
The reason is that Spark 3.0.2 and 3.1.1 doesn't accept 2.3.8 as a hive metastore version.

HiveExternalCatalogVersionsSuite downloads Spark releases from https://dist.apache.org/repos/dist/release/spark/ and run test for each release. The Spark releases are 3.0.2 and 3.1.1 for now.

With Java 11, the suite runs with a hive metastore version which corresponds to the builtin Hive version and it's 2.3.8 for the current master.

But branch-3.0 and branch-3.1 doesn't accept 2.3.8, the suite with Java 11 fails.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 26 08:21:14 UTC 2021,,,,,,,,,,"0|z0rdl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/May/21 04:36;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32670;;;","26/May/21 08:21;gurwls223;Issue resolved by pull request 32670
[https://github.com/apache/spark/pull/32670];;;",,,,,,,,,,,,,,,,,,
Storage UI tab Storage Level tool tip correction,SPARK-35516,13380357,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,lidiyag,jobitmathew,jobitmathew,25/May/21 14:09,01/Jun/21 13:04,13/Jul/23 08:47,01/Jun/21 10:04,3.1.1,,,,,,,,,,,,,,,3.2.0,,,,Web UI,,,,0,,,"Storage UI tab Storage Level tool tip correction required.
|</span>|
| |</th><th width="""" class="""">|
| |<span data-toggle=""tooltip"" title=""StorageLevel displays where the persisted RDD is stored, format of the persisted RDD (serialized or de-serialized) andreplication factor of the persisted RDD"">|
| |Storage Level|
| |</span>|

please change *andreplication * to *and replication*",,apachespark,jobitmathew,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 01 10:04:51 UTC 2021,,,,,,,,,,"0|z0rcy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/May/21 20:17;apachespark;User 'lidiyag' has created a pull request for this issue:
https://github.com/apache/spark/pull/32664;;;","25/May/21 20:27;lidiyag;I have raised a fix for this

https://github.com/apache/spark/pull/32664;;;","01/Jun/21 10:04;sarutak;Issue resolved in https://github.com/apache/spark/pull/32664.;;;",,,,,,,,,,,,,,,,,
pyspark partitionBy may encounter 'OverflowError: cannot convert float infinity to integer',SPARK-35512,13380270,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nolan.liou,nolan.liou,nolan.liou,25/May/21 08:49,12/Dec/22 18:10,13/Jul/23 08:47,09/Jun/21 01:59,3.0.2,,,,,,,,,,,,,,,3.2.0,,,,PySpark,,,,0,,,"h2. Code sample
{code:python}
# pyspark
rdd = ...
new_rdd = rdd.partitionBy(64){code}
An OverflowError is raised when there is a {color:#ff0000}big input file{color} and {color:#ff0000}executor memory{color} is not big enough.
h2. Error information: 

 
{code:java}
TaskSetManager: Lost task 312.0 in stage 1.0 (TID 748, 11.4.137.5, executor 83): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
File ""/opt/spark3/python/lib/pyspark.zip/pyspark/worker.py"", line 605, in main
process()
File ""/opt/spark3/python/lib/pyspark.zip/pyspark/worker.py"", line 597, in process
serializer.dump_stream(out_iter, outfile)
File ""/opt/spark3/python/lib/pyspark.zip/pyspark/serializers.py"", line 141, in dump_stream
for obj in iterator:
File ""/opt/spark3/python/lib/pyspark.zip/pyspark/rdd.py"", line 1899, in add_shuffle_key
OverflowError: cannot convert float infinity to integer
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)
at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)
at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)
at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)
at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:156)
at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
at org.apache.spark.scheduler.Task.run(Task.scala:130)
at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1420)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748){code}
h2. Spark code

 [https://github.com/apache/spark/blob/master/python/pyspark/rdd.py#L2072]
{code:python}
            for k, v in iterator:
                buckets[partitionFunc(k) % numPartitions].append((k, v))
                c += 1                # check used memory and avg size of chunk of objects 
                if (c % 1000 == 0 and get_used_memory() > limit
                        or c > batch):
                    n, size = len(buckets), 0
                    for split in list(buckets.keys()):
                        yield pack_long(split)
                        d = outputSerializer.dumps(buckets[split])
                        del buckets[split]
                        yield d
                        size += len(d)                    avg = int(size / n) >> 20
                    # let 1M < avg < 10M
                    if avg < 1:
                        batch *= 1.5
                    elif avg > 10:
                        batch = max(int(batch / 1.5), 1)
                    c = 0
{code}
h2. Explanation

*`batch`* may grow infinity when `*get_used_memory() > limit*` is true, then overflow at `*max(int(batch / 1.5), 1)*`

 

 ",,apachespark,nolan.liou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 09 01:59:18 UTC 2021,,,,,,,,,,"0|z0rcew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/May/21 03:27;apachespark;User 'nolanliou' has created a pull request for this issue:
https://github.com/apache/spark/pull/32667;;;","09/Jun/21 01:59;gurwls223;Issue resolved by pull request 32667
[https://github.com/apache/spark/pull/32667];;;",,,,,,,,,,,,,,,,,,
count distinct asterisk ,SPARK-35504,13380158,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,Melchizedek13,Melchizedek13,24/May/21 17:19,12/Dec/22 18:10,13/Jul/23 08:47,25/May/21 12:37,3.0.0,,,,,,,,,,,,,,,,,,,SQL,,,,0,,,"Hi everyone,

I hope you're well!

 

Today I came across a very interesting case when the result of the execution of the algorithm for counting unique rows differs depending on the form (count(distinct *) vs count( * ) from derived table) of the Spark SQL queries.

I still can't figure out on my own if this is a bug or a feature and I would like to share what I found.

 

I run Spark SQL queries through the Thrift (and not only) connecting to the Spark cluster. I use the DBeaver app to execute Spark SQL queries.

 

So, I have two identical Spark SQL queries from an algorithmic point of view that return different results.

 

The first query:
{code:sql}
select count(distinct *) unique_amt from storage_datamart.olympiads
; -- Rows: 13437678
{code}
 

The second query:
{code:sql}
select count(*) from (select distinct * from storage_datamart.olympiads)
; -- Rows: 36901430
{code}
 

The result of the two queries is different. (But it must be the same, right!?)
{code:sql}
select 'The first query' description, count(distinct *) unique_amt from storage_datamart.olympiads
 union all
select 'The second query', count(*) from (select distinct * from storage_datamart.olympiads)
;
{code}
 

The result of the above query is the following:
{code:java}
The first query    13437678
The second query   36901430
{code}
 
 I can easily calculate the unique number of rows in the table:
{code:sql}
select count(*) from (
  select student_id, olympiad_id, tour, grade
    from storage_datamart.olympiads
   group by student_id, olympiad_id, tour, grade
  having count(*) = 1
)
; -- Rows: 36901365
{code}
 

The table DDL is the following:
{code:sql}
CREATE TABLE `storage_datamart`.`olympiads` (
  `ptn_date` DATE,
  `student_id` BIGINT,
  `olympiad_id` STRING,
  `grade` BIGINT,
  `grade_type` STRING,
  `tour` STRING,
  `created_at` TIMESTAMP,
  `created_at_local` TIMESTAMP,
  `olympiad_num` BIGINT,
  `olympiad_name` STRING,
  `subject` STRING,
  `started_at` TIMESTAMP,
  `ended_at` TIMESTAMP,
  `region_id` BIGINT,
  `region_name` STRING,
  `municipality_name` STRING,
  `school_id` BIGINT,
  `school_name` STRING,
  `school_status` BOOLEAN,
  `oly_n_common` INT,
  `num_day` INT,
  `award_type` STRING,
  `new_student_legacy` INT,
  `segment` STRING,
  `total_start` TIMESTAMP,
  `total_end` TIMESTAMP,
  `year_learn` STRING,
  `parent_id` BIGINT,
  `teacher_id` BIGINT,
  `parallel` BIGINT,
  `olympiad_type` STRING)
USING parquet
LOCATION 's3a://uchiru-bi-dwh/storage/datamart/olympiads.parquet'
;
{code}
 

Could you please tell me why in the first Spark SQL query counting the unique number of rows using the construction `count(distinct *)` does not count correctly and why the result of the two Spark SQL queries is different??

Thanks in advance.

 

p.s. I could not find a description of such behaviour of the function `count(distinct *)` in the [official Spark documentation|https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#aggregate-functions]:
{quote}count(DISTINCT expr[, expr...]) -> Returns the number of rows for which the supplied expression(s) are unique and non-null.
{quote}
 ","{code:java}
uname -a

Linux 5.4.0-1038-aws #40~18.04.1-Ubuntu SMP Sat Feb 6 01:56:56 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
{code}
 
{code:java}
lsb_release -a

No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 18.04.4 LTS
Release:	18.04
Codename:	bionic
{code}
 
{code:java}
/opt/spark/bin/spark-submit --version

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0
      /_/

Using Scala version 2.12.10, OpenJDK 64-Bit Server VM, 1.8.0_292
Branch HEAD
Compiled by user ubuntu on 2020-06-06T13:05:28Z
Revision 3fdfce3120f307147244e5eaf46d61419a723d50
Url https://gitbox.apache.org/repos/asf/spark.git
Type --help for more information.
{code}
{code:java}
lscpu

Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Byte Order:          Little Endian
CPU(s):              4
On-line CPU(s) list: 0-3
Thread(s) per core:  2
Core(s) per socket:  2
Socket(s):           1
NUMA node(s):        1
Vendor ID:           GenuineIntel
CPU family:          6
Model:               85
Model name:          Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz
Stepping:            7
CPU MHz:             3602.011
BogoMIPS:            6000.01
Hypervisor vendor:   KVM
Virtualization type: full
L1d cache:           32K
L1i cache:           32K
L2 cache:            1024K
L3 cache:            36608K
NUMA node0 CPU(s):   0-3
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke
{code}
 ",Melchizedek13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/21 17:49;Melchizedek13;SPARK-35504_first_query_plan.log;https://issues.apache.org/jira/secure/attachment/13025863/SPARK-35504_first_query_plan.log","24/May/21 17:49;Melchizedek13;SPARK-35504_second_query_plan.log;https://issues.apache.org/jira/secure/attachment/13025864/SPARK-35504_second_query_plan.log",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 26 01:38:28 UTC 2021,,,,,,,,,,"0|z0rbq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/May/21 17:50;Melchizedek13;Added execution plans for two SparkSQL queries:
 * [^SPARK-35504_first_query_plan.log]
 * [^SPARK-35504_second_query_plan.log];;;","25/May/21 03:50;gurwls223;Just a wild guess but:

{quote}
count(DISTINCT expr[, expr...])
{quote}

doesn't count NULLs but:

{quote}
count\(*\) from (select distinct * from storage_datamart.olympiads)
{quote}

counts nulls?

;;;","25/May/21 11:44;Melchizedek13;[~hyukjin.kwon] thanks for the hint!

 

I've just counted any nulls column's value by using the following script:
{code:sql}
select count(1)
  from storage_datamart.olympiads
 where ptn_date is null
    or student_id is null
    or olympiad_id is null
    or grade is null
    or grade_type is null
    or tour is null
    or created_at is null
    or created_at_local is null
    or olympiad_num is null
    or olympiad_name is null
    or subject is null
    or started_at is null
    or ended_at is null
    or region_id is null
    or region_name is null
    or municipality_name is null
    or school_id is null
    or school_name is null
    or school_status is null
    or oly_n_common is null
    or num_day is null
    or award_type is null
    or new_student_legacy is null
    or segment is null
    or total_start is null
    or total_end is null
    or year_learn is null
    or parent_id is null
    or teacher_id is null
    or parallel is null
    or olympiad_type is null
;
{code}
 

I've got 23463820 rows.;;;","25/May/21 11:53;Melchizedek13;It's really close to true:

{code:sql}
select 36901430 - 23463820
-- 13437610
{code}

[~hyukjin.kwon] thank you!
  ;;;","25/May/21 12:11;Melchizedek13;Subtracted from the number of all rows of the table the number of rows containing NULL values in at least one column and got what I was looking for:

 
{code:sql}
select (select count(1) amt from storage_datamart.olympiads) - 
(
select count(1)
  from storage_datamart.olympiads
 where ptn_date is null
    or student_id is null
    or olympiad_id is null
    or grade is null
    or grade_type is null
    or tour is null
    or created_at is null
    or created_at_local is null
    or olympiad_num is null
    or olympiad_name is null
    or subject is null
    or started_at is null
    or ended_at is null
    or region_id is null
    or region_name is null
    or municipality_name is null
    or school_id is null
    or school_name is null
    or school_status is null
    or oly_n_common is null
    or num_day is null
    or award_type is null
    or new_student_legacy is null
    or segment is null
    or total_start is null
    or total_end is null
    or year_learn is null
    or parent_id is null
    or teacher_id is null
    or parallel is null
    or olympiad_type is null
)
;  -- 13437678
{code}
{code:sql}
select amt - 23463820 from (
    select count(1) amt
      from storage_datamart.olympiads
)
;  -- 13437678
{code}
 

This is a feature that is documented.

I apologize.

I'll close this task.

 

Thank you!
  ;;;","25/May/21 12:37;Melchizedek13;I could not fully comprehend what was written in the documentation.

Helped to figure it out.;;;","26/May/21 01:38;gurwls223;Thanks for confirmation and investigation man ;;;",,,,,,,,,,,,,
spark.blockManager.port does not work for driver pod,SPARK-35493,13379978,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,23/May/21 10:12,23/May/21 15:14,13/Jul/23 08:47,23/May/21 15:14,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,2.4.5,2.4.6,2.4.7,2.4.8,3.0.2,3.1.1,3.2.0,,,,3.0.3,3.1.2,3.2.0,,Kubernetes,,,,0,,,spark.blockManager.port should work as fallback conf for Kubernetes as same as other cluster managers,,apachespark,dongjoon,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 23 15:14:29 UTC 2021,,,,,,,,,,"0|z0ram8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/May/21 10:24;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/32639;;;","23/May/21 15:14;dongjoon;Issue resolved by pull request 32639
[https://github.com/apache/spark/pull/32639];;;",,,,,,,,,,,,,,,,,,
Upgrade ORC to 1.6.8,SPARK-35489,13379931,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,22/May/21 15:04,22/May/21 17:36,13/Jul/23 08:47,22/May/21 17:36,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Build,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 22 17:36:05 UTC 2021,,,,,,,,,,"0|z0rac0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/May/21 15:08;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32635;;;","22/May/21 17:36;dongjoon;Issue resolved by pull request 32635
[https://github.com/apache/spark/pull/32635];;;",,,,,,,,,,,,,,,,,,
MemoryConsumer reservations that trigger a partial self-spill can fail even if memory is available,SPARK-35486,13379839,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankurd,ankurd,ankurd,21/May/21 18:58,26/May/21 07:20,13/Jul/23 08:47,25/May/21 10:14,2.4.8,3.0.2,3.1.1,,,,,,,,,,,,,3.2.0,,,,Spark Core,,,,0,,,"When a memory reservation triggers a self-spill, ExecutionMemoryPool#releaseMemory() will immediately notify waiting tasks that memory has been freed. If there are any waiting tasks with less than 1/2N of the memory pool, they may acquire the newly-freed memory before the current task has a chance to do so. This will cause the original memory reservation to fail. If the initial spill did not release all available memory, the reservation could have been satisfied by asking it to spill again.

For example, the following test fails when added to MemoryManagerSuite:

{code:scala}
test(""SPARK-35486: memory freed by self-spilling is taken by another task"") {
    val memoryManager = createMemoryManager(1000L)
    val t1MemManager = new TaskMemoryManager(memoryManager, 1)
    val t2MemManager = new TaskMemoryManager(memoryManager, 2)
    val c1 = new TestPartialSpillingMemoryConsumer(t1MemManager)
    val c2 = new TestMemoryConsumer(t2MemManager)
    val futureTimeout: Duration = 20.seconds

    // t1 acquires 1000 bytes. This should succeed immediately.
    val t1Result1 = Future { c1.acquireMemory(1000L) }
    assert(ThreadUtils.awaitResult(t1Result1, futureTimeout) === 1000L)
    assert(c1.getUsed() === 1000L)

    // t2 attempts to acquire 500 bytes. This should block since there is no memory available.
    val t2Result1 = Future { c2.acquireMemory(500L) }
    Thread.sleep(300)
    assert(!t2Result1.isCompleted)
    assert(c2.getUsed() === 0L)

    // t1 attempts to acquire 500 bytes, causing its existing reservation to spill partially. t2 is
    // first in line for the freed memory.
    //
    // SPARK-35486: This currently causes the reservation to fail. Instead, t1 should try again,
    // causing the rest of the reservation to spill.
    val t1Result2 = Future { c1.acquireMemory(500L) }

    // The spill should release enough memory for both t1's and t2's reservations to be satisfied.
    assert(ThreadUtils.awaitResult(t2Result1, futureTimeout) === 500L)
    // SPARK-35486: This assertion fails: 0L != 500L.
    // assert(ThreadUtils.awaitResult(t1Result2, futureTimeout) === 500L)
  }
{code}


{code:java}
/**
 * A TestMemoryConsumer which, when asked to spill, releases only enough memory to satisfy the
 * request rather than releasing all its memory.
 */
public class TestPartialSpillingMemoryConsumer extends TestMemoryConsumer {
  public TestPartialSpillingMemoryConsumer(TaskMemoryManager memoryManager, MemoryMode mode) {
    super(memoryManager, mode);
  }
  public TestPartialSpillingMemoryConsumer(TaskMemoryManager memoryManager) {
    super(memoryManager);
  }

  @Override
  public long spill(long size, MemoryConsumer trigger) throws IOException {
    long used = getUsed();
    long released = Math.min(used, size);
    free(released);
    return released;
  }
}
{code}
",,ankurd,apachespark,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 26 07:20:11 UTC 2021,,,,,,,,,,"0|z0r9rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/May/21 19:40;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/32625;;;","25/May/21 10:14;Ngone51;Issue resolved by https://github.com/apache/spark/pull/32625;;;","26/May/21 07:19;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32672;;;","26/May/21 07:20;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32672;;;",,,,,,,,,,,,,,,,
case sensitive block manager port key should be used in BasicExecutorFeatureStep,SPARK-35482,13379755,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,21/May/21 12:04,22/May/21 04:58,13/Jul/23 08:47,21/May/21 15:45,2.3.0,2.3.1,2.3.2,2.3.3,2.3.4,2.4.8,3.0.2,3.1.1,3.2.0,,,,,,,3.0.3,3.1.2,3.2.0,,Kubernetes,,,,0,,,"spark.block[m|M]anger.port is not the same",,apachespark,dongjoon,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 21 18:18:24 UTC 2021,,,,,,,,,,"0|z0r98w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/May/21 13:03;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/32621;;;","21/May/21 13:04;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/32621;;;","21/May/21 15:45;dongjoon;This is resolved via https://github.com/apache/spark/pull/32621;;;","21/May/21 18:18;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/32624;;;","21/May/21 18:18;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/32624;;;",,,,,,,,,,,,,,,
percentile_approx function doesn't work with pivot,SPARK-35480,13379638,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,chrismbryant,chrismbryant,21/May/21 04:17,12/Dec/22 18:11,13/Jul/23 08:47,22/May/21 22:36,3.1.1,,,,,,,,,,,,,,,3.2.0,,,,PySpark,SQL,,,0,,,"The percentile_approx PySpark function does not appear to treat the ""accuracy"" parameter correctly when pivoting on a column, causing the query below to fail (this also fails if the accuracy parameter is left unspecified):
----
{{import pyspark.sql.functions as F}}

{{df = sc.parallelize([}}
 {{    [""a"", -1.0],}}
 {{    [""a"", 5.5],}}
 {{    [""a"", 2.5],}}
 {{    [""b"", 3.0],}}
 {{    [""b"", 5.2]}}
 {{]).toDF([""type"", ""value""])}}
 {{    .groupBy()}}
 {{    .pivot(""type"", [""a"", ""b""])}}
 {{    .agg(F.percentile_approx(""value"", [0.5], 10000).alias(""percentiles""))}}
----
Error message: 

{{AnalysisException: cannot resolve 'percentile_approx((IF((`type` <=> CAST('a' AS STRING)), `value`, CAST(NULL AS DOUBLE))), (IF((`type` <=> CAST('a' AS STRING)), array(0.5D), NULL)), (IF((`type` <=> CAST('a' AS STRING)), 10000, CAST(NULL AS INT))))' due to data type mismatch: The accuracy or percentage provided must be a constant literal; 'Aggregate [percentile_approx(if ((type#242 <=> cast(a as string))) value#243 else cast(null as double), if ((type#242 <=> cast(a as string))) array(0.5) else cast(null as array<double>), if ((type#242 <=> cast(a as string))) 10000 else cast(null as int), 0, 0) AS a#251, percentile_approx(if ((type#242 <=> cast(b as string))) value#243 else cast(null as double), if ((type#242 <=> cast(b as string))) array(0.5) else cast(null as array<double>), if ((type#242 <=> cast(b as string))) 10000 else cast(null as int), 0, 0) AS b#253|#242 <=> cast(a as string))) value#243 else cast(null as double), if ((type#242 <=> cast(a as string))) array(0.5) else cast(null as array<double>), if ((type#242 <=> cast(a as string))) 10000 else cast(null as int), 0, 0) AS a#251, percentile_approx(if ((type#242 <=> cast(b as string))) value#243 else cast(null as double), if ((type#242 <=> cast(b as string))) array(0.5) else cast(null as array<double>), if ((type#242 <=> cast(b as string))) 10000 else cast(null as int), 0, 0) AS b#253] +- LogicalRDD [type#242, value#243|#242, value#243], false}}

 ",,apachespark,chrismbryant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 22 22:36:41 UTC 2021,,,,,,,,,,"0|z0r8iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/May/21 10:02;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32619;;;","21/May/21 10:03;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32619;;;","22/May/21 22:36;gurwls223;Fixed in https://github.com/apache/spark/pull/32619;;;",,,,,,,,,,,,,,,,,
Skip checking checksum on a system doesn't have `shasum`,SPARK-35463,13379584,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,20/May/21 20:54,23/May/21 00:33,13/Jul/23 08:47,20/May/21 21:38,3.0.3,3.1.2,3.2.0,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,Build,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 23 00:33:58 UTC 2021,,,,,,,,,,"0|z0r86w:",9223372036854775807,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,,,,,,,"20/May/21 21:01;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32613;;;","20/May/21 21:01;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32613;;;","20/May/21 21:38;dongjoon;Issue resolved by pull request 32613
[https://github.com/apache/spark/pull/32613];;;","23/May/21 00:33;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32637;;;",,,,,,,,,,,,,,,,
ARM CI failed: failed to validate maven sha512,SPARK-35458,13379447,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yikunkero,yikunkero,yikunkero,20/May/21 06:56,20/May/21 21:00,13/Jul/23 08:47,20/May/21 20:59,3.2.0,,,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,Build,,,,0,,,"Log:

 

Veryfing checksum from /home/jenkins/workspace/spark-master-test-maven-arm/build/apache-maven-3.6.3-bin.tar.gz.sha512
 *Unknown option: q*
 *Type shasum -h for help*
 Bad checksum from [https://archive.apache.org/dist/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz.sha512]

 

Looks like shasum validation had some wrong change in:

[https://github.com/apache/spark/commit/6c5fcac6b787d01ebf3d9f53410db2c894ab9abd#diff-590845f9441f6be1f05f517fd1caf31d64d0b5126ea9a2a13d79c74f761417ce]

 

[1] [https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-arm/]

[2] [https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.7/]",,apachespark,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-35373,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 20 20:59:55 UTC 2021,,,,,,,,,,"0|z0r7cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/May/21 07:46;yikunkero;This is not a Arm only problem, but also a problem on the system which is using shasum version before 6.x.;;;","20/May/21 07:58;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32604;;;","20/May/21 20:59;srowen;Issue resolved by pull request 32604
[https://github.com/apache/spark/pull/32604];;;",,,,,,,,,,,,,,,,,
Ambiguous self-join doesn't fail after transfroming the dataset to dataframe,SPARK-35454,13379419,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,20/May/21 03:27,28/May/21 13:05,13/Jul/23 08:47,21/May/21 09:22,3.1.0,3.1.1,,,,,,,,,,,,,,3.1.3,3.2.0,,,SQL,,,,0,,,"{code:java}
test(""SPARK-28344: fail ambiguous self join - Dataset.colRegex as column ref"") {
  val df1 = spark.range(3)
  val df2 = df1.filter($""id"" > 0)

  withSQLConf(
    SQLConf.FAIL_AMBIGUOUS_SELF_JOIN_ENABLED.key -> ""true"",
    SQLConf.CROSS_JOINS_ENABLED.key -> ""true"") {
    assertAmbiguousSelfJoin(df1.join(df2, df1.colRegex(""id"") > df2.colRegex(""id"")))
  }
}
{code}
For this unit test, if we append `.toDF()` to both df1 and df2, the query won't fail. ",,apachespark,dongjoon,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33536,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 28 07:29:55 UTC 2021,,,,,,,,,,"0|z0r768:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/May/21 02:15;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/32616;;;","21/May/21 15:48;dongjoon;This is reverted from `branch-3.1` due to UT failure. Please refer the discussion on the PR.;;;","28/May/21 07:29;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/32692;;;",,,,,,,,,,,,,,,,,
"Investigate the failure of ""PVs with local storage"" integration test on Docker driver",SPARK-35430,13378944,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,18/May/21 03:55,02/Aug/21 16:19,13/Jul/23 08:47,02/Aug/21 16:19,3.2.0,,,,,,,,,,,,,,,3.3.0,,,,Kubernetes,,,,0,,,"With https://issues.apache.org/jira/browse/SPARK-34738 integration tests are migrated to docker but ""PVs with local storage"" was failing so we created a separate test tag in https://github.com/apache/spark/pull/31829 called ""persistentVolume"" test tag which not used by the dev-run-integration-tests.sh so this way that tests is skipped.

Here we should revert ""persistentVolume"" and investigate the error.",,apachespark,attilapiros,shaneknapp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 02 16:19:06 UTC 2021,,,,,,,,,,"0|z0r48o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/21 03:50;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/32793;;;","06/Jun/21 03:51;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/32793;;;","02/Aug/21 16:19;shaneknapp;Issue resolved by pull request 32793
[https://github.com/apache/spark/pull/32793];;;",,,,,,,,,,,,,,,,,
Pin jinja2 in spark-rm/Dockerfile and add as a required dependency in the release README.md,SPARK-35425,13378807,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,17/May/21 12:33,15/Nov/21 18:26,13/Jul/23 08:47,18/May/21 07:50,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,Documentation,,,,0,,,"SPARK-35375 confined the version of Jinja to <3.0.0.
So it's good to note about it in docs/README.md",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 18 08:53:02 UTC 2021,,,,,,,,,,"0|z0r3e8:",9223372036854775807,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,,,,,,,"17/May/21 12:41;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32573;;;","17/May/21 12:42;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32573;;;","18/May/21 07:50;sarutak;This issue was resolved in https://github.com/apache/spark/pull/32573.;;;","18/May/21 08:15;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32579;;;","18/May/21 08:53;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32580;;;",,,,,,,,,,,,,,,
The output of PCA is inconsistent,SPARK-35423,13378771,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shahid,cqfrog,cqfrog,17/May/21 09:49,09/Jun/21 15:24,13/Jul/23 08:47,09/Jun/21 15:24,3.1.1,,,,,,,,,,,,,,,3.2.0,,,,MLlib,,,,0,,,"1. The example from doc

 
{code:java}
import org.apache.spark.ml.feature.PCA
import org.apache.spark.ml.linalg.Vectors

val data = Array(
  Vectors.sparse(5, Seq((1, 1.0), (3, 7.0))),
  Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),
  Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)
)
val df = spark.createDataFrame(data.map(Tuple1.apply)).toDF(""features"")

val pca = new PCA()
  .setInputCol(""features"")
  .setOutputCol(""pcaFeatures"")
  .setK(3)
  .fit(df)

val result = pca.transform(df).select(""pcaFeatures"")
result.show(false)
{code}
 

 

the output show:
{code:java}
+-----------------------------------------------------------+
|pcaFeatures                                                |
+-----------------------------------------------------------+
|[1.6485728230883807,-4.013282700516296,-5.524543751369388] |
|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|
|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |
+-----------------------------------------------------------+
{code}
2. change the Vector format

I modified the code from ""Vectors.sparse(5, Seq((1, 1.0), (3, 7.0)))"" to ""Vectors.dense(0.0,1.0,0.0,7.0,0.0)"" 。

but the output show：
{code:java}
+------------------------------------------------------------+
|pcaFeatures                                                 |
+------------------------------------------------------------+
|[1.6485728230883814,-4.0132827005162985,-1.0091435193998504]|
|[-4.645104331781533,-1.1167972663619048,-1.0091435193998501]|
|[-6.428880535676488,-5.337951427775359,-1.009143519399851]  |
+------------------------------------------------------------+
{code}
It's strange that the two outputs are inconsistent. Why?

Thanks.

 ",Spark Version： 3.1.1 ,apachespark,cqfrog,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 09 15:24:04 UTC 2021,,,,,,,,,,"0|z0r368:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/21 05:24;shahid;I would like to analyse this issue;;;","01/Jun/21 18:57;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/32734;;;","09/Jun/21 15:24;srowen;Issue resolved by pull request 32734
[https://github.com/apache/spark/pull/32734];;;",,,,,,,,,,,,,,,,,
Replace the usage of toStringHelper with ToStringBuilder,SPARK-35420,13378714,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,17/May/21 04:37,18/May/21 07:51,13/Jul/23 08:47,17/May/21 12:48,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Build,,,,0,,,SPARK-30272 removed the usage of Guava that breaks in 27 but toStringHelper is introduced again.,,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 17 12:48:21 UTC 2021,,,,,,,,,,"0|z0r2tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/May/21 04:49;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32567;;;","17/May/21 12:48;sarutak;This issue was resolved in https://github.com/apache/spark/pull/32567.;;;",,,,,,,,,,,,,,,,,,
Fix a bug in groupBy of year-month/day-time intervals,SPARK-35412,13378642,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,16/May/21 09:59,16/May/21 17:52,13/Jul/23 08:47,16/May/21 17:52,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"I found a bug in groupBy of year-month/day-time intervals;
{code}
scala> Seq(java.time.Duration.ofDays(1)).toDF(""a"").groupBy(""a"").count().show()
scala.MatchError: DayTimeIntervalType (of class org.apache.spark.sql.types.DayTimeIntervalType$)
  at org.apache.spark.sql.execution.aggregate.HashMapGenerator.genComputeHash(HashMapGenerator.scala:159)
  at org.apache.spark.sql.execution.aggregate.HashMapGenerator.$anonfun$generateHashFunction$1(HashMapGenerator.scala:102)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at scala.collection.TraversableLike.map(TraversableLike.scala:238)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
  at scala.collection.immutable.List.map(List.scala:298)
  at org.apache.spark.sql.execution.aggregate.HashMapGenerator.genHashForKeys$1(HashMapGenerator.scala:99)
  at org.apache.spark.sql.execution.aggregate.HashMapGenerator.generateHashFunction(HashMapGenerator.scala:111)
{code} 

 ",,apachespark,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 16 17:52:00 UTC 2021,,,,,,,,,,"0|z0r2dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/May/21 10:08;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32560;;;","16/May/21 17:52;dongjoon;Issue resolved by pull request 32560
[https://github.com/apache/spark/pull/32560];;;",,,,,,,,,,,,,,,,,,
PIP packaging test is skipped in GitHub Actions build,SPARK-35393,13378230,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,13/May/21 03:29,12/Dec/22 17:51,13/Jul/23 08:47,13/May/21 17:36,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,PySpark,,,,0,,,"See https://github.com/apache/spark/runs/2568923639?check_suite_focus=true

{code}
========================================================================
Running PySpark packaging tests
========================================================================
Constructing virtual env for testing
Missing virtualenv & conda, skipping pip installability tests
Cleaning up temporary directory - /tmp/tmp.iILYWISPXW
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 13 17:36:19 UTC 2021,,,,,,,,,,"0|z0qzu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/May/21 13:23;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32537;;;","13/May/21 17:36;dongjoon;Issue resolved by pull request 32537
[https://github.com/apache/spark/pull/32537];;;",,,,,,,,,,,,,,,,,,
Flaky Test: GaussianMixture at ml/clustering.py and Word2Vec at ml/feature.py,SPARK-35392,13378229,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,gurwls223,,13/May/21 03:27,12/Dec/22 18:10,13/Jul/23 08:47,13/May/21 13:24,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,ML,PySpark,,,0,,,"Some python tests should be updated/removed, because of [SPARK-35295|https://issues.apache.org/jira/browse/SPARK-35295]


[https://github.com/apache/spark/runs/2568540411]
{code:java}
**********************************************************************
File ""/__w/spark/spark/python/pyspark/ml/clustering.py"", line 276, in __main__.GaussianMixture
Failed example:
    summary.logLikelihood
Expected:
    65.02945...
Got:
    93.36008975083433
**********************************************************************
{code}

and

{code}
**********************************************************************
File ""/__w/spark/spark/python/pyspark/ml/feature.py"", line 4681, in __main__.Word2Vec
Failed example:
    model.getVectors().show()
Expected:
    +----+--------------------+
    |word|              vector|
    +----+--------------------+
    |   a|[0.09511678665876...|
    |   b|[-1.2028766870498...|
    |   c|[0.30153277516365...|
    +----+--------------------+
    ...
Got:
    +----+--------------------+
    |word|              vector|
    +----+--------------------+
    |   a|[0.09511695802211...|
    |   b|[-1.2028766870498...|
    |   c|[0.30153274536132...|
    +----+--------------------+
    <BLANKLINE>
**********************************************************************
{code}",,apachespark,dongjoon,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-35295,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 13 13:24:55 UTC 2021,,,,,,,,,,"0|z0qzu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/May/21 03:28;gurwls223;cc  [~ruifengz] per https://github.com/apache/spark/commit/111e9038d88feef63806457796f3b633f41ef32b and [~viirya] FYI;;;","13/May/21 03:31;gurwls223;I see the first test failure at https://github.com/apache/spark/commit/101b0cc313cb4a6fb0027d470f313314d77bea08 .. but doesn't look related from a cursory look.;;;","13/May/21 03:52;podongfeng;This GMM test is highly unstable, it tend to fail if: change number of partitions or just change the way to compute the sum of weights.

I think we can just disable this check of {{summary.logLikelihood}} for now, and use another test in the future.++

 

as to this failure, is it related to [https://github.com/apache/spark/pull/32415?] [~srowen];;;","13/May/21 04:00;gurwls223;I think disabling is fine for now if it's tricky to fix.;;;","13/May/21 04:02;srowen;Yes disable it. Yes almost certainly because of that change; we found that during testing it but tests did pass;;;","13/May/21 04:13;dongjoon;+1 for disabling, too.;;;","13/May/21 06:01;gurwls223;[~podongfeng] would you mind making a quick PR please?;;;","13/May/21 06:05;podongfeng;[~hyukjin.kwon] OK, I will send a PR;;;","13/May/21 06:09;gurwls223;Thanks man!;;;","13/May/21 06:13;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/32533;;;","13/May/21 13:24;gurwls223;Fixed in https://github.com/apache/spark/pull/32533;;;",,,,,,,,,
Memory leak in ExecutorAllocationListener breaks dynamic allocation under high load,SPARK-35391,13378188,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vkolpakov,vkolpakov,vkolpakov,12/May/21 20:31,21/Jun/21 13:27,13/Jul/23 08:47,21/Jun/21 13:25,3.1.1,,,,,,,,,,,,,,,3.1.3,3.2.0,,,Spark Core,,,,0,,,"ExecutorAllocationListener doesn't clean up data properly. ExecutorAllocationListener performs progressively slower and eventually fails to process events in time.

There are two problems:
 * a bug (typo?) in totalRunningTasksPerResourceProfile() method
 getOrElseUpdate() is used instead of getOrElse().
 If spark-dynamic-executor-allocation thread calls schedule() after a SparkListenerTaskEnd event for the last task in a stage
 but before SparkListenerStageCompleted event for the stage, then stageAttemptToNumRunningTask will not be cleaned up properly.
 * resourceProfileIdToStageAttempt clean-up is broken
 If a SparkListenerTaskEnd event for the last task in a stage was processed before SparkListenerStageCompleted for that stage,
 then resourceProfileIdToStageAttempt will not be cleaned up properly.

 

Bugs were introduced in this commit: https://github.com/apache/spark/commit/496f6ac86001d284cbfb7488a63dd3a168919c0f .

Steps to reproduce:
 # Launch standalone master and worker with 'spark.shuffle.service.enabled=true'
 # Run spark-shell with --conf 'spark.shuffle.service.enabled=true' --conf 'spark.dynamicAllocation.enabled=true' and paste this script
{code:java}
for (_ <- 0 until 10) {
    Seq(1, 2, 3, 4, 5).toDF.repartition(100).agg(""value"" -> ""sum"").show()
}
{code}

 # make a heap dump and examine ExecutorAllocationListener.totalRunningTasksPerResourceProfile and ExecutorAllocationListener.resourceProfileIdToStageAttempt fields

Expected: totalRunningTasksPerResourceProfile and resourceProfileIdToStageAttempt(defaultResourceProfileId) are empty
Actual: totalRunningTasksPerResourceProfile and resourceProfileIdToStageAttempt(defaultResourceProfileId) contain non-relevant data

 ",,apachespark,vkolpakov,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 12 20:58:00 UTC 2021,,,,,,,,,,"0|z0qzkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/May/21 20:58;apachespark;User 'VasilyKolpakov' has created a pull request for this issue:
https://github.com/apache/spark/pull/32526;;;",,,,,,,,,,,,,,,,,,,
Fix lambda variable name issues in nested DataFrame functions in Python APIs,SPARK-35382,13377998,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ueshin,gurwls223,,12/May/21 04:11,12/Dec/22 18:10,13/Jul/23 08:47,13/May/21 06:11,3.1.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,PySpark,,,,0,correctness,,"Python side also has the same issue as SPARK-34794

{code}
from pyspark.sql.functions import *
df = sql(""SELECT array(1, 2, 3) as numbers, array('a', 'b', 'c') as letters"")
df.select(
    transform(
        ""numbers"",
        lambda n: transform(""letters"", lambda l: struct(n.alias(""n""), l.alias(""l"")))
    )
).show()
{code}

{code}
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|transform(numbers, lambdafunction(transform(letters, lambdafunction(struct(namedlambdavariable() AS n, namedlambdavariable() AS l), namedlambdavariable())), namedlambdavariable()))|
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                [[{a, a}, {b, b},...|
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
{code}",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34794,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 13 06:11:02 UTC 2021,,,,,,,,,,"0|z0qyeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/May/21 18:56;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/32523;;;","13/May/21 06:11;gurwls223;Fixed in https://github.com/apache/spark/pull/32523;;;",,,,,,,,,,,,,,,,,,
Fix lambda variable name issues in nested DataFrame functions in R APIs,SPARK-35381,13377997,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,gurwls223,,12/May/21 04:03,12/Dec/22 18:10,13/Jul/23 08:47,12/May/21 07:54,3.1.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SparkR,,,,0,correctness,,"R's higher order functions also have the same problem with SPARK-34794:

{code}
df <- sql(""SELECT array(1, 2, 3) as numbers, array('a', 'b', 'c') as letters"")
collect(select(
  df,
  array_transform(""numbers"", function(number) {
    array_transform(""letters"", function(latter) {
      struct(alias(number, ""n""), alias(latter, ""l""))
    })
  })
))
{code}

{code}
  transform(numbers, lambdafunction(transform(letters, lambdafunction(struct(namedlambdavariable() AS n, namedlambdavariable() AS l), namedlambdavariable())), namedlambdavariable()))
1                                                                                                                                 a, a, b, b, c, c, a, a, b, b, c, c, a, a, b, b, c, c
{code}",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34794,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 12 07:54:41 UTC 2021,,,,,,,,,,"0|z0qyeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/May/21 04:40;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32517;;;","12/May/21 04:41;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32517;;;","12/May/21 07:54;gurwls223;Fixed in https://github.com/apache/spark/pull/32517;;;",,,,,,,,,,,,,,,,,
Use Jinja2 < 3.0.0 for Python linter dependency in GA,SPARK-35375,13377972,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,11/May/21 23:51,12/Dec/22 18:10,13/Jul/23 08:47,12/May/21 01:14,3.2.0,,,,,,,,,,,,,,,3.1.2,3.2.0,,,Project Infra,,,,0,,,"From a few hours ago, Python linter fails in GA.
The latest Jinja 3.0.0 seems to cause this failure.
https://pypi.org/project/Jinja2/

{code}
Run ./dev/lint-python
starting python compilation test...
python compilation succeeded.

starting pycodestyle test...
pycodestyle checks passed.

starting flake8 test...
flake8 checks passed.

starting mypy test...
mypy checks passed.

starting sphinx-build tests...
sphinx-build checks failed:
Running Sphinx v3.0.4
making output directory... done
[autosummary] generating autosummary for: development/contributing.rst, development/debugging.rst, development/index.rst, development/setting_ide.rst, development/testing.rst, getting_started/index.rst, getting_started/install.rst, getting_started/quickstart.ipynb, index.rst, migration_guide/index.rst, ..., reference/pyspark.ml.rst, reference/pyspark.mllib.rst, reference/pyspark.resource.rst, reference/pyspark.rst, reference/pyspark.sql.rst, reference/pyspark.ss.rst, reference/pyspark.streaming.rst, user_guide/arrow_pandas.rst, user_guide/index.rst, user_guide/python_packaging.rst

Exception occurred:
  File ""/__w/spark/spark/python/docs/source/_templates/autosummary/class_with_docs.rst"", line 26, in top-level template code
    {% if '__init__' in methods %}
jinja2.exceptions.UndefinedError: 'methods' is undefined
The full traceback has been saved in /tmp/sphinx-err-ypgyi75y.log, if you want to report the issue to the developers.
Please also report this if it was a user error, so that a better error message can be provided next time.
A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!
make: *** [Makefile:20: html] Error 2

re-running make html to print full warning list:
Running Sphinx v3.0.4
making output directory... done
[autosummary] generating autosummary for: development/contributing.rst, development/debugging.rst, development/index.rst, development/setting_ide.rst, development/testing.rst, getting_started/index.rst, getting_started/install.rst, getting_started/quickstart.ipynb, index.rst, migration_guide/index.rst, ..., reference/pyspark.ml.rst, reference/pyspark.mllib.rst, reference/pyspark.resource.rst, reference/pyspark.rst, reference/pyspark.sql.rst, reference/pyspark.ss.rst, reference/pyspark.streaming.rst, user_guide/arrow_pandas.rst, user_guide/index.rst, user_guide/python_packaging.rst

Exception occurred:
  File ""/__w/spark/spark/python/docs/source/_templates/autosummary/class_with_docs.rst"", line 26, in top-level template code
    {% if '__init__' in methods %}
jinja2.exceptions.UndefinedError: 'methods' is undefined
The full traceback has been saved in /tmp/sphinx-err-fvtmvvwv.log, if you want to report the issue to the developers.
Please also report this if it was a user error, so that a better error message can be provided next time.
A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!
make: *** [Makefile:20: html] Error 2
Error: Process completed with exit code 2.
{code}",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 12 01:14:17 UTC 2021,,,,,,,,,,"0|z0qy8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/May/21 23:58;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32509;;;","12/May/21 01:14;gurwls223;Fixed in https://github.com/apache/spark/pull/32509;;;",,,,,,,,,,,,,,,,,,
JDK 11 compilation failure due to StackOverflowError,SPARK-35372,13377847,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,11/May/21 11:47,12/Dec/22 17:50,13/Jul/23 08:47,11/May/21 17:21,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Build,,,,0,,,"See https://github.com/apache/spark/runs/2554067779

{code}
java.lang.StackOverflowError
scala.reflect.internal.Trees$UnderConstructionTransformer.transform(Trees.scala:1741)
scala.reflect.internal.Trees$UnderConstructionTransformer.transform$(Trees.scala:1740)
scala.tools.nsc.transform.ExplicitOuter$OuterPathTransformer.transform(ExplicitOuter.scala:289)
scala.tools.nsc.transform.ExplicitOuter$ExplicitOuterTransformer.transform(ExplicitOuter.scala:477)
scala.tools.nsc.transform.ExplicitOuter$ExplicitOuterTransformer.transform(ExplicitOuter.scala:330)
scala.reflect.api.Trees$Transformer.$anonfun$transformStats$1(Trees.scala:2597)
scala.reflect.api.Trees$Transformer.transformStats(Trees.scala:2595)
scala.reflect.internal.Trees.itransform(Trees.scala:1404)
scala.reflect.internal.Trees.itransform$(Trees.scala:1374)
scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:28)
scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:28)
scala.reflect.api.Trees$Transformer.transform(Trees.scala:2563)
scala.tools.nsc.transform.TypingTransformers$TypingTransformer.transform(TypingTransformers.scala:51)
scala.tools.nsc.transform.ExplicitOuter$OuterPathTransformer.scala$reflect$internal$Trees$UnderConstructionTransformer$$super$transform(ExplicitOuter.scala:212)
scala.reflect.internal.Trees$UnderConstructionTransformer.transform(Trees.scala:1745)
scala.reflect.internal.Trees$UnderConstructionTransformer.transform$(Trees.scala:1740)
scala.tools.nsc.transform.ExplicitOuter$OuterPathTransformer.transform(ExplicitOuter.scala:289)
scala.tools.nsc.transform.ExplicitOuter$ExplicitOuterTransformer.transform(ExplicitOuter.scala:477)
scala.tools.nsc.transform.ExplicitOuter$ExplicitOuterTransformer.transform(ExplicitOuter.scala:330)
scala.reflect.internal.Trees.itransform(Trees.scala:1383)
{code}",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 11 17:21:39 UTC 2021,,,,,,,,,,"0|z0qxh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/May/21 11:51;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32502;;;","11/May/21 11:52;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32502;;;","11/May/21 17:21;sarutak;This issue was resolved in https://github.com/apache/spark/pull/32502.;;;",,,,,,,,,,,,,,,,,
Insert data with char/varchar datatype will fail when data length exceed length limitation,SPARK-35359,13377534,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yghu,yghu,yghu,10/May/21 01:23,17/May/21 16:15,13/Jul/23 08:47,17/May/21 16:14,3.1.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"In Spark3.1.1 have support Char/Varchar type, but when insert data with char/varchar datatype will fail when data length exceed length limitation even when spark.sql.legacy.charVarcharAsString is true.

reproduce:

create table chartb01(a char(3));

insert into chartb01 select 'aaaaa';",,apachespark,cloud_fan,yghu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 17 16:14:28 UTC 2021,,,,,,,,,,"0|z0qvjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/May/21 01:29;yghu;I'd like to work on this.;;;","11/May/21 09:18;apachespark;User 'fhygh' has created a pull request for this issue:
https://github.com/apache/spark/pull/32501;;;","11/May/21 09:18;apachespark;User 'fhygh' has created a pull request for this issue:
https://github.com/apache/spark/pull/32501;;;","17/May/21 16:14;cloud_fan;Issue resolved by pull request 32501
[https://github.com/apache/spark/pull/32501];;;",,,,,,,,,,,,,,,,
Attributes become unknown in RepartitionByExpression after aliased,SPARK-35331,13377133,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,07/May/21 03:47,08/May/21 12:01,13/Jul/23 08:47,08/May/21 12:01,2.4.8,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"
{code:java}
explain extended select a b from values(1) t(a) distribute by a;
== Parsed Logical Plan ==
'RepartitionByExpression ['a]
+- 'Project ['a AS b#42]
   +- 'SubqueryAlias t
      +- 'UnresolvedInlineTable [a], [List(1)]

== Analyzed Logical Plan ==
org.apache.spark.sql.AnalysisException: cannot resolve 'a' given input columns: [b]; line 1 pos 62;
'RepartitionByExpression ['a]
+- Project [a#48 AS b#42]
   +- SubqueryAlias t
      +- LocalRelation [a#48]
{code}
",,apachespark,dongjoon,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 08 12:01:15 UTC 2021,,,,,,,,,,"0|z0qt2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/May/21 03:59;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/32465;;;","08/May/21 12:01;dongjoon;Issue resolved by pull request 32465
[https://github.com/apache/spark/pull/32465];;;",,,,,,,,,,,,,,,,,,
Upgrade Jersey to 2.34,SPARK-35326,13377036,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,06/May/21 13:05,06/May/21 15:40,13/Jul/23 08:47,06/May/21 15:36,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.2.0,,,,Build,,,,0,,,"CVE-2021-28168, a local information disclosure vulnerability, is reported.
Spark 3.1.1, 3.0.2 and 3.2.0 use an affected version 2.30.
https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-28168",,akodikal,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 06 15:40:42 UTC 2021,,,,,,,,,,"0|z0qshc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/May/21 13:09;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32453;;;","06/May/21 13:09;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32453;;;","06/May/21 15:36;dongjoon;Issue resolved by pull request 32453
[https://github.com/apache/spark/pull/32453];;;","06/May/21 15:40;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32458;;;",,,,,,,,,,,,,,,,
Spark 3.x can't talk to HMS 1.2.x and lower due to get_all_functions Thrift API missing,SPARK-35321,13376874,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csun,csun,csun,05/May/21 21:20,20/Oct/21 08:15,13/Jul/23 08:47,12/Jun/21 02:33,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"https://issues.apache.org/jira/browse/HIVE-10319 introduced a new API {{get_all_functions}} which is only supported in Hive 1.3.0/2.0.0 and up. This is called when creating a new {{Hive}} object:
{code}
  private Hive(HiveConf c, boolean doRegisterAllFns) throws HiveException {
    conf = c;
    if (doRegisterAllFns) {
      registerAllFunctionsOnce();
    }
  }
{code}

{{registerAllFunctionsOnce}} will reload all the permanent functions by calling the {{get_all_functions}} API from the megastore. In Spark, we always pass {{doRegisterAllFns}} as true, and this will cause failure:
{code}
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.TApplicationException: Invalid method name: 'get_all_functions'
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3897)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:248)
	at org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:231)
	... 96 more
Caused by: org.apache.thrift.TApplicationException: Invalid method name: 'get_all_functions'
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_all_functions(ThriftHiveMetastore.java:3845)
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_all_functions(ThriftHiveMetastore.java:3833)
{code}

It looks like Spark doesn't really need to call {{registerAllFunctionsOnce}} since it loads the Hive permanent function directly from HMS API. The Hive {{FunctionRegistry}} is only used for loading Hive built-in functions.",,apachespark,csun,dongjoon,viirya,xkrogen,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-37069,,HIVE-21563,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 12 02:33:25 UTC 2021,,,,,,,,,,"0|z0qrhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/May/21 21:38;xkrogen;Isn't this what the {{IsolatedClientLoader}} is for? You should use e.g. {{spark.sql.hive.metastore.version=1.1}} and this issue will go away IIUC.;;;","05/May/21 21:44;csun;[~xkrogen] yes that can help to solve the issue, but users need to specify both {{spark.sql.hive.metastore.version}} and {{spark.sql.hive.metastore.jars}}. The latter is not so easy to setup: the {{maven}} option usually takes a very long time to download all the jars, while the {{path}} option require users to download all the relevant Hive jars with the specific version and it's tedious. 

I think this specific issue is worth fixing in Spark itself regardless since it doesn't really need to load all the permanent functions when starting up Hive client from what I can see. The process could also be pretty expensive if there are many UDFs registered in HMS.;;;","05/May/21 21:51;xkrogen;Gotcha. Yeah, agreed that if it's unnecessary we may as well improve the cross-version compatibility.;;;","06/May/21 00:19;yumwang;Could we add a parameter to disable registerAllFunctionsOnce? https://issues.apache.org/jira/browse/HIVE-21563;;;","06/May/21 00:33;csun;[~yumwang] I'm thinking of using [Hive#getWithFastCheck|https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java#L389] for this purpose, which allows us to set the flag to false. The fast check flag also offers a way to compare {{HiveConf}} faster when the conf rarely changes.;;;","06/May/21 00:39;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/32446;;;","06/May/21 00:40;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/32446;;;","07/May/21 22:10;dongjoon;https://github.com/apache/spark/pull/32446;;;","07/May/21 22:19;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/32471;;;","07/May/21 22:20;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/32472;;;","08/May/21 20:03;dongjoon;This is reverted due to https://github.com/apache/spark/pull/32446#issuecomment-835419237 .;;;","11/Jun/21 21:10;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/32887;;;","11/Jun/21 21:11;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/32887;;;","12/Jun/21 02:33;yumwang;Issue resolved by pull request 32887
[https://github.com/apache/spark/pull/32887];;;",,,,,,
Upgrade K8s client to 5.3.1,SPARK-35319,13376822,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,05/May/21 15:49,06/May/21 02:51,13/Jul/23 08:47,06/May/21 02:51,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Kubernetes,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 06 02:51:03 UTC 2021,,,,,,,,,,"0|z0qr60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/May/21 15:52;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32443;;;","06/May/21 02:51;dongjoon;Issue resolved by pull request 32443
[https://github.com/apache/spark/pull/32443];;;",,,,,,,,,,,,,,,,,,
Fix a bug in SPARK-35266 that creates benchmark files in the invalid path with the wrong name,SPARK-35308,13376523,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,byungsoo.oh,byungsoo.oh,byungsoo.oh,04/May/21 09:25,12/Dec/22 18:10,13/Jul/23 08:47,04/May/21 10:41,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Tests,,,,0,easyfix,,"Change in SPARK-35266 has a bug that benchmark files are created in the invalid path with the wrong name.
 e.g. For {{BLASBenchmark}},
 * Desired: Creates {{BLASBenchmark-results.txt}} in \{SPARK_HOME}/mllib-local/benchmarks/
 * Current: Creates {{benchmarksBLASBenchmark-results.txt}} in \{SPARK_HOME}/mllib-local/",,apachespark,byungsoo.oh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 04 10:41:14 UTC 2021,,,,,,,,,,"0|z0qpbk:",9223372036854775807,,,,,hyukjin.kwon,,,,,,,,,,,,,,,,,,"04/May/21 09:28;byungsoo.oh;I'll submit a PR to fix this bug. ;;;","04/May/21 09:42;apachespark;User 'byungsoo-oh' has created a pull request for this issue:
https://github.com/apache/spark/pull/32432;;;","04/May/21 10:41;gurwls223;Issue resolved by pull request 32432
[https://github.com/apache/spark/pull/32432];;;",,,,,,,,,,,,,,,,,
Enable pinned thread mode by default,SPARK-35303,13376474,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,04/May/21 04:09,12/Dec/22 18:11,13/Jul/23 08:47,18/Jun/21 03:17,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,PySpark,,,,0,,,Pinned thread mode was added at SPARK-22340. We should enable it back to map Python thread to JVM thread in order to prevent potential issues such as thread local inheritance.,,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-37004,SPARK-37012,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 18 08:10:18 UTC 2021,,,,,,,,,,"0|z0qp0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/May/21 04:23;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32429;;;","04/May/21 04:24;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32429;;;","18/Jun/21 03:17;gurwls223;Issue resolved by pull request 32429
[https://github.com/apache/spark/pull/32429];;;","18/Jun/21 08:09;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32962;;;","18/Jun/21 08:10;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32962;;;",,,,,,,,,,,,,,,
Benchmark workflow should create new files for new benchmarks,SPARK-35302,13376472,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,04/May/21 04:03,12/Dec/22 18:10,13/Jul/23 08:47,04/May/21 10:03,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Project Infra,,,,0,,,"Currently, it fails at {{git diff --name-only}} when new benchmarks are added, see https://github.com/HyukjinKwon/spark/actions/runs/808870999
",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34821,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 04 10:03:17 UTC 2021,,,,,,,,,,"0|z0qp08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/May/21 04:15;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32428;;;","04/May/21 04:15;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32428;;;","04/May/21 10:03;gurwls223;Issue resolved by pull request 32428
[https://github.com/apache/spark/pull/32428];;;",,,,,,,,,,,,,,,,,
Dataset.observe fails with an assertion,SPARK-35296,13376401,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,tanelk,tanelk,03/May/21 15:47,10/Jun/21 17:23,13/Jul/23 08:47,10/Jun/21 17:22,3.1.1,3.2.0,,,,,,,,,,,,,,3.0.3,3.1.3,3.2.0,,SQL,,,,0,,,"I hit this assertion error when using dataset.observe:
{code}
java.lang.AssertionError: assertion failed
	at scala.Predef$.assert(Predef.scala:208) ~[scala-library-2.12.10.jar:?]
	at org.apache.spark.sql.execution.AggregatingAccumulator.setState(AggregatingAccumulator.scala:204) ~[spark-sql_2.12-3.1.1.jar:3.1.1]
	at org.apache.spark.sql.execution.CollectMetricsExec.$anonfun$doExecute$2(CollectMetricsExec.scala:72) ~[spark-sql_2.12-3.1.1.jar:3.1.1]
	at org.apache.spark.sql.execution.CollectMetricsExec.$anonfun$doExecute$2$adapted(CollectMetricsExec.scala:71) ~[spark-sql_2.12-3.1.1.jar:3.1.1]
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125) ~[spark-core_2.12-3.1.1.jar:3.1.1]
	at org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124) ~[spark-core_2.12-3.1.1.jar:3.1.1]
	at org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124) ~[spark-core_2.12-3.1.1.jar:3.1.1]
	at org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137) ~[spark-core_2.12-3.1.1.jar:3.1.1]
	at org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135) ~[spark-core_2.12-3.1.1.jar:3.1.1]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.10.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.10.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.10.jar:?]
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135) ~[spark-core_2.12-3.1.1.jar:3.1.1]
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124) ~[spark-core_2.12-3.1.1.jar:3.1.1]
	at org.apache.spark.scheduler.Task.run(Task.scala:147) ~[spark-core_2.12-3.1.1.jar:3.1.1]
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) ~[spark-core_2.12-3.1.1.jar:3.1.1]
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) [spark-core_2.12-3.1.1.jar:3.1.1]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) [spark-core_2.12-3.1.1.jar:3.1.1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_282]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_282]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_282]
{code}

A workaround, that I used was to add .coalesce(1) before calling this method.

It happens in a quite complex query and I have not been able to reproduce this with a simpler query

Added an screenshot of the debugger, at the moment of exception
 !2021-05-03_18-34.png! ",,apachespark,cloud_fan,tanelk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/21 15:59;tanelk;2021-05-03_18-34.png;https://issues.apache.org/jira/secure/attachment/13024932/2021-05-03_18-34.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 10 17:22:03 UTC 2021,,,,,,,,,,"0|z0qokg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/May/21 15:55;tanelk;I tried to change an excisting UT to reproduce this:
{code}
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala
index b3d29df1b2..0fcd31a09c 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala
@@ -246,7 +246,7 @@ class DataFrameCallbackSuite extends QueryTest
     }
     spark.listenerManager.register(listener)
     try {
-      val df = spark.range(100)
+      val df = spark.range(100).repartition(2)
         .observe(
           name = ""my_event"",
           min($""id"").as(""min_val""),
{code}

But did not hit the same exception:
{code}
[info] - get observable metrics by callback *** FAILED *** (324 milliseconds)
[info]   0 did not equal 2 (DataFrameCallbackSuite.scala:261)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
[info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)
[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)
[info]   at org.apache.spark.sql.util.DataFrameCallbackSuite.checkMetrics$1(DataFrameCallbackSuite.scala:261)
[info]   at org.apache.spark.sql.util.DataFrameCallbackSuite.$anonfun$new$15(DataFrameCallbackSuite.scala:270)
{code}

Although i think that this should not happen either.;;;","03/May/21 15:57;tanelk;[~hvanhovell] The assertion in AggregatingAccumulator is added by you. Perhaps you have some idea on why it happens. ;;;","03/May/21 16:26;tanelk;I finally managed to change the UT in such way, that the assertion error happens - the following coalesce was the missing link.
{code}
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala
index b3d29df1b2..16ebddd75c 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/util/DataFrameCallbackSuite.scala
@@ -246,7 +246,7 @@ class DataFrameCallbackSuite extends QueryTest
     }
     spark.listenerManager.register(listener)
     try {
-      val df = spark.range(100)
+      val df = spark.range(0, 100, 1, 5)
         .observe(
           name = ""my_event"",
           min($""id"").as(""min_val""),
@@ -256,6 +256,7 @@ class DataFrameCallbackSuite extends QueryTest
         .observe(
           name = ""other_event"",
           avg($""id"").cast(""int"").as(""avg_val""))
+        .coalesce(2)

       def checkMetrics(metrics: Map[String, Row]): Unit = {
         assert(metrics.size === 2)
{code};;;","03/Jun/21 07:39;tanelk;Perhaps someone who knows the internals better, ccould help here. [~cloud_fan]? 
I tried to take another look, but I can't figure it out.;;;","04/Jun/21 19:16;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32786;;;","04/Jun/21 19:17;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32786;;;","10/Jun/21 17:22;cloud_fan;Issue resolved by pull request 32786
[https://github.com/apache/spark/pull/32786];;;",,,,,,,,,,,,,
unionByName with null filling fails for some nested structs,SPARK-35290,13376276,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kimahriman,kimahriman,kimahriman,02/May/21 14:05,12/Dec/22 18:10,13/Jul/23 08:47,24/Jun/21 16:21,3.1.1,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"We've encountered a few weird edge cases that seem to fail the new null filling unionByName (which has been a great addition!). It seems to stem from the fields being sorted by name and corrupted along the way. The simple reproduction is:
{code:java}
df = spark.createDataFrame([[]])
df1 = (df
    .withColumn('top', F.struct(
        F.struct(
            F.lit('ba').alias('ba')
        ).alias('b')
    ))
)
df2 = (df
    .withColumn('top', F.struct(
        F.struct(
            F.lit('aa').alias('aa')
        ).alias('a'),
        F.struct(
            F.lit('bb').alias('bb')
        ).alias('b'),
    ))
)
df1.unionByName(df2, True).printSchema()
{code}
This results in the exception:
{code:java}
pyspark.sql.utils.AnalysisException: Union can only be performed on tables with the compatible column types. struct<a:struct<aa:string>,b:struct<ba:string,bb:string>> <> struct<a:struct<aa:string>,b:struct<aa:string,bb:string>> at the first column of the second table;
{code}
You can see in the second schema that it has 
{code:java}
b:struct<aa:string,bb:string>
{code}
when it should be
{code:java}
b:struct<ba:string:bb:string>
{code}
It seems to happen somewhere during [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveUnion.scala#L73,] as everything seems correct up to that point from my testing. It's either modifying one expression during the transformUp then corrupts other expressions that are then modified, or the ExtractValue before the addFieldsInto is remembering the ordinal position in the struct that is then changing and causing issues.

 

I found that simply using sortStructFields instead of sortStructFieldsInWithFields gets things working correctly, but definitely has a performance impact. The deep expr unionByName test takes ~1-2 seconds normally but ~12-15 seconds with this change. I assume because the original method tried to rewrite existing expressions vs the sortStructFields just adds expressions on top of existing ones to project the new order.

I'm not sure if it makes sense to take the slower but works in the edge cases method (assuming it doesn't break other cases, all existing tests pass), or if there's a way to fix the existing method for cases like this.

 ",,apachespark,kimahriman,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-36918,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 24 16:21:57 UTC 2021,,,,,,,,,,"0|z0qnsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/May/21 10:17;gurwls223;cc [~viirya] FYI;;;","03/May/21 16:47;viirya;Thanks [~hyukjin.kwon] for ping me.;;;","03/May/21 16:47;viirya;I will take a look.;;;","03/May/21 17:46;kimahriman;I've also been playing around with rewriting some of the logic to just directly recursively create a named struct and taking out the need for the UpdateField/WithField logic. I've gotten all existing tests to pass (including a new one for this case), without the 12-15 second overhead mentioned in the description for the deeply nested case, but I think there's still some case insensitivity I might need to take care of. I can put up a PR soon with what that looks like.;;;","03/May/21 19:01;viirya;Thanks [~Kimahriman]. ;;;","04/May/21 11:24;kimahriman;Running into some issues trying to get case insensitivity working correctly. I was hoping to be able to leave everything on both sides of the union in it's existing casing, and just get things in the right order, but I'm running into this issue:
{code:java}
>>> from pyspark.sql.functions import *
>>> df1 = spark.range(1).withColumn('top', struct(lit('A').alias('A')))
>>> df2 = spark.range(1).withColumn('top', struct(lit('a').alias('a')))
>>> spark.conf.set('spark.sql.caseSensitive', 'true')
...
pyspark.sql.utils.AnalysisException: Union can only be performed on tables with the compatible column types. struct<a:string> <> struct<A:string> at the second column of the second table;

>>> spark.conf.set('spark.sql.caseSensitive', 'false')
>>> df1.union(df2)
DataFrame[id: bigint, top: struct<A:string,a:string>]
>>> df1.unionByName(df2)
DataFrame[id: bigint, top: struct<A:string,a:string>]
>>> df1.unionByName(df2, True)
DataFrame[id: bigint, top: struct<A:string,a:string>]
{code}
With case sensitivity enabled, it errors out as expected because the two structs are different types. However, when case sensitivity is disabled, the union is happy because it sees them as the same type, but when the schemas are merged, it treats them as two separate fields. I assume it's related to the StructType.merge method, but I don't exactly know where that gets called in the context of a Union. I don't see anything in that merge function that handles case insensitivity. Is that a bug in itself or a feature?;;;","06/May/21 01:35;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/32448;;;","06/May/21 01:35;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/32448;;;","23/Jun/21 11:41;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/33040;;;","24/Jun/21 16:21;viirya;Issue resolved by pull request 33040
[https://github.com/apache/spark/pull/33040];;;",,,,,,,,,,
StaticInvoke should find the method without exact argument classes match,SPARK-35288,13376226,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,01/May/21 20:19,07/May/21 16:10,13/Jul/23 08:47,07/May/21 16:10,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,SQL,,,,0,,,"Unlike Invoke, StaticInvoke only tries to get the method with exact argument classes match. If the calling method's parameter types are not exactly matched with the argument classes, StaticInvoke cannot find the method.

StaticInvoke should be able to find the method under the cases too.",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 07 16:10:19 UTC 2021,,,,,,,,,,"0|z0qnhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/May/21 20:45;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32413;;;","01/May/21 20:46;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32413;;;","07/May/21 16:10;viirya;Issue resolved by pull request 32413
[https://github.com/apache/spark/pull/32413];;;",,,,,,,,,,,,,,,,,
RemoveRedundantProjects removes non-redundant projects,SPARK-35287,13376217,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,chungmin,chungmin,01/May/21 15:24,12/Dec/22 18:10,13/Jul/23 08:47,24/May/21 16:27,3.1.1,,,,,,,,,,,,,,,3.1.3,3.2.0,,,SQL,,,,0,,,"RemoveRedundantProjects erroneously removes non-redundant projects which are required to convert rows coming from DataSourceV2ScanExec to UnsafeRow. There is a code for this case, but it only looks at the child. The bug occurs when DataSourceV2ScanExec is not a child of the project, but a descendant. The method {{isRedundant}} in {{RemoveRedundantProjects}} should be updated to account for descendants too.

The original scenario requires Iceberg to reproduce the issue. In theory, it should be able to reproduce the bug with Spark SQL only, and someone more knowledgeable with Spark SQL should be able to make such a scenario. The following is my reproduction scenario (Spark 3.1.1, Iceberg 0.11.1): 

{code:java}
import scala.collection.JavaConverters._

import org.apache.iceberg.{PartitionSpec, TableProperties}
import org.apache.iceberg.hadoop.HadoopTables
import org.apache.iceberg.spark.SparkSchemaUtil
import org.apache.spark.sql.{DataFrame, QueryTest, SparkSession}
import org.apache.spark.sql.internal.SQLConf

class RemoveRedundantProjectsTest extends QueryTest {
  override val spark: SparkSession = SparkSession
    .builder()
    .master(""local[4]"")
    .config(""spark.driver.bindAddress"", ""127.0.0.1"")
    .appName(suiteName)
    .getOrCreate()
  test(""RemoveRedundantProjects removes non-redundant projects"") {
    withSQLConf(
      SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> ""-1"",
      SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key -> ""false"",
      SQLConf.REMOVE_REDUNDANT_PROJECTS_ENABLED.key -> ""true"") {
      withTempDir { dir =>
        val path = dir.getCanonicalPath
        val data = spark.range(3).toDF
        val table = new HadoopTables().create(
          SparkSchemaUtil.convert(data.schema),
          PartitionSpec.unpartitioned(),
          Map(TableProperties.WRITE_NEW_DATA_LOCATION -> path).asJava,
          path)
        data.write.format(""iceberg"").mode(""overwrite"").save(path)
        table.refresh()

        val df = spark.read.format(""iceberg"").load(path)
        val dfX = df.as(""x"")
        val dfY = df.as(""y"")
        val join = dfX.filter(dfX(""id"") > 0).join(dfY, ""id"")
        join.explain(""extended"")
        assert(join.count() == 2)
      }
    }
  }
}
{code}

Stack trace:

{noformat}
[info] - RemoveRedundantProjects removes non-redundant projects *** FAILED ***
[info]   org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 4) (xeroxms100.northamerica.corp.microsoft.com executor driver): java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.GenericInternalRow cannot be cast to org.apache.spark.sql.catalyst.expressions.UnsafeRow
[info]  at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)
[info]  at org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119)
[info]  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
[info]  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
[info]  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[info]  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
[info]  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
[info]  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
[info]  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
[info]  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
[info]  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[info]  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
[info]  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
[info]  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[info]  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
[info]  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
[info]  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[info]  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
[info]  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
[info]  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[info]  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
[info]  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
[info]  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
[info]  at org.apache.spark.scheduler.Task.run(Task.scala:131)
[info]  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
[info]  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
[info]  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
[info]  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]  at java.lang.Thread.run(Thread.java:748)
[info]
[info] Driver stacktrace:
[info]   at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)
[info]   at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)
[info]   at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)
[info]   at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[info]   at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[info]   at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[info]   at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)
[info]   at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)
[info]   at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)
[info]   at scala.Option.foreach(Option.scala:407)
[info]   ...
[info]   Cause: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.GenericInternalRow cannot be cast to org.apache.spark.sql.catalyst.expressions.UnsafeRow
[info]   at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226)
[info]   at org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119)
[info]   at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
[info]   at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
[info]   at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[info]   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
[info]   at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
[info]   at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
[info]   at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
[info]   at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
[info]   ...
{noformat}",,apachespark,chungmin,cloud_fan,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 24 16:27:00 UTC 2021,,,,,,,,,,"0|z0qnfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/May/21 10:19;gurwls223;cc [~aokolnychyi] FYI;;;","20/May/21 09:34;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32606;;;","24/May/21 16:27;cloud_fan;Issue resolved by pull request 32606
[https://github.com/apache/spark/pull/32606];;;",,,,,,,,,,,,,,,,,
Invoke should find the method with correct number of parameters,SPARK-35278,13375934,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,30/Apr/21 03:38,01/May/21 21:59,13/Jul/23 08:47,01/May/21 21:58,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,SQL,,,,0,,,"`Invoke` will find out the method on the target object with given method name. If there are more than one method with the name, currently it is undeterministic which method will be used. We should add the condition of parameter number when finding the method.",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 01 21:58:47 UTC 2021,,,,,,,,,,"0|z0qlpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/21 03:47;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32404;;;","01/May/21 17:47;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32412;;;","01/May/21 21:58;viirya;Issue resolved by pull request 32412
[https://github.com/apache/spark/pull/32412];;;",,,,,,,,,,,,,,,,,
Fix an error in BenchmarkBase.scala that occurs when creating a benchmark file in a non-existent directory,SPARK-35266,13375736,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,byungsoo.oh,byungsoo.oh,byungsoo.oh,29/Apr/21 04:32,12/Dec/22 18:10,13/Jul/23 08:47,03/May/21 09:06,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Tests,,,,0,easyfix,,"When submitting a benchmark job using _org.apache.spark.benchmark.Benchmarks_ class with _SPARK_GENERATE_BENCHMARK_FILES=1_ option, an exception is raised if the directory where the benchmark file will be generated does not exist.
 For example, if you execute _BLASBenchmark_ like the command below, you get an error unless you manually create _benchmarks/_ directory under _spark/mllib-local/_.
{code:java}
SPARK_GENERATE_BENCHMARK_FILES=1 bin/spark-submit \
--driver-memory 6g --class org.apache.spark.benchmark.Benchmarks \
--jars ""`find . -name '*-SNAPSHOT-tests.jar' -o -name '*avro*-SNAPSHOT.jar' | paste -sd ',' -`"" \
""`find . -name 'spark-core*-SNAPSHOT-tests.jar'`"" \
""org.apache.spark.ml.linalg.BLASBenchmark""
{code}
This is caused by the code in _BenchmarkBase.scala_ where an attempt is made to create the benchmark file without validating the path.",,apachespark,byungsoo.oh,wylee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 04 09:44:13 UTC 2021,,,,,,,,,,"0|z0qkhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/21 04:45;byungsoo.oh;I fixed this issue and checked it's working fine. If it's okay, I will submit a pull request for this.;;;","29/Apr/21 07:17;apachespark;User 'obs0811' has created a pull request for this issue:
https://github.com/apache/spark/pull/32394;;;","29/Apr/21 07:17;apachespark;User 'obs0811' has created a pull request for this issue:
https://github.com/apache/spark/pull/32394;;;","03/May/21 09:06;gurwls223;Issue resolved by pull request 32394
[https://github.com/apache/spark/pull/32394];;;","04/May/21 09:43;apachespark;User 'byungsoo-oh' has created a pull request for this issue:
https://github.com/apache/spark/pull/32432;;;","04/May/21 09:44;apachespark;User 'byungsoo-oh' has created a pull request for this issue:
https://github.com/apache/spark/pull/32432;;;",,,,,,,,,,,,,,
Streaming-batch intersects are incorrectly allowed through UnsupportedOperationsChecker,SPARK-35246,13375403,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joseph.torres,joseph.torres,joseph.torres,27/Apr/21 17:25,12/Dec/22 18:10,13/Jul/23 08:47,28/Apr/21 01:47,3.0.0,3.1.0,,,,,,,,,,,,,,3.2.0,,,,Structured Streaming,,,,0,,,"The UnsupportedOperationChecker currently rejects streaming intersects only if both sides are streaming, but they don't work if even one side is streaming. The following simple test, for example, fails with a cryptic None.get error because the state store can't plan itself properly.
{code:java}
  test(""intersect"") {
    val input = MemoryStream[Long]
    val df = input.toDS().intersect(spark.range(10).as[Long])
    testStream(df) (
      AddData(input, 1L),
      CheckAnswer(1)
    )
  }
{code}",,apachespark,joseph.torres,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 28 01:47:23 UTC 2021,,,,,,,,,,"0|z0qifc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/21 17:30;apachespark;User 'jose-torres' has created a pull request for this issue:
https://github.com/apache/spark/pull/32371;;;","28/Apr/21 01:47;gurwls223;Fixed in [https://github.com/apache/spark/pull/32371];;;",,,,,,,,,,,,,,,,,,
invoke should throw the original exception,SPARK-35244,13375380,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,27/Apr/21 16:06,12/Dec/22 18:10,13/Jul/23 08:47,28/Apr/21 01:45,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,SQL,,,,0,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 28 18:29:57 UTC 2021,,,,,,,,,,"0|z0qia8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/21 16:12;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/32370;;;","28/Apr/21 01:45;gurwls223;[https://github.com/apache/spark/pull/32370];;;","28/Apr/21 18:29;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/32387;;;",,,,,,,,,,,,,,,,,
Nested column pruning should retain column metadata,SPARK-35232,13375131,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csun,csun,csun,26/Apr/21 18:19,08/May/21 05:38,13/Jul/23 08:47,08/May/21 05:38,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,It seems we should retain column metadata when pruning nested columns. Otherwise the info will be lost and will affect things such as re-constructing CHAR/VARCHAR type (SPARK-33901).,,apachespark,csun,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 08 05:38:31 UTC 2021,,,,,,,,,,"0|z0qgqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/21 20:03;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/32354;;;","26/Apr/21 20:04;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/32354;;;","08/May/21 05:38;viirya;Issue resolved by pull request 32354
[https://github.com/apache/spark/pull/32354];;;",,,,,,,,,,,,,,,,,
JDBC datasources should accept refreshKrb5Config parameter,SPARK-35226,13374979,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,26/Apr/21 05:25,22/May/21 08:04,13/Jul/23 08:47,29/Apr/21 04:57,3.1.1,3.2.0,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"In the current master, JDBC datasources can't accept refreshKrb5Config which is defined in Krb5LoginModule.
So even if we change the krb5.conf after establishing  a connection, the change will not be reflected.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 22 08:04:39 UTC 2021,,,,,,,,,,"0|z0qft4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/21 05:52;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32344;;;","29/Apr/21 04:57;sarutak;This issue was resolved in https://github.com/apache/spark/pull/32344.;;;","22/May/21 08:04;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32632;;;",,,,,,,,,,,,,,,,,
Corrupt DataFrame for certain withField patterns,SPARK-35213,13374846,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kimahriman,kimahriman,kimahriman,25/Apr/21 00:20,26/Apr/21 06:40,13/Jul/23 08:47,26/Apr/21 06:40,3.1.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,Optimizer,SQL,,,0,,,"We encountered a very weird bug heavily using withField in production with the Spark 3.1.1. Jobs were dying with a lot of very weird JVM crashing errors (like jshort_disjoint_arraycopy during a copyMemory call), and occasional NegativeArraySize exceptions. We finally found a work around by ordering our withField calls in a certain way, and I was finally able to create some minimal examples to reproduce similar weird/broken behavior.

It seems to stem from the optimizations added in [https://github.com/apache/spark/pull/29812.] Because the same new optimization was added as an analyzer, there seems to be two different ways this issue can crop up, once at analysis time and once at runtime.

While these examples might seem odd, they represent how we've created a helper function that can create columns in arbitrary nested fields even if the intermediate fields don't exist yet.

Example of what I assume is an issue during analysis:
{code:java}
import pyspark.sql.functions as F 
df = spark.range(1).withColumn('data', F.struct()
    .withField('a', F.struct())
    .withField('b', F.struct())
    .withField('a.aa', F.lit('aa'))
    .withField('b.ba', F.lit('ba'))
    .withField('a.ab', F.lit('ab'))
    .withField('b.bb', F.lit('bb'))
    .withField('a.ac', F.lit('ac'))
)
df.printSchema(){code}
Output schema:
{code:java}
root
 |-- id: long (nullable = false)
 |-- data: struct (nullable = false)
 | |-- b: struct (nullable = false)
 | | |-- aa: string (nullable = false)
 | | |-- ab: string (nullable = false)
 | | |-- bb: string (nullable = false)
 | |-- a: struct (nullable = false)
 | | |-- aa: string (nullable = false)
 | | |-- ab: string (nullable = false)
 | | |-- ac: string (nullable = false){code}
And an example of runtime data issue:
{code:java}
df = (spark.range(1)
 .withColumn('data', F.struct()
   .withField('a', F.struct().withField('aa', F.lit('aa')))
   .withField('b', F.struct().withField('ba', F.lit('ba')))
 )
 .withColumn('data', F.col('data').withField('b.bb', F.lit('bb')))
 .withColumn('data', F.col('data').withField('a.ab', F.lit('ab')))
)
df.printSchema()
df.groupBy('data.a.aa', 'data.a.ab', 'data.b.ba', 'data.b.bb').count().show()
{code}
 Output:
{code:java}
root
 |-- id: long (nullable = false)
 |-- data: struct (nullable = false)
 | |-- a: struct (nullable = false)
 | | |-- aa: string (nullable = false)
 | | |-- ab: string (nullable = false)
 | |-- b: struct (nullable = false)
 | | |-- ba: string (nullable = false)
 | | |-- bb: string (nullable = false)
+---+---+---+---+-----+
| aa| ab| ba| bb|count|
+---+---+---+---+-----+
| ba| bb| aa| ab|    1|
+---+---+---+---+-----+
{code}
The columns have the wrong data in them, even though the schema is correct. Additionally, if you add another column you get an exception:
{code:java}
df = (spark.range(1)
 .withColumn('data', F.struct()
   .withField('a', F.struct().withField('aa', F.lit('aa')))
   .withField('b', F.struct().withField('ba', F.lit('ba')))
 )
 .withColumn('data', F.col('data').withField('a.ab', F.lit('ab')))
 .withColumn('data', F.col('data').withField('b.bb', F.lit('bb')))
 .withColumn('data', F.col('data').withField('a.ac', F.lit('ac')))
)
df.groupBy('data.a.aa', 'data.a.ab', 'data.a.ac', 'data.b.ba', 'data.b.bb').count().show()
java.lang.ArrayIndexOutOfBoundsException: 2 at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.genericGet(rows.scala:201) at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getAs(rows.scala:35) at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getUTF8String(rows.scala:46) at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getUTF8String$(rows.scala:46) at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getUTF8String(rows.scala:195) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doConsume_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
{code}
 But if you reorder the withField expressions, you get correct behavior:
{code:java}
df = (spark.range(1)
 .withColumn('data', F.struct()
   .withField('a', F.struct().withField('aa', F.lit('aa')))
   .withField('b', F.struct().withField('ba', F.lit('ba')))
 )
 .withColumn('data', F.col('data').withField('a.ab', F.lit('ab')))
 .withColumn('data', F.col('data').withField('a.ac', F.lit('ac')))
 .withColumn('data', F.col('data').withField('b.bb', F.lit('bb')))
)
df.groupBy('data.a.aa', 'data.a.ab', 'data.a.ac', 'data.b.ba', 'data.b.bb').count().show()
+---+---+---+---+---+-----+
| aa| ab| ac| ba| bb|count|
+---+---+---+---+---+-----+
| aa| ab| ac| ba| bb|    1|
+---+---+---+---+---+-----+
{code}
 I think this has to do with the double "".reverse"" method to dedupe expressions in OptimizeUpdateFields. I'm working on a PR to try to fix these issues.

 

 ",,apachespark,kimahriman,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 26 06:40:34 UTC 2021,,,,,,,,,,"0|z0qezs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/21 15:11;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/32338;;;","25/Apr/21 15:12;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/32338;;;","26/Apr/21 06:40;viirya;Issue resolved by pull request 32338
[https://github.com/apache/spark/pull/32338];;;",,,,,,,,,,,,,,,,,
Upgrade Jetty to 9.4.40 to fix ERR_CONNECTION_RESET issue,SPARK-35210,13374782,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sarutak,sarutak,sarutak,24/Apr/21 06:20,25/Apr/21 03:56,13/Jul/23 08:47,24/Apr/21 10:28,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,Build,,,,0,,,"SPARK-34988 upgraded Jetty to 9.4.39 for CVE-2021-28165.
But after the upgrade, Jetty 9.4.40 was released to fix the ERR_CONNECTION_RESET issue (https://github.com/eclipse/jetty.project/issues/6152).
This issue seems to affect Jetty 9.4.39 when POST method is used with SSL.
For Spark, job submission using REST and ThriftServer with HTTPS protocol can be affected.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 24 10:47:47 UTC 2021,,,,,,,,,,"0|z0qelk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/21 06:27;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32318;;;","24/Apr/21 06:28;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32318;;;","24/Apr/21 10:28;sarutak;This issue is resolved in https://github.com/apache/spark/pull/32318 for 3.2.0.;;;","24/Apr/21 10:33;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32322;;;","24/Apr/21 10:34;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32322;;;","24/Apr/21 10:47;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32323;;;","24/Apr/21 10:47;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32324;;;","24/Apr/21 10:47;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32324;;;",,,,,,,,,,,,
hash() and other hash builtins do not normalize negative zero,SPARK-35207,13374739,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,planga82,tarmstrong,tarmstrong,23/Apr/21 21:47,14/May/21 04:42,13/Jul/23 08:47,14/May/21 04:42,3.1.1,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,correctness,,"I would generally expect that {{x = y => hash( x ) = hash( y )}}. However +-0 hash to different values for floating point types. 

{noformat}
scala> spark.sql(""select hash(cast('0.0' as double)), hash(cast('-0.0' as double))"").show
+-------------------------+--------------------------+
|hash(CAST(0.0 AS DOUBLE))|hash(CAST(-0.0 AS DOUBLE))|
+-------------------------+--------------------------+
|              -1670924195|                -853646085|
+-------------------------+--------------------------+
scala> spark.sql(""select cast('0.0' as double) == cast('-0.0' as double)"").show
+--------------------------------------------+
|(CAST(0.0 AS DOUBLE) = CAST(-0.0 AS DOUBLE))|
+--------------------------------------------+
|                                        true|
+--------------------------------------------+
{noformat}

I'm not sure how likely this is to cause issues in practice, since only a limited number of calculations can produce -0 and joining or aggregating with floating point keys is a bad practice as a general rule, but I think it would be safer if we normalised -0.0 to +0.0.",,apachespark,planga82,tarmstrong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 11 00:29:46 UTC 2021,,,,,,,,,,"0|z0qec0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/May/21 00:15;planga82;Hi [~tarmstrong] ,

I have read about signed zero and it's a little bit tricky, from what I have read in IEEE 754 and it's implementation in Java I understand the same as you it's not consistent to have different hash values for 0 and -0. This applays to double and float.

Here is an extract from IEEE 754: ""The two zeros are distinguishable arithmetically only by either division-byzero ( producing appropriately signed infinities ) or else by the CopySign function recommended by IEEE 754 /854. Infinities, SNaNs, NaNs and Subnormal numbers necessitate four more special cases""

I will raise a PR with this.

Regards;;;","11/May/21 00:29;apachespark;User 'planga82' has created a pull request for this issue:
https://github.com/apache/spark/pull/32496;;;",,,,,,,,,,,,,,,,,,
CombineConcats should call transformAllExpressions,SPARK-35183,13374303,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,buyingyi,buyingyi,buyingyi,22/Apr/21 06:57,22/Apr/21 13:10,13/Jul/23 08:47,22/Apr/21 13:10,3.1.0,,,,,,,,,,,,,,,3.2.0,,,,Optimizer,,,,0,,,"{code:java}
plan transformExpressions { ... }{code}
only applies the transformation node `plan` itself, but not its children. We should call transformAllExpressions instead of transformExpressions in CombineConcats. ",,apachespark,buyingyi,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 22 13:10:57 UTC 2021,,,,,,,,,,"0|z0qbnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/21 07:25;apachespark;User 'sigmod' has created a pull request for this issue:
https://github.com/apache/spark/pull/32290;;;","22/Apr/21 07:25;apachespark;User 'sigmod' has created a pull request for this issue:
https://github.com/apache/spark/pull/32290;;;","22/Apr/21 13:10;cloud_fan;Issue resolved by pull request 32290
[https://github.com/apache/spark/pull/32290];;;",,,,,,,,,,,,,,,,,
maven autodownload failing,SPARK-35178,13374183,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,bersprockets,bersprockets,21/Apr/21 17:26,22/Apr/21 02:01,13/Jul/23 08:47,22/Apr/21 02:01,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,Build,,,,0,,,"I attempted to build a fresh clone of Spark using mvn (on two different networks) and got this error:
{noformat}
exec: curl --silent --show-error -L https://www.apache.org/dyn/closer.lua?action=download&filename=/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz
tar: Unrecognized archive format
tar: Error exit delayed from previous errors.
Using `mvn` from path: /tmp/testmvn/spark-mvn-download/build/apache-maven-3.6.3/bin/mvn
build/mvn: line 126: /tmp/testmvn/spark-mvn-download/build/apache-maven-3.6.3/bin/mvn: No such file or directory
{noformat}
if I change the mirror as below, the issue goes away:
{noformat}
-    local APACHE_MIRROR=${APACHE_MIRROR:-'https://www.apache.org/dyn/closer.lua?action=download&filename='}
+    local APACHE_MIRROR=${APACHE_MIRROR:-'https://https://downloads.apache.org'}
{noformat}
 ",,apachespark,bersprockets,dongjoon,Tonix517,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,INFRA-21767,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 22 02:01:18 UTC 2021,,,,,,,,,,"0|z0qawo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/21 18:01;srowen;I agree, it looks like the automatic redirector has changed behavior. It still sends you to an HTML page for the mirror, but previously that link would cause it to redirect straight to the download.While the script can fallback to archive.apache.org, it doesn't because the HTML downloads successfully -- just is not the distribution!Either we detect this or have to hack this more to get the mirror URL from the redirector, then attach it to the path.;;;","21/Apr/21 19:46;bersprockets;I also posted https://issues.apache.org/jira/browse/INFRA-21767. Maybe they have some insight.;;;","21/Apr/21 20:51;bersprockets;In INFRA-21767, Daniel Gruno responded:
{quote}
Please use this format instead:
https://www.apache.org/dyn/closer.lua/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz?action=download

that is, https://www.apache.org/dyn/closer.lua/path/to/file.tar.gz?action=download
{quote}

 ;;;","21/Apr/21 21:08;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32277;;;","21/Apr/21 21:09;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32277;;;","22/Apr/21 02:00;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32282;;;","22/Apr/21 02:01;dongjoon;This is resolved via https://github.com/apache/spark/pull/32277;;;","22/Apr/21 02:01;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32282;;;",,,,,,,,,,,,
 Raise TypeError in inappropriate type case rather than ValueError,SPARK-35176,13374160,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yikunkero,yikunkero,yikunkero,21/Apr/21 14:47,12/Dec/22 18:11,13/Jul/23 08:47,03/May/21 06:34,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,PySpark,,,,0,,,"There are many wrong error type usages on ValueError type.

When an operation or function is applied to an object of inappropriate type, we should use TypeError rather than ValueError.

such as:

[https://github.com/apache/spark/blob/355c39939d9e4c87ffc9538eb822a41cb2ff93fb/python/pyspark/sql/dataframe.py#L1137]

[https://github.com/apache/spark/blob/355c39939d9e4c87ffc9538eb822a41cb2ff93fb/python/pyspark/sql/dataframe.py#L1228]

 

We should do some correction in some right time, note that if we do these corrections, it will break some catch on original ValueError.

 

[1] https://docs.python.org/3/library/exceptions.html#TypeError",,apachespark,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 03 06:34:53 UTC 2021,,,,,,,,,,"0|z0qark:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/21 04:39;yikunkero;I write up a POC in [https://github.com/Yikun/annotation-type-checker/pull/4] to add some simple way to do input validation (runtime type checker).;;;","23/Apr/21 10:55;gurwls223;[~yikunkero] Please go ahead for a PR;;;","26/Apr/21 08:21;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32349;;;","26/Apr/21 08:22;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32349;;;","26/Apr/21 10:45;yikunkero;[https://github.com/apache/spark/pull/32349] in order to convenient to review, this patch only address the typeerror, and the decorator part would be submitted as a speparated patch.;;;","27/Apr/21 13:42;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32368;;;","03/May/21 06:34;gurwls223;Issue resolved by pull request 32368
[https://github.com/apache/spark/pull/32368];;;",,,,,,,,,,,,,
`OneVsRest` classifier uses incorrect data type for `rawPrediction` column,SPARK-35142,13373733,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,harupy,harupy,harupy,20/Apr/21 01:29,24/Apr/21 21:38,13/Jul/23 08:47,24/Apr/21 21:37,3.0.0,3.0.2,3.1.0,3.1.1,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,ML,,,,0,,,"`OneVsRest` classifier uses an incorrect data type for the `rawPrediction` column.

 Code to reproduce the issue:
{code:java}
from pyspark.ml.classification import LogisticRegression, OneVsRest
from pyspark.ml.linalg import Vectors
from pyspark.sql import SparkSession
from sklearn.datasets import load_iris

spark = SparkSession.builder.getOrCreate()

X, y = load_iris(return_X_y=True)
df = spark.createDataFrame(
 [(Vectors.dense(features), int(label)) for features, label in zip(X, y)], [""features"", ""label""]
)
train, test = df.randomSplit([0.8, 0.2])
lor = LogisticRegression(maxIter=5)
ovr = OneVsRest(classifier=lor)
ovrModel = ovr.fit(train)
pred = ovrModel.transform(test)

pred.printSchema()
# This prints out:
# root
#  |-- features: vector (nullable = true)
#  |-- label: long (nullable = true)
#  |-- rawPrediction: string (nullable = true)  # <- should not be string
#  |-- prediction: double (nullable = true)

# pred.show()  # this fails because of the incorrect datatype{code}
I ran the code above using GitHub Actiosn:

[https://github.com/harupy/SPARK-35142/pull/1]

 

It looks like the UDF to compute the `rawPrediction` column is generated without specyfing the return type:
 [https://github.com/apache/spark/blob/0494dc90af48ce7da0625485a4dc6917a244d580/python/pyspark/ml/classification.py#L3154]
{code:java}
rawPredictionUDF = udf(func)
{code}
 ",,apachespark,harupy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 21 13:04:50 UTC 2021,,,,,,,,,,"0|z0q84w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/21 02:35;apachespark;User 'harupy' has created a pull request for this issue:
https://github.com/apache/spark/pull/32245;;;","20/Apr/21 02:36;apachespark;User 'harupy' has created a pull request for this issue:
https://github.com/apache/spark/pull/32245;;;","21/Apr/21 08:42;apachespark;User 'harupy' has created a pull request for this issue:
https://github.com/apache/spark/pull/32269;;;","21/Apr/21 13:03;apachespark;User 'harupy' has created a pull request for this issue:
https://github.com/apache/spark/pull/32275;;;","21/Apr/21 13:04;apachespark;User 'harupy' has created a pull request for this issue:
https://github.com/apache/spark/pull/32275;;;",,,,,,,,,,,,,,,
Initial null value of LiveStage.info can lead to NPE,SPARK-35136,13373572,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sgoos-db,sgoos-db,sgoos-db,19/Apr/21 09:56,19/Apr/21 15:13,13/Jul/23 08:47,19/Apr/21 15:09,3.1.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,Spark Core,,,,0,,,"The `AppStatusListener.getOrCreateStage` method creates a LiveStage object with the `info` field set to null and right after that set it to a specific StageInfo object. This can lead to a race condition when the `livestages` are read in between those calls. This could then lead to a null pointer exception in, for instance: `activeStages`.",,apachespark,cloud_fan,sgoos-db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 19 15:09:54 UTC 2021,,,,,,,,,,"0|z0q754:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/21 10:10;apachespark;User 'sander-goos' has created a pull request for this issue:
https://github.com/apache/spark/pull/32233;;;","19/Apr/21 10:11;apachespark;User 'sander-goos' has created a pull request for this issue:
https://github.com/apache/spark/pull/32233;;;","19/Apr/21 15:09;cloud_fan;Issue resolved by pull request 32233
[https://github.com/apache/spark/pull/32233];;;",,,,,,,,,,,,,,,,,
setup.py will copy different version netty jars into deps/jars when execute PySpark pip packaging tests,SPARK-35134,13373463,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,19/Apr/21 06:48,12/Dec/22 18:10,13/Jul/23 08:47,20/Apr/21 05:39,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Tests,,,,0,,,"[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/137500/console]

 
copying deps/jars/netty-all-4.1.51.Final.jar -> pyspark-3.2.0.dev0/deps/jars
copying deps/jars/netty-buffer-4.1.50.Final.jar -> pyspark-3.2.0.dev0/deps/jars
copying deps/jars/netty-codec-4.1.50.Final.jar -> pyspark-3.2.0.dev0/deps/jars
copying deps/jars/netty-common-4.1.50.Final.jar -> pyspark-3.2.0.dev0/deps/jars
copying deps/jars/netty-handler-4.1.50.Final.jar -> pyspark-3.2.0.dev0/deps/jars
copying deps/jars/netty-resolver-4.1.50.Final.jar -> pyspark-3.2.0.dev0/deps/jars
copying deps/jars/netty-transport-4.1.50.Final.jar -> pyspark-3.2.0.dev0/deps/jars
copying deps/jars/netty-transport-native-epoll-4.1.50.Final.jar -> pyspark-3.2.0.dev0/deps/jars
 

Master relies on netty 4.1.51 but there will copy 4.1.51 and 4.1.50 to pyspark-3.2.0.dev0/deps/jars

 

-------------------------------------

 

[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/137547/console]

 
copying deps/jars/netty-all-4.1.63.Final.jar -> pyspark-3.2.0.dev0/deps/jars
copying deps/jars/netty-buffer-4.1.50.Final.jar -> pyspark-3.2.0.dev0/deps/jars
copying deps/jars/netty-codec-4.1.50.Final.jar -> pyspark-3.2.0.dev0/deps/jars
copying deps/jars/netty-common-4.1.50.Final.jar -> pyspark-3.2.0.dev0/deps/jars
copying deps/jars/netty-handler-4.1.50.Final.jar -> pyspark-3.2.0.dev0/deps/jars
copying deps/jars/netty-resolver-4.1.50.Final.jar -> pyspark-3.2.0.dev0/deps/jars
copying deps/jars/netty-transport-4.1.50.Final.jar -> pyspark-3.2.0.dev0/deps/jars
copying deps/jars/netty-transport-native-epoll-4.1.50.Final.jar -> pyspark-3.2.0.dev0/deps/jars
 

SPARK-35132 try to upgrade netty to 4.1.63.Final, but  still copy 4.1.63 and 4.1.50 to pyspark-3.2.0.dev0/deps/jars

 

 

 ",,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 20 05:39:28 UTC 2021,,,,,,,,,,"0|z0q6gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/21 06:54;LuciferYang;execute 

mvn dependency:tree -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive

 

there was no netty 4.1.50 dependency in the results.;;;","19/Apr/21 07:21;LuciferYang;org.apache.zookeeper relies on netty 4.1.50;;;","19/Apr/21 08:38;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32230;;;","19/Apr/21 08:38;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32230;;;","20/Apr/21 05:39;gurwls223;Issue resolved by pull request 32230
[https://github.com/apache/spark/pull/32230];;;",,,,,,,,,,,,,,,
UI progress bar no longer highlights in progress tasks,SPARK-35117,13373249,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kimahriman,kimahriman,kimahriman,17/Apr/21 21:03,22/Apr/21 00:56,13/Jul/23 08:47,20/Apr/21 03:03,3.1.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,Web UI,,,,0,,,"The Spark UI was updated to Bootstrap 4, and during the update the progress bar in the UI was updated to highlight the whole bar once any tasks were in progress, versus highlighting just the number of tasks that were in progress. The was a great visual queue of seeing what percentage of the stage/job was currently being worked on, and it'd be great to get that functionality back.

The change can be found here: https://github.com/apache/spark/pull/27370/files#diff-809c93c57cc59e5fe3c3eb54a24aa96a38147d02323f3e690ae6b5309a3284d2L448",,apachespark,kimahriman,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 22 00:56:07 UTC 2021,,,,,,,,,,"0|z0q55c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/21 21:30;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/32214;;;","20/Apr/21 03:03;sarutak;This issue was resolved in https://github.com/apache/spark/pull/32214.;;;","22/Apr/21 00:56;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32281;;;",,,,,,,,,,,,,,,,,
HadoopMapReduceCommitProtocol performs bad rename when dynamic partition overwrite is used,SPARK-35106,13373062,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yuzhousun,xkrogen,xkrogen,16/Apr/21 16:04,24/Sep/21 22:38,13/Jul/23 08:47,19/May/21 07:47,3.1.1,,,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,Input/Output,Spark Core,,,0,,,"Recently when evaluating the code in {{HadoopMapReduceCommitProtocol#commitJob}}, I found some bad codepath under the {{dynamicPartitionOverwrite == true}} scenario:

{code:language=scala}
      # BLOCK 1
      if (dynamicPartitionOverwrite) {
        val absPartitionPaths = filesToMove.values.map(new Path(_).getParent).toSet
        logDebug(s""Clean up absolute partition directories for overwriting: $absPartitionPaths"")
        absPartitionPaths.foreach(fs.delete(_, true))
      }
      # BLOCK 2
      for ((src, dst) <- filesToMove) {
        fs.rename(new Path(src), new Path(dst))
      }

      # BLOCK 3
      if (dynamicPartitionOverwrite) {
        val partitionPaths = allPartitionPaths.foldLeft(Set[String]())(_ ++ _)
        logDebug(s""Clean up default partition directories for overwriting: $partitionPaths"")
        for (part <- partitionPaths) {
          val finalPartPath = new Path(path, part)
          if (!fs.delete(finalPartPath, true) && !fs.exists(finalPartPath.getParent)) {
            // According to the official hadoop FileSystem API spec, delete op should assume
            // the destination is no longer present regardless of return value, thus we do not
            // need to double check if finalPartPath exists before rename.
            // Also in our case, based on the spec, delete returns false only when finalPartPath
            // does not exist. When this happens, we need to take action if parent of finalPartPath
            // also does not exist(e.g. the scenario described on SPARK-23815), because
            // FileSystem API spec on rename op says the rename dest(finalPartPath) must have
            // a parent that exists, otherwise we may get unexpected result on the rename.
            fs.mkdirs(finalPartPath.getParent)
          }
          fs.rename(new Path(stagingDir, part), finalPartPath)
        }
      }
{code}

Assuming {{dynamicPartitionOverwrite == true}}, we have the following sequence of events:
# Block 1 deletes all parent directories of {{filesToMove.values}}
# Block 2 attempts to rename all {{filesToMove.keys}} to {{filesToMove.values}}
# Block 3 does directory-level renames to place files into their final locations

All renames in Block 2 will always fail, since all parent directories of {{filesToMove.values}} were just deleted in Block 1. Under a normal HDFS scenario, the contract of {{fs.rename}} is to return {{false}} under such a failure scenario, as opposed to throwing an exception. There is a separate issue here that Block 2 should probably be checking for those {{false}} return values -- but this allows for {{dynamicPartitionOverwrite}} to ""work"", albeit with a bunch of failed renames in the middle. Really, we should only run Block 2 in the {{dynamicPartitionOverwrite == false}} case, and consolidate Blocks 1 and 3 to run in the {{true}} case.

We discovered this issue when testing against a {{FileSystem}} implementation which was throwing an exception for this failed rename scenario instead of returning false, escalating the silent/ignored rename failures into actual failures.",,apachespark,cloud_fan,xkrogen,yuzhousun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 20 03:36:03 UTC 2021,,,,,,,,,,"0|z0q3zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/21 16:15;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32207;;;","13/May/21 01:40;apachespark;User 'YuzhouSun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32529;;;","13/May/21 01:40;apachespark;User 'YuzhouSun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32529;;;","13/May/21 03:09;apachespark;User 'YuzhouSun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32530;;;","13/May/21 03:10;apachespark;User 'YuzhouSun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32530;;;","19/May/21 07:47;cloud_fan;Issue resolved by pull request 32530
[https://github.com/apache/spark/pull/32530];;;","20/May/21 03:36;yuzhousun;Thanks Erik and Wenchen for initializing and reviewing the fix!;;;",,,,,,,,,,,,,
Fix ugly indentation of multiple JSON records in a single split file generated by JacksonGenerator when pretty option is true,SPARK-35104,13372812,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,16/Apr/21 06:05,16/Apr/21 08:01,13/Jul/23 08:47,16/Apr/21 08:01,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"When writing multiple JSON records into a single split file with pretty option true, indentation will be broken except for the first JSON record.
{code:java}
// Run in the Spark Shell.
// Set spark.sql.leafNodeDefaultParallelism to 1 for the current master.
// Or set spark.default.parallelism for the previous releases.
spark.conf.set(""spark.sql.leafNodeDefaultParallelism"", 1)
val df = Seq(""a"", ""b"", ""c"").toDF
df.write.option(""pretty"", ""true"").json(""/path/to/output"")

# Run in a Shell
$ cat /path/to/output/*.json
{
  ""value"" : ""a""
}
 {
  ""value"" : ""b""
}
 {
  ""value"" : ""c""
}
{code}",,apachespark,maxgekk,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 16 08:01:25 UTC 2021,,,,,,,,,,"0|z0q2g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/21 06:25;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32203;;;","16/Apr/21 08:01;maxgekk;Issue resolved by pull request 32203
[https://github.com/apache/spark/pull/32203];;;",,,,,,,,,,,,,,,,,,
foreachBatch throws ArrayIndexOutOfBoundsException if schema is case Insensitive,SPARK-35096,13372707,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandeep.katta2007,sandeep.katta2007,sandeep.katta2007,15/Apr/21 17:47,22/Apr/21 05:59,13/Jul/23 08:47,21/Apr/21 07:19,3.0.0,,,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,Spark Core,,,,0,,,"Below code works fine before spark3, running on spark3 throws 

java.lang.ArrayIndexOutOfBoundsException
{code:java}
val inputPath = ""/Users/xyz/data/testcaseInsensitivity""
val output_path = ""/Users/xyz/output""

spark.range(10).write.format(""parquet"").save(inputPath)

def process_row(microBatch: DataFrame, batchId: Long): Unit = {
  val df = microBatch.select($""ID"".alias(""other"")) // Doesn't work
  df.write.format(""parquet"").mode(""append"").save(output_path)

}

val schema = new StructType().add(""id"", LongType)

val stream_df = spark.readStream.schema(schema).format(""parquet"").load(inputPath)
stream_df.writeStream.trigger(Trigger.Once).foreachBatch(process_row _)
  .start().awaitTermination()
{code}
Stack Trace:
{code:java}
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
  at org.apache.spark.sql.types.StructType.apply(StructType.scala:414)
  at org.apache.spark.sql.catalyst.optimizer.ObjectSerializerPruning$$anonfun$apply$4.$anonfun$applyOrElse$3(objects.scala:216)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at scala.collection.TraversableLike.map(TraversableLike.scala:238)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
  at scala.collection.immutable.List.map(List.scala:298)
  at org.apache.spark.sql.catalyst.optimizer.ObjectSerializerPruning$$anonfun$apply$4.applyOrElse(objects.scala:215)
  at org.apache.spark.sql.catalyst.optimizer.ObjectSerializerPruning$$anonfun$apply$4.applyOrElse(objects.scala:203)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:309)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:149)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:147)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:149)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:147)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:298)
  at org.apache.spark.sql.catalyst.optimizer.ObjectSerializerPruning$.apply(objects.scala:203)
  at org.apache.spark.sql.catalyst.optimizer.ObjectSerializerPruning$.apply(objects.scala:121)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:149)
  at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
  at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
  at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:146)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:138)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:138)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:116)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:116)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:82)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:133)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:133)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:82)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:79)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$writePlans$4(QueryExecution.scala:197)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$.append(QueryPlan.scala:381)
  at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$writePlans(QueryExecution.scala:197)
  at org.apache.spark.sql.execution.QueryExecution.toString(QueryExecution.scala:207)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:95)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)
  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)
  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)
  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)
  at process_row(<console>:32)
  at $anonfun$res4$1(<console>:30)
  at $anonfun$res4$1$adapted(<console>:30)
  at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:36)
  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:573)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:571)
  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)
  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)
  at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)
  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:571)
  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:223)
  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)
  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)
  at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)
  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)
  at org.apache.spark.sql.execution.streaming.OneTimeExecutor.execute(TriggerExecutor.scala:39)
  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)
  at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)
  ... 1 more
{code}",,apachespark,sandeep.katta2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 22 04:23:46 UTC 2021,,,,,,,,,,"0|z0q1sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/21 17:47;sandeep.katta2007;Working on fix, soon raise  PR for this;;;","15/Apr/21 18:03;apachespark;User 'sandeep-katta' has created a pull request for this issue:
https://github.com/apache/spark/pull/32194;;;","15/Apr/21 18:03;apachespark;User 'sandeep-katta' has created a pull request for this issue:
https://github.com/apache/spark/pull/32194;;;","22/Apr/21 04:23;apachespark;User 'sandeep-katta' has created a pull request for this issue:
https://github.com/apache/spark/pull/32284;;;","22/Apr/21 04:23;apachespark;User 'sandeep-katta' has created a pull request for this issue:
https://github.com/apache/spark/pull/32284;;;",,,,,,,,,,,,,,,
"[k8s] On Spark 3, jars listed in spark.jars and spark.jars.packages are not added to sparkContext",SPARK-35084,13372538,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ocworld,ocworld,ocworld,15/Apr/21 05:25,15/Jan/23 01:49,13/Jul/23 08:47,15/Jan/23 01:46,3.0.0,3.0.2,3.1.1,,,,,,,,,,,,,3.4.0,,,,Kubernetes,,,,1,,,"I'm trying to migrate spark 2 to spark 3 in k8s.

 

In my environment, on Spark 3.x, jars listed in spark.jars and spark.jars.packages are not added to sparkContext.

After driver's process is launched, jars are not propagated to Executors. So, NoClassDefException is raised in executors.

 

In spark.properties, the only main application jar is contained in spark.jars. It is different from Spark 2.

 

How to solve this situation? Is it any changed spark options in spark 3 from spark 2?",,apachespark,ocworld,roczei,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33929,SPARK-34870,SPARK-32775,SPARK-23153,SPARK-32545,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 15 01:46:32 UTC 2023,,,,,,,,,,"0|z0q0rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/21 02:06;ocworld;*Spark 2.4.5*

[https://github.com/apache/spark/blob/v2.4.5/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala]
{code:java}
     if (!isMesosCluster && !isStandAloneCluster) {
      // Resolve maven dependencies if there are any and add classpath to jars. Add them to py-files
      // too for packages that include Python code
      val resolvedMavenCoordinates = DependencyUtils.resolveMavenDependencies(
        args.packagesExclusions, args.packages, args.repositories, args.ivyRepoPath,
        args.ivySettingsPath)
      
      if (!StringUtils.isBlank(resolvedMavenCoordinates)) {
        args.jars = mergeFileLists(args.jars, resolvedMavenCoordinates)
        if (args.isPython || isInternal(args.primaryResource)) {
          args.pyFiles = mergeFileLists(args.pyFiles, resolvedMavenCoordinates)
        }
      } 
      
      // install any R packages that may have been passed through --jars or --packages.
      // Spark Packages may contain R source code inside the jar.
      if (args.isR && !StringUtils.isBlank(args.jars)) {
        RPackageUtils.checkAndBuildRPackage(args.jars, printStream, args.verbose)
      }
    } {code}
 

*Spark 3.0.2*

[https://github.com/apache/spark/blob/v3.0.2/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala]
{code:java}
       if (!StringUtils.isBlank(resolvedMavenCoordinates)) {
        // In K8s client mode, when in the driver, add resolved jars early as we might need
        // them at the submit time for artifact downloading.
        // For example we might use the dependencies for downloading
        // files from a Hadoop Compatible fs eg. S3. In this case the user might pass:
        // --packages com.amazonaws:aws-java-sdk:1.7.4:org.apache.hadoop:hadoop-aws:2.7.6
        if (isKubernetesClusterModeDriver) {
          val loader = getSubmitClassLoader(sparkConf)
          for (jar <- resolvedMavenCoordinates.split("","")) {
            addJarToClasspath(jar, loader)
          }
        } else if (isKubernetesCluster) {
          // We need this in K8s cluster mode so that we can upload local deps
          // via the k8s application, like in cluster mode driver
          childClasspath ++= resolvedMavenCoordinates.split("","")
        } else {
          args.jars = mergeFileLists(args.jars, resolvedMavenCoordinates)
          if (args.isPython || isInternal(args.primaryResource)) {
            args.pyFiles = mergeFileLists(args.pyFiles, resolvedMavenCoordinates)
          }
        }
      }{code}
 

When using k8s master, in spark 2, jars derived from maven are added to args.jars.

However, in spark 3, maven dependencies are not merged to args.jars.

 

I assume that because of it k8s cluster mode spark-submit is not supported spark.jars.packages I expected.

So, jars from packages are not added to spark context.

 

How to use maven packages in k8s cluster mode?;;;","29/Apr/21 13:01;apachespark;User 'ocworld' has created a pull request for this issue:
https://github.com/apache/spark/pull/32397;;;","29/Apr/21 13:02;apachespark;User 'ocworld' has created a pull request for this issue:
https://github.com/apache/spark/pull/32397;;;","12/Oct/22 14:03;roczei;Hi [~ocworld],

[Your pull request|https://github.com/apache/spark/pull/32397] has been automatically closed by the github action, I would like to create a new pull request based on yours and continue to work on this if you agree.;;;","29/Nov/22 00:32;apachespark;User 'ocworld' has created a pull request for this issue:
https://github.com/apache/spark/pull/38828;;;","15/Jan/23 01:46;attilapiros;Issue resolved by pull request 38828
https://github.com/apache/spark/pull/38828
;;;",,,,,,,,,,,,,,
Correlated subqueries with equality predicates can return wrong results,SPARK-35080,13372505,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,allisonwang-db,allisonwang-db,allisonwang-db,15/Apr/21 00:08,10/Feb/22 19:18,13/Jul/23 08:47,20/Apr/21 03:26,3.2.0,,,,,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,SQL,,,,0,,,"Correlated subqueries with aggregate that pass CheckAnalysis (with only correlated equality predicates) can still return wrong results. This is because equality predicates do not guarantee one-to-one mappings between inner and outer attributes, and the semantics of the plan will be changed when the inner attributes are pulled up through an Aggregate, which gives us wrong results. Currently, the decorrelation framework does not support these types of correlated subqueries, and they should be blocked in CheckAnalysis.

Example 1:
{code:sql}
create or replace view t1(c) as values ('a'), ('b')
create or replace view t2(c) as values ('ab'), ('abc'), ('bc')

select c, (select count(*) from t2 where t1.c = substring(t2.c, 1, 1)) from t1
{code}
Correct results: [(a, 2), (b, 1)]
 Spark results:
{code:java}
+---+-----------------+
|c  |scalarsubquery(c)|
+---+-----------------+
|a  |1                |
|a  |1                |
|b  |1                |
+---+-----------------+{code}
Example 2:
{code:sql}
create or replace view t1(a, b) as values (0, 6), (1, 5), (2, 4), (3, 3);
create or replace view t2(c) as values (6);

select c, (select count(*) from t1 where a + b = c) from t2;{code}
Correct results: [(6, 4)]
 Spark results:
{code:java}
+---+-----------------+
|c  |scalarsubquery(c)|
+---+-----------------+
|6  |1                |
|6  |1                |
|6  |1                |
|6  |1                |
+---+-----------------+
{code}
 ",,allisonwang-db,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-36114,SPARK-38180,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 20 03:26:51 UTC 2021,,,,,,,,,,"0|z0q0k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/21 00:32;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/32179;;;","20/Apr/21 03:26;cloud_fan;Issue resolved by pull request 32179
[https://github.com/apache/spark/pull/32179];;;",,,,,,,,,,,,,,,,,,
Transform with udf gives incorrect result,SPARK-35079,13372499,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,koert,koert,14/Apr/21 21:46,18/May/21 14:40,13/Jul/23 08:47,11/May/21 01:29,3.1.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"i think this is a correctness bug in spark 3.1.1

the behavior is correct in spark 3.0.1

in spark 3.0.1:
{code:java}
scala> import spark.implicits._

scala> import org.apache.spark.sql.functions._

scala> val x = Seq(Seq(""aa"", ""bb"", ""cc"")).toDF
x: org.apache.spark.sql.DataFrame = [value: array<string>]

scala> x.select(transform(col(""value""), col => udf((_: String).drop(1)).apply(col))).show
+---------------------------------------------------+
|transform(value, lambdafunction(UDF(lambda 'x), x))|
+---------------------------------------------------+
|                                          [a, b, c]|
+---------------------------------------------------+
{code}
in spark 3.1.1:
{code:java}
scala> import spark.implicits._

scala> import org.apache.spark.sql.functions._

scala> val x = Seq(Seq(""aa"", ""bb"", ""cc"")).toDF
x: org.apache.spark.sql.DataFrame = [value: array<string>]

scala> x.select(transform(col(""value""), col => udf((_: String).drop(1)).apply(col))).show
+---------------------------------------------------+
|transform(value, lambdafunction(UDF(lambda 'x), x))|
+---------------------------------------------------+
|                                          [c, c, c]|
+---------------------------------------------------+
{code}",,koert,maropu,shahid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 18 14:40:10 UTC 2021,,,,,,,,,,"0|z0q0io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/May/21 22:28;shahid;Seems It is not reproducible with master branch?
`

{code:java}
+-----------------------------------------------------------------------------+
|transform(value, lambdafunction(UDF(lambda x_0#3993), namedlambdavariable()))|
+-----------------------------------------------------------------------------+
|                                                                    [a, b, c]|
+-----------------------------------------------------------------------------+


{code}

`

;;;","10/May/21 23:16;maropu;Could you check if branch-3.1 has the issue?;;;","11/May/21 01:28;maropu;I've checked it and the issue has already resolved in latest branch-3.1;
{code:java}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.2-SNAPSHOT
      /_/
         
Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import spark.implicits._

scala> import org.apache.spark.sql.functions._

scala> val x = Seq(Seq(""aa"", ""bb"", ""cc"")).toDF
x: org.apache.spark.sql.DataFrame = [value: array<string>]


scala> x.select(transform(col(""value""), col => udf((_: String).drop(1)).apply(col))).show
+-------------------------------------------------------+
|transform(value, lambdafunction(UDF(lambda 'x_0), x_0))|
+-------------------------------------------------------+
|                                              [a, b, c]|
+-------------------------------------------------------+
 {code}
So, I will close this. Anyway, thank you for the report.;;;","18/May/21 14:40;koert;looks to me like this is a duplicate of SPARK-34829;;;",,,,,,,,,,,,,,,,
Close the inputStream in FileAppender when writing the logs failure,SPARK-35027,13371210,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jhu,jhu,jhu,12/Apr/21 08:47,21/Jul/21 02:27,13/Jul/23 08:47,21/Jul/21 02:26,3.1.1,,,,,,,,,,,,,,,3.0.4,3.1.3,3.2.0,,Spark Core,,,,0,,,"In Spark Cluster, the ExecutorRunner uses FileAppender  to redirect the stdout/stderr of executors to file, when the writing processing is failure due to some reasons: disk full, the FileAppender will only close the input stream to file, but leave the pipe's stdout/stderr open, following writting operation in executor side may be hung. 

need to close the inputStream in FileAppender ?",,apachespark,jhu,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 21 02:27:09 UTC 2021,,,,,,,,,,"0|z0pskw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/21 18:23;srowen;Are you sure? stop() is called on an error on these FileAppenders.;;;","01/Jul/21 06:34;jhu;Of course, the ""stop"" in FileAppender does nothing but set a flag. 

The exception will be thrown in ""[appendStreamToFile|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala#L59]"", but the cloure in finally only closes the [output stream (to file)|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/logging/FileAppender.scala#L79], but leave the ""inputStream"" open., which is the pipe's output stream. ;;;","08/Jul/21 09:31;apachespark;User 'jhu-chang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33263;;;","08/Jul/21 09:32;apachespark;User 'jhu-chang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33263;;;","21/Jul/21 02:27;srowen;Resolved by https://github.com/apache/spark/pull/33263;;;",,,,,,,,,,,,,,,
A foldable expression could not be replaced by an AttributeReference,SPARK-35014,13370732,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,buyingyi,buyingyi,buyingyi,09/Apr/21 19:37,13/Apr/21 12:36,13/Jul/23 08:47,13/Apr/21 12:36,3.0.0,,,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,Optimizer,,,,0,,,"The PhysicalAggregation pattern can replace a foldable expression with an AttributeReference. This would subsequently fail the checkInputDataTypes() call for certain expressions, e.g., RegExpReplace – `pos` cannot be an AttributeReference.",,apachespark,buyingyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 09 19:43:25 UTC 2021,,,,,,,,,,"0|z0pqaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/21 19:42;apachespark;User 'sigmod' has created a pull request for this issue:
https://github.com/apache/spark/pull/32113;;;","09/Apr/21 19:43;apachespark;User 'sigmod' has created a pull request for this issue:
https://github.com/apache/spark/pull/32113;;;",,,,,,,,,,,,,,,,,,
False active executor in UI that caused by BlockManager reregistration,SPARK-35011,13370715,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,sumeet.gajjar,sumeet.gajjar,09/Apr/21 17:58,02/Dec/22 08:19,13/Jul/23 08:47,12/Nov/21 00:19,3.1.1,3.2.0,,,,,,,,,,,,,,3.3.0,,,,Spark Core,,,,0,BlockManager,core,"*Note:* This is a follow-up on SPARK-34949, even after the heartbeat fix, driver reports dead executors as alive.

*Problem:*

I was testing Dynamic Allocation on K8s with about 300 executors. While doing so, when the executors were torn down due to ""spark.dynamicAllocation.executorIdleTimeout"", I noticed all the executor pods being removed from K8s, however, under the ""Executors"" tab in SparkUI, I could see some executors listed as alive. 

[spark.sparkContext.statusTracker.getExecutorInfos.length|https://github.com/apache/spark/blob/65da9287bc5112564836a555cd2967fc6b05856f/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala#L100] also returned a value greater than 1. 

 

*Cause:*
 * ""CoarseGrainedSchedulerBackend"" issues async ""StopExecutor"" on executorEndpoint
 * ""CoarseGrainedSchedulerBackend"" removes that executor from Driver's internal data structures and publishes ""SparkListenerExecutorRemoved"" on the ""listenerBus"".
 * Executor has still not processed ""StopExecutor"" from the Driver
 * Driver receives heartbeat from the Executor, since it cannot find the ""executorId"" in its data structures, it responds with ""HeartbeatResponse(reregisterBlockManager = true)""
 * ""BlockManager"" on the Executor reregisters with the ""BlockManagerMaster"" and ""SparkListenerBlockManagerAdded"" is published on the ""listenerBus""
 * Executor starts processing the ""StopExecutor"" and exits
 * ""AppStatusListener"" picks the ""SparkListenerBlockManagerAdded"" event and updates ""AppStatusStore""
 * ""statusTracker.getExecutorInfos"" refers ""AppStatusStore"" to get the list of executors which returns the dead executor as alive.

 

*Proposed Solution:*

Maintain a Cache of recently removed executors on Driver. During the registration in BlockManagerMasterEndpoint if the BlockManager belongs to a recently removed executor, return None indicating the registration is ignored since the executor will be shutting down soon.

On BlockManagerHeartbeat, if the BlockManager belongs to a recently removed executor, return true indicating the driver knows about it, thereby preventing reregisteration.",,apachespark,dongjoon,Ngone51,sumeet.gajjar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-41360,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 12 00:19:27 UTC 2021,,,,,,,,,,"0|z0pq74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/21 17:59;sumeet.gajjar;I am working on this.;;;","11/Apr/21 02:38;apachespark;User 'sumeetgajjar' has created a pull request for this issue:
https://github.com/apache/spark/pull/32114;;;","11/Apr/21 02:39;apachespark;User 'sumeetgajjar' has created a pull request for this issue:
https://github.com/apache/spark/pull/32114;;;","11/Apr/21 07:43;sumeet.gajjar;[~holden], [~dongjoon], [~attilapiros] could you please take a look at this?;;;","03/Jun/21 03:20;Ngone51;Issue resolved by [https://github.com/apache/spark/pull/32114];;;","17/Aug/21 19:31;apachespark;User 'sumeetgajjar' has created a pull request for this issue:
https://github.com/apache/spark/pull/33771;;;","17/Aug/21 19:31;apachespark;User 'sumeetgajjar' has created a pull request for this issue:
https://github.com/apache/spark/pull/33771;;;","19/Aug/21 02:26;apachespark;User 'sumeetgajjar' has created a pull request for this issue:
https://github.com/apache/spark/pull/33782;;;","19/Aug/21 02:27;apachespark;User 'sumeetgajjar' has created a pull request for this issue:
https://github.com/apache/spark/pull/33782;;;","09/Sep/21 04:50;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/33942;;;","09/Sep/21 04:51;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/33942;;;","10/Sep/21 03:28;dongjoon;This is reverted from master branch via https://github.com/apache/spark/pull/33942 for Apache Spark 3.3.0.;;;","10/Sep/21 12:44;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/33959;;;","10/Sep/21 12:49;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/33960;;;","10/Sep/21 12:50;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/33960;;;","10/Sep/21 12:54;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/33961;;;","10/Sep/21 16:34;dongjoon;This is reverted from all branches.;;;","09/Nov/21 15:51;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/34536;;;","12/Nov/21 00:19;dongjoon;Issue resolved by pull request 34536
[https://github.com/apache/spark/pull/34536];;;",
Avoid creating multiple Monitor threads for reused python workers for the same TaskContext,SPARK-35009,13370695,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,09/Apr/21 15:50,29/Apr/21 16:39,13/Jul/23 08:47,29/Apr/21 16:39,3.0.3,3.1.0,3.1.1,3.1.2,3.2.0,,,,,,,,,,,3.2.0,,,,PySpark,,,,0,,,"Currently this code will stop because of the high number of created threads:

{noformat}
    import pyspark

    conf=pyspark.SparkConf().setMaster(""local[64]"").setAppName(""Test1"")
    sc=pyspark.SparkContext.getOrCreate(conf)
    rows=70000
    data=list(range(rows))
    rdd=sc.parallelize(data,rows)
    assert rdd.getNumPartitions()==rows
    rdd0=rdd.filter(lambda x:False)
    assert rdd0.getNumPartitions()==rows
    rdd00=rdd0.coalesce(1)
    data=rdd00.collect()
    assert data==[]
{noformat}


The error is:


{noformat}
1/04/08 12:12:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/04/08 12:12:29 WARN TaskSetManager: Stage 0 contains a task of very large size (4732 KiB). The maximum recommended task size is 1000 KiB.
[Stage 0:>                                                          (0 + 1) / 1][423.190s][warning][os,thread] Attempt to protect stack guard pages failed (0x00007f43d23ff000-0x00007f43d2403000).
[423.190s][warning][os,thread] Attempt to deallocate stack guard pages failed.
OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f43d300b000, 16384, 0) failed; error='Not enough space' (errno=12)
[423.231s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 16384 bytes for committing reserved memory.
# An error report file with more information is saved as:
# /home/ubuntu/PycharmProjects/<projekt-dir>/tests/hs_err_pid17755.log
[thread 17966 also had an error]
OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f4b7bd81000, 262144, 0) failed; error='Not enough space' (errno=12)
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1207, in send_command
    raise Py4JNetworkError(""Answer from Java side is empty"")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1033, in send_command
    response = connection.send_command(command)
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1211, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while receiving
ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:42439)
Traceback (most recent call last):
  File ""/opt/spark/python/pyspark/rdd.py"", line 889, in collect
    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1304, in __call__
    return_value = get_return_value(
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py"", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 977, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1115, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
Traceback (most recent call last):
  File ""/opt/spark/python/pyspark/rdd.py"", line 889, in collect
    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1304, in __call__
    return_value = get_return_value(
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py"", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 977, in _get_connection
    connection = self.deque.pop()
IndexError: pop from an empty deque
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1115, in start
    self.socket.connect((self.address, self.port))
ConnectionRefusedError: [Errno 111] Connection refused
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""<input>"", line 3, in <module>
  File ""/opt/pycharm-2020.2.3/plugins/python/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""/opt/pycharm-2020.2.3/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/home/ubuntu/PycharmProjects/SPO_as_a_Service/tests/test_modeling_paf.py"", line 992, in <module>
    test_70000()
  File ""/home/ubuntu/PycharmProjects/SPO_as_a_Service/tests/test_modeling_paf.py"", line 974, in test_70000
    data=rdd00.collect()
  File ""/opt/spark/python/pyspark/rdd.py"", line 889, in collect
    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File ""/opt/spark/python/pyspark/traceback_utils.py"", line 78, in __exit__
    self._context._jsc.setCallSite(None)
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1303, in __call__
    answer = self.gateway_client.send_command(command)
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1031, in send_command
    connection = self._get_connection()
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 979, in _get_connection
    connection = self._create_connection()
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 985, in _create_connection
    connection.start()
  File ""/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1127, in start
    raise Py4JNetworkError(msg, e)
py4j.protocol.Py4JNetworkError: An error occurred while trying to connect to the Java server (127.0.0.1:42439)
{noformat}

",,apachespark,attilapiros,roczei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 29 16:39:24 UTC 2021,,,,,,,,,,"0|z0pq2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/21 15:51;attilapiros;I am working on this;;;","14/Apr/21 14:54;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/32169;;;","29/Apr/21 16:39;attilapiros;Issue resolved by pull request 32169
[https://github.com/apache/spark/pull/32169];;;",,,,,,,,,,,,,,,,,
"Fix Incorrect assertion of ""master/worker web ui available behind front-end reverseProxy"" in MasterSuite",SPARK-35004,13370585,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,09/Apr/21 07:53,09/Apr/21 13:20,13/Jul/23 08:47,09/Apr/21 13:19,3.1.1,3.2.0,,,,,,,,,,,,,,3.1.2,3.2.0,,,Tests,,,,0,,,"[https://github.com/apache/spark/blob/bfba7fadd2e65c853971fb2983bdea1c52d1ed7f/core/src/test/scala/org/apache/spark/deploy/master/MasterSuite.scala#L425]
{code:java}
// with LocalCluster, we have masters and workers in the same JVM, each overwriting
// system property spark.ui.proxyBase.
// so we need to manage this property explicitly for test
System.getProperty(""spark.ui.proxyBase"") should startWith  (s""$reverseProxyUrl/proxy/worker-"") // Line 425
{code}
Line 425 in MasterSuite is considered as unused expression,  If we merge lines 424 and 425 into one：
{code:java}
System.getProperty(""spark.ui.proxyBase"") should startWith (s""$reverseProxyUrl/proxy/worker-"")
{code}
this assertion will fail:
{code:java}
- master/worker web ui available behind front-end reverseProxy *** FAILED ***
  The code passed to eventually never returned normally. Attempted 45 times over 5.091914027 seconds. Last failure message: ""http://proxyhost:8080/path/to/spark"" did not start with substring ""http://proxyhost:8080/path/to/spark/proxy/worker-"". (MasterSuite.scala:405)

{code}
 

 

 

 ",,apachespark,Gengliang.Wang,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 09 13:19:47 UTC 2021,,,,,,,,,,"0|z0ppe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/21 08:14;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32105;;;","09/Apr/21 13:19;Gengliang.Wang;Issue resolved by pull request 32105
[https://github.com/apache/spark/pull/32105];;;",,,,,,,,,,,,,,,,,,
Fix git error when pushing the tag after release script succeeds,SPARK-34994,13370458,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,08/Apr/21 18:29,08/Apr/21 23:26,13/Jul/23 08:47,08/Apr/21 23:26,2.4.8,,,,,,,,,,,,,,,2.4.8,,,,Build,,,,0,,,"When I ran release script for cutting 2.4.8 RC1, either in dry-run or normal run at the last step ""push the tag after success"", I encounter the following error:

{code}
fatal: Not a git repository (or any parent up to mount parent ....)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
{code}
",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 08 23:26:47 UTC 2021,,,,,,,,,,"0|z0pom0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/21 18:33;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32100;;;","08/Apr/21 18:33;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32100;;;","08/Apr/21 23:26;viirya;Issue resolved by pull request 32100
[https://github.com/apache/spark/pull/32100];;;",,,,,,,,,,,,,,,,,
Upgrade Jetty for CVE-2021-28165,SPARK-34988,13370318,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,08/Apr/21 08:36,08/Apr/21 15:43,13/Jul/23 08:47,08/Apr/21 10:59,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,Spark Core,,,,0,,,CVE-2021-28165 affects the version of Jetty that Spark uses and it seems to be a little bit serious.,,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 08 12:04:23 UTC 2021,,,,,,,,,,"0|z0pnqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/21 09:03;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32091;;;","08/Apr/21 12:03;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32093;;;","08/Apr/21 12:04;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32095;;;","08/Apr/21 12:04;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32094;;;",,,,,,,,,,,,,,,,
LIST FILES/JARS/ARCHIVES cannot handle multiple arguments properly when at least one path is quoted.,SPARK-34977,13369948,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,07/Apr/21 05:34,14/Apr/21 01:36,13/Jul/23 08:47,14/Apr/21 01:36,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"`LIST FILES/JARS/ARCHIVES path1 path2 ...` cannot list all paths if at least one path is quoted.
An example here.

{code}
ADD FILE /tmp/test1;
ADD FILE /tmp/test2;

LIST FILES /tmp/test1 /tmp/test2;
file:/tmp/test1
file:/tmp/test2

LIST FILES /tmp/test1 ""/tmp/test2"";
file:/tmp/test2
{code}

In this example, the second `LIST FILES` doesn't show `file:/tmp/test1`.
",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 01:36:03 UTC 2021,,,,,,,,,,"0|z0plgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/21 05:42;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32074;;;","14/Apr/21 01:36;sarutak;This issue is resolved in https://github.com/apache/spark/pull/32074 .;;;",,,,,,,,,,,,,,,,,,
Failed to excute python/run-tests-with-coverage cmd,SPARK-34968,13369800,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yikunkero,yikunkero,yikunkero,06/Apr/21 12:57,06/Apr/21 22:21,13/Jul/23 08:47,06/Apr/21 22:21,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Tests,,,,0,,,"~/spark$ python/run-tests-with-coverage --testnames pyspark.sql.tests.test_arrow --python-executables=python

Running PySpark tests. Output is in /home/spark/spark/python/unit-tests.log

*rm: missing operand*
*Try 'rm --help' for more information.*


~/spark$ echo $?
123

 

We should add ""-fr"" to ""rm""

 ",,apachespark,dongjoon,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 06 22:21:14 UTC 2021,,,,,,,,,,"0|z0pkjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/21 13:00;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32064;;;","06/Apr/21 22:21;dongjoon;Issue resolved by pull request 32064
[https://github.com/apache/spark/pull/32064];;;",,,,,,,,,,,,,,,,,,
Remove .sbtopts that duplicately sets the default memory,SPARK-34965,13369766,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,06/Apr/21 11:11,12/Dec/22 17:51,13/Jul/23 08:47,06/Apr/21 22:17,3.1.1,3.2.0,,,,,,,,,,,,,,3.1.2,3.2.0,,,Build,,,,0,,,See https://github.com/apache/spark/pull/29286#discussion_r607754500,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 06 22:17:26 UTC 2021,,,,,,,,,,"0|z0pkc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/21 11:20;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/32062;;;","06/Apr/21 22:17;dongjoon;Issue resolved by pull request 32062
[https://github.com/apache/spark/pull/32062];;;",,,,,,,,,,,,,,,,,,
Nested column pruning fails to extract case-insensitive struct field from array,SPARK-34963,13369701,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,06/Apr/21 06:40,10/Apr/21 09:25,13/Jul/23 08:47,09/Apr/21 19:14,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,SQL,,,,0,,,"Under case-insensitive mode, nested column pruning rule cannot correctly push down extractor of a struct field of an array of struct, e.g.,

{code}
val query = spark.table(""contacts"").select(""friends.First"", ""friends.MiDDle"")
{code}",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 09 19:37:58 UTC 2021,,,,,,,,,,"0|z0pjxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/21 06:46;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32059;;;","06/Apr/21 06:47;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32059;;;","09/Apr/21 19:36;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32112;;;","09/Apr/21 19:37;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32112;;;",,,,,,,,,,,,,,,,
ADD JAR command cannot add jar files which contains whitespaces in the path,SPARK-34955,13369540,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,05/Apr/21 06:02,07/Apr/21 18:43,13/Jul/23 08:47,07/Apr/21 18:43,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"ADD JAR command cannot add jar files which contains white spaces in the path.

If we have `/some/path/test file.jar` and execute the following command:

{code}
ADD JAR ""/some/path/test file.jar"";
{code}
The following exception is thrown.
{code}
21/04/05 10:40:38 ERROR SparkSQLDriver: Failed in [add jar ""/some/path/test file.jar""]
java.lang.IllegalArgumentException: Illegal character in path at index 9: /some/path/test file.jar
	at java.net.URI.create(URI.java:852)
	at org.apache.spark.sql.hive.HiveSessionResourceLoader.addJar(HiveSessionStateBuilder.scala:129)
	at org.apache.spark.sql.execution.command.AddJarCommand.run(resources.scala:34)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
{code}

This is because `HiveSessionStateBuilder` and `SessionStateBuilder` don't check whether the form of the path is URI or plain path and it always regards the path as URI form.
Whitespces should be encoded to `%20` so `/some/path/test file.jar` is rejected.
We can resolve this part by checking whether the given path is URI form or not.

Unfortunatelly, if we fix this part, another problem occurs.
When we execute `ADD JAR` command, Hive's `ADD JAR` command is executed in `HiveClientImpl.addJar` and `AddResourceProcessor.run` is transitively invoked.
In `AddResourceProcessor.run`, the command line is just split by `\\s+` and the path is also split into `/some/path/test` and `file.jar` and passed to `ss.add_resources`.
https://github.com/apache/hive/blob/f1e87137034e4ecbe39a859d4ef44319800016d7/ql/src/java/org/apache/hadoop/hive/ql/processors/AddResourceProcessor.java#L56-L75
So, the command still fails.

Even if we convert the form of the path to URI like `file:/some/path/test%20file.jar` and execute the following command:
{code}
ADD JAR ""file:/some/path/test%20file"";
{code}
The following exception is thrown.
{code}
21/04/05 10:40:53 ERROR SessionState: file:/some/path/test%20file.jar does not exist
java.lang.IllegalArgumentException: file:/some/path/test%20file.jar does not exist
	at org.apache.hadoop.hive.ql.session.SessionState.validateFiles(SessionState.java:1168)
	at org.apache.hadoop.hive.ql.session.SessionState$ResourceType.preHook(SessionState.java:1289)
	at org.apache.hadoop.hive.ql.session.SessionState$ResourceType$1.preHook(SessionState.java:1278)
	at org.apache.hadoop.hive.ql.session.SessionState.add_resources(SessionState.java:1378)
	at org.apache.hadoop.hive.ql.session.SessionState.add_resources(SessionState.java:1336)
	at org.apache.hadoop.hive.ql.processors.AddResourceProcessor.run(AddResourceProcessor.java:74)
{code}
The reason is `Utilities.realFile` invoked in `SessionState.validateFiles` returns `null` as the result of `fs.exists(path)` is `false`.
https://github.com/apache/hive/blob/f1e87137034e4ecbe39a859d4ef44319800016d7/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java#L1052-L1064

`fs.exists` checks the existence of the given path by comparing the string representation of Hadoop's `Path`.
The string representation of `Path` is similar to URI but it's actually different.
`Path` doesn't encode the given path.
For example, the URI form of `/some/path/jar file.jar` is `file:/some/path/jar%20file.jar` but the `Path` form of it is `file:/some/path/jar file.jar`. So `fs.exists` returns false.",,apachespark,dongjoon,sarutak,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 07 18:43:23 UTC 2021,,,,,,,,,,"0|z0piy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/21 06:21;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32052;;;","05/Apr/21 06:21;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32052;;;","07/Apr/21 18:43;dongjoon;Issue resolved by pull request 32052
[https://github.com/apache/spark/pull/32052];;;",,,,,,,,,,,,,,,,,
Executor.reportHeartBeat reregisters blockManager even when Executor is shutting down,SPARK-34949,13369390,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sumeet.gajjar,sumeet.gajjar,sumeet.gajjar,03/Apr/21 05:49,18/Aug/21 23:24,13/Jul/23 08:47,05/Apr/21 22:34,3.1.1,3.2.0,,,,,,,,,,,,,,3.0.4,3.1.2,3.2.0,,Spark Core,,,,0,Executor,heartbeat,"*Problem:*

I was testing Dynamic Allocation on K8s with about 300 executors. While doing so, when the executors were torn down due to ""spark.dynamicAllocation.executorIdleTimeout"", I noticed all the executor pods being removed from K8s, however, under the ""Executors"" tab in SparkUI, I could see some executors listed as alive. 

[spark.sparkContext.statusTracker.getExecutorInfos.length|https://github.com/apache/spark/blob/65da9287bc5112564836a555cd2967fc6b05856f/core/src/main/scala/org/apache/spark/SparkStatusTracker.scala#L100] also returned a value greater than 1. 

 

*Cause:*
 * ""CoarseGrainedSchedulerBackend"" issues RemoveExecutor on a ""executorEndpoint"" and publishes ""SparkListenerExecutorRemoved"" on the ""listenerBus""
 * ""CoarseGrainedExecutorBackend"" starts the executor shutdown
 * ""HeartbeatReceiver"" picks the ""SparkListenerExecutorRemoved"" event and removes the executor from ""executorLastSeen""
 * In the meantime, the executor reports a Heartbeat. Now ""HeartbeatReceiver"" cannot find the ""executorId"" in ""executorLastSeen"" and hence responds with ""HeartbeatResponse(reregisterBlockManager = true)""
 * The Executor now calls ""env.blockManager.reregister()"" and reregisters itself thus creating inconsistency

 

*Proposed Solution:*

The ""reportHeartBeat"" method is not aware of the fact that Executor is shutting down, it should check ""executorShutdown"" before reregistering. ",Resource Manager: K8s,apachespark,mridulm80,sumeet.gajjar,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 17 19:24:01 UTC 2021,,,,,,,,,,"0|z0pi0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/21 05:49;sumeet.gajjar;I am working on this.;;;","03/Apr/21 06:50;apachespark;User 'sumeetgajjar' has created a pull request for this issue:
https://github.com/apache/spark/pull/32043;;;","03/Apr/21 06:51;apachespark;User 'sumeetgajjar' has created a pull request for this issue:
https://github.com/apache/spark/pull/32043;;;","05/Apr/21 22:34;mridulm80;Issue resolved by pull request 32043
[https://github.com/apache/spark/pull/32043];;;","17/Aug/21 19:24;apachespark;User 'sumeetgajjar' has created a pull request for this issue:
https://github.com/apache/spark/pull/33770;;;",,,,,,,,,,,,,,,
Add ownerReference to executor configmap to fix leakages,SPARK-34948,13369383,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,03/Apr/21 03:10,03/Apr/21 07:05,13/Jul/23 08:47,03/Apr/21 07:05,3.1.1,3.2.0,,,,,,,,,,,,,,3.1.2,3.2.0,,,Kubernetes,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30985,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 03 07:05:16 UTC 2021,,,,,,,,,,"0|z0phz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/21 03:15;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32042;;;","03/Apr/21 03:16;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32042;;;","03/Apr/21 07:05;dongjoon;Issue resolved by pull request 32042
[https://github.com/apache/spark/pull/32042];;;",,,,,,,,,,,,,,,,,
Throw fetch failure exception when unable to deserialize broadcasted map statuses,SPARK-34939,13369237,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,02/Apr/21 04:26,04/Feb/22 18:52,13/Jul/23 08:47,04/Apr/21 01:38,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,Spark Core,,,,0,,,"One customer encountered application error. From the log, it is caused by accessing non-existing broadcasted value. The broadcasted value is map statuses. There is a race-condition.

After map statuses are broadcasted and the executors obtain serialized broadcasted map statuses. If any fetch failure happens after, Spark scheduler invalidates cached map statuses and destroy broadcasted value of the map statuses. Then any executor trying to deserialize serialized broadcasted map statuses and access broadcasted value, {{IOException}} will be thrown. Currently we don't catch it in {{MapOutputTrackerWorker}} and above exception will fail the application.

Normally we should throw a fetch failure exception for such case and let Spark scheduler handle this.",,apachespark,dongjoon,viirya,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30849,,,,SPARK-38101,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 04 01:38:55 UTC 2021,,,,,,,,,,"0|z0ph2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/21 04:31;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32033;;;","02/Apr/21 04:31;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32033;;;","03/Apr/21 18:51;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32045;;;","03/Apr/21 18:51;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/32045;;;","04/Apr/21 01:38;dongjoon;Issue resolved by pull request 32033
[https://github.com/apache/spark/pull/32033];;;",,,,,,,,,,,,,,,
Recover the interval case in the benchmark of ExtractBenchmark,SPARK-34938,13369224,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,gurwls223,,02/Apr/21 01:52,12/Dec/22 18:11,13/Jul/23 08:47,02/Apr/21 06:46,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Tests,,,,0,,,"{code}
Running benchmark: Invoke extract for interval
  Running case: cast to interval
21/04/02 10:40:01 INFO BlockManagerInfo: Removed broadcast_349_piece0 on 192.168.35.219:55076 in memory (size: 7.3 KiB, free: 434.4 MiB)
21/04/02 10:40:01 INFO BlockManagerInfo: Removed broadcast_348_piece0 on 192.168.35.219:55076 in memory (size: 7.3 KiB, free: 434.4 MiB)
21/04/02 10:40:01 INFO BlockManagerInfo: Removed broadcast_345_piece0 on 192.168.35.219:55076 in memory (size: 7.3 KiB, free: 434.4 MiB)
21/04/02 10:40:01 INFO BlockManagerInfo: Removed broadcast_346_piece0 on 192.168.35.219:55076 in memory (size: 7.3 KiB, free: 434.4 MiB)
21/04/02 10:40:01 INFO BlockManagerInfo: Removed broadcast_347_piece0 on 192.168.35.219:55076 in memory (size: 7.3 KiB, free: 434.4 MiB)
Exception in thread ""main"" org.apache.spark.sql.AnalysisException: cannot resolve 'subtractdates(CAST(timestamp_seconds(id) AS DATE), DATE '0001-01-01') + subtracttimestamps(timestamp_seconds(id), TIMESTAMP '1000-01-01 01:02:03.123456')' due to data type mismatch: argument 1 requires timestamp type, however, 'subtractdates(CAST(timestamp_seconds(id) AS DATE), DATE '0001-01-01')' is of day-time interval type.; line 1 pos 0;
'Project [unresolvedalias(cast(subtractdates(cast(timestamp_seconds(id#1400L) as date), 0001-01-01, false) + subtracttimestamps(timestamp_seconds(id#1400L), 1000-01-01 01:02:03.123456) as day-time interval), Some(org.apache.spark.sql.Column$$Lambda$1282/0x0000000800bd5040@5bff7f13))]
+- Range (1262304000, 1272304000, step=1, splits=Some(1))
{code}

Benchmark case is broken.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 02 06:46:21 UTC 2021,,,,,,,,,,"0|z0pgzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/21 06:15;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/32035;;;","02/Apr/21 06:46;gurwls223;Issue resolved by pull request 32035
[https://github.com/apache/spark/pull/32035];;;",,,,,,,,,,,,,,,,,,
Race condition while registering source in MetricsSystem ,SPARK-34934,13369059,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,BOOTMGR,BOOTMGR,BOOTMGR,01/Apr/21 09:12,05/Apr/21 13:42,13/Jul/23 08:47,05/Apr/21 13:42,2.3.2,3.1.1,,,,,,,,,,,,,,3.2.0,,,,Spark Core,,,,0,,,"{{MetricsSystem}} manages {{mutable.ArrayBuffer}} of metric sources. {{registerSource}} and {{removeSource}} methods are provided to add/remove new source from Metric system. Both these methods are not synchronised. Also, underlying {{mutable.ArrayBuffer}} not being thread safe, unexpected behaviours are possible if called concurrently.

Some background:
We have created one custom RpcEndPoint which receives messages from executors and create new metrics by registering custom sources. These messages are processed concurrently on driver side causing this issue. 

Also, this will go unnoticed as {{Inbox}} will ignore these exceptions.

We found this issue in Spark 2.3.2, but it should be present in all later versions.

For ex, we got below exception due to race condition
{noformat}
java.lang.ArrayIndexOutOfBoundsException
	at scala.collection.mutable.ResizableArray$class.ensureSize(ResizableArray.scala:104)
	at scala.collection.mutable.ArrayBuffer.ensureSize(ArrayBuffer.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$eq(ArrayBuffer.scala:84)
	at org.apache.spark.metrics.MetricsSystem.registerSource(MetricsSystem.scala:157)

	* some closure *

	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:221)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745){noformat}
While shutting down
{noformat}
Exception in thread ""stream execution thread for [id = 9a36a08a-f1be-4ad8-b1dd-093f0b53d37d, runId = d668a19c-aced-45c4-963c-c0b93411d1a4]"" java.lang.ArrayIndexOutOfBoundsException: 32
	at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:44)
	at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:48)
	at scala.collection.IndexedSeqOptimized$class.segmentLength(IndexedSeqOptimized.scala:195)
	at scala.collection.mutable.ArrayBuffer.segmentLength(ArrayBuffer.scala:48)
	at scala.collection.IndexedSeqOptimized$class.indexWhere(IndexedSeqOptimized.scala:204)
	at scala.collection.mutable.ArrayBuffer.indexWhere(ArrayBuffer.scala:48)
	at scala.collection.GenSeqLike$class.indexOf(GenSeqLike.scala:145)
	at scala.collection.AbstractSeq.indexOf(Seq.scala:41)
	at scala.collection.GenSeqLike$class.indexOf(GenSeqLike.scala:129)
	at scala.collection.AbstractSeq.indexOf(Seq.scala:41)
	at scala.collection.mutable.BufferLike$class.$minus$eq(BufferLike.scala:127)
	at scala.collection.mutable.AbstractBuffer.$minus$eq(Buffer.scala:49)
	at org.apache.spark.metrics.MetricsSystem.removeSource(MetricsSystem.scala:167)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runStream$2.apply(StreamExecution.scala:324)
	at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:308)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:189){noformat}",,apachespark,BOOTMGR,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 05 13:42:51 UTC 2021,,,,,,,,,,"0|z0pfz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/21 09:25;apachespark;User 'BOOTMGR' has created a pull request for this issue:
https://github.com/apache/spark/pull/32024;;;","05/Apr/21 13:42;srowen;Issue resolved by pull request 32024
[https://github.com/apache/spark/pull/32024];;;",,,,,,,,,,,,,,,,,,
"Remove the description that || and && can be used as logical operators from the document.",SPARK-34933,13369051,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,01/Apr/21 08:27,15/Nov/21 18:26,13/Jul/23 08:47,01/Apr/21 22:16,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,Documentation,SQL,,,0,,,"At the Compatibility with Apache Hive section in the migration guide, it describes that || and && can be used as logical operators.
But, in fact, they cannot be used as described.
AFAIK, Hive also doesn't support && and || as logical operators.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 01 22:16:12 UTC 2021,,,,,,,,,,"0|z0pfxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/21 08:35;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32023;;;","01/Apr/21 08:36;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32023;;;","01/Apr/21 22:16;srowen;Resolved by https://github.com/apache/spark/pull/32023;;;",,,,,,,,,,,,,,,,,
PartitionUtils.getPathFragment should handle null value,SPARK-34926,13368990,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhu,angerszhuuu,angerszhuuu,01/Apr/21 02:31,12/Apr/21 13:53,13/Jul/23 08:47,02/Apr/21 07:26,3.2.0,,,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,SQL,,,,0,,,"We support partition value as null, 
PartitionUtils.getPathFragment  should support this too.",,angerszhuuu,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 12 08:34:43 UTC 2021,,,,,,,,,,"0|z0pfjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/21 02:44;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32018;;;","01/Apr/21 02:44;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32018;;;","02/Apr/21 07:26;maxgekk;Issue resolved by pull request 32018
[https://github.com/apache/spark/pull/32018];;;","12/Apr/21 08:26;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32127;;;","12/Apr/21 08:34;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32128;;;","12/Apr/21 08:34;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32128;;;",,,,,,,,,,,,,,
conv() does not convert negative inputs to unsigned correctly,SPARK-34909,13368700,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tarmstrong,tarmstrong,tarmstrong,30/Mar/21 17:38,01/Jun/21 23:00,13/Jul/23 08:47,31/Mar/21 05:02,3.1.0,,,,,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,SQL,,,,0,correctness,,"{noformat}
scala> spark.sql(""select conv('-10', 11, 7)"").show(20, 150)
+-----------------------+
|       conv(-10, 11, 7)|
+-----------------------+
|4501202152252313413456|
+-----------------------+
scala> spark.sql(""select hex(conv('-10', 11, 7))"").show(20, 150)
+----------------------------------------------+
|                         hex(conv(-10, 11, 7))|
+----------------------------------------------+
|3435303132303231353232353233313334313334353600|
+----------------------------------------------+
{noformat}

The correct result is 45012021522523134134555. The above output has an incorrect second-to-last digit (6 instead of 5) and the last digit is a non-printing character the null byte.

I tracked the bug down to NumberConverter.unsignedLongDiv returning incorrect results. I tried replacing with java.lang.Long.divideUnsigned and that fixed it.",,apachespark,cloud_fan,tarmstrong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 31 05:02:15 UTC 2021,,,,,,,,,,"0|z0pds0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/21 17:59;apachespark;User 'timarmstrong' has created a pull request for this issue:
https://github.com/apache/spark/pull/32006;;;","30/Mar/21 18:00;apachespark;User 'timarmstrong' has created a pull request for this issue:
https://github.com/apache/spark/pull/32006;;;","31/Mar/21 05:02;cloud_fan;Issue resolved by pull request 32006
[https://github.com/apache/spark/pull/32006];;;",,,,,,,,,,,,,,,,,
Some `spark-submit`  commands used to run benchmarks in the user's guide is wrong,SPARK-34900,13368384,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,LuciferYang,LuciferYang,29/Mar/21 14:26,12/Dec/22 18:10,13/Jul/23 08:47,30/Mar/21 02:58,3.2.0,,,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,Tests,,,,0,,,"For example, the guide for running JoinBenchmark as follows:

 
{code:java}
/**
 * Benchmark to measure performance for joins.
 * To run this benchmark:
 * {{{
 *   1. without sbt:
 *      bin/spark-submit --class <this class> --jars <spark core test jar> <spark sql test jar>
 *   2. build/sbt ""sql/test:runMain <this class>""
 *   3. generate result:
 *      SPARK_GENERATE_BENCHMARK_FILES=1 build/sbt ""sql/test:runMain <this class>""
 *      Results will be written to ""benchmarks/JoinBenchmark-results.txt"".
 * }}}
 */
object JoinBenchmark extends SqlBasedBenchmark {
{code}
 

 

but if we run JoinBenchmark with commnad

 
{code:java}
bin/spark-submit --class org.apache.spark.sql.execution.benchmark.JoinBenchmark --jars spark-core_2.12-3.2.0-SNAPSHOT-tests.jar spark-sql_2.12-3.2.0-SNAPSHOT-tests.jar 
{code}
 

The following exception will be thrown：

 
{code:java}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/sql/catalyst/plans/SQLHelper
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:756)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:468)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:74)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:369){code}
 

because SqlBasedBenchmark trait extends BenchmarkBase and SQLHelper, SQLHelper def in spark-catalyst-tests.jar.

 ",,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 30 11:11:11 UTC 2021,,,,,,,,,,"0|z0pbts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/21 14:46;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31995;;;","30/Mar/21 02:58;gurwls223;Fixed in https://github.com/apache/spark/pull/31995;;;","30/Mar/21 11:06;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32002;;;","30/Mar/21 11:10;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32003;;;","30/Mar/21 11:11;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/32003;;;",,,,,,,,,,,,,,,
Support reconcile schemas based on index after nested column pruning,SPARK-34897,13368326,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,29/Mar/21 08:51,24/Apr/21 12:20,13/Jul/23 08:47,24/Apr/21 12:18,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,SQL,,,,0,,,"How to reproduce this issue:
{code:scala}
spark.sql(
  """"""
    |CREATE TABLE `t1` (
    |  `_col0` INT,
    |  `_col1` STRING,
    |  `_col2` STRUCT<`c1`: STRING, `c2`: STRING, `c3`: STRING, `c4`: BIGINT>,
    |  `_col3` STRING)
    |USING orc
    |PARTITIONED BY (_col3)
    |"""""".stripMargin)

spark.sql(""INSERT INTO `t1` values(1, '2', null, '2021-02-01')"")

spark.sql(""SELECT _col2.c1, _col0 FROM `t1` WHERE _col3 = '2021-02-01'"").show
{code}


Error message:
{noformat}
java.lang.AssertionError: assertion failed: The given data schema struct<_col0:int,_col2:struct<c1:string>> has less fields than the actual ORC physical schema, no idea which columns were dropped, fail to read. Try to disable 
	at scala.Predef$.assert(Predef.scala:223)
	at org.apache.spark.sql.execution.datasources.orc.OrcUtils$.requestedColumnIds(OrcUtils.scala:159)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.$anonfun$buildReaderWithPartitionValues$3(OrcFileFormat.scala:180)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2620)
	at org.apache.spark.sql.execution.datasources.orc.OrcFileFormat.$anonfun$buildReaderWithPartitionValues$1(OrcFileFormat.scala:178)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:117)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:165)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:94)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:756)
{noformat}

",,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,SPARK-35010,SPARK-35191,SPARK-35190,,,,,HIVE-4243,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 24 12:20:17 UTC 2021,,,,,,,,,,"0|z0pbgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/21 09:16;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/31993;;;","29/Mar/21 09:17;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/31993;;;","21/Apr/21 23:57;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/32279;;;","21/Apr/21 23:57;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/32279;;;","23/Apr/21 08:26;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/32310;;;","24/Apr/21 12:20;yumwang;Issue resolved by pull request 31993
https://github.com/apache/spark/pull/31993;;;",,,,,,,,,,,,,,
RewriteDistinctAggregates can cause a bug if the aggregator does not ignore NULLs,SPARK-34882,13368229,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tanelk,tanelk,tanelk,28/Mar/21 19:09,31/Mar/21 22:44,13/Jul/23 08:47,31/Mar/21 22:44,2.4.8,3.0.3,3.1.2,3.2.0,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,correctness,,"{code:title=group-by.sql}
SELECT
    first(DISTINCT a), last(DISTINCT a),
    first(a), last(a),
    first(DISTINCT b), last(DISTINCT b),
    first(b), last(b)
FROM testData WHERE a IS NOT NULL AND b IS NOT NULL;{code}
{code:title=group-by.sql.out}
-- !query schema
struct<first(DISTINCT a):int,last(DISTINCT a):int,first(a):int,last(a):int,first(DISTINCT b):int,last(DISTINCT b):int,first(b):int,last(b):int>
-- !query output
NULL	1	1	3	1	NULL	1	2
{code}

The results should not be NULL, because NULL inputs are filtered out.",,apachespark,maropu,tanelk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 31 22:44:02 UTC 2021,,,,,,,,,,"0|z0pavc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/21 20:12;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31983;;;","31/Mar/21 22:44;maropu;Resolved by https://github.com/apache/spark/pull/31983;;;",,,,,,,,,,,,,,,,,,
Non-nullable aggregates can return NULL in a correlated subquery,SPARK-34876,13367858,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tanelk,tanelk,tanelk,26/Mar/21 13:53,12/Dec/22 18:10,13/Jul/23 08:47,29/Mar/21 02:48,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,SQL,,,,0,,,"
Test case in scalar-subquery-select.sql:

{code:title=query}
SELECT t1a,
    (SELECT count(t2d) FROM t2 WHERE t2a = t1a) count_t2,
    (SELECT count_if(t2d > 0) FROM t2 WHERE t2a = t1a) count_if_t2,
    (SELECT approx_count_distinct(t2d) FROM t2 WHERE t2a = t1a) approx_count_distinct_t2,
    (SELECT collect_list(t2d) FROM t2 WHERE t2a = t1a) collect_list_t2,
    (SELECT collect_set(t2d) FROM t2 WHERE t2a = t1a) collect_set_t2,
    (SELECT hex(count_min_sketch(t2d, 0.5d, 0.5d, 1)) FROM t2 WHERE t2a = t1a) collect_set_t2
FROM t1;
{code}

{code:title=Result}
val1a	0	0	NULL	NULL	NULL	NULL
val1a	0	0	NULL	NULL	NULL	NULL
val1a	0	0	NULL	NULL	NULL	NULL
val1a	0	0	NULL	NULL	NULL	NULL
val1b	6	6	3	[19,119,319,19,19,19]	[19,119,319]	0000000100000000000000060000000100000004000000005D8D6AB90000000000000000000000000000000400000000000000010000000000000001
val1c	2	2	2	[219,19]	[219,19]	0000000100000000000000020000000100000004000000005D8D6AB90000000000000000000000000000000100000000000000000000000000000001
val1d	0	0	NULL	NULL	NULL	NULL
val1d	0	0	NULL	NULL	NULL	NULL
val1d	0	0	NULL	NULL	NULL	NULL
val1e	1	1	1	[19]	[19]	0000000100000000000000010000000100000004000000005D8D6AB90000000000000000000000000000000100000000000000000000000000000000
val1e	1	1	1	[19]	[19]	0000000100000000000000010000000100000004000000005D8D6AB90000000000000000000000000000000100000000000000000000000000000000
val1e	1	1	1	[19]	[19]	0000000100000000000000010000000100000004000000005D8D6AB90000000000000000000000000000000100000000000000000000000000000000
{code}",,apachespark,tanelk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 29 07:46:00 UTC 2021,,,,,,,,,,"0|z0p8m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/21 14:08;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31973;;;","26/Mar/21 14:08;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31973;;;","29/Mar/21 02:48;gurwls223;Issue resolved by pull request 31973
[https://github.com/apache/spark/pull/31973];;;","29/Mar/21 07:46;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31991;;;",,,,,,,,,,,,,,,,
Recover test reports for failed GA builds,SPARK-34874,13367801,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,26/Mar/21 08:41,12/Dec/22 18:10,13/Jul/23 08:47,26/Mar/21 09:13,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,Project Infra,,,,0,,,[https://github.com/dawidd6/action-download-artifact/commit/621becc6d7c440318382ce6f4cb776f27dd3fef3#r48726074] there was a behaviour change in the download artifact plugin and it disabled the test reporting in failed builds.,,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 26 09:13:30 UTC 2021,,,,,,,,,,"0|z0p89k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/21 08:50;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/31970;;;","26/Mar/21 09:13;gurwls223;Issue resolved by pull request 31970
[https://github.com/apache/spark/pull/31970];;;",,,,,,,,,,,,,,,,,,
quoteIfNeeded should quote a name which contains non-word characters,SPARK-34872,13367653,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,25/Mar/21 16:20,29/Mar/21 09:32,13/Jul/23 08:47,29/Mar/21 09:32,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,quoteIfNeeded quotes a name only when it contains . (dots) or ` (backticks) but the method should quote it if it contains non-word characters.,,apachespark,cloud_fan,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 29 09:32:15 UTC 2021,,,,,,,,,,"0|z0p7co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/21 16:29;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31964;;;","29/Mar/21 09:32;cloud_fan;Issue resolved by pull request 31964
[https://github.com/apache/spark/pull/31964];;;",,,,,,,,,,,,,,,,,,
Move checkpoint resolving logic to the rule ResolveWriteToStream,SPARK-34871,13367640,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,25/Mar/21 15:53,12/Dec/22 18:10,13/Jul/23 08:47,26/Mar/21 01:30,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Structured Streaming,,,,0,,,"After SPARK-34748, we have a rule ResolveWriteToStream for the analysis logic for the resolving logic of stream write plans. Based on it, we can further move the checkpoint location resolving work in the rule.",,apachespark,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 26 01:30:08 UTC 2021,,,,,,,,,,"0|z0p79s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/21 16:13;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/31963;;;","26/Mar/21 01:30;gurwls223;Issue resolved by pull request 31963
[https://github.com/apache/spark/pull/31963];;;",,,,,,,,,,,,,,,,,,
Vectorized parquet reader needs synchronization among pages for column index,SPARK-34859,13367411,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,csun,lxian2,lxian2,25/Mar/21 02:21,13/Aug/21 18:46,13/Jul/23 08:47,30/Jun/21 21:21,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,correctness,,"the current implementation has a problem. the pages returned by `readNextFilteredRowGroup` may not be aligned, some columns may have more rows than others.

Parquet is using `org.apache.parquet.column.impl.SynchronizingColumnReader` with `rowIndexes` to make sure that rows are aligned. 

Currently `VectorizedParquetRecordReader` doesn't have such synchronizing among pages from different columns. Using `readNextFilteredRowGroup` may result in incorrect result.

 
I have attache an example parquet file. This file is generated with `spark.range(0, 2000).map(i => (i.toLong, i.toInt))` and the layout of this file is listed below.
row group 0
--------------------------------------------------------------------------------
_1:  INT64 SNAPPY DO:0 FPO:4 SZ:8161/16104/1.97 VC:2000 ENC:PLAIN,BIT_PACKED [more]...
_2:  INT32 SNAPPY DO:0 FPO:8165 SZ:8061/8052/1.00 VC:2000 ENC:PLAIN,BIT_PACKED [more]...

    _1 TV=2000 RL=0 DL=0
    ----------------------------------------------------------------------------
    page 0:  DLE:BIT_PACKED RLE:BIT_PACKED VLE:PLAIN ST:[no stats for  [more]... VC:500
    page 1:  DLE:BIT_PACKED RLE:BIT_PACKED VLE:PLAIN ST:[no stats for  [more]... VC:500
    page 2:  DLE:BIT_PACKED RLE:BIT_PACKED VLE:PLAIN ST:[no stats for  [more]... VC:500
    page 3:  DLE:BIT_PACKED RLE:BIT_PACKED VLE:PLAIN ST:[no stats for  [more]... VC:500

    _2 TV=2000 RL=0 DL=0
    ----------------------------------------------------------------------------
    page 0:  DLE:BIT_PACKED RLE:BIT_PACKED VLE:PLAIN ST:[no stats for  [more]... VC:1000
    page 1:  DLE:BIT_PACKED RLE:BIT_PACKED VLE:PLAIN ST:[no stats for  [more]... VC:1000
 
As you can see in the row group 0, column1 has 4 data pages each with 500 values and column 2 has 2 data pages with 1000 values each. 
If we want to filter the rows by values with _1 = 510 using columnindex, parquet will return the page 1 of column _1 and page 0 of column _2. Page 1 of column _1 starts with row 500, and page 0 of column _2 starts with row 0, and it will be incorrect if we simply read the two values as one row.
 
As an example, If you try filter with  _1 = 510 with column index on in current version, it will give you the wrong result
+---+---+
|_1 |_2 |
+---+---+
|510|10 |
+---+---+
And if turn columnindex off, you can get the correct result
+---+---+
|_1 |_2 |
+---+---+
|510|510|
+---+---+
 ",,apachespark,csun,dongjoon,jamestaylor,lucacanali,lxian2,neilagupta,rajesh.balamohan,ulysses,viirya,yumwang,,,,,,,,,,,SPARK-35743,,,,,,,,,SPARK-36123,,SPARK-26345,,,,,,,,"26/May/21 03:21;lxian2;part-00000-bee08cae-04cd-491c-9602-4c66791af3d0-c000.snappy.parquet;https://issues.apache.org/jira/secure/attachment/13025965/part-00000-bee08cae-04cd-491c-9602-4c66791af3d0-c000.snappy.parquet",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 30 21:21:57 UTC 2021,,,,,,,,,,"0|z0p5uw:",9223372036854775807,,,,,,,,,,,,,3.2.0,,,,,,,,,,"25/Mar/21 07:01;yumwang;[~lxian2] Thank you for reporting this issue. Would you like work on this?;;;","25/Mar/21 18:07;dongjoon;Thank you for creating a new JIRA, [~lxian2]!;;;","26/Mar/21 10:24;lxian2;[~yumwang] Yes, I'm interested in this issue and I would like to work on it. ;;;","29/Mar/21 19:30;apachespark;User 'lxian' has created a pull request for this issue:
https://github.com/apache/spark/pull/31998;;;","29/Mar/21 19:30;apachespark;User 'lxian' has created a pull request for this issue:
https://github.com/apache/spark/pull/31998;;;","29/Mar/21 19:40;lxian2;My plan is to use rowIndexes to do a filtering on the values read in org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader#readBatch

The values are written into a temporary WritableColumnVector first. And it will loop over the row indexes to find out which value should be written into the result vector.

I did some tests in [https://github.com/apache/spark/pull/31998] and I think it looks good so far;;;","26/May/21 23:20;dongjoon;Hi, All. I'll raise this to a release blocker for Apache Spark 3.2.0.;;;","02/Jun/21 17:40;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/32753;;;","02/Jun/21 17:41;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/32753;;;","30/Jun/21 21:21;dongjoon;Issue resolved by pull request 32753
[https://github.com/apache/spark/pull/32753];;;",,,,,,,,,,
AtLeastNNonNulls does not show up correctly in explain,SPARK-34857,13367317,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tarmstrong,tarmstrong,tarmstrong,24/Mar/21 17:44,12/Dec/22 18:10,13/Jul/23 08:47,25/Mar/21 08:20,3.1.2,3.2.0,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"I see this in an explain plan

{noformat}
(12) Filter
Input [3]: [c1#2410L, c2#2419, c3#2422]
Condition : AtLeastNNulls(n, c1#2410L)

I expect it to be AtLeastNNonNulls and n to have the actual value.
{noformat}

Proposed fix is to change https://github.com/databricks/runtime/blob/09ad623c458c40cbaa0dbb4b9f600d96531d35f5/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/nullExpressions.scala#L384 to
{noformat}
override def toString: String = s""AtLeastNNonNulls(${n}, ${children.mkString("","")})""
{noformat}
Or maybe it's OK to remove and use a default implementation?",,apachespark,maropu,tarmstrong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 25 08:20:56 UTC 2021,,,,,,,,,,"0|z0p5a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/21 17:44;tarmstrong;[~LI,Xiao] i would like to take this one on.;;;","25/Mar/21 00:49;maropu;Please feel free to make a PR for this issue.;;;","25/Mar/21 02:53;apachespark;User 'timarmstrong' has created a pull request for this issue:
https://github.com/apache/spark/pull/31956;;;","25/Mar/21 08:20;gurwls223;Issue resolved by pull request 31956
[https://github.com/apache/spark/pull/31956];;;",,,,,,,,,,,,,,,,
SparkContext - avoid using local lazy val,SPARK-34855,13367199,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lxian2,lxian2,lxian2,24/Mar/21 09:44,12/Dec/22 18:10,13/Jul/23 08:47,29/Mar/21 11:14,2.4.7,,,,,,,,,,,,,,,2.4.8,,,,Spark Core,,,,0,,,"`org.apache.spark.SparkContext#getCallSite` uses local lazy val for `callsite`. But in scala 2.11, local lazy val need synchronization on the containing object `this` (see [https://docs.scala-lang.org/sips/improved-lazy-val-initialization.html#version-6---no-synchronization-on-this-and-concurrent-initialization-of-fields] and [https://github.com/scala/scala-dev/issues/133] )

`getCallSite` is called at the job submission, and thus will be a bottle neck if we are submitting a large amount of jobs on a single spark session. We observed thread blocked due to this in our load test.

!image-2021-03-24-17-42-50-412.png!",,apachespark,lxian2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/21 09:44;lxian2;Screen Shot 2021-03-24 at 5.41.22 PM.png;https://issues.apache.org/jira/secure/attachment/13022890/Screen+Shot+2021-03-24+at+5.41.22+PM.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 29 11:14:07 UTC 2021,,,,,,,,,,"0|z0p4js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/21 09:49;lxian2;[https://github.com/apache/spark/pull/31953] I have submitted a PR to remove the local lazy val;;;","24/Mar/21 09:49;apachespark;User 'lxian' has created a pull request for this issue:
https://github.com/apache/spark/pull/31953;;;","29/Mar/21 04:38;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31988;;;","29/Mar/21 11:14;gurwls223;Issue resolved by pull request 31988
[https://github.com/apache/spark/pull/31988];;;",,,,,,,,,,,,,,,,
ProcfsMetricsGetter.computeAllMetrics may return partial metrics when some of child pids metrics are missing,SPARK-34845,13367107,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Baohe Zhang,Baohe Zhang,Baohe Zhang,23/Mar/21 21:43,29/Mar/21 14:49,13/Jul/23 08:47,29/Mar/21 14:49,3.0.0,3.0.1,3.0.2,3.1.0,3.1.1,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,Spark Core,,,,0,,,"When the procfs metrics of some child pids are unavailable, ProcfsMetricsGetter.computeAllMetrics() may return partial metrics (the sum of a subset of child pids), instead of an all 0 result. This can be misleading and is undesired per the current code comments in [https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/executor/ProcfsMetricsGetter.scala#L214].

How to reproduce it?

This unit test is kind of self-explanatory:
{code:java}
    val p = new ProcfsMetricsGetter(getTestResourcePath(""ProcfsMetrics""))
    val mockedP = spy(p)

    // proc file of pid 22764 doesn't exist, so partial metrics shouldn't be returned
    var ptree = Set(26109, 22764, 22763)
    when(mockedP.computeProcessTree).thenReturn(ptree)
    var r = mockedP.computeAllMetrics
    assert(r.jvmVmemTotal == 0)
    assert(r.jvmRSSTotal == 0)
    assert(r.pythonVmemTotal == 0)
    assert(r.pythonRSSTotal == 0)
{code}
In the current implementation, computeAllMetrics will reset the allMetrics to 0 when processing 22764 because 22764's proc file doesn't exist, but then it will continue processing pid 22763, and update allMetrics to procfs metrics of pid 22763.

Also, a side effect of this bug is that it can lead to a verbose warning log if many pids' stat files are missing. An early terminating can make the warning logs more concise.

How to solve it?

The issue can be fixed by throwing IOException to computeAllMetrics(), in that case, computeAllMetrics can aware that at lease one child pid's procfs metrics is missing and then terminate the metrics reporting.",,apachespark,Baohe Zhang,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 29 14:49:03 UTC 2021,,,,,,,,,,"0|z0p3zc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/21 22:49;apachespark;User 'baohe-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31945;;;","23/Mar/21 22:50;apachespark;User 'baohe-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31945;;;","29/Mar/21 14:49;dongjoon;Issue resolved by pull request 31945
[https://github.com/apache/spark/pull/31945];;;",,,,,,,,,,,,,,,,,
Corrects the type of date_dim.d_quarter_name in the TPCDS schema,SPARK-34842,13367060,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,dongjoon,23/Mar/21 17:17,23/Mar/21 17:24,13/Jul/23 08:47,23/Mar/21 17:23,3.1.1,3.2.0,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,Tests,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34083,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 23 17:23:52 UTC 2021,,,,,,,,,,"0|z0p3ow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/21 17:22;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31943;;;","23/Mar/21 17:22;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31943;;;","23/Mar/21 17:23;dongjoon;Issue resolved by pull request 31943
[https://github.com/apache/spark/pull/31943];;;",,,,,,,,,,,,,,,,,
There is a potential Netty memory leak in TransportResponseHandler.,SPARK-34834,13366962,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,weixiuli,weixiuli,weixiuli,23/Mar/21 11:35,14/Apr/21 16:48,13/Jul/23 08:47,14/Apr/21 16:48,2.4.7,3.0.2,3.1.0,3.1.1,,,,,,,,,,,,2.4.9,3.0.3,3.1.2,3.2.0,Shuffle,,,,0,,,There is a potential Netty memory leak in TransportResponseHandler.,,apachespark,weixiuli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 16:48:19 UTC 2021,,,,,,,,,,"0|z0p334:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/21 11:49;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/31942;;;","14/Apr/21 16:48;srowen;Issue resolved by pull request 31942
[https://github.com/apache/spark/pull/31942];;;",,,,,,,,,,,,,,,,,,
Apply right-padding correctly for correlated subqueries,SPARK-34833,13366901,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,maropu,maropu,maropu,23/Mar/21 06:57,25/Mar/21 13:28,13/Jul/23 08:47,24/Mar/21 23:33,3.1.0,3.1.1,3.2.0,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,correctness,,"This ticket aim at  fixing the bug that does not apply right-padding for char types inside correlated subquries.
For example,  a query below returns nothing in master, but a correct result is `c`.
{code}
scala> sql(s""CREATE TABLE t1(v VARCHAR(3), c CHAR(5)) USING parquet"")
scala> sql(s""CREATE TABLE t2(v VARCHAR(5), c CHAR(7)) USING parquet"")
scala> sql(""INSERT INTO t1 VALUES ('c', 'b')"")
scala> sql(""INSERT INTO t2 VALUES ('a', 'b')"")
scala> val df = sql(""""""
  |SELECT v FROM t1
  |WHERE 'a' IN (SELECT v FROM t2 WHERE t2.c = t1.c )"""""".stripMargin)

scala> df.show()
+---+
|  v|
+---+
+---+

{code}

This is because `ApplyCharTypePadding`  does not handle the case above to apply right-padding into `'abc'`. ",,apachespark,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33480,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 25 13:28:17 UTC 2021,,,,,,,,,,"0|z0p2pk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/21 07:20;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31940;;;","23/Mar/21 11:22;dongjoon;I linked SPARK-33480 and updated the Affected Version.;;;","23/Mar/21 12:15;maropu;Ah, thanks, Dongjoon. I forgot to check the previous versions.;;;","24/Mar/21 23:33;maropu;Resolved by https://github.com/apache/spark/pull/31940;;;","25/Mar/21 13:27;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31959;;;","25/Mar/21 13:28;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31959;;;",,,,,,,,,,,,,,
ExternalAppendOnlyUnsafeRowArrayBenchmark can't run with spark-submit,SPARK-34832,13366888,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,23/Mar/21 05:44,12/Dec/22 18:10,13/Jul/23 08:47,24/Mar/21 06:00,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,SQL,,,,0,,,"The following exception will appear when run ExternalAppendOnlyUnsafeRowArrayBenchmark with
{code:java}
bin/spark-submit --class org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArrayBenchmark --jars spark-core_2.12-3.2.0-SNAPSHOT-tests.jar spark-sql_2.12-3.2.0-SNAPSHOT-tests.jar 
{code}
command :
{code:java}
Exception in thread ""main"" java.lang.IllegalStateException: SparkContext should only be created and accessed on the driver.Exception in thread ""main"" java.lang.IllegalStateException: SparkContext should only be created and accessed on the driver. at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$assertOnDriver(SparkContext.scala:2679) at org.apache.spark.SparkContext.<init>(SparkContext.scala:89) at org.apache.spark.SparkContext.<init>(SparkContext.scala:137) at org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArrayBenchmark$.withFakeTaskContext(ExternalAppendOnlyUnsafeRowArrayBenchmark.scala:53) at org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArrayBenchmark$.testAgainstRawArrayBuffer(ExternalAppendOnlyUnsafeRowArrayBenchmark.scala:120) at org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArrayBenchmark$.$anonfun$runBenchmarkSuite$1(ExternalAppendOnlyUnsafeRowArrayBenchmark.scala:190) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.benchmark.BenchmarkBase.runBenchmark(BenchmarkBase.scala:40) at org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArrayBenchmark$.runBenchmarkSuite(ExternalAppendOnlyUnsafeRowArrayBenchmark.scala:187) at org.apache.spark.benchmark.BenchmarkBase.main(BenchmarkBase.scala:58) at org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArrayBenchmark.main(ExternalAppendOnlyUnsafeRowArrayBenchmark.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52) at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951) at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180) at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203) at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90) at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1030) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1039) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}
 ",,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 24 06:00:36 UTC 2021,,,,,,,,,,"0|z0p2mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/21 06:20;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31939;;;","24/Mar/21 06:00;gurwls223;Issue resolved by pull request 31939
[https://github.com/apache/spark/pull/31939];;;",,,,,,,,,,,,,,,,,,
transform_values return identical values when it's used with udf that returns reference type,SPARK-34829,13366836,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,ChernikovP,ChernikovP,22/Mar/21 23:07,12/May/21 07:20,13/Jul/23 08:47,28/Mar/21 17:42,3.1.1,3.2.0,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,correctness,,"If return value of an {{udf}} that is passed to {{transform_values}} is an {{AnyRef}}, then the transformation returns identical new values for each map key (to be more precise, each newly obtained value overrides values for all previously processed keys).

Consider following examples:
{code:java}
case class Bar(i: Int)
val square = udf((b: Bar) => b.i * b.i)
val df = Seq(Map(1 -> Bar(1), 2 -> Bar(2), 3 -> Bar(3))).toDF(""map"")
df.withColumn(""map_square"", transform_values(col(""map""), (_, v) => square(v))).show(truncate = false)
+------------------------------+------------------------+
|map                           |map_square              |
+------------------------------+------------------------+
|{1 -> {1}, 2 -> {2}, 3 -> {3}}|{1 -> 1, 2 -> 4, 3 -> 9}|
+------------------------------+------------------------+
{code}
vs 
{code:java}
case class Bar(i: Int)
case class BarSquare(i: Int)
val square = udf((b: Bar) => BarSquare(b.i * b.i))
val df = Seq(Map(1 -> Bar(1), 2 -> Bar(2), 3 -> Bar(3))).toDF(""map"")
df.withColumn(""map_square"", transform_values(col(""map""), (_, v) => square(v))).show(truncate = false)
+------------------------------+------------------------------+
|map                           |map_square                    |
+------------------------------+------------------------------+
|{1 -> {1}, 2 -> {2}, 3 -> {3}}|{1 -> {9}, 2 -> {9}, 3 -> {9}}|
+------------------------------+------------------------------+
{code}
or even just this one
{code:java}
case class Foo(s: String)
val reverse = udf((f: Foo) => f.s.reverse)
val df = Seq(Map(1 -> Foo(""abc""), 2 -> Foo(""klm""), 3 -> Foo(""xyz""))).toDF(""map"")
df.withColumn(""map_reverse"", transform_values(col(""map""), (_, v) => reverse(v))).show(truncate = false)
+------------------------------------+------------------------------+
|map                                 |map_reverse                   |
+------------------------------------+------------------------------+
|{1 -> {abc}, 2 -> {klm}, 3 -> {xyz}}|{1 -> zyx, 2 -> zyx, 3 -> zyx}|
+------------------------------------+------------------------------+
{code}
After playing with {{org.apache.spark.sql.catalyst.expressions.TransformValues}} it looks like something wrong is happening while executing this line:
{code:java}
resultValues.update(i, functionForEval.eval(inputRow)){code}
To be more precise , it's all about {{functionForEval.eval(inputRow)}} , because if you do something like this:
{code:java}
println(s""RESULTS PRIOR TO EVALUATION - $resultValues"")
val resultValue = functionForEval.eval(inputRow)
println(s""RESULT - $resultValue"")
println(s""RESULTS PRIOR TO UPDATE - $resultValues"")
resultValues.update(i, resultValue)
println(s""RESULTS AFTER UPDATE - $resultValues""){code}
You'll see in the logs, something like:
{code:java}
RESULTS PRIOR TO EVALUATION - [null,null,null] 
RESULT - [0,1] 
RESULTS PRIOR TO UPDATE - [null,null,null]
RESULTS AFTER UPDATE - [[0,1],null,null]
------
RESULTS PRIOR TO EVALUATION - [[0,1],null,null] 
RESULT - [0,4]
RESULTS PRIOR TO UPDATE - [[0,4],null,null] 
RESULTS  AFTER UPDATE - [[0,4],[0,4],null]
------
RESULTS PRIOR TO EVALUATION - [[0,4],[0,4],null] 
RESULT - [0,9]
RESULTS PRIOR TO UPDATE - [[0,9],[0,9],null]
RESULTS  AFTER UPDATE - [[0,9],[0,9],[0,9]
{code}
 ",,apachespark,ChernikovP,dongjoon,dsolow1,maropu,petertoth,,,,,,,,,,,,,,,,,,,,SPARK-34830,SPARK-35371,,,,,,,,,,,SPARK-32154,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 28 17:42:37 UTC 2021,,,,,,,,,,"0|z0p2b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/21 00:18;ChernikovP;On the contrary, this will work as expected:
{code:java}
case class Bar(i: Int)
def square(barC: Column): Column = {
  val iC = barC.getField(""i"")
  struct((iC * iC).as(""i""))
}
val df = Seq(Map(1 -> Bar(1), 2 -> Bar(2), 3 -> Bar(3))).toDF(""map"")
df.withColumn(""map_square"", transform_values(col(""map""), (_, v) => square(v))).show(truncate = false)
+------------------------------+------------------------------+
|map                           |map_square                    |
+------------------------------+------------------------------+
|{1 -> {1}, 2 -> {2}, 3 -> {3}}|{1 -> {1}, 2 -> {4}, 3 -> {9}}|
+------------------------------+------------------------------+{code}
and this as well:
{code:java}
case class Foo(s: String)
def reverse(fooC: Column): Column = 
  org.apache.spark.sql.functions.reverse(fooC.getField(""s""))
val df = Seq(Map(1 -> Foo(""abc""), 2 -> Foo(""klm""), 3 -> Foo(""xyz""))).toDF(""map"")
df.withColumn(""map_reverse"", transform_values(col(""map""), (_, v) => reverse(v))).show(truncate = false)
+------------------------------------+------------------------------+
|map                                 |map_reverse                   |
+------------------------------------+------------------------------+
|{1 -> {abc}, 2 -> {klm}, 3 -> {xyz}}|{1 -> cba, 2 -> mlk, 3 -> zyx}|
+------------------------------------+------------------------------+{code};;;","24/Mar/21 17:24;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31955;;;","28/Mar/21 17:42;dongjoon;This is resolved via https://github.com/apache/spark/pull/31955;;;",,,,,,,,,,,,,,,,,
K8s Integration test failed (due to libldap installation failed),SPARK-34820,13366661,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,22/Mar/21 07:34,22/Mar/21 17:48,13/Jul/23 08:47,22/Mar/21 17:14,3.1.0,3.1.1,,,,,,,,,,,,,,3.1.2,3.2.0,,,Kubernetes,SparkR,Tests,,0,,,"Err:20

[http://security.debian.org/debian-security]

buster/updates/main amd64 libldap-common all 2.4.47+dfsg-3+deb10u4 404 Not Found [IP: 151.101.194.132 80] Err:21

[http://security.debian.org/debian-security]

buster/updates/main amd64 libldap-2.4-2 amd64 2.4.47+dfsg-3+deb10u4 404 Not Found [IP: 151.101.194.132 80] E: Failed to fetch

[http://security.debian.org/debian-security/pool/updates/main/o/openldap/libldap-common_2.4.47+dfsg-3+deb10u4_all.deb]

404 Not Found [IP: 151.101.194.132 80] E: Failed to fetch

[http://security.debian.org/debian-security/pool/updates/main/o/openldap/libldap-2.4-2_2.4.47+dfsg-3+deb10u4_amd64.deb]

404 Not Found [IP: 151.101.194.132 80]

[1] http://mail-archives.apache.org/mod_mbox/spark-dev/202103.mbox/%3CCAGFcPdZY_TZ-qD7_SLvN5%2B1jjt9cp4GyTwxbXHbVHnD-stLSqw%40mail.gmail.com%3E",,apachespark,dongjoon,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 22 17:14:17 UTC 2021,,,,,,,,,,"0|z0p188:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/21 07:53;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31923;;;","22/Mar/21 07:54;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31923;;;","22/Mar/21 17:14;dongjoon;Issue resolved by pull request 31923
[https://github.com/apache/spark/pull/31923];;;",,,,,,,,,,,,,,,,,
LikeSimplification should handle NULL,SPARK-34814,13366616,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhu,yumwang,yumwang,22/Mar/21 02:20,12/Dec/22 18:10,13/Jul/23 08:47,29/Mar/21 03:06,3.2.0,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"It will throw NPE if add this test to {{RegexpExpressionsSuite}}:
{code:scala}
import org.apache.spark.sql.catalyst.optimizer.ConstantFolding
import org.apache.spark.sql.internal.SQLConf
withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key ->
  ConstantFolding.getClass.getName.stripSuffix(""$"")) {
  checkEvaluation(Literal.create(""foo"", StringType)
    .likeAll(""%foo%"", Literal.create(null, StringType)), null)
}
{code}",,angerszhuuu,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 29 03:06:15 UTC 2021,,,,,,,,,,"0|z0p0y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/21 01:34;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31976;;;","27/Mar/21 01:35;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31976;;;","29/Mar/21 03:06;gurwls223;Issue resolved by pull request 31976
[https://github.com/apache/spark/pull/31976];;;",,,,,,,,,,,,,,,,,
Redact fs.s3a.access.key like secret and token,SPARK-34811,13366555,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,21/Mar/21 06:50,21/Mar/21 21:09,13/Jul/23 08:47,21/Mar/21 21:09,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,Spark Core,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 21 21:09:48 UTC 2021,,,,,,,,,,"0|z0p0ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/21 06:55;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31912;;;","21/Mar/21 06:56;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31912;;;","21/Mar/21 21:09;dongjoon;Issue resolved by pull request 31912
[https://github.com/apache/spark/pull/31912];;;",,,,,,,,,,,,,,,,,
PySpark loses metadata in DataFrame fields when selecting nested columns,SPARK-34805,13366410,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,Pyrrho,Pyrrho,19/Mar/21 17:42,21/Sep/22 06:08,13/Jul/23 08:47,21/Mar/22 16:38,3.0.1,3.1.1,,,,,,,,,,,,,,3.3.0,,,,PySpark,,,,4,,,"For a DataFrame schema with nested StructTypes, where metadata is set for fields in the schema, that metadata is lost when a DataFrame selects nested fields.  For example, suppose
{code:java}
df.schema.fields[0].dataType.fields[0].metadata
{code}
returns a non-empty dictionary, then
{code:java}
df.select('Field0.SubField0').schema.fields[0].metadata{code}
returns an empty dictionary, where ""Field0"" is the name of the first field in the DataFrame and ""SubField0"" is the name of the first nested field under ""Field0"".

 ",,apachespark,cloud_fan,emitc2h,holden,kevinwallimann,Pyrrho,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/21 17:43;Pyrrho;jsonMetadataTest.py;https://issues.apache.org/jira/secure/attachment/13022671/jsonMetadataTest.py","19/Jan/22 11:49;kevinwallimann;nested_columns_metadata.scala;https://issues.apache.org/jira/secure/attachment/13039082/nested_columns_metadata.scala",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 21 16:38:49 UTC 2022,,,,,,,,,,"0|z0ozog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/21 16:23;emitc2h;I believe this is not a PySpark-specific issue. We have a unit test in [transmogif.ai|https://transmogrif.ai/] where we are specifying [column metadata manually|https://github.com/salesforce/TransmogrifAI/blob/90a0f298f14506a27c84a71de414d53a30cf687f/core/src/test/scala/com/salesforce/op/stages/impl/preparators/SanityCheckerTest.scala#L137] and check whether the metadata is properly passed on to a model that consumes this column. The column metadata is properly given to the column using {{.as(columnName, metadata)}}, but is immediately lost once the select is executed. I've traced the issue to the changes in {{ExpressionEncoder}}:
 * In Spark 2.4, [it takes it a schema argument|https://github.com/apache/spark/blob/e89526d2401b3a04719721c923a6f630e555e286/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoder.scala#L222] through which the column metadata is passed along
 * In Spark 3.0, [it no longer takes|https://github.com/apache/spark/blob/39889df32a7a916d826e255fda6fc62e2a3d7971/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoder.scala#L232] this schema parameter and it seems like the column metadata is lost as a result

I can't tell if this was intentional or not, but it renders the metadata argument of the {{.as}} [method|https://github.com/apache/spark/blob/39889df32a7a916d826e255fda6fc62e2a3d7971/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L1133] in {{Column}} mostly useless.;;;","19/Jan/22 11:51;kevinwallimann;The problem happens in Scala as well. I attached a scala file [^nested_columns_metadata.scala] to demonstrate the issue. I tried it in the spark-shell of versions 2.4.7, 3.1.2 and 3.2.0, always with the same result. This behavior is a bug, because the documentation for {{StructField}} clearly says that the ""metadata should be preserved during transformation if the content of the column is not modified, e.g, in selection"";;;","21/Jan/22 10:47;apachespark;User 'kevinwallimann' has created a pull request for this issue:
https://github.com/apache/spark/pull/35270;;;","21/Jan/22 10:47;apachespark;User 'kevinwallimann' has created a pull request for this issue:
https://github.com/apache/spark/pull/35270;;;","21/Mar/22 16:38;cloud_fan;Issue resolved by pull request 35270
[https://github.com/apache/spark/pull/35270];;;",,,,,,,,,,,,,,,
Util methods requiring certain versions of Pandas & PyArrow don't pass through the raised ImportError,SPARK-34803,13366392,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,johnhany97,johnhany97,johnhany97,19/Mar/21 16:12,12/Dec/22 18:10,13/Jul/23 08:47,22/Mar/21 14:30,3.1.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,PySpark,,,,0,,,"When checking that the we can import either {{pandas}} or {{pyarrow}}, we except any {{ImportError}} and raise an error declaring the minimum version of the respective package that's required to be in the Python environment.

We don't however, pass the {{ImportError}} that might have been thrown by the package itself. Take {{pandas}} as an example, when we call {{import pandas}}, pandas itself might be in the environment, but can throw an {{ImportError}} [https://github.com/pandas-dev/pandas/blob/0.24.x/pandas/compat/__init__.py#L438] if another package it requires isn't there. This error wouldn't be passed through and we'd end up getting a misleading error message that states that {{pandas}} isn't in the environment, while in fact it is but something else makes us unable to import it.

I believe this can be improved by chaining the exceptions and am happy to provide said contribution.",,apachespark,johnhany97,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 22 14:30:36 UTC 2021,,,,,,,,,,"0|z0ozkg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/21 16:21;johnhany97;I've gone ahead and opened https://github.com/apache/spark/pull/31902;;;","19/Mar/21 16:22;apachespark;User 'johnhany97' has created a pull request for this issue:
https://github.com/apache/spark/pull/31902;;;","19/Mar/21 16:22;apachespark;User 'johnhany97' has created a pull request for this issue:
https://github.com/apache/spark/pull/31902;;;","22/Mar/21 14:30;gurwls223;Issue resolved by pull request 31902
[https://github.com/apache/spark/pull/31902];;;",,,,,,,,,,,,,,,,
Fix incorrect join condition,SPARK-34798,13366224,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,opensky,yumwang,yumwang,19/Mar/21 02:31,19/Mar/21 15:47,13/Jul/23 08:47,19/Mar/21 15:47,3.2.0,,,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,SQL,Tests,,,0,,,"[https://github.com/apache/spark/blob/5988d2846450f8ec72fefcd2067f12819463cc4b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/PropagateEmptyRelationSuite.scala#L150]

[https://github.com/apache/spark/blob/04e53d2e3c9dc993b8ba88c5ef0bcf0a8a9b06b2/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeLimitZeroSuite.scala#L77]

[https://github.com/apache/spark/blob/04e53d2e3c9dc993b8ba88c5ef0bcf0a8a9b06b2/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeLimitZeroSuite.scala#L89]

[https://github.com/apache/spark/blob/04e53d2e3c9dc993b8ba88c5ef0bcf0a8a9b06b2/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/OptimizeLimitZeroSuite.scala#L91]

 ",,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 19 15:47:18 UTC 2021,,,,,,,,,,"0|z0oyj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/21 03:33;apachespark;User 'opensky142857' has created a pull request for this issue:
https://github.com/apache/spark/pull/31890;;;","19/Mar/21 03:34;apachespark;User 'opensky142857' has created a pull request for this issue:
https://github.com/apache/spark/pull/31890;;;","19/Mar/21 15:47;yumwang;Issue resolved by pull request 31890
[https://github.com/apache/spark/pull/31890];;;",,,,,,,,,,,,,,,,,
Codegen compilation error for query with LIMIT operator and without AQE,SPARK-34796,13366210,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,chengsu,chengsu,chengsu,19/Mar/21 00:46,12/Dec/22 18:10,13/Jul/23 08:47,20/Mar/21 02:22,3.1.0,3.1.1,3.2.0,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"Example (reproduced in unit test):

 
{code:java}
  test(""failed limit query"") {
    withTable(""left_table"", ""empty_right_table"", ""output_table"") {
      spark.range(5).toDF(""k"").write.saveAsTable(""left_table"")
      spark.range(0).toDF(""k"").write.saveAsTable(""empty_right_table"")
         
      withSQLConf(SQLConf.ADAPTIVE_EXECUTION_ENABLED.key -> ""false"") {
        spark.sql(""CREATE TABLE output_table (k INT) USING parquet"")
        spark.sql(
          s""""""
             |INSERT INTO TABLE output_table
             |SELECT t1.k FROM left_table t1
             |JOIN empty_right_table t2
             |ON t1.k = t2.k
             |LIMIT 3
             |"""""".stripMargin)
      }
    }
  }
{code}
Result:

 

https://gist.github.com/c21/ea760c75b546d903247582be656d9d66",,apachespark,chengsu,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 21 05:14:46 UTC 2021,,,,,,,,,,"0|z0oyg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/21 00:50;chengsu;FYI I am working on a PR to fix it now.;;;","19/Mar/21 01:06;gurwls223;[~chengsu] I am lowering the priority because we enable AQE by default now and users get affected less.;;;","19/Mar/21 01:12;chengsu;[~hyukjin.kwon] - yeah I agree with it. Thanks for correcting priority here.;;;","19/Mar/21 05:08;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/31892;;;","20/Mar/21 02:22;maropu;Resolved by https://github.com/apache/spark/pull/31892;;;","21/Mar/21 05:14;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/31911;;;",,,,,,,,,,,,,,
Nested higher-order functions broken in DSL,SPARK-34794,13366200,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dsolow,dsolow1,dsolow1,18/Mar/21 21:55,12/May/21 04:12,13/Jul/23 08:47,05/May/21 03:50,3.1.1,3.2.0,,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,SQL,,,,0,correctness,,"In Spark 3, if I have:

{code:java}
val df = Seq(
    (Seq(1,2,3), Seq(""a"", ""b"", ""c""))
).toDF(""numbers"", ""letters"")
{code}
and I want to take the cross product of these two arrays, I can do the following in SQL:

{code:java}
df.selectExpr(""""""
    FLATTEN(
        TRANSFORM(
            numbers,
            number -> TRANSFORM(
                letters,
                letter -> (number AS number, letter AS letter)
            )
        )
    ) AS zipped
"""""").show(false)
+------------------------------------------------------------------------+
|zipped                                                                  |
+------------------------------------------------------------------------+
|[{1, a}, {1, b}, {1, c}, {2, a}, {2, b}, {2, c}, {3, a}, {3, b}, {3, c}]|
+------------------------------------------------------------------------+
{code}

This works fine. But when I try the equivalent using the scala DSL, the result is wrong:

{code:java}
df.select(
    f.flatten(
        f.transform(
            $""numbers"",
            (number: Column) => { f.transform(
                $""letters"",
                (letter: Column) => { f.struct(
                    number.as(""number""),
                    letter.as(""letter"")
                ) }
            ) }
        )
    ).as(""zipped"")
).show(10, false)
+------------------------------------------------------------------------+
|zipped                                                                  |
+------------------------------------------------------------------------+
|[{a, a}, {b, b}, {c, c}, {a, a}, {b, b}, {c, c}, {a, a}, {b, b}, {c, c}]|
+------------------------------------------------------------------------+
{code}

Note that the numbers are not included in the output. The explain for this second version is:

{code:java}
== Parsed Logical Plan ==
'Project [flatten(transform('numbers, lambdafunction(transform('letters, lambdafunction(struct(NamePlaceholder, lambda 'x AS number#442, NamePlaceholder, lambda 'x AS letter#443), lambda 'x, false)), lambda 'x, false))) AS zipped#444]
+- Project [_1#303 AS numbers#308, _2#304 AS letters#309]
   +- LocalRelation [_1#303, _2#304]

== Analyzed Logical Plan ==
zipped: array<struct<number:string,letter:string>>
Project [flatten(transform(numbers#308, lambdafunction(transform(letters#309, lambdafunction(struct(number, lambda x#446, letter, lambda x#446), lambda x#446, false)), lambda x#445, false))) AS zipped#444]
+- Project [_1#303 AS numbers#308, _2#304 AS letters#309]
   +- LocalRelation [_1#303, _2#304]

== Optimized Logical Plan ==
LocalRelation [zipped#444]

== Physical Plan ==
LocalTableScan [zipped#444]
{code}

Seems like variable name x is hardcoded. And sure enough: https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L3647


",3.1.1,apachespark,dsolow1,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-35381,SPARK-35382,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 05 03:50:18 UTC 2021,,,,,,,,,,"0|z0oyds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/21 00:08;apachespark;User 'dmsolow' has created a pull request for this issue:
https://github.com/apache/spark/pull/31887;;;","19/Mar/21 00:09;dsolow1;PR is at https://github.com/apache/spark/pull/31887

Thanks to [~rspitzer] for suggesting AtomicInteger;;;","03/May/21 15:04;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32424;;;","05/May/21 03:50;maropu;Resolved by https://github.com/apache/spark/pull/32424;;;",,,,,,,,,,,,,,,,
[UI] StagePage input size/records not show when records greater than zero,SPARK-34777,13365889,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,warrenzhu25,warrenzhu25,warrenzhu25,17/Mar/21 16:55,30/Aug/22 04:42,13/Jul/23 08:47,30/Aug/22 04:42,3.1.1,,,,,,,,,,,,,,,3.4.0,,,,Web UI,,,,0,,,"!No input size records.png|width=547,height=212!

The `Input Size / Records` should show in summary metrics table and task columns, as input records greater than zero and bytes is zero. One example is spark streaming job read from kafka",,apachespark,warrenzhu25,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/21 16:57;warrenzhu25;No input size records.png;https://issues.apache.org/jira/secure/attachment/13022518/No+input+size+records.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 30 04:42:22 UTC 2022,,,,,,,,,,"0|z0owgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/21 17:11;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/31869;;;","13/Feb/22 03:15;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/35498;;;","13/Feb/22 03:15;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/35498;;;","30/Aug/22 04:42;yumwang;Issue resolved by pull request 35498
[https://github.com/apache/spark/pull/35498];;;",,,,,,,,,,,,,,,,
Catalyst error on on certain struct operation (Couldn't find _gen_alias_),SPARK-34776,13365838,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,dsolow1,dsolow1,17/Mar/21 13:13,12/Dec/22 18:10,13/Jul/23 08:47,19/Mar/21 18:44,3.1.1,3.2.0,,,,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,SQL,,,,0,,,"When I run the following:
{code:java}
import org.apache.spark.sql.{functions => f}
import org.apache.spark.sql.expressions.Window

val df = Seq(
    (""t1"", ""123"", ""bob""),
    (""t1"", ""456"", ""bob""),
    (""t2"", ""123"", ""sam"")
).toDF(""type"", ""value"", ""name"")

val test = df.select(
    $""*"",
    f.struct(f.count($""*"").over(Window.partitionBy($""type"", $""value"", $""name"")).as(""count""), $""name"").as(""name_count"")
).select(
  $""*"",
  f.max($""name_count"").over(Window.partitionBy($""type"", $""value"")).as(""best_name"")
)

test.printSchema
{code}
I get the following schema, which is fine:
{code:java}
root
 |-- type: string (nullable = true)
 |-- value: string (nullable = true)
 |-- name: string (nullable = true)
 |-- name_count: struct (nullable = false)
 |    |-- count: long (nullable = false)
 |    |-- name: string (nullable = true)
 |-- best_name: struct (nullable = true)
 |    |-- count: long (nullable = false)
 |    |-- name: string (nullable = true)
{code}
However when I get a subfield of the best_name struct, I get an error:
{code:java}
test.select($""best_name.name"").show(10, false)
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: _gen_alias_3458#3458
  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:75)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:74)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:317)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:317)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:322)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:322)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:306)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:74)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:96)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at scala.collection.TraversableLike.map(TraversableLike.scala:238)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
  at scala.collection.AbstractTraversable.map(Traversable.scala:108)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:96)
  at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.<init>(InterpretedMutableProjection.scala:35)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$19.applyOrElse(Optimizer.scala:1589)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$19.applyOrElse(Optimizer.scala:1586)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:317)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:317)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:322)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:322)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:322)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:322)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:306)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1586)
  at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1585)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
  at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)
  at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)
  at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:87)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:84)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:84)
  at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:95)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:113)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:110)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$simpleString$2(QueryExecution.scala:161)
  at org.apache.spark.sql.execution.ExplainUtils$.processPlan(ExplainUtils.scala:115)
  at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:161)
  at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:206)
  at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:175)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:98)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)
  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:827)
  ... 49 elided
Caused by: java.lang.RuntimeException: Couldn't find _gen_alias_3458#3458 in [type#3427,value#3428,name#3429]
  at scala.sys.package$.error(package.scala:30)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.$anonfun$applyOrElse$1(BoundAttribute.scala:81)
  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
  ... 144 more
{code}

Note that when I cast the struct type it works:


{code:java}
import org.apache.spark.sql.types._
test.select(
    $""best_name"".cast(StructType(Seq(StructField(""count"", LongType), StructField(""name"", StringType))))(""name"")
).show(10, false)

+--------------+
|best_name.name|
+--------------+
|bob           |
|sam           |
|bob           |
+--------------+
{code}
","Spark 3.1.1

Scala 2.12.10",apachespark,dongjoon,dsolow1,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 20 01:37:11 UTC 2021,,,,,,,,,,"0|z0ow5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/21 01:26;gurwls223;cc [~viirya] FYI;;;","19/Mar/21 03:46;viirya;Thanks for ping. Let me take a look.;;;","19/Mar/21 08:13;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31897;;;","19/Mar/21 08:14;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31897;;;","19/Mar/21 18:44;dongjoon;Issue resolved by pull request 31897
[https://github.com/apache/spark/pull/31897];;;","20/Mar/21 01:36;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31904;;;","20/Mar/21 01:37;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31904;;;",,,,,,,,,,,,,
The `change-scala- version.sh` script not replaced scala.version property correctly,SPARK-34774,13365733,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,17/Mar/21 07:22,19/Mar/21 21:14,13/Jul/23 08:47,18/Mar/21 12:34,3.2.0,,,,,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,Build,,,,0,,,"Execute the following commands in order
 # dev/change-scala-version.sh 2.13

 # dev/change-scala-version.sh 2.12

 # git status

there will generate git diff as follow:
{code:java}
diff --git a/pom.xml b/pom.xml
index ddc4ce2f68..f43d8c8f78 100644
--- a/pom.xml
+++ b/pom.xml
@@ -162,7 +162,7 @@
     <commons.math3.version>3.4.1</commons.math3.version>
     <!-- managed up from 3.2.1 for SPARK-11652 -->
     <commons.collections.version>3.2.2</commons.collections.version>
-    <scala.version>2.12.10</scala.version>
+    <scala.version>2.13.5</scala.version>
     <scala.binary.version>2.12</scala.binary.version>
     <scalatest-maven-plugin.version>2.0.0</scalatest-maven-plugin.version>
     <scalafmt.parameters>--test</scalafmt.parameters>
{code}
seem 'scala.version' property was not replaced correctly",,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34773,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 19 06:27:01 UTC 2021,,,,,,,,,,"0|z0ovi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/21 08:20;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31865;;;","18/Mar/21 12:34;srowen;Issue resolved by pull request 31865
[https://github.com/apache/spark/pull/31865];;;","19/Mar/21 06:27;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31893;;;",,,,,,,,,,,,,,,,,
RebaseDateTime loadRebaseRecords should use Spark classloader instead of context,SPARK-34772,13365728,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ulysses,ulysses,ulysses,17/Mar/21 07:07,22/Mar/21 00:44,13/Jul/23 08:47,19/Mar/21 05:39,3.1.1,3.2.0,,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,SQL,,,,0,,,"With custom `spark.sql.hive.metastore.version` and `spark.sql.hive.metastore.jars`.

Spark would use date formatter in `HiveShim` that convert `date` to `string`, if we set `spark.sql.legacy.timeParserPolicy=LEGACY` the `RebaseDateTime` code will be invoked. At that moment, if `RebaseDateTime` is initialized the first time then context class loader is `IsolatedClientLoader`. Such error msg would throw:

{code:java}
java.lang.IllegalArgumentException: argument ""src"" is null
  at com.fasterxml.jackson.databind.ObjectMapper._assertNotNull(ObjectMapper.java:4413)
  at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3157)
  at com.fasterxml.jackson.module.scala.ScalaObjectMapper.readValue(ScalaObjectMapper.scala:187)
  at com.fasterxml.jackson.module.scala.ScalaObjectMapper.readValue$(ScalaObjectMapper.scala:186)
  at org.apache.spark.sql.catalyst.util.RebaseDateTime$$anon$1.readValue(RebaseDateTime.scala:267)
  at org.apache.spark.sql.catalyst.util.RebaseDateTime$.loadRebaseRecords(RebaseDateTime.scala:269)
  at org.apache.spark.sql.catalyst.util.RebaseDateTime$.<init>(RebaseDateTime.scala:291)
  at org.apache.spark.sql.catalyst.util.RebaseDateTime$.<clinit>(RebaseDateTime.scala)
  at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaDate(DateTimeUtils.scala:109)
  at org.apache.spark.sql.catalyst.util.LegacyDateFormatter.format(DateFormatter.scala:95)
  at org.apache.spark.sql.catalyst.util.LegacyDateFormatter.format$(DateFormatter.scala:94)
  at org.apache.spark.sql.catalyst.util.LegacySimpleDateFormatter.format(DateFormatter.scala:138)
  at org.apache.spark.sql.hive.client.Shim_v0_13$ExtractableLiteral$1$.unapply(HiveShim.scala:661)
  at org.apache.spark.sql.hive.client.Shim_v0_13.convert$1(HiveShim.scala:785)
  at org.apache.spark.sql.hive.client.Shim_v0_13.$anonfun$convertFilters$4(HiveShim.scala:826)
{code}
",,apachespark,ulysses,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 22 00:44:44 UTC 2021,,,,,,,,,,"0|z0ovh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/21 08:03;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/31864;;;","19/Mar/21 05:39;yumwang;Issue resolved by pull request [31864|https://github.com/apache/spark/pull/31864]
[https://github.com/apache/spark/pull/31864];;;","22/Mar/21 00:44;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31916;;;","22/Mar/21 00:44;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31916;;;",,,,,,,,,,,,,,,,
InMemoryCatalog.tableExists should not fail if database doesn't exist,SPARK-34770,13365711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,17/Mar/21 05:37,17/Mar/21 08:46,13/Jul/23 08:47,17/Mar/21 08:46,3.2.0,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 17 08:46:12 UTC 2021,,,,,,,,,,"0|z0ovdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/21 06:02;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31860;;;","17/Mar/21 08:46;cloud_fan;Issue resolved by pull request 31860
[https://github.com/apache/spark/pull/31860];;;",,,,,,,,,,,,,,,,,,
Respect the default input buffer size in Univocity,SPARK-34768,13365702,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,17/Mar/21 04:42,12/Dec/22 18:10,13/Jul/23 08:47,17/Mar/21 10:58,3.0.2,3.1.1,,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,SQL,,,,0,,,"Currenty Univocity 2.9.1 faces a bug such as https://github.com/uniVocity/univocity-parsers/issues/449.

While this is a bug, another factor is that we don't respect Univocity's default value which makes Spark exposed to non-test coverage in Univocity.

We should resect Univocity's default input buffer value.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 17 10:57:58 UTC 2021,,,,,,,,,,"0|z0ovbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/21 05:02;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/31858;;;","17/Mar/21 10:57;gurwls223;Fixed in https://github.com/apache/spark/pull/31858;;;",,,,,,,,,,,,,,,,,,
"col(), $""<name>"" and df(""name"") should handle quoted column names properly.",SPARK-34763,13365568,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,16/Mar/21 15:17,20/Dec/21 09:26,13/Jul/23 08:47,24/Mar/21 05:44,3.1.1,3.2.0,,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,SQL,,,,0,,,"Quoted column names like `a``b.c` cannot be represented with col(), $""<name>"" and df("""") because they don't handle such column names properly.

For example, if we have a following DataFrame.
{code}
val df1 = spark.sql(""SELECT 'col1' AS `a``b.c`"")
{code}

For the DataFrame, this query is successfully executed.
{code}
scala> df1.selectExpr(""`a``b.c`"").show
+-----+
|a`b.c|
+-----+
| col1|
+-----+
{code}

But the following query will fail because df1(""`a``b.c`"") throws an exception.
{code}
scala> df1.select(df1(""`a``b.c`"")).show
org.apache.spark.sql.AnalysisException: syntax error in attribute name: `a``b.c`;
  at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute$.e$1(unresolved.scala:152)
  at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute$.parseAttributeName(unresolved.scala:162)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveQuoted(LogicalPlan.scala:121)
  at org.apache.spark.sql.Dataset.resolve(Dataset.scala:221)
  at org.apache.spark.sql.Dataset.col(Dataset.scala:1274)
  at org.apache.spark.sql.Dataset.apply(Dataset.scala:1241)
  ... 49 elided
{code}",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 16 15:33:05 UTC 2021,,,,,,,,,,"0|z0ouhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/21 15:32;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31854;;;","16/Mar/21 15:33;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31854;;;",,,,,,,,,,,,,,,,,,
run JavaSQLDataSourceExample failed with Exception in runBasicDataSourceExample().,SPARK-34760,13365466,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zengrui,zengrui,zengrui,16/Mar/21 09:50,24/Mar/21 14:20,13/Jul/23 08:47,18/Mar/21 15:12,3.0.1,3.1.1,,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,Examples,,,,0,,,"run JavaSparkSQLExample failed with Exception in runBasicDataSourceExample().

when excecute 'peopleDF.write().partitionBy(""favorite_color"").bucketBy(42,""name"").saveAsTable(""people_partitioned_bucketed"");'

throws Exception: 'Exception in thread ""main"" org.apache.spark.sql.AnalysisException: partition column favorite_color is not defined in table people_partitioned_bucketed, defined table columns are: age, name;'",,apachespark,Qin Yao,zengrui,,,,,,,,,,,,,,,,,,,,,,,SPARK-34759,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 18 15:12:03 UTC 2021,,,,,,,,,,"0|z0otuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/21 13:03;apachespark;User 'zengruios' has created a pull request for this issue:
https://github.com/apache/spark/pull/31851;;;","16/Mar/21 13:04;apachespark;User 'zengruios' has created a pull request for this issue:
https://github.com/apache/spark/pull/31851;;;","18/Mar/21 15:12;Qin Yao;Issue resolved by pull request 31851
[https://github.com/apache/spark/pull/31851];;;",,,,,,,,,,,,,,,,,
Fix FileScan equality check,SPARK-34756,13365441,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,16/Mar/21 08:18,01/Jun/21 22:49,13/Jul/23 08:47,23/Mar/21 09:25,3.0.0,3.0.1,3.0.2,3.1.0,3.1.1,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,SQL,,,,0,correctness,,`&&` is missing from `FileScan.equals()`.,,apachespark,cloud_fan,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 24 09:25:29 UTC 2021,,,,,,,,,,"0|z0otpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/21 08:32;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31848;;;","16/Mar/21 08:32;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31848;;;","23/Mar/21 09:25;cloud_fan;Issue resolved by pull request 31848
[https://github.com/apache/spark/pull/31848];;;","24/Mar/21 09:24;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31952;;;","24/Mar/21 09:24;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31952;;;","24/Mar/21 09:25;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31952;;;",,,,,,,,,,,,,,
Create a rule of the analysis logic for streaming write,SPARK-34748,13365298,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,15/Mar/21 17:06,22/Mar/21 06:39,13/Jul/23 08:47,22/Mar/21 06:39,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Structured Streaming,,,,0,,,"Currently, the analysis logic for streaming write is mixed in StreamingQueryManager. If we create a specific analyzer rule and separated logical plans, it should be helpful for further extension.",,apachespark,cloud_fan,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 22 06:39:59 UTC 2021,,,,,,,,,,"0|z0ostk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/21 17:16;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/31842;;;","22/Mar/21 06:39;cloud_fan;Issue resolved by pull request 31842
[https://github.com/apache/spark/pull/31842];;;",,,,,,,,,,,,,,,,,,
Add virtual operators to the built-in function document.,SPARK-34747,13365297,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,15/Mar/21 17:05,12/Dec/22 18:10,13/Jul/23 08:47,19/Mar/21 01:21,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,Documentation,SQL,,,0,,,"After SPARK-34697, DESCRIBE FUNCTION and SHOW FUNCTIONS can describe/show built-in operators including the following virtual operators.

* !=
* <>
* between
* case
* ||

But they are still absent from the built-in functions document.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,SPARK-34782,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 19 01:21:14 UTC 2021,,,,,,,,,,"0|z0ostc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/21 17:15;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31841;;;","15/Mar/21 17:16;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31841;;;","19/Mar/21 01:21;gurwls223;Fixed in https://github.com/apache/spark/pull/31841;;;",,,,,,,,,,,,,,,,,
ExpressionEncoderSuite should use deepEquals when we expect `array of array`,SPARK-34743,13365170,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,15/Mar/21 08:21,15/Mar/21 09:42,13/Jul/23 08:47,15/Mar/21 09:42,1.6.3,2.0.2,2.1.3,2.2.3,2.3.4,2.4.7,3.0.2,3.1.1,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,SQL,Tests,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11727,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 15 09:42:12 UTC 2021,,,,,,,,,,"0|z0os14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/21 08:25;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31837;;;","15/Mar/21 08:26;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31837;;;","15/Mar/21 09:42;dongjoon;Issue resolved by pull request 31837
[https://github.com/apache/spark/pull/31837];;;",,,,,,,,,,,,,,,,,
Discrepancy between TIMESTAMP_SECONDS and cast from float,SPARK-34737,13365035,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,14/Mar/21 07:46,12/Dec/22 18:11,13/Jul/23 08:47,15/Mar/21 01:06,3.2.0,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"TIMESTAMP_SECONDS and CAST(<float> AS TIMESTAMP) have different results:
{code:sql}
spark-sql> SELECT CAST(16777215.0f AS TIMESTAMP);
1970-07-14 07:20:15
spark-sql> SELECT TIMESTAMP_SECONDS(16777215.0f);
1970-07-14 07:20:14.951424
{code}",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34727,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 17 18:57:47 UTC 2021,,,,,,,,,,"0|z0or74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/21 08:11;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31831;;;","14/Mar/21 08:12;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31831;;;","15/Mar/21 01:06;gurwls223;Issue resolved by pull request 31831
[https://github.com/apache/spark/pull/31831];;;","17/Mar/21 18:57;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31872;;;",,,,,,,,,,,,,,,,
Kubernetes and Minikube version upgrade for integration tests,SPARK-34736,13365032,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,14/Mar/21 05:23,10/May/21 16:57,13/Jul/23 08:47,10/May/21 16:57,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Kubernetes,Tests,,,0,,,"As [discussed in the mailing list|http://apache-spark-developers-list.1001551.n3.nabble.com/minikube-and-kubernetes-cluster-versions-for-integration-testing-td30856.html]: upgrading Minikube version from v0.34.1 to v1.7.3 and kubernetes version from v1.15.12 to v1.17.3.

Moreover Minikube version will be checked.

By making this upgrade we can simplify how the kubernetes client is configured for Minikube.",,apachespark,attilapiros,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 10 16:57:39 UTC 2021,,,,,,,,,,"0|z0or6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/21 05:23;attilapiros;Working on this;;;","14/Mar/21 06:40;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/31829;;;","14/Mar/21 06:41;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/31829;;;","10/May/21 16:57;attilapiros;Issue resolved by pull request 31829
[https://github.com/apache/spark/pull/31829];;;",,,,,,,,,,,,,,,,
The logForFailedTest throws an exception when driver is not started,SPARK-34732,13364979,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,attilapiros,attilapiros,attilapiros,13/Mar/21 14:37,13/Mar/21 23:28,13/Jul/23 08:47,13/Mar/21 23:28,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Kubernetes,Tests,,,0,,,"In SPARK-34426 the logForFailedTest method is introduced to add the driver and executors log to the integration-tests.log but when the driver failed to start an exception will be thrown as the list of pods is empty but we still try to access the first item from the list:


{noformat}
- PVs with local storage *** FAILED ***
  java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
  at java.util.ArrayList.rangeCheck(ArrayList.java:659)
  at java.util.ArrayList.get(ArrayList.java:435)
  at org.apache.spark.deploy.k8s.integrationtest.KubernetesSuite.logForFailedTest(KubernetesSuite.scala:83)
  at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:181)
  at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:188)
  at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:200)
  at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
  at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:200)
  at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:182)
  at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:61)
 {noformat}
",,apachespark,attilapiros,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 13 23:28:26 UTC 2021,,,,,,,,,,"0|z0oquo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/21 14:54;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/31824;;;","13/Mar/21 14:54;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/31824;;;","13/Mar/21 23:28;dongjoon;Issue resolved by pull request 31824
[https://github.com/apache/spark/pull/31824];;;",,,,,,,,,,,,,,,,,
ConcurrentModificationException in EventLoggingListener when redacting properties,SPARK-34731,13364756,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,13/Mar/21 02:45,12/Dec/22 18:10,13/Jul/23 08:47,18/Mar/21 06:00,3.1.1,3.2.0,,,,,,,,,,,,,,3.1.2,3.2.0,,,Spark Core,,,,0,,,"Reproduction:

The key elements of reproduction are enabling event logging, setting spark.executor.cores, and some bad luck:
{noformat}
$ bin/spark-shell --conf spark.ui.showConsoleProgress=false \
--conf spark.executor.cores=1 --driver-memory 4g --conf \
""spark.ui.showConsoleProgress=false"" \
--conf spark.eventLog.enabled=true \
--conf spark.eventLog.dir=/tmp/spark-events
...
scala> (0 to 500).foreach { i =>
     |   val df = spark.range(0, 20000).toDF(""a"")
     |   df.filter(""a > 12"").count
     | }
21/03/12 18:16:44 ERROR AsyncEventQueue: Listener EventLoggingListener threw an exception
java.util.ConcurrentModificationException
	at java.util.Hashtable$Enumerator.next(Hashtable.java:1387)
	at scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:424)
	at scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:420)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.mutable.MapLike.toSeq(MapLike.scala:75)
	at scala.collection.mutable.MapLike.toSeq$(MapLike.scala:72)
	at scala.collection.mutable.AbstractMap.toSeq(Map.scala:82)
	at org.apache.spark.scheduler.EventLoggingListener.redactProperties(EventLoggingListener.scala:290)
	at org.apache.spark.scheduler.EventLoggingListener.onJobStart(EventLoggingListener.scala:162)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1379)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)
{noformat}
Analysis from quick reading of the code:

DAGScheduler posts a JobSubmitted event containing a clone of a properties object [here|https://github.com/apache/spark/blob/4f1e434ec57070b52b28f98c66b53ca6ec4de7a4/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L834].

This event is handled [here|https://github.com/apache/spark/blob/4f1e434ec57070b52b28f98c66b53ca6ec4de7a4/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L2394].

DAGScheduler#handleJobSubmitted stores the properties object in a [Job object|https://github.com/apache/spark/blob/4f1e434ec57070b52b28f98c66b53ca6ec4de7a4/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1154], which in turn is [saved in the jobIdToActiveJob map|https://github.com/apache/spark/blob/4f1e434ec57070b52b28f98c66b53ca6ec4de7a4/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1163].

DAGScheduler#handleJobSubmitted posts a SparkListenerJobStart event [here|https://github.com/apache/spark/blob/4f1e434ec57070b52b28f98c66b53ca6ec4de7a4/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1169] with a reference to the same properties object that was stored indirectly in the jobIdToActiveJob map.

When the EventLoggerListener handles the SparkListenerJobStart event, it iterates over that properties object in redactProperties.

Meanwhile, the DAGScheduler#handleJobSubmitted method is not yet done. It calls submitStage, which calls submitMissingTasks, which [retrieves the same properties object|https://github.com/apache/spark/blob/4f1e434ec57070b52b28f98c66b53ca6ec4de7a4/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1306] from jobIdToActiveJob and calls addPySparkConfigsToProperties, which will modify the properties if spark.executor.cores is set.

If redactProperties just happens to still be iterating over the properties object when the modification happens, HashTable throws a ConcurrentModificationException.",,apachespark,bersprockets,jpugliesi,petertoth,Rui Balau,xkrogen,,,,,,,,,,,,,,,,,,,,SPARK-32027,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 01 18:43:27 UTC 2021,,,,,,,,,,"0|z0oph4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/21 01:25;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/31826;;;","14/Mar/21 01:26;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/31826;;;","18/Mar/21 06:00;gurwls223;Issue resolved by pull request 31826
[https://github.com/apache/spark/pull/31826];;;","29/Apr/21 17:25;Rui Balau;I'd just like to add, while either 3.1.2. or 3.2.2 is not yet released, we have managed to work around this issue by disabling the Event Log as such:
{code:java}
spark.eventLog.enabled=false
{code};;;","31/May/21 22:11;jpugliesi;To clarify, does this issue potentially prevent event logs from being created/written entirely? We're seeing this exception in some of our Spark 3.1.1 applications - namely the applications with particularly large Window queries - where the final event log is never successfully written out (using an s3a:// spark.eventLog.dir, for what it's worth):

{code:bash}
# spark-defaults.conf
spark.eventLog.enabled true
spark.eventLog.dir s3a://my-bucket/spark-event-logs/
{code};;;","01/Jun/21 18:43;bersprockets;I am working from memory, but I remember that you lose the event that the EventLoggingListener was handling at the time of the exception. If it's a job submitted event, the SHS UI doesn't show you the job (even though, I believe, the subsequent events related to the job get written to the file).;;;",,,,,,,,,,,,,,
Difference in results of casting float to timestamp,SPARK-34727,13364523,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,12/Mar/21 12:12,12/Dec/22 18:11,13/Jul/23 08:47,14/Mar/21 02:30,3.2.0,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,correctness,,"The code below portraits the issue:
{code:sql}
spark-sql> CREATE TEMP VIEW v1 AS SELECT 16777215.0f AS f;
spark-sql> SELECT * FROM v1;
1.6777215E7
spark-sql> SELECT CAST(f AS TIMESTAMP) FROM v1;
1970-07-14 07:20:15
spark-sql> CACHE TABLE v1;
spark-sql> SELECT * FROM v1;
1.6777215E7
spark-sql> SELECT CAST(f AS TIMESTAMP) FROM v1;
1970-07-14 07:20:14.951424
{code}

The result from the cached view *1970-07-14 07:20:14.951424* is different from un-cached view *1970-07-14 07:20:15*.",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34737,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 14 02:30:52 UTC 2021,,,,,,,,,,"0|z0oo5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/21 12:12;maxgekk;I am working on a fix.;;;","12/Mar/21 13:20;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31819;;;","12/Mar/21 13:21;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31819;;;","14/Mar/21 02:30;gurwls223;Fixed in https://github.com/apache/spark/pull/31819;;;",,,,,,,,,,,,,,,,
Fix collectToPython timeouts,SPARK-34726,13364085,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,12/Mar/21 09:38,22/Mar/21 16:32,13/Jul/23 08:47,22/Mar/21 16:28,2.4.7,,,,,,,,,,,,,,,2.4.8,,,,SQL,,,,0,,,"One of our customers frequently encounters ""serve-DataFrame"" java.net.SocketTimeoutException: Accept timed errors in PySpark because DataSet.collectToPython() in Spark 2.4 does the following:


#     Collects the results
#     Opens up a socket server that is then listening to the connection from Python side
#     Runs the event listeners as part of withAction on the same thread as SPARK-25680 is not available in Spark 2.4
#     Returns the address of the socket server to Python
#     The Python side connects to the socket server and fetches the data

As the customer has a custom, long running event listener the time between 2. and 5. is frequently longer than the default connection timeout and increasing the connect timeout is not a good solution as we don't know how long running the listeners can take.
",,apachespark,petertoth,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 22 16:28:32 UTC 2021,,,,,,,,,,"0|z0olg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/21 10:26;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31818;;;","12/Mar/21 10:27;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31818;;;","22/Mar/21 16:28;viirya;Issue resolved by pull request 31818
[https://github.com/apache/spark/pull/31818];;;",,,,,,,,,,,,,,,,,
Fix Interpreted evaluation by using getClass.getMethod instead of getDeclaredMethod,SPARK-34724,13364040,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,12/Mar/21 07:47,12/Dec/22 18:11,13/Jul/23 08:47,12/Mar/21 12:33,2.4.0,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,SQL,,,,0,,,This seems to be a regression at SPARK-23583,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23583,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 12 12:33:09 UTC 2021,,,,,,,,,,"0|z0ol60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/21 07:50;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31816;;;","12/Mar/21 07:51;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31816;;;","12/Mar/21 12:33;gurwls223;Fixed in https://github.com/apache/spark/pull/31816;;;",,,,,,,,,,,,,,,,,
Correct parameter type for subexpression elimination under whole-stage,SPARK-34723,13364001,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,12/Mar/21 03:30,24/Jun/21 14:42,13/Jul/23 08:47,13/Mar/21 08:06,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,SQL,,,,0,,,There is a bug in subexpression elimination under wholestage codegen if the parameter type is byte array.,,apachespark,noslowerdna,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 24 14:42:49 UTC 2021,,,,,,,,,,"0|z0okxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/21 03:37;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31814;;;","12/Mar/21 03:38;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31814;;;","13/Mar/21 08:06;viirya;Issue resolved by pull request 31814
[https://github.com/apache/spark/pull/31814];;;","24/Jun/21 14:42;noslowerdna;In case it might be helpful to anyone, the compilation failure's stack trace looks something like this.

{noformat}
[main] ERROR org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 32, Column 84: IDENTIFIER expected instead of '['
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 32, Column 84: IDENTIFIER expected instead of '['
	at org.codehaus.janino.TokenStreamImpl.read(TokenStreamImpl.java:196)
	at org.codehaus.janino.Parser.read(Parser.java:3705)
	at org.codehaus.janino.Parser.parseQualifiedIdentifier(Parser.java:446)
	at org.codehaus.janino.Parser.parseReferenceType(Parser.java:2569)
	at org.codehaus.janino.Parser.parseType(Parser.java:2549)
	at org.codehaus.janino.Parser.parseFormalParameter(Parser.java:1688)
	at org.codehaus.janino.Parser.parseFormalParameterList(Parser.java:1639)
	at org.codehaus.janino.Parser.parseFormalParameters(Parser.java:1620)
	at org.codehaus.janino.Parser.parseMethodDeclarationRest(Parser.java:1518)
	at org.codehaus.janino.Parser.parseClassBodyDeclaration(Parser.java:1028)
	at org.codehaus.janino.Parser.parseClassBody(Parser.java:841)
	at org.codehaus.janino.Parser.parseClassDeclarationRest(Parser.java:736)
	at org.codehaus.janino.Parser.parseClassBodyDeclaration(Parser.java:941)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:234)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:205)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1403)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1500)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1497)
	at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
	at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1351)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1(WholeStageCodegenExec.scala:721)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
	at org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:92)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
	at org.apache.spark.sql.execution.SortExec.doExecute(SortExec.scala:112)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:184)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)
	
[main] WARN  org.apache.spark.sql.execution.WholeStageCodegenExec - Whole-stage codegen disabled for plan ...	
{noformat}

Referenced line of generated code that doesn't compile:

{noformat}
private UTF8String project_subExpr_1(boolean scan_isNull_2, boolean scan_isNull_1, [B scan_value_1, org.apache.spark.unsafe.types.UTF8String scan_value_2) {
{noformat}

Corrected in Spark 3.1.2, it becomes:

{noformat}
private UTF8String project_subExpr_1(org.apache.spark.unsafe.types.UTF8String scan_value_2, boolean scan_isNull_2, boolean scan_isNull_1, byte[] scan_value_1) {
{noformat};;;",,,,,,,,,,,,,,,,
Incorrect star expansion logic MERGE INSERT * / UPDATE *,SPARK-34720,13363913,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,tdas,tdas,11/Mar/21 17:27,13/May/21 12:59,13/Jul/23 08:47,13/May/21 12:58,3.0.2,3.1.1,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"The natural expectation of INSERT * or UPDATE * in MERGE is to assign source columns to target columns of the same name. However, the current logic here generates the assignment by position. https://github.com/apache/spark/commit/7cfd589868b8430bc79e28e4d547008b222781a5#diff-ed19f376a63eba52eea59ca71f3355d4495fad4fad4db9a3324aade0d4986a47R1214

This can very easily lead to incorrect results without the user realizing this odd behavior.
",,apachespark,cloud_fan,petertoth,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 13 12:58:52 UTC 2021,,,,,,,,,,"0|z0okds:",9223372036854775807,,,,,,,,,,,,,3.2.0,,,,,,,,,,"15/Apr/21 15:46;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/32192;;;","15/Apr/21 15:47;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/32192;;;","13/May/21 12:58;cloud_fan;Issue resolved by pull request 32192
[https://github.com/apache/spark/pull/32192];;;",,,,,,,,,,,,,,,,,
fail if the view query has duplicated column names,SPARK-34719,13363871,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Mar/21 14:57,23/Mar/21 14:36,13/Jul/23 08:47,19/Mar/21 01:53,2.4.7,3.0.0,3.1.0,3.1.1,,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,SQL,,,,0,correctness,,,,apachespark,cloud_fan,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 22 16:22:24 UTC 2021,,,,,,,,,,"0|z0ok4g:",9223372036854775807,,,,,,,,,,,,,2.4.8,3.1.2,,,,,,,,,"11/Mar/21 15:17;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31811;;;","11/Mar/21 15:17;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31811;;;","19/Mar/21 01:53;maropu;Resolved by https://github.com/apache/spark/pull/31811;;;","19/Mar/21 06:31;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31894;;;","22/Mar/21 16:22;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31930;;;","22/Mar/21 16:22;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31930;;;",,,,,,,,,,,,,,
collect_list(struct()) fails when used with GROUP BY,SPARK-34714,13363773,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,laurikoobas,laurikoobas,11/Mar/21 07:58,12/Dec/22 18:11,13/Jul/23 08:47,17/Mar/21 01:28,3.1.1,,,,,,,,,,,,,,,3.1.2,,,,SQL,,,,0,,,"The following is failing in DBR8.0 / Spark 3.1.1, but works in earlier DBR and Spark versions:
{quote}with step_1 as (
    select 'E' as name, named_struct('subfield', 1) as field_1
)
select name, collect_list(struct(field_1.subfield))
from step_1
group by 1
{quote}
Fails with the following error message:
{quote}AnalysisException: cannot resolve 'struct(step_1.`field_1`.`subfield`)' due to data type mismatch: Only foldable string expressions are allowed to appear at odd position, got: NamePlaceholder
{quote}
If you modify the query in any of the following ways then it still works::
 * if you remove the field ""name"" and the ""group by 1"" part of the query
 * if you remove the ""struct()"" from within the collect_list()
 * if you use ""named_struct()"" instead of ""struct()"" within the collect_list()

Similarly collect_set() is broken and possibly more related functions, but I haven't done thorough testing.",Databricks Runtime 8.0,laurikoobas,maropu,mauzhang,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 29 07:59:09 UTC 2021,,,,,,,,,,"0|z0ojio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/21 04:17;gurwls223;[~laurikoobas] can you provide a self-contained reproducer?;;;","15/Mar/21 05:51;laurikoobas;Didn't I? When I copy-paste the only code example in the description of the ticket to my Databricks cluster running DBR 8.0 then it produces the error message.;;;","16/Mar/21 14:30;maropu;I've checked that branch-3.1 still has this issue (NOTE: the current master does not).;;;","17/Mar/21 01:27;maropu;Ah, I've checked the latest branch-3.1 again and I found the issue goes away. So, this issue will be resolved in v3.1.2.;;;","29/Nov/21 07:59;mauzhang;FYI, this was resolved by https://issues.apache.org/jira/browse/SPARK-34713 and https://issues.apache.org/jira/browse/SPARK-34749

 ;;;",,,,,,,,,,,,,,,
group by CreateStruct with ExtractValue fails analysis,SPARK-34713,13363771,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Mar/21 07:51,11/Mar/21 17:23,13/Jul/23 08:47,11/Mar/21 17:22,3.1.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,This is caused by https://issues.apache.org/jira/browse/SPARK-31670,,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31670,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 11 17:22:39 UTC 2021,,,,,,,,,,"0|z0oji8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/21 07:57;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31808;;;","11/Mar/21 07:58;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31808;;;","11/Mar/21 17:22;dongjoon;Issue resolved by pull request 31808
[https://github.com/apache/spark/pull/31808];;;",,,,,,,,,,,,,,,,,
Fix pyspark test when using sort_values on Pandas,SPARK-34703,13363689,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,11/Mar/21 01:16,11/Mar/21 02:43,13/Jul/23 08:47,11/Mar/21 02:43,2.4.7,,,,,,,,,,,,,,,2.4.8,,,,PySpark,,,,0,,,"Three PySpark tests are currently failed in Jenkins 2.4 build: test_column_order, test_complex_groupby, test_udf_with_key.

{code}
======================================================================                                                                                                                                                                 
ERROR: test_column_order (pyspark.sql.tests.GroupedMapPandasUDFTests)                                                                                                                                                                  
----------------------------------------------------------------------
Traceback (most recent call last):                                                                                 
  File ""/spark/python/pyspark/sql/tests.py"", line 5996, in test_column_order                                                                                                                                                           
    expected = pd_result.sort_values(['id', 'v']).reset_index(drop=True)                                           
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py"", line 4711, in sort_values                    
    for x in by]                                       
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/generic.py"", line 1702, in _get_label_or_level_values
    self._check_label_or_level_ambiguity(key, axis=axis)
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/generic.py"", line 1656, in _check_label_or_level_ambiguity
    raise ValueError(msg)                                                                                          
ValueError: 'id' is both an index level and a column label, which is ambiguous.
                                                         
======================================================================
ERROR: test_complex_groupby (pyspark.sql.tests.GroupedMapPandasUDFTests)
----------------------------------------------------------------------
Traceback (most recent call last):                      
  File ""/spark/python/pyspark/sql/tests.py"", line 5765, in test_complex_groupby
    expected = expected.sort_values(['id', 'v']).reset_index(drop=True)
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py"", line 4711, in sort_values
    for x in by]                                                                                                   
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/generic.py"", line 1702, in _get_label_or_level_values                                                                                                                       
    self._check_label_or_level_ambiguity(key, axis=axis)                                                                                                                                                                               
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/generic.py"", line 1656, in _check_label_or_level_ambiguity                                                                                                                  
    raise ValueError(msg)                                                                                                                                                                                                              
ValueError: 'id' is both an index level and a column label, which is ambiguous.                                                                                                                                                        
                                                                                                                                                                                                                                       
======================================================================                                             
ERROR: test_udf_with_key (pyspark.sql.tests.GroupedMapPandasUDFTests)                                                                                                                                                                  
----------------------------------------------------------------------                                             
Traceback (most recent call last):                                                                                                                                                                                                     
  File ""/spark/python/pyspark/sql/tests.py"", line 5922, in test_udf_with_key
    .sort_values(['id', 'v']).reset_index(drop=True)                                                                                                                                                                                   
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py"", line 4711, in sort_values                                                                                                                                        
    for x in by]                                                                                                                                                                                                                       
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/generic.py"", line 1702, in _get_label_or_level_values   
    self._check_label_or_level_ambiguity(key, axis=axis)                                                                                                                                                                               
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/generic.py"", line 1656, in _check_label_or_level_ambiguity                                                                                                                  
    raise ValueError(msg)                                                                                                                                                                                                              
ValueError: 'id' is both an index level and a column label, which is ambiguous.   
{code}
",,apachespark,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 11 02:43:23 UTC 2021,,,,,,,,,,"0|z0oj00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/21 01:27;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31803;;;","11/Mar/21 02:43;dongjoon;This is resolved via https://github.com/apache/spark/pull/31803;;;",,,,,,,,,,,,,,,,,,
"Allow DESCRIBE FUNCTION and SHOW FUNCTIONS explain about ||  (string concatenation operator).",SPARK-34697,13363626,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,10/Mar/21 17:58,12/Dec/22 18:11,13/Jul/23 08:47,11/Mar/21 13:13,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,SQL,,,,0,,,"SHOW FUNCTIONS command doesn't show the || operator and DESCRIBE FUNCTION || also says ""Function: || not found"".",,apachespark,sarutak,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 11 13:13:48 UTC 2021,,,,,,,,,,"0|z0oim8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/21 18:06;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31800;;;","11/Mar/21 13:13;gurwls223;Fixed in https://github.com/apache/spark/pull/31800;;;",,,,,,,,,,,,,,,,,,
Fix CodegenInterpretedPlanTest to generate correct test cases,SPARK-34696,13363622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dongjoon,dongjoon,dongjoon,10/Mar/21 17:42,11/Mar/21 08:01,13/Jul/23 08:47,11/Mar/21 08:01,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,2.4.5,2.4.6,2.4.7,2.4.8,3.0.0,3.0.1,3.0.2,3.1.0,3.1.1,3.2.0,2.4.8,3.0.3,3.1.2,3.2.0,SQL,Tests,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23596,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 11 08:01:43 UTC 2021,,,,,,,,,,"0|z0oilc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/21 17:47;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31766;;;","11/Mar/21 08:01;dongjoon;Issue resolved by pull request 31766
[https://github.com/apache/spark/pull/31766];;;",,,,,,,,,,,,,,,,,,
Full outer shuffled hash join when building left side produces wrong result,SPARK-34681,13363398,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,chengsu,chengsu,chengsu,10/Mar/21 00:07,10/Mar/21 06:55,13/Jul/23 08:47,10/Mar/21 06:55,3.1.0,3.1.1,3.2.0,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,correctness,,"For full outer shuffled hash join with building hash map on left side, and having non-equal condition, the join can produce wrong result.

The root cause is `boundCondition` in `HashJoin.scala` always assumes the left side row is `streamedPlan` and right side row is `buildPlan` (streamedPlan.output ++ buildPlan.output). This is valid assumption, except for full outer + build left case.

The fix is to correct `boundCondition` in `HashJoin.scala` to handle full outer + build left case properly. See reproduce in https://issues.apache.org/jira/browse/SPARK-32399?focusedCommentId=17298414&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17298414 .",,apachespark,chengsu,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32399,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 10 06:55:56 UTC 2021,,,,,,,,,,"0|z0oh7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/21 00:19;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/31792;;;","10/Mar/21 00:19;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/31792;;;","10/Mar/21 06:55;dongjoon;Issue resolved by pull request 31792
[https://github.com/apache/spark/pull/31792];;;",,,,,,,,,,,,,,,,,
TableCapabilityCheckSuite should not inherit all tests from AnalysisSuite,SPARK-34676,13363320,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,dongjoon,dongjoon,09/Mar/21 17:01,09/Mar/21 17:04,13/Jul/23 08:47,09/Mar/21 17:04,3.0.2,3.1.2,3.2.0,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,SQL,Tests,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 09 17:04:35 UTC 2021,,,,,,,,,,"0|z0ogq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/21 17:03;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31788;;;","09/Mar/21 17:04;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31788;;;","09/Mar/21 17:04;dongjoon;Issue resolved by pull request 31788
[https://github.com/apache/spark/pull/31788];;;",,,,,,,,,,,,,,,,,
Spark app on k8s doesn't terminate without call to sparkContext.stop() method,SPARK-34674,13363273,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Kotlov,Kotlov,Kotlov,09/Mar/21 13:58,12/Dec/22 18:11,13/Jul/23 08:47,22/Apr/21 05:54,3.1.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,Kubernetes,,,,0,,,"Hello!
 I have run into a problem that if I don't call the method sparkContext.stop() explicitly, then a Spark driver process doesn't terminate even after its Main method has been completed. This behaviour is different from spark on yarn, where the manual sparkContext stopping is not required.
 It looks like, the problem is in using non-daemon threads, which prevent the driver jvm process from terminating.
 At least I see two non-daemon threads, if I don't call sparkContext.stop():
{code:java}
Thread[OkHttp kubernetes.default.svc,5,main]
Thread[OkHttp kubernetes.default.svc Writer,5,main]
{code}
Could you tell please, if it is possible to solve this problem?

Docker image from the official release of spark-3.1.1 hadoop3.2 is used.",,apachespark,cloud_fan,dongjoon,KarlManong,Kotlov,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27812,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 22 05:54:59 UTC 2021,,,,,,,,,,"0|z0ogfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/21 11:40;gurwls223;It sounds more like a question. Can you loop Spark mailing list before filing an issue? Also it would be great if you describe how you reproduced this.;;;","17/Mar/21 11:38;Kotlov;Sorry, I didn't search sufficiently for existing issues.

There is already the issue https://issues.apache.org/jira/browse/SPARK-27812, and it is marked as Fixed.

But it looks like, I have this bug in Spark 3.1.1.

I just start an spark app on Amazon EKS (Kubernetes version - 1.17) by _spark-on-k8s-operator: v1beta2-1.2.0-3.0.0_ [(https://github.com/GoogleCloudPlatform/spark-on-k8s-operator)|https://github.com/GoogleCloudPlatform/spark-on-k8s-operator]

Spark docker image is built from the official release of spark-3.1.1 hadoop3.2.

 ;;;","18/Mar/21 21:38;dongjoon;I reopen this because the affected version is not the same according to [~Kotlov]. The following is a copy of my comment at the other JIRA.

1. Does your example work with any Spark versions before? Then, what is the latest Spark version?
2. BTW, `sparkContext.stop()` or `spark.stop()` should be called by App. I don't think your use case is a legit Spark example although it might be a behavior change across Spark versions.;;;","20/Mar/21 12:43;Kotlov;Thanks for response, [~dongjoon]

I haven't tried to use the previous versions of Spark on Kubernetes. Right now my company is using Spark on AWS EMR (it uses YARN), and we have never needed to call spark.stop() there. As far as I know, Spark uses the ShutdownHook to stop SparkContext anyway before exiting the JVM. But in my example, if I understand correctly, these non-daemon threads prevent the jvm process from exiting, even after the Main method of application has been completed. And I just thought, it can be considered as a bug.
P.S. I have to migrate a large number of existing Spark batch jobs (which are owned by different people across company) from EMR to K8S, and right now it is desirable to keep the code of these jobs unchanged.;;;","23/Mar/21 02:06;dongjoon;What makes you think like the following, [~Kotlov]? I don't think Apache Spark has that kind of contracts.
{quote}As far as I know, Spark uses the ShutdownHook to stop SparkContext anyway before exiting the JVM
{quote};;;","23/Mar/21 20:20;Kotlov;I saw it in the sources. But you are right, generally I should not rely on the functionality, that is not described in the official documentation.;;;","24/Mar/21 04:19;dongjoon;Let's keep this issue open for a while to collect more opinion. We need more information for this issue report.;;;","27/Mar/21 05:40;dongjoon;I'll take a look at this during this weekend, [~Kotlov].;;;","28/Mar/21 16:23;Kotlov;Thanks for looking into this problem, [~dongjoon].

As a temporary solution suitable for my purposes, I just added a finally block that closes SparkContext to [this place|https://github.com/apache/spark/blob/066c055b526aff5d2c0ace176f8eebf0eeab6f13/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L950].;;;","28/Mar/21 17:58;dongjoon;Could you make a PR to the Apache Spark repo, [~Kotlov]?;;;","07/Apr/21 12:30;KarlManong;I have the same problem, and I submitted a PR: https://github.com/apache/spark/pull/32080;;;","07/Apr/21 13:01;apachespark;User 'KarlManong' has created a pull request for this issue:
https://github.com/apache/spark/pull/32080;;;","07/Apr/21 21:24;apachespark;User 'kotlovs' has created a pull request for this issue:
https://github.com/apache/spark/pull/32081;;;","07/Apr/21 21:24;apachespark;User 'kotlovs' has created a pull request for this issue:
https://github.com/apache/spark/pull/32081;;;","07/Apr/21 21:27;Kotlov;The fix that I currently use:  [https://github.com/apache/spark/pull/32081];;;","08/Apr/21 23:52;dongjoon;Issue resolved by pull request 32081
[https://github.com/apache/spark/pull/32081];;;","09/Apr/21 04:55;dongjoon;This is reverted due to the Hive Thrift Server UT failures.;;;","22/Apr/21 02:18;apachespark;User 'kotlovs' has created a pull request for this issue:
https://github.com/apache/spark/pull/32283;;;","22/Apr/21 02:18;apachespark;User 'kotlovs' has created a pull request for this issue:
https://github.com/apache/spark/pull/32283;;;","22/Apr/21 05:54;dongjoon;Issue resolved by pull request 32283
[https://github.com/apache/spark/pull/32283];;;"
Fix docker file for creating release,SPARK-34672,13363226,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,09/Mar/21 09:07,12/Dec/22 18:11,13/Jul/23 08:47,09/Mar/21 12:31,2.4.8,,,,,,,,,,,,,,,2.4.8,,,,Build,,,,0,,,The Dockerfile of spark-rm in branch-2.4 seems not working for now. It tries to install latest pip and setuptools which are not compatible with Python2. So it fails the docker image building.,,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 09 12:31:06 UTC 2021,,,,,,,,,,"0|z0og5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/21 09:12;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31787;;;","09/Mar/21 09:13;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31787;;;","09/Mar/21 12:31;gurwls223;Issue resolved by pull request 31787
[https://github.com/apache/spark/pull/31787];;;",,,,,,,,,,,,,,,,,
Don't use ParVector with `withExistingConf` which is not thread-safe,SPARK-34660,13362926,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,08/Mar/21 07:03,08/Mar/21 07:42,13/Jul/23 08:47,08/Mar/21 07:42,3.1.0,3.1.1,3.1.2,,,,,,,,,,,,,3.1.2,,,,Tests,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33498,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 08 07:42:07 UTC 2021,,,,,,,,,,"0|z0oeaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/21 07:07;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31775;;;","08/Mar/21 07:42;dongjoon;Issue resolved by pull request 31775
[https://github.com/apache/spark/pull/31775];;;",,,,,,,,,,,,,,,,,,
org.apache.spark.sql.DataFrameNaFunctions.replace() fails for column name having a dot,SPARK-34649,13362764,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,amsharma,amsharma,amsharma,06/Mar/21 10:09,09/Mar/21 11:47,13/Jul/23 08:47,09/Mar/21 11:47,3.1.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"Code to reproduce the issue:
{code:java}
import org.apache.spark.sql.SparkSession

object ColumnNameWithDot {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder.appName(""Simple Application"")
      .config(""spark.master"", ""local"").getOrCreate()

    spark.sparkContext.setLogLevel(""OFF"")

    import spark.implicits._
    val df = Seq((""abc"", 23), (""def"", 44), (""n/a"", 0)).toDF(""ColWith.Dot"", ""Col.2"")

    df.na.replace(""`ColWith.Dot`"", Map(""n/a"" -> ""Unknown""))
      .show()
  }
}
{code}",,amsharma,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 09 11:47:18 UTC 2021,,,,,,,,,,"0|z0odaw:",9223372036854775807,,,,,cloud_fan,,,,,,,,,,,,,,,,,,"07/Mar/21 03:12;apachespark;User 'amandeep-sharma' has created a pull request for this issue:
https://github.com/apache/spark/pull/31769;;;","07/Mar/21 03:13;apachespark;User 'amandeep-sharma' has created a pull request for this issue:
https://github.com/apache/spark/pull/31769;;;","09/Mar/21 11:47;cloud_fan;Issue resolved by pull request 31769
[https://github.com/apache/spark/pull/31769];;;",,,,,,,,,,,,,,,,,
Use CRAN URL in canonical form,SPARK-34643,13362660,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,dongjoon,dongjoon,05/Mar/21 17:27,12/Dec/22 17:35,13/Jul/23 08:47,05/Mar/21 18:09,3.2.0,,,,,,,,,,,,,,,3.1.2,3.2.0,,,R,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 05 18:09:29 UTC 2021,,,,,,,,,,"0|z0ocns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/21 17:33;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/31759;;;","05/Mar/21 18:09;dongjoon;Issue resolved by pull request 31759
[https://github.com/apache/spark/pull/31759];;;",,,,,,,,,,,,,,,,,,
TypeError in Pyspark Linear Regression docs,SPARK-34642,13362659,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,05/Mar/21 17:24,06/Mar/21 15:33,13/Jul/23 08:47,06/Mar/21 15:33,3.1.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,Documentation,ML,PySpark,,0,,,"The documentation for Linear Regression in Pyspark includes an example, which contains a TypeError:

https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html?highlight=linearregression#pyspark.ml.regression.LinearRegression

{code}
>>> abs(model.transform(test1).head().newPrediction - 1.0) < 0.001
True
>>> lr.setParams(""vector"")
Traceback (most recent call last):
    ...
TypeError: Method setParams forces keyword arguments.
{code}

I'm pretty sure we don't intend this, and it can be resolved by just changing the function call to use keyword args.

(HT Brooke Wenig for flagging this)",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 06 15:33:08 UTC 2021,,,,,,,,,,"0|z0ocnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/21 17:32;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/31760;;;","06/Mar/21 15:33;dongjoon;Issue resolved by pull request 31760
[https://github.com/apache/spark/pull/31760];;;",,,,,,,,,,,,,,,,,,
"sql method in UnresolvedAttribute, AttributeReference and Alias don't quote qualified names properly.",SPARK-34636,13362518,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,05/Mar/21 07:21,18/Mar/21 20:33,13/Jul/23 08:47,12/Mar/21 02:59,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"UnresolvedReference, AttributeReference and Alias which take qualifier don't quote qualified names properly.

One instance is reported in SPARK-34626.
Other instances are like as follows.

{code}
UnresolvedAttribute(""a`b""::""c.d""::Nil).sql
a`b.`c.d` // expected: `a``b`.`c.d`

AttributeReference(""a.b"", IntegerType)(qualifier = ""c.d""::Nil).sql
c.d.`a.b` // expected: `c.d`.`a.b`

Alias(AttributeReference(""a"", IntegerType)(), ""b.c"")(qualifier = ""d.e""::Nil).sql
`a` AS d.e.`b.c` // expected: `a` AS `d.e`.`b.c`
{code}",,apachespark,cloud_fan,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 18 20:33:44 UTC 2021,,,,,,,,,,"0|z0obsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/21 07:36;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31754;;;","12/Mar/21 02:59;cloud_fan;Issue resolved by pull request 31754
[https://github.com/apache/spark/pull/31754];;;","18/Mar/21 20:33;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31885;;;","18/Mar/21 20:33;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31885;;;",,,,,,,,,,,,,,,,
Self-join with script transformation failed to resolve attribute correctly,SPARK-34634,13362502,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,EdisonWang,EdisonWang,EdisonWang,05/Mar/21 04:41,12/Dec/22 18:10,13/Jul/23 08:47,07/Mar/21 06:56,2.4.0,2.4.1,2.4.2,2.4.3,2.4.4,2.4.5,2.4.6,2.4.7,3.0.0,3.0.1,3.0.2,3.1.0,3.1.1,,,3.1.3,3.2.0,,,SQL,,,,0,,,"To reproduce,
{code:java}
// code placeholder
create temporary view t as select * from values 0, 1, 2 as t(a);
WITH temp AS (
SELECT TRANSFORM(a) USING 'cat' AS (b string) FROM t
)
SELECT t1.b FROM temp t1 JOIN temp t2 ON t1.b = t2.b

{code}
 

Spark will throw AnalysisException

 ",,apachespark,EdisonWang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 07 03:43:28 UTC 2021,,,,,,,,,,"0|z0obow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/21 04:59;apachespark;User 'WangGuangxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/31752;;;","07/Mar/21 06:56;gurwls223;Fixed in https://github.com/apache/spark/pull/31752;;;","07/Oct/21 03:43;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34205;;;","07/Oct/21 03:43;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34205;;;",,,,,,,,,,,,,,,,
UnresolvedAttribute.sql may return an incorrect sql,SPARK-34626,13362433,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,zsxwing,zsxwing,04/Mar/21 23:25,12/Mar/21 02:59,13/Jul/23 08:47,12/Mar/21 02:59,3.0.2,3.1.1,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"UnresolvedAttribute(""a"" :: ""b"" :: Nil).sql

returns
{code:java}
`a.b`{code}
The correct one should be either a.b or
{code:java}
`a`.`b`{code}
 ",,apachespark,cloud_fan,sarutak,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 12 02:59:50 UTC 2021,,,,,,,,,,"0|z0ob9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/21 05:54;sarutak;I've found some similar issues and trying to fix them now.;;;","05/Mar/21 07:37;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31754;;;","12/Mar/21 02:59;cloud_fan;Issue resolved by pull request 31754
[https://github.com/apache/spark/pull/31754];;;",,,,,,,,,,,,,,,,,
Filter non-jar dependencies from ivy/maven coordinates,SPARK-34624,13362354,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shardulm,shardulm,shardulm,04/Mar/21 17:59,05/Mar/21 18:06,13/Jul/23 08:47,05/Mar/21 18:06,3.1.1,,,,,,,,,,,,,,,3.2.0,,,,Spark Core,,,,0,,,"Some maven artifacts define non-jar dependencies. One such example is {{hive-exec}}'s dependency on the {{pom}} of {{apache-curator}} https://repo1.maven.org/maven2/org/apache/hive/hive-exec/2.3.8/hive-exec-2.3.8.pom

Today trying to depend on such an artifact using {{--packages}} will print an error but continue without including the non-jar dependency.
{code}
1/03/04 09:46:49 ERROR SparkContext: Failed to add file:/Users/smahadik/.ivy2/jars/org.apache.curator_apache-curator-2.7.1.jar to Spark environment
java.io.FileNotFoundException: Jar /Users/shardul/.ivy2/jars/org.apache.curator_apache-curator-2.7.1.jar not found
	at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1935)
	at org.apache.spark.SparkContext.addJar(SparkContext.scala:1990)
	at org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:501)
	at org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:501)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
{code}

Doing the same using {{spark.sql(""ADD JAR ivy://org.apache.hive:hive-exec:2.3.8?exclude=org.pentaho:pentaho-aggdesigner-algorithm"")}} will cause a failure
{code}
ADD JAR /Users/smahadik/.ivy2/jars/org.apache.curator_apache-curator-2.7.1.jar
/Users/smahadik/.ivy2/jars/org.apache.curator_apache-curator-2.7.1.jar does not exist

======================
END HIVE FAILURE OUTPUT
======================

org.apache.spark.sql.execution.QueryExecutionException: /Users/smahadik/.ivy2/jars/org.apache.curator_apache-curator-2.7.1.jar does not exist
  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$runHive$1(HiveClientImpl.scala:841)
  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:291)
  at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:224)
  at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:223)
  at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:273)
  at org.apache.spark.sql.hive.client.HiveClientImpl.runHive(HiveClientImpl.scala:800)
  at org.apache.spark.sql.hive.client.HiveClientImpl.runSqlHive(HiveClientImpl.scala:787)
  at org.apache.spark.sql.hive.client.HiveClientImpl.addJar(HiveClientImpl.scala:947)
  at org.apache.spark.sql.hive.HiveSessionResourceLoader.$anonfun$addJar$1(HiveSessionStateBuilder.scala:130)
  at org.apache.spark.sql.hive.HiveSessionResourceLoader.$anonfun$addJar$1$adapted(HiveSessionStateBuilder.scala:129)
  at scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)
  at org.apache.spark.sql.hive.HiveSessionResourceLoader.addJar(HiveSessionStateBuilder.scala:129)
  at org.apache.spark.sql.execution.command.AddJarCommand.run(resources.scala:40)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
  at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3705)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3703)
  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  ... 47 elided
{code}

We should exclude these non-jar artifacts as our current dependency resolution code assume artifacts to be jars. e.g. https://github.com/apache/spark/blob/17601e014c6ccb48958d35ffb04bedeac8cfc66a/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L1215 and https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L318",,apachespark,dongjoon,shardulm,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 05 18:06:51 UTC 2021,,,,,,,,,,"0|z0oas0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/21 18:29;apachespark;User 'shardulm94' has created a pull request for this issue:
https://github.com/apache/spark/pull/31741;;;","05/Mar/21 00:00;xkrogen;Thanks for reporting this [~shardulm]!

edit: moving question to PR;;;","05/Mar/21 18:06;dongjoon;Issue resolved by pull request 31741
[https://github.com/apache/spark/pull/31741];;;",,,,,,,,,,,,,,,,,
Fix view does not capture disable hint config,SPARK-34613,13362209,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ulysses,ulysses,ulysses,04/Mar/21 03:38,05/Mar/21 04:20,13/Jul/23 08:47,05/Mar/21 04:20,3.1.1,3.2.0,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"Spark use origin text sql to store view then capture and store sql config into view metadata.

Capture config will skip some config with some prefix, e.g. `spark.sql.optimizer.` but unfortunately `spark.sql.optimizer.disableHints` is start with `spark.sql.optimizer.`.

 

We need a allow list to help capture the config.",,apachespark,cloud_fan,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 05 04:20:12 UTC 2021,,,,,,,,,,"0|z0o9vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/21 03:43;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/31732;;;","05/Mar/21 04:20;cloud_fan;Issue resolved by pull request 31732
[https://github.com/apache/spark/pull/31732];;;",,,,,,,,,,,,,,,,,,
Remove unused output in  AddJarCommand,SPARK-34608,13362091,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,03/Mar/21 12:50,05/Mar/21 05:19,13/Jul/23 08:47,05/Mar/21 05:19,3.1.1,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,Remove unused output in AddJarCommand,,angerszhuuu,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 05 05:19:39 UTC 2021,,,,,,,,,,"0|z0o95k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/21 12:59;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31725;;;","03/Mar/21 12:59;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31725;;;","05/Mar/21 05:19;dongjoon;Issue resolved by pull request 31725
[https://github.com/apache/spark/pull/31725];;;",,,,,,,,,,,,,,,,,
NewInstance.resolved should not throw malformed class name error,SPARK-34607,13362088,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,rednaxelafx,rednaxelafx,03/Mar/21 12:39,12/Dec/22 18:10,13/Jul/23 08:47,05/Mar/21 00:16,2.4.7,3.0.2,3.1.1,,,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,SQL,,,,0,,,"I'd like to seek for community help on fixing this issue:

Related to SPARK-34596, one of our users had hit an issue with {{ExpressionEncoder}} when running Spark code in a Scala REPL, where {{NewInstance.resolved}} was throwing {{""Malformed class name""}} error, with the following kind of stack trace:
{code}
java.lang.InternalError: Malformed class name
 	at java.lang.Class.getSimpleBinaryName(Class.java:1450)
 	at java.lang.Class.isMemberClass(Class.java:1433)
 	at org.apache.spark.sql.catalyst.expressions.objects.NewInstance.resolved$lzycompute(objects.scala:447)
 	at org.apache.spark.sql.catalyst.expressions.objects.NewInstance.resolved(objects.scala:441)
 	at org.apache.spark.sql.catalyst.analysis.Analyzer.resolveExpressionBottomUp(Analyzer.scala:1935)
 	...
 Caused by: sbt.ForkMain$ForkError: java.lang.StringIndexOutOfBoundsException: String index out of range: -83
 	at java.lang.String.substring(String.java:1931)
 	at java.lang.Class.getSimpleBinaryName(Class.java:1448)
 	at java.lang.Class.isMemberClass(Class.java:1433)
 	at org.apache.spark.sql.catalyst.expressions.objects.NewInstance.resolved$lzycompute(objects.scala:447)
 	at org.apache.spark.sql.catalyst.expressions.objects.NewInstance.resolved(objects.scala:441)
  ...
{code}

The most important point in the stack trace is this:
{code}
java.lang.InternalError: Malformed class name
 	at java.lang.Class.getSimpleBinaryName(Class.java:1450)
 	at java.lang.Class.isMemberClass(Class.java:1433)
{code}
The most common way to hit the {{""Malformed class name""}} issue in Spark is via {{java.lang.Class.getSimpleName}}. But as this stack trace demonstrates, it can happen via other code paths from the JDK as well.

If we want to fix it in a similar fashion as {{Utils.getSimpleName}}, we'd have to emulate {{java.lang.Class.isMemberClass}} in Spark's {{Utils}}, and then use it in the {{NewInstance.resolved}} code path.

Here's a reproducer test case (in diff form against Spark master's {{ExpressionEncoderSuite}} ), which uses explicit nested classes to emulate the code structure that'd be generated by Scala's REPL:
(the level of nesting can be further reduced and still reproduce the issue)
{code}
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoderSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoderSuite.scala
index 2635264..fd1b23d 100644
--- a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoderSuite.scala
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/encoders/ExpressionEncoderSuite.scala
@@ -217,6 +217,95 @@ class ExpressionEncoderSuite extends CodegenInterpretedPlanTest with AnalysisTes
       ""nested Scala class should work"")
   }
 
+  object OuterLevelWithVeryVeryVeryLongClassName1 {
+    object OuterLevelWithVeryVeryVeryLongClassName2 {
+      object OuterLevelWithVeryVeryVeryLongClassName3 {
+        object OuterLevelWithVeryVeryVeryLongClassName4 {
+          object OuterLevelWithVeryVeryVeryLongClassName5 {
+            object OuterLevelWithVeryVeryVeryLongClassName6 {
+              object OuterLevelWithVeryVeryVeryLongClassName7 {
+                object OuterLevelWithVeryVeryVeryLongClassName8 {
+                  object OuterLevelWithVeryVeryVeryLongClassName9 {
+                    object OuterLevelWithVeryVeryVeryLongClassName10 {
+                      object OuterLevelWithVeryVeryVeryLongClassName11 {
+                        object OuterLevelWithVeryVeryVeryLongClassName12 {
+                          object OuterLevelWithVeryVeryVeryLongClassName13 {
+                            object OuterLevelWithVeryVeryVeryLongClassName14 {
+                              object OuterLevelWithVeryVeryVeryLongClassName15 {
+                                object OuterLevelWithVeryVeryVeryLongClassName16 {
+                                  object OuterLevelWithVeryVeryVeryLongClassName17 {
+                                    object OuterLevelWithVeryVeryVeryLongClassName18 {
+                                      object OuterLevelWithVeryVeryVeryLongClassName19 {
+                                        object OuterLevelWithVeryVeryVeryLongClassName20 {
+                                          case class MalformedNameExample2(x: Int)
+                                        }
+                                      }
+                                    }
+                                  }
+                                }
+                              }
+                            }
+                          }
+                        }
+                      }
+                    }
+                  }
+                }
+              }
+            }
+          }
+        }
+      }
+    }
+  }
+
+  {
+    OuterScopes.addOuterScope(
+      OuterLevelWithVeryVeryVeryLongClassName1
+        .OuterLevelWithVeryVeryVeryLongClassName2
+        .OuterLevelWithVeryVeryVeryLongClassName3
+        .OuterLevelWithVeryVeryVeryLongClassName4
+        .OuterLevelWithVeryVeryVeryLongClassName5
+        .OuterLevelWithVeryVeryVeryLongClassName6
+        .OuterLevelWithVeryVeryVeryLongClassName7
+        .OuterLevelWithVeryVeryVeryLongClassName8
+        .OuterLevelWithVeryVeryVeryLongClassName9
+        .OuterLevelWithVeryVeryVeryLongClassName10
+        .OuterLevelWithVeryVeryVeryLongClassName11
+        .OuterLevelWithVeryVeryVeryLongClassName12
+        .OuterLevelWithVeryVeryVeryLongClassName13
+        .OuterLevelWithVeryVeryVeryLongClassName14
+        .OuterLevelWithVeryVeryVeryLongClassName15
+        .OuterLevelWithVeryVeryVeryLongClassName16
+        .OuterLevelWithVeryVeryVeryLongClassName17
+        .OuterLevelWithVeryVeryVeryLongClassName18
+        .OuterLevelWithVeryVeryVeryLongClassName19
+        .OuterLevelWithVeryVeryVeryLongClassName20)
+    encodeDecodeTest(
+      OuterLevelWithVeryVeryVeryLongClassName1
+        .OuterLevelWithVeryVeryVeryLongClassName2
+        .OuterLevelWithVeryVeryVeryLongClassName3
+        .OuterLevelWithVeryVeryVeryLongClassName4
+        .OuterLevelWithVeryVeryVeryLongClassName5
+        .OuterLevelWithVeryVeryVeryLongClassName6
+        .OuterLevelWithVeryVeryVeryLongClassName7
+        .OuterLevelWithVeryVeryVeryLongClassName8
+        .OuterLevelWithVeryVeryVeryLongClassName9
+        .OuterLevelWithVeryVeryVeryLongClassName10
+        .OuterLevelWithVeryVeryVeryLongClassName11
+        .OuterLevelWithVeryVeryVeryLongClassName12
+        .OuterLevelWithVeryVeryVeryLongClassName13
+        .OuterLevelWithVeryVeryVeryLongClassName14
+        .OuterLevelWithVeryVeryVeryLongClassName15
+        .OuterLevelWithVeryVeryVeryLongClassName16
+        .OuterLevelWithVeryVeryVeryLongClassName17
+        .OuterLevelWithVeryVeryVeryLongClassName18
+        .OuterLevelWithVeryVeryVeryLongClassName19
+        .OuterLevelWithVeryVeryVeryLongClassName20
+        .MalformedNameExample2(42),
+      ""deeply nested Scala class should work"")
+  }
+
   productTest(PrimitiveData(1, 1, 1, 1, 1, 1, true))
 
   productTest(
{code}

The reason why the test case above is so convoluted is in the way Scala generates the class name for nested classes. In general, Scala generates a class name for a nested class by inserting the dollar-sign ( {{$}} ) in between each level of class nesting. The problem is that this format can concatenate into a very long string that goes beyond certain limits, so Scala will change the class name format beyond certain length threshold.

For the example above, we can see that the first two levels of class nesting have class names that look like this:
{code}
org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$OuterLevelWithVeryVeryVeryLongClassName1$
org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$OuterLevelWithVeryVeryVeryLongClassName1$OuterLevelWithVeryVeryVeryLongClassName2$
{code}
If we leave out the fact that Scala uses a dollar-sign ( {{$}} ) suffix for the class name of the companion object, {{OuterLevelWithVeryVeryVeryLongClassName1}}'s full name is a prefix (substring) of {{OuterLevelWithVeryVeryVeryLongClassName2}}.

But if we keep going deeper into the levels of nesting, you'll find names that look like:
{code}
org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$OuterLevelWithVeryVeryVeryLongClassNam$$$$2a1321b953c615695d7442b2adb1$$$$ryVeryLongClassName8$OuterLevelWithVeryVeryVeryLongClassName9$OuterLevelWithVeryVeryVeryLongClassName10$
org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$OuterLevelWithVeryVeryVeryLongClassNam$$$$2a1321b953c615695d7442b2adb1$$$$ryVeryLongClassName8$OuterLevelWithVeryVeryVeryLongClassName9$OuterLevelWithVeryVeryVeryLongClassName10$OuterLevelWithVeryVeryVeryLongClassName11$
org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$OuterLevelWithVeryVeryVeryLongClassNam$$$$85f068777e7ecf112afcbe997d461b$$$$VeryLongClassName11$OuterLevelWithVeryVeryVeryLongClassName12$
org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$OuterLevelWithVeryVeryVeryLongClassNam$$$$85f068777e7ecf112afcbe997d461b$$$$VeryLongClassName11$OuterLevelWithVeryVeryVeryLongClassName12$OuterLevelWithVeryVeryVeryLongClassName13$
org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$OuterLevelWithVeryVeryVeryLongClassNam$$$$85f068777e7ecf112afcbe997d461b$$$$VeryLongClassName11$OuterLevelWithVeryVeryVeryLongClassName12$OuterLevelWithVeryVeryVeryLongClassName13$OuterLevelWithVeryVeryVeryLongClassName14$
org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$OuterLevelWithVeryVeryVeryLongClassNam$$$$5f7ad51804cb1be53938ea804699fa$$$$VeryLongClassName14$OuterLevelWithVeryVeryVeryLongClassName15$
org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$OuterLevelWithVeryVeryVeryLongClassNam$$$$5f7ad51804cb1be53938ea804699fa$$$$VeryLongClassName14$OuterLevelWithVeryVeryVeryLongClassName15$OuterLevelWithVeryVeryVeryLongClassName16$
org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$OuterLevelWithVeryVeryVeryLongClassNam$$$$5f7ad51804cb1be53938ea804699fa$$$$VeryLongClassName14$OuterLevelWithVeryVeryVeryLongClassName15$OuterLevelWithVeryVeryVeryLongClassName16$OuterLevelWithVeryVeryVeryLongClassName17$
org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$OuterLevelWithVeryVeryVeryLongClassNam$$$$69b54f16b1965a31e88968df1a58d8$$$$VeryLongClassName17$OuterLevelWithVeryVeryVeryLongClassName18$
org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$OuterLevelWithVeryVeryVeryLongClassNam$$$$69b54f16b1965a31e88968df1a58d8$$$$VeryLongClassName17$OuterLevelWithVeryVeryVeryLongClassName18$OuterLevelWithVeryVeryVeryLongClassName19$
org.apache.spark.sql.catalyst.encoders.ExpressionEncoderSuite$OuterLevelWithVeryVeryVeryLongClassNam$$$$69b54f16b1965a31e88968df1a58d8$$$$VeryLongClassName17$OuterLevelWithVeryVeryVeryLongClassName18$OuterLevelWithVeryVeryVeryLongClassName19$OuterLevelWithVeryVeryVeryLongClassName20$
{code}
with a hash code in the middle and various levels of nesting omitted.

The {{java.lang.Class.isMemberClass}} method is implemented in JDK8u as:
http://hg.openjdk.java.net/jdk8u/jdk8u/jdk/file/tip/src/share/classes/java/lang/Class.java#l1425
{code:java}
    /**
     * Returns {@code true} if and only if the underlying class
     * is a member class.
     *
     * @return {@code true} if and only if this class is a member class.
     * @since 1.5
     */
    public boolean isMemberClass() {
        return getSimpleBinaryName() != null && !isLocalOrAnonymousClass();
    }

    /**
     * Returns the ""simple binary name"" of the underlying class, i.e.,
     * the binary name without the leading enclosing class name.
     * Returns {@code null} if the underlying class is a top level
     * class.
     */
    private String getSimpleBinaryName() {
        Class<?> enclosingClass = getEnclosingClass();
        if (enclosingClass == null) // top level class
            return null;
        // Otherwise, strip the enclosing class' name
        try {
            return getName().substring(enclosingClass.getName().length());
        } catch (IndexOutOfBoundsException ex) {
            throw new InternalError(""Malformed class name"", ex);
        }
    }
{code}
and the problematic code is {{getName().substring(enclosingClass.getName().length())}} -- if a class's enclosing class's full name is *longer* than the nested class's full name, this logic would end up going out of bounds.

The bug has been fixed in JDK9 by https://bugs.java.com/bugdatabase/view_bug.do?bug_id=8057919 , but still exists in the latest JDK8u release. So from the Spark side we'd need to do something to avoid hitting this problem.",,apachespark,kiszk,maropu,rednaxelafx,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 06 20:52:33 UTC 2021,,,,,,,,,,"0|z0o94w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/21 12:48;gurwls223;cc [~kiszk], [~maropu] and [~viirya] in case you're interested in this.;;;","03/Mar/21 14:38;maropu;okay, I'll dig into this tomorrow.;;;","04/Mar/21 01:56;gurwls223;Thank you [~maropu].;;;","04/Mar/21 04:38;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31733;;;","04/Mar/21 04:39;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31733;;;","05/Mar/21 00:12;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31745;;;","05/Mar/21 00:16;maropu;Resolved by https://github.com/apache/spark/pull/31733;;;","05/Mar/21 00:20;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31746;;;","05/Mar/21 00:30;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31747;;;","06/Mar/21 16:57;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31764;;;","06/Mar/21 16:58;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31764;;;","06/Mar/21 20:52;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31766;;;","06/Mar/21 20:52;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31766;;;",,,,,,,
INSERT INTO OVERWRITE doesn't support partition columns containing dot for DSv2,SPARK-34599,13361973,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,02/Mar/21 23:43,04/Mar/21 07:13,13/Jul/23 08:47,04/Mar/21 07:13,3.0.2,3.1.1,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"ResolveInsertInto.staticDeleteExpression is using an incorrect method to create UnresolvedAttribute, which makes it not support partition columns containing dot.",,apachespark,cloud_fan,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 04 07:13:35 UTC 2021,,,,,,,,,,"0|z0o8fc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/21 23:54;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/31713;;;","04/Mar/21 07:13;cloud_fan;Issue resolved by pull request 31713
[https://github.com/apache/spark/pull/31713];;;",,,,,,,,,,,,,,,,,,
NewInstance.doGenCode should not throw malformed class name error,SPARK-34596,13361836,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rednaxelafx,rednaxelafx,rednaxelafx,02/Mar/21 11:07,12/Dec/22 18:10,13/Jul/23 08:47,03/Mar/21 03:24,2.4.7,3.0.2,3.1.0,,,,,,,,,,,,,2.4.8,3.0.3,3.1.2,3.2.0,SQL,,,,0,,,"Similar to SPARK-32238 and SPARK-32999, the use of {{java.lang.Class.getSimpleName}} in {{NewInstance.doGenCode}} is problematic because Scala classes may trigger {{java.lang.InternalError: Malformed class name}}.

This happens more often when using nested classes in Scala (or declaring classes in Scala REPL which implies class nesting).

Note that on newer versions of JDK the underlying malformed class name no longer reproduces (fixed in the JDK by https://bugs.java.com/bugdatabase/view_bug.do?bug_id=8057919), so it's less of an issue there. But on JDK8u this problem still exists so we still have to fix it.",,apachespark,rednaxelafx,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 19 00:26:29 UTC 2021,,,,,,,,,,"0|z0o7l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/21 11:27;apachespark;User 'rednaxelafx' has created a pull request for this issue:
https://github.com/apache/spark/pull/31709;;;","03/Mar/21 03:24;gurwls223;Fixed in https://github.com/apache/spark/pull/31709;;;","06/Mar/21 16:55;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31764;;;","06/Mar/21 20:51;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31766;;;","19/Mar/21 00:25;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31888;;;","19/Mar/21 00:26;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31888;;;",,,,,,,,,,,,,,
DPP support RLIKE,SPARK-34595,13361813,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chaojun,chaojun,chaojun,02/Mar/21 10:01,06/Mar/21 11:33,13/Jul/23 08:47,06/Mar/21 11:33,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"Support this case:
{code:sql}
SELECT date_id, product_id FROM fact_sk f
JOIN dim_store s
ON f.store_id = s.store_id WHERE s.country RLIKE '[DE|US]'
{code}",,apachespark,chaojun,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 06 11:33:12 UTC 2021,,,,,,,,,,"0|z0o7g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/21 11:39;apachespark;User 'chaojun-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31710;;;","03/Mar/21 06:40;apachespark;User 'chaojun-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31720;;;","03/Mar/21 06:40;apachespark;User 'chaojun-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31720;;;","03/Mar/21 06:58;apachespark;User 'chaojun-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31722;;;","03/Mar/21 06:59;apachespark;User 'chaojun-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31722;;;","03/Mar/21 10:13;apachespark;User 'chaojun-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31710;;;","03/Mar/21 10:25;apachespark;User 'chaojun-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31720;;;","06/Mar/21 11:33;maropu;Resolved by https://github.com/apache/spark/pull/31722;;;",,,,,,,,,,,,
"When insert into a partition table with a illegal partition value, DSV2 behavior different as others",SPARK-34584,13361631,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,angerszhuuu,angerszhuuu,01/Mar/21 14:52,12/Dec/22 18:10,13/Jul/23 08:47,04/Mar/21 02:56,3.1.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"With below UT 
{code:java}
  test(""test insert to partition with wrong value"") {
    withTable(""t"") {
      val binaryStr = ""Spark SQL""
      val binaryHexStr = Hex.hex(UTF8String.fromString(binaryStr).getBytes).toString
      sql(""CREATE TABLE t(name STRING, part DATE) USING PARQUET PARTITIONED BY (part)"")
      sql(s""INSERT INTO t PARTITION(part = X'$binaryHexStr') VALUES('a')"")
      sql(""SELECT * FROM t"").show()
    }
  }
{code}
Result :
{code:java}
[info] DSV2SQLInsertTestSuite:
21:35:32.369 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
+----+----+
|name|part|
+----+----+
|   a|null|
+----+----+
[info] - test insert to partition with wrong value (4 seconds, 41 milliseconds)
21:35:37.639 WARN org.apache.spark.sql.DSV2SQLInsertTestSuite:===== POSSIBLE THREAD LEAK IN SUITE o.a.s.sql.DSV2SQLInsertTestSuite, thread names: rpc-boss-3-1, shuffle-boss-6-1 =====
[info] FileSourceSQLInsertTestSuite:
[info] - test insert to partition with wrong value *** FAILED *** (200 milliseconds)
[info]   java.time.DateTimeException: Cannot cast X'537061726B2053514C' to DateType.
[info]   at org.apache.spark.sql.catalyst.util.DateTimeUtils$.$anonfun$stringToDateAnsi$1(DateTimeUtils.scala:471)
[info]   at scala.Option.getOrElse(Option.scala:189)
[info]   at org.apache.spark.sql.catalyst.util.DateTimeUtils$.stringToDateAnsi(DateTimeUtils.scala:471)
[info]   at org.apache.spark.sql.catalyst.expressions.CastBase.$anonfun$castToDate$2(Cast.scala:509)
[info]   at org.apache.spark.sql.catalyst.expressions.CastBase.$anonfun$castToDate$2$adapted(Cast.scala:509)
[info]   at org.apache.spark.sql.catalyst.expressions.CastBase.buildCast(Cast.scala:301)
[info]   at org.apache.spark.sql.catalyst.expressions.CastBase.$anonfun$castToDate$1(Cast.scala:509)
[info]   at org.apache.spark.sql.catalyst.expressions.CastBase.nullSafeEval(Cast.scala:850)
[info]   at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:482)
[info]   at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:66)
[info]   at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$$anonfun$apply$1$$anonfun$applyOrElse$1.applyOrElse(expressions.scala:54)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:316)
[info]   at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:316)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:321)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:406)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:242)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:404)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:357)
[info]   at
{code}",,angerszhuuu,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 04 02:56:18 UTC 2021,,,,,,,,,,"0|z0o6bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/21 15:34;Gengliang.Wang;The difference is from https://github.com/apache/spark/pull/27597, which only covers the V1 write path.
We should use ANSI cast in `ResolveInsertInto` for the same case as well.
cc [~Ngone51] [~cloud_fan];;;","03/Mar/21 14:06;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31726;;;","03/Mar/21 14:06;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31726;;;","04/Mar/21 02:56;gurwls223;Issue resolved by pull request 31726
[https://github.com/apache/spark/pull/31726];;;",,,,,,,,,,,,,,,,
BoundAttribute issue after optimization by BooleanSimplification and PushFoldableIntoBranches,SPARK-34581,13361568,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,yumwang,yumwang,01/Mar/21 10:21,02/May/21 05:53,13/Jul/23 08:47,02/May/21 05:53,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"BoundAttribute issue will occur after optimization by BooleanSimplification and PushFoldableIntoBranches. How to reproduce this issue:
{code:scala}
spark.sql(""CREATE TABLE t1 (a INT, b INT) USING parquet"")
spark.sql(""CREATE TABLE t2 (a INT, b INT) USING parquet"")

      spark.sql(
        """"""
          |SELECT cnt,
          |       NOT ( buyer_id ) AS buyer_id2
          |FROM   (SELECT t1.a IS NOT NULL AS buyer_id,
          |               Count(*)         AS cnt
          |        FROM   t1
          |               INNER JOIN t2
          |                       ON t1.a = t2.a
          |        GROUP  BY 1) t 
          |"""""".stripMargin).collect()
{code}
{noformat}
Couldn't find a#4 in [CASE WHEN isnotnull(a#4) THEN 1 ELSE 2 END#10,count(1)#3L]
java.lang.IllegalStateException: Couldn't find a#4 in [CASE WHEN isnotnull(a#4) THEN 1 ELSE 2 END#10,count(1)#3L]
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:316)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:321)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:242)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:404)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:357)
{noformat}
Another case:
{code:scala}
spark.sql(
  """"""
    |SELECT cnt,
    |       CASE WHEN ( buyer_id = 2 AND cnt > 3 ) THEN 2 ELSE 3 END AS buyer_id2
    |FROM   (SELECT CASE WHEN t1.a IS NOT NULL THEN 1 ELSE 2 END AS buyer_id, Count(*) AS cnt
    |        FROM   t1 INNER JOIN t2 ON t1.a = t2.a
    |        GROUP  BY 1) t
    |"""""".stripMargin).collect()
{code}",,apachespark,cloud_fan,dongjoon,petertoth,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 02 05:53:31 UTC 2021,,,,,,,,,,"0|z0o5xk:",9223372036854775807,,,,,,,,,,,,,3.2.0,,,,,,,,,,"02/Mar/21 12:20;yumwang;cc [~Ngone51];;;","21/Mar/21 09:39;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31913;;;","21/Mar/21 09:40;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31913;;;","19/Apr/21 04:58;cloud_fan;Issue resolved by pull request 31913
[https://github.com/apache/spark/pull/31913];;;","29/Apr/21 12:40;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/32396;;;","29/Apr/21 12:41;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/32396;;;","02/May/21 05:53;cloud_fan;Issue resolved by pull request 32396
[https://github.com/apache/spark/pull/32396];;;",,,,,,,,,,,,,
Fix wrong test in SQLQuerySuite,SPARK-34579,13361561,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,angerszhu,angerszhu,vvin,01/Mar/21 09:42,03/Mar/21 01:35,13/Jul/23 08:47,03/Mar/21 01:35,3.1.1,,,,,,,,,,,,,,,3.2.0,,,,SQL,Tests,,,0,,,Fix wrong test in SQLQuerySuite,,dongjoon,maropu,vvin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 03 01:35:53 UTC 2021,,,,,,,,,,"0|z0o5w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/21 12:01;dongjoon;Hi, [~vvin] and [~angerszhu]. 
I switched the content of SPARK-34569 and SPARK-34579 to mitigate the following commit mistake.
- https://github.com/apache/spark/commit/d574308864816b74372346b1f0b497f2e71c2000

Sorry for your inconvenience.;;;","02/Mar/21 00:46;maropu;Could we resolve this? Btw, [~angerszhu], please describe more in the description cuz I don't tell what this ticket means.;;;","03/Mar/21 01:35;dongjoon;Thank you, [~maropu]. Yes, this is resolved.;;;",,,,,,,,,,,,,,,,,
CreateTableAsSelect should have metrics update too,SPARK-34567,13361407,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,28/Feb/21 06:09,04/Mar/21 12:43,13/Jul/23 08:47,04/Mar/21 12:43,3.1.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"For command `CreateTableAsSelect` we use `InsertIntoHiveTable`, `InsertIntoHadoopFsRelationCommand` to insert data.
We will update metrics of `InsertIntoHiveTable`, `InsertIntoHadoopFsRelationCommand` in `FileFormatWriter.write()`, but we only show CreateTableAsSelectCommand in WebUI SQL Tab.
We need to update `CreateTableAsSelectCommand`'s metrics too.",,angerszhuuu,apachespark,cloud_fan,maropu,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 04 12:43:33 UTC 2021,,,,,,,,,,"0|z0o4xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/21 06:09;angerszhuuu;raise a pr soon;;;","28/Feb/21 07:44;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31679;;;","28/Feb/21 07:44;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31679;;;","28/Feb/21 13:04;maropu;Please fill the description.;;;","04/Mar/21 12:43;cloud_fan;Issue resolved by pull request 31679
[https://github.com/apache/spark/pull/31679];;;",,,,,,,,,,,,,,,
Collapse Window nodes with Project between them,SPARK-34565,13361394,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tanelk,tanelk,tanelk,27/Feb/21 22:44,21/Jun/21 13:11,13/Jul/23 08:47,21/Jun/21 13:11,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"The CollapseWindow optimizer rule can be improved to also collapse Window nodes, that have a Project between them. This sort of Window - Project - Window chains will happen when chaining the dataframe.withColumn calls.",,apachespark,maropu,tanelk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 21 13:11:47 UTC 2021,,,,,,,,,,"0|z0o4uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/21 23:01;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31677;;;","27/Feb/21 23:02;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31677;;;","21/Jun/21 13:11;maropu;Resolved by https://github.com/apache/spark/pull/31677;;;",,,,,,,,,,,,,,,,,
Cannot join datasets of SHOW TABLES,SPARK-34560,13361355,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,27/Feb/21 12:34,01/Mar/21 18:32,13/Jul/23 08:47,01/Mar/21 18:32,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"The example portraits the issue:

{code:scala}
scala> sql(""CREATE NAMESPACE ns1"")
res8: org.apache.spark.sql.DataFrame = []

scala> sql(""CREATE NAMESPACE ns2"")
res9: org.apache.spark.sql.DataFrame = []

scala> sql(""CREATE TABLE ns1.tbl1 (c INT)"")
res10: org.apache.spark.sql.DataFrame = []

scala> sql(""CREATE TABLE ns2.tbl2 (c INT)"")
res11: org.apache.spark.sql.DataFrame = []

scala> val show1 = sql(""SHOW TABLES IN ns1"")
show1: org.apache.spark.sql.DataFrame = [namespace: string, tableName: string ... 1 more field]

scala> val show2 = sql(""SHOW TABLES IN ns2"")
show2: org.apache.spark.sql.DataFrame = [namespace: string, tableName: string ... 1 more field]

scala> show1.show
+---------+---------+-----------+
|namespace|tableName|isTemporary|
+---------+---------+-----------+
|      ns1|     tbl1|      false|
+---------+---------+-----------+


scala> show2.show
+---------+---------+-----------+
|namespace|tableName|isTemporary|
+---------+---------+-----------+
|      ns2|     tbl2|      false|
+---------+---------+-----------+


scala> show1.join(show2).where(show1(""tableName"") =!= show2(""tableName"")).show
org.apache.spark.sql.AnalysisException: Column tableName#17 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(""a"").join(df.as(""b""), $""a.id"" > $""b.id"")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.
  at org.apache.spark.sql.execution.analysis.DetectAmbiguousSelfJoin$.apply(DetectAmbiguousSelfJoin.scala:157)
{code}
",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 01 18:32:51 UTC 2021,,,,,,,,,,"0|z0o4m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/21 13:04;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31675;;;","27/Feb/21 13:04;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31675;;;","01/Mar/21 18:32;cloud_fan;Issue resolved by pull request 31675
[https://github.com/apache/spark/pull/31675];;;",,,,,,,,,,,,,,,,,
warehouse path should be resolved ahead of populating and use,SPARK-34558,13361311,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,27/Feb/21 05:57,21/May/21 16:26,13/Jul/23 08:47,02/Mar/21 15:14,3.0.2,3.1.2,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"Currently, the warehouse path gets fully qualified in the caller side for creating a database, table, partition, etc. An unqualified path is populated into Spark and Hadoop confs, which leads to inconsistent API behaviors.  We should make it qualified ahead.


When the value is a relative path `spark.sql.warehouse.dir=lakehouse`, for example.

If the default database is absent at runtime, the app fails with

{code:java}
Caused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:./datalake
	at org.apache.hadoop.fs.Path.initialize(Path.java:263)
	at org.apache.hadoop.fs.Path.<init>(Path.java:254)
	at org.apache.hadoop.hive.metastore.Warehouse.getDnsPath(Warehouse.java:133)
	at org.apache.hadoop.hive.metastore.Warehouse.getDnsPath(Warehouse.java:137)
	at org.apache.hadoop.hive.metastore.Warehouse.getWhRoot(Warehouse.java:150)
	at org.apache.hadoop.hive.metastore.Warehouse.getDefaultDatabasePath(Warehouse.java:163)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB_core(HiveMetaStore.java:636)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:655)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)
	... 73 more
{code}

If the default database is present at runtime, the app can work with it, and if we create a database, it gets fully qualified, for example


{code:sql}
spark-sql> create database test2 location 'datalake';
21/02/26 21:52:57 WARN ObjectStore: Failed to get database test2, returning NoSuchObjectException
Time taken: 0.052 seconds
spark-sql> desc database test;
Database Name	test
Comment
Location	file:/Users/kentyao/Downloads/spark/spark-3.2.0-SNAPSHOT-bin-20210226/datalake/test.db
Owner	kentyao
Time taken: 0.023 seconds, Fetched 4 row(s)
{code}

Another thing is that the log becomes nubilous, for example.

{code:java}
21/02/27 13:54:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('datalake').
21/02/27 13:54:17 INFO SharedState: Warehouse path is 'datalake'.
{code}



",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 21 16:26:06 UTC 2021,,,,,,,,,,"0|z0o4cg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/21 06:21;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31671;;;","02/Mar/21 15:14;cloud_fan;Issue resolved by pull request 31671
[https://github.com/apache/spark/pull/31671];;;","17/Mar/21 14:59;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31868;;;","21/May/21 14:28;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/32622;;;","21/May/21 14:28;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/32622;;;","21/May/21 16:26;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32623;;;",,,,,,,,,,,,,,
Checking duplicate static partition columns doesn't respect case sensitive conf,SPARK-34556,13361293,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,27/Feb/21 01:10,12/Dec/22 18:10,13/Jul/23 08:47,03/Mar/21 02:36,2.4.7,3.0.2,3.1.1,,,,,,,,,,,,,3.0.3,3.1.2,,,SQL,,,,0,,,"When parsing the partition spec, Spark will call `org.apache.spark.sql.catalyst.parser.ParserUtils.checkDuplicateKeys` to check if there are duplicate partition column names in the list. But this method is always case sensitive and doesn't discover duplicate partition column names when using different cases.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 03 02:36:56 UTC 2021,,,,,,,,,,"0|z0o48g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/21 01:17;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/31669;;;","27/Feb/21 01:17;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/31669;;;","03/Mar/21 01:03;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/31714;;;","03/Mar/21 01:04;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/31714;;;","03/Mar/21 02:36;gurwls223;Issue resolved by pull request 31714
[https://github.com/apache/spark/pull/31714];;;",,,,,,,,,,,,,,,
Resolve metadata output from DataFrame,SPARK-34555,13361271,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,karenfeng,karenfeng,karenfeng,26/Feb/21 20:20,03/Mar/21 14:16,13/Jul/23 08:47,03/Mar/21 14:16,3.1.0,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"Today, we can't resolve a metadata column from a Spark dataframe from Scala with {{df.col(""metadataColName"")}}. This is because metadata output is only used during {{resolveChildren}} (used during SQL resolution); it should likely also be used by {{resolve}} (used during Scala resolution).",,apachespark,cloud_fan,karenfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 03 14:16:03 UTC 2021,,,,,,,,,,"0|z0o43k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/21 20:21;karenfeng;I'm working on this.;;;","26/Feb/21 21:37;apachespark;User 'karenfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/31668;;;","26/Feb/21 21:38;apachespark;User 'karenfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/31668;;;","03/Mar/21 14:16;cloud_fan;Issue resolved by pull request 31668
[https://github.com/apache/spark/pull/31668];;;",,,,,,,,,,,,,,,,
"generate-contributors.py, releaseutils.py and translate-contributors.py are broken",SPARK-34551,13361135,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,26/Feb/21 08:36,12/Dec/22 18:10,13/Jul/23 08:47,26/Feb/21 11:20,3.2.0,,,,,,,,,,,,,,,3.1.2,,,,Project Infra,,,,0,,,"This is broken after SPARK-32719. {{raw_input}} does not exist in {{releaseutils}} but in Python 2:

{code}
Traceback (most recent call last):
  File ""dev/create-release/generate-contributors.py"", line 25, in <module>
    from releaseutils import tag_exists, raw_input, get_commits, yesOrNoPrompt, get_date, \
ImportError: cannot import name 'raw_input' from 'releaseutils' (/.../spark/dev/create-release/releaseutils.py)
{code}

In addition, these scripts look only working with Python 2. We should drop Python 2.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 26 11:20:17 UTC 2021,,,,,,,,,,"0|z0o39c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/21 09:12;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/31660;;;","26/Feb/21 11:20;gurwls223;Issue resolved by pull request 31660
[https://github.com/apache/spark/pull/31660];;;",,,,,,,,,,,,,,,,,,
Resolve using child metadata attributes as fallback,SPARK-34547,13361062,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,karenfeng,karenfeng,karenfeng,25/Feb/21 23:58,02/Mar/21 09:27,13/Jul/23 08:47,02/Mar/21 09:27,3.1.0,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"Today, child expressions may be resolved based on ""real"" or metadata output attributes. If a ""real"" attribute and a metadata attribute have the same name, we should prefer the real one during resolution. This resolution fails today, although the user may not be aware of the metadata.",,apachespark,cloud_fan,karenfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 02 09:27:55 UTC 2021,,,,,,,,,,"0|z0o2t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/21 23:58;karenfeng;I'm working on this.;;;","26/Feb/21 00:11;apachespark;User 'karenfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/31654;;;","02/Mar/21 09:27;cloud_fan;Issue resolved by pull request 31654
[https://github.com/apache/spark/pull/31654];;;",,,,,,,,,,,,,,,,,
PySpark Python UDF return inconsistent results when applying 2 UDFs with different return type to 2 columns together,SPARK-34545,13361007,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,petertoth,Baohe Zhang,Baohe Zhang,25/Feb/21 17:29,04/May/21 12:46,13/Jul/23 08:47,08/Mar/21 01:13,3.0.0,,,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,PySpark,SQL,,,0,correctness,,"Python UDF returns inconsistent results between evaluating 2 columns together and evaluating one by one.

The issue occurs after we upgrading to spark3, so seems it doesn't exist in spark2.

How to reproduce it?

{code:python}
df = spark.createDataFrame([([(1.0, ""1""), (1.0, ""2""), (1.0, ""3"")], [(1, ""1""), (1, ""2""), (1, ""3"")]), ([(2.0, ""1""), (2.0, ""2""), (2.0, ""3"")], [(2, ""1""), (2, ""2""), (2, ""3"")]), ([(3.1, ""1""), (3.1, ""2""), (3.1, ""3"")], [(3, ""1""), (3, ""2""), (3, ""3"")])], ['c1', 'c2'])

from pyspark.sql.functions import udf
from pyspark.sql.types import *

def getLastElementWithTimeMaster(data_type):
    def getLastElementWithTime(list_elm):
        # x should be a list of (val, time)
        y = sorted(list_elm, key=lambda x: x[1]) # default is ascending
        return y[-1][0]
    return udf(getLastElementWithTime, data_type)

# Add 2 columns whcih apply Python UDF
df = df.withColumn(""c3"", getLastElementWithTimeMaster(DoubleType())(""c1""))
df = df.withColumn(""c4"", getLastElementWithTimeMaster(IntegerType())(""c2""))

# Show the results
df.select(""c3"").show()
df.select(""c4"").show()
df.select(""c3"", ""c4"").show()
{code}

Results:
{noformat}
>>> df.select(""c3"").show()
+---+                                                                           
| c3|
+---+
|1.0|
|2.0|
|3.1|
+---+
>>> df.select(""c4"").show()
+---+
| c4|
+---+
|  1|
|  2|
|  3|
+---+
>>> df.select(""c3"", ""c4"").show()
+---+----+
| c3|  c4|
+---+----+
|1.0|null|
|2.0|null|
|3.1|   3|
+---+----+
{noformat}

The test was done in branch-3.1 local mode.
",,apachespark,Baohe Zhang,maropu,petertoth,sanket991,,,,,,,,,,,,,,,,,,,,,SPARK-35108,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 08 10:41:43 UTC 2021,,,,,,,,,,"0|z0o2gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/21 19:28;sanket991;cc [~hyukjin.kwon];;;","26/Feb/21 19:44;Baohe Zhang;This is a correctness bug, so I would like to raise the priority to blocker and draw more attention from the community.;;;","26/Feb/21 23:01;Baohe Zhang;A simpler code to reproduce the error:
{code:python}
>>> from pyspark.sql.functions import udf
>>> from pyspark.sql.types import *
>>> 
>>> def udf1(data_type):
...     def u1(e):
...         return e[0]
...     return udf(u1, data_type)
... 
>>> df = spark.createDataFrame(
...   [((1.0, 1.0), (1, 1))],
...    ['c1', 'c2'])
>>> 
>>> 
>>> df = df.withColumn(""c3"", udf1(DoubleType())(""c1""))
>>> df = df.withColumn(""c4"", udf1(IntegerType())(""c2""))
>>> 
>>> # Show the results
... df.select(""c3"").show()
+---+
| c3|
+---+
|1.0|
+---+

>>> df.select(""c4"").show()
+---+
| c4|
+---+
|  1|
+---+

>>> df.select(""c3"", ""c4"").show()
+---+----+
| c3|  c4|
+---+----+
|1.0|null|
+---+----+
{code};;;","28/Feb/21 13:52;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31682;;;","28/Feb/21 13:53;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31682;;;","01/Mar/21 01:53;maropu;Added `correctness` in the label.;;;","08/Mar/21 01:13;srowen;Issue resolved by pull request 31682
[https://github.com/apache/spark/pull/31682];;;","08/Mar/21 10:41;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31778;;;",,,,,,,,,,,,
Fixed an issue where data could not be cleaned up when unregisterShuffle,SPARK-34541,13360949,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kaifeiYi,kaifeiYi,kaifeiYi,25/Feb/21 13:04,08/Mar/21 13:08,13/Jul/23 08:47,08/Mar/21 13:08,3.0.0,,,,,,,,,,,,,,,3.2.0,,,,Spark Core,,,,0,,,"While we use the old shuffle fetch protocol, we use partitionId as mapId in the ShuffleBlockId construction,but we use `context.taskAttemptId()` as mapId that it is cached in `taskIdMapsForShuffle` when we `getWriter[K, V]`.

where data could not be cleaned up when unregisterShuffle ,because we remove a shuffle's metadata from the `taskIdMapsForShuffle`'s mapIds, the mapId is `context.taskAttemptId()` instead of partitionId.

 ",,apachespark,kaifeiYi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 08 13:08:46 UTC 2021,,,,,,,,,,"0|z0o240:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/21 14:10;apachespark;User 'yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/31648;;;","26/Feb/21 10:46;apachespark;User 'yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/31664;;;","26/Feb/21 10:47;apachespark;User 'yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/31664;;;","08/Mar/21 13:08;srowen;Issue resolved by pull request 31664
[https://github.com/apache/spark/pull/31664];;;",,,,,,,,,,,,,,,,
New protocol FetchShuffleBlocks in OneForOneBlockFetcher lead to data loss or correctness,SPARK-34534,13360866,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,yuhaiyang,yuhaiyang,25/Feb/21 03:04,17/Jun/21 10:02,13/Jul/23 08:47,03/Mar/21 04:26,3.0.0,3.0.1,3.0.2,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,Shuffle,,,,0,Correctness,data-loss,"We will build a new rpc message `FetchShuffleBlocks` when `OneForOneBlockFetcher` init in replace of `OpenBlocks` to use adaptive feature, this introduce additional problems as follows.

`OneForOneBlockFetcher` will init a `blockIds` String array to catch chunk fetch success, it will use index in `blockIds` to fetch blocks and match blockId in `blockIds` when chunk data return. So the `blockIds` 's order must be consistent with fetchChunk index, but the new `FetchShuffleBlocks` return chunk order is not same as `blockIds`.

This will lead to the return data not match the blockId,  and this can lead to data corretness when retry to fetch after fetch block chunk failed.

Fetch chunk orker code and match blockId when rerun data code as follows: 

!image-2021-02-25-11-27-34-429.png|width=446,height=251!!image-2021-02-25-11-28-31-255.png|width=445,height=159!

Howerver, the fetch order in shuffle service,

!image-2021-02-25-11-30-03-834.png|width=510,height=361!

So, it will fetch some wrong block data when chunk fetch failed beause the blocks's wrong order.

!image-2021-02-25-11-31-59-110.png|width=601,height=204!

 

 ",,apachespark,cloud_fan,xkrogen,yuhaiyang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/21 03:17;yuhaiyang;image-2021-02-25-11-17-12-714.png;https://issues.apache.org/jira/secure/attachment/13021177/image-2021-02-25-11-17-12-714.png","25/Feb/21 03:27;yuhaiyang;image-2021-02-25-11-27-34-429.png;https://issues.apache.org/jira/secure/attachment/13021178/image-2021-02-25-11-27-34-429.png","25/Feb/21 03:28;yuhaiyang;image-2021-02-25-11-28-31-255.png;https://issues.apache.org/jira/secure/attachment/13021179/image-2021-02-25-11-28-31-255.png","25/Feb/21 03:30;yuhaiyang;image-2021-02-25-11-30-03-834.png;https://issues.apache.org/jira/secure/attachment/13021180/image-2021-02-25-11-30-03-834.png","25/Feb/21 03:32;yuhaiyang;image-2021-02-25-11-31-59-110.png;https://issues.apache.org/jira/secure/attachment/13021181/image-2021-02-25-11-31-59-110.png",,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 03 04:26:55 UTC 2021,,,,,,,,,,"0|z0o1lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/21 07:31;apachespark;User 'seayoun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31643;;;","25/Feb/21 07:31;apachespark;User 'seayoun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31643;;;","03/Mar/21 04:26;cloud_fan;Issue resolved by pull request 31643
[https://github.com/apache/spark/pull/31643];;;",,,,,,,,,,,,,,,,,
Remove Experimental API tag in PrometheusServlet,SPARK-34531,13360842,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,25/Feb/21 00:01,12/Dec/22 17:51,13/Jul/23 08:47,25/Feb/21 02:12,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.1.2,,,,Spark Core,,,,0,,,SPARK-31674 introduced an Experimental tag to PrometheusServlet but this is actually not needed because the class itself isn't an API.,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 25 02:12:21 UTC 2021,,,,,,,,,,"0|z0o1gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/21 00:07;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/31640;;;","25/Feb/21 00:07;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/31640;;;","25/Feb/21 02:12;dongjoon;Issue resolved by pull request 31640
[https://github.com/apache/spark/pull/31640];;;",,,,,,,,,,,,,,,,,
De-duplicated common columns cannot be resolved from USING/NATURAL JOIN,SPARK-34527,13360731,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,karenfeng,karenfeng,karenfeng,24/Feb/21 19:29,14/Apr/21 07:01,13/Jul/23 08:47,14/Apr/21 07:01,2.0.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"USING/NATURAL JOINS today have unexpectedly asymmetric behavior when resolving the duplicated common columns. For example, the left key columns can be resolved from a USING INNER JOIN, but the right key columns cannot. This is due to the Analyzer's [rewrite|https://github.com/apache/spark/blob/999d3b89b6df14a5ccb94ffc2ffadb82964e9f7d/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L3397] of NATURAL/USING JOINs, which uses Project to remove the duplicated common columns.",,apachespark,cloud_fan,karenfeng,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 07:01:55 UTC 2021,,,,,,,,,,"0|z0o19k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/21 19:30;karenfeng;I've implemented a fix for this, will push a PR.;;;","26/Feb/21 18:49;apachespark;User 'karenfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/31666;;;","26/Feb/21 18:50;apachespark;User 'karenfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/31666;;;","26/Mar/21 23:34;apachespark;User 'karenfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/31975;;;","26/Mar/21 23:35;apachespark;User 'karenfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/31975;;;","14/Apr/21 07:01;cloud_fan;Issue resolved by pull request 31666
[https://github.com/apache/spark/pull/31666];;;",,,,,,,,,,,,,,
Skip checking glob path in FileStreamSink.hasMetadata,SPARK-34526,13360713,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XuanYuan,XuanYuan,XuanYuan,24/Feb/21 18:17,06/May/21 13:49,13/Jul/23 08:47,06/May/21 13:49,3.1.0,,,,,,,,,,,,,,,3.2.0,,,,Structured Streaming,,,,0,,,"When checking the path in {{FileStreamSink.hasMetadata}}, we should ignore the error and assume the user wants to read a batch output. This is to keep the original behavior of ignoring the error.",,apachespark,kabhwan,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 06 13:49:19 UTC 2021,,,,,,,,,,"0|z0o15k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/21 18:30;apachespark;User 'xuanyuanking' has created a pull request for this issue:
https://github.com/apache/spark/pull/31638;;;","06/May/21 13:49;kabhwan;Issue resolved by pull request 31638
[https://github.com/apache/spark/pull/31638];;;",,,,,,,,,,,,,,,,,,
spark.createDataFrame does not support Pandas StringDtype extension type,SPARK-34521,13360648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nicolasazrak,pganelin,pganelin,24/Feb/21 15:02,16/Dec/21 19:58,13/Jul/23 08:47,16/Dec/21 06:03,3.0.1,,,,,,,,,,,,,,,3.3.0,,,,PySpark,,,,0,,,"The following test case demonstrates the problem:
{code:java}
import pandas as pd
from pyspark.sql import SparkSession, types

spark = SparkSession.builder.appName(__file__)\
    .config(""spark.sql.execution.arrow.pyspark.enabled"",""true"") \
    .getOrCreate()

good = pd.DataFrame([[""abc""]], columns=[""col""])

schema = types.StructType([types.StructField(""col"", types.StringType(), True)])
df = spark.createDataFrame(good, schema=schema)

df.show()

bad = good.copy()
bad[""col""]=bad[""col""].astype(""string"")

schema = types.StructType([types.StructField(""col"", types.StringType(), True)])
df = spark.createDataFrame(bad, schema=schema)

df.show(){code}
The error:
{code:java}
C:\Python\3.8.3\lib\site-packages\pyspark\sql\pandas\conversion.py:289: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Cannot specify a mask or a size when passing an object that is converted with the __arrow_array__ protocol.
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warnings.warn(msg)
{code}",,apachespark,bryanc,pganelin,ueshin,,,,,,,,,,,,,,,,,,,ARROW-11747,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 16 06:03:50 UTC 2021,,,,,,,,,,"0|z0o0r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/21 02:45;apachespark;User 'nicolasazrak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34509;;;","16/Dec/21 06:03;bryanc;Issue resolved by pull request 34509
[https://github.com/apache/spark/pull/34509];;;",,,,,,,,,,,,,,,,,,
Fix NPE if InSet contains null value during getPartitionsByFilter,SPARK-34515,13360492,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ulysses,ulysses,ulysses,24/Feb/21 04:40,24/Feb/21 13:33,13/Jul/23 08:47,24/Feb/21 13:33,3.1.2,3.2.0,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"Spark will convert InSet to `>= and <=` if it's values size over `spark.sql.hive.metastorePartitionPruningInSetThreshold` during pruning partition . At this case, if values contain a null, we will get such exception 

 
{code:java}
java.lang.NullPointerException
 at org.apache.spark.unsafe.types.UTF8String.compareTo(UTF8String.java:1389)
 at org.apache.spark.unsafe.types.UTF8String.compareTo(UTF8String.java:50)
 at scala.math.LowPriorityOrderingImplicits$$anon$3.compare(Ordering.scala:153)
 at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)
 at java.util.TimSort.sort(TimSort.java:220)
 at java.util.Arrays.sort(Arrays.java:1438)
 at scala.collection.SeqLike.sorted(SeqLike.scala:659)
 at scala.collection.SeqLike.sorted$(SeqLike.scala:647)
 at scala.collection.AbstractSeq.sorted(Seq.scala:45)
 at org.apache.spark.sql.hive.client.Shim_v0_13.convert$1(HiveShim.scala:772)
 at org.apache.spark.sql.hive.client.Shim_v0_13.$anonfun$convertFilters$4(HiveShim.scala:826)
 at scala.collection.immutable.Stream.flatMap(Stream.scala:489)
 at org.apache.spark.sql.hive.client.Shim_v0_13.convertFilters(HiveShim.scala:826)
 at org.apache.spark.sql.hive.client.Shim_v0_13.getPartitionsByFilter(HiveShim.scala:848)
 at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getPartitionsByFilter$1(HiveClientImpl.scala:750)
{code}",,apachespark,cloud_fan,LuciferYang,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 24 13:33:22 UTC 2021,,,,,,,,,,"0|z0nzsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/21 05:26;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/31632;;;","24/Feb/21 05:27;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/31632;;;","24/Feb/21 13:33;cloud_fan;Issue resolved by pull request 31632
[https://github.com/apache/spark/pull/31632];;;",,,,,,,,,,,,,,,,,
Disable validate default values when parsing Avro schemas,SPARK-34512,13360464,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,23/Feb/21 23:34,20/Oct/21 08:20,13/Jul/23 08:47,11/Jun/21 03:45,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"This is a regression problem. How to reproduce this issue:
{code:scala}
  // Add this test to HiveSerDeReadWriteSuite
  test(""SPARK-34512"") {
    withTable(""t1"") {
      hiveClient.runSqlHive(
        """"""
          |CREATE TABLE t1
          |  ROW FORMAT SERDE
          |  'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
          |  STORED AS INPUTFORMAT
          |  'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
          |  OUTPUTFORMAT
          |  'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
          |  TBLPROPERTIES (
          |    'avro.schema.literal'='{
          |      ""namespace"": ""org.apache.spark.sql.hive.test"",
          |      ""name"": ""schema_with_default_value"",
          |      ""type"": ""record"",
          |      ""fields"": [
          |         {
          |           ""name"": ""ARRAY_WITH_DEFAULT"",
          |           ""type"": {""type"": ""array"", ""items"": ""string""},
          |           ""default"": null
          |         }
          |       ]
          |    }')
          |"""""".stripMargin)

      spark.sql(""select * from t1"").show
    }
  }
{code}


{noformat}
org.apache.avro.AvroTypeException: Invalid default for field ARRAY_WITH_DEFAULT: null not a {""type"":""array"",""items"":""string""}
	at org.apache.avro.Schema.validateDefault(Schema.java:1571)
	at org.apache.avro.Schema.access$500(Schema.java:87)
	at org.apache.avro.Schema$Field.<init>(Schema.java:544)
	at org.apache.avro.Schema.parse(Schema.java:1678)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1425)
	at org.apache.avro.Schema$Parser.parse(Schema.java:1413)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.getSchemaFor(AvroSerdeUtils.java:268)
	at org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.determineSchemaOrThrowException(AvroSerdeUtils.java:111)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.determineSchemaOrReturnErrorSchema(AvroSerDe.java:187)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:107)
	at org.apache.hadoop.hive.serde2.avro.AvroSerDe.initialize(AvroSerDe.java:83)
	at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:533)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:450)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:437)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:263)
	at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:641)
	at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:624)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:831)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:867)
	at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4356)
	at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:354)
	at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)
	at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)
	at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)
	at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$runHive$1(HiveClientImpl.scala:820)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:291)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:224)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:223)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:273)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runHive(HiveClientImpl.scala:800)
	at org.apache.spark.sql.hive.client.HiveClientImpl.runSqlHive(HiveClientImpl.scala:787)

{noformat}
",,apachespark,dongjoon,iemejia,LuciferYang,xkrogen,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-37069,,HIVE-24797,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 11 03:45:09 UTC 2021,,,,,,,,,,"0|z0nzm8:",9223372036854775807,,,,,,,,,,,,,3.2.0,,,,,,,,,,"03/Jun/21 09:16;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/32750;;;","11/Jun/21 03:45;dongjoon;Issue resolved by pull request 32750
[https://github.com/apache/spark/pull/32750];;;",,,,,,,,,,,,,,,,,,
ADD JAR with ivy coordinates should be compatible with Hive transitive behavior,SPARK-34506,13360295,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shardulm,shardulm,shardulm,23/Feb/21 09:34,01/Mar/21 00:18,13/Jul/23 08:47,01/Mar/21 00:18,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Spark Core,,,,0,,,"SPARK-33084 added the ability to use ivy coordinates with `SparkContext.addJar`. [PR #29966|https://github.com/apache/spark/pull/29966] claims to mimic Hive behavior although I found a few cases where it doesn't

1) The default value of the {{transitive}} parameter is false, both in case of parameter not being specified in coordinate or parameter value being invalid. The Hive behavior is {{transitive}} is [true if not specified|https://github.com/apache/hive/blob/cb2ac3dcc6af276c6f64ee00f034f082fe75222b/ql/src/java/org/apache/hadoop/hive/ql/util/DependencyResolver.java#L169] in the coordinate and [false for invalid values|https://github.com/apache/hive/blob/cb2ac3dcc6af276c6f64ee00f034f082fe75222b/ql/src/java/org/apache/hadoop/hive/ql/util/DependencyResolver.java#L124]. Also, regardless of Hive, I think a default of {{true}} for the transitive parameter also matches [ivy's own defaults|https://ant.apache.org/ivy/history/2.5.0/ivyfile/dependency.html#_attributes].

2) The parameter value for {{transitive}} parameter is regarded as case-sensitive [based on the understanding|https://github.com/apache/spark/pull/29966#discussion_r547752259] that Hive behavior is case-sensitive. However, this is not correct, Hive [treats the parameter value case-insensitively|https://github.com/apache/hive/blob/cb2ac3dcc6af276c6f64ee00f034f082fe75222b/ql/src/java/org/apache/hadoop/hive/ql/util/DependencyResolver.java#L122].

",,apachespark,LuciferYang,maropu,shardulm,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34472,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 01 00:18:08 UTC 2021,,,,,,,,,,"0|z0nyps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/21 10:05;apachespark;User 'shardulm94' has created a pull request for this issue:
https://github.com/apache/spark/pull/31623;;;","01/Mar/21 00:18;maropu;Resolved by https://github.com/apache/spark/pull/31623;;;",,,,,,,,,,,,,,,,,,
Avoid unnecessary view resolving and remove the `performCheck` flag,SPARK-34504,13360240,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,linhongliu-db,linhongliu-db,23/Feb/21 05:51,17/Mar/21 03:37,13/Jul/23 08:47,17/Mar/21 03:37,3.1.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"in SPARK-34490, I added a `performCheck` flag to skip analysis check when resolving views. This is due to some view resolution is unnecessary. So we can avoid these unnecessary view resolution and remove the `performCheck` flag.",,apachespark,cloud_fan,imback82,linhongliu-db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 17 03:37:14 UTC 2021,,,,,,,,,,"0|z0nyds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/21 14:48;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31853;;;","16/Mar/21 14:49;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31853;;;","17/Mar/21 03:37;cloud_fan;Issue resolved by pull request 31853
[https://github.com/apache/spark/pull/31853];;;",,,,,,,,,,,,,,,,,
JDBC connection provider is not removing kerberos credentials from JVM security context,SPARK-34497,13360115,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,gsomogyi,gsomogyi,22/Feb/21 13:43,12/Dec/22 18:10,13/Jul/23 08:47,25/Feb/21 00:26,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.1.2,,,,SQL,,,,0,,,"Some of the built-in JDBC connection providers are changing the JVM security context to do the authentication which is fine. The problematic part is that executors can be reused by another query. The following situation leads to incorrect behaviour:
 * Query1 opens JDBC connection and changes JVM security context in Executor1
 * Query2 tries to open JDBC connection but it realizes there is already an entry for that DB type in Executor1
 * Query2 is not changing JVM security context and uses Query1 keytab and principal
 * Query2 fails with authentication error",,apachespark,gsomogyi,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 25 00:26:11 UTC 2021,,,,,,,,,,"0|z0nxm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/21 13:49;gsomogyi;Working on this.;;;","23/Feb/21 09:06;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/31622;;;","24/Feb/21 05:32;maropu;Please fill the description.;;;","24/Feb/21 09:44;gsomogyi;Thanks for the suggestion, filled.;;;","25/Feb/21 00:26;gurwls223;Issue resolved by pull request 31622
[https://github.com/apache/spark/pull/31622];;;",,,,,,,,,,,,,,,
table maybe resolved as a view if the table is dropped,SPARK-34490,13359970,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,linhongliu-db,linhongliu-db,linhongliu-db,22/Feb/21 02:42,23/Feb/21 09:32,13/Jul/23 08:47,23/Feb/21 09:31,3.1.2,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,see discussion in https://github.com/apache/spark/pull/31550#issuecomment-781977326,,apachespark,cloud_fan,linhongliu-db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 23 09:31:47 UTC 2021,,,,,,,,,,"0|z0nwq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/21 03:22;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/31606;;;","22/Feb/21 03:23;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/31606;;;","23/Feb/21 09:31;cloud_fan;Issue resolved by pull request 31606
[https://github.com/apache/spark/pull/31606];;;",,,,,,,,,,,,,,,,,
Module launcher build failed with profile hadoop-3.2 activated,SPARK-34480,13359744,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,artiship,artiship,20/Feb/21 07:10,20/Feb/21 07:19,13/Jul/23 08:47,20/Feb/21 07:18,3.0.1,,,,,,,,,,,,,,,,,,,Spark Submit,,,,0,,,"Build spark 3.0.1 with profile hadoop-3.2 activated
{code:java}
build/mvn -pl :spark-launcher_2.12 package -DskipTests -Phadoop-3.2 -Phive -Phive-thriftserver -Pkubernetes
{code}
When building the spark-launcher module it complains that lacking the common-lang dependency:


{code:java}
[INFO] --- scala-maven-plugin:4.3.0:compile (scala-compile-first) @ spark-launcher_2.12 ---
[INFO] Using incremental compilation using Mixed compile order
[INFO] Compiler bridge file: /Users/lichuanliang/.sbt/1.0/zinc/org.scala-sbt/org.scala-sbt-compiler-bridge_2.12-1.3.1-bin_2.12.10__52.0-1.3.1_20191012T045515.jar
[INFO] Compiling 20 Java sources to /Users/lichuanliang/IdeaProjects/qtt-spark-3.0/launcher/target/scala-2.12/classes ...
[ERROR] [Error] /Users/lichuanliang/IdeaProjects/qtt-spark-3.0/launcher/src/main/java/org/apache/spark/launcher/SparkSubmitCommandBuilder.java:20: package org.apache.commons.lang does not exist
[ERROR] [Error] /Users/lichuanliang/IdeaProjects/qtt-spark-3.0/launcher/src/main/java/org/apache/spark/launcher/SparkSubmitCommandBuilder.java:226: cannot find symbol
  symbol:   variable StringUtils
  location: class org.apache.spark.launcher.SparkSubmitCommandBuilder
[ERROR] [Error] /Users/lichuanliang/IdeaProjects/qtt-spark-3.0/launcher/src/main/java/org/apache/spark/launcher/SparkSubmitCommandBuilder.java:227: cannot find symbol
  symbol:   variable StringUtils
  location: class org.apache.spark.launcher.SparkSubmitCommandBuilder
[ERROR] [Error] /Users/lichuanliang/IdeaProjects/qtt-spark-3.0/launcher/src/main/java/org/apache/spark/launcher/SparkSubmitCommandBuilder.java:232: cannot find symbol
  symbol:   variable StringUtils
  location: class org.apache.spark.launcher.SparkSubmitCommandBuilder
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE

{code}
 

 ",,artiship,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 20 07:19:06 UTC 2021,,,,,,,,,,"0|z0nvc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/21 07:19;artiship;Please ignore this issue;;;",,,,,,,,,,,,,,,,,,,
Kryo NPEs when serializing Avro GenericData objects (except GenericRecord) ,SPARK-34477,13359730,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shardulm,shardulm,shardulm,20/Feb/21 02:10,24/Mar/21 07:46,13/Jul/23 08:47,24/Mar/21 07:45,2.0.0,3.0.0,,,,,,,,,,,,,,3.2.0,,,,Spark Core,,,,0,,,"SPARK-746 added KryoSerializer for GenericRecord and GenericData.Record Avro objects. However, Kryo serialization of other GenericData types like array, enum and fixed fails. Note that if such objects are within a GenericRecord, then current code works. However if these types are top level objects we want to distribute, then Kryo fails.

We should register KryoSerializer(s) for these GenericData types.

Code to reproduce:
{code:scala}
import org.apache.avro.{Schema, SchemaBuilder}
import org.apache.avro.generic.GenericData.Array

val arraySchema = SchemaBuilder.array().items().intType()
val array = new Array[Integer](1, arraySchema)
array.add(1)

sc.parallelize((0 until 10).map((_, array)), 2).collect
{code}
Similar code can be written for enums and fixed types

Errors:
 GenericData.Array
{code:java}
java.io.IOException: java.lang.NullPointerException
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1410)
	at org.apache.spark.rdd.ParallelCollectionPartition.readObject(ParallelCollectionRDD.scala:69)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1158)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2176)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2285)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2209)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:458)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at org.apache.avro.generic.GenericData$Array.add(GenericData.java:383)
	at java.util.AbstractList.add(AbstractList.java:108)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:134)
	at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:40)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:731)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:391)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:302)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:813)
	at com.twitter.chill.WrappedArraySerializer.read(WrappedArraySerializer.scala:35)
	at com.twitter.chill.WrappedArraySerializer.read(WrappedArraySerializer.scala:23)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:813)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:303)
	at org.apache.spark.rdd.ParallelCollectionPartition.$anonfun$readObject$2(ParallelCollectionRDD.scala:79)
	at org.apache.spark.rdd.ParallelCollectionPartition.$anonfun$readObject$2$adapted(ParallelCollectionRDD.scala:79)
	at org.apache.spark.util.Utils$.deserializeViaNestedStream(Utils.scala:171)
	at org.apache.spark.rdd.ParallelCollectionPartition.$anonfun$readObject$1(ParallelCollectionRDD.scala:79)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1403)
	... 20 more
{code}
GenericData.EnumSymbol
{code:java}
com.esotericsoftware.kryo.KryoException: java.lang.NullPointerException
Serialization trace:
props (org.apache.avro.Schema$EnumSchema)
schema (org.apache.avro.generic.GenericData$EnumSymbol)
	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:575)
	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:79)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)
	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:384)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:543)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at org.apache.avro.JsonProperties$2$1$1.<init>(JsonProperties.java:175)
	at org.apache.avro.JsonProperties$2$1.iterator(JsonProperties.java:174)
	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:98)
	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:575)
	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:79)
	... 13 more
{code}
GenericData.Fixed
{code:java}
com.esotericsoftware.kryo.KryoException: java.lang.NullPointerException
Serialization trace:
props (org.apache.avro.Schema$FixedSchema)
schema (org.apache.avro.generic.GenericData$Fixed)
	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:575)
	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:79)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:361)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:302)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:651)
	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:384)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:543)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NullPointerException
	at org.apache.avro.JsonProperties$2$1$1.<init>(JsonProperties.java:175)
	at org.apache.avro.JsonProperties$2$1.iterator(JsonProperties.java:174)
	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:98)
	at com.esotericsoftware.kryo.serializers.MapSerializer.write(MapSerializer.java:39)
	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:575)
	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:79)
	... 13 more
{code}",,apachespark,Gengliang.Wang,shardulm,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-746,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 24 07:45:22 UTC 2021,,,,,,,,,,"0|z0nv8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/21 03:11;apachespark;User 'shardulm94' has created a pull request for this issue:
https://github.com/apache/spark/pull/31597;;;","24/Mar/21 07:45;Gengliang.Wang;Issue resolved by pull request 31597
[https://github.com/apache/spark/pull/31597];;;",,,,,,,,,,,,,,,,,,
avoid NPE in DataFrameReader.schema(StructType),SPARK-34473,13359601,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,19/Feb/21 14:30,22/Feb/21 13:12,13/Jul/23 08:47,22/Feb/21 13:12,3.1.0,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 22 13:12:37 UTC 2021,,,,,,,,,,"0|z0nug8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/21 14:37;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31593;;;","22/Feb/21 13:12;cloud_fan;Issue resolved by pull request 31593
[https://github.com/apache/spark/pull/31593];;;",,,,,,,,,,,,,,,,,,
SparkContext.addJar with an ivy path fails in cluster mode with a custom ivySettings file,SPARK-34472,13359563,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shardulm,shardulm,shardulm,19/Feb/21 10:00,20/Apr/21 18:37,13/Jul/23 08:47,20/Apr/21 18:37,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Spark Core,,,,0,,,"SPARK-33084 introduced support for Ivy paths in {{sc.addJar}} or Spark SQL {{ADD JAR}}. If we use a custom ivySettings file using {{spark.jars.ivySettings}}, it is loaded at [https://github.com/apache/spark/blob/b26e7b510bbaee63c4095ab47e75ff2a70e377d7/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L1280.] However, this file is only accessible on the client machine. In cluster mode, this file is not available on the driver and so {{addJar}} fails.

{code:sh}
spark-submit --master yarn --deploy-mode cluster --class IvyAddJarExample --conf spark.jars.ivySettings=/path/to/ivySettings.xml example.jar
{code}

{code}
java.lang.IllegalArgumentException: requirement failed: Ivy settings file /path/to/ivySettings.xml does not exist
	at scala.Predef$.require(Predef.scala:281)
	at org.apache.spark.deploy.SparkSubmitUtils$.loadIvySettings(SparkSubmit.scala:1331)
	at org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:176)
	at org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:156)
	at org.apache.spark.sql.internal.SessionResourceLoader.resolveJars(SessionState.scala:166)
	at org.apache.spark.sql.hive.HiveSessionResourceLoader.addJar(HiveSessionStateBuilder.scala:133)
	at org.apache.spark.sql.execution.command.AddJarCommand.run(resources.scala:40)
 {code}

We should ship the ivySettings file to the driver so that {{addJar}} is able to find it.",,apachespark,shardulm,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34506,SPARK-35072,,,,SPARK-35073,SPARK-35074,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 23 18:43:27 UTC 2021,,,,,,,,,,"0|z0nu7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/21 10:15;shardulm;I will be sending a PR for this soon.;;;","19/Feb/21 11:18;apachespark;User 'shardulm94' has created a pull request for this issue:
https://github.com/apache/spark/pull/31591;;;","19/Feb/21 11:18;apachespark;User 'shardulm94' has created a pull request for this issue:
https://github.com/apache/spark/pull/31591;;;","23/Feb/21 18:43;shardulm;[~xkrogen] raised a good point at https://github.com/apache/spark/pull/31591#discussion_r579324686 that we should refactor YarnClusterSuite to extract common parameter handling code to be shared across tests. Will do this as a followup.;;;",,,,,,,,,,,,,,,,
toPandas failed with error: buffer source array is read-only when Arrow with self-destruct is enabled,SPARK-34463,13359314,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lidavidm,weichenxu123,weichenxu123,18/Feb/21 13:11,12/Dec/22 18:10,13/Jul/23 08:47,30/Mar/21 04:30,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,PySpark,,,,0,,,"Environment:

apache/spark master 
 pandas version > 1.0.5

Reproduce code:
{code:java}
spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)
spark.conf.set('spark.sql.execution.arrow.pyspark.selfDestruct.enabled', True)
spark.createDataFrame(sc.parallelize([(i,) for i in range(13)], 1), 'id long').selectExpr('IF(id % 3==0, id+1, NULL) AS f1', '(id+1) % 2 AS label').toPandas()['label'].value_counts()
{code}
Get error like:
{quote}Traceback (most recent call last): 
 File ""<stdin>"", line 1, in <module>
 File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/base.py"", line 1033, in value_counts
 dropna=dropna,
 File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/algorithms.py"", line 820, in value_counts
 keys, counts = value_counts_arraylike(values, dropna)
 File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/algorithms.py"", line 865, in value_counts_arraylike
 keys, counts = f(values, dropna)
 File ""pandas/_libs/hashtable_func_helper.pxi"", line 1098, in pandas._libs.hashtable.value_count_int64
 File ""stringsource"", line 658, in View.MemoryView.memoryview_cwrapper
 File ""stringsource"", line 349, in View.MemoryView.memoryview.__cinit__
 ValueError: buffer source array is read-only
{quote}",,apachespark,bryanc,lidavidm,viirya,weichenxu123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32953,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 30 04:30:44 UTC 2021,,,,,,,,,,"0|z0nsog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/21 13:15;weichenxu123;[~bryanc] [~lidavidm] [~hyukjin.kwon] Any idea ?
This is bug introduced by https://github.com/apache/spark/pull/29818;;;","18/Feb/21 13:17;lidavidm;[~weichenxu123] Does it work when selfDestruct is not enabled? If so, this is something that needs to be fixed in Pandas - it doesn't always deal well with immutable backing arrays, and selfDestruct makes it more likely that PyArrow will find zero-copy opportunities and hence give Pandas an immutable backing array.;;;","18/Feb/21 13:23;weichenxu123;[~lidavidm] Yes it works when selfDestruct disabled.

And the bug only happen on pandas > 1.0.5

Should we disable the feature when pandas > 1.0.5 ?;;;","18/Feb/21 13:26;lidavidm;This is a ""case by case"" kind of thing - it'll depend on what you do in Pandas and what Pandas version you have. I don't think we need to disable the feature, but maybe we should make it clearer what caveats it has (I know Bryan suggested updating the docs, which I should get to soon). I would suggest trying to build a reproduction case (without Spark) and reporting it to Pandas, I've done it before: [https://github.com/pandas-dev/pandas/pull/35532]

Things like this are why it's not universally enabled by default.;;;","18/Feb/21 13:31;weichenxu123;[~lidavidm] OK. Could you help build a simple reproduce case without spark ? Seems you're more familiar about this. Thanks!;;;","18/Feb/21 13:32;weichenxu123;[~lidavidm]
Btw, could you tell me why ""use_threads=false"" is set when selfDestruct enabled ? Is it required ?;;;","18/Feb/21 13:35;lidavidm;I'll take a look.

The reason for the three options is as follows:
 * use_threads=False - convert each column sequentially to minimize memory usage (overhead = 1 column's worth of memory at any time instead of N columns where N = parallelism level)
 * split_blocks=True - create a separate Pandas block for each column. This is an internal implementation detail, but it makes it more likely for PyArrow to find zero-copy opportunities and reduce memory usage/conversion overhead even more
 * self_destruct=True - free the PyArrow array after each conversion to save memory (technically it's just decrementing a refcount so if you have any other references to the array, no memory is freed);;;","21/Feb/21 07:57;gurwls223;I think it only fails in the master branch because SPARK-32953 is only merged into master, right? I will change the affect version.;;;","02/Mar/21 18:11;bryanc;As David said, it depends on what is done in Pandas that might lead to this. I'm not sure why `value_counts()` would cause this error, but other operations should work. I believe you could also workaround by making a copy of the DataFrame yourself. I think this example shows that the self destruct feature should be clearly documented to be experimental and only used if you know absolutely what you are doing.;;;","02/Mar/21 18:43;lidavidm;I've been a bit backlogged but I'll try to get docs and look at reproducing this soon. (I am similarly confused why value_counts would need a mutable backing array, but likely it's just an oversight in a parameter declaration somewhere.);;;","03/Mar/21 21:21;lidavidm;I've checked that the master build of Pandas fixes this specific issue, and I believe it's due to [this commit|https://github.com/pandas-dev/pandas/commit/f1d4026973bfb650be63cbe87443570aa01935dd#diff-f4f1cb812ad02cac95da4070745323e8cf31dfbcc65da52744bb6721439210bbR37]. I'll add notes to the docs about these sorts of issues in general.;;;","04/Mar/21 16:38;apachespark;User 'lidavidm' has created a pull request for this issue:
https://github.com/apache/spark/pull/31738;;;","30/Mar/21 04:30;gurwls223;Issue resolved by pull request 31738
[https://github.com/apache/spark/pull/31738];;;",,,,,,,
Upgrade Jetty to fix CVE-2020-27218,SPARK-34449,13358984,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,17/Feb/21 03:22,12/Dec/22 18:10,13/Jul/23 08:47,20/Feb/21 03:43,2.4.7,3.0.1,3.1.1,3.2.0,,,,,,,,,,,,2.4.8,3.0.3,3.1.1,,Build,,,,0,,,"CVE-2020-27218 affects the currently used Jetty 9.4.34 so let's upgrade it.
https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-27218.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 20 03:43:59 UTC 2021,,,,,,,,,,"0|z0nqn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/21 03:27;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31574;;;","17/Feb/21 03:27;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31574;;;","18/Feb/21 12:10;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31583;;;","20/Feb/21 03:43;gurwls223;Issue resolved by pull request 31583
[https://github.com/apache/spark/pull/31583];;;",,,,,,,,,,,,,,,,
Binary logistic regression incorrectly computes the intercept and coefficients when data is not centered,SPARK-34448,13358883,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,ykerzhner,ykerzhner,16/Feb/21 15:12,26/Mar/21 17:32,13/Jul/23 08:47,26/Mar/21 17:32,2.4.5,3.0.0,,,,,,,,,,,,,,3.2.0,,,,ML,MLlib,,,0,correctness,,"I have written up a fairly detailed gist that includes code to reproduce the bug, as well as the output of the code and some commentary:
[https://gist.github.com/ykerzhner/51358780a6a4cc33266515f17bf98a96]
To summarize: under certain conditions, the minimization that fits a binary logistic regression contains a bug that pulls the intercept value towards the log(odds) of the target data.  This is mathematically only correct when the data comes from distributions with zero means.  In general, this gives incorrect intercept values, and consequently incorrect coefficients as well.
As I am not so familiar with the spark code base, I have not been able to find this bug within the spark code itself.  A hint to this bug is here: 
[https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/LogisticRegression.scala#L894-L904]
based on the code, I don't believe that the features have zero means at this point, and so this heuristic is incorrect.  But an incorrect starting point does not explain this bug.  The minimizer should drift to the correct place.  I was not able to find the code of the actual objective function that is being minimized.",,apachespark,podongfeng,ykerzhner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 26 17:32:09 UTC 2021,,,,,,,,,,"0|z0nq0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/21 03:08;srowen;So one coarse response is - I'm surprised if the initialization should matter _that_ much? Starting the intercept at this value is kind of like starting it at the mean of the response in linear regression - probably the best a priori guess. That's why I am wondering about convergence (but sounds like it converges?) or what scikit does. Given the small data set, is the answer under-determined in this case?

I'd have to actually look at your test case to answer those questions, but that's what I'm thinking here. Maybe you have already thought it through.

What's a better initial value of the intercept?;;;","23/Feb/21 14:50;ykerzhner;As I said in the description, I do not believe that the starting point should cause this bug; the minimizer should still drift to the proper minimum.  I said the fact that the log(odds) was made the starting point seems to suggest that whoever wrote the code believed that the intercept should be close to the log(odds), which is only true if the data is centered.  If I had to guess, I would guess that there is something in the objective function that pulls the intercept towards the log(odds).  This would be a bug, as the log(odds) is a good approximation for the intercept if and only if the data is centered.  For non-centered data, it is completely wrong to have the intercept equal (or be close to) the log(odds).  My test shows precisely this, that when the data is not centered, spark still returns an intercept equal to the log(odds) (test 2.b, Intercept: -3.5428941035683303, log(odds): -3.542495168380248, correct intercept: -4).  Indeed, even for centered data, (test 1.b), it returns an intercept almost equal to the log(odds), (test 1.b. log(odds): -3.9876303002978997 Intercept: -3.987260922443554, correct intercept: -4).  So we need to dig into the objective function, and whether somewhere in there is a term that penalizes the intercept moving away from the log(odds).   If there is nothing there of this sort, then a step through of the minimization process should shed some clues as to why the intercept isnt budging from the initial value given.;;;","24/Feb/21 16:25;srowen;Yes I believe you're definitely correct there's a problem here. [~dbtsai] can I add you in here? I think you worked on the LR solver many years ago.

I skimmed the source code in sklearn and looks like the SAG solver starts with a 0 intercept:
https://github.com/scikit-learn/scikit-learn/blob/638b7689bbbfae4bcc4592c6f8a43ce86b571f0b/sklearn/linear_model/tests/test_sag.py#L73

Maybe ... this is the issue? I can try porting your test case to Scala to see if it fixes it. But the existing test suites seem to pass with a 0 initial intercept, at least.;;;","24/Feb/21 18:11;srowen;I crudely ported the test setup to a Scala test, and tried a 0 initial intercept in the LR implementation. It still gets the -3.5 intercept in the case where the 'const_feature' column is added, but -4 without. So, I'm not sure that's it.

Let me ping [~podongfeng] or maybe even [~sethah] who have worked on that code a bit and might have more of an idea about why the intercept wouldn't quite fit right in this case. I'm wondering if there is some issue in LogisticAggregator's treatment of the intercept? no idea, this is outside my expertise.

https://github.com/apache/spark/blob/3ce4ab545bfc28db7df2c559726b887b0c8c33b7/mllib/src/main/scala/org/apache/spark/ml/optim/aggregator/LogisticAggregator.scala#L244

BTW here's my hacked up test: 

{code}
  test(""BLR"") {
    val centered = false
    val regParam = 1.0e-8
    val num_distribution_samplings = 1000
    val num_rows_per_sampling = 1000
    val theta_1 = 0.3f
    val theta_2 = 0.2f
    val intercept = -4.0f

    val (feature1, feature2, target) = generate_blr_data(theta_1, theta_2, intercept, centered,
      num_distribution_samplings, num_rows_per_sampling)

    val num_rows = num_distribution_samplings * num_rows_per_sampling

    val const_feature = Array.fill(num_rows)(1.0f)
    (0 until num_rows / 10).foreach { i => const_feature(i) = 0.9f }


    val data = (0 until num_rows).map { i =>
      (feature1(i), feature2(i), const_feature(i), target(i))
    }

    val spark_df = spark.createDataFrame(data).toDF(""feature1"", ""feature2"", ""const_feature"", ""label"").cache()

    val vec = new VectorAssembler().setInputCols(Array(""feature1"", ""feature2"")).setOutputCol((""features""))
    val spark_df1 = vec.transform(spark_df).cache()

    val lr = new LogisticRegression().
      setMaxIter(100).setRegParam(regParam).setElasticNetParam(0.5).setFitIntercept(true)
    val lrModel = lr.fit(spark_df1)
    println(""Just the blr data"")
    println(""Coefficients: "" + lrModel.coefficients)
    println(""Intercept: "" + lrModel.intercept)

    val vec2 = new VectorAssembler().setInputCols(Array(""feature1"", ""feature2"", ""const_feature"")).
      setOutputCol((""features""))
    val spark_df2 = vec2.transform(spark_df).cache()

    val lrModel2 = lr.fit(spark_df2)
    println(""blr data plus one vector that is filled with 1's and .9's"")
    println(""Coefficients: "" + lrModel2.coefficients)
    println(""Intercept: "" + lrModel2.intercept)

  }

  def generate_blr_data(theta_1: Float,
                        theta_2: Float,
                        intercept: Float,
                        centered: Boolean,
                        num_distribution_samplings: Int,
                        num_rows_per_sampling: Int): (Array[Float], Array[Float], Array[Int]) = {
    val random = new Random(12345L)
    val uniforms = Array.fill(num_distribution_samplings)(random.nextFloat())
    val uniforms2 = Array.fill(num_distribution_samplings)(random.nextFloat())

    if (centered) {
      uniforms.transform(f => f - 0.5f)
      uniforms2.transform(f => 2.0f * f - 1.0f)
    } else {
      uniforms2.transform(f => f + 1.0f)
    }

    val h_theta = uniforms.zip(uniforms2).map { case (a, b) => intercept + theta_1 * a + theta_2 * b }
    val prob = h_theta.map(t => 1.0 / (1.0 + math.exp(-t)))
    val array = Array.ofDim[Int](num_distribution_samplings, num_rows_per_sampling)
    array.indices.foreach { i =>
      (0 until math.round(num_rows_per_sampling * prob(i)).toInt).foreach { j =>
        array(i)(j) = 1
      }
    }

    val num_rows = num_distribution_samplings * num_rows_per_sampling

    val feature_1 = uniforms.map(f => Array.fill(num_rows_per_sampling)(f)).flatten
    val feature_2 = uniforms2.map(f => Array.fill(num_rows_per_sampling)(f)).flatten
    val target = array.flatten

    return (feature_1, feature_2, target)
  }
{code};;;","25/Feb/21 03:14;podongfeng;[~srowen] Thanks for pinging me, I am going to look into this issue;;;","26/Feb/21 04:26;podongfeng;[~srowen] [~weichenxu123]  [~ykerzhner]

My findings until now:

1, as to param {{standardization, its name and doc is misleading. No matter whether it is true (by default) or false, LR always `standardize` the input vectors in a special way (x => x / std(x)), but the transformed vectors are not centered;}}

{{2, for the scala testsuite above, I log the internal gradient and model (intercept & coef) at each iteration. I check the objective function and gradient, and it seems that they are calculated correctly;}}

{{3, for the case with const_feature(0.9 & 1.0) above,}} the mean & std of three input features are:
{code:java}
featuresMean: [0.4999142959117828,1.4847274177074965,0.9899999976158129]
featuresStd: [0.28501348037270735,0.28375633081273305,0.03000002215257344]{code}
{{note that const_feature (its std is 0.03) will be scaled to (30.0 & 33.3).}}

 

*{{I suspect that the underlying solvers (OWLQN/LBFGS/LBFGSB) can not handle a feature with such large(>30) values.}}*

{{3.1, Since std vec affects both the internal scaling and regularization, I disable regularization by setting regParam 0.0 to see whether this scaling matters.}}

With *LBFGS* Solver, the issue also exists, the solution with const_feature is:
{code:java}
Coefficients: [0.29713531586902586,0.1928976631256973,-0.44332696536594945]
Intercept: -3.548585606117963 {code}
{{ }}

{{Then I manually set std vec to one values:}}
{code:java}
 val featuresStd = Array.fill(featuresMean.length)(1.0){code}
{{Then the optimization procedure behaviors as expectations, and the solution is:}}
{code:java}
Coefficients: [0.298868144564205,0.20101389459979044,0.008381706578824933]
Intercept: -4.009204134794202 {code}
 

{{3.2, here I reset the regParam, with *OWLQN* Solver, the solution with all ones std is:}}
{code:java}
Coefficients: [0.296817926857017,0.19312282148846005,-0.17682584221569103]
Intercept: -3.8124413640824466 {code}
 

{{Compared with previous solution:}}
{code:java}
Coefficients: [0.2997261304455311,0.18830032771483074,-0.44301560942213103]
Intercept: -3.5428941035683303 {code}
{{I think the new solution with unit std vec fits better.}}

 

{{To sum up, I guess the internal standardization should center the vectors in some way to match existing solvers.}}

 

{{TODO:}}

{{1, I will refer to other impls to see how standardization is impled;}}

{{2, I will go on this issue to see what will happen if the vectors are centered;}}

{{3, This issue may also exist in LiR/SVC/etc. I will check in the future;}}

 

 ;;;","26/Feb/21 04:37;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/31657;;;","26/Feb/21 04:41;podongfeng;My test code and log is here

https://github.com/apache/spark/pull/31657;;;","26/Feb/21 10:41;podongfeng;1, I just make a simple impl(https://github.com/apache/spark/pull/31657/commits/49141bbb178ac28af3263efa31299f8eb835830b) that internally center the vectors,

then the solution seems ok.
{code:java}
Coefficients: [0.29886424895473795,0.20097637066670226,0.0081964409252861]
Intercept: -4.0089605363236664 {code}
Moreover, it converge much faster.

 

2, however, if we center the vectors, then a lot of (>24) existing testsuite fails. It seems existing scaling was designed on purpose.

 

3, existing scaling (x/std_x) was added in https://issues.apache.org/jira/browse/SPARK-7262 , and aimed to keep in line with {{glmnet}}. But I am not familiar with {{glmnet}}.

{{In sklearn, linear_model.LogisticRegression}} does not standardize input vectors, while other linear models (i.e linear_model.ElasticNet) will 'subtracting the mean and dividing by the l2-norm'.

 

 ;;;","26/Feb/21 10:43;podongfeng;I am not sure to: 1, center the vector in existing impl (I can image that lots of testsuite will needed to change); 2, warn the end users to standardize the vectors outside of LR, if abs(mean(x)) is too large (maybe > 1.0?);;;","26/Feb/21 16:51;ykerzhner;The faster convergence when using standardization that includes centering makes sense as you can ahead of time guess the value of the intercept (it should equal the log(odds)).   What I still don't understand is how is it that in the case that the data is not centered, the intercept after the minimization is almost exactly equal to the log(odds).  This seems extremely strange to me and I can't find a mathematical reason for this to be happening.  In the original test example, could you print out the x, f(x), grad(x) as the minimizer moves from (0, 0, 0, log(odds)) to the minimum?;;;","01/Mar/21 09:42;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/31693;;;","01/Mar/21 09:42;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/31693;;;","01/Mar/21 14:48;srowen;[~ykerzhner] you may be interested in reviewing https://github.com/apache/spark/pull/31693;;;","01/Mar/21 16:45;ykerzhner;I took a look over the weekend.  It seems good, and somewhat matches what I did in my test example where I centered before running the fitting.  Unfortunately, I am not very well versed in scala, so actually reviewing the code is a bit hard.  I appreciate the printouts for the test case in the PR, and I now understand why spark was returning the log(odds) for the intercept:  The division of a non centered vector with a small std dev creates a vector with very large entries that looks roughly like a constant vector.  When the minimizer computes the gradient, it assigns far more weight to this big vector than it does the intercept, as the magnitude appears more important than the fact that it isnt exactly constant.  When the optimizer then moves in the direction of the gradient, it finds that the value of the objective function actually increased (because of the fact that this big vector isnt exactly constant), and backtracks several times.  By the time it has backtracked enough to actually get a lower value on the objective function, the movement of the intercept is nearly 0.  So essentially, the intercept never moves during the entire calibration.  This is also why it takes so much longer (because of all the backtracking).  Once things are centered, the entries in the gradient for the intercept become dominant compared to the vector that is sort of constant, and so the minimizer begins adjusting the intercept, and moves it to the correct spot.;;;","01/Mar/21 17:02;ykerzhner;I will try to do a code review, but will focus on the comments so that people who see this in the future will understand what is happening.;;;","26/Mar/21 17:32;srowen;Issue resolved by pull request 31693
[https://github.com/apache/spark/pull/31693];;;",,,
Pin Sphinx version to `sphinx<3.5.0`,SPARK-34442,13358727,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,15/Feb/21 22:16,15/Feb/21 22:59,13/Jul/23 08:47,15/Feb/21 22:59,3.0.2,,,,,,,,,,,,,,,3.0.2,,,,Documentation,PySpark,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 15 22:59:54 UTC 2021,,,,,,,,,,"0|z0np20:",9223372036854775807,,,,,,,,,,,,,3.0.2,,,,,,,,,,"15/Feb/21 22:17;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31568;;;","15/Feb/21 22:59;dongjoon;Issue resolved by pull request 31568
[https://github.com/apache/spark/pull/31568];;;",,,,,,,,,,,,,,,,,,
DPP support LIKE ANY/ALL,SPARK-34436,13358538,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,14/Feb/21 13:10,02/Mar/21 10:02,13/Jul/23 08:47,25/Feb/21 10:08,3.2.0,,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"Support this case:
{code:sql}
SELECT date_id, product_id FROM fact_sk f
JOIN dim_store s
ON f.store_id = s.store_id WHERE s.country LIKE ANY ('%D%E%', '%A%B%')
{code}",,apachespark,cloud_fan,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 25 10:08:35 UTC 2021,,,,,,,,,,"0|z0nnw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/21 13:28;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/31563;;;","25/Feb/21 10:08;cloud_fan;Issue resolved by pull request 31563
[https://github.com/apache/spark/pull/31563];;;",,,,,,,,,,,,,,,,,,
KMeansSummary class is omitted from PySpark documentation,SPARK-34429,13358442,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,srowen,JohnHBauer,JohnHBauer,13/Feb/21 04:43,12/Mar/21 13:22,13/Jul/23 08:47,12/Mar/21 13:22,2.4.7,3.0.1,,,,,,,,,,,,,,3.1.1,,,,Documentation,ML,,,0,,,"`KMeansSummary` is missing from `__all__` in clustering.py, Sphinx omits it from emitted documentation, and the class is invisible when imported by other modules.",,apachespark,JohnHBauer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 12 13:22:25 UTC 2021,,,,,,,,,,"0|z0nnao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/21 04:50;apachespark;User 'JohnHBauer' has created a pull request for this issue:
https://github.com/apache/spark/pull/31530;;;","13/Feb/21 04:51;apachespark;User 'JohnHBauer' has created a pull request for this issue:
https://github.com/apache/spark/pull/31530;;;","12/Mar/21 13:22;srowen;Resolved in commit https://github.com/apache/spark/commit/891c5e661a77c713d06b102b7aba3be0667add33;;;",,,,,,,,,,,,,,,,,
Custom functions can't be used in temporary views with CTEs,SPARK-34421,13358100,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,petertoth,laurikoobas,laurikoobas,11/Feb/21 06:32,23/Mar/21 14:08,13/Jul/23 08:47,23/Mar/21 14:08,3.1.0,,,,,,,,,,,,,,,3.0.3,3.1.1,,,SQL,,,,0,,,"The following query works in Spark 3.0 not Spark 3.1.
  
 Start with:
 {{spark.udf.registerJavaFunction(""custom_func"", ""com.stuff.path.custom_func"", LongType())}}
  
 Works: * {{select custom_func()}}
 * {{create temporary view blaah as select custom_func()}}
 * {{with step_1 as ( select custom_func() ) select * from step_1}}

Broken:
 {{create temporary view blaah as with step_1 as ( select custom_func() ) select * from step_1}}
  
 followed by:
 {{select * from blaah}}
  
 Error:
 {{Error in SQL statement: AnalysisException: No handler for UDF/UDAF/UDTF '}}{{com.stuff.path.custom_func}}{{';}}",Databricks Runtime 8.0,apachespark,cloud_fan,laurikoobas,petertoth,viirya,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33142,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 23 14:08:28 UTC 2021,,,,,,,,,,"0|z0nl6w:",9223372036854775807,,,,,,,,,,,,,3.1.0,,,,,,,,,,"11/Feb/21 14:54;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31550;;;","19/Feb/21 10:15;cloud_fan;Issue resolved by pull request 31550
[https://github.com/apache/spark/pull/31550];;;","19/Feb/21 12:48;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/31592;;;","23/Mar/21 07:46;laurikoobas;This has been re-introduced with DBR 8.1 (Spark 3.1.1).

It works in DBR 8.0, but is broken in the same way in DBR 8.1;;;","23/Mar/21 14:08;cloud_fan;[~laurikoobas] It's an internal mistake that this fix doesn't go to DBR 8.1, and it's already fixed 8 hours ago. Apache Spark has no problem at all.;;;",,,,,,,,,,,,,,,
"org.apache.spark.sql.DataFrameNaFunctions.fillMap(values: Seq[(String, Any)]) fails for column name having a dot",SPARK-34417,13357909,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,amsharma,amsharma,amsharma,10/Feb/21 10:39,02/Mar/21 09:24,13/Jul/23 08:47,02/Mar/21 09:15,3.0.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,Spark Core,,,,0,,,"Code to reproduce the issue:
{code:java}
import org.apache.spark.sql.SparkSession

object ColumnNameWithDot {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession.builder.appName(""Simple Application"")
      .config(""spark.master"", ""local"").getOrCreate()

    spark.sparkContext.setLogLevel(""OFF"")

    import spark.implicits._
    val df = Seq((""abc"", 23), (""def"", 44), (null, 0)).toDF(""ColWith.Dot"", ""Col"")
    df.na.fill(Map(""`ColWith.Dot`"" -> ""na""))
      .show()

  }
}
{code}
*Analysis*

*------------------------------PART-I-----------------------------------*

Debugged the spark code. It is due to a bug in the spark-catalyst code at

[https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/package.scala#L266%23L268]

Function in question resolves the column as per the code-comments in the following order until a match is found.
 * Consider pattern dbName.tableName.columnName
 * Consider tableName.columnName
 * Consider everything as columnName

But implementation considers only the first part for the resolution in the third step. It should join all parts using dot(.).

*------------------------------PART-II-----------------------------------*

If we don’t use column name with back-tick them it fails at [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala#L400]

 If it is quoted, the condition at [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala#L413] becomes false as *k* has value quoted with back-tick whereas *f.name* is not. Then it fails at [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala#L422]

It is failing due to the reason mentioned in the PART-I.

*Solution*

Make changes in [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/package.scala#L266%23L268] as below:

{color:#ff0000}-val name = nameParts.head{color}
 {color:#00875a}+ val name = nameParts.mkString(""."") // join all part using .{color}
 val attributes = collectMatches(name, direct.get(name.toLowerCase(Locale.ROOT)))
 {color:#ff0000}- (attributes, nameParts.tail){color}
 {color:#00875a}+ (attributes, Seq.empty){color}

*{color:#172b4d}Workaround{color}*

{color:#172b4d}We can make change in [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrameNaFunctions.scala#L396]{color}

{color:#172b4d}While we are resolving the input columns, create a new map with the name of the resolved column and replace value as below.{color}

{color:#172b4d}Idea is to use resolved named instead of input name while filling null values.{color}
{code:java}
private def fillMap(values: Seq[(String, Any)]): DataFrame = {
  // Error handling
  var resolved: Map[String, Any] = Map()
  values.foreach { case (colName, replaceValue) =>
    // Check column name exists
    val resolvedColumn = df.resolve(colName)

    // Check data type
    replaceValue match {
      case _: jl.Double | _: jl.Float | _: jl.Integer | _: jl.Long | _: jl.Boolean | _: String =>
      // This is good
      case _ => throw new IllegalArgumentException(
        s""Unsupported value type ${replaceValue.getClass.getName} ($replaceValue)."")
    }
    resolved += (resolvedColumn.name -> replaceValue)
  }

  val columnEquals = df.sparkSession.sessionState.analyzer.resolver
  val projections = df.schema.fields.map { f =>
    resolved.find { case (k, _) => columnEquals(k, f.name) }.map { case (_, v) =>
      v match {
        case v: jl.Float => fillCol[Float](f, v)
        case v: jl.Double => fillCol[Double](f, v)
        case v: jl.Long => fillCol[Long](f, v)
        case v: jl.Integer => fillCol[Integer](f, v)
        case v: jl.Boolean => fillCol[Boolean](f, v.booleanValue())
        case v: String => fillCol[String](f, v)
      }
    }.getOrElse(df.col(f.name))
  }
  df.select(projections : _*)
}
{code}","Spark version - 3.0.1

OS - macOS 10.15.7",amsharma,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 02 09:15:04 UTC 2021,,,,,,,,,,"0|z0nk0g:",9223372036854775807,,,,,sowen,,,,,,,,,,,,,,,,,,"10/Feb/21 16:39;apachespark;User 'amandeep-sharma' has created a pull request for this issue:
https://github.com/apache/spark/pull/31545;;;","02/Mar/21 09:15;cloud_fan;Issue resolved by pull request 31545
[https://github.com/apache/spark/pull/31545];;;",,,,,,,,,,,,,,,,,,
The getMetricsSnapshot method of the PrometheusServlet class has a wrong value,SPARK-34405,13357668,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,397090770,397090770,397090770,09/Feb/21 03:42,26/Feb/21 09:44,13/Jul/23 08:47,09/Feb/21 05:20,3.0.1,3.1.0,3.1.1,3.2.0,,,,,,,,,,,,3.0.2,3.1.1,,,Spark Core,,,,0,,,"The mean value of timersLabels in the PrometheusServlet class is wrong, You can look at line 105 of this class: [L105.|https://github.com/apache/spark/blob/37fe8c6d3cd1c5aae3a61fb9b32ca4595267f1bb/core/src/main/scala/org/apache/spark/metrics/sink/PrometheusServlet.scala#L105]
{code:java}
// code placeholder
sb.append(s""${prefix}Mean$timersLabels ${snapshot.getMax}\n""){code}",,397090770,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 09 05:20:10 UTC 2021,,,,,,,,,,"0|z0niiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/21 04:00;apachespark;User '397090770' has created a pull request for this issue:
https://github.com/apache/spark/pull/31532;;;","09/Feb/21 04:01;apachespark;User '397090770' has created a pull request for this issue:
https://github.com/apache/spark/pull/31532;;;","09/Feb/21 05:20;dongjoon;Issue resolved by pull request 31532
[https://github.com/apache/spark/pull/31532];;;",,,,,,,,,,,,,,,,,
Invalid ID for offset-based ZoneId since Spark 3.0,SPARK-34392,13357364,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wangshikai,yumwang,yumwang,07/Feb/21 08:31,12/Dec/22 18:11,13/Jul/23 08:47,26/Feb/21 17:04,3.0.0,3.0.1,,,,,,,,,,,,,,3.0.3,3.1.2,3.2.0,,SQL,,,,0,,,"How to reproduce this issue:
{code:sql}
select to_utc_timestamp(""2020-02-07 16:00:00"", ""GMT+8:00"");
{code}
Spark 2.4:
{noformat}
spark-sql> select to_utc_timestamp(""2020-02-07 16:00:00"", ""GMT+8:00"");
2020-02-07 08:00:00
Time taken: 0.089 seconds, Fetched 1 row(s)
{noformat}

Spark 3.x:
{noformat}
spark-sql> select to_utc_timestamp(""2020-02-07 16:00:00"", ""GMT+8:00"");
21/02/07 01:24:32 ERROR SparkSQLDriver: Failed in [select to_utc_timestamp(""2020-02-07 16:00:00"", ""GMT+8:00"")]
java.time.DateTimeException: Invalid ID for offset-based ZoneId: GMT+8:00
	at java.time.ZoneId.ofWithPrefix(ZoneId.java:437)
	at java.time.ZoneId.of(ZoneId.java:407)
	at java.time.ZoneId.of(ZoneId.java:359)
	at java.time.ZoneId.of(ZoneId.java:315)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.getZoneId(DateTimeUtils.scala:53)
	at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toUTCTime(DateTimeUtils.scala:814)
{noformat}


",,apachespark,maxgekk,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 05 07:36:43 UTC 2021,,,,,,,,,,"0|z0ngnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/21 02:41;yumwang;https://docs.oracle.com/javase/8/docs/api/java/time/ZoneId.html;;;","09/Feb/21 06:03;gurwls223;cc [~maxgekk] FYI;;;","09/Feb/21 15:25;maxgekk;The ""GMT+8:00"" string is unsupported format in 3.0, see docs for the to_utc_timestamp() function (https://github.com/apache/spark/blob/30468a901577e82c855fbc4cb78e1b869facb44c/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L3397-L3402):
{code:scala}
@param tz A string detailing the time zone ID that the input should be adjusted to. It should
  be in the format of either region-based zone IDs or zone offsets. Region IDs must
  have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in
  the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are
  supported as aliases of '+00:00'. Other short names are not recommended to use
  because they can be ambiguous.
{code};;;","10/Feb/21 03:11;yumwang;PostgreSQL:
{noformat}
postgres=# SELECT TIMESTAMP WITH TIME ZONE '2020-02-07 16:00:00' AT TIME ZONE 'GMT+8:00', current_timestamp, current_timestamp AT TIME ZONE 'GMT+8:00', version();
      timezone       |       current_timestamp       |          timezone          |                                                             version
---------------------+-------------------------------+----------------------------+----------------------------------------------------------------------------------------------------------------------------------
 2020-02-07 08:00:00 | 2021-02-10 03:10:48.407459+00 | 2021-02-09 19:10:48.407459 | PostgreSQL 11.3 (Debian 11.3-1.pgdg90+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516, 64-bit
(1 row)
{noformat}

Hive:
{noformat}
hive> SELECT current_timestamp, to_utc_timestamp(current_timestamp, ""GMT+8:00""), version();
2021-02-09 20:09:16.201	2021-02-09 12:09:16.201	2.3.7 rcb213d88304034393d68cc31a95be24f5aac62b6
Time taken: 0.095 seconds, Fetched: 1 row(s)
hive>
{noformat}

 Presto/Trino
{noformat}
trino:sf1> SELECT current_timestamp, current_timestamp AT TIME ZONE 'GMT+8:00';
            _col0            |             _col1
-----------------------------+--------------------------------
 2021-02-10 03:07:27.807 UTC | 2021-02-10 11:07:27.807 +08:00
(1 row)

Query 20210210_030727_00015_2i5r6, FINISHED, 1 node
Splits: 1 total, 1 done (100.00%)
0.22 [0 rows, 0B] [0 rows/s, 0B/s]
{noformat}
;;;","23/Feb/21 13:53;apachespark;User 'Karl-WangSK' has created a pull request for this issue:
https://github.com/apache/spark/pull/31624;;;","26/Feb/21 17:04;srowen;Issue resolved by pull request 31624
[https://github.com/apache/spark/pull/31624];;;","05/Mar/21 07:36;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31755;;;",,,,,,,,,,,,,
c,SPARK-34381,13357170,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,brunofortes,brunofortes,05/Feb/21 19:41,05/Feb/21 19:55,13/Jul/23 08:47,05/Feb/21 19:53,2.4.0,,,,,,,,,,,,,,,2.4.0,,,,Spark Core,,,,0,,,,,brunofortes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-02-05 19:41:29.0,,,,,,,,,,"0|z0nfg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Map JDBC RowID to StringType rather than LongType,SPARK-34379,13357148,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,05/Feb/21 18:17,20/Feb/21 14:48,13/Jul/23 08:47,20/Feb/21 14:48,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"In the current implementation, JDBC RowID type is mapped to LongType except for OracleDialect, but there is no guarantee to be able to convert RowID to long.
java.sql.RowId declares toString and the specification of java.sql.RowId says

_all methods on the RowId interface must be fully implemented if the JDBC driver supports the data type_
(https://docs.oracle.com/javase/8/docs/api/java/sql/RowId.html)

So, we should prefer StringType to LongType.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 20 14:48:12 UTC 2021,,,,,,,,,,"0|z0nfbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/21 18:32;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31491;;;","05/Feb/21 18:32;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31491;;;","20/Feb/21 14:48;sarutak;This issue is resolved by https://github.com/apache/spark/pull/31491 .;;;",,,,,,,,,,,,,,,,,
HiveThriftServer2 startWithContext may hang with a race issue ,SPARK-34373,13356890,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,05/Feb/21 02:19,12/Dec/22 18:10,13/Jul/23 08:47,21/Feb/21 08:39,3.0.1,3.1.0,,,,,,,,,,,,,,3.1.1,,,,SQL,,,,0,,,"```
21:43:26.809 WARN org.apache.thrift.server.TThreadPoolServer: Transport error occurred during acceptance of message.
org.apache.thrift.transport.TTransportException: No underlying server socket.
	at org.apache.thrift.transport.TServerSocket.acceptImpl(TServerSocket.java:126)
	at org.apache.thrift.transport.TServerSocket.acceptImpl(TServerSocket.java:35)
	at org.apache.thrift.transport.TServerTransport.acceException in thread ""Thread-15"" java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:336)
	at java.io.FilterInputStream.read(FilterInputStream.java:107)
	at scala.sys.process.BasicIO$.loop$1(BasicIO.scala:238)
	at scala.sys.process.BasicIO$.transferFullyImpl(BasicIO.scala:246)
	at scala.sys.process.BasicIO$.transferFully(BasicIO.scala:227)
	at scala.sys.process.BasicIO$.$anonfun$toStdOut$1(BasicIO.scala:221)
```
the TServer might try to serve even the stop is called",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 21 08:39:11 UTC 2021,,,,,,,,,,"0|z0ndq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/21 02:29;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31479;;;","05/Feb/21 02:30;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31479;;;","21/Feb/21 08:39;gurwls223;Fixed in https://github.com/apache/spark/pull/31479;;;",,,,,,,,,,,,,,,,,
"Supporting Avro schema evolution for partitioned Hive tables using ""avro.schema.url""",SPARK-34370,13356853,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,04/Feb/21 20:49,07/Feb/21 01:26,13/Jul/23 08:47,07/Feb/21 01:26,2.3.1,2.4.0,3.0.1,3.1.0,3.2.0,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"This came up in https://github.com/apache/spark/pull/31133#issuecomment-773567152.


The use case is the following there is a partitioned Hive table with Avro data. The schema is specified via ""avro.schema.url"".
With time the schema is evolved and the new schema is set for the table ""avro.schema.url"" when data is read from the old partition this new evolved schema must be used.",,apachespark,attilapiros,dongjoon,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 07 01:26:01 UTC 2021,,,,,,,,,,"0|z0ndi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/21 20:50;attilapiros;I am working on this.;;;","06/Feb/21 23:04;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/31501;;;","07/Feb/21 01:26;dongjoon;Issue resolved by pull request 31501
[https://github.com/apache/spark/pull/31501];;;",,,,,,,,,,,,,,,,,
Dynamic allocation on K8s kills executors with running tasks,SPARK-34361,13356800,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,04/Feb/21 16:06,03/Mar/21 05:11,13/Jul/23 08:47,03/Mar/21 00:59,3.0.0,3.0.1,3.0.2,3.1.0,3.1.1,3.1.2,3.2.0,,,,,,,,,3.1.2,3.2.0,,,Kubernetes,,,,0,,,"There is race between executor POD allocator and cluster scheduler backend. 
During downscaling (in dynamic allocation) we experienced a lot of killed new executors with running task on them.

The pattern in the log is the following:

{noformat}
21/02/01 15:12:03 INFO ExecutorMonitor: New executor 312 has registered (new total is 138)
...
21/02/01 15:12:03 INFO TaskSetManager: Starting task 247.0 in stage 4.0 (TID 2079, 100.100.18.138, executor 312, partition 247, PROCESS_LOCAL, 8777 bytes)
21/02/01 15:12:03 INFO ExecutorPodsAllocator: Deleting 3 excess pod requests (408,312,307).
...
21/02/01 15:12:04 ERROR TaskSchedulerImpl: Lost executor 312 on 100.100.18.138: The executor with id 312 was deleted by a user or the framework.
{noformat}",,apachespark,attilapiros,irashid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 03 05:11:39 UTC 2021,,,,,,,,,,"0|z0nd68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/21 16:07;attilapiros;I am working on this.;;;","07/Feb/21 19:21;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/31513;;;","03/Mar/21 05:11;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/31719;;;","03/Mar/21 05:11;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/31719;;;",,,,,,,,,,,,,,,,
CostBasedJoinReorder can fail on self-join,SPARK-34354,13356696,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,04/Feb/21 08:40,12/Dec/22 18:10,13/Jul/23 08:47,02/Apr/21 06:23,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,," 

For example:
{code:java}
test(""join reorder with self-join"") {
  val plan = t2.join(t1, Inner, Some(nameToAttr(""t1.k-1-2"") === nameToAttr(""t2.k-1-5"")))
    .select(nameToAttr(""t1.v-1-10""))
    .join(t2, Inner, Some(nameToAttr(""t1.v-1-10"") === nameToAttr(""t2.k-1-5"")))

  // this can fail
  Optimize.execute(plan.analyze)
}
{code}
error:
{code:java}
[info]   java.lang.AssertionError: assertion failed
[info]   at scala.Predef$.assert(Predef.scala:208)
[info]   at org.apache.spark.sql.catalyst.optimizer.JoinReorderDP$.search(CostBasedJoinReorder.scala:178)
[info]   at org.apache.spark.sql.catalyst.optimizer.CostBasedJoinReorder$.org$apache$spark$sql$catalyst$optimizer$CostBasedJoinReorder$$reorder(CostBasedJoinReorder.scala:64)
[info]   at org.apache.spark.sql.catalyst.optimizer.CostBasedJoinReorder$$anonfun$1.applyOrElse(CostBasedJoinReorder.scala:45)
[info]   at org.apache.spark.sql.catalyst.optimizer.CostBasedJoinReorder$$anonfun$1.applyOrElse(CostBasedJoinReorder.scala:41)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:317)
[info]   at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:317)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
[info]   at org.apache.spark.sql.catalyst.optimizer.CostBasedJoinReorder$.apply(CostBasedJoinReorder.scala:41)
[info]   at org.apache.spark.sql.catalyst.optimizer.CostBasedJoinReorder$.apply(CostBasedJoinReorder.scala:35)
{code}
 ",,apachespark,cloud_fan,mannswinky,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-35445,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 02 06:23:14 UTC 2021,,,,,,,,,,"0|z0ncj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/21 09:10;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/31470;;;","31/Mar/21 06:29;cloud_fan;Issue resolved by pull request 31470
[https://github.com/apache/spark/pull/31470];;;","01/Apr/21 03:49;gurwls223;Reverted in https://github.com/apache/spark/commit/cc451c16a3d28bcdba226fa05f1b786ff2f02612;;;","01/Apr/21 14:27;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/32027;;;","02/Apr/21 06:23;cloud_fan;Issue resolved by pull request 32027
[https://github.com/apache/spark/pull/32027];;;",,,,,,,,,,,,,,,
Improve SQLQueryTestSuite so as could run on windows system,SPARK-34352,13356635,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,beliefer,beliefer,04/Feb/21 03:43,12/Dec/22 18:11,13/Jul/23 08:47,09/Feb/21 02:00,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"The current implement of SQLQueryTestSuite cannot run on windows system.
Becasue the code below will fail on windows system:
assume(TestUtils.testCommandAvailable(""/bin/bash""))",,apachespark,beliefer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 09 02:00:31 UTC 2021,,,,,,,,,,"0|z0nc5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/21 03:49;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/31466;;;","09/Feb/21 02:00;gurwls223;Fixed in https://github.com/apache/spark/pull/31466;;;",,,,,,,,,,,,,,,,,,
io.file.buffer.size set by spark.buffer.size will override by hive-site.xml may cause perf regression,SPARK-34346,13356509,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,Qin Yao,Qin Yao,Qin Yao,03/Feb/21 14:13,13/Feb/21 11:07,13/Jul/23 08:47,05/Feb/21 21:21,3.0.2,3.1.1,,,,,,,,,,,,,,3.0.2,3.1.1,,,Spark Core,SQL,,,0,,,"In many real-world cases, when interacting with hive catalog through Spark SQL, users may just share the `hive-site.xml` for their hive jobs and make a copy to `SPARK_HOME`/conf w/o modification. In Spark, when we generate Hadoop configurations, we will use `spark.buffer.size(65536)` to reset  `io.file.buffer.size(4096)`. But when we load the hive-site.xml, we may ignore this behavior and reset `io.file.buffer.size` again according to `hive-site.xml`.

1. The configuration priority for setting Hadoop and Hive config here is not right, while literally, the order should be `spark > spark.hive > spark.hadoop > hive > hadoop`

2. This breaks `spark.buffer.size` congfig's behavior for tuning the IO performance w/ HDFS if there is an existing `io.file.buffer.size` in hive-site.xml 
",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34431,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 08 00:25:48 UTC 2021,,,,,,,,,,"0|z0nbdk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/21 14:40;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31460;;;","03/Feb/21 14:40;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31460;;;","05/Feb/21 05:58;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31482;;;","05/Feb/21 18:37;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31492;;;","05/Feb/21 18:38;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31492;;;","08/Feb/21 00:25;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31515;;;",,,,,,,,,,,,,,
./build/mvn error output on aarch64,SPARK-34341,13356397,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yikunkero,yikunkero,yikunkero,03/Feb/21 07:53,12/Dec/22 18:10,13/Jul/23 08:47,04/Feb/21 01:43,3.1.2,,,,,,,,,,,,,,,3.2.0,,,,Build,,,,0,,,"You can see the below error ouput in every spark arm jenkins job [1]:
{quote}/build/zinc-0.3.15/bin/nailgun: line 50: /root/dev/spark/build/zinc-0.3.15/bin/ng/linux32/ng: cannot execute binary file: Exec format error
{quote}
And you can also reproduce this error output using the simple cmd: `build/mvn -v` on aarch64:
{quote}root@yikun-arm:~/dev/spark# uname -a
 Linux yikun-arm 4.15.0-70-generic #79-Ubuntu SMP Tue Nov 12 10:36:10 UTC 2019 aarch64 aarch64 aarch64 GNU/Linux
 root@yikun-arm:~/dev/spark# uname -m
 aarch64
 root@yikun-arm:~/dev/spark# build/mvn -v
 */root/dev/spark/build/zinc-0.3.15/bin/nailgun: line 50: /root/dev/spark/build/zinc-0.3.15/bin/ng/linux32/ng: cannot execute binary file: Exec format error*
 */root/dev/spark/build/zinc-0.3.15/bin/nailgun: line 50: /root/dev/spark/build/zinc-0.3.15/bin/ng/linux32/ng: cannot execute binary file: Exec format error*
 Using `mvn` from path: /root/dev/spark/build/apache-maven-3.6.3/bin/mvn
 Apache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f)
 Maven home: /root/dev/spark/build/apache-maven-3.6.3
 Java version: 1.8.0_222, vendor: Private Build, runtime: /usr/lib/jvm/java-8-openjdk-arm64/jre
 Default locale: en, platform encoding: UTF-8
 OS name: ""linux"", version: ""4.15.0-70-generic"", arch: ""aarch64"", family: ""unix""
 */root/dev/spark/build/zinc-0.3.15/bin/nailgun: line 50: /root/dev/spark/build/zinc-0.3.15/bin/ng/linux32/ng: cannot execute binary file: Exec format error*
{quote}
The reason of this error output is that the zinc is not supported well on aarch64, unfortunately, the stand alone zinc is deprecated now[2], so that we have no chance to support it on aarch64 in stand alone verison.

Looks like we should skip the zinc related command on aarch64 platform first, and then upgrade the sbt/zinc[3] in future.
  

[1] [https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-arm/547/consoleFull]
 [2] [https://github.com/typesafehub/zinc]
 [3] [https://github.com/sbt/zinc]",,apachespark,ganeshraju,v_ganeshraju,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 04 01:43:08 UTC 2021,,,,,,,,,,"0|z0naoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/21 08:22;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31454;;;","04/Feb/21 01:43;gurwls223;Issue resolved by pull request 31454
[https://github.com/apache/spark/pull/31454];;;",,,,,,,,,,,,,,,,,,
Fix PostgresDialect to handle money types properly,SPARK-34333,13356255,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,02/Feb/21 17:20,17/Feb/21 01:54,13/Jul/23 08:47,17/Feb/21 01:54,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"In the current master, PostgresDialect supports money and money[] types.
But those types seems not to be able to handle those types properly. 

Error of money type.
{code}
[info]   org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.204 executor driver): org.postgresql.util.PSQLException: Bad value for type double : 1,000.00
[info] 	at org.postgresql.jdbc.PgResultSet.toDouble(PgResultSet.java:3104)
[info] 	at org.postgresql.jdbc.PgResultSet.getDouble(PgResultSet.java:2432)
[info] 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$5(JdbcUtils.scala:418)
{code}

Error of money[] type.
{code}
[info]   org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.204 executor driver): org.postgresql.util.PSQLException: Bad value for type double : $2,000.00
[info] 	at org.postgresql.jdbc.PgResultSet.toDouble(PgResultSet.java:3104)
[info] 	at org.postgresql.jdbc.ArrayDecoding$5.parseValue(ArrayDecoding.java:235)
[info] 	at org.postgresql.jdbc.ArrayDecoding$AbstractObjectStringArrayDecoder.populateFromString(ArrayDecoding.java:122)
[info] 	at org.postgresql.jdbc.ArrayDecoding.readStringArray(ArrayDecoding.java:764)
[info] 	at org.postgresql.jdbc.PgArray.buildArray(PgArray.java:310)
[info] 	at org.postgresql.jdbc.PgArray.getArrayImpl(PgArray.java:171)
[info] 	at org.postgresql.jdbc.PgArray.getArray(PgArray.java:111)
{code}

These errors seem to related to the following issues.
https://github.com/pgjdbc/pgjdbc/issues/100
https://github.com/pgjdbc/pgjdbc/issues/1405",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 17 01:54:17 UTC 2021,,,,,,,,,,"0|z0n9t4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/21 17:37;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31442;;;","02/Feb/21 17:38;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31442;;;","17/Feb/21 01:54;sarutak;This issue is resolved by https://github.com/apache/spark/pull/31442;;;",,,,,,,,,,,,,,,,,
Speed up DS v2 metadata col resolution,SPARK-34331,13356222,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,cloud_fan,cloud_fan,02/Feb/21 14:57,05/Feb/21 08:40,13/Jul/23 08:47,05/Feb/21 08:40,3.1.0,,,,,,,,,,,,,,,3.1.1,,,,SQL,,,,0,,,There is a performance regression in Spark 3.1.1. Please refer to the PR description since the fix is ready.,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 05 08:40:49 UTC 2021,,,,,,,,,,"0|z0n9ls:",9223372036854775807,,,,,,,,,,,,,3.1.1,,,,,,,,,,"02/Feb/21 15:55;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31440;;;","02/Feb/21 15:55;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31440;;;","05/Feb/21 08:40;cloud_fan;Issue resolved by pull request 31440
[https://github.com/apache/spark/pull/31440];;;",,,,,,,,,,,,,,,,,
Cannot work with Spark,SPARK-34328,13356154,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,toprak,toprak,02/Feb/21 09:55,02/Feb/21 10:27,13/Jul/23 08:47,02/Feb/21 10:27,3.0.1,,,,,,,,,,,,,,,,,,,PySpark,,,,0,,,"I have already googled this problem and tried the other suggestions. I don't even understand why SPARK is this hard to set up and work properly without any problems. I have already googled this problem and tried the other suggestions. I don't even understand why SPARK is this hard to set up and work properly without any problems. 
I set up Spark to my ubuntu server. I use Python 3.6 with Spark 3.0.1 / Hadoop 2.7
The code that I'm trying to run is this, it's from a udemy lecture.


```
from pyspark import SparkConf, SparkContextimport collectionsimport py4j
conf = SparkConf().setMaster('local').setAppName('RatingsHist')sc   = SparkContext(conf = conf)lines = sc.textFile(""/home/toprak/Codes/spark/uDemyData/Friends/fakefriends.csv"")
def parseLine(line):    fields = line.split(',')    age = int(fields[21])    numFriends = int(fields[3])    return (age, numFriends)
rdd = lines.map(parseLine)
totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])results = averagesByAge.collect()for result in results:    print(result)

```When I try to run, it gives me:


```---------------------------------------------------------------------------Py4JJavaError                             Traceback (most recent call last)<ipython-input-8-47d1d28735c7> in <module>      1 totalsByAge = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))      2 averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])----> 3 results = averagesByAge.collect()      4 for result in results:      5     print(result)
~/.local/lib/python3.6/site-packages/pyspark/rdd.py in collect(self)    887         """"""    888         with SCCallSiteSync(self.context) as css:--> 889             sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())    890         return list(_load_from_socket(sock_info, self._jrdd_deserializer))    891 
~/.local/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args)   1303         answer = self.gateway_client.send_command(command)   1304         return_value = get_return_value(-> 1305             answer, self.gateway_client, self.target_id, self.name)   1306    1307         for temp_arg in temp_args:
~/.local/lib/python3.6/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)    326                 raise Py4JJavaError(    327                     ""An error occurred while calling \{0}{1}\{2}.\n"".--> 328                     format(target_id, ""."", name), value)    329             else:    330                 raise Py4JError(
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 2, 192.168.1.27, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py"", line 605, in main    process()  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py"", line 595, in process    out_iter = func(split_index, iterator)  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/rdd.py"", line 2596, in pipeline_func    return func(split, prev_func(split, iterator))  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/rdd.py"", line 2596, in pipeline_func    return func(split, prev_func(split, iterator))  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/rdd.py"", line 425, in func    return f(iterator)  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/rdd.py"", line 1946, in combineLocally    merger.mergeValues(iterator)  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py"", line 238, in mergeValues    for k, v in iterator:  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py"", line 107, in wrapper    return f(*args, **kwargs)  File ""<ipython-input-5-ae8b1b1c8ea5>"", line 3, in parseLineIndexError: list index out of range
 at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503) at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638) at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621) at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209) at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132) at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52) at org.apache.spark.scheduler.Task.run(Task.scala:127) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)
Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059) at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008) at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007) at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007) at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973) at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973) at scala.Option.foreach(Option.scala:407) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2120) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2139) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2164) at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:388) at org.apache.spark.rdd.RDD.collect(RDD.scala:1003) at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168) at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:282) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:238) at java.lang.Thread.run(Thread.java:748)Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py"", line 605, in main    process()  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py"", line 595, in process    out_iter = func(split_index, iterator)  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/rdd.py"", line 2596, in pipeline_func    return func(split, prev_func(split, iterator))  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/rdd.py"", line 2596, in pipeline_func    return func(split, prev_func(split, iterator))  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/rdd.py"", line 425, in func    return f(iterator)  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/rdd.py"", line 1946, in combineLocally    merger.mergeValues(iterator)  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py"", line 238, in mergeValues    for k, v in iterator:  File ""/home/toprak/.local/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py"", line 107, in wrapper    return f(*args, **kwargs)  File ""<ipython-input-5-ae8b1b1c8ea5>"", line 3, in parseLineIndexError: list index out of range
 at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503) at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638) at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621) at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209) at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132) at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52) at org.apache.spark.scheduler.Task.run(Task.scala:127) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ... 1 more
```",,toprak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2021-02-02 09:55:15.0,,,,,,,,,,"0|z0n96o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Omit inlining passwords during build process.,SPARK-34327,13356144,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prashant,prashant,prashant,02/Feb/21 08:30,12/Dec/22 18:10,13/Jul/23 08:47,03/Feb/21 06:03,2.4.7,3.0.1,3.1.1,3.2.0,,,,,,,,,,,,2.4.8,3.0.2,3.1.1,,Build,,,,0,,,"Spark release process uses github urls embedded with passwords. And this script would inline those passwords into build info and then release it.

It is important to strip this, before such inadvertent exposure.",,apachespark,prashant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 03 06:03:58 UTC 2021,,,,,,,,,,"0|z0n94g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/21 08:35;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/31436;;;","02/Feb/21 08:36;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/31436;;;","03/Feb/21 06:03;gurwls223;Issue resolved by pull request 31436
[https://github.com/apache/spark/pull/31436];;;",,,,,,,,,,,,,,,,,
"""SPARK-31793: FileSourceScanExec metadata should contain limited file paths"" fails in some edge-case",SPARK-34326,13356138,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,02/Feb/21 07:58,12/Dec/22 18:10,13/Jul/23 08:47,03/Feb/21 23:46,3.1.0,,,,,,,,,,,,,,,3.1.1,,,,SQL,,,,0,,,"Our internal build failed with this test, and looks like the calculation in UT is missing some points about the format of location.",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 03 23:46:59 UTC 2021,,,,,,,,,,"0|z0n934:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/21 07:58;kabhwan;Will provide a PR shortly.;;;","02/Feb/21 08:15;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/31435;;;","02/Feb/21 08:15;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/31435;;;","02/Feb/21 22:38;kabhwan;Issue resolved via https://github.com/apache/spark/pull/31435

(Let me set the version 3.1.1 as there's no indication of new RC yet.);;;","03/Feb/21 03:35;gurwls223;Reverted https://github.com/apache/spark/commit/e927bf90e0e035a5103e029f2524239ee11c2961;;;","03/Feb/21 04:46;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/31449;;;","03/Feb/21 23:46;kabhwan;Issue resolved by pull request 31449
[https://github.com/apache/spark/pull/31449];;;",,,,,,,,,,,,,
Self-join after cogroup applyInPandas fails due to unresolved conflicting attributes,SPARK-34319,13356089,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,02/Feb/21 02:53,12/Dec/22 18:10,13/Jul/23 08:47,02/Feb/21 07:27,3.0.0,3.0.1,3.1.0,3.2.0,,,,,,,,,,,,3.0.2,3.1.1,,,SQL,,,,0,,," 
{code:java}
df = spark.createDataFrame([(1, 1)], (""column"", ""value""))row = df.groupby(""ColUmn"").cogroup(
    df.groupby(""COLUMN"")
).applyInPandas(lambda r, l: r + l, ""column long, value long"")
row.join(row).show()
{code}
{code:java}
Conflicting attributes: column#163321L,value#163322L
;;
’Join Inner
:- FlatMapCoGroupsInPandas [ColUmn#163312L], [COLUMN#163312L], <lambda>(column#163312L, value#163313L, column#163312L, value#163313L), [column#163321L, value#163322L]
:  :- Project [ColUmn#163312L, column#163312L, value#163313L]
:  :  +- LogicalRDD [column#163312L, value#163313L], false
:  +- Project [COLUMN#163312L, column#163312L, value#163313L]
:     +- LogicalRDD [column#163312L, value#163313L], false
+- FlatMapCoGroupsInPandas [ColUmn#163312L], [COLUMN#163312L], <lambda>(column#163312L, value#163313L, column#163312L, value#163313L), [column#163321L, value#163322L]
   :- Project [ColUmn#163312L, column#163312L, value#163313L]
   :  +- LogicalRDD [column#163312L, value#163313L], false
   +- Project [COLUMN#163312L, column#163312L, value#163313L]
      +- LogicalRDD [column#163312L, value#163313L], false
{code}
 

 ",,apachespark,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-27463,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 02 07:27:32 UTC 2021,,,,,,,,,,"0|z0n8s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/21 03:05;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/31429;;;","02/Feb/21 03:06;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/31429;;;","02/Feb/21 07:27;gurwls223;Issue resolved by pull request 31429
[https://github.com/apache/spark/pull/31429];;;",,,,,,,,,,,,,,,,,
Dataset.colRegex should work with column names and qualifiers which contain newlines,SPARK-34318,13356066,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,01/Feb/21 22:24,08/Mar/21 09:21,13/Jul/23 08:47,02/Feb/21 12:50,3.2.0,,,,,,,,,,,,,,,2.4.8,3.0.2,3.1.1,3.2.0,SQL,,,,0,,,"In the current master, Dataset.colRegex doesn't work with column names or qualifiers which contain newlines.",,apachespark,maropu,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 03 11:58:53 UTC 2021,,,,,,,,,,"0|z0n8n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/21 22:35;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31426;;;","02/Feb/21 12:50;maropu;Resolved by https://github.com/apache/spark/pull/31426;;;","03/Feb/21 11:48;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31457;;;","03/Feb/21 11:49;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31457;;;","03/Feb/21 11:55;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31458;;;","03/Feb/21 11:56;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31458;;;","03/Feb/21 11:58;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31459;;;",,,,,,,,,,,,,
Wrong discovered partition value,SPARK-34314,13356034,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,01/Feb/21 19:29,23/Apr/21 11:18,13/Jul/23 08:47,19/Feb/21 08:36,2.4.8,3.0.2,3.1.0,3.2.0,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"The example below portraits the issue:
{code:scala}
      val df = Seq((0, ""AA""), (1, ""-0"")).toDF(""id"", ""part"")
      df.write
        .partitionBy(""part"")
        .format(""parquet"")
        .save(path)
      val readback = spark.read.parquet(path)
      readback.printSchema()
      readback.show(false)
{code}

It write the partition value as string:
{code}
/private/var/folders/p3/dfs6mf655d7fnjrsjvldh0tc0000gn/T/spark-e09eae99-7ecf-4ab2-b99b-f63f8dea658d
├── _SUCCESS
├── part=-0
│   └── part-00001-02144398-2896-4d21-9628-a8743d098cb4.c000.snappy.parquet
└── part=AA
    └── part-00000-02144398-2896-4d21-9628-a8743d098cb4.c000.snappy.parquet
{code}
*""-0""* and ""AA"".

but when Spark reads data back, it transforms ""-0"" to ""0""
{code}
root
 |-- id: integer (nullable = true)
 |-- part: string (nullable = true)

+---+----+
|id |part|
+---+----+
|0  |AA  |
|1  |0   |
+---+----+
{code}
",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,SPARK-35123,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 19 08:36:32 UTC 2021,,,,,,,,,,"0|z0n8g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/21 19:51;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31423;;;","01/Feb/21 19:52;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31423;;;","11/Feb/21 11:56;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31549;;;","19/Feb/21 08:36;cloud_fan;Issue resolved by pull request 31549
[https://github.com/apache/spark/pull/31549];;;",,,,,,,,,,,,,,,,
PostgresDialect can't treat arrays of some types,SPARK-34311,13355941,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,01/Feb/21 13:33,10/Feb/21 02:30,13/Jul/23 08:47,10/Feb/21 02:30,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"PostgresDialect implements the logic to treat array of some types but it's not enough.
Currently, the following types can't be treated.

* xml
* tsvector
* tsquery
* macaddr
* txid_snapshot
* point
* line
* lseg
* box
* path
* polygon
* circle
* pg_lsn

 ",,apachespark,maropu,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 10 02:30:16 UTC 2021,,,,,,,,,,"0|z0n7vc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/21 13:45;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31419;;;","10/Feb/21 02:30;maropu;Resolved by https://github.com/apache/spark/pull/31419;;;",,,,,,,,,,,,,,,,,,
Fix of typos in documentation of pyspark.sql.functions and output of lint-python,SPARK-34300,13355682,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,DavidToneian,DavidToneian,DavidToneian,30/Jan/21 23:24,12/Dec/22 18:10,13/Jul/23 08:47,02/Feb/21 00:31,3.0.1,,,,,,,,,,,,,,,3.1.1,,,,PySpark,,,,0,,,"Minor documentation and standard output issues:

* {{dev/lint-python}} contains a typo when printing a warning regarding bad Sphinx version (""lower then 3.1"" rather than ""lower than 3.1"")
* The documentations of the functions {{lag}} and {{lead}} of {{pyspark.sql.functions}} refer to a parameter {{defaultValue}}, which in reality is named {{default}}.
* The documentation strings of functions in {{pyspark.sql.functions}} make reference to the {{Column}} class, which is not resolved by Sphinx unless fully qualified as {{pyspark.sql.Column}}",,apachespark,DavidToneian,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 02 00:31:39 UTC 2021,,,,,,,,,,"0|z0n6a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/21 23:34;apachespark;User 'DavidToneian' has created a pull request for this issue:
https://github.com/apache/spark/pull/31401;;;","30/Jan/21 23:35;apachespark;User 'DavidToneian' has created a pull request for this issue:
https://github.com/apache/spark/pull/31401;;;","02/Feb/21 00:31;gurwls223;Issue resolved by pull request 31401
[https://github.com/apache/spark/pull/31401];;;",,,,,,,,,,,,,,,,,
Do not reregister BlockManager when SparkContext is stopped,SPARK-34273,13355070,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,28/Jan/21 03:41,28/Jan/21 21:17,13/Jul/23 08:47,28/Jan/21 21:16,2.4.7,3.0.1,3.1.1,3.2.0,,,,,,,,,,,,2.4.8,3.0.2,3.1.1,,Spark Core,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 28 21:16:22 UTC 2021,,,,,,,,,,"0|z0n2i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/21 03:44;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31373;;;","28/Jan/21 21:16;dongjoon;Issue resolved by pull request 31373
[https://github.com/apache/spark/pull/31373];;;",,,,,,,,,,,,,,,,,,
Combine StateStoreMetrics should not override StateStoreCustomMetric,SPARK-34270,13355028,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,27/Jan/21 21:27,30/Jan/21 05:21,13/Jul/23 08:47,30/Jan/21 05:21,2.3.4,2.4.7,3.0.1,3.1.2,3.2.0,,,,,,,,,,,2.4.8,3.0.2,3.1.1,,Structured Streaming,,,,0,,,"For stateful join in structured streaming, we need to combine StateStoreMetrics from both left and right side. Currently we simply take arbitrary one from custom metrics with same name from left and right. By doing this we miss half of metric number.",,apachespark,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 30 05:21:45 UTC 2021,,,,,,,,,,"0|z0n28w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/21 21:35;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31369;;;","30/Jan/21 05:21;dongjoon;Issue resolved by pull request 31369
[https://github.com/apache/spark/pull/31369];;;",,,,,,,,,,,,,,,,,,
The Signature for ConcatWs in Spark SQL Docs Is Inconsistent with the Actual Behavior,SPARK-34268,13355009,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yuningzh,yuningzh,yuningzh,27/Jan/21 18:44,12/Dec/22 18:10,13/Jul/23 08:47,28/Jan/21 05:08,3.2.0,,,,,,,,,,,,,,,2.4.8,3.0.2,3.1.1,,Documentation,,,,0,,,"In [https://spark.apache.org/docs/latest/api/sql/index.html#concat_ws], the signature of concat_ws is concat_ws(sep, [str | array(str)]+). However, it doesn't actually need any str or array(str) arguments. Only having the sep argument seems to work.

For example, `select concat_ws(""a"")` returns an empty string.
{code:java}
scala> sql(""""""select concat_ws(""a"")"""""").show
+------------+
|concat_ws(a)|
+------------+
|            |
+------------+
{code}",,apachespark,yuningzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 28 05:08:04 UTC 2021,,,,,,,,,,"0|z0n24o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/21 01:02;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/31370;;;","28/Jan/21 05:08;gurwls223;Issue resolved by pull request 31370
[https://github.com/apache/spark/pull/31370];;;",,,,,,,,,,,,,,,,,,
UnresolvedException when creating temp view twice,SPARK-34260,13354885,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,linhongliu-db,linhongliu-db,linhongliu-db,27/Jan/21 09:49,29/Jan/21 03:29,13/Jul/23 08:47,28/Jan/21 04:59,2.4.7,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,2.4.8,3.0.2,3.1.1,,SQL,,,,0,,,"when creating temp view twice, there is an UnresolvedException, queries to reproduce:

{code:java}
sql(""create or replace temp view v as select * from (select * from range(10))"")
sql(""create or replace temp view v as select * from (select * from range(10))"")
{code}

error message:

{noformat}
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to toAttribute on unresolved object, tree: *
        at org.apache.spark.sql.catalyst.analysis.Star.toAttribute(unresolved.scala:295)
        at org.apache.spark.sql.catalyst.plans.logical.Project.$anonfun$output$1(basicLogicalOperators.scala:62)
        at scala.collection.immutable.List.map(List.scala:293)
        at org.apache.spark.sql.catalyst.plans.logical.Project.output(basicLogicalOperators.scala:62)
        at org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.output(basicLogicalOperators.scala:945)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$allAttributes$1(QueryPlan.scala:431)
        at scala.collection.immutable.List.flatMap(List.scala:366)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.allAttributes$lzycompute(QueryPlan.scala:431)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.allAttributes(QueryPlan.scala:431)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$doCanonicalize$2(QueryPlan.scala:404)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:132)
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
        at scala.collection.immutable.List.foreach(List.scala:431)
        at scala.collection.TraversableLike.map(TraversableLike.scala:286)
        at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
        at scala.collection.immutable.List.map(List.scala:305)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:132)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:137)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:137)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.doCanonicalize(QueryPlan.scala:389)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:373)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:372)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.sameResult(QueryPlan.scala:420)
        at org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:118)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)
        at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:228)
        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3699)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3697)
        at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
        at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
        at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
        at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
        at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
{noformat}
",,apachespark,dongjoon,linhongliu-db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 29 02:06:50 UTC 2021,,,,,,,,,,"0|z0n1dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/21 11:12;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/31360;;;","27/Jan/21 11:13;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/31360;;;","27/Jan/21 14:26;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/31365;;;","27/Jan/21 14:27;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/31365;;;","28/Jan/21 04:59;dongjoon;Issue resolved by pull request 31360
[https://github.com/apache/spark/pull/31360];;;","29/Jan/21 02:05;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/31388;;;","29/Jan/21 02:06;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/31388;;;",,,,,,,,,,,,,
TRUNCATE TABLE resets stats for non-empty v1 table,SPARK-34251,13354716,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,26/Jan/21 16:41,27/Jan/21 07:02,13/Jul/23 08:47,27/Jan/21 07:02,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"The example below portraits the issue:
{code:sql}
spark-sql> CREATE TABLE tbl (c0 int, part int) PARTITIONED BY (part);
spark-sql> INSERT INTO tbl PARTITION (part=0) SELECT 0;
spark-sql> INSERT INTO tbl PARTITION (part=1) SELECT 1;
spark-sql> ANALYZE TABLE tbl COMPUTE STATISTICS;
spark-sql> DESCRIBE TABLE EXTENDED tbl;
...
Statistics	4 bytes, 2 rows
...
{code}
Let's truncate one partition:
{code:sql}
spark-sql> TRUNCATE TABLE tbl PARTITION (part=1);
spark-sql> DESCRIBE TABLE EXTENDED tbl;
...
Statistics	0 bytes, 0 rows
...
spark-sql> SELECT * FROM tbl;
0	0
{code}
*The last query returns a row but stats show 0 rows. *",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 27 07:02:20 UTC 2021,,,,,,,,,,"0|z0n0bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/21 16:41;maxgekk;I am working on bug fix.;;;","26/Jan/21 17:44;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31350;;;","27/Jan/21 07:02;cloud_fan;Issue resolved by pull request 31350
[https://github.com/apache/spark/pull/31350];;;",,,,,,,,,,,,,,,,,
v2 Overwrite w/ null static partition raise Cannot translate expression to source filter: null ,SPARK-34236,13354590,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,26/Jan/21 06:22,27/Jan/21 04:13,13/Jul/23 08:47,27/Jan/21 04:13,3.1.0,,,,,,,,,,,,,,,3.1.1,,,,SQL,,,,0,,,"
{code:java}
SPARK-34223: static partition with null raise NPE *** FAILED *** (19 milliseconds)
[info]   org.apache.spark.sql.AnalysisException: Cannot translate expression to source filter: null
[info]   at org.apache.spark.sql.execution.datasources.v2.V2Writes$$anonfun$apply$1.$anonfun$applyOrElse$1(V2Writes.scala:50)
[info]   at scala.collection.immutable.List.flatMap(List.scala:366)
[info]   at org.apache.spark.sql.execution.datasources.v2.V2Writes$$anonfun$apply$1.applyOrElse(V2Writes.scala:47)
[info]   at org.apache.spark.sql.execution.datasources.v2.V2Writes$$anonfun$apply$1.applyOrElse(V2Writes.scala:39)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:317)
[info]   at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:317)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
[info]   at org.apache.spark.sql.execution.datasources.v2.V2Writes$.apply(V2Writes.scala:39)
[info]   at org.apache.spark.sql.execution.datasources.v2.V2Writes$.apply(V2Writes.scala:35)
[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
[info]   at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
[info]   at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
[info]   at scala.collection.immutable.List.foldLeft(List.scala:91)
[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
[info]   at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
[info]   at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:87)
[info]   at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
[info]   at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
[info]   at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[info]   at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
[info]   at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:84)
[info]   at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:84)
[info]   at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:95)
[info]   at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:113)
[info]   at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:110)
[info]   at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:101)
[info]   at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[info]   at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[info]   at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[info]   at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[info]   at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3697)
[info]   at org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)
[info]   at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
[info]   at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[info]   at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
[info]   at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
[info]   at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[info]   at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
[info]   at org.apache.spark.sql.test.SQLTestUtilsBase.$anonfun$sql$1(SQLTestUtils.scala:231)
[info]   at org.apache.spark.sql.SQLInsertTestSuite.$anonfun$$init$$42(SQLInsertTestSuite.scala:207)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1437)
[info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withTable(SQLTestUtils.scala:305)
[info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withTable$(SQLTestUtils.scala:303)
[info]   at org.apache.spark.sql.DSV2SQLInsertTestSuite.withTable(SQLInsertTestSuite.scala:220)
[info]   at org.apache.spark.sql.SQLInsertTestSuite.$anonfun$$init$$41(SQLInsertTestSuite.scala:205)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:190)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:176)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:188)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:200)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:200)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:182)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:61)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:61)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:233)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
{code}
",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 27 04:13:45 UTC 2021,,,,,,,,,,"0|z0mzjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/21 06:44;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31339;;;","26/Jan/21 06:45;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31339;;;","27/Jan/21 04:13;cloud_fan;Issue resolved by pull request 31339
[https://github.com/apache/spark/pull/31339];;;",,,,,,,,,,,,,,,,,
NPE for char padding in binary comparison,SPARK-34233,13354564,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,26/Jan/21 02:56,01/Feb/21 14:19,13/Jul/23 08:47,27/Jan/21 07:00,3.1.0,3.1.1,,,,,,,,,,,,,,3.1.1,,,,SQL,,,,0,,,"test(""char type comparison: top-level columns"") {
    withTable(""t"") {
      sql(s""CREATE TABLE t(c1 CHAR(2), c2 CHAR(5)) USING $format"")
      sql(""INSERT INTO t VALUES ('a', 'a')"")
      testConditions(spark.table(""t""), Seq(
        (""c1 = 'a'"", true),
        (""'a' = c1"", true),
        (""c1 = 'a  '"", true),
        (""c1 > 'a'"", false),
        (""c1 IN ('a', 'b')"", true),
        (""c1 = c2"", true),
        (""c1 < c2"", false),
        (""c1 IN (c2)"", true),
       {color:red} (""c1 = null"", false))){color}
    }
  }

the red one will raise NPE",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 01 04:33:38 UTC 2021,,,,,,,,,,"0|z0mze0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/21 03:35;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31336;;;","26/Jan/21 03:36;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31336;;;","27/Jan/21 07:00;cloud_fan;Issue resolved by pull request 31336
[https://github.com/apache/spark/pull/31336];;;","01/Feb/21 04:33;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31407;;;","01/Feb/21 04:33;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31407;;;",,,,,,,,,,,,,,,
redact credentials not working when log slow event enabled,SPARK-34232,13354558,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,warrenzhu25,warrenzhu25,warrenzhu25,26/Jan/21 01:56,12/Dec/22 18:10,13/Jul/23 08:47,26/Jan/21 08:12,3.0.1,,,,,,,,,,,,,,,3.0.2,3.1.1,,,Spark Core,,,,0,,,"When process time of event SparkListenerEnvironmentUpdate exceeded logSlowEventThreshold, the credentials will be logged without consideration of redact.",,apachespark,warrenzhu25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 24 09:07:42 UTC 2021,,,,,,,,,,"0|z0mzco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/21 02:04;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/31335;;;","26/Jan/21 08:12;gurwls223;Fixed in https://github.com/apache/spark/pull/31335;;;","24/Feb/21 09:07;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31634;;;",,,,,,,,,,,,,,,,,
AvroSuite has test failure when run from IDE due to bad loading of resource file,SPARK-34231,13354539,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xkrogen,xkrogen,xkrogen,25/Jan/21 21:58,12/Dec/22 18:10,13/Jul/23 08:47,27/Jan/21 07:19,3.2.0,,,,,,,,,,,,,,,2.4.8,3.0.2,3.1.1,,SQL,Tests,,,0,,,"Within {{AvroSuite}} the test {{Ignore corrupt Avro file if flag IGNORE_CORRUPT_FILES enabled}} attempts to load a resource file like:
{code}
        val srcFile = new File(""src/test/resources/episodes.avro"")
        val destFile = new File(dir, ""episodes.avro"")
        FileUtils.copyFile(srcFile, destFile)
{code}
This works okay when tests are run via Maven/sbt , but really isn't the right way to access a resource, causing failures when run via IntelliJ, and actually {{episodes.avro}} has already been loaded properly as {{episodesAvro}}. Switch over to loading it properly:

{code}
        Files.copy(
          Paths.get(new URL(episodesAvro).toURI),
          Paths.get(dir.getCanonicalPath, ""episodes.avro""))
{code}",,apachespark,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 27 07:19:16 UTC 2021,,,,,,,,,,"0|z0mz8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/21 07:16;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/31332;;;","27/Jan/21 07:16;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/31332;;;","27/Jan/21 07:19;gurwls223;Fixed in https://github.com/apache/spark/pull/31332;;;",,,,,,,,,,,,,,,,,
Avro should read decimal values with the file schema,SPARK-34229,13354522,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,25/Jan/21 20:32,12/Dec/22 18:10,13/Jul/23 08:47,26/Jan/21 05:32,2.4.7,3.0.1,3.1.1,3.2.0,,,,,,,,,,,,2.4.8,3.0.2,3.1.1,,SQL,,,,0,correctness,,"{code:java}
scala> sql(""SELECT 3.14 a"").write.format(""avro"").save(""/tmp/avro"")
scala> spark.read.schema(""a DECIMAL(4, 3)"").format(""avro"").load(""/tmp/avro"").show
+-----+
|    a|
+-----+
|0.314|
+-----+ {code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 26 05:32:28 UTC 2021,,,,,,,,,,"0|z0mz4o:",9223372036854775807,,,,,,,,,,,,,2.4.8,3.0.2,3.1.1,,,,,,,,"25/Jan/21 20:36;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31329;;;","26/Jan/21 05:32;gurwls223;Issue resolved by pull request 31329
[https://github.com/apache/spark/pull/31329];;;",,,,,,,,,,,,,,,,,,
WindowFunctionFrame should clear its states during preparation,SPARK-34227,13354465,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,25/Jan/21 15:18,07/Jun/22 02:07,13/Jul/23 08:47,26/Jan/21 08:50,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,correctness,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 26 08:50:30 UTC 2021,,,,,,,,,,"0|z0mys0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/21 15:25;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31325;;;","26/Jan/21 08:50;cloud_fan;Issue resolved by pull request 31325
[https://github.com/apache/spark/pull/31325];;;",,,,,,,,,,,,,,,,,,
Jars or file paths which contain spaces are generating FileNotFoundException exception,SPARK-34225,13354435,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,lucian.timar,lucian.timar,25/Jan/21 13:26,14/Apr/21 08:40,13/Jul/23 08:47,22/Mar/21 05:10,3.0.1,,,,,,,,,,,,,,,3.1.2,3.2.0,,,Spark Core,Spark Submit,Windows,,0,,," Whenever I try to use jars or files whose paths contain spaces FileNotFoundException exception is generated

just run spark-shell command having spaces in paths

See below
{code:java}
c:>spark-shell --files ""c:\Program Files\...\myjar.jar""
or
c:>spark-shell --jars""c:\Program Files\...\myjar.jar""
or 
c:>spark-shell --conf spark.jars=""c:\Program Files\...\myjar.jar""

any combination produce the same exception

 java.io.FileNotFoundException: Jar c:\Program%20Files\........ not found
        at org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1833)
        at org.apache.spark.SparkContext.addJar(SparkContext.scala:1887)
        at org.apache.spark.SparkContext.$anonfun$new$11(SparkContext.scala:490)
        at org.apache.spark.SparkContext.$anonfun$new$11$adapted(SparkContext.scala:490)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:490)
        at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2574)
        at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:934)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:928)
        at org.apache.spark.repl.Main$.createSparkSession(Main.scala:106)
{code}
I have noticed that the following function is causing the issue.
{code:java}
def addJar(path: String): Unit = {
...
} else {
  val uri = new Path(path).toUri
  {code}
the path as the string is 

""file:///C:/Program%20Files/Nokia/....jar""

this call generates the following uri

""file:///C:/Program*%25*20Files/Nokia/....jar""

which results in an invalid file name.

Using 
{code:java}
val uri = Utils.resolveURI(path){code}
seems to resolve the issue.

 Until a fix is provided are there any workarounds to overcome this issue?

 ",,apachespark,lucian.timar,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 14 08:40:37 UTC 2021,,,,,,,,,,"0|z0mylc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/21 04:26;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31718;;;","22/Mar/21 05:10;sarutak;This issue is resolved in [https://github.com/apache/spark/pull/31718] .;;;","14/Apr/21 08:40;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32164;;;",,,,,,,,,,,,,,,,,
Ensure all resource opened by `Source.fromXXX` method are closed,SPARK-34224,13354430,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,25/Jan/21 13:03,12/Dec/22 18:10,13/Jul/23 08:47,26/Jan/21 14:44,3.2.0,,,,,,,,,,,,,,,3.0.2,3.1.1,,,Tests,,,,0,,,"Using a function like .mkString or .getLines directly on a Source opened by fromFile, fromURL, fromURI will leak the underlying file handle, we should ensure these resource closed.",,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 26 14:44:52 UTC 2021,,,,,,,,,,"0|z0myk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/21 13:29;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31323;;;","26/Jan/21 11:16;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31344;;;","26/Jan/21 11:36;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31345;;;","26/Jan/21 11:37;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31345;;;","26/Jan/21 14:44;gurwls223;Issue resolved by pull request 31344
[https://github.com/apache/spark/pull/31344];;;",,,,,,,,,,,,,,,
NPE for static partion with null in InsertIntoHadoopFsRelationCommand,SPARK-34223,13354384,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,25/Jan/21 11:04,26/Jan/21 04:27,13/Jul/23 08:47,26/Jan/21 04:27,3.0.1,3.1.0,,,,,,,,,,,,,,3.0.2,3.1.1,,,SQL,,,,0,,,"a test case
{code:java}
 test(""NPE"") {
    withTable(""t"") {
      sql(s""CREATE TABLE t(i STRING, c string) USING $format PARTITIONED BY (c)"")
      sql(""INSERT OVERWRITE t PARTITION (c=null) VALUES ('1')"")
      checkAnswer(spark.table(""t""), Row(""1"", null))
    }
  }
{code}
",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 26 04:27:47 UTC 2021,,,,,,,,,,"0|z0mya0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/21 11:19;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31320;;;","26/Jan/21 04:27;cloud_fan;Issue resolved by pull request 31320
[https://github.com/apache/spark/pull/31320];;;",,,,,,,,,,,,,,,,,,
"In a special scenario, the error message of the stage in the UI page is blank",SPARK-34221,13354256,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,echohlne,echohlne,echohlne,25/Jan/21 04:34,27/Jan/21 18:03,13/Jul/23 08:47,27/Jan/21 18:03,3.0.1,,,,,,,,,,,,,,,3.0.2,3.1.1,,,Web UI,,,,0,,,"version: spark 3.0.1
problem: as shown in the attchement,  When we visited Spark's historical task information UI page, we found that many stages of the current task failed, but some stages displayed correct error messages, while others were left blank
",,apachespark,dongjoon,echohlne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/21 04:35;echohlne;problem.png;https://issues.apache.org/jira/secure/attachment/13019269/problem.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 27 18:03:04 UTC 2021,,,,,,,,,,"0|z0mxhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/21 04:51;apachespark;User 'akiyamaneko' has created a pull request for this issue:
https://github.com/apache/spark/pull/31314;;;","25/Jan/21 04:51;apachespark;User 'akiyamaneko' has created a pull request for this issue:
https://github.com/apache/spark/pull/31314;;;","27/Jan/21 18:03;dongjoon;Issue resolved by pull request 31314
[https://github.com/apache/spark/pull/31314];;;",,,,,,,,,,,,,,,,,
Fix Scala 2.12 release profile,SPARK-34217,13354211,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,24/Jan/21 22:20,24/Jan/21 23:01,13/Jul/23 08:47,24/Jan/21 23:01,3.0.0,3.0.1,3.1.0,3.2.0,,,,,,,,,,,,3.0.2,3.1.1,3.2.0,,Project Infra,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-26132,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 24 23:01:44 UTC 2021,,,,,,,,,,"0|z0mx7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/21 22:39;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31310;;;","24/Jan/21 23:01;dongjoon;This is resolved via https://github.com/apache/spark/pull/31310;;;",,,,,,,,,,,,,,,,,,
"For parquet table, after changing the precision and scale of decimal type in hive, spark reads incorrect value",SPARK-34212,13354105,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,jack86596,jack86596,23/Jan/21 13:09,08/Apr/21 08:26,13/Jul/23 08:47,26/Jan/21 23:33,2.4.5,3.0.1,3.1.1,,,,,,,,,,,,,2.4.8,3.0.2,3.1.1,,SQL,,,,0,correctness,,"In Hive, 

{code}
create table test_decimal(amt decimal(18,2)) stored as parquet; 
insert into test_decimal select 100;
alter table test_decimal change amt amt decimal(19,3);
{code}


In Spark,

{code}
select * from test_decimal;
{code}

{code}
+--------+
|    amt |
+--------+
| 10.000 |
+--------+
{code}
",,apachespark,csun,dongjoon,jack86596,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 08 08:26:03 UTC 2021,,,,,,,,,,"0|z0mwk0:",9223372036854775807,,,,,,,,,,,,,3.0.2,3.1.1,,,,,,,,,"24/Jan/21 21:42;dongjoon;Thank you for reporting, [~jack86596].;;;","25/Jan/21 05:18;dongjoon;I confirmed that this exists in Apache Spark 3.1.1 RC1, too.
{code:java}
spark-sql> select * from test_decimal;
10.000
Time taken: 3.436 seconds, Fetched 1 row(s)

spark-sql> select version();
3.1.1 53fe365edb948d0e05a5ccb62f349cd9fcb4bb5d
Time taken: 0.158 seconds, Fetched 1 row(s)
{code};;;","25/Jan/21 05:21;dongjoon;As a workaround, you may want to turn off `spark.sql.hive.convertMetastoreParquet`.
{code:java}
spark-sql> set spark.sql.hive.convertMetastoreParquet=false;
spark.sql.hive.convertMetastoreParquet	false
Time taken: 0.04 seconds, Fetched 1 row(s)

spark-sql> select * from test_decimal;
100.000
Time taken: 1.695 seconds, Fetched 1 row(s)
{code};;;","25/Jan/21 05:41;dongjoon;Also, this happens only in Parquet. For ORC, it works correctly.
{code:java}
spark-sql> select * from orc_decimal;
100.000
Time taken: 1.631 seconds, Fetched 1 row(s) {code};;;","25/Jan/21 07:16;dongjoon;It seems that I found the root cause. I'll make a PR soon.;;;","25/Jan/21 10:42;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31319;;;","25/Jan/21 10:43;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31319;;;","26/Jan/21 14:38;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31347;;;","26/Jan/21 23:33;dongjoon;Issue resolved by pull request 31319
[https://github.com/apache/spark/pull/31319];;;","27/Jan/21 08:37;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31357;;;","27/Jan/21 08:38;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31357;;;","30/Jan/21 06:12;jack86596;Thanks for resolving the issue.;;;","02/Feb/21 17:51;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31443;;;","08/Apr/21 08:25;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/32090;;;","08/Apr/21 08:26;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/32090;;;",,,,,
FileSource table null partition can not be dropped,SPARK-34203,13353949,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,Qin Yao,Qin Yao,22/Jan/21 11:19,08/Feb/21 13:54,13/Jul/23 08:47,25/Jan/21 22:45,3.0.1,3.1.0,3.2.0,,,,,,,,,,,,,3.0.2,3.1.1,3.2.0,,SQL,,,,0,,,"*case to reproduce*

create table ppp(i string, j string) using parquet partitioned by (j);

INSERT INTO ppp PARTITION (j=null) VALUES ('1');
 alter table ppp drop partition(j=null); --- success

INSERT OVERWRITE ppp VALUES ('1', null);  --- error
Error in query: No partition is dropped. One partition spec 'Map(j -> null)' does not exist in table 'ppp' database 'default';

alter table ppp drop partition(j='__HIVE_DEFAULT_PARTITION__'); --- then, this is need
",,apachespark,cloud_fan,petertoth,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 21:37:12 UTC 2021,,,,,,,,,,"0|z0mvlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/21 06:45;cloud_fan;[~maxgekk] can you take a look?;;;","25/Jan/21 13:13;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31322;;;","25/Jan/21 18:29;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31326;;;","25/Jan/21 21:37;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31331;;;",,,,,,,,,,,,,,,,
Fix the test code build error for docker-integration-tests in branch-3.0,SPARK-34201,13353932,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,22/Jan/21 08:58,15/Feb/21 21:09,13/Jul/23 08:47,15/Feb/21 21:09,3.0.0,3.0.1,3.0.2,,,,,,,,,,,,,3.0.2,,,,SQL,Tests,,,0,,,"Currently, building test code for docker-integration-tests in branch-3.0 seems to fail due to ojdbc6:11.2.0.1.0 and jdb2jcc4:10.5.0.5 are not available",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 15 21:09:38 UTC 2021,,,,,,,,,,"0|z0mvhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/21 09:09;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31291;;;","15/Feb/21 21:09;dongjoon;This is resolved via https://github.com/apache/spark/pull/31291;;;",,,,,,,,,,,,,,,,,,
ambiguous column reference should consider attribute availability,SPARK-34200,13353902,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,22/Jan/21 05:16,12/Dec/22 18:10,13/Jul/23 08:47,22/Jan/21 11:24,3.0.0,3.1.0,,,,,,,,,,,,,,3.0.2,3.1.1,,,SQL,,,,0,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 22 11:24:17 UTC 2021,,,,,,,,,,"0|z0mvaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/21 05:30;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31287;;;","22/Jan/21 05:31;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/31287;;;","22/Jan/21 11:24;gurwls223;Fixed in https://github.com/apache/spark/pull/31287;;;",,,,,,,,,,,,,,,,,
Block `count(table.*)` to follow ANSI standard and other SQL engines,SPARK-34199,13353893,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,linhongliu-db,linhongliu-db,linhongliu-db,22/Jan/21 03:51,02/Feb/21 07:50,13/Jul/23 08:47,02/Feb/21 07:50,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"In spark, the count(table.*) may cause very weird result, for example:

select count(*) from (select 1 as a, null as b) t;

output: 1

select count(t.*) from (select 1 as a, null as b) t;

output: 0

 

After checking the ANSI standard, count(*) is always treated as count(1) while count(t.*) is not allowed. What's more, this is also not allowed by common databases, e.g. MySQL, oracle.",,apachespark,cloud_fan,linhongliu-db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 02 07:50:06 UTC 2021,,,,,,,,,,"0|z0mv8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/21 04:58;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/31286;;;","02/Feb/21 07:50;cloud_fan;Issue resolved by pull request 31286
[https://github.com/apache/spark/pull/31286];;;",,,,,,,,,,,,,,,,,,
refreshTable() should not invalidate the relation cache for temporary views,SPARK-34197,13353838,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,21/Jan/21 20:18,25/Jan/21 07:37,13/Jul/23 08:47,25/Jan/21 07:37,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,The SessionCatalog.refreshTable() should not invalidate the entry in the relation cache for a table when refreshTable() refreshes a temp view.,,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 07:37:42 UTC 2021,,,,,,,,,,"0|z0muwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/21 20:31;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31265;;;","21/Jan/21 20:32;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31265;;;","25/Jan/21 07:37;cloud_fan;Issue resolved by pull request 31265
[https://github.com/apache/spark/pull/31265];;;",,,,,,,,,,,,,,,,,
Potential race condition during decommissioning with TorrentBroadcast,SPARK-34193,13353828,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,holden,holden,holden,21/Jan/21 19:28,12/Dec/22 18:10,13/Jul/23 08:47,27/Jan/21 21:17,3.1.0,3.1.1,3.1.2,3.2.0,,,,,,,,,,,,3.1.1,,,,Spark Core,,,,0,,,"I found this while back porting so the line numbers should be ignored, but the core of the issue is that we shouldn't be failing the job on this (I don't think). We could fix this by allowing broadcast blocks to be put or having the torrent broadcast ignore this exception.

[info]   org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3.0 (TID 8, 192.168.1.57, executor 1): java.io.IOException: org.apache.spark.storage.BlockSavedOnDecommissionedBlockManagerException: Block broadcast_2_piece0 cannot be saved on decommissioned executor[info]   org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3.0 (TID 8, 192.168.1.57, executor 1): java.io.IOException: org.apache.spark.storage.BlockSavedOnDecommissionedBlockManagerException: Block broadcast_2_piece0 cannot be saved on decommissioned executor[info] at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1333)[info] at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:215)[info] at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)[info] at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)[info] at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)[info] at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)[info] at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:84)[info] at org.apache.spark.scheduler.Task.run(Task.scala:123)[info] at org.apache.spark.executor.Executor$TaskRunner$$anonfun$12.apply(Executor.scala:448)[info] at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[info] at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:454)[info] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[info] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[info] at java.lang.Thread.run(Thread.java:748)[info] Caused by: org.apache.spark.storage.BlockSavedOnDecommissionedBlockManagerException: Block broadcast_2_piece0 cannot be saved on decommissioned executor[info] at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1105)[info] at org.apache.spark.storage.BlockManager.doPutBytes(BlockManager.scala:1010)[info] at org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:986)[info] at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:181)[info] at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:159)[info] at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:159)[info] at scala.collection.immutable.List.foreach(List.scala:392)[info] at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:159)[info] at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2.apply(TorrentBroadcast.scala:239)[info] at scala.Option.getOrElse(Option.scala:121)[info] at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:219)[info] at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)[info] ... 13 more[info][info] Driver stacktrace:[info]   at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1928)[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1916)[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1915)[info]   at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)[info]   at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)[info]   at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1915)[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:951)[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:951)[info]   at scala.Option.foreach(Option.scala:257)[info]   at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:951)[info]   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2149)[info]   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2098)[info]   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2087)[info]   at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)[info]   at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:762)[info]   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2089)[info]   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2110)[info]   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2129)[info]   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)[info]   at org.apache.spark.rdd.RDD.count(RDD.scala:1213)[info]   at org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite.org$apache$spark$storage$BlockManagerDecommissionIntegrationSuite$$runDecomTest(BlockManagerDecommissionIntegrationSuite.scala:276)[info]   at org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite$$anonfun$1.apply$mcV$sp(BlockManagerDecommissionIntegrationSuite.scala:61)[info]   at org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite$$anonfun$1.apply(BlockManagerDecommissionIntegrationSuite.scala:61)[info]   at org.apache.spark.storage.BlockManagerDecommissionIntegrationSuite$$anonfun$1.apply(BlockManagerDecommissionIntegrationSuite.scala:61)[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186)[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:147)[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:183)[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:196)[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289)[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:196)[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:54)[info]   at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:221)[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:54)[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:229)[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:396)[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:384)[info]   at scala.collection.immutable.List.foreach(List.scala:392)[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384)[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:379)[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461)[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:229)[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1560)[info]   at org.scalatest.Suite$class.run(Suite.scala:1147)[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560)[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:233)[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:521)[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:233)[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:54)[info]   at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:213)[info]   at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:210)[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:54)[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314)[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:480)[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:296)[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:286)[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[info]   at java.lang.Thread.run(Thread.java:748)[info]   Cause: java.io.IOException: org.apache.spark.storage.BlockSavedOnDecommissionedBlockManagerException: Block broadcast_2_piece0 cannot be saved on decommissioned executor[info]   at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1333)[info]   at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:215)[info]   at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)[info]   at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)[info]   at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)[info]   at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)[info]   at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:84)[info]   at org.apache.spark.scheduler.Task.run(Task.scala:123)[info]   at org.apache.spark.executor.Executor$TaskRunner$$anonfun$12.apply(Executor.scala:448)[info]   at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[info]   at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:454)[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[info]   at java.lang.Thread.run(Thread.java:748)[info]   Cause: org.apache.spark.storage.BlockSavedOnDecommissionedBlockManagerException: Block broadcast_2_piece0 cannot be saved on decommissioned executor[info]   at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1105)[info]   at org.apache.spark.storage.BlockManager.doPutBytes(BlockManager.scala:1010)[info]   at org.apache.spark.storage.BlockManager.putBytes(BlockManager.scala:986)[info]   at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:181)[info]   at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:159)[info]   at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:159)[info]   at scala.collection.immutable.List.foreach(List.scala:392)[info]   at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:159)[info]   at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2.apply(TorrentBroadcast.scala:239)[info]   at scala.Option.getOrElse(Option.scala:121)[info]   at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:219)[info]   at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)[info]   at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:215)[info]   at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)[info]   at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)[info]   at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)[info]   at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)[info]   at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:84)[info]   at org.apache.spark.scheduler.Task.run(Task.scala:123)[info]   at org.apache.spark.executor.Executor$TaskRunner$$anonfun$12.apply(Executor.scala:448)[info]   at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)[info]   at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:454)[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[info]   at java.util.concurrent.ThreadPoolExecutor$W",,apachespark,holden,xkrogen,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 27 21:17:32 UTC 2021,,,,,,,,,,"0|z0muug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/21 19:29;holden;Note, so far I've only triggered this once and in the back porting, the ""Affects Version"" is currently a guess.;;;","22/Jan/21 20:34;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31298;;;","27/Jan/21 21:17;gurwls223;Issue resolved by pull request 31298
[https://github.com/apache/spark/pull/31298];;;",,,,,,,,,,,,,,,,,
Use available offset range obtained during polling when checking offset validation,SPARK-34187,13353664,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,21/Jan/21 07:24,08/Feb/21 13:54,13/Jul/23 08:47,24/Jan/21 19:52,2.4.7,3.0.1,3.1.0,,,,,,,,,,,,,2.4.8,3.0.2,3.1.1,,Structured Streaming,,,,0,correctness,,"We support non-consecutive offsets for Kafka since 2.4.0. In `fetchRecord`, we do offset validation by checking if the offset is in available offset range. But currently we obtain latest available offset range to do the check. It looks not correct as the available offset range could be changed during the batch, so the available offset range is different than the one when we polling the records from Kafka.

It is possible that an offset is valid when polling, but at the time we do the above check, it is out of latest available offset range. We will wrongly consider it as data loss case and fail the query or drop the record.",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 21:34:40 UTC 2021,,,,,,,,,,"0|z0mtu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/21 07:41;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31275;;;","25/Jan/21 18:52;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31328;;;","25/Jan/21 21:34;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31330;;;",,,,,,,,,,,,,,,,,
Fix the regression brought by SPARK-33888 for PostgresDialect,SPARK-34180,13353505,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sarutak,sarutak,sarutak,20/Jan/21 10:40,12/Dec/22 18:10,13/Jul/23 08:47,22/Jan/21 04:03,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,, A regression brought by SPARK-33888 affects PostgresDialect causing `PostgreSQLIntegrationSuite` to fail.,,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33888,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 22 04:03:27 UTC 2021,,,,,,,,,,"0|z0msuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/21 10:51;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31262;;;","21/Jan/21 04:15;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31270;;;","21/Jan/21 04:16;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31270;;;","22/Jan/21 04:03;gurwls223;Issue resolved by pull request 31262
[https://github.com/apache/spark/pull/31262];;;",,,,,,,,,,,,,,,,
"Reading parquet with Decimal(8,2) written as a Decimal64 blows up",SPARK-34167,13353446,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,razajafri,razajafri,razajafri,20/Jan/21 06:01,22/Feb/21 04:50,13/Jul/23 08:47,22/Feb/21 04:49,3.0.1,,,,,,,,,,,,,,,3.2.0,,,,Input/Output,,,,0,,,"When reading a parquet file written with Decimals with precision < 10 as a 64-bit representation, Spark tries to read it as an INT and fails. I generated this file using [https://github.com/rapidsai/cudf.] It allowed me to create a Decimal(8,2) backed by a 64-bit representation (LongDecimal). I have attached the files that can be read successfully using a 3rd party parquet reader (I used [nathonhowell/parquet-tools|https://hub.docker.com/r/nathanhowell/parquet-tools])

 

Steps to reproduce:

Read the attached file that has a single Decimal(8,2) column with 10 values
{code:java}
scala> spark.read.parquet(""/tmp/pyspark_tests/936454/PARQUET_DATA"").show

...
Caused by: java.lang.NullPointerException
  at org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putLong(OnHeapColumnVector.java:327)
  at org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readLongs(VectorizedRleValuesReader.java:370)
  at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readLongBatch(VectorizedColumnReader.java:514)
  at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:256)
  at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:273)
  at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:171)
  at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
  at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:497)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:756)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:127)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:480)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1426)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:483)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
...

{code}
 

 

Here are my findings. The *{{VectorizedParquetRecordReader}}* starts to read in the long value from parquet file correctly because its basing the read on the [requestedSchema|https://github.com/apache/spark/blob/e6f019836c099398542b443f7700f79de81da0d5/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java#L150] which is a *MessageType* and has the underlying data stored correctly as {{INT64}} where as the *WritableColumnVector*  is initialized based on the [batchSchema|https://github.com/apache/spark/blob/e6f019836c099398542b443f7700f79de81da0d5/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase.java#L151] which is coming from {{org.apache.spark.sql.parquet.row.requested_schema}} that is set by the reader which is a *{{StructType}}* and only has {{Decimal(__,__)}} in it.

[https://github.com/apache/spark/blob/a44e008de3ae5aecad9e0f1a7af6a1e8b0d97f4e/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java#L224]

 

So we can see the problem above is the *WritableColumnVector* is initialized to store an int array, while the *VectorizedParquetReader* method calls the *readLongBatch* method which in turn calls the *VectorizedRleValuesReader.readLongs* which reads the long values and tries to call *WritableColumnVector.putLong* which will throw a NPE because *WritableColumnVector* wasn't initialized to store a long array.

 In the case where the file has a dictionaryPage a different exception is thrown

 
{code:java}
Caused by: java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainLongDictionary
  at org.apache.parquet.column.Dictionary.decodeToInt(Dictionary.java:45)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToInt(ParquetDictionary.java:31)
  at org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getInt(OnHeapColumnVector.java:298)
  at org.apache.spark.sql.execution.vectorized.WritableColumnVector.getDecimal(WritableColumnVector.java:353)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:127)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)

{code}
In this case we have to make sure the correct dictionary is initialized i.e. *PlainIntDictionary* by setting the correct type in the *ColumnDescriptor*

 

Attached are two files, one with Decimal(8,2) ther other with Decimal(1,1) both written as Decimal backed by INT64. Decimal(1,1) results in a different exception but same for the same reason

 

 ",,apachespark,attilapiros,cloud_fan,razajafri,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/21 06:01;razajafri;part-00000-7fecd321-b247-4f7e-bff5-c2e4d8facaa0-c000.snappy.parquet;https://issues.apache.org/jira/secure/attachment/13019038/part-00000-7fecd321-b247-4f7e-bff5-c2e4d8facaa0-c000.snappy.parquet","20/Jan/21 06:04;razajafri;part-00000-940f44f1-f323-4a5e-b828-1e65d87895aa-c000.snappy.parquet;https://issues.apache.org/jira/secure/attachment/13019039/part-00000-940f44f1-f323-4a5e-b828-1e65d87895aa-c000.snappy.parquet",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 22 04:49:15 UTC 2021,,,,,,,,,,"0|z0mshc:",9223372036854775807,,,,,revans2,,,,,,,,3.1.0,,,,,,,,,,"21/Jan/21 09:49;attilapiros;[~razajafri] could you please share with us how the parquet files are created?

I tried to reproduce this issue in the following way but I had no luck:

{noformat}
Spark context Web UI available at http://192.168.0.17:4045
Spark context available as 'sc' (master = local, app id = local-1611221568779).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.1
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_242)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import java.math.BigDecimal
import java.math.BigDecimal

scala> import org.apache.spark.sql.Row
import org.apache.spark.sql.Row

scala> import org.apache.spark.sql.types.{DecimalType, StructField, StructType}
import org.apache.spark.sql.types.{DecimalType, StructField, StructType}

scala> val schema = StructType(Array(StructField(""num"", DecimalType(8,2),true)))
schema: org.apache.spark.sql.types.StructType = StructType(StructField(num,DecimalType(8,2),true))

scala> val rdd = sc.parallelize((0 to 9).map(v => new BigDecimal(s""123456.7$v"")))
rdd: org.apache.spark.rdd.RDD[java.math.BigDecimal] = ParallelCollectionRDD[0] at parallelize at <c
onsole>:27

scala> val df = spark.createDataFrame(rdd.map(Row(_)), schema)
df: org.apache.spark.sql.DataFrame = [num: decimal(8,2)]

scala> df.show()
+---------+
|      num|
+---------+
|123456.70|
|123456.71|
|123456.72|
|123456.73|
|123456.74|
|123456.75|
|123456.76|
|123456.77|
|123456.78|
|123456.79|
+---------+


scala> df.write.parquet(""num.parquet"")

scala> spark.read.parquet(""num.parquet"").show()
+---------+
|      num|
+---------+
|123456.70|
|123456.71|
|123456.72|
|123456.73|
|123456.74|
|123456.75|
|123456.76|
|123456.77|
|123456.78|
|123456.79|
+---------+

{noformat}

;;;","22/Jan/21 00:11;apachespark;User 'razajafri' has created a pull request for this issue:
https://github.com/apache/spark/pull/31284;;;","22/Jan/21 00:12;apachespark;User 'razajafri' has created a pull request for this issue:
https://github.com/apache/spark/pull/31284;;;","22/Jan/21 15:07;tgraves;[~razajafri] can you finish detailing your finding as to the flow of what breaks this?  You state that spark tries to read it as an INT but the backtrace is in readLong.  You started to detail the schema differences but can you followup with how that leads to reading as an INT even though its calling readLong?

 

For example you state ""The *{{VectorizedParquetRecordReader}}* reads in the parquet file correctly "" but that is where the null pointer exception is thrown. If you could detail it more it would help in understanding it.;;;","22/Jan/21 19:19;razajafri;[~tgraves] I have updated the description to help trace the problem. 

 

[~attilapiros]  I used [rapidsai/cudf|https://github.com/rapidsai/cudf.] to create the file. I have updated the description with more details please let me know if you have any further questions;;;","22/Feb/21 04:49;cloud_fan;Issue resolved by pull request 31284
[https://github.com/apache/spark/pull/31284];;;",,,,,,,,,,,,,,
Unify the output of DDL and pass output attributes properly,SPARK-34156,13353220,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,beliefer,beliefer,19/Jan/21 08:30,13/Sep/21 22:20,13/Jul/23 08:47,13/Sep/21 22:20,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,The current implement of some DDL not unify the output and not pass the output properly to physical command.,,angerszhuuu,beliefer,holden,pingsutw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 13 22:19:37 UTC 2021,,,,,,,,,,"0|z0mr34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/21 22:19;holden;All of the sub issues are resolved so I'm going to go ahead and resolve this.;;;",,,,,,,,,,,,,,,,,,,
Flaky Test: LocalityPlacementStrategySuite.handle large number of containers and tasks (SPARK-18750),SPARK-34154,13353170,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,dongjoon,dongjoon,19/Jan/21 04:35,12/Dec/22 18:10,13/Jul/23 08:47,27/Jan/21 23:06,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.0.2,3.1.1,3.2.0,,YARN,,,,0,,,"`LocalityPlacementStrategySuite` hangs sometimes like the following. We can retriever, but it takes our resource significantly because it hangs until the timeout (6 hours) occurs.

[https://github.com/apache/spark/runs/1719480243]

[https://github.com/apache/spark/runs/1724459002]

[https://github.com/apache/spark/runs/1717958874]

[https://github.com/apache/spark/runs/1731673955] (branch-3.0)
{code:java}
[info] LocalityPlacementStrategySuite:
17299[info] *** Test still running after 3 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17300[info] *** Test still running after 8 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17301[info] *** Test still running after 13 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17302[info] *** Test still running after 18 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17303[info] *** Test still running after 23 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17304[info] *** Test still running after 28 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17305[info] *** Test still running after 33 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17306[info] *** Test still running after 38 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17307[info] *** Test still running after 43 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17308[info] *** Test still running after 48 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17309[info] *** Test still running after 53 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17310[info] *** Test still running after 58 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17311[info] *** Test still running after 1 hour, 3 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17312[info] *** Test still running after 1 hour, 8 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17313[info] *** Test still running after 1 hour, 13 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17314[info] *** Test still running after 1 hour, 18 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17315[info] *** Test still running after 1 hour, 23 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17316[info] *** Test still running after 1 hour, 28 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17317[info] *** Test still running after 1 hour, 33 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17318[info] *** Test still running after 1 hour, 38 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17319[info] *** Test still running after 1 hour, 43 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17320[info] *** Test still running after 1 hour, 48 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17321[info] *** Test still running after 1 hour, 53 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17322[info] *** Test still running after 1 hour, 58 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17323[info] *** Test still running after 2 hours, 3 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17324[info] *** Test still running after 2 hours, 8 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17325[info] *** Test still running after 2 hours, 13 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17326[info] *** Test still running after 2 hours, 18 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17327[info] *** Test still running after 2 hours, 23 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17328[info] *** Test still running after 2 hours, 28 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17329[info] *** Test still running after 2 hours, 33 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17330[info] *** Test still running after 2 hours, 38 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17331[info] *** Test still running after 2 hours, 43 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17332[info] *** Test still running after 2 hours, 48 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17333[info] *** Test still running after 2 hours, 53 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17334[info] *** Test still running after 2 hours, 58 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17335[info] *** Test still running after 3 hours, 3 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17336[info] *** Test still running after 3 hours, 8 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17337[info] *** Test still running after 3 hours, 13 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17338[info] *** Test still running after 3 hours, 18 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17339[info] *** Test still running after 3 hours, 23 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17340[info] *** Test still running after 3 hours, 28 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17341[info] *** Test still running after 3 hours, 33 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17342[info] *** Test still running after 3 hours, 38 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17343[info] *** Test still running after 3 hours, 43 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17344[info] *** Test still running after 3 hours, 48 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17345[info] *** Test still running after 3 hours, 53 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17346[info] *** Test still running after 3 hours, 58 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17347[info] *** Test still running after 4 hours, 3 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17348[info] *** Test still running after 4 hours, 8 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17349[info] *** Test still running after 4 hours, 13 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750). 
17350[info] *** Test still running after 4 hours, 18 minutes, 6 seconds: suite name: LocalityPlacementStrategySuite, test name: handle large number of containers and tasks (SPARK-18750).  {code}",,apachespark,attilapiros,dongjoon,,,,,,,,,,,,,,,,,,,,,,,SPARK-34159,,,,,,,,,,,,,,SPARK-29220,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 29 12:06:02 UTC 2021,,,,,,,,,,"0|z0mqs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/21 01:23;dongjoon;I also observed the hanged instance at branch-3.0 GitHub Action and added the link to the PR description. The affected version is all 3.x now.;;;","27/Jan/21 13:47;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/31363;;;","27/Jan/21 13:48;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/31363;;;","27/Jan/21 13:49;attilapiros;I tried to reproduce this issue by running it locally 100 times but all of them was successful. 
So my next idea is to reproduce it by the PR builder (and getting the stack trace of the thread which got stuck):
https://github.com/apache/spark/pull/31363

;;;","27/Jan/21 16:50;attilapiros;In my PR the bug does not surfaced even after 200 runs.

I think the root cause could be something with the hostname lookup, I have checked and it is called from here:

{code:java}
	at org.apache.hadoop.net.NetUtils.normalizeHostName(NetUtils.java)
	at org.apache.hadoop.net.NetUtils.normalizeHostNames(NetUtils.java:585)
	at org.apache.hadoop.net.CachedDNSToSwitchMapping.resolve(CachedDNSToSwitchMapping.java:109)
	at org.apache.spark.deploy.yarn.SparkRackResolver.coreResolve(SparkRackResolver.scala:75)
	at org.apache.spark.deploy.yarn.SparkRackResolver.resolve(SparkRackResolver.scala:66)
	at org.apache.spark.deploy.yarn.LocalityPreferredContainerPlacementStrategy.$anonfun$localityOfRequestedContainers$3(LocalityPreferredContainerPlacementStrategy.scala:142)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)
	at org.apache.spark.deploy.yarn.LocalityPreferredContainerPlacementStrategy.localityOfRequestedContainers(LocalityPreferredContainerPlacementStrategy.scala:138)
	at org.apache.spark.deploy.yarn.LocalityPlacementStrategySuite.org$apache$spark$deploy$yarn$LocalityPlacementStrategySuite$$runTest(LocalityPlacementStrategySuite.scala:94)
	at org.apache.spark.deploy.yarn.LocalityPlacementStrategySuite$$anon$1.run(LocalityPlacementStrategySuite.scala:40)
	at java.lang.Thread.run(Thread.java:748)
{code}

To get more information I will modify my code a bit and make it mergeable. This way when it fails again in somebody's PR we will have at least the stack trace. In addition it will fail fast. It won't run for hour(s) just for 30 seconds.
;;;","27/Jan/21 23:06;gurwls223;Fixed in https://github.com/apache/spark/pull/31363;;;","29/Jan/21 12:06;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/31397;;;",,,,,,,,,,,,,
java.time.Instant and java.time.LocalDate not handled when writing to tables,SPARK-34144,13352774,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Chircu,Chircu,Chircu,17/Jan/21 11:57,12/Dec/22 18:10,13/Jul/23 08:47,29/Jan/21 08:49,3.0.1,3.1.0,3.1.1,,,,,,,,,,,,,3.0.2,3.1.1,,,SQL,,,,0,,,"When using the new java time API (spark.sql.datetime.java8API.enabled=true) LocalDate and Instant aren't handled in org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils#makeSetter so Instant and LocalDate are cast to Timestamp and Date when attempting to write values to a table.

Driver stacktrace:Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059) at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008) at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007) at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007) at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973) at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973) at scala.Option.foreach(Option.scala:407) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2120) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2139) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2164) at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:994) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:388) at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:992) at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:856) at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68) at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68) at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90) at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175) at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171) at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122) at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121) at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64) at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963) at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415) at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)

Caused by: java.lang.ClassCastException: class java.time.LocalDate cannot be cast to class java.sql.Date (java.time.LocalDate is in module java.base of loader 'bootstrap'; java.sql.Date is in module java.sql of loader 'platform') at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$11(JdbcUtils.scala:573) at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeSetter$11$adapted(JdbcUtils.scala:572) at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:678) at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:858) at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:856) at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:994) at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:994) at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2139) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:127) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449) ... 3 more",,apachespark,Chircu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 29 08:57:56 UTC 2021,,,,,,,,,,"0|z0moc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/21 17:32;apachespark;User 'cristichircu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31264;;;","20/Jan/21 17:33;apachespark;User 'cristichircu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31264;;;","28/Jan/21 09:12;apachespark;User 'cristichircu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31382;;;","28/Jan/21 09:13;apachespark;User 'cristichircu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31382;;;","29/Jan/21 08:49;gurwls223;Issue resolved by pull request 31382
[https://github.com/apache/spark/pull/31382];;;","29/Jan/21 08:57;Chircu;[~hyukjin.kwon] shouldn't the fix version be 3.1.1 instead of 3.1.2?;;;",,,,,,,,,,,,,,
Adding partitions to fully partitioned v2 table,SPARK-34143,13352764,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,17/Jan/21 10:14,08/Feb/21 13:54,13/Jul/23 08:47,19/Jan/21 05:40,3.2.0,,,,,,,,,,,,,,,3.1.1,3.2.0,,,SQL,,,,0,,,"The test below fails:
{code:scala}
    withNamespaceAndTable(""ns"", ""tbl"") { t =>
      sql(s""CREATE TABLE $t (p0 INT, p1 STRING) $defaultUsing PARTITIONED BY (p0, p1)"")
      sql(s""ALTER TABLE $t ADD PARTITION (p0 = 0, p1 = 'abc')"")
      checkPartitions(t, Map(""p0"" -> ""0"", ""p1"" -> ""abc""))
      checkAnswer(sql(s""SELECT * FROM $t""), Row(0, ""abc""))
    }
{code}

 ",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 19 19:08:47 UTC 2021,,,,,,,,,,"0|z0moa0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/21 10:31;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31216;;;","17/Jan/21 10:32;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31216;;;","19/Jan/21 05:40;cloud_fan;Issue resolved by pull request 31216
[https://github.com/apache/spark/pull/31216];;;","19/Jan/21 19:07;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31247;;;","19/Jan/21 19:08;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31247;;;",,,,,,,,,,,,,,,
ExtractGenerator analyzer should handle lazy projectlists,SPARK-34141,13352737,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tanelk,tanelk,tanelk,16/Jan/21 23:56,06/Feb/21 19:27,13/Jul/23 08:47,06/Feb/21 19:27,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"With the dataframe api it is possible to have a lazy sequence as the output field on a LogicalPlan class. When exploding a column on this dataframe using the withColumn method, the ExtractGenerator does not extract the generator.",,apachespark,tanelk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 06 19:27:33 UTC 2021,,,,,,,,,,"0|z0mo40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/21 00:13;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31213;;;","17/Jan/21 00:14;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31213;;;","06/Feb/21 19:27;srowen;Issue resolved by pull request 31213
[https://github.com/apache/spark/pull/31213];;;",,,,,,,,,,,,,,,,,
The tree string does not contain statistics for nested scalar sub queries,SPARK-34137,13352680,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhu,yumwang,yumwang,16/Jan/21 14:56,12/Feb/21 06:03,13/Jul/23 08:47,10/Feb/21 03:22,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,SQL,,,,0,,,"How to reproduce:
{code:scala}
spark.sql(""create table t1 using parquet as select id as a, id as b from range(1000)"")
spark.sql(""create table t2 using parquet as select id as c, id as d from range(2000)"")

spark.sql(""ANALYZE TABLE t1 COMPUTE STATISTICS FOR ALL COLUMNS"")
spark.sql(""ANALYZE TABLE t2 COMPUTE STATISTICS FOR ALL COLUMNS"")
spark.sql(""set spark.sql.cbo.enabled=true"")

spark.sql(
  """"""
    |WITH max_store_sales AS
    |  (SELECT max(csales) tpcds_cmax
    |  FROM (SELECT
    |    sum(b) csales
    |  FROM t1 WHERE a < 100 ) x),
    |best_ss_customer AS
    |  (SELECT
    |    c
    |  FROM t2
    |  WHERE d > (SELECT * FROM max_store_sales))
    |
    |SELECT c FROM best_ss_customer
    |"""""".stripMargin).explain(""cost"")
{code}

Output:
{noformat}
== Optimized Logical Plan ==
Project [c#4263L], Statistics(sizeInBytes=31.3 KiB, rowCount=2.00E+3)
+- Filter (isnotnull(d#4264L) AND (d#4264L > scalar-subquery#4262 [])), Statistics(sizeInBytes=46.9 KiB, rowCount=2.00E+3)
   :  +- Aggregate [max(csales#4260L) AS tpcds_cmax#4261L]
   :     +- Aggregate [sum(b#4266L) AS csales#4260L]
   :        +- Project [b#4266L]
   :           +- Filter ((a#4265L < 100) AND isnotnull(a#4265L))
   :              +- Relation default.t1[a#4265L,b#4266L] parquet, Statistics(sizeInBytes=23.4 KiB, rowCount=1.00E+3)
   +- Relation default.t2[c#4263L,d#4264L] parquet, Statistics(sizeInBytes=46.9 KiB, rowCount=2.00E+3)
{noformat}

Another case is TPC-DS q23a.",,apachespark,cloud_fan,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 10 03:22:04 UTC 2021,,,,,,,,,,"0|z0mnrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/21 14:57;yumwang;cc [~maxgekk];;;","06/Feb/21 15:29;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31485;;;","06/Feb/21 15:30;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31485;;;","10/Feb/21 03:22;cloud_fan;Issue resolved by pull request 31485
[https://github.com/apache/spark/pull/31485];;;",,,,,,,,,,,,,,,,
[AVRO] Respect case sensitivity when performing Catalyst-to-Avro field matching,SPARK-34133,13352573,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xkrogen,xkrogen,xkrogen,15/Jan/21 22:19,25/Jan/21 04:55,13/Jul/23 08:47,25/Jan/21 04:55,2.4.0,3.2.0,,,,,,,,,,,,,,3.1.1,,,,Input/Output,SQL,,,0,,,"Spark SQL is normally case-insensitive (by default), but currently when {{AvroSerializer}} and {{AvroDeserializer}} perform matching between Catalyst schemas and Avro schemas, the matching is done in a case-sensitive manner. So for example the following will fail:
{code}
      val avroSchema =
        """"""
          |{
          |  ""type"" : ""record"",
          |  ""name"" : ""test_schema"",
          |  ""fields"" : [
          |    {""name"": ""foo"", ""type"": ""int""},
          |    {""name"": ""BAR"", ""type"": ""int""}
          |  ]
          |}
      """""".stripMargin
      val df = Seq((1, 3), (2, 4)).toDF(""FOO"", ""bar"")

      df.write.option(""avroSchema"", avroSchema).format(""avro"").save(savePath)
{code}
The same is true on the read path, if we assume {{testAvro}} has been written using the schema above, the below will fail to match the fields:
{code}
df.read.schema(new StructType().add(""FOO"", IntegerType).add(""bar"", IntegerType))
  .format(""avro"").load(testAvro)
{code}
",,apachespark,cloud_fan,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34182,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 25 04:55:13 UTC 2021,,,,,,,,,,"0|z0mn3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/21 22:44;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/31201;;;","15/Jan/21 22:45;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/31201;;;","25/Jan/21 04:55;cloud_fan;Issue resolved by pull request 31201
[https://github.com/apache/spark/pull/31201];;;",,,,,,,,,,,,,,,,,
 Suppress excessive logging of TTransportExceptions in Spark ThriftServer,SPARK-34128,13352477,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,15/Jan/21 12:46,20/Mar/21 04:18,13/Jul/23 08:47,20/Mar/21 04:18,3.0.1,3.1.0,,,,,,,,,,,,,,3.1.2,3.2.0,,,SQL,,,,0,,,"Since Spark 3.0， the `libthrift` has been bumped up from 0.9.3 to 0.12.0.

Due to THRIFT-4805, The SparkThrift Server will print annoying TExceptions. For example, the current thrift server module test in Github action workflow outputs more than 200MB of data for this error only, while the total size of the test log only about 1GB.

 

I checked the latest `hive-service-rpc` module in the maven center,  [https://mvnrepository.com/artifact/org.apache.hive/hive-service-rpc/3.1.2.]  It still uses the 0.9.3 version. 

 

Due to THRIFT-5274 , It looks like we need to wait for thrift 0.14.0 to release or downgrade to 0.9.3 to fix this issue if any of them is appropriate

 ",,apachespark,dongjoon,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,THRIFT-4805,THRIFT-5274,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 20 04:18:03 UTC 2021,,,,,,,,,,"0|z0mmig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/21 13:03;Qin Yao;cc [~dongjoon] [~srowen] [~hyukjin.kwon];;;","13/Mar/21 13:51;Qin Yao;Unfortunately, I tried the newly released `libthrift 0.14.1`, it breaks the metastore client side.



{code:java}
java.lang.NoSuchMethodError: org.apache.thrift.transport.TSocket.<init>(Ljava/lang/String;II)V
{code}
;;;","19/Mar/21 06:54;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31895;;;","19/Mar/21 06:55;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31895;;;","20/Mar/21 04:18;dongjoon;This is resolved via https://github.com/apache/spark/pull/31895;;;",,,,,,,,,,,,,,,
Make EventLoggingListener.codecMap thread-safe,SPARK-34125,13352431,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dzcxzl,dzcxzl,dzcxzl,15/Jan/21 08:14,18/Jan/21 00:05,13/Jul/23 08:47,18/Jan/21 00:05,2.4.7,,,,,,,,,,,,,,,2.4.8,,,,Spark Core,,,,0,,,"2.x version of history server
 EventLoggingListener.codecMap is of type mutable.HashMap, which is not thread safe
 This will cause the history server to suddenly get stuck and not work.

The 3.x version was changed to EventLogFileReader.codecMap to ConcurrentHashMap type, so there is no such problem.(-SPARK-28869-)

PID 117049 0x1c939

!top.png!

 

!jstack.png!

 

 

 ",,apachespark,dzcxzl,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/21 08:14;dzcxzl;jstack.png;https://issues.apache.org/jira/secure/attachment/13018822/jstack.png","15/Jan/21 08:14;dzcxzl;top.png;https://issues.apache.org/jira/secure/attachment/13018823/top.png",,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 18 00:05:31 UTC 2021,,,,,,,,,,"0|z0mm88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/21 08:23;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/31194;;;","18/Jan/21 00:05;kabhwan;Issue resolved by pull request 31194
[https://github.com/apache/spark/pull/31194];;;",,,,,,,,,,,,,,,,,,
should not trim right for read-side char length check and padding,SPARK-34114,13352220,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,Qin Yao,Qin Yao,Qin Yao,14/Jan/21 10:13,15/Jan/21 04:30,13/Jul/23 08:47,15/Jan/21 04:30,3.1.1,,,,,,,,,,,,,,,3.1.1,,,,SQL,,,,0,,,,,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 15 04:30:52 UTC 2021,,,,,,,,,,"0|z0mkxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/21 10:29;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31181;;;","14/Jan/21 10:30;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31181;;;","15/Jan/21 04:30;cloud_fan;Issue resolved by pull request 31181
[https://github.com/apache/spark/pull/31181];;;",,,,,,,,,,,,,,,,,
Deconflict the jars jakarta.servlet-api-4.0.3.jar and javax.servlet-api-3.1.0.jar,SPARK-34111,13352155,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,Qin Yao,gurwls223,,14/Jan/21 06:14,12/Dec/22 18:11,13/Jul/23 08:47,18/Jan/21 12:37,3.1.0,,,,,,,,,,,,,,,3.1.1,,,,Build,,,,0,,,"After SPARK-33705, we now happened to have two jars in the release artifact with Hadoop 3:

{{dev/deps/spark-deps-hadoop-3.2-hive-2.3}}:

{code}
...
jakarta.servlet-api/4.0.3//jakarta.servlet-api-4.0.3.jar
...
javax.servlet-api/3.1.0//javax.servlet-api-3.1.0.jar
...
{code}

It can potentially cause an issue, and we should better remove {{javax.servlet-api-3.1.0.jar}} which is apparently only required for YARN tests.",,apachespark,dongjoon,LuciferYang,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 18 12:37:27 UTC 2021,,,,,,,,,,"0|z0mkiw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/21 06:14;gurwls223;cc [~Qin Yao] FYI;;;","14/Jan/21 06:23;dongjoon;Oh, is this a blocker?;;;","14/Jan/21 06:24;Qin Yao;thanks [~hyukjin.kwon] for pinging me. ;;;","14/Jan/21 06:29;gurwls223;I just marked it as a blocker because the duplicated jars might cause an issue that's hard to debug. However, I am fine with lowering the priority, [~dongjoon]. I will leave it to you.;;;","15/Jan/21 03:54;Qin Yao;I have found the problem, LOL

 

In Hadoop 3.2: （{color:#FF0000}*should be*{color}）

<exclusion>
 <groupId>javax.servlet</groupId>
 <artifactId>javax.servlet-api</artifactId>
</exclusion>

 

In Hadoop 2.7:
<exclusion>
 <groupId>javax.servlet</groupId>
 <artifactId>servlet-api</artifactId>
</exclusion>;;;","15/Jan/21 04:02;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31185;;;","17/Jan/21 04:57;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31214;;;","17/Jan/21 04:58;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31214;;;","18/Jan/21 12:37;gurwls223;Issue resolved by pull request 31214
[https://github.com/apache/spark/pull/31214];;;",,,,,,,,,,,
Fix MiMaExcludes by moving SPARK-23429 from 2.4.x to 3.0.x,SPARK-34103,13352113,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,13/Jan/21 22:31,12/Dec/22 18:11,13/Jul/23 08:47,14/Jan/21 00:30,3.0.0,3.0.1,3.1.0,3.1.1,3.2.0,,,,,,,,,,,3.0.2,3.1.1,,,Project Infra,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-23429,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 14 00:30:41 UTC 2021,,,,,,,,,,"0|z0mk9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/21 22:34;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31174;;;","14/Jan/21 00:30;gurwls223;Issue resolved by pull request 31174
[https://github.com/apache/spark/pull/31174];;;",,,,,,,,,,,,,,,,,,
Shuffle batch fetch can't be disabled once it's enabled previously,SPARK-34091,13351783,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,12/Jan/21 13:31,12/Jan/21 15:46,13/Jul/23 08:47,12/Jan/21 15:46,3.0.0,3.1.0,,,,,,,,,,,,,,3.1.1,,,,SQL,,,,0,,,"{code:java}
  if (SQLConf.get.fetchShuffleBlocksInBatch) {
    dependency.rdd.context.setLocalProperty(
      SortShuffleManager.FETCH_SHUFFLE_BLOCKS_IN_BATCH_ENABLED_KEY, ""true"")
  }
{code}
The current code has a problem that once we set `fetchShuffleBlocksInBatch` to true first, we can never disable the batch fetch even if set `fetchShuffleBlocksInBatch` to false later.

 ",,apachespark,cloud_fan,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 12 15:46:02 UTC 2021,,,,,,,,,,"0|z0mi88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/21 13:47;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/31155;;;","12/Jan/21 15:46;cloud_fan;Issue resolved by pull request 31155
[https://github.com/apache/spark/pull/31155];;;",,,,,,,,,,,,,,,,,,
HadoopDelegationTokenManager.isServiceEnabled used in KafkaTokenUtil.needTokenUpdate needs to be cached because it slows down Kafka stream processing in case of delegation token,SPARK-34090,13351781,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,gsomogyi,gsomogyi,12/Jan/21 13:19,12/Dec/22 18:10,13/Jul/23 08:47,13/Jan/21 02:05,3.1.1,,,,,,,,,,,,,,,3.1.1,,,,Structured Streaming,,,,0,,,,,apachespark,gsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 13 02:05:47 UTC 2021,,,,,,,,,,"0|z0mi7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/21 13:38;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/31154;;;","12/Jan/21 13:39;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/31154;;;","13/Jan/21 02:05;gurwls223;Issue resolved by pull request 31154
[https://github.com/apache/spark/pull/31154];;;",,,,,,,,,,,,,,,,,
MemoryConsumer's memory mode should respect MemoryManager's memory mode,SPARK-34089,13351773,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,12/Jan/21 12:48,22/Mar/21 08:17,13/Jul/23 08:47,22/Mar/21 08:17,2.4.7,3.0.0,3.1.0,,,,,,,,,,,,,3.2.0,,,,Spark Core,,,,0,,,"Currently, the memory mode always set to ON_HEAP for memory consumer when it's not explicitly set.

However, we actually can know the specific memory mode by taskMemoryManager.getTungstenMemoryMode().

 

[https://github.com/apache/spark/blob/3a299aa6480ac22501512cd0310d31a441d7dfdc/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java#L43-L45]

 

 ",,apachespark,cloud_fan,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 22 08:17:37 UTC 2021,,,,,,,,,,"0|z0mi60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/21 13:03;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/31152;;;","12/Jan/21 13:04;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/31152;;;","22/Mar/21 08:17;cloud_fan;Issue resolved by pull request 31152
[https://github.com/apache/spark/pull/31152];;;",,,,,,,,,,,,,,,,,
a memory leak occurs when we clone the spark session,SPARK-34087,13351742,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,fchen,fchen,12/Jan/21 11:06,12/Apr/21 00:57,13/Jul/23 08:47,23/Mar/21 07:39,3.0.1,,,,,,,,,,,,,,,3.2.0,,,,Spark Core,,,,0,,,"In Spark-3.0.1, the memory leak occurs when we keep cloning the spark session because a new ExecutionListenerBus instance will add to AsyncEventQueue when we clone a new session.",,apachespark,cloud_fan,fchen,yumwang,,,,,,,,,,,,,,,,,,,,,,SPARK-34689,,,,,,,,,,,,,,,"12/Jan/21 11:37;fchen;1610451044690.jpg;https://issues.apache.org/jira/secure/attachment/13018622/1610451044690.jpg",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 23 07:39:08 UTC 2021,,,,,,,,,,"0|z0mhz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/21 11:37;fchen;*bug replay*

here is code for replay this bug:
{code:java}
// run with spark-3.0.1
test(""bug replay"") {
  (1 to 1000).foreach(i => { 
    spark.cloneSession() 
    SparkSession.clearActiveSession()
  })
  val cnt = spark.sparkContext
   .listenerBus
   .listeners
   .asScala
   .collect{ case e: ExecutionListenerBus => e}
   .size
  println(s""total ExecutionListenerBus count ${cnt}."")
  Thread.sleep(Int.MaxValue) 
}
{code}
*output:*

total ExecutionListenerBus count 1001.

*jmap*

  !1610451044690.jpg!

Each ExecutionListenerBus holds one SparkSession instance, so JVM can't collect these SparkSession object;;;","12/Jan/21 17:07;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/31156;;;","12/Jan/21 17:08;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/31156;;;","18/Jan/21 08:56;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/31231;;;","15/Mar/21 14:07;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/31839;;;","18/Mar/21 07:06;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/31881;;;","22/Mar/21 03:17;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/31919;;;","22/Mar/21 03:17;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/31919;;;","23/Mar/21 07:39;cloud_fan;Issue resolved by pull request 31919
[https://github.com/apache/spark/pull/31919];;;",,,,,,,,,,,
RaiseError generates too much code and may fails codegen in length check for char varchar,SPARK-34086,13351738,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,12/Jan/21 10:41,14/Jan/21 03:52,13/Jul/23 08:47,14/Jan/21 03:52,3.1.0,,,,,,,,,,,,,,,3.1.1,,,,SQL,,,,0,,,"https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/133928/testReport/org.apache.spark.sql.execution/LogicalPlanTagInSparkPlanSuite/q41/

We can reduce more than 8000 bytes by removing the unnecessary CONCAT expression.",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34083,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 14 03:52:12 UTC 2021,,,,,,,,,,"0|z0mhy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/21 11:04;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31150;;;","13/Jan/21 10:41;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31168;;;","14/Jan/21 03:52;cloud_fan;Issue resolved by pull request 31168
[https://github.com/apache/spark/pull/31168];;;",,,,,,,,,,,,,,,,,
ALTER TABLE .. ADD PARTITION does not update table stats,SPARK-34084,13351697,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,12/Jan/21 09:19,13/Jan/21 12:57,13/Jul/23 08:47,12/Jan/21 14:35,3.0.2,3.1.1,3.2.0,,,,,,,,,,,,,3.0.2,3.1.1,3.2.0,,SQL,,,,0,,,"The example below portraits the issue:
{code:sql}
spark-sql> create table tbl (col0 int, part int) partitioned by (part);
spark-sql> insert into tbl partition (part = 0) select 0;
spark-sql> set spark.sql.statistics.size.autoUpdate.enabled=true;
spark-sql> alter table tbl add partition (part = 1);
{code}
There are no stats:
{code:sql}
spark-sql> describe table extended tbl;
col0	int	NULL
part	int	NULL
# Partition Information
# col_name	data_type	comment
part	int	NULL

# Detailed Table Information
Database	default
Table	tbl
Owner	maximgekk
Created Time	Tue Jan 12 12:00:03 MSK 2021
Last Access	UNKNOWN
Created By	Spark 3.2.0-SNAPSHOT
Type	MANAGED
Provider	hive
Table Properties	[transient_lastDdlTime=1610442003]
Location	file:/Users/maximgekk/proj/fix-stats-in-add-partition/spark-warehouse/tbl
Serde Library	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat	org.apache.hadoop.mapred.TextInputFormat
OutputFormat	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Storage Properties	[serialization.format=1]
Partition Provider	Catalog
{code}

*As we can see there is no stats.* For instance, ALTER TABLE .. DROP PARTITION updates stats:
{code:sql}
spark-sql> alter table tbl drop partition (part = 1);
spark-sql> describe table extended tbl;
col0	int	NULL
part	int	NULL
# Partition Information
# col_name	data_type	comment
part	int	NULL

# Detailed Table Information
...
Statistics	2 bytes
{code}",strong text,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,SPARK-34062,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 12 18:29:44 UTC 2021,,,,,,,,,,"0|z0mhp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/21 10:29;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31149;;;","12/Jan/21 14:35;cloud_fan;Issue resolved by pull request 31149
[https://github.com/apache/spark/pull/31149];;;","12/Jan/21 18:07;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31157;;;","12/Jan/21 18:29;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31158;;;","12/Jan/21 18:29;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31158;;;",,,,,,,,,,,,,,,
SQLContext.dropTempTable fails if cache is non-empty,SPARK-34076,13351595,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csun,csun,csun,11/Jan/21 22:05,13/Jan/21 13:22,13/Jul/23 08:47,13/Jan/21 13:22,3.1.0,,,,,,,,,,,,,,,3.1.1,,,,SQL,,,,0,,,"{{SQLContext.dropTempView}} calls {{CatalogImpl.dropTempView}} which is implemented as following:
{code}
  override def dropTempView(viewName: String): Boolean = {
    sparkSession.sessionState.catalog.getTempView(viewName).exists { viewDef =>
      sparkSession.sharedState.cacheManager.uncacheQuery(
        sparkSession, viewDef, cascade = false)
      sessionCatalog.dropTempView(viewName)
    }
  }
{code}

Here, the logical plan {{viewDef}} is not resolved, and when passing to {{uncacheQuery}}, it could fail at {{sameResult}} call, when canonicalized plan is compared. The error message looks like:
{code}
Invalid call to qualifier on unresolved object, tree: 'key
{code}

This can be reproduced via:
{code}
  test(""XXX"") {
    val t = ""t""
    val v = ""v""
    withTable(t) {
      withTempView(v) {
        sql(s""CREATE TABLE $t AS SELECT * FROM src"")
        sql(s""CACHE TABLE $t"")
        sql(s""CREATE TEMPORARY VIEW $v AS SELECT key FROM src LIMIT 10"")
      }
    }
  }
{code}

through test. ",,apachespark,cloud_fan,csun,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33138,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 13 13:22:56 UTC 2021,,,,,,,,,,"0|z0mh2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/21 23:30;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/31136;;;","11/Jan/21 23:30;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/31136;;;","13/Jan/21 13:22;cloud_fan;Issue resolved by pull request 31136
[https://github.com/apache/spark/pull/31136];;;",,,,,,,,,,,,,,,,,
Hidden directories are being listed for partition inference,SPARK-34075,13351575,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,Gengliang.Wang,brkyvz,brkyvz,11/Jan/21 19:52,12/Dec/22 18:11,13/Jul/23 08:47,14/Jan/21 00:42,3.1.0,,,,,,,,,,,,,,,3.1.1,,,,SQL,,,,0,,,"Marking this as a blocker since it seems to be a regression. We are running Delta's tests against Spark 3.1 as part of QA here: [https://github.com/delta-io/delta/pull/579]

 

We have noticed that one of our tests regressed with:
{code:java}
java.lang.AssertionError: assertion failed: Conflicting directory structures detected. Suspicious paths:
[info] 	file:/private/var/folders/_2/xn1c9yr11_93wjdk2vkvmwm00000gp/t/spark-18706bcc-23ea-4853-b8bc-c4cc2a5ed551
[info] 	file:/private/var/folders/_2/xn1c9yr11_93wjdk2vkvmwm00000gp/t/spark-18706bcc-23ea-4853-b8bc-c4cc2a5ed551/_delta_log
[info] 
[info] If provided paths are partition directories, please set ""basePath"" in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.
[info]   at scala.Predef$.assert(Predef.scala:223)
[info]   at org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:172)
[info]   at org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:104)
[info]   at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:158)
[info]   at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:73)
[info]   at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:50)
[info]   at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)
[info]   at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:418)
[info]   at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:62)
[info]   at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile$$anonfun$apply$1.applyOrElse(rules.scala:45)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)
[info]   at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)
[info]   at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:45)
[info]   at org.apache.spark.sql.execution.datasources.ResolveSQLOnFile.apply(rules.scala:40)
[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
[info]   at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
[info]   at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
[info]   at scala.collection.immutable.List.foldLeft(List.scala:89)
[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
[info]   at scala.collection.immutable.List.foreach(List.scala:392)
[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:195)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:189) {code}
It seems like a hidden directory is not being filtered out, when it actually should.",,apachespark,brkyvz,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 14 00:42:12 UTC 2021,,,,,,,,,,"0|z0mgy8:",9223372036854775807,,,,,,,,,,,,,3.1.1,,,,,,,,,,"13/Jan/21 10:44;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31169;;;","13/Jan/21 10:44;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31169;;;","14/Jan/21 00:42;gurwls223;Issue resolved by pull request 31169
[https://github.com/apache/spark/pull/31169];;;",,,,,,,,,,,,,,,,,
Kill barrier tasks should respect SPARK_JOB_INTERRUPT_ON_CANCEL,SPARK-34069,13351416,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,11/Jan/21 09:59,18/Jan/21 07:38,13/Jul/23 08:47,12/Jan/21 19:21,3.2.0,,,,,,,,,,,,,,,3.1.1,,,,Spark Core,,,,0,,,We should interrupt task thread if user set local property `SPARK_JOB_INTERRUPT_ON_CANCEL` to true.,,apachespark,mridulm80,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 12 19:21:36 UTC 2021,,,,,,,,,,"0|z0mfyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/21 10:07;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/31127;;;","12/Jan/21 19:21;mridulm80;Issue resolved by pull request 31127
[https://github.com/apache/spark/pull/31127];;;",,,,,,,,,,,,,,,,,,
Please reduce GitHub Actions matrix or improve the build time,SPARK-34053,13351157,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,kamil.bregula,vladimirsitnikov,vladimirsitnikov,08/Jan/21 19:42,12/Dec/22 18:10,13/Jul/23 08:47,11/Jan/21 00:20,3.0.1,3.1.0,3.2.0,,,,,,,,,,,,,3.2.0,,,,Project Infra,,,,0,,,"GitHub Actions queue is very high for Apache projects, and it looks like a significant number of executors are occupied by Spark jobs :-(

Note: all Apache projects share the same limit of shared GitHub Actions runners, and based on the chart below Spark is consuming 20+ runners while the total limit (for all ASF projects) is 180.


See https://lists.apache.org/thread.html/r5303eec41cc1dfc51c15dbe44770e37369330f9644ef09813f649120%40%3Cbuilds.apache.org%3E

>number of GA workflows in/progress/queued per project and they clearly show the situation is getting worse by day: https://pasteboard.co/JIJa5Xg.png",,apachespark,dongjoon,kamil.bregula,vladimirsitnikov,xkrogen,,,,,,,,,,,,,,,,INFRA-21287,,,,,,,,,,,,,,,,,,,,"08/Jan/21 22:57;dongjoon;Screen Shot 2021-01-08 at 2.57.07 PM.png;https://issues.apache.org/jira/secure/attachment/13018402/Screen+Shot+2021-01-08+at+2.57.07+PM.png",,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 12 13:49:47 UTC 2021,,,,,,,,,,"0|z0medk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/21 21:59;dongjoon;Thank you for reporting, [~vladimirsitnikov].;;;","08/Jan/21 22:13;dongjoon;BTW, according to the graph, the majority looks like `pulsa` project, doesn't it?
> it looks like a significant number of executors are occupied by Spark jobs;;;","08/Jan/21 22:56;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31098;;;","08/Jan/21 22:57;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/31098;;;","08/Jan/21 22:58;dongjoon;Unfortunately, it turns out that the suggestion is not applicable in Apache project for the security reason, [~vladimirsitnikov].

 !Screen Shot 2021-01-08 at 2.57.07 PM.png! ;;;","08/Jan/21 23:10;dongjoon;I removed the suggested plugin from the JIRA description because it's considered unsafe according to the Apache policy as of now.;;;","08/Jan/21 23:18;dongjoon;In addition, [~vladimirsitnikov]. The following is also wrong, isn't it?

> For instance, as of now, there are 23  queued jobs for master branch build.
> https://github.com/apache/spark/actions?query=is%3Aqueued

22 jobs are 16+ months ago. And 1 job is 3 months ago.;;;","08/Jan/21 23:19;dongjoon;I'll remove the above wrong claim to avoid any confusion in the community.;;;","08/Jan/21 23:27;gurwls223;BTW, i have been contacting GitHub to address this resource issue separately few days ago.

There are already too many repos in ASF organisation, and we Spark have tried to reduce the matrix. Actually there are more combinations to test but we're currently not adding them due the the resource issue.;;;","08/Jan/21 23:41;dongjoon;I filed INFRA-21287 .;;;","09/Jan/21 00:59;gurwls223;+1 thanks [~dongjoon];;;","09/Jan/21 10:45;vladimirsitnikov;{quote} 22 jobs are 16+ months ago. And 1 job is 3 months ago.{quote}
Oh, I missed that. Sorry.

By the way, do you have ""cancel job"" in GitHub UI to cancel those workflows?;;;","09/Jan/21 16:01;apachespark;User 'mik-laj' has created a pull request for this issue:
https://github.com/apache/spark/pull/31104;;;","09/Jan/21 16:02;kamil.bregula;I submitted PR: https://github.com/apache/spark/pull/31104
 Can I ask for the fast review to unblock CI for other Apache projects?;;;","11/Jan/21 00:20;dongjoon;Issue resolved by pull request 31104
[https://github.com/apache/spark/pull/31104];;;","11/Jan/21 00:25;dongjoon;For the record, this will kill the previous commit's run.
Although we know the behavior change, Apache Spark community merge this approach because it's inevitable until Apache project has enough resource pool or better management in GitHub Action runners.;;;","12/Jan/21 13:49;apachespark;User 'potiuk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31153;;;","12/Jan/21 13:49;apachespark;User 'potiuk' has created a pull request for this issue:
https://github.com/apache/spark/pull/31153;;;",,
Add Kafka delegation token truststore and keystore type confiuration,SPARK-34032,13350715,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gsomogyi,gsomogyi,gsomogyi,06/Jan/21 17:45,08/Jan/21 11:05,13/Jul/23 08:47,08/Jan/21 11:05,3.0.1,,,,,,,,,,,,,,,3.2.0,,,,Structured Streaming,,,,0,,,,,apachespark,gsomogyi,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 08 11:05:22 UTC 2021,,,,,,,,,,"0|z0mbnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/21 17:53;apachespark;User 'gaborgsomogyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/31070;;;","08/Jan/21 11:05;kabhwan;Issue resolved by pull request 31070
[https://github.com/apache/spark/pull/31070];;;",,,,,,,,,,,,,,,,,,
SparkR not on CRAN (again),SPARK-34016,13349713,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,pauljohn32,pauljohn32,05/Jan/21 18:41,12/Dec/22 18:10,13/Jul/23 08:47,06/Jul/21 02:09,2.4.7,,,,,,,,,,,,,,,,,,,R,,,,0,,,"SparkR is removed from CRAN 

[https://cran.r-project.org/web/packages/SparkR/index.html#:~:text=Package%20'SparkR'%20was%20removed%20from,be%20obtained%20from%20the%20archive.&text=A%20summary%20of%20the%20most,to%20link%20to%20this%20page.]

Is there anything we can do to help?

 ",,pauljohn32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 06 02:09:38 UTC 2021,,,,,,,,,,"0|z0m5a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/21 01:34;gurwls223;cc [~dongjoon], [~felixcheung], [~shivaram] FYI;;;","07/Jan/21 05:04;gurwls223;This will probably be fixed in https://issues.apache.org/jira/browse/SPARK-34021, and SparkR 3.1.1 will likely be uploaded there.;;;","06/Jul/21 02:09;gurwls223;It's on now;;;",,,,,,,,,,,,,,,,,
SparkR partition timing summary reports input time correctly,SPARK-34015,13349250,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WamBamBoozle,WamBamBoozle,WamBamBoozle,05/Jan/21 18:05,12/Dec/22 18:10,13/Jul/23 08:47,06/Jan/21 02:41,2.3.2,3.0.1,,,,,,,,,,,,,,3.1.1,3.2.0,,,SparkR,,,,0,,,"When sparkR is run at log level INFO, a summary of how the worker spent its time processing the partition is printed. There is a logic error where it is over-reporting the time inputting rows.

In detail: the variable inputElap in a wider context is used to mark the beginning of reading rows, but in the part changed here it was used as a local variable for measuring compute time. Thus, the error is not observable if there is only one group per partition, which is what you get in unit tests.

For our application, here's what a log entry looks like before these changes were applied:

{{20/10/09 04:08:58 WARN RRunner: Times: boot = 0.013 s, init = 0.005 s, broadcast = 0.000 s, read-input = 529.471 s, compute = 492.037 s, write-output = 0.020 s, total = 1021.546 s}}

this indicates that we're spending more time reading rows than operating on the rows.

After these changes, it looks like this:

{{20/12/15 06:43:29 WARN RRunner: Times: boot = 0.013 s, init = 0.010 s, broadcast = 0.000 s, read-input = 120.275 s, compute = 1680.161 s, write-output = 0.045 s, total = 1812.553 s}}",Observed on CentOS-7 running spark 2.3.1 and on my mac running master,apachespark,WamBamBoozle,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Wed Jan 06 02:41:53 UTC 2021,,,,,,,,,,"0|z0m2f4:",9223372036854775807,,,,,hyukjin.kwon,,,,,,,,,,,,,,,,,,"05/Jan/21 18:11;apachespark;User 'WamBamBoozle' has created a pull request for this issue:
https://github.com/apache/spark/pull/31021;;;","05/Jan/21 18:12;apachespark;User 'WamBamBoozle' has created a pull request for this issue:
https://github.com/apache/spark/pull/31021;;;","06/Jan/21 02:41;gurwls223;Fixed in https://github.com/apache/spark/pull/31021;;;",,,,,,,,,,,,,,,,,
Keep behavior consistent when conf `spark.sql.legacy.parser.havingWithoutGroupByAsWhere` is true with migration guide,SPARK-34012,13349187,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,05/Jan/21 12:11,12/Dec/22 18:10,13/Jul/23 08:47,05/Jan/21 23:56,2.4.8,3.0.2,3.1.0,3.2.0,,,,,,,,,,,,2.4.8,3.0.2,3.1.1,3.2.0,SQL,,,,0,,,"According to [https://github.com/apache/spark/pull/29087#issuecomment-754389257]

fix bug and add UT",,angerszhuuu,apachespark,dongjoon,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-25708,SPARK-31519,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 07 03:21:34 UTC 2021,,,,,,,,,,"0|z0m214:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/21 12:14;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31039;;;","05/Jan/21 23:56;maropu;Resolved by https://github.com/apache/spark/pull/31039;;;","06/Jan/21 02:15;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31049;;;","06/Jan/21 02:54;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31050;;;","06/Jan/21 02:55;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31050;;;","07/Jan/21 03:12;dongjoon;I updated the version to 3.1.1 , [~hyukjin.kwon].;;;","07/Jan/21 03:21;gurwls223;Thanks!;;;",,,,,,,,,,,,,
Use python3 instead of python in SQL documentation build,SPARK-34010,13349157,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,gurwls223,,05/Jan/21 10:37,12/Dec/22 18:10,13/Jul/23 08:47,05/Jan/21 10:49,3.0.1,3.1.0,,,,,,,,,,,,,,3.0.2,3.1.0,,,Documentation,SQL,,,0,,,"After SPARK-29672, we use {{sql/create-docs.sh}} everywhere in Spark dev. We should fix it in {{sql/create-docs.sh}} too.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 05 10:49:42 UTC 2021,,,,,,,,,,"0|z0m1ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/21 10:37;gurwls223;This blocks release because the release container does not have {{python}} but only {{python3}}. I marked it as a bug because the release script is broken by this.;;;","05/Jan/21 10:43;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/31041;;;","05/Jan/21 10:49;gurwls223;Issue resolved by pull request 31041
[https://github.com/apache/spark/pull/31041];;;",,,,,,,,,,,,,,,,,
Rule conflicts between PaddingAndLengthCheckForCharVarchar and ResolveAggregateFunctions,SPARK-34003,13349064,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,Qin Yao,Qin Yao,Qin Yao,05/Jan/21 06:58,12/Jan/21 09:23,13/Jul/23 08:47,08/Jan/21 09:06,3.1.0,,,,,,,,,,,,,,,3.1.1,,,,SQL,,,,0,,,"ResolveAggregateFunctions is a hacky rule and it calls `executeSameContext` to generate a `resolved agg` to determine which unresolved sort attribute should be pushed into the agg. However, after we add the PaddingAndLengthCheckForCharVarchar rule which will rewrite the query output, thus, the `resolved agg` cannot match original attributes anymore. 

It causes some dissociative sort attribute to be pushed in and fails the query


{code:java}
[info]   Failed to analyze query: org.apache.spark.sql.AnalysisException: expression 'testcat.t1.`v`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;
[info]   Project [v#14, sum(i)#11L]
[info]   +- Sort [aggOrder#12 ASC NULLS FIRST], true
[info]      +- !Aggregate [v#14], [v#14, sum(cast(i#7 as bigint)) AS sum(i)#11L, v#13 AS aggOrder#12]
[info]         +- SubqueryAlias testcat.t1
[info]            +- Project [if ((length(v#6) <= 3)) v#6 else if ((length(rtrim(v#6, None)) > 3)) cast(raise_error(concat(input string of length , cast(length(v#6) as string),  exceeds varchar type length limitation: 3)) as string) else rpad(rtrim(v#6, None), 3,  ) AS v#14, i#7]
[info]               +- RelationV2[v#6, i#7, index#15, _partition#16] testcat.t1
[info]
[info]   Project [v#14, sum(i)#11L]
[info]   +- Sort [aggOrder#12 ASC NULLS FIRST], true
[info]      +- !Aggregate [v#14], [v#14, sum(cast(i#7 as bigint)) AS sum(i)#11L, v#13 AS aggOrder#12]
[info]         +- SubqueryAlias testcat.t1
[info]            +- Project [if ((length(v#6) <= 3)) v#6 else if ((length(rtrim(v#6, None)) > 3)) cast(raise_error(concat(input string of length , cast(length(v#6) as string),  exceeds varchar type length limitation: 3)) as string) else rpad(rtrim(v#6, None), 3,  ) AS v#14, i#7]
[info]               +- RelationV2[v#6, i#7, index#15, _partition#16] testcat.t1
{code}
",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34083,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 11 10:30:00 UTC 2021,,,,,,,,,,"0|z0m1a0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/21 07:34;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31027;;;","08/Jan/21 09:06;cloud_fan;Issue resolved by pull request 31027
[https://github.com/apache/spark/pull/31027];;;","11/Jan/21 10:29;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31129;;;","11/Jan/21 10:30;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31129;;;",,,,,,,,,,,,,,,,
Broken UDF Encoding,SPARK-34002,13349058,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,viirya,mhamilton,mhamilton,05/Jan/21 05:54,11/Jan/21 19:32,13/Jul/23 08:47,11/Jan/21 19:32,3.1.0,3.2.0,,,,,,,,,,,,,,3.1.1,,,,SQL,,,,0,,,"UDFs can behave differently depending on if a dataframe is cached, despite the dataframe being identical

 

Repro:

 
{code:java}
import org.apache.spark.sql.expressions.UserDefinedFunction 
import org.apache.spark.sql.functions.{col, udf}

case class Bar(a: Int)
 
import spark.implicits._

def f1(bar: Bar): Option[Bar] = {
 None
}

def f2(bar: Bar): Option[Bar] = {
 Option(bar)
}

val udf1: UserDefinedFunction = udf(f1 _)
val udf2: UserDefinedFunction = udf(f2 _)

// Commenting in the cache will make this example work
val df = (1 to 10).map(i => Tuple1(Bar(1))).toDF(""c0"")//.cache()
val newDf = df
 .withColumn(""c1"", udf1(col(""c0"")))
 .withColumn(""c2"", udf2(col(""c1"")))
newDf.show()
{code}
 

Error:

Testing started at 12:58 AM ...Testing started at 12:58 AM ...""C:\Program Files\Java\jdk1.8.0_271\bin\java.exe"" ""-javaagent:C:\Program Files\JetBrains\IntelliJ IDEA 2020.2.3\lib\idea_rt.jar=56657:C:\Program Files\JetBrains\IntelliJ IDEA 2020.2.3\bin"" -Dfile.encoding=UTF-8 -classpath ""C:\Users\marhamil\AppData\Roaming\JetBrains\IntelliJIdea2020.2\plugins\Scala\lib\runners.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_271\jre\lib\rt.jar;C:\code\mmlspark\target\scala-2.12\test-classes;C:\code\mmlspark\target\scala-2.12\classes;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\aopalliance\aopalliance\1.0\aopalliance-1.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\beust\jcommander\1.27\jcommander-1.27.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\chuusai\shapeless_2.12\2.3.3\shapeless_2.12-2.3.3.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\clearspring\analytics\stream\2.9.6\stream-2.9.6.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\esotericsoftware\kryo-shaded\4.0.2\kryo-shaded-4.0.2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\esotericsoftware\minlog\1.3.0\minlog-1.3.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\fasterxml\jackson\core\jackson-annotations\2.10.0\jackson-annotations-2.10.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\fasterxml\jackson\core\jackson-core\2.10.0\jackson-core-2.10.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\fasterxml\jackson\core\jackson-databind\2.10.0\jackson-databind-2.10.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\fasterxml\jackson\module\jackson-module-paranamer\2.10.0\jackson-module-paranamer-2.10.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\fasterxml\jackson\module\jackson-module-scala_2.12\2.10.0\jackson-module-scala_2.12-2.10.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\github\fommil\netlib\core\1.1.2\core-1.1.2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\github\luben\zstd-jni\1.4.4-3\zstd-jni-1.4.4-3.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\github\spotbugs\spotbugs-annotations\3.1.9\spotbugs-annotations-3.1.9.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\github\vowpalwabbit\vw-jni\8.8.1\vw-jni-8.8.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\github\wendykierp\JTransforms\3.1\JTransforms-3.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\google\code\findbugs\jsr305\3.0.2\jsr305-3.0.2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\google\code\gson\gson\2.2.4\gson-2.2.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\google\flatbuffers\flatbuffers-java\1.9.0\flatbuffers-java-1.9.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\google\guava\guava\16.0.1\guava-16.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\google\inject\guice\3.0\guice-3.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\jcraft\jsch\0.1.54\jsch-0.1.54.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\linkedin\isolation-forest\isolation-forest_3.0.0_2.12\1.0.1\isolation-forest_3.0.0_2.12-1.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\microsoft\cntk\cntk\2.4\cntk-2.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\mmlspark.blob.core.windows.net\maven\com\microsoft\cognitiveservices\speech\client-sdk\1.11.0\client-sdk-1.11.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\microsoft\ml\lightgbm\lightgbmlib\2.3.180\lightgbmlib-2.3.180.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\sun\istack\istack-commons-runtime\3.0.8\istack-commons-runtime-3.0.8.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\thoughtworks\paranamer\paranamer\2.8\paranamer-2.8.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\twitter\chill-java\0.9.5\chill-java-0.9.5.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\twitter\chill_2.12\0.9.5\chill_2.12-0.9.5.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\com\univocity\univocity-parsers\2.9.0\univocity-parsers-2.9.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\commons-beanutils\commons-beanutils\1.7.0\commons-beanutils-1.7.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\commons-cli\commons-cli\1.2\commons-cli-1.2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\commons-codec\commons-codec\1.10\commons-codec-1.10.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\commons-collections\commons-collections\3.2.2\commons-collections-3.2.2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\commons-io\commons-io\2.4\commons-io-2.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\commons-logging\commons-logging\1.2\commons-logging-1.2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\commons-net\commons-net\3.1\commons-net-3.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\io\airlift\aircompressor\0.10\aircompressor-0.10.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\io\dropwizard\metrics\metrics-core\4.1.1\metrics-core-4.1.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\io\dropwizard\metrics\metrics-graphite\4.1.1\metrics-graphite-4.1.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\io\dropwizard\metrics\metrics-jmx\4.1.1\metrics-jmx-4.1.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\io\dropwizard\metrics\metrics-json\4.1.1\metrics-json-4.1.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\io\dropwizard\metrics\metrics-jvm\4.1.1\metrics-jvm-4.1.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\io\netty\netty-all\4.1.47.Final\netty-all-4.1.47.Final.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\io\netty\netty\3.10.6.Final\netty-3.10.6.Final.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\io\spray\spray-json_2.12\1.3.2\spray-json_2.12-1.3.2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\jakarta\annotation\jakarta.annotation-api\1.3.5\jakarta.annotation-api-1.3.5.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\jakarta\validation\jakarta.validation-api\2.0.2\jakarta.validation-api-2.0.2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\jakarta\ws\rs\jakarta.ws.rs-api\2.1.6\jakarta.ws.rs-api-2.1.6.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\jakarta\xml\bind\jakarta.xml.bind-api\2.3.2\jakarta.xml.bind-api-2.3.2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\javax\activation\activation\1.1.1\activation-1.1.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\javax\inject\javax.inject\1\javax.inject-1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\javax\servlet\jsp\jsp-api\2.1\jsp-api-2.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\jline\jline\0.9.94\jline-0.9.94.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\log4j\log4j\1.2.17\log4j-1.2.17.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\net\razorvine\pyrolite\4.30\pyrolite-4.30.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\net\sf\opencsv\opencsv\2.3\opencsv-2.3.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\net\sf\py4j\py4j\0.10.9\py4j-0.10.9.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\net\sourceforge\f2j\arpack_combined_all\0.1\arpack_combined_all-0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\antlr\antlr4-runtime\4.7.1\antlr4-runtime-4.7.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\arrow\arrow-format\0.15.1\arrow-format-0.15.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\arrow\arrow-memory\0.15.1\arrow-memory-0.15.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\arrow\arrow-vector\0.15.1\arrow-vector-0.15.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\avro\avro-ipc\1.8.2\avro-ipc-1.8.2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\avro\avro-mapred\1.8.2\avro-mapred-1.8.2-hadoop2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\avro\avro\1.8.2\avro-1.8.2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\commons\commons-compress\1.8.1\commons-compress-1.8.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\commons\commons-lang3\3.9\commons-lang3-3.9.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\commons\commons-text\1.6\commons-text-1.6.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\curator\curator-client\2.7.1\curator-client-2.7.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\curator\curator-framework\2.7.1\curator-framework-2.7.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\curator\curator-recipes\2.7.1\curator-recipes-2.7.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hadoop\hadoop-annotations\2.7.4\hadoop-annotations-2.7.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hadoop\hadoop-auth\2.7.4\hadoop-auth-2.7.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hadoop\hadoop-client\2.7.4\hadoop-client-2.7.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hadoop\hadoop-common\2.7.4\hadoop-common-2.7.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hadoop\hadoop-hdfs\2.7.4\hadoop-hdfs-2.7.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hadoop\hadoop-mapreduce-client-app\2.7.4\hadoop-mapreduce-client-app-2.7.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hadoop\hadoop-mapreduce-client-common\2.7.4\hadoop-mapreduce-client-common-2.7.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hadoop\hadoop-mapreduce-client-core\2.7.4\hadoop-mapreduce-client-core-2.7.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.7.4\hadoop-mapreduce-client-jobclient-2.7.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.7.4\hadoop-mapreduce-client-shuffle-2.7.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hadoop\hadoop-yarn-api\2.7.4\hadoop-yarn-api-2.7.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hadoop\hadoop-yarn-client\2.7.4\hadoop-yarn-client-2.7.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hadoop\hadoop-yarn-common\2.7.4\hadoop-yarn-common-2.7.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hadoop\hadoop-yarn-server-common\2.7.4\hadoop-yarn-server-common-2.7.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hadoop\hadoop-yarn-server-nodemanager\2.7.4\hadoop-yarn-server-nodemanager-2.7.4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\hive\hive-storage-api\2.7.1\hive-storage-api-2.7.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\htrace\htrace-core\3.1.0-incubating\htrace-core-3.1.0-incubating.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\httpcomponents\httpclient\4.5.6\httpclient-4.5.6.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\httpcomponents\httpcore\4.4.10\httpcore-4.4.10.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\orc\orc-core\1.5.10\orc-core-1.5.10.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\orc\orc-mapreduce\1.5.10\orc-mapreduce-1.5.10.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\orc\orc-shims\1.5.10\orc-shims-1.5.10.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\parquet\parquet-column\1.10.1\parquet-column-1.10.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\parquet\parquet-common\1.10.1\parquet-common-1.10.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\parquet\parquet-encoding\1.10.1\parquet-encoding-1.10.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\parquet\parquet-format\2.4.0\parquet-format-2.4.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\parquet\parquet-hadoop\1.10.1\parquet-hadoop-1.10.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\parquet\parquet-jackson\1.10.1\parquet-jackson-1.10.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\spark\spark-avro_2.12\3.0.1\spark-avro_2.12-3.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\spark\spark-catalyst_2.12\3.0.1\spark-catalyst_2.12-3.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\spark\spark-core_2.12\3.0.1\spark-core_2.12-3.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\spark\spark-graphx_2.12\3.0.1\spark-graphx_2.12-3.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\spark\spark-kvstore_2.12\3.0.1\spark-kvstore_2.12-3.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\spark\spark-launcher_2.12\3.0.1\spark-launcher_2.12-3.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\spark\spark-mllib-local_2.12\3.0.1\spark-mllib-local_2.12-3.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\spark\spark-mllib_2.12\3.0.1\spark-mllib_2.12-3.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\spark\spark-network-common_2.12\3.0.1\spark-network-common_2.12-3.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\spark\spark-network-shuffle_2.12\3.0.1\spark-network-shuffle_2.12-3.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\spark\spark-sketch_2.12\3.0.1\spark-sketch_2.12-3.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\spark\spark-sql_2.12\3.0.1\spark-sql_2.12-3.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\spark\spark-streaming_2.12\3.0.1\spark-streaming_2.12-3.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\spark\spark-tags_2.12\3.0.1\spark-tags_2.12-3.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\spark\spark-unsafe_2.12\3.0.1\spark-unsafe_2.12-3.0.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\xbean\xbean-asm7-shaded\4.15\xbean-asm7-shaded-4.15.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\yetus\audience-annotations\0.5.0\audience-annotations-0.5.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\apache\zookeeper\zookeeper\3.4.14\zookeeper-3.4.14.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\beanshell\bsh\2.0b4\bsh-2.0b4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\codehaus\jackson\jackson-jaxrs\1.9.13\jackson-jaxrs-1.9.13.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\codehaus\janino\commons-compiler\3.0.16\commons-compiler-3.0.16.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\codehaus\janino\janino\3.0.16\janino-3.0.16.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\codehaus\jettison\jettison\1.1\jettison-1.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\glassfish\hk2\external\aopalliance-repackaged\2.6.1\aopalliance-repackaged-2.6.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\glassfish\hk2\external\jakarta.inject\2.6.1\jakarta.inject-2.6.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\glassfish\hk2\hk2-api\2.6.1\hk2-api-2.6.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\glassfish\hk2\hk2-locator\2.6.1\hk2-locator-2.6.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\glassfish\hk2\hk2-utils\2.6.1\hk2-utils-2.6.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\glassfish\hk2\osgi-resource-locator\1.0.3\osgi-resource-locator-1.0.3.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\glassfish\jaxb\jaxb-runtime\2.3.2\jaxb-runtime-2.3.2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\glassfish\jersey\containers\jersey-container-servlet-core\2.30\jersey-container-servlet-core-2.30.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\glassfish\jersey\containers\jersey-container-servlet\2.30\jersey-container-servlet-2.30.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\glassfish\jersey\core\jersey-client\2.30\jersey-client-2.30.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\glassfish\jersey\core\jersey-common\2.30\jersey-common-2.30.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\glassfish\jersey\core\jersey-server\2.30\jersey-server-2.30.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\glassfish\jersey\inject\jersey-hk2\2.30\jersey-hk2-2.30.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\glassfish\jersey\media\jersey-media-jaxb\2.30\jersey-media-jaxb-2.30.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\javassist\javassist\3.25.0-GA\javassist-3.25.0-GA.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\json4s\json4s-ast_2.12\3.6.6\json4s-ast_2.12-3.6.6.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\json4s\json4s-core_2.12\3.6.6\json4s-core_2.12-3.6.6.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\json4s\json4s-jackson_2.12\3.6.6\json4s-jackson_2.12-3.6.6.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\json4s\json4s-scalap_2.12\3.6.6\json4s-scalap_2.12-3.6.6.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\lz4\lz4-java\1.7.1\lz4-java-1.7.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\mortbay\jetty\jetty-sslengine\6.1.26\jetty-sslengine-6.1.26.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\objenesis\objenesis\2.5.1\objenesis-2.5.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\openpnp\opencv\3.2.0-1\opencv-3.2.0-1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\roaringbitmap\RoaringBitmap\0.7.45\RoaringBitmap-0.7.45.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\roaringbitmap\shims\0.7.45\shims-0.7.45.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\scala-lang\modules\scala-collection-compat_2.12\2.1.1\scala-collection-compat_2.12-2.1.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\scala-lang\modules\scala-parser-combinators_2.12\1.1.2\scala-parser-combinators_2.12-1.1.2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\scala-lang\modules\scala-xml_2.12\1.2.0\scala-xml_2.12-1.2.0.jar;C:\Users\marhamil\.sbt\boot\scala-2.12.10\lib\scala-library.jar;C:\Users\marhamil\.sbt\boot\scala-2.12.10\lib\scala-reflect.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\scalactic\scalactic_2.12\3.0.5\scalactic_2.12-3.0.5.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\scalanlp\breeze-macros_2.12\1.0\breeze-macros_2.12-1.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\scalanlp\breeze_2.12\1.0\breeze_2.12-1.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\scalatest\scalatest_2.12\3.0.5\scalatest_2.12-3.0.5.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\slf4j\jcl-over-slf4j\1.7.30\jcl-over-slf4j-1.7.30.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\slf4j\jul-to-slf4j\1.7.30\jul-to-slf4j-1.7.30.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\slf4j\slf4j-api\1.7.30\slf4j-api-1.7.30.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\slf4j\slf4j-log4j12\1.7.30\slf4j-log4j12-1.7.30.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\testng\testng\6.8.8\testng-6.8.8.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\threeten\threeten-extra\1.5.0\threeten-extra-1.5.0.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\tukaani\xz\1.5\xz-1.5.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\typelevel\algebra_2.12\2.0.0-M2\algebra_2.12-2.0.0-M2.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\typelevel\cats-kernel_2.12\2.0.0-M4\cats-kernel_2.12-2.0.0-M4.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\typelevel\machinist_2.12\0.6.8\machinist_2.12-0.6.8.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\typelevel\macro-compat_2.12\1.1.1\macro-compat_2.12-1.1.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\typelevel\spire-macros_2.12\0.17.0-M1\spire-macros_2.12-0.17.0-M1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\typelevel\spire-platform_2.12\0.17.0-M1\spire-platform_2.12-0.17.0-M1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\typelevel\spire-util_2.12\0.17.0-M1\spire-util_2.12-0.17.0-M1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\typelevel\spire_2.12\0.17.0-M1\spire_2.12-0.17.0-M1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\org\xerial\snappy\snappy-java\1.1.7.5\snappy-java-1.1.7.5.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\oro\oro\2.0.8\oro-2.0.8.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\pl\edu\icm\JLargeArrays\1.5\JLargeArrays-1.5.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\xerces\xercesImpl\2.9.1\xercesImpl-2.9.1.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\xml-apis\xml-apis\1.3.04\xml-apis-1.3.04.jar;C:\Users\marhamil\AppData\Local\Coursier\cache\v1\https\repo1.maven.org\maven2\xmlenc\xmlenc\0.52\xmlenc-0.52.jar"" org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner -s com.microsoft.ml.spark.io.split1.JsonOutputParserSuite -testName sanityCheck -showProgressMessages true


Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties21/01/05 00:58:52 INFO SparkContext: Running Spark version 3.0.121/01/05 00:58:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable21/01/05 00:58:53 INFO ResourceUtils: ==============================================================21/01/05 00:58:53 INFO ResourceUtils: Resources for spark.driver:
21/01/05 00:58:53 INFO ResourceUtils: ==============================================================21/01/05 00:58:53 INFO SparkContext: Submitted application: JsonOutputParserSuite21/01/05 00:58:53 INFO SparkContext: Spark configuration:spark.app.name=JsonOutputParserSuitespark.driver.maxResultSize=6gspark.logConf=truespark.master=local[*]spark.sql.crossJoin.enabled=truespark.sql.shuffle.partitions=20spark.sql.warehouse.dir=file:/code/mmlspark/spark-warehouse21/01/05 00:58:53 INFO SecurityManager: Changing view acls to: marhamil21/01/05 00:58:53 INFO SecurityManager: Changing modify acls to: marhamil21/01/05 00:58:53 INFO SecurityManager: Changing view acls groups to: 21/01/05 00:58:53 INFO SecurityManager: Changing modify acls groups to: 21/01/05 00:58:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(marhamil); groups with view permissions: Set(); users  with modify permissions: Set(marhamil); groups with modify permissions: Set()21/01/05 00:58:54 INFO Utils: Successfully started service 'sparkDriver' on port 56701.21/01/05 00:58:54 INFO SparkEnv: Registering MapOutputTracker21/01/05 00:58:54 INFO SparkEnv: Registering BlockManagerMaster21/01/05 00:58:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information21/01/05 00:58:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up21/01/05 00:58:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat21/01/05 00:58:54 INFO DiskBlockManager: Created local directory at C:\Users\marhamil\AppData\Local\Temp\blockmgr-3dfa3260-d9b6-481d-8aa3-b64e30ce23ed21/01/05 00:58:54 INFO MemoryStore: MemoryStore started with capacity 4.0 GiB21/01/05 00:58:54 INFO SparkEnv: Registering OutputCommitCoordinator21/01/05 00:58:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.21/01/05 00:58:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://host.docker.internal:404021/01/05 00:58:55 INFO Executor: Starting executor ID driver on host host.docker.internal21/01/05 00:58:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56746.21/01/05 00:58:55 INFO NettyBlockTransferService: Server created on host.docker.internal:5674621/01/05 00:58:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy21/01/05 00:58:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, host.docker.internal, 56746, None)21/01/05 00:58:55 INFO BlockManagerMasterEndpoint: Registering block manager host.docker.internal:56746 with 4.0 GiB RAM, BlockManagerId(driver, host.docker.internal, 56746, None)21/01/05 00:58:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, host.docker.internal, 56746, None)21/01/05 00:58:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, host.docker.internal, 56746, None)21/01/05 00:58:55 WARN SharedState: Not allowing to set spark.sql.warehouse.dir or hive.metastore.warehouse.dir in SparkSession's options, it should be set statically for cross-session usages

Failed to execute user defined function(JsonOutputParserSuite$$Lambda$574/51376124: (struct<a:int>) => struct<a:int>)org.apache.spark.SparkException: Failed to execute user defined function(JsonOutputParserSuite$$Lambda$574/51376124: (struct<a:int>) => struct<a:int>) at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1130) at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:156) at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(InterpretedMutableProjection.scala:83) at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$17.$anonfun$applyOrElse$71(Optimizer.scala:1508) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238) at scala.collection.Iterator.foreach(Iterator.scala:941) at scala.collection.Iterator.foreach$(Iterator.scala:941) at scala.collection.AbstractIterator.foreach(Iterator.scala:1429) at scala.collection.IterableLike.foreach(IterableLike.scala:74) at scala.collection.IterableLike.foreach$(IterableLike.scala:73) at scala.collection.AbstractIterable.foreach(Iterable.scala:56) at scala.collection.TraversableLike.map(TraversableLike.scala:238) at scala.collection.TraversableLike.map$(TraversableLike.scala:231) at scala.collection.AbstractTraversable.map(Traversable.scala:108) at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$17.applyOrElse(Optimizer.scala:1508) at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$17.applyOrElse(Optimizer.scala:1503) at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:309) at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72) at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:309) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:149) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:147) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314) at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399) at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237) at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397) at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350) at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:149) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:147) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314) at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399) at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237) at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397) at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350) at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:149) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:147) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:314) at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:399) at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:237) at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:397) at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:350) at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:314) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:149) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:147) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:298) at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1503) at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$.apply(Optimizer.scala:1502) at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:149) at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60) at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68) at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38) at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:146) at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:138) at scala.collection.immutable.List.foreach(List.scala:392) at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:138) at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:116) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88) at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:116) at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:82) at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111) at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:133) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764) at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:133) at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:82) at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:79) at org.apache.spark.sql.execution.QueryExecution.$anonfun$writePlans$4(QueryExecution.scala:197) at org.apache.spark.sql.catalyst.plans.QueryPlan$.append(QueryPlan.scala:381) at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$writePlans(QueryExecution.scala:197) at org.apache.spark.sql.execution.QueryExecution.toString(QueryExecution.scala:207) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:95) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616) at org.apache.spark.sql.Dataset.head(Dataset.scala:2697) at org.apache.spark.sql.Dataset.take(Dataset.scala:2904) at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300) at org.apache.spark.sql.Dataset.showString(Dataset.scala:337) at org.apache.spark.sql.Dataset.show(Dataset.scala:824) at org.apache.spark.sql.Dataset.show(Dataset.scala:783) at org.apache.spark.sql.Dataset.show(Dataset.scala:792) at com.microsoft.ml.spark.io.split1.JsonOutputParserSuite.$anonfun$new$1(ParserSuite.scala:84) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:186) at org.scalatest.TestSuite.withFixture(TestSuite.scala:196) at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195) at org.scalatest.FunSuite.withFixture(FunSuite.scala:1560) at org.scalatest.FunSuiteLike.invokeWithFixture$1(FunSuiteLike.scala:184) at org.scalatest.FunSuiteLike.$anonfun$runTest$1(FunSuiteLike.scala:196) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289) at org.scalatest.FunSuiteLike.runTest(FunSuiteLike.scala:196) at org.scalatest.FunSuiteLike.runTest$(FunSuiteLike.scala:178) at com.microsoft.ml.spark.core.test.base.TestBase.org$scalatest$BeforeAndAfterEachTestData$$super$runTest(TestBase.scala:74) at org.scalatest.BeforeAndAfterEachTestData.runTest(BeforeAndAfterEachTestData.scala:194) at org.scalatest.BeforeAndAfterEachTestData.runTest$(BeforeAndAfterEachTestData.scala:187) at com.microsoft.ml.spark.core.test.base.TestBase.runTest(TestBase.scala:74) at org.scalatest.FunSuiteLike.$anonfun$runTests$1(FunSuiteLike.scala:229) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396) at scala.collection.immutable.List.foreach(List.scala:392) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384) at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:379) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:461) at org.scalatest.FunSuiteLike.runTests(FunSuiteLike.scala:229) at org.scalatest.FunSuiteLike.runTests$(FunSuiteLike.scala:228) at org.scalatest.FunSuite.runTests(FunSuite.scala:1560) at org.scalatest.Suite.run(Suite.scala:1147) at org.scalatest.Suite.run$(Suite.scala:1129) at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1560) at org.scalatest.FunSuiteLike.$anonfun$run$1(FunSuiteLike.scala:233) at org.scalatest.SuperEngine.runImpl(Engine.scala:521) at org.scalatest.FunSuiteLike.run(FunSuiteLike.scala:233) at org.scalatest.FunSuiteLike.run$(FunSuiteLike.scala:232) at com.microsoft.ml.spark.core.test.base.TestBase.org$scalatest$BeforeAndAfterAll$$super$run(TestBase.scala:74) at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213) at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) at com.microsoft.ml.spark.core.test.base.TestBase.run(TestBase.scala:74) at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45) at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1346) at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1340) at scala.collection.immutable.List.foreach(List.scala:392) at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1340) at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:1031) at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:1010) at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1506) at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1010) at org.scalatest.tools.Runner$.run(Runner.scala:850) at org.scalatest.tools.Runner.run(Runner.scala) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2or3(ScalaTestRunner.java:41) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)Caused by: java.lang.RuntimeException: Error while decoding: java.lang.NullPointerExceptionnewInstance(class com.microsoft.ml.spark.io.split1.Bar) at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:188) at org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$createToScalaConverter$1(ScalaUDF.scala:115) at org.apache.spark.sql.catalyst.expressions.ScalaUDF.$anonfun$f$2(ScalaUDF.scala:157) at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1127) ... 148 moreCaused by: java.lang.NullPointerException at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:184) ... 151 more
Info Provided - Suite JsonOutputParserSuite took 7.715sInfo Provided - Shutting down spark session
Creating a spark session for suite JsonOutputParserSuite

Process finished with exit code 0

Expected output when cache is uncommented:
{code:java}
+---+----+----+
| c0|  c1|  c2|
+---+----+----+
|[1]|null|null|
|[1]|null|null|
|[1]|null|null|
|[1]|null|null|
|[1]|null|null|
|[1]|null|null|
|[1]|null|null|
|[1]|null|null|
|[1]|null|null|
|[1]|null|null|
+---+----+----+

{code}","Windows 10 Surface book 3

Local Spark

IntelliJ Idea

 ",apachespark,dongjoon,LuciferYang,mhamilton,viirya,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32154,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 11 19:32:18 UTC 2021,,,,,,,,,,"0|z0m18o:",9223372036854775807,,,,,,,,,,,,,3.1.1,,,,,,,,,,"09/Jan/21 09:46;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/31103;;;","11/Jan/21 19:27;dongjoon;I marked this as a blocker for Apache Spark 3.1.1 because this is a regression due to SPARK-32154 at 3.1.0.;;;","11/Jan/21 19:32;dongjoon;Issue resolved by pull request 31103
[https://github.com/apache/spark/pull/31103];;;",,,,,,,,,,,,,,,,,
ExecutorAllocationListener threw an exception java.util.NoSuchElementException,SPARK-34000,13349039,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,05/Jan/21 03:05,05/Jan/21 07:43,13/Jul/23 08:47,05/Jan/21 05:38,3.0.1,3.1.0,3.2.0,,,,,,,,,,,,,3.0.2,3.1.0,,,Spark Core,,,,0,,,"21/01/04 03:00:32,259 WARN [task-result-getter-2] scheduler.TaskSetManager:69 : Lost task 306.1 in stage 600.0 (TID 283610, hdc49-mcc10-01-0510-4108-039-tess0097.stratus.rno.ebay.com, executor 27): TaskKilled (another attempt succeeded)
21/01/04 03:00:32,259 INFO [task-result-getter-2] scheduler.TaskSetManager:57 : Task 306.1 in stage 600.0 (TID 283610) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the
previous stage needs to be re-run, or because a different copy of the task has already succeeded).
21/01/04 03:00:32,259 INFO [task-result-getter-2] cluster.YarnClusterScheduler:57 : Removed TaskSet 600.0, whose tasks have all completed, from pool default
21/01/04 03:00:32,259 INFO [HiveServer2-Handler-Pool: Thread-5853] thriftserver.SparkExecuteStatementOperation:190 : Returning result set with 50 rows from offsets [5378600, 5378650) with 1fe245f8-a7f9-4ec0-bcb5-8cf324cbbb47
21/01/04 03:00:32,260 ERROR [spark-listener-group-executorManagement] scheduler.AsyncEventQueue:94 : Listener ExecutorAllocationListener threw an exception
java.util.NoSuchElementException: key not found: Stage 600 (Attempt 0)
        at scala.collection.MapLike.default(MapLike.scala:235)
        at scala.collection.MapLike.default$(MapLike.scala:234)
        at scala.collection.AbstractMap.default(Map.scala:63)
        at scala.collection.mutable.HashMap.apply(HashMap.scala:69)
        at org.apache.spark.ExecutorAllocationManager$ExecutorAllocationListener.onTaskEnd(ExecutorAllocationManager.scala:621)
        at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:45)
        at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
        at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:38)
        at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:38)
        at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:115)
        at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:99)
        at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)
        at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
        at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:102)
        at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:97)
        at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1320)
        at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:97)",,apachespark,cltlfcjin,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 05 05:38:58 UTC 2021,,,,,,,,,,"0|z0m14g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/21 03:36;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/31025;;;","05/Jan/21 05:38;dongjoon;Issue resolved by pull request 31025
[https://github.com/apache/spark/pull/31025];;;",,,,,,,,,,,,,,,,,,
Make sbt unidoc success with JDK11,SPARK-33999,13349036,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,05/Jan/21 02:27,12/Dec/22 18:10,13/Jul/23 08:47,05/Jan/21 10:04,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Build,,,,0,,,"With the current master, sbt unidoc fails because the generated Java sources cause syntax error.

As of JDK11, the default doclet seems to refuse such syntax error.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,SPARK-24417,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 05 10:04:13 UTC 2021,,,,,,,,,,"0|z0m13s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/21 02:47;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/31023;;;","05/Jan/21 10:04;gurwls223;Issue resolved by pull request 31023
[https://github.com/apache/spark/pull/31023];;;",,,,,,,,,,,,,,,,,,
resolveOperatorsUpWithNewOutput should wrap allowInvokingTransformsInAnalyzer,SPARK-33992,13348940,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,04/Jan/21 13:01,12/Jan/21 09:10,13/Jul/23 08:47,05/Jan/21 05:34,3.1.0,,,,,,,,,,,,,,,3.1.0,,,,SQL,Tests,,,0,,,"PaddingAndLengthCheckForCharVarchar could fail query when resolveOperatorsUpWithNewOutput
with 


{code:java}
[info] - char/varchar resolution in sub query  *** FAILED *** (367 milliseconds)
[info]   java.lang.RuntimeException: This method should not be called in the analyzer
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.assertNotAnalysisRule(AnalysisHelper.scala:150)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.assertNotAnalysisRule$(AnalysisHelper.scala:146)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.assertNotAnalysisRule(LogicalPlan.scala:29)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:161)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:160)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
[info]   at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$updateOuterReferencesInSubquery(QueryPlan.scala:267)
{code}
",,apachespark,cloud_fan,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34083,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 05 05:34:44 UTC 2021,,,,,,,,,,"0|z0m0io:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/21 13:07;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31013;;;","05/Jan/21 05:34;cloud_fan;Issue resolved by pull request 31013
[https://github.com/apache/spark/pull/31013];;;",,,,,,,,,,,,,,,,,,
invalidate char/varchar in spark.readStream.schema,SPARK-33980,13348881,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,04/Jan/21 07:42,12/Dec/22 18:10,13/Jul/23 08:47,04/Jan/21 21:00,3.1.0,,,,,,,,,,,,,,,3.1.0,,,,Structured Streaming,,,,0,,,invalidate char/varchar in spark.readStream.schema just like what we do for spark.read.schema,,apachespark,dongjoon,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 05 07:43:10 UTC 2021,,,,,,,,,,"0|z0m05k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/21 07:50;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31003;;;","04/Jan/21 07:51;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31003;;;","04/Jan/21 21:00;dongjoon;Issue resolved by pull request 31003
[https://github.com/apache/spark/pull/31003];;;","05/Jan/21 07:43;gurwls223;I need to recreate rc1 tag. I failed to create a RC due to an dependency issue SPARK-34007.  I am correcting the fix version to 3.1.0;;;",,,,,,,,,,,,,,,,
Fix incorrect min partition condition in getRanges,SPARK-33962,13348741,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,viirya,viirya,02/Jan/21 18:06,03/Jan/21 09:29,13/Jul/23 08:47,03/Jan/21 09:29,3.2.0,,,,,,,,,,,,,,,3.2.0,,,,Structured Streaming,,,,0,,,"When calculating offset ranges, we consider minPartitions configuration. If minPartitions is not set or is less than or equal the size of given ranges, it means there are enough partitions at Kafka so we don't need to split offsets to satisfy min partition requirement. But the current condition is offsetRanges.size > minPartitions.get and is not correct. Currently getRanges will split offsets in unnecessary case.",,apachespark,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 03 09:29:30 UTC 2021,,,,,,,,,,"0|z0lzag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jan/21 18:08;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/30994;;;","03/Jan/21 09:29;dongjoon;Issue resolved by pull request 30994
[https://github.com/apache/spark/pull/30994];;;",,,,,,,,,,,,,,,,,,
Upgrade SBT to 1.4.6,SPARK-33961,13348740,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,02/Jan/21 17:41,02/Jan/21 22:49,13/Jul/23 08:47,02/Jan/21 22:49,3.1.0,,,,,,,,,,,,,,,3.1.0,,,,Build,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 02 22:49:36 UTC 2021,,,,,,,,,,"0|z0lza8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jan/21 17:43;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/30993;;;","02/Jan/21 22:49;dongjoon;Issue resolved by pull request 30993
[https://github.com/apache/spark/pull/30993];;;",,,,,,,,,,,,,,,,,,
