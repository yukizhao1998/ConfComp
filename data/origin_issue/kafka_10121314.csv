Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Container),Outward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Inward issue link (Incorporates),Outward issue link (Incorporates),Outward issue link (Incorporates),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Regression),Inward issue link (Required),Outward issue link (Supercedes),Inward issue link (dependent),Inward issue link (dependent),Outward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
metadata.fetch.timeout.ms set to zero blocks forever,KAFKA-1836,12764075,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jaikiran,ppearcy,ppearcy,29/Dec/14 20:28,23/Nov/15 03:29,14/Jul/23 05:38,12/Jan/15 23:59,0.8.2.0,,,0.9.0.0,,,,,,,clients,,,0,newbie,,,"You can easily work around this by setting the timeout value to 1ms, but 0ms should mean 0ms or at least have the behavior documented. ",,ewencp,jaikiran,jkreps,nehanarkhede,ppearcy,stevenz3wu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/15 04:09;jaikiran;KAFKA-1836-new-patch.patch;https://issues.apache.org/jira/secure/attachment/12691038/KAFKA-1836-new-patch.patch","05/Jan/15 07:04;jaikiran;KAFKA-1836.patch;https://issues.apache.org/jira/secure/attachment/12690024/KAFKA-1836.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 12 23:59:44 UTC 2015,,,,,,,,,,"0|i23ttj:",9223372036854775807,,nehanarkhede,,,,,,,,,,,,,,,,,,"29/Dec/14 22:16;jkreps;Looks like Metadata.awaitUpdate() always calls Object.wait(maxWaitMs) even if maxWaitMs is 0. Unfortunately Object.wait(0) doesn't mean wait for zero ms, it means wait forever. So it should be an easy fix--we need an if statement here to check for a wait time of 0.;;;","05/Jan/15 07:04;jaikiran;I've attached a patch which handles <= 0 values in the awaitUpdate method. I've also added a testcase to verify this change. Can you take a look at this and see if this makes sense?
;;;","07/Jan/15 20:37;ewencp;A couple of notes on the current patch:

* I don't think we need to change the config limits -- 0 should be an acceptable setting, it just isn't behaving properly right now.
* You can get the right behavior just by adding an if statement as Jay suggested -- don't call wait() if maxWaitMs is 0. The subsequent check that generates the TimeoutException should then work.
* Checking for the invalid value < 0 is ok, but the code that calls it guarantees it won't since the config is already validated to have a value >= 0.
* In the test, maybe check that the timeout is correct rather than whether you got the right metadata back since the test is supposed to focus on the timeouts.;;;","09/Jan/15 04:12;jaikiran;Thanks Ewen for your feedback. I've taken into account your comments and updated the patch accordingly and created a review request https://reviews.apache.org/r/29752/. I've also uploaded the same patch here to the JIRA.;;;","09/Jan/15 18:56;nehanarkhede;[~jaikiran] Will help you check this in after [~ewencp]'s comment is addressed.;;;","10/Jan/15 03:03;jaikiran;Thanks [~nehanarkhede], I've updated the review board patch to address [~ewencp]'s latest comment.  Thanks [~ewencp] for reviewing it.;;;","12/Jan/15 23:59;nehanarkhede;Thanks for the patches! Pushed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No Response when handle LeaderAndIsrRequest some case,KAFKA-1834,12763991,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,tedxia,tedxia,29/Dec/14 07:52,12/Sep/17 13:46,14/Jul/23 05:39,12/Sep/17 13:46,0.8.2.0,,,,,,,,,,,,,0,easyfix,,,"When a replica become leader or follower, if this broker no exist in assigned replicas, there are no response.",,githubbot,omkreddy,tedxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/14 05:59;tedxia;KAFKA-1834.patch;https://issues.apache.org/jira/secure/attachment/12689439/KAFKA-1834.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 12 13:46:53 UTC 2017,,,,,,,,,,"0|i23tb3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Dec/14 07:56;githubbot;GitHub user tedxia opened a pull request:

    https://github.com/apache/kafka/pull/40

    KAFKA-1834: No Response when handle LeaderAndIsrRequest some case

    PR for [KAFKA-1834](https://issues.apache.org/jira/browse/KAFKA-1834)
    
    When a replica become leader or follower, if this broker no exist in assigned replicas, there are no response.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/tedxia/kafka fix-noresponse-on-become-leader-or-follower

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/40.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #40
    
----
commit 80858f72c39eaa974a6085f78e797de4e8c55aae
Author: xiajun <xiajun@xiaomi.com>
Date:   2014-12-29T07:53:44Z

    No Response when handle LeaderAndIsrRequest some case

----
;;;","30/Dec/14 05:59;tedxia;Created reviewboard https://reviews.apache.org/r/29476/diff/
 against branch apache/0.8.2;;;","02/Feb/16 23:10;githubbot;Github user stumped2 closed the pull request at:

    https://github.com/apache/kafka/pull/40
;;;","12/Sep/17 13:46;omkreddy;This was fixed in newer Kafka versions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Async Producer will cause 'java.net.SocketException: Too many open files' when broker host does not exist,KAFKA-1832,12763960,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,barneywill,barneywill,29/Dec/14 03:01,15/Aug/17 17:47,14/Jul/23 05:39,15/Aug/17 17:47,0.8.1,0.8.1.1,,,,,,,,,producer ,,,0,,,,"h3.How to replay the problem:
* producer configuration:
** producer.type=async
** metadata.broker.list=not.existed.com:9092
Make sure the host '*not.existed.com*' does not exist in DNS server or /etc/hosts;
* send a lot of messages continuously using the above producer
It will cause '*java.net.SocketException: Too many open files*' after a while, or you can use '*lsof -p $pid|wc -l*' to check the count of open files which will be increasing as time goes by until it reaches the system limit(check by '*ulimit -n*').

h3.Problem cause:
{code:title=kafka.network.BlockingChannel|borderStyle=solid} 
channel.connect(new InetSocketAddress(host, port))
{code}
this line will throw an exception '*java.nio.channels.UnresolvedAddressException*' when broker host does not exist, and at this same time the field '*connected*' is false;
In *kafka.producer.SyncProducer*, '*disconnect()*' will not invoke '*blockingChannel.disconnect()*' because '*blockingChannel.isConnected*' is false which means the FileDescriptor will be created but never closed;

h3.More:
When the broker is an non-existent ip(for example: metadata.broker.list=1.1.1.1:9092) instead of an non-existent host, the problem will not appear;
In *SocketChannelImpl.connect()*, '*Net.checkAddress()*' is not in try-catch block but '*Net.connect()*' is in, that makes the difference;

h3.Temporary Solution:
{code:title=kafka.network.BlockingChannel|borderStyle=solid} 
try
{
    channel.connect(new InetSocketAddress(host, port))
}
catch
{
    case e: UnresolvedAddressException => 
    {
        disconnect();
        throw e
    }
}
{code}",linux,barneywill,omkreddy,pasthelod,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 15 17:47:45 UTC 2017,,,,,,,,,,"0|i23t47:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/17 17:47;omkreddy;Fixed in  KAFKA-1041;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
in ConsoleProducer - properties key.separator and parse.key no longer work,KAFKA-1824,12762628,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gwenshap,gwenshap,gwenshap,18/Dec/14 17:19,28/Feb/15 15:42,14/Jul/23 05:39,28/Feb/15 15:42,,,,0.9.0.0,,,,,,,tools,,,0,,,,"Looks like the change in kafka-1711 breaks them accidentally.

reader.init is called with readerProps which is initialized with commandline properties as defaults.

the problem is that reader.init checks:
    if(props.containsKey(""parse.key""))
and defaults don't return true in this case.",,gwenshap,joestein,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/14 01:56;gwenshap;KAFKA-1824.patch;https://issues.apache.org/jira/secure/attachment/12688211/KAFKA-1824.patch","18/Dec/14 19:08;gwenshap;KAFKA-1824.patch;https://issues.apache.org/jira/secure/attachment/12688090/KAFKA-1824.patch","28/Feb/15 00:40;gwenshap;KAFKA-1824.v1.patch;https://issues.apache.org/jira/secure/attachment/12701505/KAFKA-1824.v1.patch","23/Dec/14 00:17;gwenshap;KAFKA-1824_2014-12-22_16:17:42.patch;https://issues.apache.org/jira/secure/attachment/12688747/KAFKA-1824_2014-12-22_16%3A17%3A42.patch","27/Feb/15 07:05;gwenshap;KAFKA-1824_2015-02-26_23:05:10.patch;https://issues.apache.org/jira/secure/attachment/12701287/KAFKA-1824_2015-02-26_23%3A05%3A10.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 28 15:42:44 UTC 2015,,,,,,,,,,"0|i23l1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Dec/14 19:08;gwenshap;Created reviewboard https://reviews.apache.org/r/29211/diff/
 against branch trunk;;;","19/Dec/14 01:06;nehanarkhede;Thanks for the patch. Pushed to trunk.;;;","19/Dec/14 01:08;gwenshap;Thanks for the quick review [~nehanarkhede]!;;;","19/Dec/14 01:56;gwenshap;Created reviewboard https://reviews.apache.org/r/29231/diff/
 against branch trunk;;;","19/Dec/14 01:58;gwenshap;My apologies! 

Additional round of tests revealed that my first patch accidentally breaks what was fixed in KAFKA-1711 - i.e. the ""WARN Property topic is not valid"" message returned. Those VerifiableProperties are tricky!

I added a new patch, on top of the one already committed that removes the extra properties before creating the producer and eliminates the WARN messages.;;;","19/Dec/14 02:21;nehanarkhede;[~gwenshap] Good catch. I wonder if we should just add tests given the tricky logic involved.;;;","19/Dec/14 02:28;gwenshap;Yes, lets do that. Will help us avoid another round of the break-and-fix cycle.

Hold off on this patch and I'll provide tests in a day or two.;;;","23/Dec/14 00:17;gwenshap;Updated reviewboard https://reviews.apache.org/r/29231/diff/
 against branch trunk;;;","23/Dec/14 00:23;gwenshap;I added tests, I also did some refactoring to ConsoleProducer to allow testing (i.e. pulled some portions off main() to separate functions where we can test them). 

One caveat - This patch removes an undocumented feature: 
Before the patch, properties specified in --property were passed to the producer (even though it is documented as properties for the MessageReader), so users could configure producer properties not supported by the console producer directly to the underlying producer.
Now the properties specified in --property are only sent to the reader, as documented.

If we think that allowing users to send custom properties to producer is useful, I can add an additional option (--producer-property).;;;","23/Dec/14 03:55;joestein;Shouldn't this go into 0.8.2 branch also since it is a fix for a regression bug?;;;","26/Dec/14 19:14;gwenshap;[~joestein] 0.8.2 branch is actually ok (i.e. you get a warning about ""invalid property"", but key.separator and parse.key work fine).
So this just needs to go into trunk.;;;","19/Jan/15 22:31;gwenshap;I noticed that [~junrao] committed KAFKA-1711 to 0.8.2 branch last week, so we need this patch on 0.8.2 as well.;;;","20/Jan/15 00:20;junrao;Gwen, thanks for point this out. I am reverting KAFKA-1711 from the 0.8.2 branch before cutting RC1.;;;","20/Jan/15 00:55;junrao;Reopening the issue since the followup patch hasn't been committed yet.;;;","20/Jan/15 03:14;gwenshap;Thanks, Jun. 
I changed status to Patch Available, so once the 0.8.2 madness calms down a bit, we can review and get it into trunk.;;;","12/Feb/15 21:17;gwenshap;pinging for review :);;;","22/Feb/15 03:42;nehanarkhede;[~gwenshap] Is this the rb you are asking a review on? https://reviews.apache.org/r/29231/diff/;;;","22/Feb/15 06:17;gwenshap;Thats the right RB, [~nehanarkhede].

;;;","22/Feb/15 20:17;nehanarkhede;[~gwenshap] I'm having trouble applying the patch on trunk -
{code}
nnarkhed-mn1:kafka nnarkhed$ git apply --check 1824.patch 
error: patch failed: core/src/main/scala/kafka/tools/ConsoleProducer.scala:36
error: core/src/main/scala/kafka/tools/ConsoleProducer.scala: patch does not apply
error: patch failed: core/src/main/scala/kafka/tools/ConsoleProducer.scala:34
error: core/src/main/scala/kafka/tools/ConsoleProducer.scala: patch does not apply
{code};;;","27/Feb/15 07:05;gwenshap;Updated reviewboard https://reviews.apache.org/r/29231/diff/
 against branch trunk;;;","28/Feb/15 00:29;nehanarkhede;[~gwenshap] Still doesn't apply. Not sure if I'm doing something incorrectly-
{code}
nnarkhed-mn1:kafka nnarkhed$ git apply --check ~/Projects/kafka-patches/1824.patch 
error: patch failed: core/src/main/scala/kafka/tools/ConsoleProducer.scala:36
error: core/src/main/scala/kafka/tools/ConsoleProducer.scala: patch does not apply
error: patch failed: core/src/main/scala/kafka/tools/ConsoleProducer.scala:34
error: core/src/main/scala/kafka/tools/ConsoleProducer.scala: patch does not apply
error: core/src/test/scala/kafka/tools/ConsoleProducerTest.scala: already exists in working directory
error: patch failed: core/src/main/scala/kafka/tools/ConsoleProducer.scala:76
error: core/src/main/scala/kafka/tools/ConsoleProducer.scala: patch does not apply
error: patch failed: core/src/main/scala/kafka/tools/ConsoleProducer.scala:266
error: core/src/main/scala/kafka/tools/ConsoleProducer.scala: patch does not apply
{code};;;","28/Feb/15 00:40;gwenshap;I'll blame the patch-tool.

I generated new patch with ""git diff"" and tested on trunk. Seems to work :);;;","28/Feb/15 15:42;nehanarkhede;Thanks for the patches. Pushed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Example shell scripts broken,KAFKA-1821,12761825,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,leland_fy,leland_fy,15/Dec/14 18:20,15/Aug/17 17:39,14/Jul/23 05:39,15/Aug/17 17:39,0.8.1.1,,,,,,,,,,packaging,tools,,0,,,,"After run ./gradlew jarAll to generate all jars including for examples, I try to run the producer-consumer demo from shell scripts. But it doesn't work and throw  ClassNotFoundException.  It seems the shell scripts (java-producer-consumer-demo and java-simple-consumer-demo) still work on the library structure for sbt. So it cannot find jar files under new structure forced by gradle. ","Ubuntu 14.04, Linux 75477193b766 3.13.0-24-generic #46-Ubuntu SMP Thu Apr 10 19:11:08 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux. Scala: 2.8.0",leland_fy,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 15 17:39:37 UTC 2017,,,,,,,,,,"0|i23g5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Aug/17 17:39;omkreddy;It is working in newer Kafka versions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleaner gets confused about deleted and re-created topics,KAFKA-1819,12761671,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,gwenshap,gian,gian,15/Dec/14 02:24,13/Jan/15 05:33,14/Jul/23 05:39,13/Jan/15 05:33,,,,0.8.2.0,,,,,,,,,,0,,,,"I get an error like this after deleting a compacted topic and re-creating it. I think it's because the brokers don't remove cleaning checkpoints from the cleaner-offset-checkpoint file. This is from a build based off commit bd212b7.

java.lang.IllegalArgumentException: requirement failed: Last clean offset is 587607 but segment base offset is 0 for log foo-6.
        at scala.Predef$.require(Predef.scala:233)
        at kafka.log.Cleaner.buildOffsetMap(LogCleaner.scala:502)
        at kafka.log.Cleaner.clean(LogCleaner.scala:300)
        at kafka.log.LogCleaner$CleanerThread.cleanOrSleep(LogCleaner.scala:214)
        at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:192)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)",,donnchadh,gian,gwenshap,jjkoshy,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/14 18:59;gwenshap;KAFKA-1819.patch;https://issues.apache.org/jira/secure/attachment/12688087/KAFKA-1819.patch","26/Dec/14 21:58;gwenshap;KAFKA-1819_2014-12-26_13:58:44.patch;https://issues.apache.org/jira/secure/attachment/12689200/KAFKA-1819_2014-12-26_13%3A58%3A44.patch","31/Dec/14 00:01;gwenshap;KAFKA-1819_2014-12-30_16:01:19.patch;https://issues.apache.org/jira/secure/attachment/12689598/KAFKA-1819_2014-12-30_16%3A01%3A19.patch","12/Jan/15 18:34;gwenshap;KAFKA-1819_2015-01-12_10:34:07.patch;https://issues.apache.org/jira/secure/attachment/12691690/KAFKA-1819_2015-01-12_10%3A34%3A07.patch","12/Jan/15 19:18;gwenshap;KAFKA-1819_2015-01-12_11:17:53.patch;https://issues.apache.org/jira/secure/attachment/12691703/KAFKA-1819_2015-01-12_11%3A17%3A53.patch","13/Jan/15 01:01;gwenshap;KAFKA-1819_2015-01-12_17:01:53.patch;https://issues.apache.org/jira/secure/attachment/12691811/KAFKA-1819_2015-01-12_17%3A01%3A53.patch",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 13 05:33:32 UTC 2015,,,,,,,,,,"0|i23f7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Dec/14 03:07;gwenshap;Yep, looks like a bug.

When we delete a topic we abort the cleanup for the log:
 if (cleaner != null)
        cleaner.abortCleaning(topicAndPartition)

I think we need to add to the LogCleanerManager class a removeLog method. 
This should take a lock, read the checkpoints, remove the log we are deleting from the map and write the checkpoints back.

Then we need to call it after aborting the cleaning.

If this makes sense to someone who knows what they are doing ([~nehanarkhede] ?) I can implement this.;;;","16/Dec/14 08:16;jjkoshy;This is a similar stack-trace to KAFKA-1641 but that is a different (in fact uncertain) root cause.

I think the log is already removed from the map (pool) in LogManager.deleteLog - that same map is passed through to the cleaner. The issue is that in doneCleaning (where the checkpoint file is written) we never remove entries. So we can probably just filter out topic-partitions that are no longer present in the logs map when writing the checkpoint file. We will need to refactor a little to force write the checkpoints if cleaning for that topicPartition was not in progress.;;;","16/Dec/14 18:19;gwenshap;Thanks for pointing the similar issue [~jjkoshy].

The log is indeed removed from the pool in LogManager.deleteLog, and we could remove them in doneCleaning. 

However, I think we want to be able to force cleaning as part of the topic delete.
If we don't do it, the checkpoint file will only get updated some time later when doneCleaning is called. This can be more challenging to troubleshoot and also may not happen before Gian creates a new topic with same name.;;;","17/Dec/14 04:07;nehanarkhede;[~jjkoshy] Shouldn't we remove the deleted topic from all files that maintain either a cleaner or recovery checkpoint before delete topic is considered completed?;;;","17/Dec/14 04:34;gwenshap;The recovery checkpoints are currently handled correctly (By LogManager, I think?), the only issue is with the cleaner file.;;;","17/Dec/14 06:08;jjkoshy;[~gwenshap] that's right - that's what I meant by ""force write the checkpoints if cleaning was not in progress"". As you said, it needs to happen proactively when deleting a log, but we probably don't need to force a cleaning for that since we just need to update the cleaner checkpoint file. So I was thinking we could refactor the code a tiny bit to have a helper write out the checkpoint file and call that from both doneCleaning as well as when deleting logs.;;;","17/Dec/14 17:27;nehanarkhede;+1 on [~jjkoshy]'s suggestion. That makes more sense. ;;;","18/Dec/14 18:59;gwenshap;Created reviewboard https://reviews.apache.org/r/29210/diff/
 against branch trunk;;;","26/Dec/14 21:58;gwenshap;Updated reviewboard https://reviews.apache.org/r/29210/diff/
 against branch trunk;;;","29/Dec/14 22:15;nehanarkhede;[~gwenshap] Thanks for the updated patch! Left a minor comment. I can help you check this in after that is addressed.;;;","31/Dec/14 00:01;gwenshap;Updated reviewboard https://reviews.apache.org/r/29210/diff/
 against branch trunk;;;","05/Jan/15 22:04;nehanarkhede;[~gwenshap]. Thanks for incorporating the review suggestions. Left another suggestion to refactor the test, so that all other delete topics tests, current and future, can benefit from the cleaner checkpoint validation check. ;;;","06/Jan/15 01:26;gwenshap;Moving the check will not always verify deletion from checkpoint file. 
The checkpoints file only exists if brokers are configured with ""compact"" and not ""delete"" and cleanup actually happened before topic deletion (this requires very small segments and writing to the topic before its deleted). Thats why testDeleteTopicWithCleaner is a fairly involved test with very specific broker configuration.  So by moving the check we will check the contents of an empty file most of the time.

Since the new test covers the Cleaner codepath, I'm not sure what are the benefits of adding the check to all tests.  Do you want to activate the Cleaner on every test? Or run the validation regardless of whether the cleaner was active?
;;;","08/Jan/15 02:19;nehanarkhede;[~gwenshap] What I was trying to achieve by moving the check to the generic validator API is to benefit future tests that also end up using the compaction policy. I'm also ok with doing that when we write the next test that has a use for it :)
Will go ahead and merge your patch.;;;","09/Jan/15 18:01;nehanarkhede;[~gwenshap] Saw the following unit test failures on the latest patch on 0.8.2 branch and trunk, while trying to check it in -

{code}
kafka.server.ServerShutdownTest > testCleanShutdownWithDeleteTopicEnabled FAILED
    junit.framework.AssertionFailedError: expected:<0> but was:<1>
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.failNotEquals(Assert.java:277)
        at junit.framework.Assert.assertEquals(Assert.java:64)
        at junit.framework.Assert.assertEquals(Assert.java:195)
        at junit.framework.Assert.assertEquals(Assert.java:201)
        at kafka.server.ServerShutdownTest.verifyNonDaemonThreadsStatus(ServerShutdownTest.scala:145)
        at kafka.server.ServerShutdownTest.testCleanShutdownWithDeleteTopicEnabled(ServerShutdownTest.scala:114)

kafka.server.ServerShutdownTest > testCleanShutdownAfterFailedStartup FAILED
    junit.framework.AssertionFailedError: expected:<0> but was:<1>
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.failNotEquals(Assert.java:277)
        at junit.framework.Assert.assertEquals(Assert.java:64)
        at junit.framework.Assert.assertEquals(Assert.java:195)
        at junit.framework.Assert.assertEquals(Assert.java:201)
        at kafka.server.ServerShutdownTest.verifyNonDaemonThreadsStatus(ServerShutdownTest.scala:145)
        at kafka.server.ServerShutdownTest.testCleanShutdownAfterFailedStartup(ServerShutdownTest.scala:141)

kafka.server.ServerShutdownTest > testCleanShutdown FAILED
    junit.framework.AssertionFailedError: expected:<0> but was:<1>
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.failNotEquals(Assert.java:277)
        at junit.framework.Assert.assertEquals(Assert.java:64)
        at junit.framework.Assert.assertEquals(Assert.java:195)
        at junit.framework.Assert.assertEquals(Assert.java:201)
        at kafka.server.ServerShutdownTest.verifyNonDaemonThreadsStatus(ServerShutdownTest.scala:145)
        at kafka.server.ServerShutdownTest.testCleanShutdown(ServerShutdownTest.scala:101)
{code};;;","09/Jan/15 21:51;gwenshap;I'm having trouble reproducing the errors:

{code}
gshapira-MBP:kafka082 gshapira$ ./gradlew -Dtest.single=ServerShutdownTest core::test
To honour the JVM settings for this build a new JVM will be forked. Please consider using the daemon: http://gradle.org/docs/2.0/userguide/gradle_daemon.html.
Building project 'core' with Scala version 2.10.4
:clients:compileJava UP-TO-DATE
:clients:processResources UP-TO-DATE
:clients:classes UP-TO-DATE
:clients:jar UP-TO-DATE
:core:compileJava UP-TO-DATE
:core:compileScala UP-TO-DATE
:core:processResources UP-TO-DATE
:core:classes UP-TO-DATE
:core:compileTestJava UP-TO-DATE
:core:compileTestScala UP-TO-DATE
:core:processTestResources UP-TO-DATE
:core:testClasses UP-TO-DATE
:core:test

kafka.server.ServerShutdownTest > testCleanShutdown PASSED

kafka.server.ServerShutdownTest > testCleanShutdownWithDeleteTopicEnabled PASSED

kafka.server.ServerShutdownTest > testCleanShutdownAfterFailedStartup PASSED

BUILD SUCCESSFUL

Total time: 8.504 secs
{code};;;","09/Jan/15 23:05;gwenshap;Looking more into this, I'm not even sure if it can be related. 

testCleanShutdownWithDeleteTopicEnabled does enable topic deletion, but it never creates or deletes topics - just startup and shutdown. So the code-path for deleting topics (and our changes) don't seem to execute here.

Still trying to figure out what's going on.;;;","12/Jan/15 06:53;gwenshap;Hey [~nehanarkhede], can you double check that you tested on branches with KAFKA-1815 applied?
It seems to resolve the exact error you encountered.;;;","12/Jan/15 07:02;gwenshap;Ick, never mind. I see that the DeleteLog test I added doesn't shut down properly. Thats probably it :(
I'll upload a new patch tomorrow morning.

I still have no idea why the error doesn't reproduce in my environment.;;;","12/Jan/15 18:34;gwenshap;Updated reviewboard https://reviews.apache.org/r/29210/diff/
 against branch trunk;;;","12/Jan/15 18:40;nehanarkhede;[~gwenshap] Thanks for looking into the possible cause. I just looked at the patch and looks like there are a few changes unrelated to the patch - https://reviews.apache.org/r/29210/diff/3-4/ ? I guess the only changed expected should be in the tests right?;;;","12/Jan/15 18:42;gwenshap;Ouch. Yes. Good catch! 

Let me check how this got in. Probably unclean repo.;;;","12/Jan/15 19:18;gwenshap;Updated reviewboard https://reviews.apache.org/r/29210/diff/
 against branch trunk;;;","13/Jan/15 01:01;gwenshap;Updated reviewboard https://reviews.apache.org/r/29210/diff/
 against branch trunk;;;","13/Jan/15 05:33;nehanarkhede;Thanks for the patches, [~gwenshap]. Appreciate your help getting this into the 0.8.2 release. Pushed to trunk and 0.8.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ServerShutdownTest fails in trunk.,KAFKA-1815,12760475,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,fanatoly,fanatoly,fanatoly,09/Dec/14 16:44,13/Dec/14 16:10,14/Jul/23 05:39,12/Dec/14 19:39,,,,0.9.0.0,,,,,,,,,,0,,,,"I ran into these failures consistently when trying to build Kafka locally:

kafka.server.ServerShutdownTest > testCleanShutdown FAILED
    java.lang.NullPointerException
        at kafka.server.ServerShutdownTest$$anonfun$verifyNonDaemonThreadsStatus$2.apply(ServerShutdownTest.scala:147)
        at kafka.server.ServerShutdownTest$$anonfun$verifyNonDaemonThreadsStatus$2.apply(ServerShutdownTest.scala:147)
        at scala.collection.TraversableOnce$$anonfun$count$1.apply(TraversableOnce.scala:114)
        at scala.collection.TraversableOnce$$anonfun$count$1.apply(TraversableOnce.scala:113)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:105)
        at scala.collection.TraversableOnce$class.count(TraversableOnce.scala:113)
        at scala.collection.mutable.ArrayOps$ofRef.count(ArrayOps.scala:105)
        at kafka.server.ServerShutdownTest.verifyNonDaemonThreadsStatus(ServerShutdownTest.scala:147)
        at kafka.server.ServerShutdownTest.testCleanShutdown(ServerShutdownTest.scala:101)

kafka.server.ServerShutdownTest > testCleanShutdownWithDeleteTopicEnabled FAILED
    java.lang.NullPointerException
        at kafka.server.ServerShutdownTest$$anonfun$verifyNonDaemonThreadsStatus$2.apply(ServerShutdownTest.scala:147)
        at kafka.server.ServerShutdownTest$$anonfun$verifyNonDaemonThreadsStatus$2.apply(ServerShutdownTest.scala:147)
        at scala.collection.TraversableOnce$$anonfun$count$1.apply(TraversableOnce.scala:114)
        at scala.collection.TraversableOnce$$anonfun$count$1.apply(TraversableOnce.scala:113)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:105)
        at scala.collection.TraversableOnce$class.count(TraversableOnce.scala:113)
        at scala.collection.mutable.ArrayOps$ofRef.count(ArrayOps.scala:105)
        at kafka.server.ServerShutdownTest.verifyNonDaemonThreadsStatus(ServerShutdownTest.scala:147)
        at kafka.server.ServerShutdownTest.testCleanShutdownWithDeleteTopicEnabled(ServerShutdownTest.scala:114)

kafka.server.ServerShutdownTest > testCleanShutdownAfterFailedStartup FAILED
    java.lang.NullPointerException
        at kafka.server.ServerShutdownTest$$anonfun$verifyNonDaemonThreadsStatus$2.apply(ServerShutdownTest.scala:147)
        at kafka.server.ServerShutdownTest$$anonfun$verifyNonDaemonThreadsStatus$2.apply(ServerShutdownTest.scala:147)
        at scala.collection.TraversableOnce$$anonfun$count$1.apply(TraversableOnce.scala:114)
        at scala.collection.TraversableOnce$$anonfun$count$1.apply(TraversableOnce.scala:113)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:105)
        at scala.collection.TraversableOnce$class.count(TraversableOnce.scala:113)
        at scala.collection.mutable.ArrayOps$ofRef.count(ArrayOps.scala:105)
        at kafka.server.ServerShutdownTest.verifyNonDaemonThreadsStatus(ServerShutdownTest.scala:147)
        at kafka.server.ServerShutdownTest.testCleanShutdownAfterFailedStartup(ServerShutdownTest.scala:141)

It looks like Jenkins also had issues with these tests:

https://builds.apache.org/job/Kafka-trunk/351/console

I would like to provide a patch that fixes this.",,copester,fanatoly,joestein,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/14 16:45;fanatoly;shutdown_test_fix.patch;https://issues.apache.org/jira/secure/attachment/12686031/shutdown_test_fix.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 13 16:10:30 UTC 2014,,,,,,,,,,"0|i238e7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Dec/14 18:02;copester;Dear Committers, I have verified that shutdown_test_fix.patch does indeed fix the test failures in trunk. Please apply.

Cheers!;;;","12/Dec/14 15:56;copester;Unexpectedly, this was fixed by https://github.com/apache/kafka/commit/e8ffbd0fee0bc715ad0fe6c9afe85715f84d8e51 that [~joestein] committed to fix KAFKA-1812. I'm looking through his commit and don't see why it would fix this bug, but the test results don't lie.;;;","12/Dec/14 16:10;joestein;CI says it is still broken https://builds.apache.org/view/All/job/Kafka-trunk/ and it was broken for me when I did that commit... I didn't see this ticket until just now will look through it later when I have some time towards fixing this, I mentioned KAFKA-1650 about it also;;;","12/Dec/14 16:20;copester;I spoke too soon. I'm now getting different test results after running this a bunch of times on our test farm. Sometimes the 3 tests on this ticket fail. Sometimes testMetricsLeak fails. Sometimes all 4 fail together.;;;","12/Dec/14 19:39;junrao;Thanks for the patch. +1 and committed to trunk.;;;","13/Dec/14 02:17;copester;Thanks, [~junrao], though that was actually [~fanatoly]'s patch. In terms of current state of tests passing since the latest commit 523b36589e942cb99a95debd2c45e795ae533d08 for KAFKA-1813, and I'm seeing consistent passing of all the tests except for the occasional KAFKA-1501 failures which continue to haunt me. Thanks!;;;","13/Dec/14 16:10;junrao;Thanks, updated the assignee to Anatoly.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build fails for scala 2.9.2,KAFKA-1813,12760463,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,fanatoly,fanatoly,fanatoly,09/Dec/14 15:28,13/Dec/14 02:22,14/Jul/23 05:39,12/Dec/14 19:54,,,,0.9.0.0,,,,,,,build,,,0,,,,"Currently, in trunk, the 2.9.2 build fails with the following error:

MirrorMaker.scala:507 overloaded method value commitOffsets with alternatives:
  (isAutoCommit: Boolean,topicPartitionOffsets: scala.collection.immutable.Map[kafka.common.TopicAndPartition,kafka.common.OffsetAndMetadata])Unit <and>
  (isAutoCommit: Boolean)Unit <and>
  => Unit
 cannot be applied to (isAutoCommit: Boolean, scala.collection.immutable.Map[kafka.common.TopicAndPartition,kafka.common.OffsetAndMetadata])
        connector.commitOffsets(isAutoCommit = false, offsetsToCommit)

It looks like the 2.9.2 compiler cannot resolve an overloaded method when mixing named and ordered parameters.

I ran into this when I cloned the repo and ran ./gradlew test.",,fanatoly,guozhang,joestein,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/14 15:31;fanatoly;fix_2_9_2_build.patch;https://issues.apache.org/jira/secure/attachment/12686025/fix_2_9_2_build.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Dec 13 02:22:44 UTC 2014,,,,,,,,,,"0|i238bj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Dec/14 15:31;fanatoly;patch fixing scala 2.9.2 build;;;","09/Dec/14 15:53;joestein;Did you run the new bootstrap we added to the README?;;;","09/Dec/14 16:37;fanatoly;Yes. The project was bootstrapped. I was wrong about running into this when I ran test. I must have run test_core_2_9_2 directly.;;;","12/Dec/14 19:54;junrao;Thanks for the patch. The issue was introduced by KAFKA-1650. +1 and committed to trunk.;;;","13/Dec/14 02:22;guozhang;Thanks Anatoly.

When I ran the tests / builds before checking in KAFKA-1650 I did not see either this issue or KAFKA-1815, which is a bit wired. Will do some local testing again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Allow IpV6 in configuration with parseCsvMap,KAFKA-1812,12760349,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jholoman,jholoman,jholoman,09/Dec/14 04:37,06/Jan/15 16:42,14/Jul/23 05:39,12/Dec/14 08:48,,,,0.9.0.0,,,,,,,,,,0,newbie,,,"The current implementation of parseCsvMap in Utils expects k:v,k:v. This modifies that function to accept a string with multiple "":"" characters and splitting on the last occurrence per pair. 

This limitation is noted in the Reviewboard comments for KAFKA-1512",,gwenshap,jholoman,joestein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1512,,,,,,,,,,,,,"11/Dec/14 02:39;jholoman;KAFKA-1812_2014-12-10_21:38:59.patch;https://issues.apache.org/jira/secure/attachment/12686422/KAFKA-1812_2014-12-10_21%3A38%3A59.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 12 08:48:42 UTC 2014,,,,,,,,,,"0|i237mf:",9223372036854775807,,gwenshap,,,,,,,,,,,,,,,,,,"09/Dec/14 21:11;jholoman;https://reviews.apache.org/r/28859/diff/;;;","11/Dec/14 02:39;jholoman;Updated reviewboard https://reviews.apache.org/r/28859/diff/
 against branch origin/trunk;;;","12/Dec/14 05:39;gwenshap;[~joestein] - this patch & tests look good to me. 
If you can take a look and commit if it looks good to you as well (LGTY2?), I'd appreciate.;;;","12/Dec/14 08:48;joestein;+ 1 committed to trunk, thanks for the patch Jeff and the review Gwen!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
broker fetch request uses old leader offset which is higher than current leader offset causes error,KAFKA-1806,12759528,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,lokeshbirla,lokeshbirla,04/Dec/14 22:16,24/Dec/14 01:06,14/Jul/23 05:39,24/Dec/14 01:06,0.8.1.1,,,0.8.2.0,,,,,,,consumer,,,0,,,,"Although following issue: https://issues.apache.org/jira/browse/KAFKA-727
is marked fixed but I still see this issue in 0.8.1.1. I am able to reproducer the issue consistently. 

[2014-08-18 06:43:58,356] ERROR [KafkaApi-1] Error when processing fetch request for partition [mmetopic4,2] offset 1940029 from consumer with correlation id 21 (kafka.server.Kaf
kaApis)
java.lang.IllegalArgumentException: Attempt to read with a maximum offset (1818353) less than the start offset (1940029).
        at kafka.log.LogSegment.read(LogSegment.scala:136)
        at kafka.log.Log.read(Log.scala:386)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSet(KafkaApis.scala:530)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:476)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:471)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:119)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
        at scala.collection.immutable.Map$Map1.map(Map.scala:107)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSets(KafkaApis.scala:471)
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:783)
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:765)
        at kafka.server.RequestPurgatory$ExpiredRequestReaper.run(RequestPurgatory.scala:216)
        at java.lang.Thread.run(Thread.java:745)

",,eapache,joestein,kzadorozhny,lokeshbirla,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 24 01:04:53 UTC 2014,,,,,,,,,,"0|i232nj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Dec/14 22:19;lokeshbirla;I have 3 node cluster kafka broker running one broker on each blade. I have one zookeeper running on another blade. 
I created 4 partitions with replication factor 3 each and producer is sending messages from one blade and consumer is getting from another blade. I see above issue consistently. 

However this issue did not exist when I use same configuaration with upto 3 topics. I increase the heap size from 4GB to 16GB however same issue. ;;;","08/Dec/14 17:48;lokeshbirla;is there any update on this?;;;","09/Dec/14 04:59;nehanarkhede;[~lokeshbirla] Please can you provide the steps to reproduce this issue?;;;","10/Dec/14 22:32;lokeshbirla;ok. I'll create the steps to reproducer this. Basically I am using sarama client for kafka which is go client. ;;;","11/Dec/14 18:42;nehanarkhede;[~lokeshbirla] We don't support that client. You may have to loop in the maintainer of that library. Let us know if you see this behavior with the java/scala client.;;;","16/Dec/14 02:06;lokeshbirla;Neha,

I can see this problem quite often. 


[2014-08-29 11:37:34,980] ERROR [ReplicaFetcherThread-2-2], Current offset 11396282 for partition [mmetopic1,1] out of range; reset offset to 3006843 (kafka.server.ReplicaFetcherThread)

kafka.common.OffsetOutOfRangeException: Request for offset 7602056 but we only have log segments in the range 0 to 7471002.

I'll send you steps for this. 
;;;","16/Dec/14 19:37;eapache;Sarama client maintainer here (via https://github.com/Shopify/sarama/issues/226); this looks like a kafka bug to me since the error in the log message is from a ReplicaFetcherThread, but I'm happy to provide extra information on the behaviour of the client if you think it's relevant.;;;","16/Dec/14 21:34;lokeshbirla;This problem occurs multiple times in server.log. 
Currently I am using:

 #added replica fetchers
num.replica.fetchers=4


[2014-08-30 04:00:58,419] ERROR [ReplicaFetcherThread-1-2], Current offset 7343326909 for partition [mmetopic1,0] out of range; reset offset to 7351079341 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:01:58,351] ERROR [ReplicaFetcherThread-1-2], Current offset 7352830699 for partition [mmetopic1,0] out of range; reset offset to 7360600212 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:01:58,398] ERROR [ReplicaFetcherThread-2-2], Current offset 7362122784 for partition [mmetopic1,1] out of range; reset offset to 7369788902 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:01:58,428] ERROR [ReplicaFetcherThread-3-2], Current offset 7349217662 for partition [mmetopic1,2] out of range; reset offset to 7356979468 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:02:58,380] ERROR [ReplicaFetcherThread-3-2], Current offset 7358748697 for partition [mmetopic1,2] out of range; reset offset to 7366511359 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:02:58,431] ERROR [ReplicaFetcherThread-2-2], Current offset 7371546217 for partition [mmetopic1,1] out of range; reset offset to 7379322019 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:02:58,491] ERROR [ReplicaFetcherThread-1-2], Current offset 7362381355 for partition [mmetopic1,0] out of range; reset offset to 7370131818 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:03:58,553] ERROR [ReplicaFetcherThread-3-2], Current offset 7368280588 for partition [mmetopic1,2] out of range; reset offset to 7376042337 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:03:58,606] ERROR [ReplicaFetcherThread-1-2], Current offset 7371895090 for partition [mmetopic1,0] out of range; reset offset to 7379659373 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:03:58,745] ERROR [ReplicaFetcherThread-2-2], Current offset 7381073377 for partition [mmetopic1,1] out of range; reset offset to 7388856060 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:04:58,377] ERROR [ReplicaFetcherThread-2-2], Current offset 7390601461 for partition [mmetopic1,1] out of range; reset offset to 7398383811 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:04:58,378] ERROR [ReplicaFetcherThread-1-2], Current offset 7381410731 for partition [mmetopic1,0] out of range; reset offset to 7389193402 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:04:58,462] ERROR [ReplicaFetcherThread-3-2], Current offset 7377936663 for partition [mmetopic1,2] out of range; reset offset to 7385573885 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:05:58,440] ERROR [ReplicaFetcherThread-2-2], Current offset 7400170911 for partition [mmetopic1,1] out of range; reset offset to 7407915357 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:05:58,441] ERROR [ReplicaFetcherThread-1-2], Current offset 7390968588 for partition [mmetopic1,0] out of range; reset offset to 7398725995 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:05:58,442] ERROR [ReplicaFetcherThread-3-2], Current offset 7387325243 for partition [mmetopic1,2] out of range; reset offset to 7395096361 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:06:58,326] ERROR [ReplicaFetcherThread-1-2], Current offset 7400572665 for partition [mmetopic1,0] out of range; reset offset to 7411422730 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:06:58,346] ERROR [ReplicaFetcherThread-2-2], Current offset 7409827554 for partition [mmetopic1,1] out of range; reset offset to 7417436416 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:06:58,511] ERROR [ReplicaFetcherThread-3-2], Current offset 7396889418 for partition [mmetopic1,2] out of range; reset offset to 7404620618 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:07:58,328] ERROR [ReplicaFetcherThread-2-2], Current offset 7419467753 for partition [mmetopic1,1] out of range; reset offset to 7420615385 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:07:58,362] ERROR [ReplicaFetcherThread-3-2], Current offset 7406461331 for partition [mmetopic1,2] out of range; reset offset to 7410977640 (kafka.server.ReplicaFetcherThread)
[2014-08-30 04:07:58,588] ERROR [ReplicaFetcherThread-1-2], Current offset 7413376626 for partition [mmetopic1,0] out of range; reset offset to 7414599975 (kafka.server.ReplicaFetcherThread)

;;;","18/Dec/14 17:48;lokeshbirla;Hi Neha,

What is the status of fixing this issue? This issue happens on every run. I have seen, if I use: num.replica.fetchers=1, then sometimes this issue goes away however I see other problem of leadership changes very often even when all brokers are running. 

If I set: num.replica.fetchers=4, then I can reproduce this issue on every run. 

Please let me or Evan (from sarama) know if you need any help to fix this. ;;;","19/Dec/14 02:24;nehanarkhede;[~lokeshbirla] I was looking for steps to reproduce this. So if I download 0.8.2-beta and go through your steps, I should be able to see the same error you see.;;;","19/Dec/14 18:53;joestein;I am not sure if this is directly related but perhaps in some way possibly so I wanted to bring it up. I just created https://issues.apache.org/jira/browse/KAFKA-1825 which is a case where the Sarama client is putting Kafka in a bad state.  I suspect this might be the same type of scenario too.

[~lokeshbirla] is there some chance of getting code to reproduce your issue succinctly (please see my KAFKA-1825 sample code to reproduce and even a binary for folks to try out). 

<< sometimes this issue goes away however I see other problem of leadership changes very often even when all brokers are running.
This is a another issue I see in production with the Sarama client. I am working on hunting down the root cause but right now the thinking is that it is related to https://issues.apache.org/jira/browse/KAFKA-766 and https://github.com/Shopify/sarama/issues/236 with https://github.com/Shopify/sarama/commit/03ad601663634fd75eb357fee6782653f5a9a5ed being a fix for it.  ;;;","20/Dec/14 06:14;lokeshbirla;Hi Joe,

Your problem described in  https://issues.apache.org/jira/browse/KAFKA-1825 is very similar. I get leadership changes quite often even with 3 partitions itself. I also found that using sarma fix where I am restricting batch size to 1000, DOES not resolve the problem. I tried to slow down the producer speed by using batchsize=1000. Even with 72k message/sec (msg size 150 bytes), I still see leadership change issue and broker offset error. 

I only filed sarama issue:  https://github.com/Shopify/sarama/issues/236. 

--Lokesh;;;","20/Dec/14 06:53;lokeshbirla;Hello Neha,

I did further debugging on this by turning trace on and found the following. 

1. Broker 1 and broker 3 have different offset for partition 0 for topic mmetopic1.  Broker 1 has higher offset than Broker 3.  
2. Due to kafka leadership changed, Broker 3 became the leader which has lower offset and when Broker 1 send fetch request with higher offset, error comes from broker 3 since it does NOT have that higher offset. 

Here is important trace information. 

Broker 1 (log)

[2014-09-02 06:53:55,466] DEBUG Partition [mmetopic1,0] on broker 1: Old hw for partition [mmetopic1,0] is 1330329. New hw is 1330329. All leo's are 1371212,1330329,1331850 (kafka.cluster.Partition)[2014-09-02 06:53:55,537] INFO Truncating log mmetopic1-0 to offset 1329827. (kafka.log.Log)
[2014-09-02 06:53:55,477] INFO [ReplicaFetcherManager on broker 1] Added fetcher for partitions ArrayBuffer, [[mmetopic1,0], initOffset 1330329 to broker id:3,host:10.1.130.1,port:9092] ) (kafka.server.ReplicaFetcherManager
[2014-09-02 06:53:55,479] TRACE [KafkaApi-1] Handling request: Name:UpdateMetadataRequest;Version:0;Controller:2;ControllerEpoch:2;CorrelationId:5;ClientId:id_2-host_null-port_9092;AliveBrokers:id:3,host:10.1.130.1,port:9092,id:2,host:10.1.129.1,port:9092,id:1,host:10.1.128.1,port:9092;PartitionState:[mmetopic1,0] -> (LeaderAndIsrInfo:(Leader:3,ISR:3,2,LeaderEpoch:2,ControllerEpoch:2),ReplicationFactor:3),AllReplicas:1,2,3) from client: /10.1.128.1:59805 (kafka.server.KafkaApis)
[2014-09-02 06:53:55,490] TRACE [ReplicaFetcherThread-0-3], issuing to broker 3 of fetch request Name: FetchRequest; Version: 0; CorrelationId: 3687; ClientId: ReplicaFetcherThread-0-3; ReplicaId: 1; MaxWait: 500 ms; MinBytes: 1 bytes; RequestInfo: [mmetopic1,0] -> PartitionFetchInfo(1330329,2097152) (kafka.server.ReplicaFetcherThread)
[2014-09-02 06:53:55,543] WARN [ReplicaFetcherThread-0-3], Replica 1 for partition [mmetopic1,0] reset its fetch offset to current leader 3's latest offset 1329827 (kafka.server.ReplicaFetcherThread)
[2014-09-02 06:53:55,543] ERROR [ReplicaFetcherThread-0-3], Current offset 1330329 for partition [mmetopic1,0] out of range; reset offset to 1329827 (kafka.server.ReplicaFetcherThread)

Broker 3 (log)
[2014-09-02 06:53:06,525] TRACE Setting log end offset for replica 2 for partition [mmetopic1,0] to 1330329 (kafka.cluster.Replica)
[2014-09-02 06:53:06,526] DEBUG Partition [mmetopic1,0] on broker 3: Old hw for partition [mmetopic1,0] is 1329827. New hw is 1329827. All leo's are 1329827,1330329 (kafka.cluster.Partition)




=========================================================================================================

[2014-09-02 06:53:06,530] ERROR [KafkaApi-3] Error when processing fetch request for partition [mmetopic1,0] offset 1330329 from follower with correlation id 3686 (kafka.server.KafkaApis)
kafka.common.OffsetOutOfRangeException: Request for offset 1330329 but we only have log segments in the range 0 to 1329827.
        at kafka.log.Log.read(Log.scala:380)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSet(KafkaApis.scala:530)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:476)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:471)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
        at scala.collection.immutable.Map$Map3.foreach(Map.scala:164)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
        at scala.collection.immutable.Map$Map3.map(Map.scala:144)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSets(KafkaApis.scala:471)
        at kafka.server.KafkaApis.handleFetchRequest(KafkaApis.scala:437)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:186)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:42)
        at java.lang.Thread.run(Thread.java:745)

==========================================================================================

;;;","22/Dec/14 19:46;lokeshbirla;Neha,

Could you please update on this? With my recent comment and title change for the problem you should now know the detail information about the problem. Please let me know if you need further information. 

Lokesh;;;","23/Dec/14 17:54;nehanarkhede;Pasting my comment above 

[~lokeshbirla] I was looking for steps to reproduce this. So if I download 0.8.2-beta and go through your steps, I should be able to see the same error you see. 

If you can provide this, someone can help out.;;;","24/Dec/14 01:04;lokeshbirla;Neha,

I used 0.8.2-beta and it works great. I did not see any problem so far. I think this issue can be closed now. I found multiple issues with 0.8.1.1 however I see NO issue with 0.8.2-beta. 

Thanks,
Lokesh;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka network thread lacks top exception handler,KAFKA-1804,12758900,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,ovgolovin,ovgolovin,02/Dec/14 14:41,07/Sep/17 19:32,14/Jul/23 05:39,13/Oct/15 18:21,0.8.2.0,,,0.9.0.0,,,,,,,core,,,3,,,,"We have faced the problem that some kafka network threads may fail, so that jstack attached to Kafka process showed fewer threads than we had defined in our Kafka configuration. This leads to API requests processed by this thread getting stuck unresponed.

There were no error messages in the log regarding thread failure.

We have examined Kafka code to find out there is no top try-catch block in the network thread code, which could at least log possible errors.

Could you add top-level try-catch block for the network thread, which should recover network thread in case of exception?",,aozeritsky,ataraxer,ijuma,jkreps,olindaspider,ovgolovin,sekikn,sriharsha,vladap,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-2595,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 13 18:21:50 UTC 2015,,,,,,,,,,"0|i22yrz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Dec/14 12:10;ovgolovin;`kafka-socket-acceptor` has the same problem.;;;","15/Jan/15 22:24;aozeritsky;We've written the simple patch for kafka-network-thread:
{code:java}
  override def run(): Unit = {
    try {
      iteration() // = the original run()
    } catch {
      case e: Throwable => 
        error(""ERROR IN NETWORK THREAD: %s"".format(e), e)
        Runtime.getRuntime.halt(1)
    }
  }
{code}
and got the following trace:
{code}
[2015-01-15 23:04:08,537] ERROR ERROR IN NETWORK THREAD: java.util.NoSuchElementException: None.get (kafka.network.Processor)
java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:313)
        at scala.None$.get(Option.scala:311)
        at kafka.network.ConnectionQuotas.dec(SocketServer.scala:544)
        at kafka.network.AbstractServerThread.close(SocketServer.scala:165)
        at kafka.network.AbstractServerThread.close(SocketServer.scala:157)
        at kafka.network.Processor.close(SocketServer.scala:394)
        at kafka.network.Processor.processNewResponses(SocketServer.scala:426)
        at kafka.network.Processor.iteration(SocketServer.scala:328)
        at kafka.network.Processor.run(SocketServer.scala:381)
        at java.lang.Thread.run(Thread.java:745)
{code};;;","21/Jan/15 19:55;sriharsha;[~jjkoshy] [~aozeritsky] this looks to be similar in nature to KAFKA-1577.  Do you have any steps to reproduce this.;;;","22/Jan/15 16:52;aozeritsky;The last time we saw the bug during restart the network switch on a cluster of 20 machines. kafka-network-threads fell down on more than half machines. As a result, the cluster became unavailable. We are trying to find the specific steps that reproduce the problem.
;;;","07/Feb/15 22:40;jkreps;The remaining issue is the lack of logging. However we actually do set an uncaught exception handler that should log any uncaught exception.  [~aozeritsky] is there any chance this was just showing up in a different log?;;;","28/Sep/15 00:11;olindaspider;I ran into a similar issue where that same ""java.util.NoSuchElementException: None.get"" exception was being thrown in the ConnectionQuotas.dec method. I was able to reproduce it, and I believe I have found the root cause of all cases of these.

The call to ""close(key)"" on this line https://github.com/apache/kafka/blob/0.8.2/core/src/main/scala/kafka/network/SocketServer.scala#L406 is the culprit. This call should not be done there because, as the debug log on the line just above says, the socket is already closed. In other words, a ""close(key)"" using that key has already occurred. This causes an extra call on ConnectionQuotas.dec against that InetAddress. This sets up the situation where later on during the closing of an actually open key that there is now a None value in ConnectionQuotas count for that InetAddress.

I have a log files if needed.;;;","28/Sep/15 22:09;sriharsha;[~olindaspider] that sounds right to me. Are you planning on sending a patch.;;;","28/Sep/15 22:29;olindaspider;I was not planning on sending a patch. I have it working on my servers, so I am all set. ;;;","12/Oct/15 11:53;vladap;Hello, I just would like to report that this error was thrown after 4 days run on Kafka version: kafka-0.8.2.2-2.11. We were not attempting to shutdown. Unfortunately we were not able to reproduce the error yet. I just want to increase the attention to this issue.

{code}
[2015-10-08 14:40:47,176] ERROR Uncaught exception in thread 'kafka-network-thread-9092-1': (kafka.utils.Utils$)
java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:347)
        at scala.None$.get(Option.scala:345)
        at kafka.network.ConnectionQuotas.dec(SocketServer.scala:524)
        at kafka.network.AbstractServerThread.close(SocketServer.scala:165)
        at kafka.network.AbstractServerThread.close(SocketServer.scala:157)
        at kafka.network.Processor.close(SocketServer.scala:374)
        at kafka.network.Processor.processNewResponses(SocketServer.scala:406)
        at kafka.network.Processor.run(SocketServer.scala:318)
        at java.lang.Thread.run(Thread.java:745)
[2015-10-08 14:40:47,177] INFO Closing socket connection to /10.33.167.154. (kafka.network.Processor)
[2015-10-08 14:40:47,177] INFO Closing socket connection to /10.33.167.154. (kafka.network.Processor)
[2015-10-08 14:40:47,177] INFO Closing socket connection to /10.33.167.154. (kafka.network.Processor)
[2015-10-08 14:40:47,177] ERROR Uncaught exception in thread 'kafka-network-thread-9092-0': (kafka.utils.Utils$)
java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:347)
        at scala.None$.get(Option.scala:345)
        at kafka.network.ConnectionQuotas.dec(SocketServer.scala:524)
        at kafka.network.AbstractServerThread.close(SocketServer.scala:165)
        at kafka.network.AbstractServerThread.close(SocketServer.scala:157)
        at kafka.network.Processor.close(SocketServer.scala:374)
        at kafka.network.Processor.run(SocketServer.scala:350)
        at java.lang.Thread.run(Thread.java:745)
{code};;;","12/Oct/15 12:34;ijuma;Note that the code in trunk looks different and the following code was removed:

{code}
 catch {
        case e: CancelledKeyException => {
          debug(""Ignoring response for closed socket."")
          close(key)
        }
      } 
{code}

There were also some fixes as part of KAFKA-2614.

Furthermore, `Processor.run` and `Acceptor.run` also have try/catch blocks in trunk. All of this code is in `SocketServer.scala`:

https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/network/SocketServer.scala

So, this looks fixed to me. Could one of the people that have pointed out the issue in the code verify that my assessment is correct?;;;","12/Oct/15 17:02;olindaspider;It looks likely fixed in the trunk, but remains an issue in the 0.8.x line.

What is the intention with the trunk changes? Is that an entirely new major version? Is there a road map somewhere explaining what is going on in the trunk development that I missed?;;;","12/Oct/15 17:08;ijuma;trunk will be released as 0.9.0.0, see http://search-hadoop.com/m/uyzND1LUUpN1qRojN1 for the discussion thread where the decision was made to name the next release 0.9.0 instead of 0.8.3.;;;","13/Oct/15 18:21;ijuma;Resolving this as it has been fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UncleanLeaderElectionEnableProp in LogConfig should be of boolean,KAFKA-1803,12758793,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,metadave,junrao,junrao,02/Dec/14 00:36,03/Dec/14 01:54,14/Jul/23 05:39,03/Dec/14 01:54,,,,0.9.0.0,,,,,,,core,,,0,newbie,,,"Now that KAFKA-1798 is fixed, we should define UncleanLeaderElectionEnableProp as a boolean, instead of String in LogConfig and get rid of the customized validation for boolean.",,junrao,metadave,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/14 01:28;metadave;KAFKA1803.2.patch;https://issues.apache.org/jira/secure/attachment/12684769/KAFKA1803.2.patch","02/Dec/14 02:49;metadave;KAFKA1803.patch;https://issues.apache.org/jira/secure/attachment/12684537/KAFKA1803.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 03 01:54:16 UTC 2014,,,,,,,,,,"0|i22y4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Dec/14 02:49;metadave;patch submitted!;;;","03/Dec/14 00:49;junrao;Thanks for the patch. In the following, we can use the api that doesn't require the validator.

      .define(UncleanLeaderElectionEnableProp, BOOLEAN, Defaults.UncleanLeaderElectionEnable,
        null, MEDIUM, UncleanLeaderElectionEnableDoc)
;;;","03/Dec/14 01:28;metadave;fixed in KAFKA1803.2.patch, thanks!;;;","03/Dec/14 01:54;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaException was not recorded at the per-topic metrics,KAFKA-1800,12757844,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,25/Nov/14 23:46,05/Nov/20 19:12,14/Jul/23 05:39,05/Nov/20 19:12,,,,1.0.0,,,,,,,,,,0,,,,"When KafkaException was thrown from producer.send() call, it is not recorded on the per-topic record-error-rate, but only the global error-rate.

Since users are usually monitoring on the per-topic metrics, loosing all dropped message counts at this level that are caused by kafka producer thrown exceptions such as BufferExhaustedException could be very dangerous.",,boniek,guozhang,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/14 20:21;guozhang;KAFKA-1800.patch;https://issues.apache.org/jira/secure/attachment/12683910/KAFKA-1800.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 05 19:12:07 UTC 2020,,,,,,,,,,"0|i22sdz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Nov/14 20:21;guozhang;Created reviewboard https://reviews.apache.org/r/28479/diff/
 against branch origin/trunk;;;","27/Nov/14 00:23;guozhang;There is still a corner case after this patch that the per-topic metrics cannot be recorded because they are not registered: when KafkaProducer's waitOnMetadata throws a TimeoutException because the topic metadata is not available, this error cannot be recorded at the per-topic metrics because they are only registered at the sender level when the produce requests are being sent (in the patch it is changed to when the it is refreshed).

To solve this issue, one proposal is that:

1. In Metrics.registerMetric() function, when the metric already exists, treat it as a no-op instead of throwing IllegalArgumentException.
2. Expose a registerSenderMetrics() API of sender in kafka producer, which will be triggered before metadata.awaitUpdate(version, remainingWaitMs) in waitForMetadata.

This fix is a little bit hacky though, so I would like to hear opinions from other people? [~jkreps]
;;;","01/Dec/14 17:49;jkreps;Is it important to count this exception at the topic level? Maybe just count it at the global level?;;;","01/Dec/14 18:06;guozhang;It is actually quite important for monitoring topic level error / retry rate for producer thrown exceptions since many users do not monitor global level metrics but only on topics that they are interested in.;;;","04/Dec/14 02:02;guozhang;Copying [~jjkoshy]'s comment on the RB here:

{code}
It would be useful to clarify the comment on why this needed to be moved further up as you explained offline - i.e., since buffer exhaustion (for example) can happen before the sender gets a chance to register the metrics.

Also, we should probably discuss on the jira the additional caveat of failed metadata fetches. i.e., since that happens in the network-client the true record error rate would be higher than what's counted by sendermetrics.

The options that we have are:
* Expose Sender's maybeRegisterTopicMetrics and use that in NetworkClient maybeUpdateMetadata if there are no known partitions for a topic
* Keep it as you have it for now and just accept the above discrepancy - (or we could address that in a separate jira as it is orthogonal).
{code};;;","05/Nov/20 19:12;guozhang;This has been fixed since 1.0.0; closing now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG doesn't work,KAFKA-1799,12757837,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,omkreddy,junrao,junrao,25/Nov/14 23:13,02/Dec/14 00:14,14/Jul/23 05:39,02/Dec/14 00:14,0.8.2.0,,,0.8.2.0,,,,,,,,,,0,newbie++,,,"When running the following test, we got an unknown configuration exception.

    @Test
    public void testMetricsReporter() {
        Properties producerProps = new Properties();
        producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, ""host1:123"");
        producerProps.put(ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG, ""org.apache.kafka.clients.producer.new-metrics-reporter"");
        new KafkaProducer(producerProps);
    }

org.apache.kafka.common.config.ConfigException: Unknown configuration 'org.apache.kafka.clients.producer.new-metrics-reporter'
	at org.apache.kafka.common.config.AbstractConfig.get(AbstractConfig.java:60)
	at org.apache.kafka.common.config.AbstractConfig.getClass(AbstractConfig.java:91)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstances(AbstractConfig.java:147)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:105)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:94)
",,junrao,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/14 10:21;omkreddy;KAFKA-1799.patch;https://issues.apache.org/jira/secure/attachment/12684238/KAFKA-1799.patch","30/Nov/14 10:30;omkreddy;KAFKA-1799_2014-11-30_15:58:32.patch;https://issues.apache.org/jira/secure/attachment/12684289/KAFKA-1799_2014-11-30_15%3A58%3A32.patch","30/Nov/14 10:35;omkreddy;KAFKA-1799_2014-11-30_16:04:16.patch;https://issues.apache.org/jira/secure/attachment/12684290/KAFKA-1799_2014-11-30_16%3A04%3A16.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 02 00:14:16 UTC 2014,,,,,,,,,,"0|i22scf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Nov/14 23:20;junrao;This is because ConfigDef.parseType() assumes that if the input is a list, it's a list of integers. However, in ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG, are expecting a list of classes. So, in AbstractConfig.getConfiguredInstances(), we need to explicitly convert each string item to a class.;;;","25/Nov/14 23:21;junrao;Marking this an 0.8.2 blocker.;;;","29/Nov/14 10:21;omkreddy;Created reviewboard https://reviews.apache.org/r/28536/diff/
 against branch origin/trunk;;;","30/Nov/14 10:30;omkreddy;Updated reviewboard https://reviews.apache.org/r/28536/diff/
 against branch origin/trunk;;;","30/Nov/14 10:35;omkreddy;Updated reviewboard https://reviews.apache.org/r/28536/diff/
 against branch origin/trunk;;;","02/Dec/14 00:14;junrao;Thanks for the patch. +1 and committed to trunk and 0.8.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigDef.parseType() should throw exception on invalid boolean value,KAFKA-1798,12757830,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,dokovan,junrao,junrao,25/Nov/14 22:42,02/Dec/14 00:33,14/Jul/23 05:39,02/Dec/14 00:33,0.8.2.0,,,0.9.0.0,,,,,,,,,,0,newbie,,,"ConfigDef.parseType() currently uses Boolean.parseBoolean(trimmed) to parse boolean value from a String. However, it simply returns false for anything that's not ""true"". It would be better if we throw an exception if the input string is not either ""true"" or ""false"".",,airbots,dokovan,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/14 10:41;dokovan;0001-Changes.patch;https://issues.apache.org/jira/secure/attachment/12684292/0001-Changes.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 02 00:33:18 UTC 2014,,,,,,,,,,"0|i22sav:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Nov/14 10:40;dokovan;Fix + tests;;;","02/Dec/14 00:33;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing slash in documentation for the Zookeeper paths in ZookeeperConsumerConnector,KAFKA-1783,12756233,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,jfim,jfim,jfim,18/Nov/14 21:53,01/Dec/14 18:13,14/Jul/23 05:39,01/Dec/14 18:13,,,,,,,,,,,consumer,,,0,,,,"The documentation for the ZookeeperConsumerConnector refers to the consumer id registry location as /consumers/[group_id]/ids[consumer_id], it should be /consumers/[group_id]/ids/[consumer_id], as evidenced by registerConsumerInZK() and TopicCount.scala line 61.

A patch is provided that adds the missing forwards slash.",,guozhang,jfim,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/14 21:54;jfim;kafka-missing-doc-slash.patch;https://issues.apache.org/jira/secure/attachment/12682252/kafka-missing-doc-slash.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 01 18:10:53 UTC 2014,,,,,,,,,,"0|i22j0v:",9223372036854775807,,guozhang,,,,,,,,,,,,,,,,,,"18/Nov/14 21:54;jfim;Patch that adds the missing slash for the consumer id registry path.;;;","19/Nov/14 20:18;guozhang;Thanks for the findings, LGTM.;;;","26/Nov/14 01:10;nehanarkhede;[~guozhang] Assigning to you for review. Feel free to reassign.;;;","01/Dec/14 18:10;guozhang;Thanks for the patch, committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Junit3 Misusage,KAFKA-1782,12756206,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,apakulov,guozhang,guozhang,18/Nov/14 20:35,15/Aug/15 04:54,14/Jul/23 05:39,15/Aug/15 04:54,,,,0.9.0.0,,,,,,,,,,0,newbie,,,"This is found while I was working on KAFKA-1580: in many of our cases where we explicitly extend from junit3suite (e.g. ProducerFailureHandlingTest), we are actually misusing a bunch of features that only exist in Junit4, such as (expected=classOf). For example, the following code

{code}
import org.scalatest.junit.JUnit3Suite
import org.junit.Test

import java.io.IOException

class MiscTest extends JUnit3Suite {
  @Test (expected = classOf[IOException])
  def testSendOffset() {
  }
}
{code}

will actually pass even though IOException was not thrown since this annotation is not supported in Junit3. Whereas

{code}
import org.junit._

import java.io.IOException

class MiscTest extends JUnit3Suite {
  @Test (expected = classOf[IOException])
  def testSendOffset() {
  }
}
{code}

or

{code}
import org.scalatest.junit.JUnitSuite
import org.junit._

import java.io.IOException

class MiscTest extends JUnit3Suite {
  @Test (expected = classOf[IOException])
  def testSendOffset() {
  }
}
{code}

or

{code}
import org.junit._

import java.io.IOException

class MiscTest {
  @Test (expected = classOf[IOException])
  def testSendOffset() {
  }
}
{code}

will fail.

I would propose to not rely on Junit annotations other than @Test itself but use scala unit test annotations instead, for example:

{code}
import org.junit._

import java.io.IOException

class MiscTest {
  @Test
  def testSendOffset() {
    intercept[IOException] {
      //nothing
    }
  }
}

{code}

will fail with a clearer stacktrace.
",,apakulov,githubbot,guozhang,hachikuji,ijuma,jholoman,jjkoshy,mliesenberg,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-2398,,,,,,,,,,,,,,,,,,,,,"16/Jul/15 18:47;apakulov;KAFKA-1782.patch;https://issues.apache.org/jira/secure/attachment/12745667/KAFKA-1782.patch","18/Jun/15 18:28;apakulov;KAFKA-1782.patch;https://issues.apache.org/jira/secure/attachment/12740464/KAFKA-1782.patch","18/Jun/15 18:53;apakulov;KAFKA-1782_2015-06-18_11:52:49.patch;https://issues.apache.org/jira/secure/attachment/12740471/KAFKA-1782_2015-06-18_11%3A52%3A49.patch","15/Jul/15 23:58;apakulov;KAFKA-1782_2015-07-15_16:57:44.patch;https://issues.apache.org/jira/secure/attachment/12745551/KAFKA-1782_2015-07-15_16%3A57%3A44.patch","16/Jul/15 18:50;apakulov;KAFKA-1782_2015-07-16_11:50:05.patch;https://issues.apache.org/jira/secure/attachment/12745670/KAFKA-1782_2015-07-16_11%3A50%3A05.patch","16/Jul/15 18:56;apakulov;KAFKA-1782_2015-07-16_11:56:11.patch;https://issues.apache.org/jira/secure/attachment/12745672/KAFKA-1782_2015-07-16_11%3A56%3A11.patch",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 15 04:54:41 UTC 2015,,,,,,,,,,"0|i22ivb:",9223372036854775807,,guozhang,,,,,,,,,,,,,,,,,,"18/Nov/14 21:35;guozhang;For more information:

http://www.asjava.com/junit/junit-3-vs-junit-4-comparison/

And

http://www.artima.com/docs-scalatest-2.2.0-RC1/index.html#org.scalatest.junit.package (look for class Junit3Suite and JunitSuite);;;","25/Dec/14 16:27;jholoman;Given the timing, is it best to move the fix version to 0.8.3?;;;","26/Dec/14 05:57;jjkoshy;I think it would be beneficial to at least survey affected unit tests and verify those tests manually - i.e., there may be test failures that are in fact escaping due to this issue.;;;","28/Dec/14 00:26;jholoman;I did a check through the tests looking for things like '(expected' and 'JUnit3Suite'. The good news is I don't think there are any cases that tests are passing where they shouldn't be, and I didn't find any instances against trunk where a test would pass silently due features that aren't implemented in JUnit 3. There is one exception (HighwatermarkPersistenceTest) where the teardown is not being called due to use of the  @After notation. There is also a bit of ""mixing"" where both JUnitSuite and the older junit.framework.Assert (vs. org.junit.Assert) is being used. 

So how would we like to proceed here? It probably makes sense to have a standard set of libraries that are imported in each test. 
Are we ok with using scalatest features like intercept[] rather than @Test(expected..) ?. If we remove all the references to JUnit3Suite there is some cleanup work (mostly in setup/teardown and adding annotations). 
;;;","29/Dec/14 22:00;nehanarkhede;[~guozhang], [~jjkoshy] It looks like we can push this to 0.8.3. Any objections?;;;","21/Jan/15 18:41;guozhang;Jeff,

Sorry for getting late on this.

I would recommend we remove all the references to JUnit3Suite as it is 1) no longer the latest version and 2) is confusing to people for ""expected"" usage. And we will also remove other annotations other than ""@Test"" itself but use scalatest features instead.;;;","21/Jan/15 23:03;jholoman;Thank you for the feedback [~guozhang]. I will get to work on this.;;;","18/Jun/15 18:28;apakulov;Created reviewboard https://reviews.apache.org/r/35615/diff/
 against branch origin/trunk;;;","18/Jun/15 18:53;apakulov;Updated reviewboard https://reviews.apache.org/r/35615/diff/
 against branch upstream/trunk;;;","13/Jul/15 21:03;apakulov;[~guozhang] [~junrao] is this ticket still relevant?;;;","14/Jul/15 01:16;guozhang;[~apakulov] Yes this is still relevant, sorry for being late on the reviews, will take a look at it soon.;;;","15/Jul/15 23:58;apakulov;Updated reviewboard https://reviews.apache.org/r/35615/diff/
 against branch upstream/trunk;;;","16/Jul/15 18:47;apakulov;Created reviewboard https://reviews.apache.org/r/36552/diff/
 against branch origin/trunk;;;","16/Jul/15 18:50;apakulov;Updated reviewboard https://reviews.apache.org/r/35615/diff/
 against branch trunk;;;","16/Jul/15 18:56;apakulov;Updated reviewboard https://reviews.apache.org/r/35615/diff/
 against branch trunk;;;","13/Aug/15 02:57;guozhang;Issue resolved by pull request 135
[https://github.com/apache/kafka/pull/135];;;","13/Aug/15 02:59;guozhang;[~apakulov] Thanks for the patch, [~ewencp] helped fixing another issue and it has not been committed to trunk.

EDIT: not => now;;;","13/Aug/15 07:36;ijuma;Guozhang meant that it has now been committed to trunk.;;;","13/Aug/15 17:53;apakulov;[~ewencp] [~guozhang] thanks for taking care of it.;;;","13/Aug/15 23:21;apakulov;[~guozhang] am I able to use github PR to send changset or kafka-review tool is still a preferred way?;;;","13/Aug/15 23:32;guozhang;[~apakulov] we are currently moving to the PR for contribution / reviews, you can take a look at this wiki (section ""Contributor and Reviewer Workflow""):

https://cwiki.apache.org/confluence/display/KAFKA/Patch+submission+and+review;;;","14/Aug/15 20:13;hachikuji;It looks like this patch broke some of the tests (e.g. ConsumerTest). We probably need to go through and add the Test annotation to all the test cases which were converted.;;;","14/Aug/15 22:44;githubbot;GitHub user ewencp opened a pull request:

    https://github.com/apache/kafka/pull/140

    KAFKA-1782: Follow up - add missing @Test annotations.

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/ewencp/kafka kafka-1782-followup

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/140.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #140
    
----
commit 1dcaf39d489c26b564186fbe8d1bddb987f38e3e
Author: Ewen Cheslack-Postava <me@ewencp.org>
Date:   2015-08-14T22:43:56Z

    KAFKA-1782: Follow up - add missing @Test annotations.

----
;;;","15/Aug/15 04:54;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/140
;;;","15/Aug/15 04:54;guozhang;Issue resolved by pull request 140
[https://github.com/apache/kafka/pull/140];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The description of UnknownTopicOrPartitionException in doc is not accurate.,KAFKA-1770,12755413,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,becket_qin,becket_qin,becket_qin,14/Nov/14 16:51,21/Nov/14 22:29,14/Jul/23 05:39,21/Nov/14 22:29,,,,,,,,,,,,,,0,,,,"It was ""Indicates an unknown topic or a partition id not between 0 and numPartitions-1"", whereas should be
""Indicates one of the following situation: 
1. Partition id is not between 0 - numPartitions-1
2. Partition id for the topic does not exist on the broker (This could happen when partitions are reassigned).""",,becket_qin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/14 16:57;becket_qin;KAFKA-1770.patch;https://issues.apache.org/jira/secure/attachment/12681563/KAFKA-1770.patch","20/Nov/14 00:37;becket_qin;KAFKA-1770_2014-11-19_16:37:11.patch;https://issues.apache.org/jira/secure/attachment/12682539/KAFKA-1770_2014-11-19_16%3A37%3A11.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 20 00:37:17 UTC 2014,,,,,,,,,,"0|i22e2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Nov/14 16:57;becket_qin;Created reviewboard https://reviews.apache.org/r/28040/diff/
 against branch origin/trunk;;;","20/Nov/14 00:37;becket_qin;Updated reviewboard https://reviews.apache.org/r/28040/diff/
 against branch origin/trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ecosystem docs subsection has wrong anchor,KAFKA-1766,12754403,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,qrilka,qrilka,11/Nov/14 08:18,15/Nov/14 01:58,14/Jul/23 05:39,15/Nov/14 01:58,,,,0.8.2.0,,,,,,,,,,0,,,,"the following portion of html at http://kafka.apache.org/documentation.html seems to be wrong:

<h3><a id=""upgrade"">1.4 Ecosystem</a></h3>

it should be

<h3><a id=""ecosystem"">1.4 Ecosystem</a></h3>

Why don't you have Kafka docs in github also? If you had it would be trivial to create a PR to fix this issue",,junrao,qrilka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Nov 15 01:58:13 UTC 2014,,,,,,,,,,"0|i2282f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Nov/14 01:58;junrao;Thanks for pointing this out. Fix the site.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
slf4j binding conflict in slf4j-log4j12 and kafka-assembly,KAFKA-1765,12754396,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,tomWang,tomWang,11/Nov/14 07:20,07/Feb/15 22:29,14/Jul/23 05:39,07/Feb/15 22:29,0.8.0,,,,,,,,,,log,,,0,,,,"Before our project use slf4j which binded to log4j for logging. But after import 
kafka-assembly.0.8.0.jar, log cannot work as expected. It just keep printing log in console now instead of log files. Looked into kafka-assembly.0.8.0.jar and find there is one SimpleLogger:

 StringBuffer buf = new StringBuffer();
        long millis = System.currentTimeMillis();
        buf.append(millis - startTime);
        buf.append("" ["");
        buf.append(Thread.currentThread().getName());
        buf.append(""] "");
        buf.append(level);
        buf.append("" "");
        buf.append(name);
        buf.append("" - "");
        buf.append(message);
        buf.append(LINE_SEPARATOR);
        System.err.print(buf.toString());
        if(t != null)
            t.printStackTrace(System.err);
        System.err.flush();

but I don't want this SimpleLogger in kafka-assembly. Can you advise how can I get rid of this reference to this class(cannot remove the jar, because is necessary)",slf4j-log4j12 kafka-assembly,junrao,tomWang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 11 17:57:13 UTC 2014,,,,,,,,,,"0|i2280v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/14 17:57;junrao;Yes, SimpleLogger was dragged in incorrectly in 0.8.0. You would have to explicitly remove that jar dependency if you want to use slf4j-log4j12. This issue has since been fixed in 0.8.1.x and after.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZookeeperConsumerConnector could put multiple shutdownCommand to the same data chunk queue.,KAFKA-1764,12754355,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,becket_qin,becket_qin,becket_qin,11/Nov/14 03:25,14/Nov/14 19:08,14/Jul/23 05:39,14/Nov/14 18:15,,,,0.8.2.0,,,,,,,,,,0,,,,"In ZookeeperConsumerConnector shutdown(), we could potentially put multiple shutdownCommand into the same data chunk queue, provided the topics are sharing the same data chunk queue in topicThreadIdAndQueues.

From email thread to document:

In ZookeeperConsumerConnector shutdown(), we could potentially put
multiple shutdownCommand into the same data chunk queue, provided the
topics are sharing the same data chunk queue in topicThreadIdAndQueues.

In our case, we only have 1 consumer stream for all the topics, the data
chunk queue capacity is set to 1. The execution sequence causing problem is
as below:
1. ZookeeperConsumerConnector shutdown() is called, it tries to put
shutdownCommand for each queue in topicThreadIdAndQueues. Since we only
have 1 queue, multiple shutdownCommand will be put into the queue.
2. In sendShutdownToAllQueues(), between queue.clean() and
queue.put(shutdownCommand), consumer iterator receives the shutdownCommand
and put it back into the data chunk queue. After that,
ZookeeperConsumerConnector tries to put another shutdownCommand into the
data chunk queue but will block forever.

The thread stack trace is as below:
{code}
""Thread-23"" #58 prio=5 os_prio=0 tid=0x00007ff440004800 nid=0x40a waiting
on condition [0x00007ff4f0124000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x0000000680b96bf0> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at
java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:350)
        at
kafka.consumer.ZookeeperConsumerConnector$$anonfun$sendShutdownToAllQueues$1.apply(ZookeeperConsumerConnector.scala:262)
        at
kafka.consumer.ZookeeperConsumerConnector$$anonfun$sendShutdownToAllQueues$1.apply(ZookeeperConsumerConnector.scala:259)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at
scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at
kafka.consumer.ZookeeperConsumerConnector.sendShutdownToAllQueues(ZookeeperConsumerConnector.scala:259)
        at
kafka.consumer.ZookeeperConsumerConnector.liftedTree1$1(ZookeeperConsumerConnector.scala:199)
        at
kafka.consumer.ZookeeperConsumerConnector.shutdown(ZookeeperConsumerConnector.scala:192)
        - locked <0x0000000680dd5848> (a java.lang.Object)
        at
kafka.tools.MirrorMaker$$anonfun$cleanShutdown$1.apply(MirrorMaker.scala:185)
        at
kafka.tools.MirrorMaker$$anonfun$cleanShutdown$1.apply(MirrorMaker.scala:185)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at kafka.tools.MirrorMaker$.cleanShutdown(MirrorMaker.scala:185)
{code}",,becket_qin,copester,jjkoshy,junrao,sslavic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/14 22:59;becket_qin;KAFKA-1764.patch;https://issues.apache.org/jira/secure/attachment/12680932/KAFKA-1764.patch","12/Nov/14 22:05;becket_qin;KAFKA-1764_2014-11-12_14:05:35.patch;https://issues.apache.org/jira/secure/attachment/12681152/KAFKA-1764_2014-11-12_14%3A05%3A35.patch","14/Nov/14 07:57;becket_qin;KAFKA-1764_2014-11-13_23:57:51.patch;https://issues.apache.org/jira/secure/attachment/12681506/KAFKA-1764_2014-11-13_23%3A57%3A51.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 14 18:14:57 UTC 2014,,,,,,,,,,"0|i227rr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Nov/14 22:59;becket_qin;Created reviewboard https://reviews.apache.org/r/27890/diff/
 against branch origin/trunk;;;","12/Nov/14 22:05;becket_qin;Updated reviewboard https://reviews.apache.org/r/27890/diff/
 against branch origin/trunk;;;","14/Nov/14 02:14;jjkoshy;Committed to trunk.

[~junrao] do you think this should be in 0.8.2 as well?;;;","14/Nov/14 02:25;junrao;[~jjkoshy], since the patch is small, I think we can include it in 0.8.2.;;;","14/Nov/14 03:24;copester;Why do builds always break just as I put the kids to sleep and grab a glass of wine?;;;","14/Nov/14 03:26;copester;{code}
/home/bamboo/bamboo-agent-home/xml-data/build-dir/STREAM-KAFKA-JOB1/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala:259: missing parameter type
    for (queue <- topicThreadIdAndQueues.values.toSet) {
         ^
one error found
{code};;;","14/Nov/14 05:00;jjkoshy;That was my bad - I should have double-checked that before check-in. [~copester] thanks for pointing it out. I can't dig into it now so I reverted the checkin. [~becket_qin] please take a look;;;","14/Nov/14 05:16;becket_qin;My bad... I tested the toSet on scala commandLine and assumed it would work. I also realized that there are some unit tests that need change as well. I'll submit a new patch.;;;","14/Nov/14 07:57;becket_qin;Updated reviewboard https://reviews.apache.org/r/27890/diff/
 against branch origin/trunk;;;","14/Nov/14 08:42;sslavic;Is this issue duplicate of KAFKA-1716 ?;;;","14/Nov/14 14:00;copester;This now builds and all 547 tests pass. Thanks!;;;","14/Nov/14 18:14;jjkoshy;Thanks for the fix. Committed to trunk and 0.8.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update max-inflight-request doc string,KAFKA-1762,12754234,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,10/Nov/14 19:51,14/Nov/14 19:09,14/Jul/23 05:39,13/Nov/14 02:31,,,,0.9.0.0,,,,,,,,,,0,,,,"The new Producer client introduces a config for the max # of inFlight messages. When it is set > 1 on MirrorMaker, however, there is a risk for data loss even with KAFKA-1650 because the offsets recorded in the MM's offset map is no longer continuous.

Another issue is that when this value is set > 1, there is a risk of message re-ordering in the producer

Changes:
    1. Set max # of inFlight messages = 1 in MM
    2. Leave comments explaining what the risks are of changing",,guozhang,jjkoshy,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/14 22:53;guozhang;KAFKA-1762.patch;https://issues.apache.org/jira/secure/attachment/12680677/KAFKA-1762.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 12 23:56:28 UTC 2014,,,,,,,,,,"0|i2273r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/14 19:54;jkreps;Can you elaborate on how this happens? Also are you proposing changing the default in the producer or just for mirror maker?;;;","10/Nov/14 20:06;guozhang;Updated the title / description.;;;","10/Nov/14 22:53;guozhang;Created reviewboard https://reviews.apache.org/r/27834/diff/
 against branch origin/trunk;;;","12/Nov/14 23:56;jjkoshy;Committed the doc change to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
num.partitions  documented default is 1 while actual default is 2,KAFKA-1761,12754151,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,omkreddy,sslavic,sslavic,10/Nov/14 14:38,19/Jan/15 16:45,14/Jul/23 05:39,19/Jan/15 16:45,0.8.1.1,,,0.8.2.0,,,,,,,log,,,0,,,,"Default {{num.partitions}} documented in http://kafka.apache.org/08/configuration.html is 1, while server configuration defaults same parameter to 2 (see https://github.com/apache/kafka/blob/0.8.1/config/server.properties#L63 )

Please have this inconsistency fixed.",,joestein,junrao,omkreddy,sslavic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/15 06:19;omkreddy;KAFKA-1761.patch;https://issues.apache.org/jira/secure/attachment/12693017/KAFKA-1761.patch","19/Jan/15 06:24;omkreddy;KAFKA-1761_2015-01-19_11:51:58.patch;https://issues.apache.org/jira/secure/attachment/12693019/KAFKA-1761_2015-01-19_11%3A51%3A58.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 19 16:45:30 UTC 2015,,,,,,,,,,"0|i226lr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Nov/14 14:51;joestein;[~sslavic] There are a handful of cases (e.g. https://github.com/apache/kafka/blob/0.8.1/core/src/main/scala/kafka/server/KafkaConfig.scala#L69 which match docs but are overridden to 9092 in property (and we should keep as 9092 since that is what everyone uses now imho)) of this where the properties files are changing what the actual default in the code (when no property is set) has. Here the default partition is actually 1 (like the docs say) https://github.com/apache/kafka/blob/0.8.1/core/src/main/scala/kafka/server/KafkaConfig.scala#L98 but the property file has it set it to 2 and we document 1 (because that is what the code is). I think all 3 should be consistent and we should try to clean this up in all cases (maybe use this ticket to-do so?). If we want 2 to be the default then lets change it in the code (though I think 1 as the default for setting might ease some FAQ related questions) and use the property file as what we think/feel people should be changing during normal operations matching 1:1 with the code defaults.;;;","10/Nov/14 20:07;junrao;It seems that server.properties is already back to 1 for num.partitions in 0.8.2.;;;","10/Nov/14 20:27;sslavic;Changed in [commit|https://github.com/apache/kafka/commit/b428d8cc48237099af648de12d18be78d54446eb#diff-fe795615cd3ca9a55e864ad330f3344c] for KAFKA-1531;;;","19/Jan/15 06:19;omkreddy;Created reviewboard https://reviews.apache.org/r/30022/diff/
 against branch origin/trunk;;;","19/Jan/15 06:24;omkreddy;Updated reviewboard https://reviews.apache.org/r/30022/diff/
 against branch origin/trunk;;;","19/Jan/15 06:32;omkreddy;1. we need to update the default values of the following config properties in 0.8.2 docs.

||Config property||Default value in code|| Default value in docs||
|background.threads | 10 | 4 |
|controller.message.queue.size| Int.MaxValue | 10 |
|fetch.purgatory.purge.interval.requests | 1000 | 10000 |
|producer.purgatory.purge.interval.requests| 1000 | 10000 |
|offset.metadata.max.bytes | 4096 | 1024 |
|log.cleaner.io.max.bytes.per.second|Double.MaxValue|None|
|log.flush.interval.messages|Long.MaxValue|None|
|log.flush.scheduler.interval.ms|Long.MaxValue|3000|
|log.flush.interval.ms|Long.MaxValue|3000|
|queued.max.message.chunks|2|10|

2. Set the default port to 9092 in code (As suggested by [~joestein])

3. The following needs to be corrected in server.properties

||Config property||Default value in code|| Default value in conf||
|zookeeper.connection.timeout.ms|	6000 |2000|
|socket.receive.buffer.bytes|102400| 65536|

Point 1 can be done part of KAFKA-1728. Uploaded a simple patch for Point 2 and 3 
;;;","19/Jan/15 16:45;junrao;Thanks for the patch. Committed to 0.8.2 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
corrupt recovery file prevents startup,KAFKA-1758,12753616,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,omkreddy,jbrosenberg@gmail.com,jbrosenberg@gmail.com,07/Nov/14 04:53,03/Sep/15 00:04,14/Jul/23 05:39,19/Jun/15 01:51,,,,0.9.0.0,,,,,,,log,,,2,newbie,,,"Hi,

We recently had a kafka node go down suddenly. When it came back up, it apparently had a corrupt recovery file, and refused to startup:

{code}
2014-11-06 08:17:19,299  WARN [main] server.KafkaServer - Error starting up KafkaServer
java.lang.NumberFormatException: For input string: ""^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@
^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Integer.parseInt(Integer.java:481)
        at java.lang.Integer.parseInt(Integer.java:527)
        at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)
        at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
        at kafka.server.OffsetCheckpoint.read(OffsetCheckpoint.scala:76)
        at kafka.log.LogManager$$anonfun$loadLogs$1.apply(LogManager.scala:106)
        at kafka.log.LogManager$$anonfun$loadLogs$1.apply(LogManager.scala:105)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
        at kafka.log.LogManager.loadLogs(LogManager.scala:105)
        at kafka.log.LogManager.<init>(LogManager.scala:57)
        at kafka.server.KafkaServer.createLogManager(KafkaServer.scala:275)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:72)
{code}

And the app is under a monitor (so it was repeatedly restarting and failing with this error for several minutes before we got to it)…

We moved the ‘recovery-point-offset-checkpoint’ file out of the way, and it then restarted cleanly (but of course re-synced all it’s data from replicas, so we had no data loss).

Anyway, I’m wondering if that’s the expected behavior? Or should it not declare it corrupt and then proceed automatically to an unclean restart?

Should this NumberFormatException be handled a bit more gracefully?

We saved the corrupt file if it’s worth inspecting (although I doubt it will be useful!)….

The corrupt files appeared to be all zeroes.",,jbrosenberg@gmail.com,jkreps,junrao,nehanarkhede,omkreddy,otis,panih2o,wangbo23,zhiwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/15 18:37;omkreddy;KAFKA-1758.patch;https://issues.apache.org/jira/secure/attachment/12697534/KAFKA-1758.patch","09/May/15 07:02;omkreddy;KAFKA-1758_2015-05-09_12:29:20.patch;https://issues.apache.org/jira/secure/attachment/12731688/KAFKA-1758_2015-05-09_12%3A29%3A20.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 19 01:51:19 UTC 2015,,,,,,,,,,"0|i223d3:",9223372036854775807,,nehanarkhede,,,,,,,,,,,,,,,,,,"07/Feb/15 23:12;jkreps;This is actually not a very difficult change--in LogManager.loadLogs we would need to basically handle an error in reading the recovery checkpoint, log it, and then just treat it as though our recovery point was 0 (or something like that) for all logs.;;;","09/Feb/15 18:37;omkreddy;Created reviewboard https://reviews.apache.org/r/30801/diff/
 against branch origin/trunk;;;","09/Feb/15 18:43;omkreddy;Attaching a patch which handles NumberFormatException while reading   recovery checkpoint file. We still fail for other IOExceptions. On NumberFormatException we will set the last recovery point to zero.;;;","18/Apr/15 11:39;omkreddy;[~jkreps] Can I get review for this simple patch?;;;","26/Apr/15 18:54;nehanarkhede;[~omkreddy] I took a quick look and left a few review comments. Should be able to merge once you fix those.;;;","09/May/15 07:02;omkreddy;Updated reviewboard https://reviews.apache.org/r/30801/diff/
 against branch origin/trunk;;;","19/Jun/15 01:51;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not delete Topic index on Windows,KAFKA-1757,12753416,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,sriharsha,Lucky-V,Lucky-V,06/Nov/14 15:15,24/Feb/15 04:43,14/Jul/23 05:39,24/Feb/15 04:43,0.8.2.0,,,0.9.0.0,,,,,,,log,,,0,,,,"When running the Kafka 0.8.2-Beta (Scala 2.10) on Windows, an attempt to delete the Topic throwed an error:

ERROR [KafkaApi-1] error when handling request Name: StopReplicaRequest; Version: 0; CorrelationId: 38; ClientId: ; DeletePartitions: true; ControllerId: 0; ControllerEpoch: 3; Partitions: [test,0] (kafka.server.KafkaApis)
kafka.common.KafkaStorageException: Delete of index 00000000000000000000.index failed.
        at kafka.log.LogSegment.delete(LogSegment.scala:283)
        at kafka.log.Log$$anonfun$delete$1.apply(Log.scala:608)
        at kafka.log.Log$$anonfun$delete$1.apply(Log.scala:608)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
        at kafka.log.Log.delete(Log.scala:608)
        at kafka.log.LogManager.deleteLog(LogManager.scala:375)
        at kafka.cluster.Partition$$anonfun$delete$1.apply$mcV$sp(Partition.scala:144)
        at kafka.cluster.Partition$$anonfun$delete$1.apply(Partition.scala:139)
        at kafka.cluster.Partition$$anonfun$delete$1.apply(Partition.scala:139)
        at kafka.utils.Utils$.inLock(Utils.scala:535)
        at kafka.utils.Utils$.inWriteLock(Utils.scala:543)
        at kafka.cluster.Partition.delete(Partition.scala:139)
        at kafka.server.ReplicaManager.stopReplica(ReplicaManager.scala:158)
        at kafka.server.ReplicaManager$$anonfun$stopReplicas$3.apply(ReplicaManager.scala:191)
        at kafka.server.ReplicaManager$$anonfun$stopReplicas$3.apply(ReplicaManager.scala:190)
        at scala.collection.immutable.Set$Set1.foreach(Set.scala:74)
        at kafka.server.ReplicaManager.stopReplicas(ReplicaManager.scala:190)
        at kafka.server.KafkaApis.handleStopReplicaRequest(KafkaApis.scala:96)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:59)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:59)
        at java.lang.Thread.run(Thread.java:744)



When I have investigated the issue I figured out that the index file (in my environment it was C:\tmp\kafka-logs\00000000-0000-0000-0000-000000000014-0\00000000000000000000.index) was locked by the kafka process and the OS did not allow to delete that file.

I tried to fix the problem in source codes and when I added close() method call into LogSegment.delete(), the Topic deletion started to work.

I will add here (not sure how to upload the file during issue creation) a diff with the changes I have made so You can take a look on that whether it is reasonable or not. It would be perfect if it could make it into the product...

In the end I would like to say that on Linux the deletion works just fine...",,junrao,Lucky-V,nehanarkhede,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/15 20:16;sriharsha;KAFKA-1757.patch;https://issues.apache.org/jira/secure/attachment/12691515/KAFKA-1757.patch","06/Nov/14 15:17;Lucky-V;lucky-v.patch;https://issues.apache.org/jira/secure/attachment/12679855/lucky-v.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 24 04:43:08 UTC 2015,,,,,,,,,,"0|i2227b:",9223372036854775807,,jkreps,,,,,,,,,,,,,,,,,,"06/Nov/14 15:17;Lucky-V;A patch that looks like a fix for the issue.;;;","15/Nov/14 02:22;junrao;Thanks for the patch. It's not clear to me if this really fixes the problem. OffsetIndex.close() doesn't really close any channels or handlers. It simply remaps the memory mapped file.;;;","29/Dec/14 22:23;nehanarkhede;I'm leaning towards pushing this out of 0.8.2. Let me know if anyone has concerns with this.;;;","10/Jan/15 20:16;sriharsha;Created reviewboard https://reviews.apache.org/r/29794/diff/
 against branch origin/trunk;;;","10/Jan/15 20:18;sriharsha;[~junrao] On windows MappedByteBuffer needs to be unmapped before deleting the file. The above patch tested on windows 8. ;;;","11/Feb/15 16:21;sriharsha;[~junrao] Can you please review the patch. Thanks.;;;","24/Feb/15 04:43;sriharsha;patch merged by [~jkreps] closing this as fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve error handling in log cleaner,KAFKA-1755,12753080,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jjkoshy,jjkoshy,jjkoshy,05/Nov/14 16:15,21/Dec/15 19:22,14/Jul/23 05:39,21/Dec/15 19:22,,,,0.9.0.0,,,,,,,,,,0,newbie++,,,"The log cleaner is a critical process when using compacted topics.
However, if there is any error in any topic (notably if a key is missing) then the cleaner exits and all other compacted topics will also be adversely affected - i.e., compaction stops across the board.

This can be improved by just aborting compaction for a topic on any error and keep the thread from exiting.

Another improvement would be to reject messages without keys that are sent to compacted topics although this is not enough by itself.
",,criccomini,guozhang,gwenshap,jjkoshy,jkreps,jonbringhurst,vanyatka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1581,,,,,,,,,,"23/Feb/15 14:43;jjkoshy;KAFKA-1755.patch;https://issues.apache.org/jira/secure/attachment/12700201/KAFKA-1755.patch","23/Feb/15 22:30;jjkoshy;KAFKA-1755_2015-02-23_14:29:54.patch;https://issues.apache.org/jira/secure/attachment/12700276/KAFKA-1755_2015-02-23_14%3A29%3A54.patch","26/Feb/15 18:54;jjkoshy;KAFKA-1755_2015-02-26_10:54:50.patch;https://issues.apache.org/jira/secure/attachment/12701143/KAFKA-1755_2015-02-26_10%3A54%3A50.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 21 19:22:47 UTC 2015,,,,,,,,,,"0|i22067:",9223372036854775807,,jkreps,,,,,,,,,,,,,,,,,,"05/Nov/14 16:20;criccomini;It might also be desirable to allow the log compaction to continue on the topic in question, and simply keep all messages without keys without doing any compaction on them.;;;","18/Nov/14 17:42;jjkoshy;There are a couple of issues that I was thinking of in scope for this jira:
* Log cleaner threads quitting on errors (which may be a non-issue as discussed further below).
* Dealing with cleaner failures due to unkeyed messages.
* Other cleaner failures are possible as well (for e.g., compressed message sets until KAFKA-1374 is reviewed and checked-in)

The reason this jira was filed is because the log cleaner compacts all compacted topics so one topic should (ideally) not affect another. Any practical deployment would need to set up alerts on the cleaner thread dying. Right now, I think the most reliable way to alert (with the currently available metrics) would be to monitor the max-dirty-ratio. If we set up this alert, then allowing the cleaner to continue would in practice only delay an alert. So one can argue that it is better to fail fast - i.e., let the log cleaner die because a problematic topic is something that needs to be looked into immediately. However, I think there are further improvements with alternatives that can be made. It would be helpful if others can share their thoughts/preferences on these:
* Introduce a new LogCleaningState: LogCleaningPausedDueToError
* Introduce a metric for the number of live cleaner threads
* If the log cleaner encounters any uncaught error, there are a couple of options:
** Don't let the thread die, but move the partition to LogCleaningPausedDueToError. Other topics-partitions can still be compacted. Alerts can be set up on the number of partitions in state LogCleaningPausedDueToError.
** Let the cleaner die and decrement live cleaner count. Alerts can be set up on the number of live cleaner threads.
* If the cleaner encounters un-keyed messages:
** Delete those messages, and do nothing. i.e., ignore (or just log the count in log cleaner stats)
** Keep the messages, move the partition to LogCleaningPausedDueToError.  The motivation for this is accidental misconfiguration. i.e., it may be important to not lose those messages. The error log cleaning state can be cleared only by deleting and then recreating the topic.
* Additionally, I think we should reject producer requests containing un-keyed messages to compacted topics.
* With all of the above, a backup alert can also be set up on the max-dirty-ratio.;;;","04/Dec/14 23:23;guozhang;Here are my two cents:

1. At the end of the day, Kafka will have two types of topics, one type only accepts keyed messages and log compaction is used; the other one accepts any message and log cleaning is used. Those two types of topics never exchange, i.e. once a topic is created with one of the two types, it will never change its type until deletion.

2. Compressed message will be supported with log compaction, which will de-serialize the message set and re-serialize.

3. With these two points in mind, I would suggest for now:
  a. Broker reject non-keyed messages for compacted topics.
  b. Broker reject compressed messages for compacted topics (this will be lifted after KAFKA-1374 is checked in).
  c. With this, it should never happen that compactor thread encountering a non-keyed / compressed (this will be lifted after KAFKA-1374); if it happens, this would be a FATAL error and we should throw an exception and halt the server. It indicates some operations are needed and there are some code fixes before it can be restarted.;;;","23/Dec/14 11:41;jjkoshy;Thanks for the comments. The issue with 3a is that once we do have compression support for compacted topics it will be very ugly to implement that check on message arrival. This is because we need to do a deep iteration on incoming messages to look at the key field. The only time we do that currently on the broker is when assigning offsets. However, this code is in ByteBufferMessageSet which is fairly low-level and has no log-config information associated with it. We would have to ""leak"" some flag indicating whether non-keyed messages are allowed or not which is ugly. That is why I prefer not doing that check on message arrival and just have the log cleaner drop/ignore non-keyed messages with a warning. Ultimately, the effect is the same. However, the benefit of rejecting is that the producer is made aware of it. So I guess I changed my mind with regard to my earlier comment - i.e., I would recommend against doing this unless we can think of an elegant implementation. 3b is easy to do and we can implement that until KAFKA-1374 is in place.;;;","23/Dec/14 17:52;jkreps;Rejecting messages without a key doesn't actually solve the problem, I think as you can change the retention setting of a topic to compaction later at which point there may already be null keys.

Perhaps the most consistent thing to do would actually be to treat null as a key value. So the cleaner would retain a single null value and remove the others. ;;;","23/Feb/15 14:43;jjkoshy;Created reviewboard https://reviews.apache.org/r/31306/
 against branch origin/trunk;;;","23/Feb/15 15:23;jjkoshy;I thought a bit more about this and here is a patch that summarizes my thoughts.

This patch does message validation on arrival, and drops unkeyed messages during log compaction.

I actually think it is better to reject invalid messages (unkeyed and for now compressed) up front as opposed to accepting those messages and only dropping/warning during compaction. This way the producer is given early indication via a client-side error that it is doing something wrong which is better than just a broker-side warning/invalid metric. We still need to deal with unkeyed messages that may already be in the log but that is orthogonal I think - this includes the case when you change a non-compacted topic to be compacted. That  is perhaps an invalid operation - i.e., you should ideally delete the topic before doing that, but in any event this patch handles that case by deleting invalid messages during log compaction.

Case in point: at LinkedIn we use Kafka-based offset management for some of our consumers. We recently discovered compressed messages in the offsets topic which caused the log cleaner to quit. We saw this issue in the past with Samza checkpoint topics and suspected that  Samza was doing something wrong. However, after seeing it in the __consumer_offsets topic it is more likely to be an actual bug in the broker - either in the log cleaner itself, or even at the lower level byte-buffer message set API level. We currently do not know. If we at least reject invalid messages on arrival we can rule out clients as being the issue.;;;","23/Feb/15 15:25;jjkoshy;Also, I have an incremental patch that prevents the log cleaner from quitting due to uncaught errors while cleaning a specific partition. It basically moves that partition to a permanent failed state and allows the cleaner to continue compacting other partitions. It      continues to include the failed partition when computing the max dirty ratio so you can still accurately alert on that metric. We can discuss whether we want to add that or not.;;;","23/Feb/15 22:30;jjkoshy;Updated reviewboard https://reviews.apache.org/r/31306/diff/
 against branch origin/trunk;;;","26/Feb/15 18:54;jjkoshy;Updated reviewboard https://reviews.apache.org/r/31306/diff/
 against branch origin/trunk;;;","21/Dec/15 19:22;gwenshap;This was in fact committed to trunk and is in 0.9.0.0:

commit 1cd6ed9e2c07a63474ed80a8224bd431d5d4243c  Joel Koshy committed on Mar 3
https://github.com/apache/kafka/commit/1cd6ed9e2c07a63474ed80a8224bd431d5d4243c#diff-d7330411812d23e8a34889bee42fedfe
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestcaseEnv improperly shares state between instances,KAFKA-1747,12752504,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ewencp,ewencp,ewencp,03/Nov/14 19:45,07/Nov/14 02:15,14/Jul/23 05:39,07/Nov/14 02:13,0.8.1.1,,,0.9.0.0,,,,,,,system tests,,,0,,,,"TestcaseEnv in system tests uses class variables instead of instance variables for a bunch of state. This causes the data to persist between tests. In some cases this can cause tests to break (e.g. there will be state from a service running in a previous test that doesn't exist in the current test; trying to look up state about that service raises an exception or produces invalid data).",,ewencp,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/14 19:46;ewencp;KAFKA-1747.patch;https://issues.apache.org/jira/secure/attachment/12679008/KAFKA-1747.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 07 02:13:59 UTC 2014,,,,,,,,,,"0|i21wo7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/14 19:46;ewencp;Created reviewboard https://reviews.apache.org/r/27535/diff/
 against branch origin/trunk;;;","07/Nov/14 02:13;nehanarkhede;Thanks for the patch, Ewen! Pushed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System tests don't handle errors well,KAFKA-1746,12752503,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ewencp,ewencp,ewencp,03/Nov/14 19:43,07/Nov/14 02:16,14/Jul/23 05:39,07/Nov/14 02:16,0.8.1.1,,,0.9.0.0,,,,,,,system tests,,,0,,,,"The system test scripts don't handle errors well. A couple of key issues:

* Unexpected exceptions during tests are just ignored and the tests appear to be successful in the reports.
* The scripts exit code is always 0, even if tests fail.
* Almost no subprocess calls are checked. In a lot of cases this is ok, and sometimes it's not possible (e.g. after starting a long-running remote process), but in some cases such as calls to DumpLogSegments, the tests can miss that the tools is exiting with an exception and the test appears to be successful even though no data was verified.
",,ewencp,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/14 19:46;ewencp;KAFKA-1746.patch;https://issues.apache.org/jira/secure/attachment/12679007/KAFKA-1746.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 07 02:16:33 UTC 2014,,,,,,,,,,"0|i21wnz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Nov/14 19:46;ewencp;Created reviewboard https://reviews.apache.org/r/27534/diff/
 against branch origin/trunk;;;","07/Nov/14 02:16;nehanarkhede;Thanks for the patch! Pushed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fetch Response contains messages prior to the requested offset,KAFKA-1744,12752178,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,eapache,eapache,01/Nov/14 12:47,07/Nov/14 02:01,14/Jul/23 05:39,07/Nov/14 02:01,0.8.1.1,,,,,,,,,,core,,,0,,,,"As reported in https://github.com/Shopify/sarama/issues/166 there are cases where a FetchRequest for a particular offset returns some messages prior to that offset.

The spec does not seem to indicate that this is possible; it does state that ""As an optimization the server is allowed to return a partial message at the end of the message set."" but otherwise implies that a request for offset X will only return complete messages starting at X. 

The scala consumer does seem to handle this case gracefully though, if I am reading it correctly (my scala is not the best): https://github.com/apache/kafka/blob/0.8.1/core/src/main/scala/kafka/consumer/ConsumerIterator.scala#L96-L99

So is this a bug or just a case that needs to be added to the spec? Something like ""As an optimization the server is allowed to return some messages in the message set prior to the requested offset. Clients should handle this case.""? Although I can't imagine why sending extra data would be faster than only sending the necessary messages...",,eapache,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 07 02:00:26 UTC 2014,,,,,,,,,,"0|i21upj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/14 01:21;nehanarkhede;[~eapache] I'm assuming that you are referring to a non-java consumer right? We would always want to do this in the java consumer, so it's worth fixing the docs. Can you point me to the spec where you found this?;;;","07/Nov/14 01:28;eapache;[~nehanarkhede] this was discovered in the golang consumer I maintain - the scala consumer (as I linked) seems to handle this case already. I have not checked the java consumer.

The [spec for the fetch API|https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-FetchAPI] implies (though it does not explicitly state) that if I perform a fetch request for offset X, the fetch response will contain messages whose offset is strictly >= X. If this is not true (in practice I have seen messages with offsets < X) I would suggest explicitly noting this in the spec to avoid confusion.

Alternatively it may be a real bug in the broker, in which case the spec is fine and the broker should be fixed. I don't have enough information to say for sure.;;;","07/Nov/14 01:41;nehanarkhede;The broker sends data to the consumer using zero-copy, so it cannot filter the extra messages out. The spec already says Clients should handle this case. Should we close this JIRA?;;;","07/Nov/14 01:59;junrao;Evan,

I added the following explanation to the wiki.

""In general, the return messages will have offsets larger than or equal to the starting offset. However, with compressed messages, it's possible for the returned messages to have offsets smaller than the starting offset. The number of such messages is typically small and the caller is responsible for filter out those messages."";;;","07/Nov/14 02:00;eapache;[~junrao] thanks for the clarification, that's what I was looking for, this ticket can be closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsumerConnector.commitOffsets in 0.8.2 is not backward compatible,KAFKA-1743,12752113,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,omkreddy,junrao,junrao,31/Oct/14 23:41,19/Nov/14 03:04,14/Jul/23 05:39,19/Nov/14 03:04,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"In 0.8.1.x, ConsumerConnector has the following api:
  def commitOffsets

This is changed to the following in 0.8.2 and breaks compatibility

  def commitOffsets(retryOnFailure: Boolean = true)",,junrao,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/14 17:13;omkreddy;KAFKA-1743.patch;https://issues.apache.org/jira/secure/attachment/12679883/KAFKA-1743.patch","08/Nov/14 06:20;omkreddy;KAFKA-1743_2014-11-08_11:49:31.patch;https://issues.apache.org/jira/secure/attachment/12680368/KAFKA-1743_2014-11-08_11%3A49%3A31.patch","14/Nov/14 17:00;omkreddy;KAFKA-1743_2014-11-14_22:29:21.patch;https://issues.apache.org/jira/secure/attachment/12681567/KAFKA-1743_2014-11-14_22%3A29%3A21.patch","16/Nov/14 06:42;omkreddy;KAFKA-1743_2014-11-16_12:11:51.patch;https://issues.apache.org/jira/secure/attachment/12681779/KAFKA-1743_2014-11-16_12%3A11%3A51.patch","18/Nov/14 05:30;omkreddy;KAFKA-1743_2014-11-18_10:59:05.patch;https://issues.apache.org/jira/secure/attachment/12682093/KAFKA-1743_2014-11-18_10%3A59%3A05.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 19 03:04:25 UTC 2014,,,,,,,,,,"0|i21ub3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Nov/14 17:13;omkreddy;Created reviewboard https://reviews.apache.org/r/27684/diff/
 against branch origin/0.8.2;;;","08/Nov/14 06:20;omkreddy;Updated reviewboard https://reviews.apache.org/r/27684/diff/
 against branch origin/0.8.2;;;","14/Nov/14 17:00;omkreddy;Updated reviewboard https://reviews.apache.org/r/27684/diff/
 against branch origin/0.8.2;;;","16/Nov/14 06:42;omkreddy;Updated reviewboard https://reviews.apache.org/r/27684/diff/
 against branch origin/0.8.2;;;","18/Nov/14 05:30;omkreddy;Updated reviewboard https://reviews.apache.org/r/27684/diff/
 against branch origin/0.8.2;;;","19/Nov/14 03:04;junrao;Thanks for the patch. +1. Committed to trunk and 0.8.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ControllerContext removeTopic does not correctly update state,KAFKA-1742,12752061,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,onurkaraman,onurkaraman,onurkaraman,31/Oct/14 20:13,12/Nov/14 02:44,14/Jul/23 05:39,12/Nov/14 02:44,,,,0.8.2.0,,,,,,,,,,0,,,,"removeTopic does not correctly update the state of ControllerContext.

This is because it removes the topic from some underlying maps through dropWhile.",,guozhang,jjkoshy,nehanarkhede,onurkaraman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/14 19:44;onurkaraman;KAFKA-1742.patch;https://issues.apache.org/jira/secure/attachment/12680637/KAFKA-1742.patch","07/Nov/14 06:11;onurkaraman;KAFKA-1742.patch;https://issues.apache.org/jira/secure/attachment/12680100/KAFKA-1742.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 12 02:44:45 UTC 2014,,,,,,,,,,"0|i21tzr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/14 23:26;onurkaraman;partitionLeadershipInfo and partitionReplicaAssignment are mutable.Maps. Given that a mutable.Map may be arbitrarily ordered, dropping the longest prefix of elements that satisfy a predicate can cause incorrect results.

It's also noted in the scaladocs [http://www.scala-lang.org/api/current/index.html#scala.collection.mutable.Map@dropWhile(p:A=>Boolean):Repr]

For example:
{code}
import collection._

object Main {
  def main(args: Array[String]) {
    var m = mutable.Map(5 -> 2, 3 -> 6)
    println(""original: "" + m)
    println(""using filter: "" + m.filter(p => p._1 != 3))
    println(""using dropWhile: "" + m.dropWhile(p => p._1 == 3))
  }
}
{code}

Outputs:
{code}
original: Map(5 -> 2, 3 -> 6)
using filter: Map(5 -> 2)
using dropWhile: Map(5 -> 2, 3 -> 6)
{code};;;","07/Nov/14 18:37;guozhang;Thanks for the patch, looks good to me.;;;","07/Nov/14 23:48;jjkoshy;+1, although one minor change:
{code}
// instead of
filter(p => !p._1.topic.equals(topic)
//
filter{ case(topicPartition, _) => topicPartition.topic != topicToRemove }
{code}

I'm a bit divided on the usefulness of the unit test.;;;","10/Nov/14 02:28;nehanarkhede;+1 on the change but I think we don't want to add a separate test suite just for controller context. We may be able to do without the test. ;;;","10/Nov/14 19:31;guozhang;Actually, just realized that we do not have a ""controller"" test suite under scala.unit.kafka before, and the tests in the ""server"", and this would be a first step adding such a test suite to controller modules. The current test scope ""ControllerContextTest"" also looks fine to me, but in the future as we add more test cases into it we can always generalize the test class name.;;;","10/Nov/14 19:44;onurkaraman;Updated patch with Joel's suggestion to
{code}filter{ case (topicAndPartition, _) => ... }{code};;;","12/Nov/14 02:44;jjkoshy;+1 and committed to trunk and 0.8.2

Based on the review comments I did not include the unit test. I think it makes sense to work out a full-fledged unit test suite for controller in general.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove testComplexCompressDecompress in MessageCompressionTest,KAFKA-1739,12751806,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,omkreddy,guozhang,guozhang,30/Oct/14 22:10,10/Nov/14 04:24,14/Jul/23 05:39,10/Nov/14 04:24,,,,0.9.0.0,,,,,,,,,,0,newbie,,,"As discussed on the mailing list, we would not support nested compression in Kafka, and hence could remove this test case.",,guozhang,nehanarkhede,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/14 12:26;omkreddy;KAFKA-1739.patch;https://issues.apache.org/jira/secure/attachment/12680147/KAFKA-1739.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 10 04:24:19 UTC 2014,,,,,,,,,,"0|i21sgn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Nov/14 12:26;omkreddy;Created reviewboard https://reviews.apache.org/r/27723/diff/
 against branch origin;;;","10/Nov/14 04:24;nehanarkhede;Thanks for the patch. Pushed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partitions for topic not created after restart from forced shutdown,KAFKA-1738,12751679,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,pradeepbadiger,pradeepbadiger,30/Oct/14 13:37,07/Nov/14 18:47,14/Jul/23 05:39,07/Nov/14 18:47,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"We are using Kafka Topic APIs to create the topic. But in some cases, the topic gets created but we don't see the partition specific files and when producer/consumer tries to get the topic metadata and it fails with exception. Same happens if one tries to create using the command line.

k.p.BrokerPartitionInfo - Error while fetching metadata [{TopicMetadata for topic tloader1 -> No partition metadata for topic tloader1 due to kafka.common.UnknownTopicOrPartitionException}] for topic [tloader1]: class kafka.common.UnknownTopicOrPartitionException

Steps to reproduce - 

1.      Stop kafka using kill  -9 <PID of Kafka>
2.      Start Kafka
3.      Create Topic with partition and replication factor of 1.
4.      Check the response “Created topic <topic_name>”
5.      Run the list command to verify if its created.
6.      Now check the data directory of kakfa. There would not be any for the newly created topic.


We see issues when we are creating new topics. This happens randomly and we dont know the exact reasons. We see the below logs in controller during the time of creation of topics which doesnt have the partition files.

2014-11-03 13:12:50,625] INFO [Controller 0]: New topic creation callback for [JobJTopic,0] (kafka.controller.KafkaController)
[2014-11-03 13:12:50,626] INFO [Controller 0]: New partition creation callback for [JobJTopic,0] (kafka.controller.KafkaController)
[2014-11-03 13:12:50,626] INFO [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [JobJTopic,0] (kafka.controller.PartitionStateMachine)
[2014-11-03 13:12:50,653] INFO [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=JobJTopic,Partition=0,Replica=0] (kafka.controller.ReplicaStateMachine)
[2014-11-03 13:12:50,654] INFO [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [JobJTopic,0] (kafka.controller.PartitionStateMachine)
[2014-11-03 13:12:50,654] DEBUG [Partition state machine on Controller 0]: Live assigned replicas for partition [JobJTopic,0] are: [List(0)] (kafka.controller.PartitionStateMachine)
[2014-11-03 13:12:50,654] DEBUG [Partition state machine on Controller 0]: Initializing leader and isr for partition [JobJTopic,0] to (Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:2) (kafka.controller.PartitionStateMachine)
[2014-11-03 13:12:50,667] INFO [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=JobJTopic,Partition=0,Replica=0] (kafka.controller.ReplicaStateMachine)
[2014-11-03 13:12:50,794] WARN [Controller-0-to-broker-0-send-thread], Controller 0 fails to send a request to broker id:0,host:DMIPVM,port:9092 (kafka.controller.RequestSendThread)
java.io.EOFException: Received -1 when reading from channel, socket has likely been closed.
	at kafka.utils.Utils$.read(Utils.scala:381)
	at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)
	at kafka.network.Receive$class.readCompletely(Transmission.scala:56)
	at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29)
	at kafka.network.BlockingChannel.receive(BlockingChannel.scala:108)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:146)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2014-11-03 13:12:50,965] ERROR [Controller-0-to-broker-0-send-thread], Controller 0 epoch 2 failed to send request Name:UpdateMetadataRequest;Version:0;Controller:0;ControllerEpoch:2;CorrelationId:43;ClientId:id_0-host_null-port_9092;AliveBrokers:id:0,host:DMIPVM,port:9092;PartitionState:[JobJTopic,0] -> (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:2),ReplicationFactor:1),AllReplicas:0) to broker id:0,host:DMIPVM,port:9092. Reconnecting to broker. (kafka.controller.RequestSendThread)
java.nio.channels.ClosedChannelException
	at kafka.network.BlockingChannel.send(BlockingChannel.scala:97)
	at kafka.controller.RequestSendThread.liftedTree1$1(ControllerChannelManager.scala:132)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:131)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)


","Linux, 2GB RAM, 2 Core CPU",junrao,nehanarkhede,pradeepbadiger,schandr,thathineni.srihari@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1713,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/14 02:42;pradeepbadiger;1738.zip;https://issues.apache.org/jira/secure/attachment/12679461/1738.zip","05/Nov/14 19:21;schandr;ServerLogForFailedTopicCreation.txt;https://issues.apache.org/jira/secure/attachment/12679604/ServerLogForFailedTopicCreation.txt","05/Nov/14 19:16;schandr;ServerLogForFailedTopicCreation.txt;https://issues.apache.org/jira/secure/attachment/12679598/ServerLogForFailedTopicCreation.txt","05/Nov/14 19:16;schandr;ServerLogForSuccessfulTopicCreation.txt;https://issues.apache.org/jira/secure/attachment/12679599/ServerLogForSuccessfulTopicCreation.txt","06/Nov/14 17:34;junrao;kafka-1738.patch;https://issues.apache.org/jira/secure/attachment/12679889/kafka-1738.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 07 18:47:32 UTC 2014,,,,,,,,,,"0|i21rpz:",9223372036854775807,,nehanarkhede,,,,,,,,,,,,,,,,,,"30/Oct/14 16:58;nehanarkhede;[~pradeepbadiger] Do you also try to send data to the topic you are hoping to see the logs for?;;;","31/Oct/14 02:51;pradeepbadiger;Yes. We tried sending data but it gave the exception.

k.p.BrokerPartitionInfo - Error while fetching metadata [{TopicMetadata for topic JobCTopic -> No partition metadata for topic JobCTopic due to kafka.common.UnknownTopicOrPartitionException}] for topic [JobCTopic]: class kafka.common.UnknownTopicOrPartitionException;;;","04/Nov/14 02:13;junrao;I can't reproduce this issue by following the steps in the description. Does this happen every time?
;;;","04/Nov/14 16:41;pradeepbadiger;yes.. This is happening in 8.2.0-beta version which we are currently on. I had two topic in kakfa before killing the service and i added three more topics (ATopic, BTopic and CTopic) and all of them got created successfully as you can see below. When i list it i can see the topics in kakfa. But if i see the data folder in kafka, i dont see the partition folders/files for them. Let me know if you need more details.

[root@dmipvm temp]# service kafka status
Kafka is running as 19396.
    LISTEN on tcp port=9999
    LISTEN on tcp port=53536
    LISTEN on tcp port=9092
    LISTEN on tcp port=48330
[root@dmipvm temp]# kill -9 19396
[root@dmipvm temp]# service kafka start
Starting kafka ... STARTED.
[root@dmipvm temp]# /apps/kafka/bin/kafka-topics.sh --create --topic ATopic --partitions 1 --replication-factor 1 --zookeeper localhost:2181
Created topic ""ATopic"".
[root@dmipvm temp]# /apps/kafka/bin/kafka-topics.sh --create --topic BTopic --partitions 1 --replication-factor 1 --zookeeper localhost:2181
Created topic ""BTopic"".
[root@dmipvm temp]# /apps/kafka/bin/kafka-topics.sh --create --topic CTopic --partitions 1 --replication-factor 1 --zookeeper localhost:2181
Created topic ""CTopic"".
[root@dmipvm temp]#


[root@DMIPVM kafka]# ls -lrt
total 16
drwxr-xr-x 2 root root 4096 Nov  4 11:32 topic_1-0
drwxr-xr-x 2 root root 4096 Nov  4 11:34 topic_2-0
-rw-r--r-- 1 root root   28 Nov  4 11:35 replication-offset-checkpoint
-rw-r--r-- 1 root root   28 Nov  4 11:36 recovery-point-offset-checkpoint


[root@DMIPVM kafka]# /apps/kafka/bin/kafka-topics.sh --list --zookeeper localhost:2181
ATopic
BTopic
CTopic
topic_1
topic_2
;;;","05/Nov/14 02:42;pradeepbadiger;Attachment 1738.zip contains logs and a script which creates a topic every 10 mins.;;;","05/Nov/14 16:28;junrao;The log shows that the log dir was created for topic_A. Were you looking at the right directory?

[2014-11-04 21:36:30,724] INFO Created log for partition [topic_A,0] in /tmp/kafka-logs with properties;;;","05/Nov/14 17:01;pradeepbadiger;I suggest you to run the script once on a working setup of kafka. The only error that we see on kafka controller.log is 

[2014-11-05 11:42:53,088] DEBUG [Partition state machine on Controller 0]: Live assigned replicas for partition [topic_13,0] are: [List(0)] (kafka.controller.PartitionStateMachine)
[2014-11-05 11:42:53,088] DEBUG [Partition state machine on Controller 0]: Initializing leader and isr for partition [topic_13,0] to (Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:5) (kafka.controller.PartitionStateMachine)
[2014-11-05 11:42:53,097] WARN [Controller-0-to-broker-0-send-thread], Controller 0 fails to send a request to broker id:0,host:DMIPVM,port:9092 (kafka.controller.RequestSendThread)
java.io.EOFException: Received -1 when reading from channel, socket has likely been closed.
	at kafka.utils.Utils$.read(Utils.scala:381)
	at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)
	at kafka.network.Receive$class.readCompletely(Transmission.scala:56)
	at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29)
	at kafka.network.BlockingChannel.receive(BlockingChannel.scala:108)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:146)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2014-11-05 11:42:53,097] INFO [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=topic_13,Partition=0,Replica=0] (kafka.controller.ReplicaStateMachine)
[2014-11-05 11:42:53,097] ERROR [Controller-0-to-broker-0-send-thread], Controller 0 epoch 5 failed to send request Name:UpdateMetadataRequest;Version:0;Controller:0;ControllerEpoch:5;CorrelationId:16;ClientId:id_0-host_null-port_9092;AliveBrokers:id:0,host:DMIPVM,port:9092;PartitionState:[topic_13,0] -> (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:5),ReplicationFactor:1),AllReplicas:0) to broker id:0,host:DMIPVM,port:9092. Reconnecting to broker. (kafka.controller.RequestSendThread)
java.nio.channels.ClosedChannelException
	at kafka.network.BlockingChannel.send(BlockingChannel.scala:97)
	at kafka.controller.RequestSendThread.liftedTree1$1(ControllerChannelManager.scala:132)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:131)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2014-11-05 11:42:53,099] INFO [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:DMIPVM,port:9092 for sending state change requests (kafka.controller.RequestSendThread)
[2014-11-05 11:45:34,605] TRACE [Controller 0]: checking need to trigger partition rebalance (kafka.controller.KafkaController)
[2014-11-05 11:45:34,607] DEBUG [Controller 0]: preferred replicas by broker Map(0 -> Map([topic_8,0] -> List(0), [topic_13,0] -> List(0), [topic_6,0] -> List(0), [topic_1,0] -> List(0), [topic_9,0] -> List(0), [topic_2,0] -> List(0), [topic_11,0] -> List(0), [topic_4,0] -> List(0), [topic_12,0] -> List(0), [topic_7,0] -> List(0), [topic_3,0] -> List(0), [topic_10,0] -> List(0))) (kafka.controller.KafkaController)
[2014-11-05 11:45:34,608] DEBUG [Controller 0]: topics not in preferred replica Map() (kafka.controller.KafkaController)
[2014-11;;;","05/Nov/14 17:19;junrao;Yes, I started a kafka broker and ran your script. I was able to see local logs created.

ls /tmp/kafka-logs/
juntopic-0				test-0					topic3-0				topic_3-0
recovery-point-offset-checkpoint	topic1-0				topic_1-0				topic_4-0
replication-offset-checkpoint		topic2-0				topic_2-0				topic_5-0

The error you saw typically happens when a broker is down. Can you telnet to host DMIPVM on port 9092 when the broker is up?
;;;","05/Nov/14 17:24;pradeepbadiger;Can you provide the configuration files? We are using the default configurations and the script tries to create a topic every 10 mins. Also, i tried telnet and the broker is up.;;;","05/Nov/14 17:37;schandr;Here are the additional logs for the same issue

Controller.log
[2014-11-05 10:31:12,441] WARN [Controller-0-to-broker-0-send-thread], Controller 0 fails to send a request to broker id:0,host:localhost.localdomain,port:9092 (kafka.controller.RequestSendThread)
java.io.EOFException: Received -1 when reading from channel, socket has likely been closed.
	at kafka.utils.Utils$.read(Utils.scala:381)
	at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)
	at kafka.network.Receive$class.readCompletely(Transmission.scala:56)
	at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29)
	at kafka.network.BlockingChannel.receive(BlockingChannel.scala:108)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:146)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2014-11-05 10:31:12,445] ERROR [Controller-0-to-broker-0-send-thread], Controller 0 epoch 7 failed to send request Name:UpdateMetadataRequest;Version:0;Controller:0;ControllerEpoch:7;CorrelationId:8;ClientId:id_0-host_null-port_9092;AliveBrokers:id:0,host:localhost.localdomain,port:9092;PartitionState:[topic_30,0] -> (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:7),ReplicationFactor:1),AllReplicas:0),[topic_5,0] -> (LeaderAndIsrInfo:(Leader:-2,ISR:0,LeaderEpoch:0,ControllerEpoch:7),ReplicationFactor:1),AllReplicas:0) to broker id:0,host:localhost.localdomain,port:9092. Reconnecting to broker. (kafka.controller.RequestSendThread)
java.nio.channels.ClosedChannelException
	at kafka.network.BlockingChannel.send(BlockingChannel.scala:97)
	at kafka.controller.RequestSendThread.liftedTree1$1(ControllerChannelManager.scala:132)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:131)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2014-11-05 10:31:12,448] INFO [Controller-0-to-broker-0-send-thread], Controller 0 connected to id:0,host:localhost.localdomain,port:9092 for sending state change requests (kafka.controller.RequestSendThread)

Server.log

[2014-11-05 10:31:12,414] DEBUG Got notification sessionid:0x14980b08c110003 (org.apache.zookeeper.ClientCnxn)
[2014-11-05 10:31:12,415] DEBUG Got WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/brokers/topics for sessionid 0x14980b08c110003 (org.apache.zookeeper.ClientCnxn)
[2014-11-05 10:31:12,415] DEBUG Received event: WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/brokers/topics (org.I0Itec.zkclient.ZkClient)
[2014-11-05 10:31:12,415] DEBUG New event: ZkEvent[Children of /brokers/topics changed sent to kafka.controller.PartitionStateMachine$TopicChangeListener@1fc5681] (org.I0Itec.zkclient.ZkEventThread)
[2014-11-05 10:31:12,415] DEBUG Leaving process event (org.I0Itec.zkclient.ZkClient)
[2014-11-05 10:31:12,415] DEBUG Delivering event #3 ZkEvent[Children of /brokers/topics changed sent to kafka.controller.PartitionStateMachine$TopicChangeListener@1fc5681] (org.I0Itec.zkclient.ZkEventThread)
[2014-11-05 10:31:12,415] DEBUG Got ping response for sessionid: 0x14980b08c110003 after 0ms (org.apache.zookeeper.ClientCnxn)
[2014-11-05 10:31:12,416] DEBUG Reading reply sessionid:0x14980b08c110003, packet:: clientPath:null serverPath:null finished:false header:: 257,3  replyHeader:: 257,13610,0  request:: '/brokers/topics,T  response:: s{6,6,1415130748279,1415130748279,0,23,0,0,0,23,13610}  (org.apache.zookeeper.ClientCnxn)
[2014-11-05 10:31:12,417] DEBUG Reading reply sessionid:0x14980b08c110003, packet:: clientPath:null serverPath:null finished:false header:: 258,8  replyHeader:: 258,13610,0  request:: '/brokers/topics,T  response:: v{'topic_23,'topic_18,'topic_22,'topic_17,'topic_25,'topic_16,'topic_24,'topic_15,'topic_14,'topic_13,'topic_12,'topic_11,'topic_19,'topic_5,'topic_7,'topic_6,'Test1,'topic_10,'topic_9,'topic_8,'topic_20,'topic_30,'topic_21}  (org.apache.zookeeper.ClientCnxn)
[2014-11-05 10:31:12,420] DEBUG Reading reply sessionid:0x14980b08c110003, packet:: clientPath:null serverPath:null finished:false header:: 259,4  replyHeader:: 259,13610,0  request:: '/brokers/topics/topic_30,F  response:: #7b2276657273696f6e223a312c22706172746974696f6e73223a7b2230223a5b305d7d7d,s{13610,13610,1415205072414,1415205072414,0,0,0,0,36,0,13610}  (org.apache.zookeeper.ClientCnxn)
[2014-11-05 10:31:12,422] DEBUG Replicas assigned to topic [topic_30], partition [0] are [List(0)] (kafka.utils.ZkUtils$)
[2014-11-05 10:31:12,422] DEBUG Replicas assigned to topic [topic_30], partition [0] are [List(0)] (kafka.utils.ZkUtils$)
[2014-11-05 10:31:12,425] DEBUG Reading reply sessionid:0x14980b08c110003, packet:: clientPath:null serverPath:null finished:false header:: 260,3  replyHeader:: 260,13611,0  request:: '/brokers/topics/topic_30,T  response:: s{13610,13610,1415205072414,1415205072414,0,0,0,0,36,0,13610}  (org.apache.zookeeper.ClientCnxn)
[2014-11-05 10:31:12,425] DEBUG Subscribed data changes for /brokers/topics/topic_30 (org.I0Itec.zkclient.ZkClient)
[2014-11-05 10:31:12,427] DEBUG Reading reply sessionid:0x14980b08c110003, packet:: clientPath:null serverPath:null finished:false header:: 261,4  replyHeader:: 261,13611,0  request:: '/brokers/topics/topic_30,T  response:: #7b2276657273696f6e223a312c22706172746974696f6e73223a7b2230223a5b305d7d7d,s{13610,13610,1415205072414,1415205072414,0,0,0,0,36,0,13610}  (org.apache.zookeeper.ClientCnxn)
[2014-11-05 10:31:12,430] DEBUG Reading reply sessionid:0x14980b08c110003, packet:: clientPath:null serverPath:null finished:false header:: 262,4  replyHeader:: 262,13611,-101  request:: '/brokers/topics/topic_30/partitions/0/state,F  response::   (org.apache.zookeeper.ClientCnxn)
[2014-11-05 10:31:12,434] DEBUG Reading reply sessionid:0x14980b08c110003, packet:: clientPath:null serverPath:null finished:false header:: 263,1  replyHeader:: 263,13612,-101  request:: '/brokers/topics/topic_30/partitions/0/state,#7b22636f6e74726f6c6c65725f65706f6368223a372c226c6561646572223a302c2276657273696f6e223a312c226c65616465725f65706f6368223a302c22697372223a5b305d7d,v{s{31,s{'world,'anyone}}},0  response::   (org.apache.zookeeper.ClientCnxn)
[2014-11-05 10:31:12,435] DEBUG Reading reply sessionid:0x14980b08c110003, packet:: clientPath:null serverPath:null finished:false header:: 264,1  replyHeader:: 264,13613,-101  request:: '/brokers/topics/topic_30/partitions/0,,v{s{31,s{'world,'anyone}}},0  response::   (org.apache.zookeeper.ClientCnxn)
[2014-11-05 10:31:12,437] DEBUG Reading reply sessionid:0x14980b08c110003, packet:: clientPath:null serverPath:null finished:false header:: 265,1  replyHeader:: 265,13614,0  request:: '/brokers/topics/topic_30/partitions,,v{s{31,s{'world,'anyone}}},0  response:: '/brokers/topics/topic_30/partitions  (org.apache.zookeeper.ClientCnxn)
[2014-11-05 10:31:12,438] DEBUG Reading reply sessionid:0x14980b08c110003, packet:: clientPath:null serverPath:null finished:false header:: 266,1  replyHeader:: 266,13615,0  request:: '/brokers/topics/topic_30/partitions/0,,v{s{31,s{'world,'anyone}}},0  response:: '/brokers/topics/topic_30/partitions/0  (org.apache.zookeeper.ClientCnxn)
[2014-11-05 10:31:12,439] DEBUG Reading reply sessionid:0x14980b08c110003, packet:: clientPath:null serverPath:null finished:false header:: 267,1  replyHeader:: 267,13616,0  request:: '/brokers/topics/topic_30/partitions/0/state,#7b22636f6e74726f6c6c65725f65706f6368223a372c226c6561646572223a302c2276657273696f6e223a312c226c65616465725f65706f6368223a302c22697372223a5b305d7d,v{s{31,s{'world,'anyone}}},0  response:: '/brokers/topics/topic_30/partitions/0/state  (org.apache.zookeeper.ClientCnxn)
[2014-11-05 10:31:12,440] TRACE 131 bytes written. (kafka.network.BoundedByteBufferSend)
[2014-11-05 10:31:12,440] TRACE 131 bytes written. (kafka.network.BoundedByteBufferSend)
[2014-11-05 10:31:12,441] DEBUG Delivering event #3 done (org.I0Itec.zkclient.ZkEventThread)
[2014-11-05 10:31:12,448] DEBUG Created socket with SO_TIMEOUT = 90000 (requested 90000), SO_RCVBUF = 43690 (requested -1), SO_SNDBUF = 84580 (requested -1). (kafka.network.BlockingChannel)
[2014-11-05 10:31:12,448] DEBUG Created socket with SO_TIMEOUT = 90000 (requested 90000), SO_RCVBUF = 43690 (requested -1), SO_SNDBUF = 84580 (requested -1). (kafka.network.BlockingChannel)
[2014-11-05 10:31:12,448] DEBUG Accepted connection from /127.0.0.1 on /127.0.0.1:9092. sendBufferSize [actual|requested]: [102400|102400] recvBufferSize [actual|requested]: [102400|102400] (kafka.network.Acceptor)
[2014-11-05 10:31:12,448] DEBUG Accepted connection from /127.0.0.1 on /127.0.0.1:9092. sendBufferSize [actual|requested]: [102400|102400] recvBufferSize [actual|requested]: [102400|102400] (kafka.network.Acceptor)
[2014-11-05 10:31:12,449] TRACE Processor id 1 selection time = 228708013 ns (kafka.network.Processor)
[2014-11-05 10:31:12,449] TRACE Processor id 1 selection time = 228708013 ns (kafka.network.Processor)
[2014-11-05 10:31:12,449] DEBUG Processor 1 listening to new connection from /127.0.0.1:55693 (kafka.network.Processor)
[2014-11-05 10:31:12,449] DEBUG Processor 1 listening to new connection from /127.0.0.1:55693 (kafka.network.Processor)
[2014-11-05 10:31:12,550] TRACE Processor id 7 selection time = 301161349 ns (kafka.network.Processor)
[2014-11-05 10:31:12,550] TRACE Processor id 7 selection time = 301161349 ns (kafka.network.Processor)
[2014-11-05 10:31:12,596] TRACE Processor id 3 selection time = 300772884 ns (kafka.network.Processor)
[2014-11-05 10:31:12,596] TRACE Processor id 3 selection time = 300772884 ns (kafka.network.Processor)
[2014-11-05 10:31:12,640] TRACE Processor id 6 selection time = 301205873 ns (kafka.network.Processor)
[2014-11-05 10:31:12,640] TRACE Processor id 6 selection time = 301205873 ns (kafka.network.Processor)
[2014-11-05 10:31:12,660] TRACE Processor id 8 selection time = 301212710 ns (kafka.network.Processor)
[2014-11-05 10:31:12,660] TRACE Processor id 8 selection time = 301212710 ns (kafka.network.Processor)
[2014-11-05 10:31:12,660] TRACE Processor id 9 selection time = 301187091 ns (kafka.network.Processor)
[2014-11-05 10:31:12,660] TRACE Processor id 9 selection time = 301187091 ns (kafka.network.Processor)
[2014-11-05 10:31:12,673] TRACE Processor id 0 selection time = 301144174 ns (kafka.network.Processor)
[2014-11-05 10:31:12,673] TRACE Processor id 0 selection time = 301144174 ns (kafka.network.Processor)
[2014-11-05 10:31:12,674] TRACE Processor id 4 selection time = 301200540 ns (kafka.network.Processor)
[2014-11-05 10:31:12,674] TRACE Processor id 4 selection time = 301200540 ns (kafka.network.Processor)
[2014-11-05 10:31:12,675] TRACE Processor id 2 selection time = 301240208 ns (kafka.network.Processor)
[2014-11-05 10:31:12,675] TRACE Processor id 2 selection time = 301240208 ns (kafka.network.Processor)
[2014-11-05 10:31:12,679] TRACE Processor id 5 selection time = 301217390 ns (kafka.network.Processor)
[2014-11-05 10:31:12,679] TRACE Processor id 5 selection time = 301217390 ns (kafka.network.Processor)
[2014-11-05 10:31:12,750] TRACE Processor id 1 selection time = 300491401 ns (kafka.network.Processor)
[2014-11-05 10:31:12,750] TRACE Processor id 1 selection time = 300491401 ns (kafka.network.Processor)
[2014-11-05 10:31:12,750] TRACE 176 bytes written. (kafka.network.BoundedByteBufferSend)
[2014-11-05 10:31:12,750] TRACE 176 bytes written. (kafka.network.BoundedByteBufferSend)
[2014-11-05 10:31:12,750] TRACE Processor id 1 selection time = 232897 ns (kafka.network.Processor)
[2014-11-05 10:31:12,750] TRACE Processor id 1 selection time = 232897 ns (kafka.network.Processor)
[2014-11-05 10:31:12,750] TRACE 172 bytes read from /127.0.0.1:55693 (kafka.network.Processor)
[2014-11-05 10:31:12,750] TRACE 172 bytes read from /127.0.0.1:55693 (kafka.network.Processor)
[2014-11-05 10:31:12,751] TRACE [Kafka Request Handler 2 on Broker 0], Kafka request handler 2 on broker 0 handling request Request(1,sun.nio.ch.SelectionKeyImpl@3a6a2cf,null,1415205072751,/127.0.0.1:55693) (kafka.server.KafkaRequestHandler)
[2014-11-05 10:31:12,751] TRACE [Kafka Request Handler 2 on Broker 0], Kafka request handler 2 on broker 0 handling request Request(1,sun.nio.ch.SelectionKeyImpl@3a6a2cf,null,1415205072751,/127.0.0.1:55693) (kafka.server.KafkaRequestHandler)
[2014-11-05 10:31:12,751] TRACE [KafkaApi-0] Handling request: Name:UpdateMetadataRequest;Version:0;Controller:0;ControllerEpoch:7;CorrelationId:8;ClientId:id_0-host_null-port_9092;AliveBrokers:id:0,host:localhost.localdomain,port:9092;PartitionState:[topic_30,0] -> (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:7),ReplicationFactor:1),AllReplicas:0),[topic_5,0] -> (LeaderAndIsrInfo:(Leader:-2,ISR:0,LeaderEpoch:0,ControllerEpoch:7),ReplicationFactor:1),AllReplicas:0) from client: /127.0.0.1:55693 (kafka.server.KafkaApis)
[2014-11-05 10:31:12,751] TRACE [KafkaApi-0] Handling request: Name:UpdateMetadataRequest;Version:0;Controller:0;ControllerEpoch:7;CorrelationId:8;ClientId:id_0-host_null-port_9092;AliveBrokers:id:0,host:localhost.localdomain,port:9092;PartitionState:[topic_30,0] -> (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:7),ReplicationFactor:1),AllReplicas:0),[topic_5,0] -> (LeaderAndIsrInfo:(Leader:-2,ISR:0,LeaderEpoch:0,ControllerEpoch:7),ReplicationFactor:1),AllReplicas:0) from client: /127.0.0.1:55693 (kafka.server.KafkaApis)
[2014-11-05 10:31:12,752] TRACE Processor id 1 selection time = 660576 ns (kafka.network.Processor)
[2014-11-05 10:31:12,752] TRACE Processor id 1 selection time = 660576 ns (kafka.network.Processor)
[2014-11-05 10:31:12,753] TRACE Socket server received response to send, registering for write: Response(1,Request(1,sun.nio.ch.SelectionKeyImpl@3a6a2cf,null,1415205072751,/127.0.0.1:55693),kafka.network.BoundedByteBufferSend@6d0015b6,SendAction) (kafka.network.Processor)
[2014-11-05 10:31:12,753] TRACE Socket server received response to send, registering for write: Response(1,Request(1,sun.nio.ch.SelectionKeyImpl@3a6a2cf,null,1415205072751,/127.0.0.1:55693),kafka.network.BoundedByteBufferSend@6d0015b6,SendAction) (kafka.network.Processor)
[2014-11-05 10:31:12,753] TRACE Processor id 1 selection time = 12017 ns (kafka.network.Processor)
[2014-11-05 10:31:12,753] TRACE Processor id 1 selection time = 12017 ns (kafka.network.Processor)
[2014-11-05 10:31:12,753] TRACE 10 bytes written to /127.0.0.1:55693 using key sun.nio.ch.SelectionKeyImpl@3a6a2cf (kafka.network.Processor)
[2014-11-05 10:31:12,753] TRACE 10 bytes written to /127.0.0.1:55693 using key sun.nio.ch.SelectionKeyImpl@3a6a2cf (kafka.network.Processor)
[2014-11-05 10:31:12,753] TRACE 6 bytes read. (kafka.network.BoundedByteBufferReceive)
[2014-11-05 10:31:12,753] TRACE 6 bytes read. (kafka.network.BoundedByteBufferReceive)
[2014-11-05 10:31:12,753] TRACE Finished writing, registering for read on connection /127.0.0.1:55693 (kafka.network.Processor)
[2014-11-05 10:31:12,753] TRACE Finished writing, registering for read on connection /127.0.0.1:55693 (kafka.network.Processor)
[2014-11-05 10:31:12,851] TRACE Processor id 7 selection time = 301124890 ns (kafka.network.Processor)
[2014-11-05 10:31:12,851] TRACE Processor id 7 selection time = 301124890 ns (kafka.network.Processor)






If we look at the timestamp, when the error is thrown in the controller.log its at 10:31:12,445. But the timestamp in the server.log where the data is written to the Socket is after the exception is thrown.

Is there some kind of race condition with the acceptor and send threads within the Kafka processor?;;;","05/Nov/14 18:02;schandr;And I was able to telnet to the host:port.;;;","05/Nov/14 19:16;schandr;Please see the attached serverlog excerpts for a failed and successful topic creation. Please use the second  ServerLogForFailedTopicCreation.txt file
For a failed Topic creation the log files are not getting created under the kafka.log.dirs folder.
Looking at the successful topic creation server log, I can see the trace logs for the partition log creation, which is missing from the failed server log.
One main difference is, in the failed log, I see the Kafka handler request as UpdateMetaData request, where as in the successful log, I see both LeaderAndIsrRequest and UpdateMetaData kafka handler requests.;;;","05/Nov/14 21:30;schandr;Ok....Here is my understanding. This might probably be a bug.

1. For any requests that the controller sends to the Broker, it uses the BlockingChannel - that's initialized with the controller.socket.timeout.ms value specified in the server.properties.
2. Once the channel is established the RequestSendThread uses this channel to send any requests such as LeaderAndIsr, UpdateMetaData without checking if the channel is still open
3. Based on the value, the socket might have timed out. The following Code in the RequestSendThread calls the connectToBroker again on catching the exception, but does not send the failed request. If the request happens to be LeaderAndIsr for the new partition it results in missing log directory creation or other errors which results in producer or consumer throwing exceptions when they try to produce or consume data from the failed topic.




 var isSendSuccessful = false
        while(isRunning.get() && !isSendSuccessful) {
          // if a broker goes down for a long time, then at some point the controller's zookeeper listener will trigger a
          // removeBroker which will invoke shutdown() on this thread. At that point, we will stop retrying.
          try {
            channel.send(request)
            isSendSuccessful = true
          } catch {
            case e: Throwable => // if the send was not successful, reconnect to broker and resend the message
              error((""Controller %d epoch %d failed to send request %s to broker %s. "" +
                ""Reconnecting to broker."").format(controllerId, controllerContext.epoch,
                request.toString, toBroker.toString()), e)

              channel.disconnect()
              connectToBroker(toBroker, channel)

              isSendSuccessful = false
              // backoff before retrying the connection and send
              Utils.swallow(Thread.sleep(300))
          }
        }


In the code above, after reconnecting to the broker, it should also resend the failed request. Atleast the inline comment says so. -- // if the send was not successful, reconnect to broker and resend the message. So the code in the catch block should be

 catch {
            case e: Throwable => // if the send was not successful, reconnect to broker and resend the message
              error((""Controller %d epoch %d failed to send request %s to broker %s. "" +
                ""Reconnecting to broker."").format(controllerId, controllerContext.epoch,
                request.toString, toBroker.toString()), e)

              channel.disconnect()
              connectToBroker(toBroker, channel)

              channel.send(request)

              isSendSuccessful = false
              // backoff before retrying the connection and send
              Utils.swallow(Thread.sleep(300))
          }
;;;","06/Nov/14 02:07;junrao;Hmm, that try/catch block is in a while loop. So, the resend should happen when the logic loops back again. Also, normally when a broker goes down, the controller typically notices it immediately and will remove the corresponding BlockingChannel. When a broker comes back, a new BlockingChannel will be created.

It sounds like that you see this issue in some broker failure scenarios. Could you describe that a bit more?;;;","06/Nov/14 02:19;schandr;Thank you very much for your response.

First I think the broker is not down. Because I am able to telnet to the VM where kafka is running using the host:9092. We have tried several scenarios
1. Creating a topic through Java code using TopicCommand
2. Creating a topic in a bash script.

In either of the cases above, the topic creation fails randomly. When the bash script is modified to create the topic every 10 minutes, it fails consistently. But if the same bash script is modified to create topic every 5 minutes, then the alternate topic creation goes through. Somehow the 10 minute interval is failing consistently.
In the server log, I see the request to LeaderAndISR fails, that results in partition log file not getting created. Only zookeeper and Kafka is running in the VM where the script was ran. Let me know if you need more information like server.properties file. Also I am able to see the zk nodes for the controller and broker through an eclipse zookeeper plugin

;;;","06/Nov/14 02:33;junrao;What's your value for controller.socket.timeout.ms?;;;","06/Nov/14 02:44;schandr;controller.socket.timeout.ms=90000
controller.message.queue.size=20
auto.leader.rebalance.enable=true
queued.max.requests=20;;;","06/Nov/14 03:17;schandr;One more observance in the server log

Here is the Trace statement for the send request. This gets logged when channel.send(request) is invoked in the RequestSendThread.
The send method invokes writeCompletely method in the Send class.
0:31:12,440] TRACE 131 bytes written. (kafka.network.BoundedByteBufferSend)
[2014-11-05 10:31:12,440] TRACE 131 bytes written. (kafka.network.BoundedByteBufferSend)

If the request gets resend the same statement should be relogged in the server log based on
channel.send(request) code in the RequestSendThread which did not get logged.

And it could be a race condition between the LeaderAndIsr request resend and the UpdateMetaData request, which could lead to the channel.receive throwing a EOFException.

Once the topic creation fails, no exceptions are thrown back for us to catch and retry. Is there any workaround for this issue?;;;","06/Nov/14 17:31;junrao;You actually found a real bug, thanks! We exposed an existing problem after adding the ability to kill idle connections in KAFKA-1282. The default max idle time happens to be 10 minutes. That's why you only see the issue if the topics are created more than 10 mins apart. I will attach a patch soon.;;;","06/Nov/14 17:34;junrao;Created reviewboard https://reviews.apache.org/r/27690/diff/
 against branch origin/trunk;;;","06/Nov/14 17:37;junrao;Sri, Pradeep,

Do you think you can try the patch and see if this fixes your issue?

Also marking this as an 0.8.2 blocker.;;;","06/Nov/14 18:29;schandr;Great....And thank you for the Patch. How should we apply this patch? Are there any instructions on how to apply the patch.;;;","06/Nov/14 18:55;junrao;You can follow the instruction at https://cwiki.apache.org/confluence/display/KAFKA/Patch+submission+and+review#Patchsubmissionandreview-Reviewerworkflow: to apply the patch.;;;","06/Nov/14 19:21;nehanarkhede;Good catch. Thanks for following up on this, Jun. Reviewed the patch, looks good.;;;","06/Nov/14 19:36;schandr;Will apply the patch against the 0.8.2-beta and post the update.;;;","07/Nov/14 15:47;schandr;The patch works and thanks for the help!!!. Will this be patch be included in 0.8.2 release? If yes, do you have a date in mind?;;;","07/Nov/14 18:47;junrao;Thanks for the review. Double committed to 0.8.2 and trunk.

We expect 0.8.2 final will be released in ~3 weeks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MemoryRecords.Iterator needs to handle partial reads from compressed stream,KAFKA-1735,12750934,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,27/Oct/14 23:55,17/May/16 14:15,14/Jul/23 05:39,01/Jul/15 23:20,,,,0.9.0.0,,,,,,,,,,0,,,,"Found a bug in the MemoryRecords.Iterator implementation, where 

{code}
stream.read(recordBuffer, 0, size)
{code}

can read less than size'ed bytes, and rest of the recordBuffer would set to ""\0"".",,guozhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/14 23:59;guozhang;KAFKA-1735.patch;https://issues.apache.org/jira/secure/attachment/12677464/KAFKA-1735.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 01 23:20:23 UTC 2015,,,,,,,,,,"0|i21n5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/14 23:59;guozhang;Created reviewboard https://reviews.apache.org/r/27256/diff/
 against branch origin/trunk;;;","29/Oct/14 19:04;guozhang;Updated reviewboard https://reviews.apache.org/r/27256/diff/
against branch origin/trunk;;;","01/Jul/15 23:20;guozhang;This bug has been resolved in another ticket, closing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Producer.send will block indeterminately when broker is unavailable.,KAFKA-1733,12750836,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,MarcOnSoftware,MarcOnSoftware,MarcOnSoftware,27/Oct/14 18:11,17/Oct/16 15:40,14/Jul/23 05:39,04/Nov/14 05:18,0.8.1.1,,,0.8.2.0,0.9.0.0,,,,,,core,producer ,,0,,,,"This is a follow up to the conversation here:

https://mail-archives.apache.org/mod_mbox/kafka-dev/201409.mbox/%3CCAOG_4QYMoEJHKbo0N31+A-UjX0z5unSiSD5WbrmN-XtX7giP-Q@mail.gmail.com%3E

During ClientUtils.fetchTopicMetadata, if the broker is unavailable, socket.connect will block indeterminately. Any retry policy (message.send.max.retries) further increases the time spent waiting for the socket to connect.

The root fix is to add a connection timeout value to the BlockingChannel's socket configuration, like so:

{noformat}
-channel.socket.connect(new InetSocketAddress(host, port))
+channel.socket.connect(new InetSocketAddress(host, port), connectTimeoutMs)
{noformat}

The simplest thing to do here would be to have a constant, default value that would be applied to every socket configuration. 

Is that acceptable? ",,dpnchl,junrao,MarcOnSoftware,nehanarkhede,rtyler,solon,vanyatka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/14 20:28;MarcOnSoftware;kafka-1733-add-connectTimeoutMs.patch;https://issues.apache.org/jira/secure/attachment/12678547/kafka-1733-add-connectTimeoutMs.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 17 15:40:03 UTC 2016,,,,,,,,,,"0|i21mkf:",9223372036854775807,,junrao,,,,,,,,,,,,,,,,,,"27/Oct/14 18:13;MarcOnSoftware;I have a patch (work in progress) here: https://github.com/mchung/kafka/commit/87b8ddbfe23dc887f56fa6f9ea3669733933c49b;;;","30/Oct/14 20:26;junrao;Thanks for the patch. Could you generate a patch and attach it to the jira? This will take care of the Apache licensing stuff.;;;","30/Oct/14 20:47;rtyler;[~junrao], the commit referenced is on top of the {{0.8.1.1}} release, do you have a specific branch that we should target this patch with?;;;","30/Oct/14 20:49;junrao;Since this doesn't seem to be an 0.8.2 blocker, let's patch this for trunk.;;;","30/Oct/14 21:24;MarcOnSoftware;In this patch, I've set the connectTimeoutMs to the same value as readTimeoutMs.

It's patched against trunk.;;;","31/Oct/14 16:28;rtyler;[~junrao] we are seeing this behavior against this client library version, fwiw:

{code}
<dependency>
<groupId>org.apache.kafka</groupId>
<artifactId>kafka_2.10</artifactId>
<version>0.8.1.1</version>
</dependency>
{code};;;","04/Nov/14 05:18;junrao;Thanks for the patch. +1 and committed to trunk.;;;","04/Nov/14 15:39;rtyler;Thanks a lot [~junrao]! As somebody not terribly familiar with the Kafka release process, is there a document you could point me to that describes the release cadence or when we might expect a release to be cut with this change?;;;","06/Nov/14 01:03;junrao;This is currently marked to be released in 0.8.3. We just released 0.8.2 beta. 0.8.2 final will probably be released in 2-3 weeks. 0.8.3 is probably going to be 4-6 months afterward.;;;","07/Nov/14 17:08;solon;Any chance of getting this into a bugfix release sooner than 0.8.3? This seems to significantly compromise Kafka's availability, since a single flaky broker can cause all producers to block.;;;","10/Nov/14 03:17;nehanarkhede;+1 on getting this in 0.8.2;;;","13/Nov/14 20:08;junrao;Since this is a small, but critical change, I double committed this to the 0.8.2 branch.;;;","17/Oct/16 15:40;dpnchl;As seen in the following file: The fix was made for the 0.9.0 release and it was also back-ported into 0.8.2
https://github.com/apache/kafka/blob/0.9.0/core/src/main/scala/kafka/network/BlockingChannel.scala
https://github.com/apache/kafka/blob/0.8.2/core/src/main/scala/kafka/network/BlockingChannel.scala

Also, as seen in the following file: The last release where this issue still exists is: 0.8.1
https://github.com/apache/kafka/blob/0.8.1/core/src/main/scala/kafka/network/BlockingChannel.scala
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DumpLogSegments tool fails when path has a '.',KAFKA-1732,12750821,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ewencp,ewencp,ewencp,27/Oct/14 17:09,28/Oct/14 04:14,14/Jul/23 05:39,27/Oct/14 23:26,0.8.1.1,,,0.8.2.0,,,,,,,tools,,,0,,,,"Using DumpLogSegments in a directory that has a '.' that isn't part of the file extension causes an exception:

{code}
16:48 $ time /Users/ewencp/kafka.git/bin/kafka-run-class.sh kafka.tools.DumpLogSegments  --file /Users/ewencp/kafka.git/system_test/replication_testsuite/testcase_1/logs/broker-3/kafka_server_3_logs/test_1-1/00000000000000016895.index --verify-index-only
Dumping /Users/ewencp/kafka.git/system_test/replication_testsuite/testcase_1/logs/broker-3/kafka_server_3_logs/test_1-1/00000000000000016895.index
Exception in thread ""main"" java.io.FileNotFoundException: /Users/ewencp/kafka.log (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at kafka.utils.Utils$.openChannel(Utils.scala:162)
	at kafka.log.FileMessageSet.<init>(FileMessageSet.scala:74)
	at kafka.tools.DumpLogSegments$.kafka$tools$DumpLogSegments$$dumpIndex(DumpLogSegments.scala:109)
	at kafka.tools.DumpLogSegments$$anonfun$main$1.apply(DumpLogSegments.scala:80)
	at kafka.tools.DumpLogSegments$$anonfun$main$1.apply(DumpLogSegments.scala:73)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:105)
	at kafka.tools.DumpLogSegments$.main(DumpLogSegments.scala:73)
	at kafka.tools.DumpLogSegments.main(DumpLogSegments.scala)
{code}",,ewencp,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/14 18:41;ewencp;KAFKA-1732.patch;https://issues.apache.org/jira/secure/attachment/12677361/KAFKA-1732.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 28 04:14:20 UTC 2014,,,,,,,,,,"0|i21mh3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Oct/14 18:41;ewencp;Created reviewboard https://reviews.apache.org/r/27238/diff/
 against branch origin/trunk;;;","27/Oct/14 23:26;nehanarkhede;Thanks for the patch. Pushed to trunk and 0.8.2;;;","28/Oct/14 04:14;nehanarkhede;Thanks [~charmalloc]. Missed updating the version myself.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix comment about message format,KAFKA-1727,12750046,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,Muneyuki Noguchi,Muneyuki Noguchi,Muneyuki Noguchi,23/Oct/14 12:56,23/Oct/14 21:53,14/Jul/23 05:39,23/Oct/14 21:53,,,,0.9.0.0,,,,,,,,,,0,,,,"The comment in Message.scala says the value of ""magic"" identifier is 2,

 bq. 2. 1 byte ""magic"" identifier to allow format changes, value is 2 currently

but the actual current ""magic"" value is 0.

{code}
  /**
   * The current ""magic"" value
   */
  val CurrentMagicValue: Byte = 0
{code}",,junrao,Muneyuki Noguchi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/14 13:08;Muneyuki Noguchi;KAFKA-1727.patch;https://issues.apache.org/jira/secure/attachment/12676575/KAFKA-1727.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 23 21:53:36 UTC 2014,,,,,,,,,,"0|i21hsf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/14 13:08;Muneyuki Noguchi;Created reviewboard https://reviews.apache.org/r/27075/diff/
 against branch origin/trunk;;;","23/Oct/14 21:53;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong message format description,KAFKA-1726,12750028,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,ovgolovin,ovgolovin,23/Oct/14 11:03,23/Oct/14 15:21,14/Jul/23 05:39,23/Oct/14 15:21,,,,,,,,,,,website,,,0,,,,"Here [in this page|https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Enriched+Message+Metadata#KafkaEnrichedMessageMetadata-CurrentMessageFormat] you describe current Kafka message format:
{code}
MessageAndOffset => MessageSize Offset Message
  MessageSize => int32
  Offset => int64
 
  Message => Crc MagicByte Attributes KeyLength Key ValueLength Value
    Crc => int32
    MagicByte => int8
    Attributes => int8
    KeyLength => int32
    Key => bytes
    ValueLength => int32
    Value => bytes
{code}

In reality _offset_ goes before _messageSize_.

",,guozhang,ovgolovin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 23 15:21:34 UTC 2014,,,,,,,,,,"0|i21hof:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Oct/14 15:21;guozhang;Thanks Oleg for pointing this out, I just fixed it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuration file bugs in system tests add noise to output and break a few tests,KAFKA-1725,12749927,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ewencp,ewencp,ewencp,22/Oct/14 23:46,24/Oct/14 16:15,14/Jul/23 05:39,24/Oct/14 16:15,,,,,,,,,,,tools,,,0,,,,There are some broken and misnamed system test configuration files (testcase_*_properties.json) that are causing a bunch of exceptions when running system tests and make it a lot harder to parse the output.,,ewencp,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/14 23:52;ewencp;KAFKA-1725.patch;https://issues.apache.org/jira/secure/attachment/12676477/KAFKA-1725.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 24 16:15:13 UTC 2014,,,,,,,,,,"0|i21h3j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Oct/14 23:52;ewencp;Created reviewboard https://reviews.apache.org/r/27060/diff/
 against branch origin/trunk;;;","24/Oct/14 16:15;nehanarkhede;Thanks for fixing the system tests! Pushed to trunk and 0.8.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Errors after reboot in single node setup,KAFKA-1724,12749836,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriharsha,hakman,hakman,22/Oct/14 18:03,24/Feb/15 22:23,14/Jul/23 05:39,24/Feb/15 22:23,0.8.2.0,,,0.9.0.0,,,,,,,,,,1,newbie,,,"In a single node setup, after reboot, Kafka logs show the following:
{code}
[2014-10-22 16:37:22,206] INFO [Controller 0]: Controller starting up (kafka.controller.KafkaController)
[2014-10-22 16:37:22,419] INFO [Controller 0]: Controller startup complete (kafka.controller.KafkaController)
[2014-10-22 16:37:22,554] INFO conflict in /brokers/ids/0 data: {""jmx_port"":-1,""timestamp"":""1413995842465"",""host"":""ip-10-91-142-54.eu-west-1.compute.internal"",""version"":1,""port"":9092} stored data: {""jmx_port"":-1,""timestamp"":""1413994171579"",""host"":""ip-10-91-142-54.eu-west-1.compute.internal"",""version"":1,""port"":9092} (kafka.utils.ZkUtils$)
[2014-10-22 16:37:22,736] INFO I wrote this conflicted ephemeral node [{""jmx_port"":-1,""timestamp"":""1413995842465"",""host"":""ip-10-91-142-54.eu-west-1.compute.internal"",""version"":1,""port"":9092}] at /brokers/ids/0 a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
[2014-10-22 16:37:25,010] ERROR Error handling event ZkEvent[Data of /controller changed sent to kafka.server.ZookeeperLeaderElector$LeaderChangeListener@a6af882] (org.I0Itec.zkclient.ZkEventThread)
java.lang.IllegalStateException: Kafka scheduler has not been started
        at kafka.utils.KafkaScheduler.ensureStarted(KafkaScheduler.scala:114)
        at kafka.utils.KafkaScheduler.shutdown(KafkaScheduler.scala:86)
        at kafka.controller.KafkaController.onControllerResignation(KafkaController.scala:350)
        at kafka.controller.KafkaController$$anonfun$2.apply$mcV$sp(KafkaController.scala:162)
        at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply$mcZ$sp(ZookeeperLeaderElector.scala:138)
        at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
        at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
        at kafka.utils.Utils$.inLock(Utils.scala:535)
        at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:134)
        at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)
        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[2014-10-22 16:37:28,757] INFO Registered broker 0 at path /brokers/ids/0 with address ip-10-91-142-54.eu-west-1.compute.internal:9092. (kafka.utils.ZkUtils$)
[2014-10-22 16:37:28,849] INFO [Kafka Server 0], started (kafka.server.KafkaServer)
[2014-10-22 16:38:56,718] INFO Closing socket connection to /127.0.0.1. (kafka.network.Processor)
[2014-10-22 16:38:56,850] INFO Closing socket connection to /127.0.0.1. (kafka.network.Processor)
[2014-10-22 16:38:56,985] INFO Closing socket connection to /127.0.0.1. (kafka.network.Processor)
{code}
The last log line repeats forever and is correlated with errors on the app side.
Restarting Kafka fixes the errors.

Steps to reproduce (with help from the mailing list):
# start zookeeper
# start kafka-broker
# create topic or start a producer writing to a topic
# stop zookeeper
# stop kafka-broker( kafka broker shutdown goes into  WARN Session
0x14938d9dc010001 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn) java.net.ConnectException: Connection refused)
# kill -9 kafka-broker
# restart zookeeper and than kafka-broker leads into the the error above
",,diederik,hakman,junrao,mazhar.shaikh.in,otis,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1596,,,,,,,,,,,,,,,,,,,"14/Nov/14 01:34;sriharsha;KAFKA-1724.patch;https://issues.apache.org/jira/secure/attachment/12681465/KAFKA-1724.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 24 00:20:56 UTC 2015,,,,,,,,,,"0|i21ghz:",9223372036854775807,,junrao,,,,,,,,,,,,,,,,,,"04/Nov/14 17:31;otis;[~harsha_ch] - I noticed you assigned yourself to this.  Are you working on this by any chance?;;;","04/Nov/14 17:39;sriharsha;[~otis] I started working on this. Will send a patch soon.;;;","04/Nov/14 18:07;otis;Great, thanks!  Still aiming for 0.8.2?;;;","14/Nov/14 01:34;sriharsha;Created reviewboard https://reviews.apache.org/r/28027/diff/
 against branch origin/trunk;;;","14/Nov/14 01:39;sriharsha;[~junrao] [~nehanarkhede]
This issue happens in a single node setup as per above steps. When the user brings up zookeeper and immediately starts a kafka broker 
ZookeeperLeaderElector will be able to read /controller data from zookeeper which will gets deleted as its a ephemeral node triggering
ZookeeperLeaderElector.handleDataDeleted calling KafkaController.onControllerResignation
as it tries shutdown KafkaScheduler which isn't started yet causing it throw up IllegalStateException. Please check the patch. Thanks.
;;;","02/Dec/14 17:18;sriharsha;[~junrao] can you please look at the reply to your review. Please let me know if this approach makes sense or not. I do see the kafka scheduler error in multi broker env too. ;;;","18/Jan/15 17:50;sriharsha;[~junrao] Can you please take a look at my reply to the review. Thanks.;;;","23/Feb/15 23:38;sriharsha;[~junrao] Thanks for the comments on the patch. So it looks like this is already fixed in the trunk.  We can close this JIRA.;;;","24/Feb/15 00:09;junrao;[~sriharsha], so, this is fixed as part of KAFKA-1760?;;;","24/Feb/15 00:20;sriharsha;[~junrao] Yes. We've isStarted in KafkaScheduler which gets set after its started and in shutdown we check isStarted and go through shutdown process.
Tested it  in a cluster to reproduce don't see any errors.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snappy compressor is not thread safe,KAFKA-1721,12749406,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ewencp,ewencp,ewencp,21/Oct/14 01:55,14/Nov/14 23:25,14/Jul/23 05:39,14/Nov/14 23:25,,,,0.8.2.0,,,,,,,compression,,,0,,,,"From the mailing list, it can generate this exception:

2014-10-20 18:55:21.841 [kafka-producer-network-thread] ERROR
org.apache.kafka.clients.producer.internals.Sender - Uncaught error in
kafka producer I/O thread:
*java.lang.NullPointerException*
at
org.xerial.snappy.BufferRecycler.releaseInputBuffer(BufferRecycler.java:153)
at org.xerial.snappy.SnappyOutputStream.close(SnappyOutputStream.java:317)
at java.io.FilterOutputStream.close(FilterOutputStream.java:160)
at org.apache.kafka.common.record.Compressor.close(Compressor.java:94)
at
org.apache.kafka.common.record.MemoryRecords.close(MemoryRecords.java:119)
at
org.apache.kafka.clients.producer.internals.RecordAccumulator.drain(RecordAccumulator.java:285)
at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:162)
at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:115)
at java.lang.Thread.run(Thread.java:744)

This appears to be an issue with the snappy-java library using ThreadLocal for an internal buffer recycling object which results in that object being shared unsafely across threads if one thread sends to multiple producers:

{quote}
I think the issue is that you're
using all your producers across a thread pool and the snappy library
uses ThreadLocal BufferRecyclers. When new Snappy streams are allocated,
they may be allocated from the same thread (e.g. one of your MyProducer
classes calls Producer.send() on multiple producers from the same
thread) and therefore use the same BufferRecycler. Eventually you hit
the code in the stacktrace, and if two producer send threads hit it
concurrently they improperly share the unsynchronized BufferRecycler.

This seems like a pain to fix -- it's really a deficiency of the snappy
library and as far as I can see there's no external control over
BufferRecycler in their API. One possibility is to record the thread ID
when we generate a new stream in Compressor and use that to synchronize
access to ensure no concurrent BufferRecycler access. That could be made
specific to snappy so it wouldn't impact other codecs. Not exactly
ideal, but it would work. Unfortunately I can't think of any way for you
to protect against this in your own code since the problem arises in the
producer send thread, which your code should never know about.

Another option would be to setup your producers differently to avoid the
possibility of unsynchronized access from multiple threads (i.e. don't
use the same thread pool approach), but whether you can do that will
depend on your use case.
{quote}",,Bmis13,ewencp,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/14 01:17;ewencp;KAFKA-1721.patch;https://issues.apache.org/jira/secure/attachment/12676804/KAFKA-1721.patch","28/Oct/14 16:25;ewencp;KAFKA-1721_2014-10-28_09:25:50.patch;https://issues.apache.org/jira/secure/attachment/12677626/KAFKA-1721_2014-10-28_09%3A25%3A50.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 14 23:25:05 UTC 2014,,,,,,,,,,"0|i21dvb:",9223372036854775807,,junrao,,,,,,,,,,,,,,,,,,"21/Oct/14 19:01;Bmis13;I have filled https://github.com/xerial/snappy-java/issues/88 for tracking for Snappy. 

There is patch provided and Thanks to [~ewencp] for testing the patch.  Please see above link for more details.


Thanks,

Bhavesh ;;;","23/Oct/14 18:32;Bmis13;[~ewencp],

Thanks for fixing this issue.  Snappy Dev has release new version with fix https://oss.sonatype.org/content/repositories/releases/org/xerial/snappy/snappy-java/1.1.1.4/ 

Thanks,
Bhavesh;;;","23/Oct/14 22:58;junrao;Bhavesh,

Do you want to submit a patch to upgrade the snappy jar in Kafka? Thanks,;;;","23/Oct/14 23:00;ewencp;I have the trivial patch, but the upstream jar seems to be broken (see the earlier Github issue). I'll follow up on this once that issue is resolved.;;;","24/Oct/14 01:17;ewencp;Created reviewboard https://reviews.apache.org/r/27124/diff/
 against branch origin/trunk;;;","28/Oct/14 16:25;ewencp;Updated reviewboard https://reviews.apache.org/r/27124/diff/
 against branch origin/trunk;;;","14/Nov/14 18:42;ewencp;[~junrao] This is a trivial version update patch. It would be nice for the fix to make it to 0.8.2, but I'm not sure we want to push a dependency version change between beta and final.;;;","14/Nov/14 23:25;junrao;Thanks for the patch. Since this change is trivial, double committed to 0.8.2 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Renaming / Comments] Delayed Operations,KAFKA-1720,12749365,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,20/Oct/14 22:09,17/May/16 14:31,14/Jul/23 05:39,16/Dec/14 07:21,,,,0.9.0.0,,,,,,,,,,0,,,,"After KAFKA-1583 checked in, we would better renaming the delayed requests to delayed operations.",,guozhang,jjkoshy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1583,,"31/Oct/14 16:52;guozhang;KAFKA-1720.patch;https://issues.apache.org/jira/secure/attachment/12678493/KAFKA-1720.patch","01/Nov/14 00:21;guozhang;KAFKA-1720_2014-10-31_17:21:46.patch;https://issues.apache.org/jira/secure/attachment/12678625/KAFKA-1720_2014-10-31_17%3A21%3A46.patch","03/Dec/14 21:34;guozhang;KAFKA-1720_2014-12-03_13:34:13.patch;https://issues.apache.org/jira/secure/attachment/12684973/KAFKA-1720_2014-12-03_13%3A34%3A13.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 16 07:21:22 UTC 2014,,,,,,,,,,"0|i21dmf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Oct/14 16:52;guozhang;Created reviewboard https://reviews.apache.org/r/27430/diff/
 against branch origin/trunk;;;","01/Nov/14 00:21;guozhang;Updated reviewboard https://reviews.apache.org/r/27430/diff/
 against branch origin/trunk;;;","03/Dec/14 21:34;guozhang;Updated reviewboard https://reviews.apache.org/r/27430/diff/
 against branch origin/trunk;;;","16/Dec/14 07:21;jjkoshy;Guozhang committed this to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove netty dependency through ZK 3.4.x,KAFKA-1717,12749272,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,junrao,junrao,20/Oct/14 15:13,20/Oct/14 18:10,14/Jul/23 05:39,20/Oct/14 18:10,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"ZK 3.4.x specifies a dependency on netty. However, that dependency is optional (ZOOKEEPER-1681). To avoid potential jar conflict, it's better to exclude netty dependency from Kafka.",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/14 15:16;junrao;kafka-1717.patch;https://issues.apache.org/jira/secure/attachment/12675842/kafka-1717.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 20 18:10:19 UTC 2014,,,,,,,,,,"0|i21d0f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/14 15:16;junrao;Created reviewboard https://reviews.apache.org/r/26936/diff/
 against branch origin/trunk;;;","20/Oct/14 18:10;junrao;Thanks for the reviews. Committed to trunk and 0.8.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
more better bootstrapping of the gradle-wrapper.jar ,KAFKA-1714,12749044,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,granthenke,joestein,joestein,18/Oct/14 13:44,01/Dec/19 21:03,14/Jul/23 05:39,22/Nov/19 06:32,0.8.2.0,,,1.0.3,1.1.2,2.0.2,2.1.2,2.2.3,2.3.2,2.4.0,build,,,1,,,,From https://issues.apache.org/jira/browse/KAFKA-1490 we moved out the gradle-wrapper.jar for our source maintenance. This makes builds for folks coming in the first step somewhat problematic.  A bootstrap step is required if this could be somehow incorporated that would be great.,,githubbot,joestein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-2124,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 21 16:06:11 UTC 2019,,,,,,,,,,"0|i21blz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Dec/18 18:10;githubbot;granthenke opened a new pull request #6031: KAFKA-1714: Fix gradle wrapper bootstrapping
URL: https://github.com/apache/kafka/pull/6031
 
 
   Given we need to follow the Apache rule of not checking
   any binaries into the source code, Kafka has always had
   a bit of a tricky Gradle bootstrap.
   Using ./gradlew as users expect doesn’t work and a
   local and compatible version of Gradle was required to
   generate the wrapper first.
   
   This patch changes the behavior of the wrapper task to
   instead generate a gradlew script that can bootstrap the
   jar itself. Additionally it adds a license, removes the bat
   script, and handles retries.
   
   The documentation in the readme was also updated.
   
   Going forward patches that upgrade gradle should run
   `gradle wrapper` before checking in the change.
   
   With this change users using ./gradlew can be sure they
   are always building with the correct version of Gradle.
   
   *More detailed description of your change,
   if necessary. The PR title and PR message become
   the squashed commit message, so use a separate
   comment to ping reviewers.*
   
   *Summary of testing strategy (including rationale)
   for the feature or bug fix. Unit and/or integration
   tests are expected for any behaviour change and
   system tests should be considered for larger changes.*
   
   ### Committer Checklist (excluded from commit message)
   - [ ] Verify design and implementation 
   - [ ] Verify test coverage and CI build status
   - [ ] Verify documentation (including upgrade notes)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","21/Nov/19 16:06;githubbot;ijuma commented on pull request #6031: KAFKA-1714: Fix gradle wrapper bootstrapping
URL: https://github.com/apache/kafka/pull/6031
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Excessive storage usage on newly added node,KAFKA-1712,12748951,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,ovgolovin,ovgolovin,17/Oct/14 19:09,28/Aug/18 19:08,14/Jul/23 05:39,28/Aug/18 19:08,,,,,,,,,,,log,,,5,,,,"When a new node is added to cluster data starts replicating into it. The mtime of creating segments will be set on the last message being written to them. Though the replication is a prolonged process, let's assume (for simplicity of explanation) that their mtime is very close to the time when the new node was added.

After the replication is done, new data will start to flow into this new node. After `log.retention.hours` the amount of data will be 2 * daily_amount_of_data_in_kafka_node (first one is the replicated data from other nodes when the node was added (let us call it `t1`) and the second is the amount of replicated data from other nodes which happened from `t1` to `t1 + log.retention.hours`). So by that time the node will have twice as much data as the other nodes.

This poses a big problem to us as our storage is chosen to fit normal amount of data (not twice this amount).

In our particular case it poses another problem. We have an emergency segment cleaner which runs in case storage is nearly full (>90%). We try to balance the amount of data for it not to run to rely solely on kafka internal log deletion, but sometimes emergency cleaner runs.
It works this way:
- it gets all kafka segments for the volume
- it filters out last segments of each partition (just to avoid unnecessary recreation of last small-size segments)
- it sorts them by segment mtime
- it changes mtime of the first N segements (with the lowest mtime) to 1, so they become really really old. Number N is chosen to free specified percentage of volume (3% in our case).  Kafka deletes these segments later (as they are very old).

Emergency cleaner works very well. Except for the case when the data is replicated to the newly added node. 
In this case segment mtime is the time the segment was replicated and does not reflect the real creation time of original data stored in this segment.
So in this case kafka emergency cleaner will delete segments with the lowest mtime, which may hold the data which is much more recent than the data in other segments.
This is not a big problem until we delete the data which hasn't been fully consumed.
In this case we loose data and this makes it a big problem.

Is it possible to retain segment mtime during initial replication on a new node?
This will help not to load the new node with the twice as large amount of data as other nodes have.

Or maybe there are another ways to sort segments by data creation times (or close to data creation time)? (for example if this ticket is implemented https://issues.apache.org/jira/browse/KAFKA-1403, we may take time of the first message from .index). In our case it will help with kafka emergency cleaner, which will be deleting really the oldest data.",,abraithwaite,aozeritsky,ataraxer,bobrik,i.galic,junrao,omkreddy,ovgolovin,Pegerto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 28 19:08:07 UTC 2018,,,,,,,,,,"0|i21b1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Oct/14 21:25;junrao;This is being discussed in https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Enriched+Message+Metadata;;;","28/Feb/17 23:51;abraithwaite;Has this been looked at recently?  We've found it's an issue for nodes which are coming back into a cluster as well.;;;","28/Aug/18 19:08;omkreddy;Fixed via https://issues.apache.org/jira/browse/KAFKA-2511;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WARN Property topic is not valid when running console producer,KAFKA-1711,12748914,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,joecrobak,junrao,junrao,17/Oct/14 16:56,12/Feb/15 21:16,14/Jul/23 05:39,24/Oct/14 00:08,0.8.2.0,,,0.9.0.0,,,,,,,core,,,0,newbie,,,"bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 
[2014-10-17 09:54:23,984] WARN Property topic is not valid (kafka.utils.VerifiableProperties)

It would be good if we can get rid of the warning.",,gwenshap,jelmer1,joecrobak,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/14 01:18;joecrobak;KAFKA-1711.patch;https://issues.apache.org/jira/secure/attachment/12676002/KAFKA-1711.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 12 21:16:41 UTC 2015,,,,,,,,,,"0|i21atr:",9223372036854775807,,junrao,,,,,,,,,,,,,,,,,,"21/Oct/14 01:18;joecrobak;Here's a patch that fixes the warning.;;;","24/Oct/14 00:08;junrao;Thanks for the patch. Committed to trunk.;;;","12/Feb/15 16:15;jelmer1;Issue should be reopened, the fix got reverted as part of https://issues.apache.org/jira/browse/KAFKA-1824;;;","12/Feb/15 21:16;gwenshap;Since KAFKA-1824 is open, I don't think we need both?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The bat script failed to start on windows,KAFKA-1703,12747637,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,chrisrc,chrisrc,13/Oct/14 03:38,06/Sep/17 09:13,14/Jul/23 05:39,06/Sep/17 09:13,0.8.1.1,,,,,,,,,,build,,,0,newbie,,,"The bat script in bin\windows can not start zookeeper and kafka correctly (where my os is just installed and only jdk ready). I modified the kafka-run-class.bat and add jars in libs folder to classpath.

for %%i in (%BASE_DIR%\core\lib\*.jar) do (
	call :concat %%i
)

#### added  begin####
for %%i in (%BASE_DIR%\..\libs\*.jar) do (
	call :concat %%i
)
#### added  end####

for %%i in (%BASE_DIR%\perf\target\scala-2.8.0/kafka*.jar) do (
	call :concat %%i
)


Now it runs correctly.

Under bin\windows:
    zookeeper-server-start.bat ..\..\config\zookeeper.properties

   kafka-server-start.bat ..\..\config\kafka.properties",,andrew.musselman,chrisrc,JK,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/15 01:14;JK;kafka-run-class.bat;https://issues.apache.org/jira/secure/attachment/12694056/kafka-run-class.bat",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 06 09:13:14 UTC 2017,,,,,,,,,,"0|i21387:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Jan/15 01:17;JK;modify 
set BASE_DIR=%CD%\..
to 
set BASE_DIR=%CD%\..\..

and add classpath for release version:
for %%i in (%BASE_DIR%\libs\*.jar) do (
	call :concat %%i
);;;","17/Oct/16 18:27;andrew.musselman;Has this been vetted/merged?;;;","06/Sep/17 09:13;omkreddy;This was fixed in newer versions.  Pl reopen if you think the issue still exists
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Messages silently Lost by producer,KAFKA-1702,12747623,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,alexismidon,alexismidon,alexismidon,12/Oct/14 23:42,13/Oct/14 23:17,14/Jul/23 05:39,13/Oct/14 23:17,0.8.1.1,,,0.9.0.0,,,,,,,producer ,,,0,,,,"
Hello,

we lost millions of messages because of this {{try/catch}} in  the producer {{DefaultEventHandler}}:
https://github.com/apache/kafka/blob/0.8.1/core/src/main/scala/kafka/producer/async/DefaultEventHandler.scala#L114-L116

If a Throwable is caught by this {{try/catch}}, the retry policy will have no effect and all yet-to-be-sent messages are lost (the error will break the loop over the broker list).
This issue is very hard to detect because: the producer (async or sync) cannot even catch the error, and *all* the metrics are updated as if everything was fine.

Only the abnormal drop in the producers network I/O, or the incoming message rate on the brokers; or the alerting on errors in producer logs could have revealed the issue. 

This behavior was introduced by KAFKA-300. I can't see a good reason for it, so here is a patch that will let the retry-policy do its job when such a {{Throwable}} occurs.

Thanks in advance for your help.

Alexis

ps: you might wonder how could this {{try/catch}} ever caught something? {{DefaultEventHandler#groupMessagesToSet}} looks so harmless. 

Here are the details:
We use Snappy compression. When the native snappy library is not installed on the host, Snappy, during the initialization of class {{org.xerial.snappy.Snappy}}  will [write a C library|https://github.com/xerial/snappy-java/blob/1.1.0/src/main/java/org/xerial/snappy/SnappyLoader.java#L312] in the JVM temp directory {{java.io.tmpdir}}.

In our scenario, {{java.io.tmpdir}} was a subdirectory of {{/tmp}}. After an instance reboot (thank you [AWS|https://twitter.com/hashtag/AWSReboot?src=hash]!), the JVM temp directory was removed. The JVM was then running with a non-existing temp dir. Snappy class would be impossible to initialize and the following message would be silently logged:

{code}
ERROR [2014-10-07 22:23:56,530] kafka.producer.async.DefaultEventHandler: Failed to send messages
! java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy
{code}

",,alexismidon,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/14 23:45;alexismidon;KAFKA-1702.0.patch;https://issues.apache.org/jira/secure/attachment/12674438/KAFKA-1702.0.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 13 23:17:03 UTC 2014,,,,,,,,,,"0|i21353:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Oct/14 04:15;junrao;Thanks for the patch. Not sure why you want to wrap groupMessagesToSet in try/catch. I thought that's where the NoClassDefFoundError is thrown and you want to propagate it to the caller.

With this patch, the producer in sync mode will get the exception. However, producer in async mode will still silently dropping messages. This is a general limitation and is being fixed in the new java producer through a callback.;;;","13/Oct/14 04:32;alexismidon;I agree. In the async mode, until there is a callback, the best we can do is to make sure all the metrics are updated correctly, in particular ResendsPerSec, FailedSendsPerSec, which is critical for monitoring of async producers.

In the sync mode, producer will get the exception, which is an improvement.

thanks for your review;;;","13/Oct/14 04:35;alexismidon;Also if #groupMessagesToSet is not in a try/catch, the error will break the loop on the broker list. All messages will get dropped, retries ignored, metrics won't get updated, etc.;;;","13/Oct/14 23:17;junrao;Got it. +1 for the patch. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
examples directory - README and shell scripts are out of date,KAFKA-1700,12747405,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,granders,granders,granders,10/Oct/14 20:40,23/Oct/14 23:14,14/Jul/23 05:39,23/Oct/14 23:14,0.8.1,,,0.9.0.0,,,,,,,,,,0,newbie,,,"sbt build files were removed during resolution of KAFKA-1254, so the README under the examples directory should no longer make reference to sbt.

Also, the paths added to CLASSPATH variable in the example shell script are no longer correct.",,granders,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/14 20:57;granders;KAFKA-1700.patch;https://issues.apache.org/jira/secure/attachment/12674255/KAFKA-1700.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 23 23:14:42 UTC 2014,,,,,,,,,,"0|i211u7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/14 20:57;granders;Created reviewboard https://reviews.apache.org/r/26575/diff/
 against branch origin/trunk;;;","23/Oct/14 23:14;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
autoRebalanceScheduler.shutdown() causes deadlock while controller shutting down,KAFKA-1699,12747179,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriharsha,sriharsha,sriharsha,10/Oct/14 01:51,10/Oct/14 04:11,14/Jul/23 05:39,10/Oct/14 04:11,0.8.2.0,,,0.8.2.0,,,,,,,,,,0,,,,when a controller shutdown starts with auto.leader.rebalance.enable set to true. Controller shutdown completes whille the process waits on autoRebalanceScheduler.shutdown().,,junrao,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1305,,,,,,,,,,,,,"10/Oct/14 02:03;sriharsha;KAFKA-1699.patch;https://issues.apache.org/jira/secure/attachment/12674082/KAFKA-1699.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 10 04:11:18 UTC 2014,,,,,,,,,,"0|i210hb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/14 02:03;sriharsha;Created reviewboard https://reviews.apache.org/r/26537/diff/
 against branch origin/trunk;;;","10/Oct/14 04:11;junrao;Thanks for the patch. +1. Committed to trunk and 0.8.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validator.ensureValid() only validates default config value,KAFKA-1698,12747178,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ewencp,junrao,junrao,10/Oct/14 01:37,21/Oct/14 01:03,14/Jul/23 05:39,21/Oct/14 01:03,,,,0.9.0.0,,,,,,,core,,,0,newbie++,,,We should use it to validate the actual configured value.,,ewencp,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/14 00:16;ewencp;KAFKA-1698.patch;https://issues.apache.org/jira/secure/attachment/12674650/KAFKA-1698.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 21 01:03:40 UTC 2014,,,,,,,,,,"0|i210h3:",9223372036854775807,,junrao,,,,,,,,,,,,,,,,,,"14/Oct/14 00:17;ewencp;Created reviewboard https://reviews.apache.org/r/26667/diff/
 against branch origin/trunk;;;","21/Oct/14 01:03;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove code related to ack>1 on the broker,KAFKA-1697,12747177,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,gwenshap,junrao,junrao,10/Oct/14 01:33,05/Jan/16 22:37,14/Jul/23 05:39,13/Feb/15 21:09,,,,0.9.0.0,,,,,,,,,,0,,,,"We removed the ack>1 support from the producer client in kafka-1555. We can completely remove the code in the broker that supports ack>1.

Also, we probably want to make NotEnoughReplicasAfterAppend a non-retriable exception and let the client decide what to do.",,ewencp,gwenshap,jjkoshy,joestein,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-3067,,,,,,,,,,"07/Jan/15 00:20;gwenshap;KAFKA-1697.patch;https://issues.apache.org/jira/secure/attachment/12690448/KAFKA-1697.patch","14/Jan/15 23:41;gwenshap;KAFKA-1697_2015-01-14_15:41:37.patch;https://issues.apache.org/jira/secure/attachment/12692378/KAFKA-1697_2015-01-14_15%3A41%3A37.patch","11/Feb/15 01:06;gwenshap;KAFKA-1697_2015-02-10_17:06:51.patch;https://issues.apache.org/jira/secure/attachment/12697948/KAFKA-1697_2015-02-10_17%3A06%3A51.patch","12/Feb/15 02:45;gwenshap;KAFKA-1697_2015-02-11_18:45:42.patch;https://issues.apache.org/jira/secure/attachment/12698288/KAFKA-1697_2015-02-11_18%3A45%3A42.patch","12/Feb/15 02:48;gwenshap;KAFKA-1697_2015-02-11_18:47:53.patch;https://issues.apache.org/jira/secure/attachment/12698291/KAFKA-1697_2015-02-11_18%3A47%3A53.patch","12/Feb/15 07:14;gwenshap;KAFKA-1697_2015-02-11_23:13:53.patch;https://issues.apache.org/jira/secure/attachment/12698342/KAFKA-1697_2015-02-11_23%3A13%3A53.patch","13/Feb/15 02:57;gwenshap;KAFKA-1697_2015-02-12_18:57:36.patch;https://issues.apache.org/jira/secure/attachment/12698610/KAFKA-1697_2015-02-12_18%3A57%3A36.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 13 21:09:10 UTC 2015,,,,,,,,,,"0|i210gv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Oct/14 01:35;junrao;We probably can wait under KAFKA-1583 is done since code change will likely be easier then.;;;","07/Jan/15 00:20;gwenshap;Created reviewboard https://reviews.apache.org/r/29647/diff/
 against branch trunk;;;","07/Jan/15 00:23;gwenshap;Either I'm missing something or this was trivial :)

From what I can see:
If acks = 0 || acks = 1, we only append to local log.
If acks = -1, we produce with delay.
So I simply removed the part where we count acks in delayed produce. 

;;;","07/Jan/15 17:51;ewencp;On a related note (which maybe should have been caught in KAFKA-1555), the docs on the broker configs is now kind of misleading. Both old and new versions say something like ""common values are"", but really the values listed are the only valid options.;;;","14/Jan/15 23:41;gwenshap;Updated reviewboard https://reviews.apache.org/r/29647/diff/
 against branch trunk;;;","14/Jan/15 23:43;gwenshap;Per [~ewencp]'s suggestion, I added early validation of ack values. 

I also wanted to test that validation, which ended up being a rather more involved test than I wanted.

If anyone has suggestions on a simpler way to test API handlers (i.e. without a SocketServer), I'm all ears.;;;","15/Jan/15 18:11;joestein;With this patch I think we should change the existing functionality with > 1 update to start to LOG as a WARN in the Broker (so it gets people attention to stop using ack >1) but keep everything else the same... the new version of the request (with a match/case) should do the new functionality and we support both.;;;","22/Jan/15 00:49;junrao;From the discussion in the mailing list, we decided not to bump up the version for the ProduceRequest. Instead, we will log a warning in 0.8.2 that ack>1 will no longer to supported. In 0.8.3, we will throw an exception to requests with ack>1 and remove the support from the code. [~gwenshap], do you want to update KIP-1 in the wiki accordingly?;;;","22/Jan/15 00:59;gwenshap;Updated! ;;;","11/Feb/15 01:06;gwenshap;Updated reviewboard https://reviews.apache.org/r/29647/diff/
 against branch trunk;;;","12/Feb/15 02:45;gwenshap;Updated reviewboard https://reviews.apache.org/r/29647/diff/
 against branch trunk;;;","12/Feb/15 02:48;gwenshap;Updated reviewboard https://reviews.apache.org/r/29647/diff/
 against branch trunk;;;","12/Feb/15 07:14;gwenshap;Updated reviewboard https://reviews.apache.org/r/29647/diff/
 against branch trunk;;;","13/Feb/15 02:57;gwenshap;Updated reviewboard https://reviews.apache.org/r/29647/diff/
 against branch trunk;;;","13/Feb/15 21:09;jjkoshy;Thanks for the patch. Pushed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue sending more messages to single Kafka server (Load testing for Kafka transport),KAFKA-1693,12746933,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,kathees,kathees,09/Oct/14 05:49,26/Jan/15 05:59,14/Jul/23 05:39,26/Jan/15 05:59,0.8.1.1,,,,,,,,,,,,,0,,,,"I tried to send 50000 messages to single Kafka server.I sent the messages to ESB using JMeter and ESB sent to Kafka server. After 28000 message I am getting following exception.Do I need to change any parameter value in Kafka server.Please give me the solution.
 
[2014-10-06 11:41:05,182] ERROR - Utils$ fetching topic metadata for topics [Set(test1)] from broker [ArrayBuffer(id:0,host:localhost,port:9092)] failed
kafka.common.KafkaException: fetching topic metadata for topics [Set(test1)] from broker [ArrayBuffer(id:0,host:localhost,port:9092)] failed
at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:67)
at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82)
at kafka.producer.async.DefaultEventHandler$$anonfun$handle$1.apply$mcV$sp(DefaultEventHandler.scala:67)
at kafka.utils.Utils$.swallow(Utils.scala:167)
at kafka.utils.Logging$class.swallowError(Logging.scala:106)
at kafka.utils.Utils$.swallowError(Utils.scala:46)
at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:67)
at kafka.producer.Producer.send(Producer.scala:76)
at kafka.javaapi.producer.Producer.send(Producer.scala:33)
at org.wso2.carbon.connector.KafkaProduce.send(KafkaProduce.java:71)
at org.wso2.carbon.connector.KafkaProduce.connect(KafkaProduce.java:27)
at org.wso2.carbon.connector.core.AbstractConnector.mediate(AbstractConnector.java:32)
at org.apache.synapse.mediators.ext.ClassMediator.mediate(ClassMediator.java:78)
at org.apache.synapse.mediators.AbstractListMediator.mediate(AbstractListMediator.java:77)
at org.apache.synapse.mediators.AbstractListMediator.mediate(AbstractListMediator.java:47)
at org.apache.synapse.mediators.template.TemplateMediator.mediate(TemplateMediator.java:77)
at org.apache.synapse.mediators.template.InvokeMediator.mediate(InvokeMediator.java:129)
at org.apache.synapse.mediators.template.InvokeMediator.mediate(InvokeMediator.java:78)
at org.apache.synapse.mediators.AbstractListMediator.mediate(AbstractListMediator.java:77)
at org.apache.synapse.mediators.AbstractListMediator.mediate(AbstractListMediator.java:47)
at org.apache.synapse.mediators.base.SequenceMediator.mediate(SequenceMediator.java:131)
at org.apache.synapse.core.axis2.ProxyServiceMessageReceiver.receive(ProxyServiceMessageReceiver.java:166)
at org.apache.axis2.engine.AxisEngine.receive(AxisEngine.java:180)
at org.apache.synapse.transport.passthru.ServerWorker.processNonEntityEnclosingRESTHandler(ServerWorker.java:344)
at org.apache.synapse.transport.passthru.ServerWorker.processEntityEnclosingRequest(ServerWorker.java:385)
at org.apache.synapse.transport.passthru.ServerWorker.run(ServerWorker.java:183)
at org.apache.axis2.transport.base.threads.NativeWorkerPool$1.run(NativeWorkerPool.java:172)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.BindException: Cannot assign requested address
at sun.nio.ch.Net.connect0(Native Method)
at sun.nio.ch.Net.connect(Net.java:465)
at sun.nio.ch.Net.connect(Net.java:457)
at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:670)
at kafka.network.BlockingChannel.connect(BlockingChannel.scala:57)
at kafka.producer.SyncProducer.connect(SyncProducer.scala:141)
at kafka.producer.SyncProducer.getOrMakeConnection(SyncProducer.scala:156)
at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:68)
at kafka.producer.SyncProducer.send(SyncProducer.scala:112)
at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:53)","Ubuntu 14, Java 6",junrao,kathees,Rasmeet Devji,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 26 05:59:35 UTC 2015,,,,,,,,,,"0|i20z07:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Oct/14 16:54;junrao;You may want to see if this is related to what's described in the following article.

http://javarevisited.blogspot.com/2014/02/fixing-javanetbindexception-cannot-assign-requested-address-JVMbind.html;;;","10/Oct/14 07:58;kathees;Thanks for your quick response.
I changed to 127.0.0.1 IP address for localhost  As said  in the above link. Still I am getting same exception. Could you please share  kafka server.properties file or give a solution to solve this problem.

kafka.common.KafkaException: fetching topic metadata for topics [Set(test1)] from broker [ArrayBuffer(id:0,host:127.0.0.1,port:9092)] failed
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:67)
	at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82)
	at kafka.producer.async.DefaultEventHandler$$anonfun$handle$2.apply$mcV$sp(DefaultEventHandler.scala:78)
	at kafka.utils.Utils$.swallow(Utils.scala:167)
	at kafka.utils.Logging$class.swallowError(Logging.scala:106)
	at kafka.utils.Utils$.swallowError(Utils.scala:46)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:78)
	at kafka.producer.Producer.send(Producer.scala:76)
	at kafka.javaapi.producer.Producer.send(Producer.scala:33)
	at org.wso2.carbon.connector.KafkaProduce.send(KafkaProduce.java:76)
	at org.wso2.carbon.connector.KafkaProduce.connect(KafkaProduce.java:27)
	at org.wso2.carbon.connector.core.AbstractConnector.mediate(AbstractConnector.java:32)
	at org.apache.synapse.mediators.ext.ClassMediator.mediate(ClassMediator.java:78)
	at org.apache.synapse.mediators.AbstractListMediator.mediate(AbstractListMediator.java:77)
	at org.apache.synapse.mediators.AbstractListMediator.mediate(AbstractListMediator.java:47)
	at org.apache.synapse.mediators.template.TemplateMediator.mediate(TemplateMediator.java:77)
	at org.apache.synapse.mediators.template.InvokeMediator.mediate(InvokeMediator.java:129)
	at org.apache.synapse.mediators.template.InvokeMediator.mediate(InvokeMediator.java:78)
	at org.apache.synapse.mediators.AbstractListMediator.mediate(AbstractListMediator.java:77)
	at org.apache.synapse.mediators.AbstractListMediator.mediate(AbstractListMediator.java:47)
	at org.apache.synapse.mediators.base.SequenceMediator.mediate(SequenceMediator.java:131)
	at org.apache.synapse.core.axis2.ProxyServiceMessageReceiver.receive(ProxyServiceMessageReceiver.java:166)
	at org.apache.axis2.engine.AxisEngine.receive(AxisEngine.java:180)
	at org.apache.synapse.transport.passthru.ServerWorker.processNonEntityEnclosingRESTHandler(ServerWorker.java:344)
	at org.apache.synapse.transport.passthru.ServerWorker.processEntityEnclosingRequest(ServerWorker.java:385)
	at org.apache.synapse.transport.passthru.ServerWorker.run(ServerWorker.java:183)
	at org.apache.axis2.transport.base.threads.NativeWorkerPool$1.run(NativeWorkerPool.java:172)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.BindException: Cannot assign requested address
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:465)
	at sun.nio.ch.Net.connect(Net.java:457)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:670)
	at kafka.network.BlockingChannel.connect(BlockingChannel.scala:57)
	at kafka.producer.SyncProducer.connect(SyncProducer.scala:141)
	at kafka.producer.SyncProducer.getOrMakeConnection(SyncProducer.scala:156)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:68)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:112)
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:53)
;;;","22/Dec/14 16:37;Rasmeet Devji;Faced the same issue with Kafka 811, Java 6 on CentOS release 5.8 (Final)

Was able to resolve it by not opening too many connections to Kafka server.

As this link http://stackoverflow.com/questions/6145108/problem-running-into-java-net-bindexception-cannot-assign-requested-address  mentions – 
By default, Linux picks dynamically assigned ports from the range 32768..61000. The others are available for static assignment, if you bind to a specific port number. The range can be changed if you want more of the ports to be available for dynamic assignment, but just be careful that you do not include ports that are used for specific services that you need (e.g. 6000 for X11). Also you should not allow ports < 1024 to be dynamically assigned since they are privileged.

What you are most likely doing is creating a producer instance for every message you send. Instead you should have a limited number of producers or may be a pool of producers for your tests. ;;;","26/Jan/15 05:59;kathees;Thanks Rasmeet. The issue is resolved.

Thanks,
Kathees;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Newly elected KafkaController might not start deletion of pending topics,KAFKA-1681,12746415,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriharsha,sriharsha,sriharsha,07/Oct/14 14:43,09/Oct/14 21:35,14/Jul/23 05:39,09/Oct/14 21:35,,,,0.8.2.0,,,,,,,,,,0,,,,As part of KAFKA-1663 deleteTopicStateChanged.set(true) is removed from start(). This will cause newly elected kafka controller not to process the existing delete topic requests.,,junrao,nehanarkhede,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1663,,,,"09/Oct/14 19:53;sriharsha;KAFKA-1681.patch;https://issues.apache.org/jira/secure/attachment/12673979/KAFKA-1681.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 09 21:35:41 UTC 2014,,,,,,,,,,"0|i20vvj:",9223372036854775807,,nehanarkhede,,,,,,,,,,,,,,,,,,"08/Oct/14 15:13;sriharsha;[~nehanarkhede] [~junrao]
adding this code to TopicDeletionManager.start method should cover this issue right?
      if (topicsToBeDeleted.size > 0)
        resumeTopicDeletionThread()
;;;","09/Oct/14 04:27;junrao;Yes, or just set deleteTopicStateChanged to true instead of calling resumeTopicDeletionThread().;;;","09/Oct/14 15:20;nehanarkhede;[~sriharsha], adding that to the end of the start() API might unnecessarily start the thread, possibly have it wait and then signal it. I think it is easier to just set deleteTopicStateChanged to true before starting the thread. Also, I thought about the other possible race condition where the controller completes starting the delete topic manager and effectively the delete topic thread and a watch related to delete topic fires before the delete topic thread waits on the condition. The controller watch would then call resumeTopicDeletion which would signal a non waiting thread. That seemed like a problem at first, but it's not since when the thread invokes awaitTopicDeletion.., it would find the deleteTopicStateChanged to be true, so it will proceed to acting on topicsToBeDeleted.

In short, the previous code was right :);;;","09/Oct/14 15:58;sriharsha;[~nehanarkhede] can you clarify bit on ""In short, the previous code was right "" . You mean code before KAFKA-1663. 
That code only waits till the thread starts running since the condition is
while(!deleteTopicsThread.isRunning.get() && !deleteTopicStateChanged.compareAndSet(true, false)) {
once thread starts and if there is resumeTopicsThread() invoked thread never waits because ""!deleteTopicsThread.isRunning.get()"". This causes a problem when deleteTopicsThread.shudown() is called . I believe the intention is to wait for notifications on delete topics and resume thread. With the above condition it won't wait and keeps running. ;;;","09/Oct/14 16:49;junrao;Sriharsha,

I think what Neha meant is that in kafka-1663, the change of the condition in awaitTopicDeletionNotification() is correct. However, kafka-1663 also removed setting deleteTopicStateChanged to true in start() and is causing a new problem. We can just add the following code in start() before the deleteTopicsThread is started.
if (topicsToBeDeleted.size > 0)
  deleteTopicStateChanged.set(true);;;","09/Oct/14 17:49;nehanarkhede;[~sriharsha], sorry for being unclear. I meant that the regression introduced by part of the KAFKA-1663 patch where you removed deleteTopicStateChanged.set(true) needs to be reverted. The other part of the patch where you changed the while loop condition is correct. So the patch for this JIRA is a one liner fix where we add that change(deleteTopicStateChanged.set(true)) back in. ;;;","09/Oct/14 19:53;sriharsha;Created reviewboard https://reviews.apache.org/r/26516/diff/
 against branch origin/trunk;;;","09/Oct/14 21:35;junrao;Thanks for the patch. +1. Committed to both trunk and 0.8.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JmxTool outputs nothing if any mbean attributes can't be retrieved,KAFKA-1679,12746310,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jrafalski,rberdeen,rberdeen,07/Oct/14 01:18,12/Sep/15 03:54,14/Jul/23 05:39,12/Sep/15 03:54,,,,0.9.0.0,,,,,,,tools,,,0,newbie,,,"JmxTool counts the number of attributes for all MBeans and if the number of attributes retrieved does not equal this number, nothing is printed.

Several {{java.lang:type=MemoryPool}} MBeans have unsupported attributes (see HADOOP-8027, for example), so running JmxTool with no arguments fails to fetch these metrics and outputs nothing while continuing to run.",,jrafalski,omkreddy,qwertymaniac,rberdeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-2278,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 12 03:54:22 UTC 2015,,,,,,,,,,"0|i20v8f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Sep/15 03:54;omkreddy;This got fixed in KAFKA-2278;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bootstrapping tidy-up,KAFKA-1675,12746125,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,edgefox,szczepiq,szczepiq,06/Oct/14 12:53,29/Aug/17 20:24,14/Jul/23 05:39,29/Aug/17 20:24,,,,,,,,,,,,,,0,,,,"I'd like to suggest following changes:

1. remove the 'gradlew' and 'gradlew.bat' scripts from the source tree. Those scripts don't work, e.g. they fail with exception when invoked. I just got a user report where those scripts were invoked by the user and it led to an exception that was not easy to grasp. Bootstrapping step will generate those files anyway.

2. move the 'gradleVersion' extra property from the 'build.gradle' into 'gradle.properties'. Otherwise it is hard to automate the bootstrapping process - in order to find out the gradle version, I need to evaluate the build script, and for that I need gradle with correct version (kind of a vicious circle). Project properties declared in the gradle.properties file can be accessed exactly the same as the 'ext' properties, for example: 'project.gradleVersion'.",,edgefox,junrao,omkreddy,szczepiq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/14 14:07;edgefox;KAFKA-1675.patch;https://issues.apache.org/jira/secure/attachment/12673091/KAFKA-1675.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 29 20:24:35 UTC 2017,,,,,,,,,,"0|i20u47:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Oct/14 14:07;edgefox;Created reviewboard https://reviews.apache.org/r/26362/diff/
 against branch apache/trunk;;;","23/Oct/14 23:06;junrao;Thanks for the patch. Looks good. It doesn't seem to apply any more. Could you rebase?;;;","29/Aug/17 20:24;omkreddy; gradlew, gradlew.bat scripts are removed from repo. Pl reopen if you think the issue still exists
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
potential  java.lang.IllegalStateException in BufferPool.allocate(),KAFKA-1673,12746032,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,06/Oct/14 02:40,06/Oct/14 14:54,14/Jul/23 05:39,06/Oct/14 14:54,0.8.2.0,,,0.8.2.0,,,,,,,,,,0,,,,,,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/14 02:44;junrao;KAFKA-1673.patch;https://issues.apache.org/jira/secure/attachment/12673049/KAFKA-1673.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 06 14:54:24 UTC 2014,,,,,,,,,,"0|i20tjj:",9223372036854775807,,jkreps,,,,,,,,,,,,,,,,,,"06/Oct/14 02:45;junrao;Created reviewboard https://reviews.apache.org/r/26355/diff/
 against branch trunk;;;","06/Oct/14 14:54;junrao;Thanks for the review. Committed to both trunk and 0.8.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
uploaded archives are missing for Scala version 2.11,KAFKA-1671,12745895,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,edgefox,joestein,joestein,04/Oct/14 05:09,17/Oct/14 19:32,14/Jul/23 05:39,17/Oct/14 19:32,,,,0.8.2.0,,,,,,,,,,0,newbie,,,https://repository.apache.org/content/groups/staging/org/apache/kafka/,,edgefox,joestein,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/14 13:38;edgefox;KAFKA-1671.patch;https://issues.apache.org/jira/secure/attachment/12673087/KAFKA-1671.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 17 19:32:06 UTC 2014,,,,,,,,,,"0|i20spb:",9223372036854775807,,joestein,,,,,,,,,,,,,,,,,,"06/Oct/14 13:38;edgefox;Created reviewboard https://reviews.apache.org/r/26360/diff/
 against branch apache/trunk;;;","17/Oct/14 19:32;junrao;Thanks for the patch. +1. Committed to trunk and 0.8.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corrupt log files for segment.bytes values close to Int.MaxInt,KAFKA-1670,12745831,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriharsha,rberdeen,rberdeen,03/Oct/14 21:06,13/Oct/14 16:37,14/Jul/23 05:39,12/Oct/14 15:54,0.8.1.1,,,0.8.2.0,,,,,,,,,,0,,,,"The maximum value for the topic-level config {{segment.bytes}} is {{Int.MaxInt}} (2147483647). *Using this value causes brokers to corrupt their log files, leaving them unreadable.*

We set {{segment.bytes}} to {{2122317824}} which is well below the maximum. One by one, the ISR of all partitions shrunk to 1. Brokers would crash when restarted, attempting to read from a negative offset in a log file. After discovering that many segment files had grown to 4GB or more, we were forced to shut down our *entire production Kafka cluster* for several hours while we split all segment files into 1GB chunks.

Looking into the {{kafka.log}} code, the {{segment.bytes}} parameter is used inconsistently. It is treated as a *soft* maximum for the size of the segment file (https://github.com/apache/kafka/blob/0.8.1.1/core/src/main/scala/kafka/log/LogConfig.scala#L26) with logs rolled only after (https://github.com/apache/kafka/blob/0.8.1.1/core/src/main/scala/kafka/log/Log.scala#L246) they exceed this value. However, much of the code that deals with log files uses *ints* to store the size of the file and the position in the file. Overflow of these ints leads the broker to append to the segments indefinitely, and to fail to read these segments for consuming or recovery.

This is trivial to reproduce:

{code}
$ bin/kafka-topics.sh --topic segment-bytes-test --create --replication-factor 2 --partitions 1 --zookeeper zkhost:2181
$ bin/kafka-topics.sh --topic segment-bytes-test --alter --config segment.bytes=2147483647 --zookeeper zkhost:2181
$ yes ""Int.MaxValue is a ridiculous bound on file size in 2014"" | bin/kafka-console-producer.sh --broker-list localhost:6667 zkhost:2181 --topic segment-bytes-test
{code}

After running for a few minutes, the log file is corrupt:

{code}
$ ls -lh data/segment-bytes-test-0/
total 9.7G
-rw-r--r-- 1 root root  10M Oct  3 19:39 00000000000000000000.index
-rw-r--r-- 1 root root 9.7G Oct  3 19:39 00000000000000000000.log
{code}

We recovered the data from the log files using a simple Python script: https://gist.github.com/also/9f823d9eb9dc0a410796",,guozhang,gwenshap,jkreps,junrao,rberdeen,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/14 19:20;sriharsha;KAFKA-1670.patch;https://issues.apache.org/jira/secure/attachment/12674372/KAFKA-1670.patch","04/Oct/14 22:28;sriharsha;KAFKA-1670.patch;https://issues.apache.org/jira/secure/attachment/12672956/KAFKA-1670.patch","05/Oct/14 03:17;sriharsha;KAFKA-1670_2014-10-04_20:17:46.patch;https://issues.apache.org/jira/secure/attachment/12672966/KAFKA-1670_2014-10-04_20%3A17%3A46.patch","06/Oct/14 16:48;sriharsha;KAFKA-1670_2014-10-06_09:48:25.patch;https://issues.apache.org/jira/secure/attachment/12673118/KAFKA-1670_2014-10-06_09%3A48%3A25.patch","07/Oct/14 20:39;sriharsha;KAFKA-1670_2014-10-07_13:39:13.patch;https://issues.apache.org/jira/secure/attachment/12673417/KAFKA-1670_2014-10-07_13%3A39%3A13.patch","07/Oct/14 20:49;sriharsha;KAFKA-1670_2014-10-07_13:49:10.patch;https://issues.apache.org/jira/secure/attachment/12673423/KAFKA-1670_2014-10-07_13%3A49%3A10.patch","08/Oct/14 01:39;sriharsha;KAFKA-1670_2014-10-07_18:39:31.patch;https://issues.apache.org/jira/secure/attachment/12673501/KAFKA-1670_2014-10-07_18%3A39%3A31.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 13 16:37:24 UTC 2014,,,,,,,,,,"0|i20sc7:",9223372036854775807,,junrao,,,,,,,,,,,,,,,,,,"04/Oct/14 22:28;sriharsha;Created reviewboard https://reviews.apache.org/r/26346/diff/
 against branch origin/trunk;;;","04/Oct/14 22:32;sriharsha;This is caused by maybeRoll  not checking for any boundary conditions for Int overflow. ;;;","05/Oct/14 03:17;sriharsha;Updated reviewboard https://reviews.apache.org/r/26346/diff/
 against branch origin/trunk;;;","05/Oct/14 03:23;sriharsha;[~nehanarkhede] [~junrao] LogConfig default values doesn't match broker configuration defaults here https://kafka.apache.org/08/configuration.html
example
  SegmentSize = 1024 * 1024 (LogConfig)  and in broker log.segment.bytes	1024 * 1024 * 1024
and also MaxIndexSize
any reason for them not to be same ?;;;","05/Oct/14 23:37;junrao;The default value in LogConfig is just for unit test (so some smaller values may make sense). In the full logic, we always pick the broker level value as the default.;;;","06/Oct/14 16:48;sriharsha;Updated reviewboard https://reviews.apache.org/r/26346/diff/
 against branch origin/trunk;;;","06/Oct/14 22:55;gwenshap;I think we'll want to change the segment size variable to ""long"" and remove the 4GB limit (which seems rather low for modern systems).
This may make sense as a separate Jira since [~harsha_ch] patch is necessary in any case.
;;;","06/Oct/14 23:11;jkreps;Actually the reason for limiting the segments to 4GB (or possibly 2GB since we are using java ints) is to keep the index pointers into the file limited to 4 bytes which keeps the index files small (4 byte relative offset and 4 byte file position). Index file size and density is important since we hope to keep those cached to make lookups cheap. We should fix the variable to avoid overflow and even extend to unsigned ints but we probably can't allow arbitrarily large segment files very easily.

The reasoning at the time was basically that there is no particular reason to want very large segment files, and since we always do recovery from the beginning of the file having 10GB segments would cause other problems when you crashed and had to do recovery on hundreds of 10GB files.;;;","06/Oct/14 23:26;gwenshap;Makes sense. Thanks [~jkreps];;;","07/Oct/14 20:39;sriharsha;Updated reviewboard https://reviews.apache.org/r/26346/diff/
 against branch origin/trunk;;;","07/Oct/14 20:49;sriharsha;Updated reviewboard https://reviews.apache.org/r/26346/diff/
 against branch origin/trunk;;;","08/Oct/14 01:39;sriharsha;Updated reviewboard https://reviews.apache.org/r/26346/diff/
 against branch origin/trunk;;;","09/Oct/14 14:44;sriharsha;[~junrao] I updated the patch with ErrorMapping and client errors change. Can you please take a look when you get a chance. Thanks.;;;","09/Oct/14 15:09;junrao;Thanks for the latest patch. +1. Committed to both 0.8.2 and trunk.;;;","10/Oct/14 18:02;guozhang;Hi [~sriharsha] [~junrao] this patch causes some regression errors on system tests, including replication / mirror maker test suites (you can try reproduce it with 5001/2/3 easily).

The log entries I saw from the producer:
{code}
[2014-10-10 05:32:46,714] ERROR Error when sending message to topic test_1 with key: 1 bytes, value: 500 bytes with error: The request included message batch larger than the configured segment size on the server. (org.apache.kafka.client
s.producer.internals.ErrorLoggingCallback)
[2014-10-10 05:32:46,714] ERROR Error when sending message to topic test_1 with key: 1 bytes, value: 500 bytes with error: The request included message batch larger than the configured segment size on the server. (org.apache.kafka.client
s.producer.internals.ErrorLoggingCallback)
[2014-10-10 05:32:46,714] ERROR Error when sending message to topic test_1 with key: 1 bytes, value: 500 bytes with error: The request included message batch larger than the configured segment size on the server. (org.apache.kafka.client
s.producer.internals.ErrorLoggingCallback)
[2014-10-10 05:32:46,714] ERROR Error when sending message to topic test_1 with key: 1 bytes, value: 500 bytes with error: The request included message batch larger than the configured segment size on the server. (org.apache.kafka.client
s.producer.internals.ErrorLoggingCallback)
[2014-10-10 05:32:46,714] ERROR Error when sending message to topic test_1 with key: 1 bytes, value: 500 bytes with error: The request included message batch larger than the configured segment size on the server. (org.apache.kafka.client
s.producer.internals.ErrorLoggingCallback)
{code};;;","10/Oct/14 18:04;sriharsha;[~guozhang] Sorry will fix those.;;;","10/Oct/14 20:05;guozhang;No worries :) Let me know if you need more information from me.;;;","11/Oct/14 19:20;sriharsha;Created reviewboard https://reviews.apache.org/r/26606/diff/
 against branch origin/trunk;;;","11/Oct/14 19:29;sriharsha;[~guozhang] log.segment.bytes on systems is set to 10240 and producer's message batch size bigger than that causing the failures.
changed the configs to 20580. ran systems tests
========================================================
Total failures count : 0
========================================================

[~junrao] rethinking a bit , should we enforce producer's message batch size  less than config segment.size? 
instead on Log side if the current message batch size is greater than config segment.size broker should push as many messages  can fit into the current segment and roll the log for the rest of messages in the batch. ;;;","12/Oct/14 15:54;junrao;Thanks for the followup patch for system tests. +1 and committed to trunk and 0.8.2.

Yes, it's possible for the broker to break the message set into smaller chunks that fit in the configured log segment size. However, there may be some benefit to keep the original message set as a whole. For example, if the message set is compressed, this guarantees that either all or none of the messages in the set are replicated to the followers.;;;","13/Oct/14 16:37;guozhang;Verified that system test has recovered, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TopicCommand doesn't warn if --topic argument doesn't match any topics,KAFKA-1668,12745807,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,omkreddy,rberdeen,rberdeen,03/Oct/14 19:25,03/Sep/15 00:04,14/Jul/23 05:39,07/Oct/14 00:20,,,,0.9.0.0,,,,,,,tools,,,0,newbie,,,"Running {{kafka-topics.sh --alter}} with an invalid {{--topic}} argument produces no output and exits with 0, indicating success.

{code}
$ bin/kafka-topics.sh --topic does-not-exist --alter --config invalid=xxx --zookeeper zkhost:2181
$ echo $?
0
{code}

An invalid topic name or a regular expression that matches 0 topics should at least print a warning.",,jkreps,nehanarkhede,omkreddy,rberdeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/14 08:32;omkreddy;KAFKA-1668.patch;https://issues.apache.org/jira/secure/attachment/12672987/KAFKA-1668.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 07 00:20:26 UTC 2014,,,,,,,,,,"0|i20s73:",9223372036854775807,,jkreps,,,,,,,,,,,,,,,,,,"05/Oct/14 08:32;omkreddy;Created reviewboard https://reviews.apache.org/r/26351/diff/
 against branch origin/trunk;;;","06/Oct/14 15:21;nehanarkhede;[~jkreps] Feel free to reassign for review.;;;","07/Oct/14 00:20;jkreps;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 topic-level configuration not validated,KAFKA-1667,12745804,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,edio,rberdeen,rberdeen,03/Oct/14 19:07,25/Nov/14 22:44,14/Jul/23 05:39,25/Nov/14 22:44,0.8.1.1,,,0.9.0.0,,,,,,,,,,0,newbie,,,"I was able to set the configuration for a topic to these invalid values:

{code}
Topic:topic-config-test  PartitionCount:1        ReplicationFactor:2     Configs:min.cleanable.dirty.ratio=-30.2,segment.bytes=-1,retention.ms=-12,cleanup.policy=lol
{code}

It seems that the values are saved as long as they are the correct type, but are not validated like the corresponding broker-level properties.",,edio,gwenshap,junrao,rberdeen,vybs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/14 18:44;edio;KAFKA-1667_2014-11-05_19:43:53.patch;https://issues.apache.org/jira/secure/attachment/12679590/KAFKA-1667_2014-11-05_19%3A43%3A53.patch","06/Nov/14 16:10;edio;KAFKA-1667_2014-11-06_17:10:14.patch;https://issues.apache.org/jira/secure/attachment/12679870/KAFKA-1667_2014-11-06_17%3A10%3A14.patch","07/Nov/14 13:28;edio;KAFKA-1667_2014-11-07_14:28:14.patch;https://issues.apache.org/jira/secure/attachment/12680160/KAFKA-1667_2014-11-07_14%3A28%3A14.patch","12/Nov/14 11:49;edio;KAFKA-1667_2014-11-12_12:49:11.patch;https://issues.apache.org/jira/secure/attachment/12681051/KAFKA-1667_2014-11-12_12%3A49%3A11.patch","16/Nov/14 17:31;edio;KAFKA-1667_2014-11-16_18:31:34.patch;https://issues.apache.org/jira/secure/attachment/12681797/KAFKA-1667_2014-11-16_18%3A31%3A34.patch","16/Nov/14 17:33;edio;KAFKA-1667_2014-11-16_18:33:10.patch;https://issues.apache.org/jira/secure/attachment/12681798/KAFKA-1667_2014-11-16_18%3A33%3A10.patch","25/Nov/14 11:04;edio;KAFKA-1667_2014-11-25_12:03:56.patch;https://issues.apache.org/jira/secure/attachment/12683523/KAFKA-1667_2014-11-25_12%3A03%3A56.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 25 22:44:11 UTC 2014,,,,,,,,,,"0|i20s6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Oct/14 01:21;gwenshap;Looks like LogConfig has very few validations. 

I think it makes sense to refactor it to use either ConfigDef (actually part of the client code, but has really nice validators) or at least ValidatedProperties instead of just Properties - and then we can add proper validation for each config.

Any thoughts on whether ConfigDef can be used in the server, or if we should keep on using ValidatedProperties? ;;;","24/Oct/14 00:37;junrao;The server side already has a dependency on clients. So, we can start using ConfigDef on the broker side as well.;;;","05/Nov/14 10:55;edio;Hi. I'm willing to work on this. 
Currently I have few questions: 

# What should be of a higher priority: keeping same interface for LogConfig or making code consistent with other configs based on ConfigDef? In the latter case changes would also impact AdminUtils, TopicCommand and perhaps some other classes.
# Are there any guidelines to assigning priorities?
# Is there some place to check for valid ranges for properties? It is always possible to induce them from the code, but would be easier, of course, to have some reference doc.

Thanks;;;","05/Nov/14 15:43;junrao;For now, we can probably just keep LogConfig and add the validation in LogConfig.fromProps(). Later on, we can change all broker side configs (KafkaConfig, ZKConfig, TopicConfig) to use ConfigDef together.;;;","05/Nov/14 16:39;junrao;If you already started on converting LogConfig to ConfigDef, that's fine too. To start with, I will assign medium importance to all those topic level config properties.;;;","05/Nov/14 16:53;edio;So, I decided to just add validation and to impact as little as possible.
There is a new method in {{ConfigDef}}, which is needed to avoid duplication in LogConfig (instead of maintaining ConfigNames we derive them from configDef)

All properties are now of MEDIUM importance and have no documentation.
All numeric values are required to be >0, compact property validated against set [""compact"", ""delete""].

I would appreciate if you take a look on the patch and suggest, whether it is ok to submit it as a fix.

P.S. sorry for my megaverbosity. That's my first contribution;;;","05/Nov/14 17:09;gwenshap;Can you load the patch to RB too? It makes reviewing and commenting much easier.;;;","05/Nov/14 17:16;gwenshap;I think we can do better validations. For example:

.define(MinCleanableDirtyRatioProp, DOUBLE, Defaults.MinCleanableDirtyRatio, atLeast(0), MEDIUM, """")

Can be:

.define(MinCleanableDirtyRatioProp, DOUBLE, Defaults.MinCleanableDirtyRatio, between(0,1), MEDIUM, """")

Since its a ratio.

Once you have RB, I can add more detail for other configs.;;;","05/Nov/14 18:44;edio;Updated reviewboard https://reviews.apache.org/r/27634/diff/
 against branch origin/trunk;;;","06/Nov/14 16:10;edio;Updated reviewboard https://reviews.apache.org/r/27634/diff/
 against branch origin/trunk;;;","06/Nov/14 16:19;edio;The main issue with the old patch was handling {{Properties}} defaults.
When properties were passed to {{fromProps(Properties)}} method, {{Properties}} object has been exposed as a {{Map}}. {{Map.get()}}, though is implemented in {{Hashtable}} and thus knows nothing about {{Properties}} defaults. Fixed that, added few trivial tests to check general correctness.

There is an impact on the client code though: boolean values are now parsed via {{Boolean.parseBoolean}} throwing no exception but instead falling back to {{false}} for invalid input.;;;","07/Nov/14 13:28;edio;Updated reviewboard https://reviews.apache.org/r/27634/diff/
 against branch origin/trunk;;;","12/Nov/14 04:36;gwenshap;btw. [~edio], you probably want to assign the JIRA to yourself, and click on ""submit patch"" so this will be more visible as something that needs review + commit.;;;","12/Nov/14 11:49;edio;Updated reviewboard https://reviews.apache.org/r/27634/diff/
 against branch origin/trunk;;;","12/Nov/14 11:52;edio;Can't assign issue to myself. Get an exception when running kafka-patch-review.py.

{code}jira.exceptions.JIRAError: HTTP 400: ""Field 'assignee' cannot be set. It is not on the appropriate screen, or unknown{code}

 Also don't have ""Assign to me"" button in JIRA.;;;","14/Nov/14 16:58;edio;Bump. Anyone willing to review?;;;","16/Nov/14 17:31;edio;Updated reviewboard https://reviews.apache.org/r/27634/diff/
 against branch origin/trunk;;;","16/Nov/14 17:33;edio;Updated reviewboard https://reviews.apache.org/r/27634/diff/
 against branch origin/trunk;;;","25/Nov/14 11:04;edio;Updated reviewboard https://reviews.apache.org/r/27634/diff/
 against branch origin/trunk;;;","25/Nov/14 22:44;junrao;Thanks for the patch. +1 and committed to trunk.

Also filed KAFKA-1798 for validating the string input for the boolean config type.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka does not properly parse multiple ZK nodes with non-root chroot,KAFKA-1664,12745375,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,singhashish,rickysaltzer,rickysaltzer,01/Oct/14 22:37,02/Mar/15 05:16,14/Jul/23 05:39,27/Feb/15 23:10,,,,,,,,,,,clients,,,0,newbie,,,"When using a non-root ZK directory for Kafka, if you specify multiple ZK servers, Kafka does not seem to properly parse the connection string. 

*Error*
{code}
[root@hodor-001 bin]# ./kafka-console-consumer.sh --zookeeper baelish-001.edh.cloudera.com:2181/kafka,baelish-002.edh.cloudera.com:2181/kafka,baelish-003.edh.cloudera.com:2181/kafka --topic test-topic
[2014-10-01 15:31:04,629] ERROR Error processing message, stopping consumer:  (kafka.consumer.ConsoleConsumer$)
java.lang.IllegalArgumentException: Path length must be > 0
	at org.apache.zookeeper.common.PathUtils.validatePath(PathUtils.java:48)
	at org.apache.zookeeper.common.PathUtils.validatePath(PathUtils.java:35)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:766)
	at org.I0Itec.zkclient.ZkConnection.create(ZkConnection.java:87)
	at org.I0Itec.zkclient.ZkClient$1.call(ZkClient.java:308)
	at org.I0Itec.zkclient.ZkClient$1.call(ZkClient.java:304)
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)
	at org.I0Itec.zkclient.ZkClient.create(ZkClient.java:304)
	at org.I0Itec.zkclient.ZkClient.createPersistent(ZkClient.java:213)
	at org.I0Itec.zkclient.ZkClient.createPersistent(ZkClient.java:223)
	at org.I0Itec.zkclient.ZkClient.createPersistent(ZkClient.java:223)
	at org.I0Itec.zkclient.ZkClient.createPersistent(ZkClient.java:223)
	at kafka.utils.ZkUtils$.createParentPath(ZkUtils.scala:245)
	at kafka.utils.ZkUtils$.createEphemeralPath(ZkUtils.scala:256)
	at kafka.utils.ZkUtils$.createEphemeralPathExpectConflict(ZkUtils.scala:268)
	at kafka.utils.ZkUtils$.createEphemeralPathExpectConflictHandleZKBug(ZkUtils.scala:306)
	at kafka.consumer.ZookeeperConsumerConnector.kafka$consumer$ZookeeperConsumerConnector$$registerConsumerInZK(ZookeeperConsumerConnector.scala:226)
	at kafka.consumer.ZookeeperConsumerConnector$WildcardStreamsHandler.<init>(ZookeeperConsumerConnector.scala:755)
	at kafka.consumer.ZookeeperConsumerConnector.createMessageStreamsByFilter(ZookeeperConsumerConnector.scala:145)
	at kafka.consumer.ConsoleConsumer$.main(ConsoleConsumer.scala:196)
	at kafka.consumer.ConsoleConsumer.main(ConsoleConsumer.scala)
{code}


*Working*
{code}
[root@hodor-001 bin]# ./kafka-console-consumer.sh --zookeeper baelish-001.edh.cloudera.com:2181/kafka --topic test-topic
{code}",,gwenshap,jcreasy,junrao,nehanarkhede,rickysaltzer,singhashish,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/14 05:45;singhashish;KAFKA-1664.1.patch;https://issues.apache.org/jira/secure/attachment/12683216/KAFKA-1664.1.patch","11/Dec/14 06:18;singhashish;KAFKA-1664.2.patch;https://issues.apache.org/jira/secure/attachment/12686456/KAFKA-1664.2.patch","16/Nov/14 19:52;singhashish;KAFKA-1664.patch;https://issues.apache.org/jira/secure/attachment/12681803/KAFKA-1664.patch","29/Jan/15 18:27;singhashish;KAFKA-1664_2015-01-29_10:26:20.patch;https://issues.apache.org/jira/secure/attachment/12695323/KAFKA-1664_2015-01-29_10%3A26%3A20.patch","24/Feb/15 19:02;singhashish;KAFKA-1664_2015-02-24_11:02:23.patch;https://issues.apache.org/jira/secure/attachment/12700546/KAFKA-1664_2015-02-24_11%3A02%3A23.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 05:16:07 UTC 2015,,,,,,,,,,"0|i20phb:",9223372036854775807,,nehanarkhede,,,,,,,,,,,,,,,,,,"01/Oct/14 22:42;gwenshap;This happens with other high-level consumer clients. So its not just the ConsoleConsumer. 

Topic admin tools don't throw an exception, but don't show topics either.

I'm working on a patch.;;;","02/Oct/14 00:51;junrao;Actually, namespace should only be included once in a ZK url, not after every host:port. So the correct ZK URL should be

 baelish-001.edh.cloudera.com:2181,baelish-002.edh.cloudera.com:2181,baelish-003.edh.cloudera.com:2181/kafka;;;","02/Oct/14 01:19;rickysaltzer;Yes I've tried that as well, same problem.

;;;","02/Oct/14 02:53;junrao;So, it seems that namespace doesn't exist. For tools, auto-creating those namespaces may not be ideal. Perhaps we just need to provide a more meaningful error.;;;","02/Oct/14 04:06;nehanarkhede;+1 on providing a more meaningful error.;;;","02/Oct/14 18:01;rickysaltzer;[~junrao] my apologies, thanks for the recommendation, that *did* actually work. +1 for a more meaningful error. ;;;","02/Oct/14 21:37;gwenshap;For a more meaningful error, I'm thinking: Catch the exception thrown by zkClient at createParentPath, log an error like ""Invalid ZooKeeper path, please check syntax and that path exists"" and throw our own ConfigException

Since the original error is thrown by zkClient, we'll need to handle that in multiple places where we call it. Does it make sense to patch zkClient instead?

;;;","05/Oct/14 23:57;junrao;Perhaps we can add a wrapper in ZkUtils for creating a path in ZK. The wrapper will first check the existence of namespace. If it doesn't, throw a meaningful exception.;;;","15/Nov/14 18:13;singhashish;[~gwenshap] I am thinking of taking a look at this. If it is OK with you, kindly assign the JIRA to me.;;;","16/Nov/14 10:05;gwenshap;Here you go, [~singhashish]. Have fun.;;;","16/Nov/14 19:52;singhashish;RB: https://reviews.apache.org/r/28108/;;;","11/Dec/14 19:48;singhashish;[~nehanarkhede] Addressed your review comment. Kindly take a look.;;;","29/Dec/14 22:38;singhashish;[~nehanarkhede] still waiting for your review.;;;","16/Jan/15 23:24;singhashish;Testing file [KAFKA-1664.2.patch|https://issues.apache.org/jira/secure/attachment/12686456/KAFKA-1664.2.patch] against branch trunk took 0:09:06.543433.

{color:red}Overall:{color} -1 due to 2 errors

{color:red}ERROR:{color} Some unit tests failed (report)
{color:red}ERROR:{color} Failed unit test: {{kafka.consumer.ZookeeperConsumerConnectorTest > testConsumerRebalanceListener FAILED
}}
{color:green}SUCCESS:{color} Gradle bootstrap was successful
{color:green}SUCCESS:{color} Clean was successful
{color:green}SUCCESS:{color} Patch applied correctly
{color:green}SUCCESS:{color} Patch add/modify test case
{color:green}SUCCESS:{color} Gradle bootstrap was successful
{color:green}SUCCESS:{color} Patch compiled

This message is automatically generated.;;;","29/Jan/15 18:27;singhashish;Updated reviewboard https://reviews.apache.org/r/28108/
 against branch trunk;;;","24/Feb/15 02:16;singhashish;[~nehanarkhede] could you review this when you get a chance.;;;","24/Feb/15 16:35;nehanarkhede;[~ashishujjain] Thanks for the reminder. The patch looks good, the only comment I have is that it introduces a bunch of build warnings-
{code}
/Users/nnarkhed/Projects/kafka/core/src/test/scala/unit/kafka/zk/ZKPathTest.scala:45: This catches all Throwables. If this is really intended, use `case exception : Throwable` to clear this warning.
      case exception => fail(""Should have thrown ConfigException"")
{code}
;;;","24/Feb/15 19:02;singhashish;Updated reviewboard https://reviews.apache.org/r/28108/
 against branch trunk;;;","24/Feb/15 19:04;singhashish;[~nehanarkhede] addressed the warnings issue. Thanks for the review.;;;","25/Feb/15 22:46;jcreasy;I think this is because it expects the string to be:

baelish-001.edh.cloudera.com:2181,baelish-002.edh.cloudera.com:2181,baelis
h-003.edh.cloudera.com:2181/kafka


I don¹t think you would ever want to have a different root per host.;;;","25/Feb/15 22:47;jcreasy;nevermind, missed Jun's response farther up. Carry on! :);;;","27/Feb/15 23:10;nehanarkhede;Thanks for the patch. Pushed to trunk.;;;","28/Feb/15 17:12;junrao;Thanks for the patch. The patch makes every create operation in ZK a bit more expensive since it has to do an extra exists check. This may slow down operations like creating a new topic, especially when there are lots of partitions.

The checking of the chroot really just needs to be done once at the beginning. I am thinking that one way to improve this is to make ZkPath a singleton. It will do the chroot check on first create and remembers the result. Subsequent create calls will just reuse the result.;;;","02/Mar/15 05:16;singhashish;[~junrao] thanks for bringing up this point. Created KAFKA-1994 to track this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller unable to shutdown after a soft failure,KAFKA-1663,12745270,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriharsha,sriharsha,sriharsha,01/Oct/14 14:52,09/Oct/14 21:39,14/Jul/23 05:39,09/Oct/14 21:39,,,,0.8.2.0,,,,,,,,,,0,,,,"As part of testing KAFKA-1558 I came across a case where inducing soft failure in the current controller elects a new controller  but the old controller doesn't shutdown properly.
steps to reproduce
1) 5 broker cluster
2) high number of topics(I tested it with 1000 topics)
3) on the current controller do kill -SIGSTOP  pid( broker's process id)
4) wait for bit over zookeeper timeout (server.properties)
5) kill -SIGCONT pid
6) There will be a new controller elected. check old controller's
log 
[2014-09-30 15:59:53,398] INFO [SessionExpirationListener on 1], ZK expired; shut down all controller components and try to re-elect (kafka.controller.KafkaController$SessionExpirationListener)
[2014-09-30 15:59:53,400] INFO [delete-topics-thread-1], Shutting down (kafka.controller.TopicDeletionManager$DeleteTopicsThread)

If it stops there and the broker  logs keeps printing 
Cached zkVersion [0] not equal to that in zookeeper, skip updating ISR (kafka.cluster.Partition)
than the controller shutdown never completes.",,junrao,nehanarkhede,sriharsha,yuanjiali,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1600,,,,,,,,,,,,,,,,,,,KAFKA-1558,KAFKA-1681,"03/Oct/14 01:31;sriharsha;KAFKA-1663.patch;https://issues.apache.org/jira/secure/attachment/12672716/KAFKA-1663.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 09 21:39:15 UTC 2014,,,,,,,,,,"0|i20osv:",9223372036854775807,,nehanarkhede,,,,,,,,,,,,,,,,,,"02/Oct/14 17:54;sriharsha;bit more info KafkaController.shutdown never coming out of  deleteTopicsThread.awaitShutdown() after a soft failure causing the above issue. This is not very consistent behavior but I am able to reproduce kafka cluster running  on ec2 m3.xlarge .;;;","02/Oct/14 21:30;nehanarkhede;[~sriharsha] Thanks for reproducing this. Could you please attach the thread dump that shows where the delete topics thread hangs?;;;","03/Oct/14 00:23;sriharsha;[~nehanarkhede] testing out few possible fixes will upload the thread dump soon. Thanks.;;;","03/Oct/14 00:35;sriharsha;[~nehanarkhede] I've a question on TopicDeletionManager code
I think the idea is to start the DeleteTopicsThread and wait for the events like topics are added to delete set and trigger the doWork() method. 
right now doWork() method calls awaitTopicDeletionNotifictation

  private def awaitTopicDeletionNotification() {
    inLock(deleteLock) {
      while(!deleteTopicsThread.isRunning.get() && !deleteTopicStateChanged.compareAndSet(true, false)) {
                       deleteTopicsCond.await()
}
This condition seems to be wrong and it doesn't block DeleteTopicThread once the TopicDeletionManager starts the deleteTopicsThread.doWork() continues to execute.

The above condition should state
 while(deleteTopicsThread.isRunning.get() && deleteTopicStateChanged.compareAndSet(true, false)) {
        deleteTopicsCond.await()
}
whenever there is topic is added we are callign resumeTopicDeletionThread() which sets deleteTopicStateChanged to true and sends deleteTopicCond.signal() which should wake up doWork() and continue with deletion of the topic.
I am testing with this change, will update with the results. 
;;;","03/Oct/14 01:31;sriharsha;Created reviewboard https://reviews.apache.org/r/26306/diff/
 against branch origin/trunk;;;","03/Oct/14 01:32;sriharsha;[~nehanarkhede] I tested the attached patch for recovering from soft failure and also few delete topic cases. but I haven't gone though the entire KAFKA-1558 test cases. I am going to run them now , meanwhile can you please review the patch. Thanks.;;;","03/Oct/14 17:35;sriharsha;[~nehanarkhede] I ran all the tests in KAFKA-1558 they all pass with the above patch and didn't see any issues with soft failure case.;;;","04/Oct/14 00:53;nehanarkhede;[~sriharsha] Awesome. I'll review this tomorrow.;;;","05/Oct/14 01:10;nehanarkhede;Thanks for the patch! Pushed to trunk;;;","05/Oct/14 23:49;junrao;Neha,

Since this is a blocker for 0.8.2, could you push this to the 0.8.2 branch too? Thanks,;;;","07/Oct/14 14:04;nehanarkhede;[~sriharsha], while talking to Jun, realized that there may have been a regression introduced by the patch by removing the deleteTopicStateChanged.set(true) from startup(). The purpose of that is to let the delete topic thread resume topic deletion on startup for topics for which deletion was initiated on the previous controller. During the review, I assumed that the controller is signaling the delete topic thread separately after startup, but that is not the case. 

However, while reading through the code, I think there is a bug in the above case where the controller needs to resume topic deletion on startup. Basically the way for the controller to notify the TopicDeletionManager of resuming the thread is via the callers of resumeTopicDeletionThread(). Each of those caller APIs are protected via the controllerLock in KafkaController. However, awaitTopicDeletionNotification is not. So there is a window when the controller might signal a thread that is not waiting on the same monitor. I think the main problem is with having 2 locks - deleteLock and controllerLock. We might have to revisit that decision and see if we consolidate on a single lock (controllerLock). Since this is a different bug, can you file it and link it back to this issue? ;;;","07/Oct/14 14:35;sriharsha;[~nehanarkhede] Both TopicDeletionManager.resumeTopicDeletionThread() and awaitTopicDeletionNoification uses deleteLock and DeleteTopicThread.doWork() waits on awaitTopicDeletionNotification before it tries to acquire controllerLock.
so simple fix would be to check if there are any topics in topicsToBeDeleted set and call resumeTopicDeletionThread() from 
start(). 
I agree that it is best to consolidate on a single lock.;;;","09/Oct/14 21:39;junrao;Committed the original patch in the jira and the patch in kafka-1681 to 0.8.2. Both changes are in trunk too.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gradle release issue permgen space,KAFKA-1662,12745250,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriharsha,joestein,joestein,01/Oct/14 13:19,04/Oct/14 02:07,14/Jul/23 05:39,04/Oct/14 02:01,,,,0.8.2.0,,,,,,,,,,0,newbie,,,"Finding issues doing the kafka release with permgen space

./gradlew releaseTarGzAll

ant:scaladoc] java.lang.OutOfMemoryError: PermGen space
:kafka-0.8.2-ALPHA1-src:core:scaladoc FAILED
:releaseTarGz_2_10_1 FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':core:scaladoc'.
> PermGen space

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED

Total time: 5 mins 55.53 secs

FAILURE: Build failed with an exception.

* What went wrong:
PermGen space

",,joestein,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/14 23:14;sriharsha;KAFKA-1662.patch;https://issues.apache.org/jira/secure/attachment/12672868/KAFKA-1662.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Oct 04 02:07:43 UTC 2014,,,,,,,,,,"0|i20oof:",9223372036854775807,,junrao,,,,,,,,,,,,,,,,,,"03/Oct/14 23:14;sriharsha;Created reviewboard https://reviews.apache.org/r/26332/diff/
 against branch origin/trunk;;;","04/Oct/14 00:25;joestein;I tried the patch and ran

{code}
./gradlew releaseTarGzAll
{code}

and got 

{code}
Unexpected exception thrown.
java.lang.OutOfMemoryError: PermGen space
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2436)
	at java.lang.Class.getDeclaredMethod(Class.java:1937)
	at java.io.ObjectStreamClass.getPrivateMethod(ObjectStreamClass.java:1377)
	at java.io.ObjectStreamClass.access$1700(ObjectStreamClass.java:50)
	at java.io.ObjectStreamClass$2.run(ObjectStreamClass.java:436)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:411)
	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:308)
	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:407)
	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:308)
	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:407)
	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:308)
	at java.io.ObjectStreamClass.<init>(ObjectStreamClass.java:407)
	at java.io.ObjectStreamClass.lookup(ObjectStreamClass.java:308)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1114)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1518)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1483)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1400)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1158)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1346)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1154)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:330)
	at org.gradle.messaging.remote.internal.Message.send(Message.java:40)
	at org.gradle.messaging.serialize.kryo.JavaSerializer$JavaWriter.write(JavaSerializer.java:62)
	at org.gradle.messaging.remote.internal.hub.MethodInvocationSerializer$MethodInvocationWriter.writeArguments(MethodInvocationSerializer.java:67)
	at org.gradle.messaging.remote.internal.hub.MethodInvocationSerializer$MethodInvocationWriter.write(MethodInvocationSerializer.java:63)
	at org.gradle.messaging.remote.internal.hub.MethodInvocationSerializer$MethodInvocationWriter.write(MethodInvocationSerializer.java:48)
	at org.gradle.messaging.serialize.kryo.TypeSafeSerializer$2.write(TypeSafeSerializer.java:46)
	at org.gradle.messaging.remote.internal.hub.InterHubMessageSerializer$MessageWriter.write(InterHubMessageSerializer.java:108)
	at org.gradle.messaging.remote.internal.hub.InterHubMessageSerializer$MessageWriter.write(InterHubMessageSer.java:93)
	at org.gradle.messaging.remote.internal.inet.SocketConnection.dispatch(SocketConnection.java:112)

{code};;;","04/Oct/14 00:40;sriharsha;[~charmalloc] can you share which os you are running this on. I was able to reproduce without this patch on OS X 10.9.4 but with this patch releaseTarGzAll works fine for me. Did you removed the existing gradle dir and reran the gradle command before running ./gradlew;;;","04/Oct/14 02:01;joestein;yup, works;;;","04/Oct/14 02:07;joestein;I committed to trunk and 0.8.2 branch.  Thanks for the patch!!!

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate broker ids allowed in replica assignment,KAFKA-1653,12744126,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ewencp,rberdeen,rberdeen,25/Sep/14 20:38,22/Oct/14 17:07,14/Jul/23 05:39,22/Oct/14 17:06,0.8.1.1,,,0.8.2.0,,,,,,,tools,,,0,newbie,,,"The reassign partitions command and the controller do not ensure that all replicas for a partition are on different brokers. For example, you could set 1,2,2 as the list of brokers for the replicas.

kafka-topics.sh --describe --under-replicated will list these partitions as under-replicated, but I can't see a reason why the controller should allow this state.",,ewencp,nehanarkhede,rberdeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/14 23:57;ewencp;KAFKA-1653.patch;https://issues.apache.org/jira/secure/attachment/12674648/KAFKA-1653.patch","16/Oct/14 21:54;ewencp;KAFKA-1653_2014-10-16_14:54:07.patch;https://issues.apache.org/jira/secure/attachment/12675361/KAFKA-1653_2014-10-16_14%3A54%3A07.patch","21/Oct/14 18:58;ewencp;KAFKA-1653_2014-10-21_11:57:50.patch;https://issues.apache.org/jira/secure/attachment/12676147/KAFKA-1653_2014-10-21_11%3A57%3A50.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 22 17:06:29 UTC 2014,,,,,,,,,,"0|i20huf:",9223372036854775807,,nehanarkhede,,,,,,,,,,,,,,,,,,"13/Oct/14 23:57;ewencp;Created reviewboard https://reviews.apache.org/r/26666/diff/
 against branch origin/trunk;;;","16/Oct/14 21:54;ewencp;Updated reviewboard https://reviews.apache.org/r/26666/diff/
 against branch origin/trunk;;;","21/Oct/14 18:58;ewencp;Updated reviewboard https://reviews.apache.org/r/26666/diff/
 against branch origin/trunk;;;","22/Oct/14 17:06;nehanarkhede;Thanks for the patches. Pushed to trunk and 0.8.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Round robin consumer balance throws an NPE when there are no topics,KAFKA-1648,12743567,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mgharat,toddpalino,toddpalino,23/Sep/14 18:11,14/Nov/14 19:15,14/Jul/23 05:39,09/Oct/14 23:36,,,,0.9.0.0,,,,,,,consumer,,,0,newbie,,,"If you use the roundrobin rebalance method with a wildcard consumer, and there are no topics in the cluster, rebalance throws a NullPointerException in the consumer and fails. It retries the rebalance, but will continue to throw the NPE.

2014/09/23 17:51:16.147 [ZookeeperConsumerConnector] [kafka-audit_lva1-app0007.corp-1411494404908-4e620544], Cleared all relevant queues for this fetcher
2014/09/23 17:51:16.147 [ZookeeperConsumerConnector] [kafka-audit_lva1-app0007.corp-1411494404908-4e620544], Cleared the data chunks in all the consumer message iterators
2014/09/23 17:51:16.148 [ZookeeperConsumerConnector] [kafka-audit_lva1-app0007.corp-1411494404908-4e620544], Committing all offsets after clearing the fetcher queues
2014/09/23 17:51:46.148 [ZookeeperConsumerConnector] [kafka-audit_lva1-app0007.corp-1411494404908-4e620544], begin rebalancing consumer kafka-audit_lva1-app0007.corp-1411494404908-4e620544 try #0
2014/09/23 17:51:46.148 ERROR [OffspringServletRuntime] [main] [kafka-console-audit] [] Boot listener com.linkedin.kafkaconsoleaudit.KafkaConsoleAuditBootListener failed
kafka.common.ConsumerRebalanceFailedException: kafka-audit_lva1-app0007.corp-1411494404908-4e620544 can't rebalance after 10 retries
	at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.syncedRebalance(ZookeeperConsumerConnector.scala:630)
	at kafka.consumer.ZookeeperConsumerConnector.kafka$consumer$ZookeeperConsumerConnector$$reinitializeConsumer(ZookeeperConsumerConnector.scala:897)
	at kafka.consumer.ZookeeperConsumerConnector$WildcardStreamsHandler.<init>(ZookeeperConsumerConnector.scala:931)
	at kafka.consumer.ZookeeperConsumerConnector.createMessageStreamsByFilter(ZookeeperConsumerConnector.scala:159)
	at kafka.javaapi.consumer.ZookeeperConsumerConnector.createMessageStreamsByFilter(ZookeeperConsumerConnector.scala:101)
	at com.linkedin.tracker.consumer.TrackingConsumerImpl.initWildcardIterators(TrackingConsumerImpl.java:88)
	at com.linkedin.tracker.consumer.TrackingConsumerImpl.getWildcardIterators(TrackingConsumerImpl.java:116)
	at com.linkedin.kafkaconsoleaudit.KafkaConsoleAudit.createAuditThreads(KafkaConsoleAudit.java:59)
	at com.linkedin.kafkaconsoleaudit.KafkaConsoleAudit.initializeAudit(KafkaConsoleAudit.java:50)
	at com.linkedin.kafkaconsoleaudit.KafkaConsoleAuditFactory.createInstance(KafkaConsoleAuditFactory.java:125)
	at com.linkedin.kafkaconsoleaudit.KafkaConsoleAuditFactory.createInstance(KafkaConsoleAuditFactory.java:20)
	at com.linkedin.util.factory.SimpleSingletonFactory.createInstance(SimpleSingletonFactory.java:20)
	at com.linkedin.util.factory.SimpleSingletonFactory.createInstance(SimpleSingletonFactory.java:14)
	at com.linkedin.util.factory.Generator.doGetBean(Generator.java:337)
	at com.linkedin.util.factory.Generator.getBean(Generator.java:270)
	at com.linkedin.kafkaconsoleaudit.KafkaConsoleAuditBootListener.onBoot(KafkaConsoleAuditBootListener.java:16)
	at com.linkedin.offspring.servlet.OffspringServletRuntime.startGenerator(OffspringServletRuntime.java:147)
	at com.linkedin.offspring.servlet.OffspringServletRuntime.start(OffspringServletRuntime.java:73)
	at com.linkedin.offspring.servlet.OffspringServletContextListener.contextInitialized(OffspringServletContextListener.java:28)
	at org.eclipse.jetty.server.handler.ContextHandler.callContextInitialized(ContextHandler.java:771)
	at org.eclipse.jetty.servlet.ServletContextHandler.callContextInitialized(ServletContextHandler.java:424)
	at org.eclipse.jetty.server.handler.ContextHandler.startContext(ContextHandler.java:763)
	at org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:249)
	at org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1250)
	at org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:706)
	at org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:492)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at com.linkedin.emweb.ContextBasedHandlerImpl.doStart(ContextBasedHandlerImpl.java:105)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at com.linkedin.emweb.WebappDeployerImpl.start(WebappDeployerImpl.java:333)
	at com.linkedin.emweb.WebappDeployerImpl.deploy(WebappDeployerImpl.java:187)
	at com.linkedin.emweb.StateKeeperWebappDeployer.deploy(StateKeeperWebappDeployer.java:75)
	at com.linkedin.emweb.StateKeeperWebappDeployer.restore(StateKeeperWebappDeployer.java:62)
	at com.linkedin.emweb.WebappRunner.restoreServerState(WebappRunner.java:171)
	at com.linkedin.emweb.BaseRunner.start(BaseRunner.java:96)
	at com.linkedin.spring.cmdline.ServerCmdLineApp.start(ServerCmdLineApp.java:86)
	at com.linkedin.spring.cmdline.ServerCmdLineApp.doRun(ServerCmdLineApp.java:102)
	at com.linkedin.spring.cmdline.CmdLineAppRunner.run(CmdLineAppRunner.java:246)
	at com.linkedin.spring.cmdline.CmdLineAppRunner.main(CmdLineAppRunner.java:480)

(note - the com.linkedin stuff in the stack trace is from the container we run our applications in and does not affect the operation of the rebalance)",,jjkoshy,mgharat,nehanarkhede,toddpalino,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/14 22:43;mgharat;KAFKA-1648.patch;https://issues.apache.org/jira/secure/attachment/12672671/KAFKA-1648.patch","05/Oct/14 00:40;mgharat;KAFKA-1648_2014-10-04_17:40:47.patch;https://issues.apache.org/jira/secure/attachment/12672963/KAFKA-1648_2014-10-04_17%3A40%3A47.patch","09/Oct/14 00:29;mgharat;KAFKA-1648_2014-10-08_17:29:14.patch;https://issues.apache.org/jira/secure/attachment/12673782/KAFKA-1648_2014-10-08_17%3A29%3A14.patch","09/Oct/14 00:46;mgharat;KAFKA-1648_2014-10-08_17:46:45.patch;https://issues.apache.org/jira/secure/attachment/12673788/KAFKA-1648_2014-10-08_17%3A46%3A45.patch","09/Oct/14 18:56;mgharat;KAFKA-1648_2014-10-09_11:56:44.patch;https://issues.apache.org/jira/secure/attachment/12673960/KAFKA-1648_2014-10-09_11%3A56%3A44.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 09 23:36:50 UTC 2014,,,,,,,,,,"0|i20egn:",9223372036854775807,,jjkoshy,,,,,,,,,,,,,,,,,,"25/Sep/14 05:02;nehanarkhede;Assigning this to Joel for review as he probably has the most context on this feature;;;","02/Oct/14 22:43;mgharat;Created reviewboard https://reviews.apache.org/r/26291/diff/
 against branch origin/trunk;;;","05/Oct/14 00:41;mgharat;Updated reviewboard https://reviews.apache.org/r/26291/diff/
 against branch origin/trunk;;;","09/Oct/14 00:29;mgharat;Updated reviewboard https://reviews.apache.org/r/26291/diff/
 against branch origin/trunk;;;","09/Oct/14 00:46;mgharat;Updated reviewboard https://reviews.apache.org/r/26291/diff/
 against branch origin/trunk;;;","09/Oct/14 18:56;mgharat;Updated reviewboard https://reviews.apache.org/r/26291/diff/
 against branch origin/trunk;;;","09/Oct/14 23:36;jjkoshy;Committed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replication offset checkpoints (high water marks) can be lost on hard kills and restarts,KAFKA-1647,12743558,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,becket_qin,jjkoshy,jjkoshy,23/Sep/14 17:35,30/Oct/14 23:33,14/Jul/23 05:39,30/Oct/14 23:33,0.8.2.0,,,0.8.2.0,,,,,,,,,,0,newbie++,,,"We ran into this scenario recently in a production environment. This can happen when enough brokers in a cluster are taken down. i.e., a rolling bounce done properly should not cause this issue. It can occur if all replicas for any partition are taken down.

Here is a sample scenario:

* Cluster of three brokers: b0, b1, b2
* Two partitions (of some topic) with replication factor two: p0, p1
* Initial state:
p0: leader = b0, ISR = {b0, b1}
p1: leader = b1, ISR = {b0, b1}
* Do a parallel hard-kill of all brokers
* Bring up b2, so it is the new controller
* b2 initializes its controller context and populates its leader/ISR cache (i.e., controllerContext.partitionLeadershipInfo) from zookeeper. The last known leaders are b0 (for p0) and b1 (for p2)
* Bring up b1
* The controller's onBrokerStartup procedure initiates a replica state change for all replicas on b1 to become online. As part of this replica state change it gets the last known leader and ISR and sends a LeaderAndIsrRequest to b1 (for p1 and p2). This LeaderAndIsr request contains: {{p0: leader=b0; p1: leader=b1;} leaders=b1}. b0 is indicated as the leader of p0 but it is not included in the leaders field because b0 is down.
* On receiving the LeaderAndIsrRequest, b1's replica manager will successfully make itself (b1) the leader for p1 (and create the local replica object corresponding to p1). It will however abort the become follower transition for p0 because the designated leader b0 is offline. So it will not create the local replica object for p0.
* It will then start the high water mark checkpoint thread. Since only p1 has a local replica object, only p1's high water mark will be checkpointed to disk. p0's previously written checkpoint  if any will be lost.

So in summary it seems we should always create the local replica object even if the online transition does not happen.

Possible symptoms of the above bug could be one or more of the following (we saw 2 and 3):
# Data loss; yes on a hard-kill data loss is expected, but this can actually cause loss of nearly all data if the broker becomes follower, truncates, and soon after happens to become leader.
# High IO on brokers that lose their high water mark then subsequently (on a successful become follower transition) truncate their log to zero and start catching up from the beginning.
# If the offsets topic is affected, then offsets can get reset. This is because during an offset load we don't read past the high water mark. So if a water mark is missing then we don't load anything (even if the offsets are there in the log).
",,becket_qin,gw4722,jjkoshy,junrao,nehanarkhede,noslowerdna,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/14 17:06;becket_qin;KAFKA-1647.patch;https://issues.apache.org/jira/secure/attachment/12673125/KAFKA-1647.patch","13/Oct/14 23:38;becket_qin;KAFKA-1647_2014-10-13_16:38:39.patch;https://issues.apache.org/jira/secure/attachment/12674645/KAFKA-1647_2014-10-13_16%3A38%3A39.patch","18/Oct/14 07:26;becket_qin;KAFKA-1647_2014-10-18_00:26:51.patch;https://issues.apache.org/jira/secure/attachment/12675659/KAFKA-1647_2014-10-18_00%3A26%3A51.patch","22/Oct/14 06:08;becket_qin;KAFKA-1647_2014-10-21_23:08:43.patch;https://issues.apache.org/jira/secure/attachment/12676274/KAFKA-1647_2014-10-21_23%3A08%3A43.patch","28/Oct/14 00:19;becket_qin;KAFKA-1647_2014-10-27_17:19:07.patch;https://issues.apache.org/jira/secure/attachment/12677471/KAFKA-1647_2014-10-27_17%3A19%3A07.patch","30/Oct/14 22:07;becket_qin;KAFKA-1647_2014-10-30_15:07:09.patch;https://issues.apache.org/jira/secure/attachment/12678325/KAFKA-1647_2014-10-30_15%3A07%3A09.patch","30/Oct/14 22:10;becket_qin;KAFKA-1647_2014-10-30_15:10:22.patch;https://issues.apache.org/jira/secure/attachment/12678327/KAFKA-1647_2014-10-30_15%3A10%3A22.patch","30/Oct/14 22:38;becket_qin;KAFKA-1647_2014-10-30_15:38:02.patch;https://issues.apache.org/jira/secure/attachment/12678335/KAFKA-1647_2014-10-30_15%3A38%3A02.patch","30/Oct/14 22:50;becket_qin;KAFKA-1647_2014-10-30_15:50:33.patch;https://issues.apache.org/jira/secure/attachment/12678337/KAFKA-1647_2014-10-30_15%3A50%3A33.patch",,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 30 23:33:33 UTC 2014,,,,,,,,,,"0|i20een:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Sep/14 20:57;nehanarkhede;[~jjkoshy] Sounds like a problem. I have a question about something that I don't quite understand. 
bq. On receiving the LeaderAndIsrRequest, b1's replica manager will successfully make b2 the leader for p1 (and create the local replica object corresponding to p1). It will however abort the become follower transition for p0 because the designated leader b2 is offline. So it will not create the local replica object for p0.

- Did you mean ""b1's replica manager will successfully make b1 the leader for p1""?
- I'm not sure why b1 ends up truncating all it's data for p0 even if it ends up aborting it's become follower transition? I see how the highwatermark checkpoint ends up removing p0's checkpoint, but not how b1 ends up deleting all it's data for p0.;;;","25/Sep/14 21:25;jjkoshy;If b1 aborts the follower transition the high watermark checkpoint can be removed. It does not truncate all the data at that point. However, on a subsequent become-follower transition (if any and if successful) it will truncate the log (since on unknown HW we take HW as zero).

This shouldn't be too difficult to fix though.;;;","26/Sep/14 03:56;nehanarkhede;I see. So for this to lose all the data for the concerned partition, b1 has to be bounced to become a follower again and at that time b0 should be up for the become follower to succeed. Right? It is a pretty corner case, but should be fixed. ;;;","26/Sep/14 05:33;jjkoshy;Yes that is one possibility but not the only one. For example, suppose this is a topic with replication factor three. This is typically when bringing up a cluster that was previously hard-killed. Suppose b1 and b2 are brought up simultaneously and lose their HW as described above. Suppose the controller then elects b1 as the leader. b2 then becomes the follower (successfully) but as part of that transition will truncate to zero. I think there should be more scenarios since I also saw this with a topic with replication factor of two but have not checked the logs yet to see if it was due to a subsequent bounce or something else.;;;","26/Sep/14 21:32;nehanarkhede;[~jjkoshy] Makes sense, thanks for the explanation! I guess the behavior of the follower dropping the partition out of the highwatermark checkpoint due to an aborted follower transition, needs to be fixed.;;;","28/Sep/14 21:45;junrao;Joel,

Is this because in ReplicaManger.makeFollowers(), we skip partition.makeFollower() if the leader is not alive? If so, we probably can just skip the check there. Then, another issue is that we will hit an exception in the following code, which will prevent the rest of the followers from being added to the fetcher. We probably need to do the try/catch there per partition and log an error if we hit an exception.
          new TopicAndPartition(partition) -> BrokerAndInitialOffset(
            leaders.find(_.id == partition.leaderReplicaIdOpt.get).get,
            partition.getReplica().get.logEndOffset.messageOffset);;;","29/Sep/14 17:53;jjkoshy;Yes that is exactly why it happens and the fix is relatively straightforward.;;;","06/Oct/14 17:06;becket_qin;Created reviewboard https://reviews.apache.org/r/26373/diff/
 against branch origin/trunk;;;","13/Oct/14 23:38;becket_qin;Updated reviewboard https://reviews.apache.org/r/26373/diff/
 against branch origin/trunk;;;","18/Oct/14 07:26;becket_qin;Updated reviewboard https://reviews.apache.org/r/26373/diff/
 against branch origin/trunk;;;","21/Oct/14 20:52;noslowerdna;[~nehanarkhede] Will this correction be included in 0.8.2-beta? It sounds like a blocker for the 0.8.2 release, in any case.;;;","22/Oct/14 06:08;becket_qin;Updated reviewboard https://reviews.apache.org/r/26373/diff/
 against branch origin/trunk;;;","22/Oct/14 16:45;nehanarkhede;[~noslowerdna] This is pretty corner case and we are trying to get it in 0.8.2 if it can go through some testing. 
[~becket_qin] I think my comment may have been lost in the reviewboard, so reposting it here - In order to accept this patch, I'd like us to repeat the kind of testing that was done to find this bug. Did you get a chance to do that on your latest patch?;;;","22/Oct/14 17:08;becket_qin;[~nehanarkhede] I haven't got a chance to do tests yet... I'll do the test later this week and verify if it works.;;;","22/Oct/14 18:56;jjkoshy;[~becket_qin] here are some steps to reproduce locally. There are probably simpler steps, but I ran into it while debugging something else, so here you go:

* Set up three brokers. Sample config: https://gist.github.com/jjkoshy/1ec36e5cef41ac4bd8fb (You will need to edit the logs directory and port)
* Create 50 topics;  each with 4 partitions; replication factor 2 {code}for i in {1..50}; do ./bin/kafka-topics.sh --create --topic test$i --zookeeper localhost:2181 --partitions 4 --replication-factor 2; done{code}
* Run producer performance: {code}./bin/kafka-producer-perf-test.sh --threads 4 --broker-list localhost:9092,localhost:9093 --vary-message-size --messages 922337203685477580 --topics test1,test2,test3,test4,test5,test6,test7,test8,test9,test10,test11,test12,test13,test14,test15,test16,test17,test18,test19,test20,test21,test22,test23,test24,test25,test26,test27,test28,test29,test30,test31,test32,test33,test34,test35,test36,test37,test38,test39,test40,test41,test42,test43,test44,test45,test46,test47,test48,test49,test50 --message-size 500{code}
* Parallel hard kill of all brokers: {{pkill -9 -f Kafka}}
* Kill producer performance
* Restart brokers
* You should see ""WARN No checkpointed highwatermark is found for partition...""
;;;","28/Oct/14 00:19;becket_qin;Updated reviewboard https://reviews.apache.org/r/26373/diff/
 against branch origin/trunk;;;","30/Oct/14 17:58;jjkoshy;Patch is good, but could you rebase?;;;","30/Oct/14 22:07;becket_qin;Updated reviewboard https://reviews.apache.org/r/26373/diff/
 against branch origin/trunk;;;","30/Oct/14 22:10;becket_qin;Updated reviewboard https://reviews.apache.org/r/26373/diff/
 against branch origin/trunk;;;","30/Oct/14 22:38;becket_qin;Updated reviewboard https://reviews.apache.org/r/26373/diff/
 against branch origin/trunk;;;","30/Oct/14 22:50;becket_qin;Updated reviewboard https://reviews.apache.org/r/26373/diff/
 against branch origin/trunk;;;","30/Oct/14 23:33;jjkoshy;Thanks for the patch - committed to 0.8.2 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
some more jars in our src release,KAFKA-1645,12743432,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,charmalloc,joestein,joestein,23/Sep/14 04:28,22/Jul/15 02:47,14/Jul/23 05:39,25/Sep/14 04:26,,,,0.8.2.0,,,,,,,,,,0,,,,"The first one is being taken care of in KAFKA-1490 

the rest... can we just delete them? Do we need/want them anymore? 

{code}

root@precise64:~/kafka-0.8.1.1-src# find ./ -name *jar
./gradle/wrapper/gradle-wrapper.jar
./lib/apache-rat-0.8.jar
./system_test/migration_tool_testsuite/0.7/lib/kafka-0.7.0.jar
./system_test/migration_tool_testsuite/0.7/lib/kafka-perf-0.7.0.jar
./system_test/migration_tool_testsuite/0.7/lib/zkclient-0.1.jar
./contrib/hadoop-consumer/lib/piggybank.jar
./contrib/hadoop-producer/lib/piggybank.jar

{code}

rat is not required in the project I can speak for that file +1 to remove it

I don't see why we have to keep the other ones nor what code changes we have to make for getting rid of them.",,charmalloc,joestein,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-863,,,,,,,,,,,,,"24/Sep/14 14:11;charmalloc;KAFKA-1645.patch;https://issues.apache.org/jira/secure/attachment/12670968/KAFKA-1645.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 25 04:14:36 UTC 2014,,,,,,,,,,"0|i20dmv:",9223372036854775807,,junrao,,,,,,,,,,,,,,,,,,"24/Sep/14 05:30;junrao;For all the jars under migration tool, they are used for system tests. Now that 0.8 has been out for a year. I fell there is not a strong need to maintain the migration tool for much longer.  We can probably just remove the migration tool test suite. For the piggybank jar, it's used only in hadoop-producer. piggybank-0.12.0.jar is available in maven and seems to work with the hadoop producer code. We can just remove the jar and add the dependency to piggybank in the build.gradle file.;;;","24/Sep/14 14:11;charmalloc;Created reviewboard https://reviews.apache.org/r/25992/
 against branch origin/trunk;;;","25/Sep/14 04:14;junrao;Thanks for the patch. +1. Committed to trunk after fixing the issues listed in the RB.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Java New Producer Kafka Trunk] CPU Usage Spike to 100% when network connection is lost,KAFKA-1642,12742642,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,ewencp,Bmis13,Bmis13,19/Sep/14 04:26,06/Jan/15 18:57,14/Jul/23 05:39,06/Jan/15 18:57,0.8.2.0,,,0.8.2.0,,,,,,,producer ,,,2,,,,"I see my CPU spike to 100% when network connection is lost for while.  It seems network  IO thread are very busy logging following error message.  Is this expected behavior ?
2014-09-17 14:06:16.830 [kafka-producer-network-thread] ERROR org.apache.kafka.clients.producer.internals.Sender - Uncaught error in kafka producer I/O thread: 

java.lang.IllegalStateException: No entry found for node -2

at org.apache.kafka.clients.ClusterConnectionStates.nodeState(ClusterConnectionStates.java:110)

at org.apache.kafka.clients.ClusterConnectionStates.disconnected(ClusterConnectionStates.java:99)

at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:394)

at org.apache.kafka.clients.NetworkClient.maybeUpdateMetadata(NetworkClient.java:380)

at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:174)

at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:175)

at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:115)

at java.lang.Thread.run(Thread.java:744)

Thanks,

Bhavesh",,becket_qin,Bmis13,donnchadh,ewencp,jkreps,junrao,nehanarkhede,soumen.sarkar,stevenz3wu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/14 01:52;Bmis13;0001-Initial-CPU-Hish-Usage-by-Kafka-FIX-and-Also-fix-CLO.patch;https://issues.apache.org/jira/secure/attachment/12683253/0001-Initial-CPU-Hish-Usage-by-Kafka-FIX-and-Also-fix-CLO.patch","01/Dec/14 23:49;ewencp;KAFKA-1642.patch;https://issues.apache.org/jira/secure/attachment/12684506/KAFKA-1642.patch","17/Oct/14 18:37;ewencp;KAFKA-1642.patch;https://issues.apache.org/jira/secure/attachment/12675537/KAFKA-1642.patch","21/Oct/14 00:34;ewencp;KAFKA-1642_2014-10-20_17:33:57.patch;https://issues.apache.org/jira/secure/attachment/12675989/KAFKA-1642_2014-10-20_17%3A33%3A57.patch","23/Oct/14 23:19;ewencp;KAFKA-1642_2014-10-23_16:19:41.patch;https://issues.apache.org/jira/secure/attachment/12676774/KAFKA-1642_2014-10-23_16%3A19%3A41.patch","06/Jan/15 02:56;ewencp;KAFKA-1642_2015-01-05_18:56:55.patch;https://issues.apache.org/jira/secure/attachment/12690231/KAFKA-1642_2015-01-05_18%3A56%3A55.patch",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 06 18:57:19 UTC 2015,,,,,,,,,,"0|i208p3:",9223372036854775807,,junrao,,,,,,,,,,,,,,,,,,"19/Sep/14 16:55;jkreps;The intended behavior is that the client will periodically attempt to reconnect and update metadata until either it can reconnect or it discovers that a new node has taken over leadership for the given partition.

There are two things that could be going on here: (1) our default backoffs could be too low or (2) the network selector could be busy waiting. The backoffs are controlled by reconnect.backoff.ms and retry.backoff.ms. reconnect.backoff.ms controls the amount of time to wait after the last connection attempt (whether successful or unsuccessful) before trying to make another connection attempt--this avoids trying to connect over and over again. This seems to default to only 10ms. The retry.backoff.ms controls the amount of time we wait before attempting to update the metadata. This defaults to 100ms.

Alternatively, [~guozhang] found and fixed a bug in the network selector that lead to busy waiting previously. Maybe there is another bug like that.

Would you be willing to try setting the two backoffs to something high and see if you can reproduce the problem. The ideal would be a short piece of code that reproduces this that we could use for testing.;;;","25/Sep/14 18:42;Bmis13;HI [~jkreps],

I will work on the sample program. We are not setting reconnect.backoff.ms and retry.backoff.ms configuration so it would be default configuration.  Only thing I can tell you is that I have 4 Producer instances per JVM.  So this might amplify issue. 

Thanks,

Bhavesh ;;;","26/Sep/14 19:49;jkreps;Yeah a sample program that reproduces the issue would be excellent. That will help us take a look.;;;","13/Oct/14 15:44;Bmis13;{code}


import java.io.IOException;
import java.io.InputStream;
import java.util.Properties;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;

import org.apache.kafka.clients.producer.Callback;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;

public class TestNetworkDownProducer {
	
	static int numberTh = 200;
	static CountDownLatch latch = new CountDownLatch(200);
	public static void main(String[] args) throws IOException, InterruptedException {

		Properties prop = new Properties();
		InputStream propFile = Thread.currentThread().getContextClassLoader()
				.getResourceAsStream(""kafkaproducer.properties"");

		String topic = ""test"";
		prop.load(propFile);
		System.out.println(""Property: "" + prop.toString());
		StringBuilder builder = new StringBuilder(1024);
		int msgLenth = 256;
		for (int i = 0; i < msgLenth; i++)
			builder.append(""a"");

		int numberOfProducer = 4;
		Producer[] producer = new Producer[numberOfProducer];

		for (int i = 0; i < producer.length; i++) {
			producer[i] = new KafkaProducer(prop);
		}
		ExecutorService service =   new ThreadPoolExecutor(numberTh, numberTh,
                0L, TimeUnit.MILLISECONDS,
                new LinkedBlockingQueue<Runnable>(numberTh *2));
		
		for(int i = 0 ; i < numberTh;i++){
			service.execute(new MyProducer(producer,100000,builder.toString(), topic));
		}		
		latch.await();
		
		System.out.println(""All Producers done...!"");
		for (int i = 0; i < producer.length; i++) {
			producer[i].close();
		}		
		service.shutdownNow();
		System.out.println(""All done...!"");

	}


	
	static class MyProducer implements Runnable {
		
		Producer[] producer;
		long maxloops;
		String msg ;
		String topic;
		
		MyProducer(Producer[] list, long maxloops,String msg,String topic){
			this.producer = list;
			this.maxloops = maxloops;
			this.msg = msg;
			this.topic = topic;
		}
		public void run() {
			ProducerRecord record = new ProducerRecord(topic, msg.toString().getBytes());
			Callback  callBack = new  MyCallback();
			try{
				for(long j=0 ; j < maxloops ; j++){
					try {
						for (int i = 0; i < producer.length; i++) {
							producer[i].send(record, callBack);
						}
						Thread.sleep(10);
					} catch (Throwable th) {
						System.err.println(""FATAL "");
						th.printStackTrace();
					}
				}

			}finally {
				latch.countDown();
			}			
		}
	}	

	static class MyCallback implements Callback {
		public void onCompletion(RecordMetadata metadata, Exception exception) {
			if(exception != null){
				System.err.println(""Msg dropped..!"");
				exception.printStackTrace();
			}
			
		}
	}
	
}
{code}

This is property file used:
{code}
# THIS IS FOR NEW PRODUCERS API TRUNK Please see the configuration at https://kafka.apache.org/documentation.html#newproducerconfigs
# Broker List
bootstrap.servers= BROKERS HERE...
#Data Acks
acks=1
# 64MB of Buffer for log lines (including all messages).
buffer.memory=134217728
compression.type=snappy
retries=3
# DEFAULT FROM THE KAFKA...
# batch size =  ((buffer.memory) / (number of partitions)) (so we can have in progress batch size created for each partition.).
batch.size=1048576
#2MiB
max.request.size=1048576
send.buffer.bytes=2097152
# We do not want to block the buffer Full so application thread will not be blocked but logs lines will be dropped...
block.on.buffer.full=false
#2MiB
send.buffer.bytes=2097152
#wait...
linger.ms=5000
{code};;;","13/Oct/14 16:12;Bmis13; [~jkreps]  Let me know if you need any other help !!

Thanks,
Bhavesh ;;;","16/Oct/14 23:36;Bmis13;[~jkreps],

Did you get chance to re-produce the problem ?  Has someone else reported this issues or similar issue ?

Thanks,

Bhavesh ;;;","17/Oct/14 18:37;ewencp;Created reviewboard https://reviews.apache.org/r/26885/diff/
 against branch origin/trunk;;;","21/Oct/14 00:34;ewencp;Updated reviewboard https://reviews.apache.org/r/26885/diff/
 against branch origin/trunk;;;","21/Oct/14 00:42;ewencp;To summarize the issues fixed now:
* Fix logic issue with ""expired"" in RecordAccumulator.ready
* Don't include nodes that can send data when computing the delay until the next check for ready data. Including these doesn't make sense since their delays will change when we send data.
* To correctly account for nodes with sendable data, use a timeout of 0 if we send any. This guarantees any necessary delay is computed immediately in the next round after some current data has been removed.
* Properly account for nodes with sendable data under connection retry backoff. Since they weren't included in computing the next check delay when looking up ready nodes, we need to account for it later, but only if we conclude the node isn't ready. We need to incorporate the amount of backoff time still required before a retry will be performed (nothing else would wakeup at the right time, unlike other conditions like a full buffer which only change if data is received).

It might be possible to break this into smaller commits for each one, but the ordering of applying them needs to be careful because some by themselves result in bad behavior -- the existing client worked because it often ended up with poll timeouts that were much more aggressive (i.e., often 0).;;;","23/Oct/14 23:19;ewencp;Updated reviewboard https://reviews.apache.org/r/26885/diff/
 against branch origin/trunk;;;","14/Nov/14 18:59;ewencp;[~junrao] I think you reviewed most of this already since we discussed it offline, so I reassigned to you. I think this should be in good shape for committing.;;;","14/Nov/14 22:30;junrao;Thanks for the patch. Committed to trunk.;;;","14/Nov/14 22:55;junrao;Since this is relatively critical and the changes are only in the new java producer, double committed to 0.8.2 as well.;;;","24/Nov/14 01:29;Bmis13;The patch provided does not solve the problem.  When you have more than one or more producer instance,  the effect amplifies. 

org.apache.kafka.clients.producer.internals.Send.run() takes 100% CPU due to infinite  loop when there is no brokers (no work to be done to dump data).


Thanks,
Bhavesh ;;;","24/Nov/14 01:52;Bmis13;Please take look at experimental patch that solve this problem by capturing the correct Node state and also not so elegant by exponential backoff run() method by sleeping (many of the value is hard coded but it is just experimental).

Also, there is another problem close() method on producer does not exit and JVM does not gracefully  shutdown because io thread is spinning in  while loop during network outage.  This is also another edge case. 

I hope this will be very helpful and solve problem.

Thanks,

Bhavesh ;;;","24/Nov/14 05:31;ewencp;[~Bmis13] I've only taken a quick look at the patch, but Sender.run() is intentionally a tight loop. The body of the loop calls Sender.run(time), which in turn calls Client.poll() with a timeout that *should* keep it from actually being a tight loop. The previous patch fixed some issues in those other methods that were causing the timeout to incorrectly be 0, leading to the high CPU usage. If you're still seeing this problem, the right fix will almost definitely involve tracking down how a 0 (or very small) timeout is consistently being computed.

The way I verified the previous patch was simple -- I ran the producer against a local cluster and then just disabled the network connection. Can you describe how you produce the error now? Things like whether there are already active connections to the brokers, if it's time sensitive (e.g. takes a certain amount of time to start using CPU), exactly how you simulate the network failure, and whether the issue is consistent or only happens intermittently would all be helpful details to know.

The issue with it not shutting down is probably because the producer doesn't timeout messages when the leader it needs to send them to isn't available, but it waits to send any outstanding messages before shutting down. KAFKA-1788 is probably really the same issue since it's also caused by messages that never get sent and don't time out, although since that issue is specifically about the RecordAccumulator/buffer pool accounting, a fix for that issue may or may not fix the shutdown issue you're describing here.
;;;","24/Nov/14 17:25;Bmis13;[~ewencp],

The way to reproduce this is to simulate network instability by turning on and off network service (or turn on/off physical cable).   The connect and see if recover and disconnect and connect again etc.. you will see the behavior again and again. 

The issue is also with connection state management :

{code}
private void initiateConnect(Node node, long now) {
        try {
            log.debug(""Initiating connection to node {} at {}:{}."", node.id(), node.host(), node.port());
            // TODO FIX java.lang.IllegalStateException: No entry found for node -3 (We need put before remove it..)..
            this.connectionStates.connecting(node.id(), now);  (This line has problem because it will loose previous last attempt made get above exception and it will try to connect to that node for ever and ever with exception )
            selector.connect(node.id(), new InetSocketAddress(node.host(), node.port()), this.socketSendBuffer, this.socketReceiveBuffer);
        } catch (IOException e) {
            /* attempt failed, we'll try again after the backoff */
            connectionStates.disconnectedWhenConnectting(node.id());
            /* maybe the problem is our metadata, update it */
            metadata.requestUpdate();
            log.debug(""Error connecting to node {} at {}:{}:"", node.id(), node.host(), node.port(), e);
        }
    }
{code}

In my opinion, regardless of what node status is in run() method needs to be safe-guarded from still CPU Cycle when there is no state for Node.  (Hence I have added exponential sleep as temp solution to not to stealing CPU cycle , I think must protect it some how and must check the execution time...)

Please let me know if you need more info  and i will be more than happy to reproduce bug and we can have conference call, and I can show you the problem.

Based on code diff I have done from 0.8.1.1 tag and this.  This issue also occur in  0.8.1.1 as well I think.

Thanks,
Bhavesh 

;;;","24/Nov/14 21:20;Bmis13;[~ewencp],

Also Regarding KafkaProder.close() method hangs for ever because of following loop, and 

{code}
Sender.java

 // okay we stopped accepting requests but there may still be
// requests in the accumulator or waiting for acknowledgment,
// wait until these are completed.
while (this.accumulator.hasUnsent() || this.client.inFlightRequestCount() > 0) {
try {
run(time.milliseconds());
} catch (Exception e) {
log.error(""Uncaught error in kafka producer I/O thread: "", e);
}
}

KafkaProducer.java

 /**
* Close this producer. This method blocks until all in-flight requests complete.
*/
@Override
public void close() {
log.trace(""Closing the Kafka producer."");
this.sender.initiateClose();
try {
this.ioThread.join();  // THIS IS BLOCKED since ioThread does not give up so it is all related in my opinion.
} catch (InterruptedException e) {
throw new KafkaException(e);
}
this.metrics.close();
log.debug(""The Kafka producer has closed."");
}
{code}

The issue describe in KAFKA-1788  is likelihood, but if you look the close call stack then calling thread that initiated the close() will hang till io thread dies (which it never dies when data is there and network is down).  

Thanks,

Bhavesh
;;;","24/Nov/14 21:43;Bmis13;Here is exact steps how to reproducer it bug: (Must have demon program continuously running).

1)  Start with happy Situation where all borkers are up everything is running fine.  And verify all top -pid JAVA_PID and your kit (kafka network threads  are taking less than 4% CPU).
2)  Shutdown network (turn off network or pull the eth0 cable)  wait for while and you will see that CPU spike to 325% under top  (if you have 4 producer) and verify your kit is showing 25% CPU consumption for for each Kafka io thread.
3) Connect back the network ( Spike will still be there but CPU after while come down to 100% or so ) and remain connected for while.  
4) again simulate network failure (to simulate network instability) repeat steps again 1 to 4 but wait for 10 or so minutes in between and you will see the trends of CPU spike along with above exception. java.lang.IllegalStateException: No entry found for node -2

Also, I see that Kafka is logging excessively when network is down (your kit shows it is taking more CPU Cycle  as compare  to normal)

Thanks,
Bhavesh ;;;","24/Nov/14 22:15;Bmis13;Also, there is issue in my experimental patch.  I did not update the lastConnectAttemptMs...in connecting state method to solve the issue with illegal sate exp:
{code}
 /**
     * Enter the connecting state for the given node.
     * @param node The id of the node we are connecting to
     * @param now The current time.
     */
    public void connecting(int node, long now) {
    	NodeConnectionState nodeConn = nodeState.get(node); 
    	if(nodeConn == null){
    		nodeState.put(node, new NodeConnectionState(ConnectionState.CONNECTING, now));
    	}else{
    		nodeConn.state = ConnectionState.CONNECTING;
    		nodeConn.lastConnectAttemptMs = now;  (This will capture and update last connection attempt) 
    		
    	}
    }
{code};;;","24/Nov/14 23:25;ewencp;Ok, so as I suspected, you need to wait awhile before the issue shows up. It looks to me like this is due to a metadata refresh. This causes metadataTimeout in Client.poll() to be 0, but then maybeUpdateMetadata is unable to make any progress since it can't connect to any nodes. The previous patch fixed issues that caused the timeout parameter to that method to be 0, so this is a similar issue. However, under normal testing it won't always show up immediately -- you need to wait until the next metadata refresh, which is currently every 5 minutes.

I need to think more about the details of the fix. That timeout shouldn't consistently be 0 if we're just trying to refresh metadata, but we need to make sure we select an appropriate timeout for each case. Looking through maybeUpdateMetadata there are a few different possibilities:

1. leastLoadedNode returns null, leaving no nodes available and we don't even try to refresh
2. The selected node is connected and we can send more data - we mark metadataFetchInProgress to avoid resending requests, but should probably also use that to determine the timeout on poll()
3. The selected node is connected but we can't send more data yet
4. The selected node is not connected, but we are allowed to try to initiate a connection based on the reconnection backoff.
4a. Trying to initiate the connection may return an immediate error
4b. Or we'll need to wait for the connection event.
5. The selected node is not connected and we aren't allowed to initiate a new connection yet.

Given that all these conditions are based on the code in maybeUpdateMetadata (and initiateConnect, which it calls), it probably makes sense to have that code return an appropriate timeout to be used in poll(). But we need to make sure the selected values are also combined correctly with the timeout passed into poll() and that any wakeups before that time also subsequently produce correct values.

The logic in the Sender.run() and NetworkClient.poll() are complex and need to handle a lot of different cases, but it should be possible to fix this problem only by adjusting that code without adding retries/backoff further up the stack. The core of this problem is just that that loop is selecting too small a timeout.;;;","24/Nov/14 23:47;Bmis13;[~ewencp],

Thanks for looking into this really appreciate your response. 

Also, do you think rapid connect and disconnect is also due to incorrect Node state management ?  connecting method and initiateConnection also ?

Also, Can we also take the defensive coding and have protection in this tight infinite loop to throttle CPU cycle if it ends up with start-end duration is below some xx ms.  This will actually prevent this issues.    We had this issue on Prod so I just wanted to highlight the impact of 325% CPU and excessive logging. 

Thanks,

Bhavesh ;;;","25/Nov/14 04:23;Bmis13;Here are some more cases to reproduce this simulating network connection issue with one of brokers only and still problem persist:

Case 1:  brokers connection is down (note according to ZK leader for partition still with b1 ) 
Have tree brokers: b1, b2, b3
1)  Start your daemon program and keep sending data to all the brokers and continue sending some data 
2)  Observed that you have data  netstat -a | grep b1|b2|b3   (keep pumping data for 5 minutes and observed normal behavior using top -pid or top -p java_pid )
3) Simulate a network connection or problem establishing new TCP connection via following as java program still continues to pump data aggressively (please note TCP connection to B1 still active and connected)
a)  sudo vi /etc/hosts 2) add entry ""b1 127.0.0.1"" 
b) /etc/init.d/network restart  after while (5 to 7 minutes you will see the issue but keep pumping data, and also repeat this for b2 it will be more CPU consumption) 
 
4) Under a heavy dumping data, now producer will try to establish new TCP connection to B1 and it will get connection refused (Note that CPU spikes up again and remain in state) just because could not establish.

Case 2) Simulate Firewall rule such as you are only allowed (4 TCP connection to each brokers) 

Do step 1,2 and 3 above.
4) use Iptable rule to reject 
To start an ""enforcing fire wall"":
iptables -A OUTPUT -p tcp -m tcp -d b1 --dport 9092 -j REJECT
5) Still pump data will while iptable rejects ( you will see CPU spike to to 200% more depending on # of producer)
To ""recover"" :
iptables -D OUTPUT -p tcp -m tcp -d b1 --dport 9092 -j REJECT
;;;","25/Nov/14 04:28;Bmis13;[~ewencp],

I hope above steps will give you comprehensive steps to reproduce problems with run() method.  It would be really great if we can make the client more resilient and  robust so network and brokers instability does not cause CPU spikes and degrade application performance. Hence, I would strongly at least detect the time run(time) is taking and do some stats based on some configuration, we can do CPU Throttling (if need) just to be more defensive or at lest detect that io thread is taking CPU cycle.

By the way the experimental patch still works for steps describe above as well due to hard coded back-off. 

Any time you have patch or any thing, please let me know I will test it out ( you have my email id) .  Once again thanks for your detail analysis and looking at this at short notice.  

Please look into to ClusterConnectionStates and how it manage the state of node when disconnecting immediately . 

please look into  connecting(int node, long now) and this (I feel connecting needs to come before not after).
selector.connect(node.id(), new InetSocketAddress(node.host(), node.port()), this.socketSendBuffer, this.socketReceiveBuffer);
this.connectionStates.connecting(node.id(), now);

Also, I still feel that produce.close() is also needs to be looked at (join() method with some configuration time out so thread does not hang)

Thanks,

Bhavesh  ;;;","25/Nov/14 04:43;Bmis13;Also, Are you going to port back the patch to 0.8.1.1 version as well ?  Please let me know also.

Thanks,
Bhavesh ;;;","26/Nov/14 20:08;Bmis13;[~ewencp],

Even setting long following parameter, states of system does get impacted does not matter what reconnect.backoff.ms and retry.backoff.ms is set to.  Once Node state is removed, the time out is set to 0.  Please see the following logs.  

#15 minutes
reconnect.backoff.ms=900000
retry.backoff.ms=900000

{code}
2014-11-26 11:01:27.898 Kafka Drop message topic=.rawlog
org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
 2014-11-26 11:02:27.903 Kafka Drop message topic=.rawlog
org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
 2014-11-26 11:03:27.903 Kafka Drop message topic=.rawlog
org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
 2014-11-26 11:04:27.903 Kafka Drop message topic=.rawlog
org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
 2014-11-26 11:05:27.904 Kafka Drop message topic=.rawlog
org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
 2014-11-26 11:06:27.905 Kafka Drop message topic=.rawlog
org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
 2014-11-26 11:07:27.906 Kafka Drop message topic=.rawlog
org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
 2014-11-26 11:08:27.908 Kafka Drop message topic=.rawlog
org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
 2014-11-26 11:09:27.908 Kafka Drop message topic=.rawlog
org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
 2014-11-26 11:10:27.909 Kafka Drop message topic=.rawlog
org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
 2014-11-26 11:11:27.909 Kafka Drop message topic=.rawlog
org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
 2014-11-26 11:12:27.910 Kafka Drop message topic=.rawlog
org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
 2014-11-26 11:13:27.911 Kafka Drop message topic=.rawlog
org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
 2014-11-26 11:14:27.912 Kafka Drop message topic=.rawlog
org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
 2014-11-26 11:15:27.914 Kafka Drop message topic=.rawlog
org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 60000 ms.
 2014-11-26 11:00:27.613 [kafka-producer-network-thread | heartbeat] ERROR org.apache.kafka.clients.producer.internals.Sender - Uncaught error in kafka producer I/O thread: 
 2014-11-26 11:00:27.613 [kafka-producer-network-thread | rawlog] ERROR org.apache.kafka.clients.producer.internals.Sender - Uncaught error in kafka producer I/O thread: 
java.lang.IllegalStateException: No entry found for node -1
	at org.apache.kafka.clients.ClusterConnectionStates.nodeState(ClusterConnectionStates.java:131)
	at org.apache.kafka.clients.ClusterConnectionStates.disconnected(ClusterConnectionStates.java:120)
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:407)
	at org.apache.kafka.clients.NetworkClient.maybeUpdateMetadata(NetworkClient.java:393)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:184)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:115)
	at java.lang.Thread.run(Thread.java:744)
java.lang.IllegalStateException: No entry found for node -3
	at org.apache.kafka.clients.ClusterConnectionStates.nodeState(ClusterConnectionStates.java:131)
	at org.apache.kafka.clients.ClusterConnectionStates.disconnected(ClusterConnectionStates.java:120)
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:407)
	at org.apache.kafka.clients.NetworkClient.maybeUpdateMetadata(NetworkClient.java:393)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:184)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:115)
	at java.lang.Thread.run(Thread.java:744)
 2014-11-26 11:00:27.613 [kafka-producer-network-thread | heartbeat] ERROR org.apache.kafka.clients.producer.internals.Sender - Uncaught error in kafka producer I/O thread: 
 2014-11-26 11:00:27.613 [kafka-producer-network-thread | error] ERROR org.apache.kafka.clients.producer.internals.Sender - Uncaught error in kafka producer I/O thread: 
 2014-11-26 11:00:27.613 [kafka-producer-network-thread | event] ERROR org.apache.kafka.clients.producer.internals.Sender - Uncaught error in kafka producer I/O thread: 
java.lang.IllegalStateException: No entry found for node -1
	at org.apache.kafka.clients.ClusterConnectionStates.nodeState(ClusterConnectionStates.java:131)
	at org.apache.kafka.clients.ClusterConnectionStates.disconnected(ClusterConnectionStates.java:120)
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:407)
	at org.apache.kafka.clients.NetworkClient.maybeUpdateMetadata(NetworkClient.java:393)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:184)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:115)
	at java.lang.Thread.run(Thread.java:744)
java.lang.IllegalStateException: No entry found for node -1
	at org.apache.kafka.clients.ClusterConnectionStates.nodeState(ClusterConnectionStates.java:131)
	at org.apache.kafka.clients.ClusterConnectionStates.disconnected(ClusterConnectionStates.java:120)
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:407)
	at org.apache.kafka.clients.NetworkClient.maybeUpdateMetadata(NetworkClient.java:393)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:184)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:115)
	at java.lang.Thread.run(Thread.java:744)
 2014-11-26 11:00:27.613 [kafka-producer-network-thread | error] ERROR org.apache.kafka.clients.producer.internals.Sender - Uncaught error in kafka producer I/O thread: 
java.lang.IllegalStateException: No entry found for node -1
	at org.apache.kafka.clients.ClusterConnectionStates.nodeState(ClusterConnectionStates.java:131)
	at org.apache.kafka.clients.ClusterConnectionStates.disconnected(ClusterConnectionStates.java:120)
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:407)
	at org.apache.kafka.clients.NetworkClient.maybeUpdateMetadata(NetworkClient.java:393)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:184)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:115)
	at java.lang.Thread.run(Thread.java:744)
 2014-11-26 11:00:27.613 [kafka-producer-network-thread | rawlog] ERROR org.apache.kafka.clients.producer.internals.Sender - Uncaught error in kafka producer I/O thread: 
java.lang.IllegalStateException: No entry found for node -1
	at org.apache.kafka.clients.ClusterConnectionStates.nodeState(ClusterConnectionStates.java:131)
	at org.apache.kafka.clients.ClusterConnectionStates.disconnected(ClusterConnectionStates.java:120)
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:407)
	at org.apache.kafka.clients.NetworkClient.maybeUpdateMetadata(NetworkClient.java:393)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:184)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:115)
	at java.lang.Thread.run(Thread.java:744)
 2014-11-26 11:00:27.613 [kafka-producer-network-thread | error] ERROR org.apache.kafka.clients.producer.internals.Sender - Uncaught error in kafka producer I/O thread: 
java.lang.IllegalStateException: No entry found for node -3
	at org.apache.kafka.clients.ClusterConnectionStates.nodeState(ClusterConnectionStates.java:131)
	at org.apache.kafka.clients.ClusterConnectionStates.disconnected(ClusterConnectionStates.java:120)
	at org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:407)
	at org.apache.kafka.clients.NetworkClient.maybeUpdateMetadata(NetworkClient.java:393)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:187)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:184)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:115)
	at java.lang.Thread.run(Thread.java:744)
{code};;;","27/Nov/14 05:37;soumen.sarkar;Is it reasonable to expect that timeout should have a lower bound (say *100 ms*) instead of being 0?;;;","28/Nov/14 01:55;Bmis13;[~soumen.sarkar],

Time out is one thing, but also IO Thread needs to be safe guarded to see how aggressive it is based on network and data to be send.  So it does not consume so much CPU cycle.

Thanks,
Bhavesh ;;;","30/Nov/14 20:24;Bmis13;I just discovered yesterday that 0.8.1.1 release does not have new producer code base jar officially released jar (kafka-clients) although code is there in 0.8.1.1 branch.   That created confusion about porting to  0.8.1.1.
  
Thanks,

Bhavesh ;;;","01/Dec/14 23:49;ewencp;Created reviewboard https://reviews.apache.org/r/28582/diff/
 against branch origin/trunk;;;","01/Dec/14 23:54;ewencp;Attached a new patch that fixes all the timeout issues I'm aware of. Here's how it addresses each of the situations I listed earlier:

1. lastNoNodeAvailableMs is updated, which forces metadata timeout for each poll to be  use a backoff period

2. Added another backoff value based on metadataFetchInProgress. Since the request actually made it out, this can be arbitrarily large -- we just need to see some sort of response or failure for the request.

3. Requires some response to arrive to clear out space, so we can wait arbitrarily long. Updating lastNoNodeAvailableMs works even though it may wake up sooner than necessary. But making all cases that didn't send the data use a single approach keeps the code simpler.

4a. This can happen if, e.g., the network interface has been taken down entirely. After fixing the ordering of marking the node as connecting and issuing the request, this cleans up after that error cleanly. lastNoNodeAvailableMs is updated since there *weren't* any nodes available. This triggers a backoff period where the connection won't be retried.

4b. This can be handled in the same way - we set lastNoNodeAvailableMs whether or not we immediately saw an error from the connection request. This causes it to sleep while waiting for the connection request. This may wake up before we get connected. However, if the node is still in the connecting state, it'll be ignored during the next round and we'll either start trying to connect to another node or we'll end up in state 1 with no nodes available. Either way, we still only wake up periodically based on the this timeout.

5. Looking more carefully at how leastLoadedNode works, this case isn't actually possible.

One additional note -- apparently you can't use Long.MAX_VALUE as a timeout, it throws an exception. That's why Integer.MAX_VALUE is there instead. We could also detect the large value and convert it to a negative value instead, which the underlying API treats as having no timeout.

[~Bmis13] can you test this out for the failure modes you found?;;;","02/Dec/14 17:59;stevenz3wu;this sounds like a serious blocker issue. once this fix is confirmed and merged, is there any chance for a patch release of 0.8.2.1? ;;;","02/Dec/14 18:02;ewencp;[~stevenz3wu] this is already a blocker on 0.8.2. 0.8.2 hasn't been released yet, only a beta.;;;","02/Dec/14 18:18;stevenz3wu;[~ewencp] forgot 0.8.2 is still beta, because we are already using 0.8.2-beta now. thanks for the clarification.;;;","02/Dec/14 20:00;Bmis13;Hi  [~ewencp],

I will not have time to validate this patch till next week.  

Here is my comments:

1) Producer.close() method issue is not address with patch. In event of network connection lost or other events happens, IO thread will not be killed and close method hangs. In patch that I have provided, I had timeout for join method and interrupted IO thread.  I think we need similar solution.

2) Also, can we please add JMX monitoring for IO tread to know how quick it is running.  It will great to add this and run() method will report duration to metric in nano sec.
{code}
            try{
            	ThreadMXBean bean = ManagementFactory.getThreadMXBean( );
            	if(bean.isThreadCpuTimeSupported() && bean.isThreadCpuTimeEnabled()){
            		this.ioTheadCPUTime = metrics.sensor(""iothread-cpu"");
                    this.ioTheadCPUTime.add(""iothread-cpu-ms"", ""The Rate Of CPU Cycle used by iothead in NANOSECONDS"", new Rate(TimeUnit.NANOSECONDS) {
                        public double measure(MetricConfig config, long now) {
                            return (now - metadata.lastUpdate()) / 1000.0;
                        }
                    });	            		
            	}
            }catch(Throwable th){
            	log.warn(""Not able to set the CPU time... etc"");
            }
{code}

3)  Please check the timeout final value in *pollTimeout* if it is zero for constantly then we need to slow IO thread down.

4)  Defensive check is need for back off  in run() method when IO thread is aggressive.  

{code}

        while (running) {
        	long start = time.milliseconds();
            try {
                run(time.milliseconds());        
            } catch (Exception e) {
                log.error(""Uncaught error in kafka producer I/O thread: "", e);
            }finally{
            	long durationInMs = time.milliseconds() - start;
            	// TODO Fix ME HERE GET DO exponential back-off sleep etc to prevent still CPU CYCLE HERE ?????? How Much ...for the edge case...
            	if(durationInMs < 200){
            		if(client.isAllRegistredNodesAreDown()){
            			countinuousRetry++;
            			 /// TODO MAKE THIS CONSTANT CONFIGURATION..... when do we rest this interval ????? so we can try aggressive again...
            			sleepInMs = ((long) Math.pow(2, countinuousRetry) * 500);
            		}else{
            			sleepInMs =  500 ; 
            			countinuousRetry = 0;
            		}
            		
            		// Wait until the desired next time arrives using nanosecond
            		// accuracy timer (wait(time) isn't accurate enough on most platforms) 
            		try {
            			// TODO SLEEP IS NOT GOOD SOLUTON..
						Thread.sleep(sleepInMs);
					} catch (InterruptedException e) {
						log.error(""While sleeping some one interupted this tread probally close method on prodcuer close () "");
					}  
            	}
            }
        }
{code}

5)  When all nodes are disconnected, do you still want to spin the IO Thread ?

6)  When you have a firewall rule that says ""you can only have 2 concurrent TCP connections from Client to Brokers"" and client still have live TCP connection to same node (Broker), but new TCP connections are rejected. Node State will be marked as Disconnected in initiateConnect ?  Is this case handled gracefully  ?

By the way, thank you very much for quick reply and with new patch.  I appreciate your help.

Thanks,

Bhavesh ;;;","03/Dec/14 00:57;ewencp;[~Bmis13], responses to each item:

1. I'm specifically trying to address the CPU usage here. I realize from your perspective they are closely related since they're both can be triggered by a loss of network connectivity, but internally they're really separate issues -- the CPU usage has to do with incorrect timeouts and the join() issues is due to the lack of timeouts on produce operations. That's why I pointed you toward KAFKA-1788. If a timeout is added for data in the producer, that would resolve the close issue as well since any data waiting in the producer would eventually timeout and the IO thread could exit. I think that's the cleanest solution since it solves both problems with a single setting (the amount of time your willing to wait before discarding data). If you think a separate timeout specifically for Producer.close() is worthwhile I'd suggest filing a separate JIRA for that.

2. I think the measure() implementation you have is incorrect, it looks like the amount of time since the last metadata update. In any case, I think you'd want a sensor there so it would take care of computing the rate for you. Is the use case for this just detecting the CPU spike? I suppose providing the average iteration rate can at least tell you the source of CPU usage compared to just monitoring CPU at the system level. But (as described below) it's not necessarily bad for the rate to be quite high. And an average also doesn't tell you much except maybe in the extreme case that we're fixing with this patch. Before just adding another metric, maybe we should think through exactly what you're trying to measure/monitor and how best to reveal that information?

3. I probably should have given a better explanation earlier for why that approach is problematic. In a lot of cases, you *want* run() to use small timeouts. If you're pumping a lot of data through, saturating your network connection, then you're likely to going to need to poll very fast and end up with consistently small timeouts (e.g. if you produce to many topics and consistently have data flowing through them, you'll end up using the linger period and your timeout will generally be a fraction of that period). In other words, if we've actually given the producer a lot of work to do, then we should expect that it will use short timeouts and eat up CPU. You found that you could work around your particular issue by backing off, but it breaks another very important use case -- high throughput to the producer. I bet if you ran performance tests with it enabled you would find diminished performance. Fundamentally the bug wasn't that we were sometimes computing small timeouts consistently, it was that we were computing *incorrect* timeouts. If we get the timeouts computed correctly, then we'll properly support both cases.

4. See explanation in 3. The rate of this loop is controlled by correct computation of timeouts. I think the only case this code could be an issue is if we compute correct timeouts, but then consistently see exceptions that are only caught by this block. That shouldn't be happening consistenly, and if you find an example where it is then we should catch that exception deeper in the call stack and handle it more gracefully.

5. The thread still has to be available so that when send() is called again it can initiate a connection. What should happen now (after my patch) if there is no work to do is that we'll still run the loop, but the timeout will be very large so it basically won't be using any CPU. If we get a request to send data, a wakeup() call wakes it back up before the timeout so it can start sending data immediately.

6. It should be handled gracefully, and should have been fixed by the original patch (although maybe the fix to the ordering in initiateConnect in the second patch was also necessary). The logic in Sender.run() adjusts timeouts when it finds nodes that aren't ready to send data (i.e. are disconnected) and makes sure we backoff when connection attempts fail. This is a fixed backoff (reconnect.backoff.ms, default 10ms), but is enough to avoid high CPU usage.
;;;","04/Dec/14 16:01;Bmis13;[~ewencp],

1) I will posted toward KAFKA-1788 and perhaps link the issue.
2) True , some sort of measure would be great 5,10...25 50, 95 and 99 percentile would be great of execution time.  The point is just measure the duration report the rate of execution. 
3) Agree with what you are saying and I have observed same behavior.  But only recommendation is to add some intelligence to *timeouts* to detect if for long period and consecutive timeout is zero then there is problem. (Little more defensive) 
4) Again I agree with you point, but based in your previous comments you had mentioned that you may consider having back-off logic further up the chain. So I was just checking run() is best place to do that check.  Again, may be add intelligence here if you get consecutive “Exception” then likelihood of high CPU is high.  
 
5) Ok.  I agree what you are saying is data needs to be de-queue so more data can be en-queue even in event of network lost.  Is my understanding correct ?

6) All I am saying is network firewall rule (such as only 2 TCP connections per source host) or Brokers running out of File Descriptor so new connection to broker is not established but Client have live and active TCP connection to same broker.  But based on what I see in the method * initiateConnect* will mark the entire Broker or Node status as disconnected.  Is this expected behavior?  So question is: will client continue to send data ?

Thank you very much for entertaining my questions so far and I will test out the patch next week.

Thanks,

Bhavesh ;;;","09/Dec/14 06:52;Bmis13;[~stevenz3wu],

0.8.2 is very well tested and worked well under heavy load.  This bug is rare only happen when broker or network has issue.  We have been producing about 7 to 10 TB per day using this new producer, so 0.8.2 is very safe to use in production.  It has survived  pick traffic of the year on large e-commerce site.  So I am fairly confident that  New Java API is indeed does true round-robin and much faster than Scala Based API.

[~ewencp],  I will verify the patch by end of this Friday, but do let me know your understanding based on my last comment. The goal is to rest this issue and cover all the use case.

Thanks,

Bhavesh;;;","23/Dec/14 23:35;Bmis13;[~ewencp],

Patch indeed solve the high CPU Problem reported by this bug.  I have tested all brokers down, one broker down and two broker down (except for last use cases where one of the brokers runs out of Socket File Descriptor a rear case) :  I am sorry for last response, I got busy with other stuff so testing got delayed.

Here are some interesting Observations from YourKit:

0)  Overall, patch has also brought down  overall consumption in Normal Healthy or Happy case where every thing is up and running.  In old code (without patch), I used to see about 10% of overall CPU used by  io threads (4 in my case), it has reduce to 5% or less now with patch.   

1)	When two brokers are down, then occasionally I see IO thread blocked. ( I did not see this when one brokers is down) 

{code}
kafka-producer-network-thread | rawlog [BLOCKED] [DAEMON]
org.apache.kafka.clients.producer.internals.Metadata.fetch() Metadata.java:70
java.lang.Thread.run() Thread.java:744
{code}

2)	record-error-rate metric remain zero despite following firewall rule.  In my opinion, it should have called  org.apache.kafka.clients.producer.Callback  but I did not see that happening either in either one or two brokers down.  Should I file another issue for this ? Please confirm.

{code}

sudo ipfw add reject tcp from me to b1.ip dst-port 9092
sudo ipfw add reject tcp from me to b2.ip dst-port 9092

00100 reject tcp from me to b1.ip dst-port 9092
00200 reject tcp from me to b2.ip dst-port 9092
{code}

{code}
	class LoggingCallBaHandler implements Callback {

		/**
		 * A callback method the user can implement to provide asynchronous
		 * handling of request completion. This method will be called when the
		 * record sent to the server has been acknowledged. Exactly one of the
		 * arguments will be non-null.
		 * 
		 * @param metadata
		 *            The metadata for the record that was sent (i.e. the
		 *            partition and offset). Null if an error occurred.
		 * @param exception
		 *            The exception thrown during processing of this record.
		 *            Null if no error occurred.
		 */
		@Override
		public void onCompletion(RecordMetadata metadata, Exception exception) {
			if(exception != null){
				exception.printStackTrace();
			}
		}
	}
{code}

I do not see any exception at all on console....not sure why ?

3)	Application does NOT gracefully shutdown when there one or more brokers are down. (io Thread never exits this is know issue ) 

{code}
""SIGTERM handler"" daemon prio=5 tid=0x00007f8bd79e4000 nid=0x17907 waiting for monitor entry [0x000000011e906000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at java.lang.Shutdown.exit(Shutdown.java:212)
        - waiting to lock <0x000000070008f7c0> (a java.lang.Class for java.lang.Shutdown)
        at java.lang.Terminator$1.handle(Terminator.java:52)
        at sun.misc.Signal$1.run(Signal.java:212)
        at java.lang.Thread.run(Thread.java:744)

""SIGTERM handler"" daemon prio=5 tid=0x00007f8bd5159000 nid=0x1cb0b waiting for monitor entry [0x000000011e803000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at java.lang.Shutdown.exit(Shutdown.java:212)
        - waiting to lock <0x000000070008f7c0> (a java.lang.Class for java.lang.Shutdown)
        at java.lang.Terminator$1.handle(Terminator.java:52)
        at sun.misc.Signal$1.run(Signal.java:212)
        at java.lang.Thread.run(Thread.java:744)

""SIGTERM handler"" daemon prio=5 tid=0x00007f8bdd147800 nid=0x15d0b waiting for monitor entry [0x000000011e30a000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at java.lang.Shutdown.exit(Shutdown.java:212)
        - waiting to lock <0x000000070008f7c0> (a java.lang.Class for java.lang.Shutdown)
        at java.lang.Terminator$1.handle(Terminator.java:52)
        at sun.misc.Signal$1.run(Signal.java:212)
        at java.lang.Thread.run(Thread.java:744)

""SIGTERM handler"" daemon prio=5 tid=0x00007f8bdf820000 nid=0x770b waiting for monitor entry [0x000000011e207000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at java.lang.Shutdown.exit(Shutdown.java:212)
        - waiting to lock <0x000000070008f7c0> (a java.lang.Class for java.lang.Shutdown)
        at java.lang.Terminator$1.handle(Terminator.java:52)
        at sun.misc.Signal$1.run(Signal.java:212)
        at java.lang.Thread.run(Thread.java:744)

""SIGTERM handler"" daemon prio=5 tid=0x00007f8bdc393800 nid=0x1c30f waiting for monitor entry [0x000000011e104000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at java.lang.Shutdown.exit(Shutdown.java:212)
        - waiting to lock <0x000000070008f7c0> (a java.lang.Class for java.lang.Shutdown)
        at java.lang.Terminator$1.handle(Terminator.java:52)
        at sun.misc.Signal$1.run(Signal.java:212)
        at java.lang.Thread.run(Thread.java:744)

""Thread-4"" prio=5 tid=0x00007f8bdb39f000 nid=0xa107 in Object.wait() [0x000000011ea89000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.$$YJP$$wait(Native Method)
        at java.lang.Object.wait(Object.java)
        at java.lang.Thread.join(Thread.java:1280)
        - locked <0x0000000705c2f650> (a org.apache.kafka.common.utils.KafkaThread)
        at java.lang.Thread.join(Thread.java:1354)
        at org.apache.kafka.clients.producer.KafkaProducer.close(KafkaProducer.java:322)
        at 

""kafka-producer-network-thread | error"" daemon prio=5 tid=0x00007f8bd814e000 nid=0x7403 runnable [0x000000011e6c0000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.KQueueArrayWrapper.$$YJP$$kevent0(Native Method)
        at sun.nio.ch.KQueueArrayWrapper.kevent0(KQueueArrayWrapper.java)
        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:200)
        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
        - locked <0x0000000705c109f8> (a sun.nio.ch.Util$2)
        - locked <0x0000000705c109e8> (a java.util.Collections$UnmodifiableSet)
        - locked <0x0000000705c105c8> (a sun.nio.ch.KQueueSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
        at org.apache.kafka.common.network.Selector.select(Selector.java:322)
        at org.apache.kafka.common.network.Selector.poll(Selector.java:212)
        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:192)
        at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:184)
        at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:128)
        at java.lang.Thread.run(Thread.java:744)
{code}

Thank you for the patch fix.

Thanks,
Bhavesh;;;","24/Dec/14 01:27;ewencp;1) Since this method just returns data, it probably just happens to have had to wait for the lock. Not a big deal, we would expect to see that occasionally.

2) Right now this is expected, as explained in KAFKA-1788. I saw there's patch for that one now, might help if you tested that out and see if gives the behavior you were expecting.

3) As you said, known issue so we can leave that for other JIRAs to resolve.;;;","24/Dec/14 01:40;Bmis13;[~ewencp],

Thanks for patch.  You may close this issue.   The only thing,  I have not tested the rare case where a single broker is out of File Descriptor and under heavy load on producer will request more connections to same broker.  According to code, it will mark the Node State to disconnect and I am not sure if data will be sent via already live connection.  

Another comment is that there is no WARN or ERROR message logged when connection fails. Can we please change the log level for following code to WAR, because in production environment people set LOG LEVEL to WARN or ERROR.  So there will no visibility if there is connection issue.  

{code}
   /**
     * Initiate a connection to the given node
     */
    private void initiateConnect(Node node, long now) {
        try {
            log.debug(""Initiating connection to node {} at {}:{}."", node.id(), node.host(), node.port());
            this.connectionStates.connecting(node.id(), now);
            selector.connect(node.id(), new InetSocketAddress(node.host(), node.port()), this.socketSendBuffer, this.socketReceiveBuffer);
        } catch (IOException e) {
            /* attempt failed, we'll try again after the backoff */
            connectionStates.disconnected(node.id());
            /* maybe the problem is our metadata, update it */
            metadata.requestUpdate();
            log.debug(""Error connecting to node {} at {}:{}:"", node.id(), node.host(), node.port(), e);
        }
    }
{code}

Thanks for all your help !

Thanks,

Bhavesh  ;;;","29/Dec/14 22:49;nehanarkhede;Thanks for following through on this, [~ewencp]! [~junrao], since you probably have most context on this, would you mind reviewing the follow up patch (https://reviews.apache.org/r/28582/diff/) so we can close this out?;;;","06/Jan/15 02:56;ewencp;Updated reviewboard https://reviews.apache.org/r/28582/diff/
 against branch origin/trunk;;;","06/Jan/15 18:57;junrao;Thanks for the latest followup patch. +1 and committed to both 0.8.2 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log cleaner exits if last cleaned offset is lower than earliest offset,KAFKA-1641,12742602,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,jjkoshy,jjkoshy,18/Sep/14 23:05,20/Sep/19 01:21,14/Jul/23 05:39,23/Oct/14 22:43,0.8.1.1,,,0.9.0.0,,,,,,,,,,0,,,,"Encountered this recently: the log cleaner exited a while ago (I think because the topic had compressed messages). That issue was subsequently addressed by having the producer only send uncompressed. However, on a subsequent restart of the broker we see this:

In this scenario I think it is reasonable to just emit a warning and have the cleaner round up its first dirty offset to the base offset of the first segment.

{code}
[kafka-server] [] [kafka-log-cleaner-thread-0], Error due to 
java.lang.IllegalArgumentException: requirement failed: Last clean offset is 54770438 but segment base offset is 382844024 for log testtopic-0.
        at scala.Predef$.require(Predef.scala:145)
        at kafka.log.Cleaner.buildOffsetMap(LogCleaner.scala:491)
        at kafka.log.Cleaner.clean(LogCleaner.scala:288)
        at kafka.log.LogCleaner$CleanerThread.cleanOrSleep(LogCleaner.scala:202)
        at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:187)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)
{code}",,davispw,deniszh,guozhang,jjkoshy,jonbringhurst,kostassoid,sriharsha,yogeshyadav,yuanjiali,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/14 22:21;guozhang;KAFKA-1641.patch;https://issues.apache.org/jira/secure/attachment/12673223/KAFKA-1641.patch","09/Oct/14 20:04;guozhang;KAFKA-1641_2014-10-09_13:04:15.patch;https://issues.apache.org/jira/secure/attachment/12673984/KAFKA-1641_2014-10-09_13%3A04%3A15.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 20 01:21:02 UTC 2019,,,,,,,,,,"0|i208gv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Sep/14 17:08;jonbringhurst;I'd just like to mention that a possible workaround (depending on your situation in regard to keys) is to stop the broker, remove the cleaner offset checkpoint, and then start the broker again for each ISR member in serial to get the thread running again. Keep in mind that the cleaner will start from the beginning if you do this.;;;","06/Oct/14 22:21;guozhang;Created reviewboard https://reviews.apache.org/r/26390/diff/
 against branch origin/trunk;;;","09/Oct/14 20:04;guozhang;Updated reviewboard https://reviews.apache.org/r/26390/diff/
 against branch origin/trunk;;;","23/Oct/14 17:27;guozhang;[~jjkoshy] Could you take another look?;;;","23/Oct/14 22:43;jjkoshy;Thanks for the patch.

Pushed to trunk (after fixing the log message issue I noted on the RB);;;","28/Oct/15 01:08;deniszh;We are hitting that bug again and again. Is it possible to apply it to current 0.8.x release ?
I can create PR on Github if allowed, for example.;;;","28/Oct/15 08:26;deniszh;Ah, according to Github and release plan 0.9.0 is planned for Nov 2015 and no releases planned for 0.8.x *sigh*
;;;","02/Dec/16 11:37;yuanjiali;We are hitting this problem in 0.10.0.0. Log as blow:
[2016-12-02 11:33:52,744] ERROR [kafka-log-cleaner-thread-0], Error due to  (kafka.log.LogCleaner)
java.lang.IllegalArgumentException: requirement failed: Last clean offset is 282330655505 but segment base offset is 0 for log __consumer_offsets-11.
        at scala.Predef$.require(Predef.scala:224)
        at kafka.log.Cleaner.buildOffsetMap(LogCleaner.scala:617)
        at kafka.log.Cleaner.clean(LogCleaner.scala:329)
        at kafka.log.LogCleaner$CleanerThread.cleanOrSleep(LogCleaner.scala:237)
        at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:215)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:63)
[2016-12-02 11:33:52,744] INFO [kafka-log-cleaner-thread-0], Stopped  (kafka.log.LogCleaner);;;","20/Jan/17 19:08;davispw;""Me too"" on 0.10.0.1 - does this issue need to be reopened?

	java.lang.IllegalArgumentException: requirement failed: Last clean offset is 43056300 but segment base offset is 42738384 for log -redacted- -0.
	at scala.Predef$.require(Predef.scala:224)
	at kafka.log.Cleaner.buildOffsetMap(LogCleaner.scala:604)
	at kafka.log.Cleaner.clean(LogCleaner.scala:329)
	at kafka.log.LogCleaner$CleanerThread.cleanOrSleep(LogCleaner.scala:237)
	at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:215)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:63)
;;;","20/Sep/19 01:21;yogeshyadav;Same here we are hitting the same problem with 0.10.0 

java.lang.IllegalArgumentException: requirement failed: Last clean offset is 1 but segment base offset is 0 for log __consumer_offsets-2.

 at scala.Predef$.require(Predef.scala:224)

 at kafka.log.Cleaner.buildOffsetMap(LogCleaner.scala:617)

 at kafka.log.Cleaner.clean(LogCleaner.scala:329)

 at kafka.log.LogCleaner$CleanerThread.cleanOrSleep(LogCleaner.scala:237)

 at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:215)

 at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:63)

 

Is there a workaround for this problem without upgrading to higher versions?

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleConsumer.fetchOffset returns wrong error code when no offset exists for topic/partition/consumer group,KAFKA-1637,12742012,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ewencp,amir.malekpour,amir.malekpour,17/Sep/14 00:59,14/Nov/14 19:16,14/Jul/23 05:39,16/Oct/14 17:39,0.8.1,0.8.1.1,,0.8.2.0,,,,,,,consumer,core,,0,newbie,,,"This concerns Kafka's Offset  Fetch API:

According to Kafka's current documentation, ""if there is no offset associated with a topic-partition under that consumer group the broker does not set an error code (since it is not really an error), but returns empty metadata and sets the offset field to -1.""  (Link below)

However, in Kafka 08.1.1 Error code '3' is returned, which effectively makes it impossible for the client to decide if there was an error, or if there is no offset associated with a topic-partition under that consumer group.


https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-MetadataAPI
",Linux,amir.malekpour,ewencp,jjkoshy,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/14 22:04;ewencp;KAFKA-1637.patch;https://issues.apache.org/jira/secure/attachment/12674862/KAFKA-1637.patch","15/Oct/14 16:08;ewencp;KAFKA-1637_2014-10-15_09:08:12.patch;https://issues.apache.org/jira/secure/attachment/12675023/KAFKA-1637_2014-10-15_09%3A08%3A12.patch","15/Oct/14 21:47;ewencp;KAFKA-1637_2014-10-15_14:47:21.patch;https://issues.apache.org/jira/secure/attachment/12675127/KAFKA-1637_2014-10-15_14%3A47%3A21.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 16 17:39:14 UTC 2014,,,,,,,,,,"0|i204wv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Oct/14 22:04;ewencp;Created reviewboard https://reviews.apache.org/r/26710/diff/
 against branch origin/trunk;;;","14/Oct/14 22:11;ewencp;The error code is for ""UnkownTopicOrPartition"", which may have been correct if the request was for a non-existent topic or partition. Previously the code seemed to be doing the correct thing, reporting this error and returning invalid offset when the consumer hadn't started reading from that group. But KAFKA-1012 (a670537aa337) actually changed that behavior. The provided patch tries to cover the different possible scenarios (missing topic, invalid partition, and valid TopicAndPartition but a consumer with no offset for it).

One potential caveat is auto topic creation since it could be reasonable to not return UnkownTopicOrPartition for a missing topic in that case. I'm not sure we really want different behavior in that case though.;;;","15/Oct/14 04:50;nehanarkhede;Thanks for the patch. Pushed to trunk and 0.8.2;;;","15/Oct/14 05:34;jjkoshy;-1 on this patch per the comment in the RB. Reverted it for now.;;;","15/Oct/14 13:51;nehanarkhede;Wups. Didn't mean to push the patch for this JIRA. Sorry! 
Had a suggestion myself for not using the replica manager which didn't get published by reviewboard.;;;","15/Oct/14 16:08;ewencp;Updated reviewboard https://reviews.apache.org/r/26710/diff/
 against branch origin/trunk;;;","15/Oct/14 21:24;nehanarkhede;[~ewencp] Your latest patch looks good. Once you've addressed [~jjkoshy]'s latest comment, I'll push it to trunk and 0.8.2;;;","15/Oct/14 21:47;ewencp;Updated reviewboard https://reviews.apache.org/r/26710/diff/
 against branch origin/trunk;;;","16/Oct/14 17:39;nehanarkhede;Pushed updated patch to trunk and 0.8.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java doc of makeLeaders in ReplicaManager is wrong,KAFKA-1635,12741690,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,cltlfcjin,cltlfcjin,cltlfcjin,16/Sep/14 05:46,17/Sep/14 10:44,14/Jul/23 05:39,17/Sep/14 05:09,,,,0.8.2.0,,,,,,,core,replication,,0,doc,server,,"ReplicaManager have an incorrect java doc. The overview of function  makeLeaders() is the same as makeFollowers().
Also see commit at https://github.com/apache/kafka/commit/6739a8e601331ad07d9856dc351785351755a5d5",,cltlfcjin,githubbot,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/14 05:50;cltlfcjin;kafka-1635-1.patch;https://issues.apache.org/jira/secure/attachment/12668981/kafka-1635-1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 17 10:44:31 UTC 2014,,,,,,,,,,"0|i202z3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/14 08:29;githubbot;GitHub user LantaoJin opened a pull request:

    https://github.com/apache/kafka/pull/33

    KAFKA-1635: Fixed incorrect java doc of makeLeaders() in ReplicaManager

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/LantaoJin/kafka KAFKA-1635

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/33.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #33
    
----
commit 6739a8e601331ad07d9856dc351785351755a5d5
Author: LantaoJin <lantao.jin@dianping.com>
Date:   2014-09-16T08:26:34Z

    KAFKA-1635: Fixed incorrect java doc of makeLeaders() in ReplicaManager

----
;;;","17/Sep/14 05:09;junrao;Thanks for the patch. +1 and committed to trunk.;;;","17/Sep/14 10:44;githubbot;Github user LantaoJin closed the pull request at:

    https://github.com/apache/kafka/pull/33
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve semantics of timestamp in OffsetCommitRequests and update documentation,KAFKA-1634,12741376,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,guozhang,nehanarkhede,nehanarkhede,15/Sep/14 01:39,30/Mar/15 20:41,14/Jul/23 05:39,30/Mar/15 20:41,,,,0.9.0.0,,,,,,,,,,0,,,,"From the mailing list -

following up on this -- I think the online API docs for OffsetCommitRequest
still incorrectly refer to client-side timestamps:

https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-OffsetCommitRequest

Wasn't that removed and now always handled server-side now?  Would one of
the devs mind updating the API spec wiki?",,asvyazin,dana.powers,guozhang,jjkoshy,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1927,,,,,KAFKA-1841,,,,,,,,,,,,,,,,,,,,,,"30/Oct/14 18:43;guozhang;KAFKA-1634.patch;https://issues.apache.org/jira/secure/attachment/12678266/KAFKA-1634.patch","06/Nov/14 23:35;guozhang;KAFKA-1634_2014-11-06_15:35:46.patch;https://issues.apache.org/jira/secure/attachment/12679995/KAFKA-1634_2014-11-06_15%3A35%3A46.patch","08/Nov/14 00:54;guozhang;KAFKA-1634_2014-11-07_16:54:33.patch;https://issues.apache.org/jira/secure/attachment/12680339/KAFKA-1634_2014-11-07_16%3A54%3A33.patch","18/Nov/14 01:42;guozhang;KAFKA-1634_2014-11-17_17:42:44.patch;https://issues.apache.org/jira/secure/attachment/12682057/KAFKA-1634_2014-11-17_17%3A42%3A44.patch","21/Nov/14 22:00;guozhang;KAFKA-1634_2014-11-21_14:00:34.patch;https://issues.apache.org/jira/secure/attachment/12682964/KAFKA-1634_2014-11-21_14%3A00%3A34.patch","01/Dec/14 19:44;guozhang;KAFKA-1634_2014-12-01_11:44:35.patch;https://issues.apache.org/jira/secure/attachment/12684467/KAFKA-1634_2014-12-01_11%3A44%3A35.patch","02/Dec/14 02:03;guozhang;KAFKA-1634_2014-12-01_18:03:12.patch;https://issues.apache.org/jira/secure/attachment/12684529/KAFKA-1634_2014-12-01_18%3A03%3A12.patch","14/Jan/15 23:50;guozhang;KAFKA-1634_2015-01-14_15:50:15.patch;https://issues.apache.org/jira/secure/attachment/12692382/KAFKA-1634_2015-01-14_15%3A50%3A15.patch","22/Jan/15 00:43;guozhang;KAFKA-1634_2015-01-21_16:43:01.patch;https://issues.apache.org/jira/secure/attachment/12693763/KAFKA-1634_2015-01-21_16%3A43%3A01.patch","23/Jan/15 02:47;guozhang;KAFKA-1634_2015-01-22_18:47:37.patch;https://issues.apache.org/jira/secure/attachment/12694072/KAFKA-1634_2015-01-22_18%3A47%3A37.patch","24/Jan/15 00:06;guozhang;KAFKA-1634_2015-01-23_16:06:07.patch;https://issues.apache.org/jira/secure/attachment/12694292/KAFKA-1634_2015-01-23_16%3A06%3A07.patch","06/Feb/15 19:01;guozhang;KAFKA-1634_2015-02-06_11:01:08.patch;https://issues.apache.org/jira/secure/attachment/12697086/KAFKA-1634_2015-02-06_11%3A01%3A08.patch","26/Mar/15 19:16;guozhang;KAFKA-1634_2015-03-26_12:16:09.patch;https://issues.apache.org/jira/secure/attachment/12707563/KAFKA-1634_2015-03-26_12%3A16%3A09.patch","26/Mar/15 19:27;guozhang;KAFKA-1634_2015-03-26_12:27:18.patch;https://issues.apache.org/jira/secure/attachment/12707568/KAFKA-1634_2015-03-26_12%3A27%3A18.patch",,,,,,,,,,,,,,14.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 20:41:08 UTC 2015,,,,,,,,,,"0|i2012f:",9223372036854775807,,jjkoshy,,,,,,,,,,,,,,,,,,"20/Sep/14 17:49;junrao;The wiki is actually consistent with the code in trunk. The timestamp is used to decide whether some old consumer offsets can be garbage collected. Typically, the timestamp should just be the time when the offset is committed (and can be obtained just on the broker side). However, this idea is that a client, if needed, can set a larger timestamp if it wants to preserve the offset for longer. It's not clear to me if this is super useful. We can debate that in a separate jira if needed.;;;","20/Sep/14 19:03;nehanarkhede;[~junrao], that explanation is actually pretty unintuitive. So seems like this is something that needs to be fixed. The client shouldn't need to specify a timestamp for an offset commit. Unless I'm missing something that it is designed for. [~jjkoshy], any idea?;;;","20/Sep/14 19:03;nehanarkhede;Reopening until we have filed a JIRA to fix the issue.;;;","25/Sep/14 04:32;junrao;One use case is that if a client never wants the offset to be garbage collected on the broker, it can set timestamp to be max long. ;;;","25/Sep/14 05:01;nehanarkhede;How general is that use case and if it just a means for picking infinite retention, then why not just have a boolean option rather than a full timestamp? My main concern is that this is looking more like an after thought instead of a well thought out API. We should think how the user would interpret the timestamp and what makes sense as a general solution. ;;;","29/Sep/14 00:47;junrao;Another use case is the following. It may not make sense to retain an offset longer than the data retention time. So, if data is kept for only 7 days, a consumer client may want to set timestamp to be now + 7 days to preserve the offset as long as it's still valid.

Joel probably can comment on the general use cases more.
;;;","29/Sep/14 18:44;nehanarkhede;Basically, I'm not sure this is great user experience. When I looked at the API, I found it awkward to use and understand why I needed to specify a timestamp. If there are use cases that are general, I'd like to hear and maybe we should update documentation. But if not, we should remove it. [~jjkoshy] Any comments?;;;","29/Sep/14 22:56;jjkoshy;[~nehanarkhede] I agree that timestamp in the request API is a bit odd since that is an implementation detail on the server-side (on how long to retain offsets) and setting ""future"" timestamps on the client-side is a hack to ""trick"" the server into retaining the offset.

The use-cases though are as Jun described - i.e., if a client wants to ensure that offsets are retained for a certain period of time (regardless of when the server cleans up stale offsets). That said, the field could be renamed and reimplemented as a retentionPeriod field (not timestamp). i.e., if set the consumer would explicitly say ""retain my offsets for X milliseconds/days/whatever"". The broker can have a second hard staleness threshold to cap this client-driven staleness threshold.;;;","01/Oct/14 14:30;nehanarkhede;[~jjkoshy] Thanks for the explanation. If retention is the use case, it is easier to understand if it was called retentionPeriod. Since 0.8.2 is when this releases, it will be great to make that renaming change before 0.8.2 goes out. If there is a broker side feature that is required for this to work properly and it's not done, then we can either get rid of this and add it when it's useful or leave it in there as a no-op or with a warning. What do you prefer?;;;","10/Oct/14 22:35;jjkoshy;Actually, one more potential source of confusion is that we use OffsetAndMetadata for both offset commits requests and offset fetch responses.

i.e., an OffsetFetchResponse will contain: offset, metadata and this timestamp field. The timestamp field should really be ignored. It is annoying to document such things - i.e., tell users to just ignore the field.

Ideally, I think we should do the following:
* Remove the timestamp from the OffsetAndMetadata class
* Move it to the top-level of the OffsetCommitRequest and rename it to retentionMs
* The broker will compute the absolute time (based off time of receipt) that the offset should be expired
* The above absolute time will continue to be stored in the offsets topic and the cleanup thread can remove those offsets when they are past their TTL.
* OffsetFetchResponse will just return OffsetAndMetadata (no timestamp)

We (linkedin and possibly others) already deployed this to some of our consumers but if we can bump up the protocol version when doing the above and translate requests that come in with the older version I think it should be okay.
;;;","10/Oct/14 22:40;jjkoshy;(BTW, I'm looking for a  +1 or -1 on the above comment :) );;;","12/Oct/14 16:05;junrao;Joel,

I don't see OffsetFetchResponse contain timestamp (it only contains offset, metadata and error code). I agree that it's better to move retentionMs to the topic-level of OffsetCommitRequest. We can bump up the protocol version if needed.;;;","12/Oct/14 21:42;nehanarkhede;+1 to your suggestions above. Thanks [~jjkoshy];;;","13/Oct/14 18:00;jjkoshy;[~junrao] yes you are right. The OffsetFetchResponse returns offset, metadata and error - it does not include the timestamp.
;;;","13/Oct/14 23:57;guozhang;I will work on this after KAFKA-1583.;;;","21/Oct/14 04:07;junrao;Moving this to 0.8.3 since it's easier to fix after KAFKA-1583 is done.;;;","29/Oct/14 23:08;guozhang;While working on this: should a retention time better be scaled at secs than ms? Do we have scenarios where people want to just retain their offsets for miliseconds? If not then we may end up always having very large number for this field.;;;","30/Oct/14 18:43;guozhang;Created reviewboard https://reviews.apache.org/r/27391/diff/
 against branch origin/trunk;;;","06/Nov/14 23:35;guozhang;Updated reviewboard https://reviews.apache.org/r/27391/diff/
 against branch origin/trunk;;;","08/Nov/14 00:54;guozhang;Updated reviewboard https://reviews.apache.org/r/27391/diff/
 against branch origin/trunk;;;","18/Nov/14 01:42;guozhang;Updated reviewboard https://reviews.apache.org/r/27391/diff/
 against branch origin/trunk;;;","21/Nov/14 22:00;guozhang;Updated reviewboard https://reviews.apache.org/r/27391/diff/
 against branch origin/trunk;;;","01/Dec/14 19:44;guozhang;Updated reviewboard https://reviews.apache.org/r/27391/diff/
 against branch origin/trunk;;;","02/Dec/14 02:03;guozhang;Updated reviewboard https://reviews.apache.org/r/27391/diff/
 against branch origin/trunk;;;","05/Jan/15 20:19;dana.powers;possibly related to this JIRA: KAFKA-1841 .  The timestamp field itself was not in the released api version 0 and if it is to be included in 0.8.2 (this JIRA suggests it is, but to be removed in 0.8.3 ?) then I think it will need to be versioned.;;;","13/Jan/15 06:35;junrao;Committed KAFKA-1841 to 0.8.2. It would be easier to incorporate the changes in KAFKA-1841 into this jira and commit them together to trunk.;;;","14/Jan/15 23:50;guozhang;Updated reviewboard https://reviews.apache.org/r/27391/diff/
 against branch origin/trunk;;;","14/Jan/15 23:51;guozhang;[~jjkoshy], [~junrao] My plan is to check in this patch first and than rebase 1481 on that, so could you take a look at the latest patch incorporating your comments?;;;","22/Jan/15 00:43;guozhang;Updated reviewboard https://reviews.apache.org/r/27391/diff/
 against branch origin/trunk;;;","22/Jan/15 01:57;junrao;I guess you mean rebasing KAFAK-1841, instead of KAFKA-1481?;;;","23/Jan/15 02:47;guozhang;Updated reviewboard https://reviews.apache.org/r/27391/diff/
 against branch origin/trunk;;;","24/Jan/15 00:06;guozhang;Updated reviewboard https://reviews.apache.org/r/27391/diff/
 against branch origin/trunk;;;","06/Feb/15 19:01;guozhang;Updated reviewboard https://reviews.apache.org/r/27391/diff/
 against branch origin/trunk;;;","18/Mar/15 16:16;junrao;[~guozhang], [~jjkoshy], in order to work on KAFKA-1927, we will need to have KAFKA-1841 merged into trunk, which depends on this jira.;;;","26/Mar/15 19:16;guozhang;Updated reviewboard https://reviews.apache.org/r/27391/diff/
 against branch origin/trunk;;;","26/Mar/15 19:27;guozhang;Updated reviewboard https://reviews.apache.org/r/27391/diff/
 against branch origin/trunk;;;","27/Mar/15 00:17;guozhang;Thanks for the reviews, committed to trunk.;;;","27/Mar/15 00:20;jjkoshy;[~guozhang] we actually need to merge KAFKA-1841 now. Or do you want to do that in a separate jira?;;;","27/Mar/15 01:00;junrao;Also, could you also update the protocol wiki (https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-OffsetCommitRequest)? For the new version, please document the release from which the new version will be supported.;;;","27/Mar/15 03:17;guozhang;Just updated the protocol wiki with the new version. As for merging KAFKA-1841, I can do that after 04/06; if that is too late some one can pick it up.;;;","27/Mar/15 17:49;jjkoshy;Reopening since we need to merge KAFKA-1841;;;","29/Mar/15 16:49;junrao;Perhaps, we can do KAFKA-1841 together with KAFKA-2068. There will probably be less duplicated code that way.;;;","30/Mar/15 20:41;jjkoshy;Yes I think that makes more sense;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sample Java code contains Scala syntax,KAFKA-1625,12739614,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,davidzchen,davidzchen,davidzchen,05/Sep/14 22:33,12/Sep/14 03:59,14/Jul/23 05:39,12/Sep/14 03:59,,,,,,,,,,,website,,,0,,,,"As I was reading the Kafka documentation, I noticed that some of the parameters use Scala syntax, even though the code appears to be Java. For example:

{code}
public static kafka.javaapi.consumer.ConsumerConnector createJavaConsumerConnector(config: ConsumerConfig);
{code}

Also, what is the reason for fully qualifying these classes? I understand that there are Scala and Java classes with the same name, but I think that fully qualifying them in the sample code would encourage that practice by users, which is not desirable in Java code.",,davidzchen,jjkoshy,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1626,,,,"05/Sep/14 22:38;davidzchen;KAFKA-1625.site.0.patch;https://issues.apache.org/jira/secure/attachment/12666922/KAFKA-1625.site.0.patch","05/Sep/14 23:22;davidzchen;KAFKA-1625.site.1.patch;https://issues.apache.org/jira/secure/attachment/12666936/KAFKA-1625.site.1.patch","09/Sep/14 21:17;davidzchen;KAFKA-1625.site.2.patch;https://issues.apache.org/jira/secure/attachment/12667488/KAFKA-1625.site.2.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 12 03:59:44 UTC 2014,,,,,,,,,,"0|i1zqxb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Sep/14 22:38;davidzchen;I am attaching a patch for this.

How do I create an RB for a change to the website? I tried to manually create the RB and entered {{/site}} for the base directory, but RB gave me the following error:

{code}
Line undefined: Repository moved permanently to 'https://svn.apache.org/repos/asf/kafka/site/081/api.html'; please relocate
{code};;;","05/Sep/14 23:22;davidzchen;Attaching a new patch that removes the full qualification of class names in the sample code.;;;","09/Sep/14 20:49;jjkoshy;Thanks for pointing out the reference to scala code from Java.

We should probably keep full qualification due to the dual-naming (between Scala and Java) that you noted - otherwise most people who try the examples out of the box would run into the question of which one do I use (say, when prompted to select imports in an IDE).;;;","09/Sep/14 21:03;davidzchen;Sounds good. Do we have plans to consolidate the classes in the way that [samza-api|https://samza.incubator.apache.org/learn/documentation/0.7.0/api/javadocs/] does?;;;","09/Sep/14 21:07;jjkoshy;Not that I know of, but the new producer is implemented in Java anyway and the new consumer (also Java) is being developed.;;;","09/Sep/14 21:17;davidzchen;Attaching a new patch that re-fully-qualifies class names.;;;","12/Sep/14 03:59;nehanarkhede;Thanks for the patch, [~davidzchen];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bump up default scala version to 2.10.4 to compile with java 8,KAFKA-1624,12739156,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,joestein,joestein,04/Sep/14 15:56,30/Dec/14 00:03,14/Jul/23 05:39,24/Nov/14 19:59,,,,0.8.2.0,,,,,,,,,,1,newbie,,,"{code}

Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0
error: error while loading CharSequence, class file '/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar(java/lang/CharSequence.class)' is broken
(class java.lang.RuntimeException/bad constant pool tag 18 at byte 10)
error: error while loading Comparator, class file '/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar(java/util/Comparator.class)' is broken
(class java.lang.RuntimeException/bad constant pool tag 18 at byte 20)
error: error while loading AnnotatedElement, class file '/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar(java/lang/reflect/AnnotatedElement.class)' is broken
(class java.lang.RuntimeException/bad constant pool tag 18 at byte 76)
error: error while loading Arrays, class file '/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar(java/util/Arrays.class)' is broken
(class java.lang.RuntimeException/bad constant pool tag 18 at byte 765)
/tmp/sbt_53783b12/xsbt/ExtractAPI.scala:395: error: java.util.Comparator does not take type parameters
	private[this] val sortClasses = new Comparator[Symbol] {
                                            ^
5 errors found
:core:compileScala FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':core:compileScala'.
> org.gradle.messaging.remote.internal.PlaceholderException (no error message)

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED

Total time: 1 mins 48.298 secs
{code}",,ewencp,guozhang,gwenshap,hzshlomi,ijuma,joestein,jonbringhurst,sslavic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/14 00:23;guozhang;KAFKA-1624.patch;https://issues.apache.org/jira/secure/attachment/12682537/KAFKA-1624.patch","24/Nov/14 19:02;guozhang;KAFKA-1624_2014-11-24_11:01:56.patch;https://issues.apache.org/jira/secure/attachment/12683386/KAFKA-1624_2014-11-24_11%3A01%3A56.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 30 00:03:48 UTC 2014,,,,,,,,,,"0|i1zos7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Sep/14 21:55;sslavic;If I'm not mistaken, this error is caused by Scala 2.10 (and older) incompatibility with (reading) Java 8.
{{./gradlew clean test_core_2_11}} with Java 8 passes successfully for me on current trunk, although even Scala 2.11 just has experimental support for Java 8.

Scala 2.12 will require Java 8 (see [Scala 2.12 roadmap|http://www.scala-lang.org/news/2.12-roadmap]).;;;","17/Sep/14 16:15;joestein;Thanks, so nothing to fix/to-do yet but lets leave this open as I expect it will come up again so folks know what is going on.  We can eventually turn it into managing multiple builds for JDK version and Scala version down the road. ;;;","04/Oct/14 21:13;ijuma;Note that Scala 2.10.4 also includes a number of fixes for Java 8 support.;;;","11/Nov/14 19:18;gwenshap;Closing with ""not a problem"", since this is just a matter of building with:
-PscalaVersion=2.10.3 or -PscalaVersion=2.11;;;","14/Nov/14 22:21;guozhang;I did some tests locally with various Scala versions. Only the default 2.10.1 seems not compile with Java 8; 2.10.2, 2.10.3 and 2.11 are all compatible with it. Shall we change the default version of Scala to at least 2.10.2?;;;","17/Nov/14 18:29;joestein;<< I did some tests locally with various Scala versions. Only the default 2.10.1 seems not compile with Java 8; 2.10.2, 2.10.3 and 2.11 are all compatible with it. Shall we change the default version of Scala to at least 2.10.2?

[~guozhang] Thanks for testing the versions out. Your suggestions makes sense to me, folks are going to keep bringing this up more and more moving forward and no reason to make them keep making a minor change we can ship in 0.8.2 final (i think it would be ok to-do it there)

Do we want to go with 2.10.3 instead of 2.10.2 since it is later version? 

Any one else issues with doing this for 0.8.2?;;;","17/Nov/14 22:14;guozhang;Thanks Joe. So I will bump up the default version to 2.10.3 if no one else have issues with it.;;;","17/Nov/14 23:36;ewencp;Isn't 2.10.4 the latest version in the 2.10 series?;;;","20/Nov/14 00:23;guozhang;Created reviewboard https://reviews.apache.org/r/28268/diff/
 against branch origin/trunk;;;","24/Nov/14 19:02;guozhang;Updated reviewboard https://reviews.apache.org/r/28268/diff/
 against branch origin/trunk;;;","30/Dec/14 00:03;joestein;committed to 0.8.2 branch, just noticed the commit message says 2.11.4 but it is 2.10.4 per the JIRA title (just changed the title);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
project shouldn't require signing to build,KAFKA-1622,12738314,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,edgefox,joestein,joestein,01/Sep/14 18:54,20/Sep/14 22:48,14/Jul/23 05:39,20/Sep/14 22:48,,,,0.8.2.0,,,,,,,,,,0,build,newbie,packaging,"we only need signing for uploadArchives that is it

The project trunk failed to build due to some signing/license checks (the diff I used to get things to build is here: https://gist.github.com/dehora/7e3c0bd75bb2b5d87557)",,edgefox,joestein,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/14 15:40;edgefox;KAFKA-1622.patch;https://issues.apache.org/jira/secure/attachment/12669426/KAFKA-1622.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 20 22:48:31 UTC 2014,,,,,,,,,,"0|i1zk67:",9223372036854775807,,joestein,,,,,,,,,,,,,,,,,,"17/Sep/14 15:40;edgefox;Created reviewboard https://reviews.apache.org/r/25738/diff/
 against branch apache/trunk;;;","20/Sep/14 22:48;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standardize --messages option in perf scripts,KAFKA-1621,12738309,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,rekhajoshm,jkreps,jkreps,01/Sep/14 17:19,26/Dec/16 22:38,14/Jul/23 05:39,28/Apr/15 16:33,0.8.1.1,,,,,,,,,,,,,0,newbie,,,"This option is specified in PerfConfig and is used by the producer, consumer and simple consumer perf commands. The docstring on the argument does not list it as required but the producer performance test requires it--others don't.

We should standardize this so that either all the commands require the option and it is marked as required in the docstring or none of them list it as required.",,githubbot,jkreps,nehanarkhede,omkreddy,rekhajoshm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-2157,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Dec 26 22:38:16 UTC 2016,,,,,,,,,,"0|i1zk53:",9223372036854775807,,jkreps,,,,,,,,,,,,,,,,,,"07/Oct/14 04:12;omkreddy;[~jkreps]  I would like to work on this issue. Are you referring kafka-consumer-perf-test.sh, kafka-producer-perf-test.sh,???,   scripts? ;;;","22/Feb/15 23:18;githubbot;GitHub user rekhajoshm opened a pull request:

    https://github.com/apache/kafka/pull/46

    KAFKA-1621 : Standardize --messages option

    KAFKA-1621: Standardize --messages option in perf scripts

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rekhajoshm/kafka KAFKA-1621

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/46.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #46
    
----
commit d123f48f85604c765b464d6b9d5cee4b3ec0de25
Author: Joshi <rekhajoshm@gmail.com>
Date:   2015-02-22T23:21:55Z

    KAFKA-1621 : Standardize --messages option

----
;;;","22/Feb/15 23:21;rekhajoshm;https://github.com/apache/kafka/pull/46;;;","06/Apr/15 20:22;rekhajoshm;Hi.missed updating earlier. everything else was setup as https://cwiki.apache.org/confluence/display/KAFKA/Patch+submission+and+review but the jira-client could not be installed (Mac 10.9.5, Python 2.7.5, pip 1.5.6), hence could not update review board.
suggestion: why not review directly on git as rest of the apache projects?

python kafka-patch-review.py --help
Traceback (most recent call last):
  File ""kafka-patch-review.py"", line 10, in <module>
    from jira.client import JIRA
ImportError: No module named jira.client

pip install jira-python
Downloading/unpacking jira-python
  Could not find any downloads that satisfy the requirement jira-python
Cleaning up...
No distributions at all found for jira-python
Storing debug log for failure in /usr/local/.pip/pip.log

sudo easy_install jira-python
Searching for jira-python
Reading http://pypi.python.org/simple/jira-python/
Couldn't find index page for 'jira-python' (maybe misspelled?)
Scanning index of all packages (this may take a while)
Reading http://pypi.python.org/simple/
No local packages or download links found for jira-python
error: Could not find suitable distribution for Requirement.parse('jira-python')

Think it could be related to security enabled on pip?, but I would prefer not to downgrade pip as used for my other projects as well.;;;","26/Apr/15 23:13;nehanarkhede;[~rekhajoshm] Sorry for the delay, reviewed your PR. However, could you send it for trunk instead of 0.8.2?;;;","27/Apr/15 21:01;githubbot;GitHub user rekhajoshm opened a pull request:

    https://github.com/apache/kafka/pull/58

    KAFKA-1621 : Standardize --messages option

    As per review comments from @nehanarkhede Thanks.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/rekhajoshm/kafka localtrunk

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/58.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #58
    
----
commit 51cab8584872457a349d6ba154a1087b9749e0f8
Author: Joshi <rekhajoshm@gmail.com>
Date:   2015-04-27T21:02:15Z

    KAFKA-1621 : Standardize --messages option

----
;;;","27/Apr/15 21:09;rekhajoshm;No worries [~nehanarkhede] Thanks for your review, PerfConfig was found updated in trunk for REQUIRED, rest applied on pull #58.Thanks.;;;","28/Apr/15 16:33;nehanarkhede;Merged PR #58;;;","21/Jul/15 05:51;githubbot;Github user rekhajoshm closed the pull request at:

    https://github.com/apache/kafka/pull/58
;;;","26/Dec/16 22:38;githubbot;Github user pono closed the pull request at:

    https://github.com/apache/kafka/pull/46
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
perf dir can be removed,KAFKA-1619,12738182,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,01/Sep/14 04:11,14/Sep/14 17:19,14/Jul/23 05:39,14/Sep/14 17:19,0.8.2.0,,,0.8.2.0,,,,,,,,,,1,newbie,,,There is no code in perf/ any more. We can also remove the perf target in build.gradle.,,junrao,sslavic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/14 17:55;junrao;KAFKA-1619_2014-09-01_10:55:38.patch;https://issues.apache.org/jira/secure/attachment/12665793/KAFKA-1619_2014-09-01_10%3A55%3A38.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Sep 14 17:19:56 UTC 2014,,,,,,,,,,"0|i1zjd3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Sep/14 17:55;junrao;Updated reviewboard https://reviews.apache.org/r/25236/diff/
 against branch origin/trunk;;;","14/Sep/14 17:19;junrao;Thanks for the reviews. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Purgatory Size and Num.Delayed.Request metrics are incorrect,KAFKA-1616,12737496,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,28/Aug/14 16:43,05/Sep/14 05:25,14/Jul/23 05:39,05/Sep/14 05:25,,,,0.8.2.0,,,,,,,,,,0,,,,"The request purgatory used two atomic integers ""watched"" and ""unsatisfied"" to record the purgatory size ( = watched + unsatisfied) and number of delayed requests ( = unsatisfied). But due to some race conditions these two atomic integers are not updated correctly, result in incorrect metrics.

Proposed solution: to have a cleaner semantics, we can define the ""purgatory size"" to be just the number of elements in the watched lists, and the ""number of delayed requests"" to be just the length of the expiry queue. And instead of using two atomic integeres we just compute the size of the lists / queue on the fly each time the metrics are pulled. This may use some more CPU cycles for these two metrics but should be minor, and the correctness is guaranteed.",,guozhang,junrao,sam.nguyen@sendgrid.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/14 17:11;guozhang;KAFKA-1616.patch;https://issues.apache.org/jira/secure/attachment/12665013/KAFKA-1616.patch","28/Aug/14 17:12;guozhang;KAFKA-1616_2014-08-28_10:12:17.patch;https://issues.apache.org/jira/secure/attachment/12665014/KAFKA-1616_2014-08-28_10%3A12%3A17.patch","01/Sep/14 21:41;guozhang;KAFKA-1616_2014-09-01_14:41:56.patch;https://issues.apache.org/jira/secure/attachment/12665819/KAFKA-1616_2014-09-01_14%3A41%3A56.patch","02/Sep/14 19:57;guozhang;KAFKA-1616_2014-09-02_12:58:07.patch;https://issues.apache.org/jira/secure/attachment/12665970/KAFKA-1616_2014-09-02_12%3A58%3A07.patch","02/Sep/14 20:22;guozhang;KAFKA-1616_2014-09-02_13:23:13.patch;https://issues.apache.org/jira/secure/attachment/12665979/KAFKA-1616_2014-09-02_13%3A23%3A13.patch","03/Sep/14 19:52;guozhang;KAFKA-1616_2014-09-03_12:53:09.patch;https://issues.apache.org/jira/secure/attachment/12666301/KAFKA-1616_2014-09-03_12%3A53%3A09.patch","04/Sep/14 20:26;guozhang;KAFKA-1616_2014-09-04_13:26:02.patch;https://issues.apache.org/jira/secure/attachment/12666569/KAFKA-1616_2014-09-04_13%3A26%3A02.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 05 05:25:46 UTC 2014,,,,,,,,,,"0|i1zg4f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Aug/14 17:11;guozhang;Created reviewboard https://reviews.apache.org/r/25155/
 against branch origin/trunk;;;","28/Aug/14 17:12;guozhang;Updated reviewboard https://reviews.apache.org/r/25155/
 against branch origin/trunk;;;","01/Sep/14 21:41;guozhang;Updated reviewboard https://reviews.apache.org/r/25155/diff/
 against branch origin/trunk;;;","02/Sep/14 19:57;guozhang;Updated reviewboard https://reviews.apache.org/r/25155/diff/
 against branch origin/trunk;;;","02/Sep/14 20:22;guozhang;Updated reviewboard https://reviews.apache.org/r/25155/diff/
 against branch origin/trunk;;;","03/Sep/14 19:52;guozhang;Updated reviewboard https://reviews.apache.org/r/25155/diff/
 against branch origin/trunk;;;","04/Sep/14 20:26;guozhang;Updated reviewboard https://reviews.apache.org/r/25155/diff/
 against branch origin/trunk;;;","05/Sep/14 05:25;junrao;Thanks for the patch. +1. Committed to trunk with the following change.

RequestPurgatory.this.delayed() => delayed();;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New producer metadata response handling should only exclude a PartitionInfo when its error is LEADER_NOT_AVAILABLE,KAFKA-1609,12736149,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,lindong,lindong,lindong,22/Aug/14 19:56,23/Aug/14 00:06,14/Jul/23 05:39,23/Aug/14 00:06,,,,0.8.2.0,,,,,,,,,,0,,,,"The new producer is not able to produce data when it sees ReplicaNotAvailable error, even when the leader of the replica is still available. 

This behavior is different from old producer, which says ""any other error code (e.g. ReplicaNotAvailable) can be ignored since the producer does not need to access the replica and isr metadata"".

To reproduce the error:
1) Start 4 brokers
2) Create a topic of 1 partition using a replication factor of 3.
3) Start producerPerformance with new producer
4) Kill the leader of the topic/partition
5) Kill the new leader of the topic/partition
6) Observe that the producerPerformance stops producing data.




",,junrao,lindong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/14 19:58;lindong;KAFKA-1609.patch;https://issues.apache.org/jira/secure/attachment/12663727/KAFKA-1609.patch","22/Aug/14 20:04;lindong;KAFKA-1609_2014-08-22_13:04:10.patch;https://issues.apache.org/jira/secure/attachment/12663728/KAFKA-1609_2014-08-22_13%3A04%3A10.patch","22/Aug/14 23:22;lindong;KAFKA-1609_2014-08-22_16:22:00.patch;https://issues.apache.org/jira/secure/attachment/12663791/KAFKA-1609_2014-08-22_16%3A22%3A00.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 23 00:06:20 UTC 2014,,,,,,,,,,"0|i1z8tz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Aug/14 19:58;lindong;Created reviewboard https://reviews.apache.org/r/24992/diff/
 against branch origin/trunk;;;","22/Aug/14 20:04;lindong;Updated reviewboard https://reviews.apache.org/r/24992/diff/
 against branch origin/trunk;;;","22/Aug/14 23:22;lindong;Updated reviewboard https://reviews.apache.org/r/24992/diff/
 against branch origin/trunk;;;","23/Aug/14 00:06;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows: Error: Could not find or load main class org.apache.zookeeper.server.quorum.QuorumPeerMain,KAFKA-1608,12735821,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,rakesh.komulwad,rakesh.komulwad,21/Aug/14 16:19,08/Sep/17 16:14,14/Jul/23 05:39,08/Sep/17 16:14,0.8.1.1,,,,,,,,,,core,,,0,windows,,,"When trying to start zookeeper getting the following error in Windows

Error: Could not find or load main class org.apache.zookeeper.server.quorum.QuorumPeerMain

Fix for this is to edit windows\kafka-run-class.bat

Change
set BASE_DIR=%CD%\..
to
set BASE_DIR=%CD%\..\..

Change
for %%i in (%BASE_DIR%\core\lib\*.jar)
to
for %%i in (%BASE_DIR%\libs\*.jar)
",Windows,omkreddy,rakesh.komulwad,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 08 16:14:17 UTC 2017,,,,,,,,,,"0|i1z6vb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Sep/17 16:14;omkreddy; This was fixed in newer versions. Pl reopen if you think the issue still exists
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reference to old zk.connect vs broker.list found in docs under Produce APIs,KAFKA-1606,12735503,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,junrao,potto,potto,20/Aug/14 19:13,15/Sep/14 05:01,14/Jul/23 05:39,15/Sep/14 05:01,0.8.1,,,,,,,,,,website,,,0,documentation,producer,,"Current documentation for Kafka 0.8.1 has the following under 5.1 API Design / Producer APIs:

* provides ZooKeeper based automatic broker discovery -
The ZooKeeper based broker discovery and load balancing can be used by specifying the ZooKeeper connection url through the zk.connect config parameter. For some applications, however, the dependence on ZooKeeper is inappropriate. In that case, the producer can take in a static list of brokers through the broker.list config parameter. Each produce requests gets routed to a random broker partition in this case. If that broker is down, the produce request fails.

This does not seem correct anymore as 1) zk.connect is no longer a valid parameter; and 2) the way the broker list is handled has changed from the above description. ",any,junrao,potto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 15 05:01:50 UTC 2014,,,,,,,,,,"0|i1z5an:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Sep/14 05:01;junrao;Thanks for pointing this out. Fixed the website.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MirrorMaker consumer does not put the message key into ProducerRecord,KAFKA-1603,12735186,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,becket_qin,becket_qin,becket_qin,19/Aug/14 21:40,20/Aug/14 00:53,14/Jul/23 05:39,20/Aug/14 00:53,,,,0.8.2.0,,,,,,,,,,0,,,,,,becket_qin,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/14 21:50;becket_qin;KAFKA-1603.patch;https://issues.apache.org/jira/secure/attachment/12662863/KAFKA-1603.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 20 00:53:48 UTC 2014,,,,,,,,,,"0|i1z2iv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Aug/14 21:50;becket_qin;Created reviewboard https://reviews.apache.org/r/24869/diff/
 against branch origin/trunk;;;","20/Aug/14 00:53;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller failover not working correctly.,KAFKA-1600,12735001,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,dinghaifeng,dinghaifeng,19/Aug/14 10:29,05/Oct/14 01:13,14/Jul/23 05:39,05/Oct/14 01:13,0.8.1,,,0.8.2.0,,,,,,,controller,,,0,,,,"We are running a 10 node Kafka 0.8.1 cluster and experienced a failure as following. 
At some time, broker A stopped acting as controller any more. We see this by kafka.controller - KafkaController - ActiveControllerCount in JMX metrics jumped from 1 to 0.
In the meanwhile, broker A was still running and registering itself in the zookeeper /kafka/controller node. So no other brokers could be elected as new controller.
Since that the cluster was running without controller. Producers and consumers still worked. But functions requiring a controller such as new topic leader election and topic leader failover were not working any more.
A force restart of broker A could lead to a controller election and bring the cluster back to a correct state.
Here is our brief observations. I can provide more necessary informations if needed.

","Linux 3.2.0-4-amd64 #1 SMP Debian 3.2.46-1 x86_64 GNU/Linux
java version ""1.7.0_03""",bcalmac,dinghaifeng,guozhang,nehanarkhede,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1663,,,,,,,,,,,,,,,,,,,"20/Aug/14 07:45;dinghaifeng;kafka_failure_logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12663071/kafka_failure_logs.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,412941,,,Sun Oct 05 01:13:43 UTC 2014,,,,,,,,,,"0|i1z1av:",412927,,nehanarkhede,,,,,,,,,,,,,,,,,,"19/Aug/14 16:29;guozhang;Hello Haifeng,

Could you upload the broker and controller log of broker A here?;;;","19/Aug/14 18:08;nehanarkhede;[~dinghaifeng], thanks for reporting the issue. It will help to have controller.log, server.log from *all* brokers. You can attach these logs to this JIRA.;;;","20/Aug/14 07:45;dinghaifeng;Guozhang and Neha, Thanks for reply.

In the attachment are controller.log and server.log from 2 of total 10 brokers. broker.id=6 is the misbehaving controller broker.

controller.log from other brokers are empty at that time. It also proves that controller failover didn't happen. server.log from other brokers are much the same with these broker and not attached.

Some critical moments I found which could help understanding the logs:
14:30:50 - a new topic ""user_action_log_from_history"" created.
16:04:51 - topic ""user_action_log_from_history"" deleted.
16:04:56 - the last line in controller.log from broker 6. The ActiveControllerCount metric also decreased to 0 since then.
16:28:48 - another broker (broker.id=1) restarted manually but failed to start. Some topic partitions on broker 1 lost their leader and were not readable and writeable since then.

What happens later:
We didn’t fully get what was wrong at that time. To bring the production system back to work ASAP, we created another Kafka cluster and switched to the new cluster. In the post-mortem analysis, we found the clues above and open this issue here. Hope it can helps. Also contact me if you need any other information.
;;;","22/Aug/14 18:38;guozhang;Haifeng,

I looked into the controller and broker logs, but cannot figure out exactly what was the issue from the logs themselves. However, there are known issues with delete topic tools that may be causing the controller to fall in a bad state (KAFKA-1558), and your issue may be related to it.;;;","15/Sep/14 03:50;nehanarkhede;Marking this as a blocker for 0.8.2 since it is related to delete topic;;;","03/Oct/14 05:21;sriharsha;[~nehanarkhede] [~guozhang] I looked at the logs this is same issue as KAFKA-1663;;;","05/Oct/14 01:13;nehanarkhede;Duplicate JIRA KAFKA-1600 is now resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix variable typo in Kafka main class,KAFKA-1598,12734290,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,jarcec,jarcec,jarcec,14/Aug/14 22:57,18/Aug/14 22:59,14/Jul/23 05:39,18/Aug/14 22:50,,,,0.8.2.0,,,,,,,,,,0,,,,"I guess that this one is super-nit, but I've noticed that the name {{kafkaServerStartble}} is miss-spelled in the main {{Kafka.scala}} file.",,jarcec,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/14 22:58;jarcec;KAFKA-1598.patch;https://issues.apache.org/jira/secure/attachment/12661924/KAFKA-1598.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,412230,,,Mon Aug 18 22:59:50 UTC 2014,,,,,,,,,,"0|i1yx0n:",412219,,,,,,,,,,,,,,,,,,,,"14/Aug/14 22:58;jarcec;Attaching trivial patch that fixes the typo.;;;","18/Aug/14 22:50;junrao;Thanks for the patch. +1 and committed to trunk.;;;","18/Aug/14 22:59;jarcec;Thank you for the review [~junrao]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Client: Infinite ""conflict in /consumers/""",KAFKA-1585,12733091,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,adenysenko,adenysenko,10/Aug/14 06:57,04/Sep/14 21:59,14/Jul/23 05:39,04/Sep/14 21:59,0.8.1.1,,,0.8.2.0,,,,,,,consumer,,,0,,,,"Periodically we have kafka consumers cycling in ""conflict in /consumers/"" and ""I wrote this conflicted ephemeral node"". 
Please see attached log extract.
After restarting the process kafka consumers are working perfectly. 

We are using Zookeeper 3.4.5


",,adenysenko,guozhang,joestein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1451,,,,,,,,,,,,,"10/Aug/14 06:58;adenysenko;kafka_consumer_ephemeral_node_extract.zip;https://issues.apache.org/jira/secure/attachment/12660842/kafka_consumer_ephemeral_node_extract.zip",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,411119,,,Thu Sep 04 21:59:00 UTC 2014,,,,,,,,,,"0|i1yqav:",411111,,,,,,,,,,,,,,,,,,,,"10/Aug/14 08:04;joestein;FWIW there were a lot of bug fixes released in the Zookeeper 3.4.6 http://zookeeper.apache.org/doc/r3.4.6/releasenotes.html from 3.4.5 version.

You could be hitting ZOOKEEPER-1382 which was fixed in the 3.4.6 release

Current Kafka 0.8.1.1 zookeeper recommend https://kafka.apache.org/documentation.html#zk though folks are using 3.4.6 in production and that should be Zookeeper version for 0.8.2

In regards to your logs, before this happened it looks like you had errors and then a reconnect and consumer shutdown

Line 132356: 18:31:38,948 [7-cloudera:2181] INFO  kafka.utils.Logging$class - [Q_dev-1407608193903-1cb30b18], Q_dev-1407608193903-1cb30b18-0 attempting to claim partition 0
Line 132357: 18:31:38,975 [26-d7f0e66a-0-0] ERROR kafka.utils.Logging$class - [ConsumerFetcherThread-Q_dev-1407608195226-d7f0e66a-0-0], Current offset 15 for partition [gk.q.event,0] out of range; reset offset to 0
Line 132358: 18:31:38,980 [62-1d81f64b-0-0] ERROR kafka.utils.Logging$class - [ConsumerFetcherThread-Q_dev-1407608193962-1d81f64b-0-0], Current offset 4 for partition [gk.q.mail.api,0] out of range; reset offset to 0
Line 132359: 18:31:38,994 [84-ceea5788-0-0] WARN  kafka.utils.Logging$class - Reconnect due to socket error: null
Line 132360: 18:31:38,995 [84-ceea5788-0-0] INFO  kafka.utils.Logging$class - [ConsumerFetcherThread-dev_dev-1407608194884-ceea5788-0-0], Stopped 
Line 132361: 18:31:38,995 [atcher_executor] INFO  kafka.utils.Logging$class - [ConsumerFetcherThread-dev_dev-1407608194884-ceea5788-0-0], Shutdown completed
Line 132362: 18:31:38,995 [atcher_executor] INFO  kafka.utils.Logging$class - [ConsumerFetcherManager-1407608194890] All connections stopped
Line 132363: 18:31:38,996 [atcher_executor] INFO  kafka.utils.Logging$class - [dev_dev-1407608194884-ceea5788], Cleared all relevant queues for this fetcher
Line 132364: 18:31:38,996 [atcher_executor] INFO  kafka.utils.Logging$class - [dev_dev-1407608194884-ceea5788], Cleared the data chunks in all the consumer message iterators
Line 132365: 18:31:38,996 [atcher_executor] INFO  kafka.utils.Logging$class - [dev_dev-1407608194884-ceea5788], Committing all offsets after clearing the fetcher queues
Line 132366: 18:31:38,996 [atcher_executor] INFO  kafka.utils.Logging$class - [dev_dev-1407608194884-ceea5788], Releasing partition ownership
Line 132367: 18:31:39,005 [7-cloudera:2181] INFO  kafka.utils.Logging$class - conflict in /consumers/Q/owners/gk.q.log/0 data: Q_dev-1407608193903-1cb30b18-0 stored data: Q_dev-1407608205503-9cfb99aa-0

likely what happened is when it reconnected the timeout with zk never occurred and it got stuck there.  Could be the Zk bug, could also be related somewhat to KAFKA-1387 or KAFKA-1451 I will link the JIRAs so when we test 0.8.2 see about reproducing this on a good zk version

To resolve that you can stop the consumer, wait for the zk nodes to expire and start up the consumers again.

;;;","10/Aug/14 21:21;adenysenko;I've tried with Zookeeper 3.4.6 - same problem.

KAFKA-1029 have some comments related to ""consumers"" as well: https://issues.apache.org/jira/browse/KAFKA-1029?focusedCommentId=13944775;;;","04/Sep/14 21:59;guozhang;I think this issue is already resolved by KAFKA-1451. Please re-open it with 0.9.0 version if you still see it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka API Refactoring,KAFKA-1583,12732945,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,08/Aug/14 21:51,30/Oct/14 02:03,14/Jul/23 05:39,30/Oct/14 02:03,,,,0.9.0.0,,,,,,,,,,0,,,,"This is the next step of KAFKA-1430. Details can be found at this page:

https://cwiki.apache.org/confluence/display/KAFKA/Kafka+API+Refactoring",,guozhang,jjkoshy,junrao,thedebugger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1720,,,,"13/Aug/14 23:16;guozhang;KAFKA-1583.patch;https://issues.apache.org/jira/secure/attachment/12661577/KAFKA-1583.patch","20/Aug/14 20:55;guozhang;KAFKA-1583_2014-08-20_13:54:38.patch;https://issues.apache.org/jira/secure/attachment/12663213/KAFKA-1583_2014-08-20_13%3A54%3A38.patch","21/Aug/14 18:30;guozhang;KAFKA-1583_2014-08-21_11:30:34.patch;https://issues.apache.org/jira/secure/attachment/12663450/KAFKA-1583_2014-08-21_11%3A30%3A34.patch","27/Aug/14 16:45;guozhang;KAFKA-1583_2014-08-27_09:44:50.patch;https://issues.apache.org/jira/secure/attachment/12664673/KAFKA-1583_2014-08-27_09%3A44%3A50.patch","02/Sep/14 01:07;guozhang;KAFKA-1583_2014-09-01_18:07:42.patch;https://issues.apache.org/jira/secure/attachment/12665824/KAFKA-1583_2014-09-01_18%3A07%3A42.patch","02/Sep/14 20:37;guozhang;KAFKA-1583_2014-09-02_13:37:47.patch;https://issues.apache.org/jira/secure/attachment/12665983/KAFKA-1583_2014-09-02_13%3A37%3A47.patch","05/Sep/14 21:08;guozhang;KAFKA-1583_2014-09-05_14:08:36.patch;https://issues.apache.org/jira/secure/attachment/12666895/KAFKA-1583_2014-09-05_14%3A08%3A36.patch","05/Sep/14 21:55;guozhang;KAFKA-1583_2014-09-05_14:55:38.patch;https://issues.apache.org/jira/secure/attachment/12666909/KAFKA-1583_2014-09-05_14%3A55%3A38.patch","14/Oct/14 02:42;guozhang;KAFKA-1583_2014-10-13_19:41:58.patch;https://issues.apache.org/jira/secure/attachment/12674665/KAFKA-1583_2014-10-13_19%3A41%3A58.patch","17/Oct/14 04:15;guozhang;KAFKA-1583_2014-10-16_21:15:40.patch;https://issues.apache.org/jira/secure/attachment/12675422/KAFKA-1583_2014-10-16_21%3A15%3A40.patch","17/Oct/14 16:56;guozhang;KAFKA-1583_2014-10-17_09:56:33.patch;https://issues.apache.org/jira/secure/attachment/12675508/KAFKA-1583_2014-10-17_09%3A56%3A33.patch","23/Oct/14 01:53;guozhang;KAFKA-1583_2014-10-22_18:52:52.patch;https://issues.apache.org/jira/secure/attachment/12676506/KAFKA-1583_2014-10-22_18%3A52%3A52.patch","28/Oct/14 22:09;guozhang;KAFKA-1583_2014-10-28_15:09:30.patch;https://issues.apache.org/jira/secure/attachment/12677716/KAFKA-1583_2014-10-28_15%3A09%3A30.patch",,,,,,,,,,,,,,,13.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,410973,,,Thu Oct 30 02:03:31 UTC 2014,,,,,,,,,,"0|i1ypf3:",410966,,jjkoshy,,,,,,,,,,,,,,,,,,"13/Aug/14 23:16;guozhang;Created reviewboard https://reviews.apache.org/r/24676/
 against branch origin/trunk;;;","20/Aug/14 20:55;guozhang;Updated reviewboard https://reviews.apache.org/r/24676/
 against branch origin/trunk;;;","21/Aug/14 18:30;guozhang;Updated reviewboard https://reviews.apache.org/r/24676/
 against branch origin/trunk;;;","27/Aug/14 16:45;guozhang;Updated reviewboard https://reviews.apache.org/r/24676/
 against branch origin/trunk;;;","02/Sep/14 01:07;guozhang;Updated reviewboard https://reviews.apache.org/r/24676/diff/
 against branch origin/trunk;;;","02/Sep/14 20:37;guozhang;Updated reviewboard https://reviews.apache.org/r/24676/diff/
 against branch origin/trunk;;;","05/Sep/14 21:08;guozhang;Updated reviewboard https://reviews.apache.org/r/24676/diff/
 against branch origin/trunk;;;","05/Sep/14 21:55;guozhang;Updated reviewboard https://reviews.apache.org/r/24676/diff/
 against branch origin/trunk;;;","19/Sep/14 20:29;guozhang;[~junrao] [~jjkoshy] Could you give this a look? Since this is a relatively large patch rebasing it multiple times with other new commits is quite some work :P.



;;;","19/Sep/14 21:13;junrao;Hi, Guozhang,

Yes, since this is a relatively large patch, I am thinking of committing it after the 0.8.2 branch is cut. I think we should be able to cut the branch in a week or so. Hopefully there won't be too many changes in the next week to make the rebase more complicated. Will this work for you? Thanks,;;;","19/Sep/14 21:17;guozhang;That is fine. I will wait then.;;;","19/Sep/14 21:18;jjkoshy;Yes I intend to review this within the next couple days.;;;","10/Oct/14 17:45;junrao;Guozhang,

Now that the 0.8.2 branch is cut, do you want to rebase the patch to latest trunk? Thanks,;;;","10/Oct/14 17:47;guozhang;Sure, will do that asap.;;;","14/Oct/14 02:42;guozhang;Updated reviewboard https://reviews.apache.org/r/24676/diff/
 against branch origin/trunk;;;","15/Oct/14 05:10;guozhang;Bump, [~junrao] could you take a look now?;;;","16/Oct/14 01:30;junrao;Just reviewed it.

Joel,

Do you want to take another look before committing this? Thanks,;;;","16/Oct/14 01:33;jjkoshy;Yes please. I will be able to do this on Friday.;;;","17/Oct/14 04:15;guozhang;Updated reviewboard https://reviews.apache.org/r/24676/diff/
 against branch origin/trunk;;;","17/Oct/14 16:56;guozhang;Updated reviewboard https://reviews.apache.org/r/24676/diff/
 against branch origin/trunk;;;","18/Oct/14 00:53;jjkoshy;Reviewing is taking a bit longer than expected - I'm about half-way through, so hopefully should be done on Monday.;;;","22/Oct/14 01:49;jjkoshy;I posted review comments, mostly minor.

Did you get a chance to run the system tests with this patch?;;;","22/Oct/14 15:25;guozhang;Yes, the system tests without the offset management test passes.;;;","23/Oct/14 01:53;guozhang;Updated reviewboard https://reviews.apache.org/r/24676/diff/
 against branch origin/trunk;;;","28/Oct/14 17:14;jjkoshy;Thanks for the updated patch. I will do this today. [~junrao] do you also want to take another look?;;;","28/Oct/14 22:09;guozhang;Updated reviewboard https://reviews.apache.org/r/24676/diff/
 against branch origin/trunk;;;","29/Oct/14 03:53;junrao;[~jjkoshy], you can commit the patch once you are done with the review.;;;","29/Oct/14 17:02;jjkoshy;Ok cool - I will re-review the latest patch and should be able to get that checked in today.;;;","30/Oct/14 02:03;jjkoshy;Thanks for the patch and your patience! Just checked in to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System Test should wait for producer to finish,KAFKA-1582,12732943,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,lindong,lindong,lindong,08/Aug/14 21:47,20/Aug/14 00:47,14/Jul/23 05:39,15/Aug/14 17:48,,,,,,,,,,,,,,0,,,,"1) start_producer_in_thread() does not wait for producer to finish before creating the next producer process. And producers may be killed before they finish.

2) Replace tab with spaces in kafka_system_test_utils.py.",,jjkoshy,lindong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/14 21:51;lindong;KAFKA-1582.patch;https://issues.apache.org/jira/secure/attachment/12660721/KAFKA-1582.patch","11/Aug/14 04:24;lindong;KAFKA-1582_2014-08-10_21:24:07.patch;https://issues.apache.org/jira/secure/attachment/12660918/KAFKA-1582_2014-08-10_21%3A24%3A07.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,410971,,,Fri Aug 15 17:48:21 UTC 2014,,,,,,,,,,"0|i1ypen:",410964,,,,,,,,,,,,,,,,,,,,"08/Aug/14 21:51;lindong;Created reviewboard https://reviews.apache.org/r/24510/diff/
 against branch origin/trunk;;;","11/Aug/14 04:24;lindong;Updated reviewboard https://reviews.apache.org/r/24510/diff/
 against branch origin/trunk;;;","15/Aug/14 17:48;jjkoshy;Committed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reject producer requests to internal topics,KAFKA-1580,12732913,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,guozhang,jjkoshy,jjkoshy,08/Aug/14 19:27,24/Nov/14 22:52,14/Jul/23 05:39,24/Nov/14 22:52,,,,0.8.2.0,,,,,,,core,,,0,,,,"Producer requests to internal topics (currently only __consumer_offset) can be disastrous.

E.g., if we allow a message to be appended to the offsets topic this could lead to fatal exceptions when loading the offsets topic and when compacting the log.

Producer requests to these topics should be rejected.",,guozhang,jjkoshy,natty,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1581,,,,,,,,,,"19/Nov/14 19:01;guozhang;KAFKA-1580.patch;https://issues.apache.org/jira/secure/attachment/12682457/KAFKA-1580.patch","12/Aug/14 23:44;natty;KAFKA-1580.patch;https://issues.apache.org/jira/secure/attachment/12661335/KAFKA-1580.patch","14/Aug/14 23:50;natty;KAFKA-1580_2014-08-14_16:50:40.patch;https://issues.apache.org/jira/secure/attachment/12661938/KAFKA-1580_2014-08-14_16%3A50%3A40.patch","14/Aug/14 23:56;natty;KAFKA-1580_2014-08-14_16:56:50.patch;https://issues.apache.org/jira/secure/attachment/12661941/KAFKA-1580_2014-08-14_16%3A56%3A50.patch","15/Aug/14 01:21;natty;KAFKA-1580_2014-08-14_18:21:38.patch;https://issues.apache.org/jira/secure/attachment/12661966/KAFKA-1580_2014-08-14_18%3A21%3A38.patch","15/Aug/14 22:05;natty;KAFKA-1580_2014-08-15_15:05:29.patch;https://issues.apache.org/jira/secure/attachment/12662174/KAFKA-1580_2014-08-15_15%3A05%3A29.patch",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,410941,,,Wed Nov 19 19:01:19 UTC 2014,,,,,,,,,,"0|i1yp8n:",410934,,jjkoshy,,,,,,,,,,,,,,,,,,"12/Aug/14 23:44;natty;Created reviewboard https://reviews.apache.org/r/24620/diff/
 against branch origin/trunk;;;","14/Aug/14 23:50;natty;Updated reviewboard https://reviews.apache.org/r/24620/diff/
 against branch origin/trunk;;;","14/Aug/14 23:56;natty;Updated reviewboard https://reviews.apache.org/r/24620/diff/
 against branch origin/trunk;;;","15/Aug/14 01:21;natty;Updated reviewboard https://reviews.apache.org/r/24620/diff/
 against branch origin/trunk;;;","15/Aug/14 22:05;natty;Updated reviewboard https://reviews.apache.org/r/24620/diff/
 against branch origin/trunk;;;","15/Aug/14 22:50;jjkoshy;Committed to trunk.;;;","14/Nov/14 20:54;jjkoshy;[~guozhang] I believe this change got lost as part of KAFKA-1583
Would you mind taking a look?

Also, I recently ran into a situation where I actually _needed_ to produce messages to the consumer offsets topic. I can send a separate email on that. So I would suggest allowing producer requests to internal topics if a special ""admin"" client id is specified (similar to how we allow fetches past the highwatermark with a special debugging client id).;;;","19/Nov/14 19:01;guozhang;Created reviewboard https://reviews.apache.org/r/28240/diff/
 against branch origin/trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller should de-register all listeners upon designation,KAFKA-1578,12732701,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,07/Aug/14 22:07,08/Aug/14 20:50,14/Jul/23 05:39,08/Aug/14 20:50,,,,0.8.2.0,,,,,,,,,,0,,,,"Today in KafkaController.onControllerResignation all the admin path listeners are not de-registered. This will cause multiple controllers listening on the same admin paths, and the fake controllers, upon failing to finish the admin commands, will also delete the admin zk path incorrectly.",,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/14 23:19;guozhang;KAFKA-1578.patch;https://issues.apache.org/jira/secure/attachment/12660501/KAFKA-1578.patch","08/Aug/14 17:28;guozhang;KAFKA-1578_2014-08-08_10:28:11.patch;https://issues.apache.org/jira/secure/attachment/12660670/KAFKA-1578_2014-08-08_10%3A28%3A11.patch","08/Aug/14 18:39;guozhang;KAFKA-1578_2014-08-08_11:39:06.patch;https://issues.apache.org/jira/secure/attachment/12660689/KAFKA-1578_2014-08-08_11%3A39%3A06.patch","08/Aug/14 20:14;guozhang;KAFKA-1578_2014-08-08_13:13:39.patch;https://issues.apache.org/jira/secure/attachment/12660705/KAFKA-1578_2014-08-08_13%3A13%3A39.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,410729,,,Fri Aug 08 20:50:49 UTC 2014,,,,,,,,,,"0|i1ynxz:",410722,,,,,,,,,,,,,,,,,,,,"07/Aug/14 23:19;guozhang;Created reviewboard https://reviews.apache.org/r/24480/
 against branch origin/trunk;;;","08/Aug/14 17:28;guozhang;Updated reviewboard https://reviews.apache.org/r/24480/
 against branch origin/trunk;;;","08/Aug/14 18:40;guozhang;Updated reviewboard https://reviews.apache.org/r/24480/
 against branch origin/trunk;;;","08/Aug/14 20:14;guozhang;Updated reviewboard https://reviews.apache.org/r/24480/
 against branch origin/trunk;;;","08/Aug/14 20:50;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception in ConnectionQuotas while shutting down,KAFKA-1577,12732652,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriharsha,jjkoshy,jjkoshy,07/Aug/14 19:47,28/Sep/15 22:03,14/Jul/23 05:39,30/Sep/14 01:26,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,newbie,,,"{code}
[2014-08-07 19:38:08,228] ERROR Uncaught exception in thread 'kafka-network-thread-9092-0': (kafka.utils.Utils$)
java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:185)
        at scala.None$.get(Option.scala:183)
        at kafka.network.ConnectionQuotas.dec(SocketServer.scala:471)
        at kafka.network.AbstractServerThread.close(SocketServer.scala:158)
        at kafka.network.AbstractServerThread.close(SocketServer.scala:150)
        at kafka.network.AbstractServerThread.closeAll(SocketServer.scala:171)
        at kafka.network.Processor.run(SocketServer.scala:338)
        at java.lang.Thread.run(Thread.java:662)
{code}",,bcalmac,german.borbolla,jjkoshy,kengo,nehanarkhede,olindaspider,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/14 22:41;sriharsha;KAFKA-1577.patch;https://issues.apache.org/jira/secure/attachment/12671574/KAFKA-1577.patch","13/Aug/14 16:11;sriharsha;KAFKA-1577.patch;https://issues.apache.org/jira/secure/attachment/12661480/KAFKA-1577.patch","21/Aug/14 02:58;sriharsha;KAFKA-1577_2014-08-20_19:57:44.patch;https://issues.apache.org/jira/secure/attachment/12663328/KAFKA-1577_2014-08-20_19%3A57%3A44.patch","26/Aug/14 14:33;sriharsha;KAFKA-1577_2014-08-26_07:33:13.patch;https://issues.apache.org/jira/secure/attachment/12664397/KAFKA-1577_2014-08-26_07%3A33%3A13.patch","27/Sep/14 02:13;sriharsha;KAFKA-1577_2014-09-26_19:13:05.patch;https://issues.apache.org/jira/secure/attachment/12671615/KAFKA-1577_2014-09-26_19%3A13%3A05.patch","30/Aug/14 16:01;bcalmac;KAFKA-1577_check_counter_before_decrementing.patch;https://issues.apache.org/jira/secure/attachment/12665570/KAFKA-1577_check_counter_before_decrementing.patch","15/Jan/15 04:22;german.borbolla;kafka-logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12692440/kafka-logs.tar.gz",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,410680,,,Mon Sep 28 22:03:08 UTC 2015,,,,,,,,,,"0|i1ynn3:",410673,,jjkoshy,,,,,,,,,,,,,,,,,,"13/Aug/14 16:11;sriharsha;Created reviewboard https://reviews.apache.org/r/24653/diff/
 against branch origin/trunk;;;","13/Aug/14 23:41;sriharsha;[~jjkoshy] Can you please share some details on which conditions this error happened. I tried reproduce the case with 3 brokers and 2 topics with  replicationFactor 2 . Shutting down any or all of the brokers didn't cause this exception to be thrown.  Thanks.;;;","14/Aug/14 01:39;jjkoshy;Unfortunately I don't have reliable steps to reproduce - it shows up rarely: bring up a single broker and shut it down immediately - let me revisit this tomorrow and I'll get back to you.;;;","14/Aug/14 21:17;jjkoshy;[~sriharsha] I tried reproducing this again and as I mentioned it is a
corner case so it is hard to catch. Anyway, here is the root cause:

This only happens during shutdown. We close out existing selection keys as
the requests are completed. When closing the key, we close the associated
channel (which decrements connection count) and then cancel the key.
Although the key is canceled, it will be removed from the selector only on
the next select. The shutdown process proceeds to close all the keys of the
selector (i.e., socket server's closeAll). However, the above key (and
associated closed channel) may still be there which is why we see this
exception.

So I think we can swallow this error if we are shutting down and place a
comment in the code on why we are swallowing it.
;;;","21/Aug/14 02:58;sriharsha;Updated reviewboard https://reviews.apache.org/r/24653/diff/
 against branch origin/trunk;;;","26/Aug/14 14:33;sriharsha;Updated reviewboard https://reviews.apache.org/r/24653/diff/
 against branch origin/trunk;;;","30/Aug/14 16:01;bcalmac;Aside from swallowing the exception at top level, wouldn't it be good to also fix {{ConnectionQuotas.dec()}} and check for {{isDefined()}} before decrementing the counter?

As an undesired side-effect of allowing the NoSuchElementException in the first place, the close() methods after dec() in the code below would not get called.

{code}
  def close(channel: SocketChannel) {
    if(channel != null) {
      debug(""Closing connection from "" + channel.socket.getRemoteSocketAddress())
      connectionQuotas.dec(channel.socket.getInetAddress)
      swallowError(channel.socket().close())
      swallowError(channel.close())
    }
  }
{code}

See https://reviews.apache.org/r/25213/ and the patch [^KAFKA-1577_check_counter_before_decrementing.patch].
;;;","05/Sep/14 19:10;jjkoshy;The exception would only occur if the socket and channel were closed in the first place, no?;;;","05/Sep/14 19:48;bcalmac;Maybe, I don't know the code well enough to draw that conclusion. But what would be a good reason to allow the exception in the first place? The exception isn't caused by an external factor but by a programming error (the assumption that the Option always has a value).

This puts pressure on all methods up the call hierarchy to do a proper cleanup after a RuntimeException. The {{close()}} method I mentioned was just an example. Why look for trouble?;;;","05/Sep/14 20:44;jjkoshy;I think the main reason to allow the exception (as opposed to an existence check) is that it should never happen. If it does, then it is a bug and we need to know about it. We can either receive a runtime exception or do an existence check and log an error. There are a number of places elsewhere in the code where we expect keys to be present. If not it is a (potentially serious) bug - we would rather let an exception be thrown rather than do an existence check and log an error especially if it is a serious issue. In this case we traced the cause of this occurrence to a race condition that only happens during shutdown which is why swallowing at that point is reasonable since that is the only circumstance under which a missing key is possible (and okay). Going forward, if the exception shows up anytime other than shutdown then we will need to again debug why that is the case and fix it - e.g., if it is related to the same race condition then we should fix that race condition.;;;","05/Sep/14 20:57;bcalmac;OK, makes sense. Now, if this is the case, shouldn't the exception be swallowed as soon as possible as below:

{code}
  def close(channel: SocketChannel) {
    if(channel != null) {
      debug(""Closing connection from "" + channel.socket.getRemoteSocketAddress())
      swallowError(connectionQuotas.dec(channel.socket.getInetAddress)) // known race condition may lead to NoSuchElementException
      swallowError(channel.socket().close())
      swallowError(channel.close())
    }
  }
{code}

It might make no difference at runtime, but the code is more readable.;;;","05/Sep/14 21:30;jjkoshy;We can do that, but it does allow the error to go undetected (apart from the error log) and execution continues (even in the non-shutdown case). It is a bug if the element does not exist - i.e., execution should not proceed beyond this point which is what an exception provides. The swallow is okay in the shutdown code because we explicitly allow the key's non-existence there.;;;","05/Sep/14 21:35;bcalmac;You're right. Thanks for the explanation.;;;","26/Sep/14 03:52;nehanarkhede;See KAFKA-1652;;;","26/Sep/14 18:40;sriharsha;[~nehanarkhede] [~jjkoshy]
This issue happens when the shutdown process started in the broker and the SocketServer.Processor.run throws EOFException and it calls close(key) which deletes the key from connectionQuota and also calls key.cancel() but it doesn't remove the key from the Selector.keys until next selection process begins  "" Cancelled keys are removed from the key set during selection operations."" (from java doc).
broker shutdown process calls SocketServer.shutdown() which goes through selector.keys and calls close on each of them since there is no selection operation happened cancelled key in the previous step still exist in selector.keys but its deleted from conncetionQuotas causing it throw that exception.
one way to fix this is to force the selection operation in closeAll() 
this.selector.selectNow()
Its an unnecessary operation in closeAll but it will clear up any cancelled keys from selector.keys(). Please let me know your thoughts on this.;;;","26/Sep/14 18:51;jjkoshy;Oh now I remember this - the root cause is as described above and in my earlier comment. The original patch only prevents the exception from escaping which actually causes shutdown to hang. The error is still logged - which I thought was annoying but can be addressed by logging. From my last comment in the original RB:

{quote}
Thanks for the patch - this is what I had in mind. The stack trace still shows up in the log though - I forgot that although swallow keeps the exception from bubbling up further, it still logs the exception which is annoying.
However, the shutdown no longer hangs (as it used to) so your patch works.
So I think it would be reasonable to just change this to swallow (which means swallowWarn) and let the exception show up in the logs for now.
There is a separate effort to improve our logging convention and the current recommendation (for this instance) is to avoid showing the stack trace. That will be done in the jiras that come out of https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Error+Handling+and+Logging
{quote}
;;;","26/Sep/14 18:56;jjkoshy;I think selectNow is a good option.;;;","26/Sep/14 19:44;nehanarkhede;[~jjkoshy] I could also reproduce the hang in addition to just the error message during shutdown.;;;","26/Sep/14 19:53;jjkoshy;Hmm.. that is really weird - since the swallow doesn't allow the exception to escape. Do you have a full threaddump when this happened?;;;","26/Sep/14 20:35;sriharsha;[~nehanarkhede] I am not able to reproduce the hang. Shutdown process goes through fine for me apart from the exception showing up in logs. Were you able to reproduce this by following steps in KAFKA-1652 with one broker . ;;;","26/Sep/14 21:03;nehanarkhede;[~jjkoshy], [~sriharsha] I remember I tried taking a thread dump and later couldn't find the output of the thread dump in server.log. Finally, just hard killed the broker and all of this happened within 2 minutes. Will try to reproduce again and spend more time collecting the thread dump. Also, thinking again, I'm actually not a 100% sure that I could reproduce the hang on latest trunk. It is possible that I hadn't rebased my local trunk :);;;","26/Sep/14 22:41;sriharsha;Created reviewboard https://reviews.apache.org/r/26107/diff/
 against branch origin/trunk;;;","27/Sep/14 02:13;sriharsha;Updated reviewboard https://reviews.apache.org/r/26107/diff/
 against branch origin/trunk;;;","30/Sep/14 01:26;jjkoshy;Committed to trunk;;;","10/Jan/15 00:55;german.borbolla;This is marked as Fix version for 0.8.2 however I just encountered the same issue using the latest source from the 0.8.2 branch.

Perhaps it was only committed to trunk? Is there any chance this will be included in 0.8.2?

Here's the stack trace: 
{noformat}
[2015-01-09 15:53:12,486] ERROR Uncaught exception in thread 'kafka-network-thread-9092-4': (kafka.utils.Utils$)
java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:322)
        at scala.None$.get(Option.scala:320)
        at kafka.network.ConnectionQuotas.dec(SocketServer.scala:524)
        at kafka.network.AbstractServerThread.close(SocketServer.scala:165)
        at kafka.network.AbstractServerThread.close(SocketServer.scala:157)
        at kafka.network.Processor.close(SocketServer.scala:374)
        at kafka.network.AbstractServerThread.closeAll(SocketServer.scala:180)
        at kafka.network.Processor.run(SocketServer.scala:364)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

Thanks;;;","10/Jan/15 03:20;sriharsha;[~german.borbolla] The patch is in 0.8.2 branch . I'll try to see if I can reproduce this again. ;;;","10/Jan/15 03:42;german.borbolla;[~sriharsha] you're right, this happened twice to me today. In both cases the broker had something around 30000 partitions and controlled shutdown finished successfully. This is a nine node cluster and each was similarly loaded. 

I should be able to get you the logs if it helps to debug.;;;","10/Jan/15 03:56;sriharsha;[~german.borbolla] can you please attach some logs . Thanks.;;;","15/Jan/15 04:22;german.borbolla;[~sriharsha] Sorry for the delay, I lost the previous logs but I was able to reproduce this.;;;","15/Jan/15 16:11;jjkoshy;[~german.borbolla] can you make sure you don't have a stray kafka jar in your classpath? I have noticed this when switching across git hashes that include changes to our build mechanism. Do a gradlew clean and ensure that ""find . -name kafka*.jar"" returns nothing and then rebuild before trying to reproduce this.;;;","15/Jan/15 16:15;german.borbolla;I will try to reproduce with the release candidate for 0.8.2;;;","28/Sep/15 22:03;olindaspider;I believe I have found the root cause of this issue as explained here: https://issues.apache.org/jira/browse/KAFKA-1804?focusedCommentId=14909940&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14909940;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unit tests can hang on socketserver shutdown,KAFKA-1574,12732331,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,06/Aug/14 16:36,11/Aug/14 14:46,14/Jul/23 05:39,11/Aug/14 14:46,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,newbie++,,,"Saw the following stacktrace.

""kafka-network-thread-59843-2"" prio=5 tid=7fc7e5943800 nid=0x11eefa000 runnable [11eef9000]
   java.lang.Thread.State: RUNNABLE
        at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method)
        at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:136)
        at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:69)
        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
        - locked <7f4a80328> (a sun.nio.ch.Util$2)
        - locked <7f4a80310> (a java.util.Collections$UnmodifiableSet)
        - locked <7f4a71968> (a sun.nio.ch.KQueueSelectorImpl)
        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
        at kafka.network.Processor.run(SocketServer.scala:296)
        at java.lang.Thread.run(Thread.java:695)

""Test worker"" prio=5 tid=7fc7e50d4800 nid=0x11534c000 waiting on condition [115349000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <7f4a69d50> (a java.util.concurrent.CountDownLatch$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)
        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:207)
        at kafka.network.AbstractServerThread.shutdown(SocketServer.scala:113)
        at kafka.network.SocketServer$$anonfun$shutdown$2.apply(SocketServer.scala:92)
        at kafka.network.SocketServer$$anonfun$shutdown$2.apply(SocketServer.scala:91)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
        at kafka.network.SocketServer.shutdown(SocketServer.scala:91)
        at kafka.server.KafkaServer$$anonfun$shutdown$3.apply$mcV$sp(KafkaServer.scala:246)
        at kafka.utils.Utils$.swallow(Utils.scala:172)
        at kafka.utils.Logging$class.swallowWarn(Logging.scala:92)
        at kafka.utils.Utils$.swallowWarn(Utils.scala:45)
        at kafka.utils.Logging$class.swallow(Logging.scala:94)
        at kafka.utils.Utils$.swallow(Utils.scala:45)
        at kafka.server.KafkaServer.shutdown(KafkaServer.scala:246)
        at kafka.admin.AdminTest$$anonfun$testPartitionReassignmentNonOverlappingReplicas$3.apply(AdminTest.scala:232)
        at kafka.admin.AdminTest$$anonfun$testPartitionReassignmentNonOverlappingReplicas$3.apply(AdminTest.scala:232)
",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/14 04:45;junrao;KAFKA-1574.patch;https://issues.apache.org/jira/secure/attachment/12660920/KAFKA-1574.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,410359,,,Mon Aug 11 14:46:18 UTC 2014,,,,,,,,,,"0|i1ylnj:",410348,,,,,,,,,,,,,,,,,,,,"06/Aug/14 16:40;junrao;This issue seems to be a race condition in AbstractServerThread. In Processor startup, we first call startupComplete(). However, if Processor.shutdown() is called before startupComplete() is called, the latter will set alive back to true and the thread will never be able to shutdown.

For the fix, since AbstractServerThread is not expected to be restarted, we can just set alive to true during the initialization, instead of during startupComplete().
;;;","11/Aug/14 04:45;junrao;Created reviewboard https://reviews.apache.org/r/24540/
 against branch origin/trunk;;;","11/Aug/14 14:46;junrao;Thanks for the reviews. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetadataeTest hangs,KAFKA-1571,12731888,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,04/Aug/14 23:13,05/Aug/14 04:22,14/Jul/23 05:39,05/Aug/14 04:22,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"Saw the following stacktrace. 

""Thread-47"" prio=10 tid=0x00007fb5b00a5000 nid=0x25de in Object.wait() [0x00007fb5af9f8000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00000006b0925e40> (a org.apache.kafka.clients.producer.internals.Metadata)
        at org.apache.kafka.clients.producer.internals.Metadata.awaitUpdate(Metadata.java:107)
        - locked <0x00000006b0925e40> (a org.apache.kafka.clients.producer.internals.Metadata)
        at org.apache.kafka.clients.producer.MetadataTest$1.run(MetadataTest.java:57)

""Thread-46"" prio=10 tid=0x00007fb5b00a3800 nid=0x25dd in Object.wait() [0x00007fb5afbfa000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00000006b0925e40> (a org.apache.kafka.clients.producer.internals.Metadata)
        at org.apache.kafka.clients.producer.internals.Metadata.awaitUpdate(Metadata.java:107)
        - locked <0x00000006b0925e40> (a org.apache.kafka.clients.producer.internals.Metadata)
        at org.apache.kafka.clients.producer.MetadataTest$1.run(MetadataTest.java:57)

""Test worker"" prio=10 tid=0x00007fb610891000 nid=0x25b1 in Object.wait() [0x00007fb5d4a5f000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00000006b0926700> (a org.apache.kafka.clients.producer.MetadataTest$1)
        at java.lang.Thread.join(Thread.java:1186)
        - locked <0x00000006b0926700> (a org.apache.kafka.clients.producer.MetadataTest$1)
        at java.lang.Thread.join(Thread.java:1239)
        at org.apache.kafka.clients.producer.MetadataTest.testMetadata(MetadataTest.java:46)
",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/14 00:40;junrao;KAFKA-1571.patch;https://issues.apache.org/jira/secure/attachment/12659780/KAFKA-1571.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,409917,,,Tue Aug 05 04:22:53 UTC 2014,,,,,,,,,,"0|i1yizb:",409911,,,,,,,,,,,,,,,,,,,,"05/Aug/14 00:40;junrao;Created reviewboard https://reviews.apache.org/r/24287/
 against branch origin/trunk;;;","05/Aug/14 04:22;junrao;Thanks for the review. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metric memory leaking after closing the clients,KAFKA-1567,12731337,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,becket_qin,becket_qin,becket_qin,01/Aug/14 15:35,14/Jun/15 08:47,14/Jul/23 05:39,13/Aug/14 20:10,,,,0.8.2.0,,,,,,,clients,,,0,patch,,,"When consumers or producers shut down, the metrics they registered in yammer registry is not deleted which causes memory leak.",,becket_qin,Bmis13,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1083,,,,,,,,,,,,,,,,,,,"01/Aug/14 21:26;becket_qin;KAFKA-1567.patch;https://issues.apache.org/jira/secure/attachment/12659254/KAFKA-1567.patch","01/Aug/14 18:06;becket_qin;KAFKA-1567.patch;https://issues.apache.org/jira/secure/attachment/12659215/KAFKA-1567.patch","01/Aug/14 18:04;becket_qin;KAFKA-1567.patch;https://issues.apache.org/jira/secure/attachment/12659213/KAFKA-1567.patch","01/Aug/14 17:47;becket_qin;KAFKA-1567.patch;https://issues.apache.org/jira/secure/attachment/12659210/KAFKA-1567.patch","07/Aug/14 21:29;becket_qin;KAFKA-1567_2014-08-07_14:26:04.patch;https://issues.apache.org/jira/secure/attachment/12660471/KAFKA-1567_2014-08-07_14%3A26%3A04.patch","09/Aug/14 01:44;becket_qin;KAFKA-1567_2014-08-08_18:44:20.patch;https://issues.apache.org/jira/secure/attachment/12660781/KAFKA-1567_2014-08-08_18%3A44%3A20.patch","11/Aug/14 22:55;becket_qin;KAFKA-1567_2014-08-11_15:55:26.patch;https://issues.apache.org/jira/secure/attachment/12661084/KAFKA-1567_2014-08-11_15%3A55%3A26.patch","12/Aug/14 22:20;becket_qin;KAFKA-1567_2014-08-12_15:20:10.patch;https://issues.apache.org/jira/secure/attachment/12661319/KAFKA-1567_2014-08-12_15%3A20%3A10.patch","13/Aug/14 15:27;becket_qin;KAFKA-1567_2014-08-13_08:25:57.patch;https://issues.apache.org/jira/secure/attachment/12661472/KAFKA-1567_2014-08-13_08%3A25%3A57.patch","13/Aug/14 18:25;becket_qin;KAFKA-1567_2014-08-13_11:24:29.patch;https://issues.apache.org/jira/secure/attachment/12661496/KAFKA-1567_2014-08-13_11%3A24%3A29.patch",,,,,,,,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,409409,,,Wed Aug 13 22:28:47 UTC 2014,,,,,,,,,,"0|i1yfxj:",409405,,,,,,,,,,,,,,,,,,,,"01/Aug/14 17:47;becket_qin;Created reviewboard  against branch origin/trunk;;;","01/Aug/14 18:04;becket_qin;Created reviewboard  against branch origin/trunk;;;","01/Aug/14 18:06;becket_qin;Created reviewboard  against branch origin/trunk;;;","01/Aug/14 21:26;becket_qin;Created reviewboard https://reviews.apache.org/r/24196/diff/
 against branch origin/trunk;;;","07/Aug/14 21:29;becket_qin;Updated reviewboard https://reviews.apache.org/r/24196/diff/
 against branch origin/trunk;;;","09/Aug/14 01:44;becket_qin;Updated reviewboard https://reviews.apache.org/r/24196/diff/
 against branch origin/trunk;;;","11/Aug/14 21:45;Bmis13;HI Team,

This is also related to this issue I have filed KAFKA-1521.. This should fix the issue as well.

Thanks,

Bhavesh ;;;","11/Aug/14 22:56;becket_qin;Updated reviewboard https://reviews.apache.org/r/24196/diff/
 against branch origin/trunk;;;","12/Aug/14 22:20;becket_qin;Updated reviewboard https://reviews.apache.org/r/24196/diff/
 against branch origin/trunk;;;","13/Aug/14 15:27;becket_qin;Updated reviewboard https://reviews.apache.org/r/24196/diff/
 against branch origin/trunk;;;","13/Aug/14 18:25;becket_qin;Updated reviewboard https://reviews.apache.org/r/24196/diff/
 against branch origin/trunk;;;","13/Aug/14 20:10;junrao;Thanks for the patch. +1 and committed to trunk.;;;","13/Aug/14 21:28;jjkoshy;Although this is in the old consumer and will be going away soon, I'm a bit curious about the approach in this patch - rather than enumerate the different metric names for consumers/producers in KafkaMetricsGroup wouldn't it have been simpler to just create a new MetricsRegistry for each instance of producer/consumer/broker? Whenever you shutdown an instance you would just need to remove all the metrics in the associated instance registry.;;;","13/Aug/14 22:28;junrao;Joel,

We thought about that. The issue is that (1) the metrics are at different levels and we have to pass the metrics registry around in many places; (2) some of the metrics in the fetcher are shared btw the consumer and the broker.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-topics.sh alter add partitions resets cleanup.policy,KAFKA-1562,12730566,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,natty,kenmacd,kenmacd,29/Jul/14 18:50,04/Aug/14 14:22,14/Jul/23 05:39,04/Aug/14 14:22,0.8.1.1,,,0.8.2.0,,,,,,,,,,0,,,,"When partitions are added to an already existing topic the cleanup.policy=compact is not retained.

{code}
./kafka-topics.sh --zookeeper localhost --create --partitions 1 --replication-factor 1 --topic KTEST --config cleanup.policy=compact

./kafka-topics.sh --zookeeper localhost --describe --topic KTEST
Topic:KTEST	PartitionCount:1	ReplicationFactor:1	Configs:cleanup.policy=compact
	Topic: KTEST	Partition: 0	Leader: 0	Replicas: 0	Isr: 0

./kafka-topics.sh --zookeeper localhost --alter --partitions 3 --topic KTEST --config cleanup.policy=compact

 ./kafka-topics.sh --zookeeper localhost --describe --topic KTEST
Topic:KTEST	PartitionCount:3	ReplicationFactor:1	Configs:
	Topic: KTEST	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
	Topic: KTEST	Partition: 1	Leader: 0	Replicas: 0	Isr: 0
	Topic: KTEST	Partition: 2	Leader: 0	Replicas: 0	Isr: 0
{code}",,junrao,kenmacd,natty,vanyatka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/14 20:16;natty;KAFKA-1562.patch;https://issues.apache.org/jira/secure/attachment/12658725/KAFKA-1562.patch","30/Jul/14 20:18;natty;KAFKA-1562_2014-07-30_13:18:21.patch;https://issues.apache.org/jira/secure/attachment/12658726/KAFKA-1562_2014-07-30_13%3A18%3A21.patch","30/Jul/14 20:51;natty;KAFKA-1562_2014-07-30_13:51:25.patch;https://issues.apache.org/jira/secure/attachment/12658734/KAFKA-1562_2014-07-30_13%3A51%3A25.patch","02/Aug/14 18:10;natty;KAFKA-1562_2014-08-02_11:10:34.patch;https://issues.apache.org/jira/secure/attachment/12659490/KAFKA-1562_2014-08-02_11%3A10%3A34.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,408639,,,Mon Aug 04 14:22:41 UTC 2014,,,,,,,,,,"0|i1yb93:",408637,,,,,,,,,,,,,,,,,,,,"30/Jul/14 19:49;natty;If it's alright, I was planning on working on this a bit. I think I know where the issue is, and I'm in the process of fixing it at the moment.;;;","30/Jul/14 20:16;natty;Created reviewboard https://reviews.apache.org/r/24113/diff/
 against branch origin/trunk;;;","30/Jul/14 20:18;natty;Updated reviewboard https://reviews.apache.org/r/24113/diff/
 against branch origin/trunk;;;","30/Jul/14 20:51;natty;Updated reviewboard https://reviews.apache.org/r/24113/diff/
 against branch origin/trunk;;;","02/Aug/14 18:10;natty;Updated reviewboard https://reviews.apache.org/r/24113/diff/
 against branch origin/trunk;;;","04/Aug/14 14:22;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data Loss for Incremented Replica Factor and Leader Election,KAFKA-1561,12730349,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,28/Jul/14 23:14,11/Dec/17 19:01,14/Jul/23 05:39,11/Dec/17 19:01,,,,0.11.0.0,,,,,,,,,,0,,,,"This is reported on the mailing list (thanks to Jad).

{quote}
Hi,

I have a test that continuously sends messages to one broker, brings up
another broker, and adds it as a replica for all partitions, with it being
the preferred replica for some. I have auto.leader.rebalance.enable=true,
so replica election gets triggered. Data is being pumped to the old broker
all the while. It seems that some data gets lost while switching over to
the new leader. Is this a bug, or do I have something misconfigured? I also
have request.required.acks=-1 on the producer.

Here's what I think is happening:

1. Producer writes message to broker 0, [EventServiceUpsertTopic,13], w/
broker 0 currently leader, with ISR=(0), so write returns successfully,
even when acks = -1. Correlation id 35836

Producer log:

[2014-07-24 14:44:26,991]  [DEBUG]  [dw-97 - PATCH
/v1/events/type_for_test_bringupNewBroker_shouldRebalance_shouldNotLoseData/event?_idPath=idField&_mergeFields=field1]
[kafka.producer.BrokerPartitionInfo]  Partition
[EventServiceUpsertTopic,13] has leader 0

[2014-07-24 14:44:26,993]  [DEBUG]  [dw-97 - PATCH
/v1/events/type_for_test_bringupNewBroker_shouldRebalance_shouldNotLoseData/event?_idPath=idField&_mergeFields=field1]
[k.producer.async.DefaultEventHandler]  Producer sent messages with
correlation id 35836 for topics [EventServiceUpsertTopic,13] to broker 0 on
localhost:56821
2. Broker 1 is still catching up

Broker 0 Log:

[2014-07-24 14:44:26,992]  [DEBUG]  [kafka-request-handler-3]
[kafka.cluster.Partition]  Partition [EventServiceUpsertTopic,13] on broker
0: Old hw for partition [EventServiceUpsertTopic,13] is 971. New hw is 971.
All leo's are 975,971

[2014-07-24 14:44:26,992]  [DEBUG]  [kafka-request-handler-3]
[kafka.server.KafkaApis]  [KafkaApi-0] Produce to local log in 0 ms

[2014-07-24 14:44:26,992]  [DEBUG]  [kafka-processor-56821-0]
[kafka.request.logger]  Completed request:Name: ProducerRequest; Version:
0; CorrelationId: 35836; ClientId: ; RequiredAcks: -1; AckTimeoutMs: 10000
ms from client /127.0.0.1:57086
;totalTime:0,requestQueueTime:0,localTime:0,remoteTime:0,responseQueueTime:0,sendTime:0
3. Leader election is triggered by the scheduler:

Broker 0 Log:

[2014-07-24 14:44:26,991]  [INFO ]  [kafka-scheduler-0]
[k.c.PreferredReplicaPartitionLeaderSelector]
[PreferredReplicaPartitionLeaderSelector]: Current leader 0 for partition [
EventServiceUpsertTopic,13] is not the preferred replica. Trigerring
preferred replica leader election

[2014-07-24 14:44:26,993]  [DEBUG]  [kafka-scheduler-0]
[kafka.utils.ZkUtils$]  Conditional update of path
/brokers/topics/EventServiceUpsertTopic/partitions/13/state with value
{""controller_epoch"":1,""leader"":1,""version"":1,""leader_epoch"":3,""isr"":[0,1]}
and expected version 3 succeeded, returning the new version: 4

[2014-07-24 14:44:26,994]  [DEBUG]  [kafka-scheduler-0]
[k.controller.PartitionStateMachine]  [Partition state machine on
Controller 0]: After leader election, leader cache is updated to
Map(<Snipped>(Leader:1,ISR:0,1,LeaderEpoch:3,ControllerEpoch:1),<EndSnip>)

[2014-07-24 14:44:26,994]  [INFO ]  [kafka-scheduler-0]
[kafka.controller.KafkaController]  [Controller 0]: Partition [
EventServiceUpsertTopic,13] completed preferred replica leader election.
New leader is 1
4. Broker 1 is still behind, but it sets the high water mark to 971!!!

Broker 1 Log:

[2014-07-24 14:44:26,999]  [INFO ]  [kafka-request-handler-6]
[kafka.server.ReplicaFetcherManager]  [ReplicaFetcherManager on broker 1]
Removed fetcher for partitions [EventServiceUpsertTopic,13]

[2014-07-24 14:44:27,000]  [DEBUG]  [kafka-request-handler-6]
[kafka.cluster.Partition]  Partition [EventServiceUpsertTopic,13] on broker
1: Old hw for partition [EventServiceUpsertTopic,13] is 970. New hw is -1.
All leo's are -1,971

[2014-07-24 14:44:27,098]  [DEBUG]  [kafka-request-handler-3]
[kafka.server.KafkaApis]  [KafkaApi-1] Maybe update partition HW due to
fetch request: Name: FetchRequest; Version: 0; CorrelationId: 1; ClientId:
ReplicaFetcherThread-0-1; ReplicaId: 0; MaxWait: 500 ms; MinBytes: 1 bytes;
RequestInfo: [EventServiceUpsertTopic,13] ->
PartitionFetchInfo(971,1048576), <Snipped>

[2014-07-24 14:44:27,098]  [DEBUG]  [kafka-request-handler-3]
[kafka.cluster.Partition]  Partition [EventServiceUpsertTopic,13] on broker
1: Recording follower 0 position 971 for partition [
EventServiceUpsertTopic,13].

[2014-07-24 14:44:27,100]  [DEBUG]  [kafka-request-handler-3]
[kafka.cluster.Partition]  Partition [EventServiceUpsertTopic,13] on broker
1: Highwatermark for partition [EventServiceUpsertTopic,13] updated to 971
5. Consumer is none the wiser. All data that was in offsets 972-975 doesn't
show up!

I tried this with 2 initial replicas, and adding a 3rd which is supposed to
be the leader for some new partitions, and this problem also happens there.
The log on the old leader gets truncated to the offset on the new leader.
What's the solution? Can I make a new broker leader for partitions that are
currently active without losing data?

Thanks,
Jad.
{quote}",,guozhang,jnaous,junrao,panih2o,vinayak10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/14 23:20;jnaous;broker0.log;https://issues.apache.org/jira/secure/attachment/12658294/broker0.log","28/Jul/14 23:20;jnaous;broker2.log;https://issues.apache.org/jira/secure/attachment/12658295/broker2.log","28/Jul/14 23:20;jnaous;consumer.log;https://issues.apache.org/jira/secure/attachment/12658296/consumer.log","28/Jul/14 23:20;jnaous;producer.log;https://issues.apache.org/jira/secure/attachment/12658297/producer.log",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,408422,,,Wed Jun 21 07:32:32 UTC 2017,,,,,,,,,,"0|i1y9yn:",408421,,,,,,,,,,,,,,,,,,,,"28/Jul/14 23:22;jnaous;Here's some more detailed info on what the latest test does (from which these logs are obtained):

0) Start two brokers, one producer, one consumer. Topic has 20 partitions, using default partitioning scheme (which seems to send data to only a couple of partitions when the keys are null, but that doesn't matter for this test).
1) Start a data generator sending data through Kafka continuously
2) Start a new broker
3) Reassign partitions:
{code}
{""version"": 1, ""partitions"":[
    {""topic"":""EventServiceUpsertTopic"",""partition"":0,        ""replicas"": [0, 1, 2]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":1,        ""replicas"": [1, 2, 0]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":2,        ""replicas"": [2, 0, 1]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":3,        ""replicas"": [0, 1, 2]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":4,        ""replicas"": [1, 2, 0]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":5,        ""replicas"": [2, 0, 1]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":6,        ""replicas"": [0, 1, 2]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":7,        ""replicas"": [1, 2, 0]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":8,        ""replicas"": [2, 0, 1]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":9,        ""replicas"": [0, 1, 2]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":10,        ""replicas"": [1, 2, 0]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":11,        ""replicas"": [2, 0, 1]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":12,        ""replicas"": [0, 1, 2]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":13,        ""replicas"": [1, 2, 0]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":14,        ""replicas"": [2, 0, 1]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":15,        ""replicas"": [0, 1, 2]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":16,        ""replicas"": [1, 2, 0]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":17,        ""replicas"": [2, 0, 1]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":18,        ""replicas"": [0, 1, 2]},
    {""topic"":""EventServiceUpsertTopic"",""partition"":19,        ""replicas"": [1, 2, 0]}]}
{code}
4) Wait until reassignment is complete (i.e. until ZkUtils.getPartitionsBeingReassigned() returns empty map)
5) Wait until all replicas are caught up (i.e. until ZkUtils.getLeaderAndIsrForPartition() returns all brokers in the ISR for each partition)
6) Trigger leader re-election by calling the PreferredReplicaLeaderElectionCommand
7) Wait until all the leaders are the preferred leaders for partitions according to the replica reassignment from step 3
8) Stop the data generator
9) Check that all the data was consumed

You can see from the producer.log that the data: {{ {""field1"": [""10""], ""idField"": ""id-5-59""} }} was sent to broker0 successfully, but the consumer never sees it.;;;","26/May/17 16:10;junrao;Hmm, interesting. From the description, when the LEOs on the leader are 574 and 571, the HW on broker 0 is still at 571. This suggests that messages between 572 and 574 haven't been committed and the producer shouldn't have received a successful ack with acks=-1.;;;","06/Jun/17 16:42;vinayak10;Hi,
[~junrao]
I have Cluster consisting of 3 Zookeeper nodes and 2 Brokers running on AWS instances. When I am trying to scale Brokers from 2 to 3 while simultaneously producing and consuming from topic I am experiencing loss of messages.

Topic :
Partitions - 40
Replication Factor - 2

I am using console producer to produce 1000 messages at a time to the topic. I do this for 200 secs and then print total no of messages produced, while simultaneously I run a script to consume from the same topic.While these scripts are running I reassign the partitions of the same topic from 2 brokers(0,1) to 3 brokers(0,1,2 ). While these reassignment of partitions is running I see producer throwing the following logs:
[2017-05-31 13:31:13,311] WARN Got error produce response with correlation id 4 on topic-partition Topic6-39, retrying (2 attempts left). Error: NOT_LEADER_FOR_PARTITION (org.apache.kafka.clients.producer.internals.Sender)
[2017-05-31 13:31:13,338] WARN Got error produce response with correlation id 4 on topic-partition Topic6-27, retrying (2 attempts left). Error: NOT_LEADER_FOR_PARTITION (org.apache.kafka.clients.producer.internals.Sender)
[2017-05-31 13:31:13,338] WARN Got error produce response with correlation id 4 on topic-partition Topic6-21, retrying (2 attempts left). Error: NOT_LEADER_FOR_PARTITION (org.apache.kafka.clients.producer.internals.Sender)
[2017-05-31 13:31:13,338] WARN Got error produce response with correlation id 4 on topic-partition Topic6-15, retrying (2 attempts left). Error: NOT_LEADER_FOR_PARTITION (org.apache.kafka.clients.producer.internals.Sender)
[2017-05-31 13:31:13,339] WARN Got error produce response with correlation id 6 on topic-partition Topic6-36, retrying (2 attempts left). Error: NOT_LEADER_FOR_PARTITION (org.apache.kafka.clients.producer.internals.Sender)
[2017-05-31 13:31:13,339] WARN Got error produce response with correlation id 6 on topic-partition Topic6-6, retrying (2 attempts left). Error: NOT_LEADER_FOR_PARTITION (org.apache.kafka.clients.producer.internals.Sender)
[2017-05-31 13:31:13,339] WARN Got error produce response with correlation id 6 on topic-partition Topic6-0, retrying (2 attempts left). Error: NOT_LEADER_FOR_PARTITION (org.apache.kafka.clients.producer.internals.Sender)
[2017-05-31 13:31:13,342] WARN Got error produce response with correlation id 6 on topic-partition Topic6-18, retrying (2 attempts left). Error: NOT_LEADER_FOR_PARTITION (org.apache.kafka.clients.producer.internals.Sender)
[2017-05-31 13:31:13,343] WARN Got error produce response with correlation id 6 on topic-partition Topic6-24, retrying (2 attempts left). Error: NOT_LEADER_FOR_PARTITION (org.apache.kafka.clients.producer.internals.Sender)
[2017-05-31 13:31:13,401] WARN Got error produce response with correlation id 11 on topic-partition Topic6-3, retrying (2 attempts left). Error: NOT_LEADER_FOR_PARTITION (org.apache.kafka.clients.producer.internals.Sender)


After 200 secs I see that producer says the total number of messages produced are 61000 while consumed messages are 60974.

Just to make sure whether it is consumer's fault or producer's fault, I run another console consumer on the same topic from the beginning and observer that there were actually 60974 messages in that topic. So that proves that the messages were lost at the producer end.

I also tried the same test with adding the following property to the topic being used:
unclean.leader.election.enable = false

And I also changed the ""leader.imbalance.check.interval.seconds"" in server.properties from 30 secs to 1 sec.

Still the loss of messages persist.

I have posted these issue on confluent-platform(Thread name = Loss of data while Scaling Kafka Brokers) also but have not got any reply yet.
Please tell me how can I completely avoid this loss of messages.
Thanks.;;;","08/Jun/17 00:48;junrao;[~vinayak10], did you set acks=all in the producer?;;;","20/Jun/17 12:01;vinayak10;[~junrao] I tried with acks=all, there is no loss of messages now.

But, what I see now is some messages are produced more than once now.

this is what I did:
I produced using the following command:
bin/kafka-console-producer.sh --broker-list 172.31.15.135:9092,172.31.17.243:9092 --topic Topic --message-send-max-retries 1000 --request-timeout-ms 60000 --request-required-acks ""all"" --max-block-ms 9223372036854775807

I also set min.insync.replicas = 2 in topic and broker configurations.

While reassigning the partitions I found out that number of messages produced were 730000 while consumed were 730012.

Can you tell me where did I go wrong?

Thanks.

;;;","20/Jun/17 14:29;junrao;[~vinayak10], currently, duplicates can be introduced during producer retry on transient failure such as leader changes. In the 0.11.0.0 release, we are introducing an idempotent producer that can avoid duplicates during retry.;;;","21/Jun/17 07:32;vinayak10;[~junrao] I will look forward to the release.
Thanks alot for your replies. It really helped.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdminUtils.deleteTopic does not work,KAFKA-1558,12730057,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriharsha,hgschmie,hgschmie,26/Jul/14 23:49,09/Oct/14 21:54,14/Jul/23 05:39,09/Oct/14 21:54,0.8.1.1,,,0.8.2.0,,,,,,,,,,0,,,,"the AdminUtils:.deleteTopic method is implemented as

{code}
    def deleteTopic(zkClient: ZkClient, topic: String) {
        ZkUtils.createPersistentPath(zkClient, ZkUtils.getDeleteTopicPath(topic))
    }
{code}

but the DeleteTopicCommand actually does

{code}
    zkClient = new ZkClient(zkConnect, 30000, 30000, ZKStringSerializer)
    zkClient.deleteRecursive(ZkUtils.getTopicPath(topic))
{code}

so I guess, that the 'createPersistentPath' above should actually be 


{code}
    def deleteTopic(zkClient: ZkClient, topic: String) {
        ZkUtils.deletePathRecursive(zkClient, ZkUtils.getTopicPath(topic))
    }
{code}
",,chelseaz,dinghaifeng,guozhang,gwenshap,hgschmie,hongyu.bi,junrao,nehanarkhede,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1663,,,,"09/Oct/14 21:13;sriharsha;KAFKA-1558.patch;https://issues.apache.org/jira/secure/attachment/12674003/KAFKA-1558.patch","30/Sep/14 05:24;sriharsha;kafka-thread-dump.log;https://issues.apache.org/jira/secure/attachment/12671963/kafka-thread-dump.log",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,408130,,,Thu Oct 09 21:54:19 UTC 2014,,,,,,,,,,"0|i1y873:",408135,,nehanarkhede,,,,,,,,,,,,,,,,,,"27/Jul/14 17:20;junrao;Delete topic is not supported in 0.8.1.1. It is implemented in trunk, but hasn't been fully tested. The way this works is not to delete the topic path directly. Instead, it first indicates in a separate path that we want to delete a topic. Once all data related to the topic is deleted, the controller will then delete the topic path.;;;","14/Aug/14 17:02;sriharsha;[~junrao] Incase if you are not working on this I would like to start working on this JIRA. Thanks.;;;","18/Aug/14 22:36;junrao;Sriharsha,

Yes, you can take this one. Clark will explain how to reproduce the problem and attach the relevant logs. Thanks,;;;","19/Aug/14 18:39;nehanarkhede;[~clarkhaskins], please can you attach all the server logs here when delete topic fails?;;;","27/Aug/14 15:02;sriharsha;[~clarkhaskins] can you please upload server logs for delete topic failures. Thanks.;;;","04/Sep/14 22:11;guozhang;It would be really great to have this in 0.8.2, but not sure we can make it though..;;;","04/Sep/14 22:55;sriharsha;[~guozhang] I am waiting some logs to show up supposed issue. I can put in sometime this week get this done if someone can give me reproduction steps or logs. Thanks;;;","11/Sep/14 00:50;sriharsha;[~junrao] Do you know any steps on reproducing this. Thanks.;;;","11/Sep/14 01:52;gwenshap;I don't think the issue as described above exists an more. 
I tested the trunk implementation of deleteTopic in simple cases and in all of them, if delete.topic.enable was true, the topic was deleted.

I think what we need now is to test deleteTopic under failure modes - leader election, partition reassignment, etc.;;;","11/Sep/14 20:00;nehanarkhede;bq. I think what we need now is to test deleteTopic under failure modes - leader election, partition reassignment, etc.

Yes, it would help if someone who has cycles can take this up and basically try to break delete topic under various failure scenarios. Currently, the approach to fixing delete topic is a little ad-hoc.;;;","11/Sep/14 20:11;sriharsha;[~gwenshap] Thanks for the info. [~nehanarkhede] I am working on testing those cases mentioned above. Thanks.;;;","11/Sep/14 20:18;nehanarkhede;Great. Thanks [~sriharsha] for taking this on.;;;","14/Sep/14 18:30;junrao;Sriharsha,

It would be useful to test if ""delete topic"" works under the following cases.
1. after the controller is restarted
2. after a soft failure (can simulate by pausing the jvm for longer that zk session timeout) of the controller
3. after a topic's partitions have been reassigned to some other brokers
4. after running a preferred leader command
5. after a topic's partition has been increased

Thanks,;;;","14/Sep/14 18:38;sriharsha;Thanks [~junrao] will test those cases.;;;","15/Sep/14 00:56;nehanarkhede;I'll add one more :)
After the controller broker is killed (kill -9);;;","15/Sep/14 23:40;sriharsha;[~junrao] [~nehanarkhede] I ran tests for above cases manually on a cluster 3 kafka nodes and 3 zookeeper nodes
for each of the tests topics are created and minimum size of the log per partition was > 1Gb

1)  after the controller is restarted
   issuing delete topic is successful , metadata is deleted and also the log file without any errors.

2)  after a soft failure (can simulate by pausing the jvm for longer that zk session timeout) of the controller
   I am not sure how to induce a pause in jvm , I tried with debug tools doesn't look it had any effect. If you have any pointers on this please let me know.

3)  after a topic's partitions have been reassigned to some other brokers
     used kafka-reassign-partitions and ran delete topic command this resulted in successful deleting of metadata and topics log files

4) after running a preferred leader command
     No issues here topic successfully deleted

5) after a topic's partition has been increased
    No issues here either . new partition data also deleted

6) controller broker is killed (kill -9)
  successfully deleted the topic and metadata. Once the killed controller back online the logfiles for that topic also got deleted.

Please let me know on the case 2. If you have any more cases that you would like to test please let me know.
Thanks.;;;","16/Sep/14 00:22;junrao;To simulate a soft failure of a broker, you can

1. kill -SIGSTOP
2. wait for a bit longer than zk session timeout
3. kill -SIGCONT;;;","16/Sep/14 17:50;sriharsha;Thanks [~junrao]
Case 2 works fine too the topic gets deleted.

Case 6 when I do kill -9 on the controller and immediately run delete topic from the same node or on different node
one of the brokers goes on printing these messages

[2014-09-16 16:38:17,607] INFO Reconnect due to socket error: java.nio.channels.ClosedChannelException (kafka.consumer.SimpleConsumer)
[2014-09-16 16:38:17,608] WARN [ReplicaFetcherThread-0-1], Error in fetch Name: FetchRequest; Version: 0; CorrelationId: 955460; ClientId: ReplicaFetcherThre
ad-0-1; ReplicaId: 2; MaxWait: 500 ms; MinBytes: 1 bytes; RequestInfo: [my-topic,1] -> PartitionFetchInfo(2558522,1048576). Possible cause: java.nio.channels
.ClosedChannelException (kafka.server.ReplicaFetcherThread)

I've to restart this broker and delete topic goes fine.
But if I wait after killing(kill -9) controller and fetcher is removed delete topic goes fine without any issues
[ReplicaFetcherManager on broker 3] Removed fetcher for partitions [my-topic,1] (kafka.server.ReplicaFetcherManager)
[2014-09-16 16:56:58,646] INFO [ReplicaFetcherManager on broker 3] Removed fetcher for partitions [my-topic,0] (kafka.server.ReplicaFetcherManager)

I am not sure if its related to delete topic though any ideas on this.

;;;","18/Sep/14 14:11;nehanarkhede;[~sriharsha] Delete topic shouldn't be affected by the replica fetcher issue. Have you let it stay as is and see if delete topic eventually completes? If not, have you taken a thread dump to see where it halts, if at all? 

For each of the tests above, an important scenario is a large number of topics being actively written to and read from. For the tests, it is important to create, say a 1000 topics, and run producers in parallel that write to those 1000 topics, consumers that consume and then introduce each of the failures above. Delete topic should succeed even if there are writes/reads happening to the topic.;;;","18/Sep/14 14:57;sriharsha;[~nehanarkhede] Should we also consider the topic's log size. I'll work on running the tests with 1000 topics and post results today or early tomorrow. Thanks.;;;","21/Sep/14 05:24;sriharsha;[~nehanarkhede] [~junrao] update on the tests.
I ran the above tests in 5 kafka brokers and 3 zookeeper node with 1000 topics with 3 partitions 3 replication factor.
All these topics being written to and read from simultaneously.

1)  after the controller is restarted
topic log files and metadata deleted successfully
2)  after a soft failure (can simulate by pausing the jvm for longer that zk session timeout) of the controller
I am not able to consistently pass this test. I am getting this error
""2014-09-21 01:21:39,101] INFO Partition [my-topic-435,0] on broker 5: Cached zkVersion [114] not equal to that in zookeeper, skip updating ISR (kafka.cluster.Partition)"" 
Initially thought it could be related KAFKA-1382
but from debug logs
""expected Some((Leader:2,ISR:2,LeaderEpoch:4,ControllerEpoch:7)) written Some((Leader:4,ISR:4,LeaderEpoch:5,ControllerEpoch:9)) (kafka.utils.ReplicationUtils$)""
Leader is different. This is causing the broker to keep on logging these messages.
I am not sure why  it didn't get the leader update. I didn't see any errors in zookeeper logs.
I'll get more details on this.
3)  after a topic's partitions have been reassigned to some other brokers
    topic deleted no issues
4) after running a preferred leader command
 topic successfully deleted
5) after a topic's partition has been increased
   topic and new partition data also deleted
6) controller broker is killed (kill -9)
successfully deleted the topic and metadata. Once the killed controller back online the logfiles for that topic partition on that broker also got deleted.
I am not able to reproduce ReplicaFetcher issue here that pointed out earlier. Before I was running tests on vms with low memory . I wasn't able to reproduce this on these nodes.


;;;","21/Sep/14 15:52;nehanarkhede;This is great, [~sriharsha]. Thanks for running these tests. For #3, #4, #5, was the reassignment, leader movement and partition increase running while the delete topic was running? It is worth trying these admin commands in parallel with delete topic. Will wait for more details on #2.
;;;","21/Sep/14 15:57;sriharsha;[~nehanarkhede] for 3,4,5 I ran them after the command is completed I'll try running them parallel.  ;;;","21/Sep/14 19:57;sriharsha;[~nehanarkhede] 3 and 5 test cases went fine with delete command running parallel and for test case 4 the topic gets deleted successfully but preferred leader command fails with
Failed to start preferred replica election
org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/topics/my-topic-693/partitions
        at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47)
which I think is to be expected.
;;;","30/Sep/14 05:22;sriharsha;[~nehanarkhede] [~junrao] Sorry couldn't get to work on this earlier. I spent more time on case 2 I can consistently reproduce this failure. 
1) kill -SIGSTOP pid
2) wait for zookeeper timeout
3) kill -SIGCONT pid
4) immediately delete  a topic
5) new controller keeps printing these messages 
DeleteTopicThread goes in a loop.  I am attaching thread dump from this broker.
[2014-09-30 05:21:39,598] INFO [delete-topics-thread-3], Handling deletion for topics my-topic-110 (kafka.controller.TopicDeletionManager$DeleteTopicsThread)
[2014-09-30 05:21:39,598] DEBUG [Replica state machine on controller 3]: Are all replicas for topic my-topic-110 deleted Map([Topic=my-topic-110,Partition=1,Replica=2] -> ReplicaDeletionSuccessful, [Topic=my-topic-110,Partition=1,Replica=3] -> ReplicaDeletionSuccessful, [Topic=my-topic-110,Partition=0,Replica=1] -> OfflineReplica, [Topic=my-topic-110,Partition=0,Replica=2] -> ReplicaDeletionSuccessful) (kafka.controller.ReplicaStateMachine)
[2014-09-30 05:21:39,599] INFO [delete-topics-thread-3], Not retrying deletion of topic my-topic-110 at this time since it is marked ineligible for deletion (kafka.controller.TopicDeletionManager$DeleteTopicsThread)
[2014-09-30 05:21:39,599] INFO [delete-topics-thread-3], Handling deletion for topics my-topic-110 (kafka.controller.TopicDeletionManager$DeleteTopicsThread)
[2014-09-30 05:21:39,599] DEBUG [Replica state machine on controller 3]: Are all replicas for topic my-topic-110 deleted Map([Topic=my-topic-110,Partition=1,Replica=2] -> ReplicaDeletionSuccessful, [Topic=my-topic-110,Partition=1,Replica=3] -> ReplicaDeletionSuccessful, [Topic=my-topic-110,Partition=0,Replica=1] -> OfflineReplica, [Topic=my-topic-110,Partition=0,Replica=2] -> ReplicaDeletionSuccessful) (kafka.controller.ReplicaStateMachine)
[2014-09-30 05:21:39,599] INFO [delete-topics-thread-3], Not retrying deletion of topic my-topic-110 at this time since it is marked ineligible for deletion (kafka.controller.TopicDeletionManager$DeleteTopicsThread)

topic doesn't get deleted 
;;;","30/Sep/14 16:21;sriharsha;[~nehanarkhede] [~junrao] This seems to be related to the earlier issue I reported https://issues.apache.org/jira/browse/KAFKA-1558?focusedCommentId=14142342&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14142342

possible issue is related to old controller doesn't shutdown properly when a new controller is elected.
I ran a simple test with 1000 topics in 5 broker cluster . This is without consumers or producers running and no delete topic command issued.

After a soft failure old controller log shows
[2014-09-30 15:59:53,398] INFO [SessionExpirationListener on 1], ZK expired; shut down all controller components and try to re-elect (kafka.controller.KafkaController$SessionExpirationListener)
[2014-09-30 15:59:53,400] INFO [delete-topics-thread-1], Shutting down (kafka.controller.TopicDeletionManager$DeleteTopicsThread)

It stops there and the server.log goes on with 
[2014-09-30 16:17:36,649] INFO Partition [my-topic-634,0] on broker 1: Shrinking ISR for partition [my-topic-634,0] from 1,3,4 to 1 (kafka.cluster.Partition)
[2014-09-30 16:17:36,653] INFO Partition [my-topic-634,0] on broker 1: Cached zkVersion [0] not equal to that in zookeeper, skip updating ISR (kafka.cluster.Partition)
[2014-09-30 16:17:36,653] INFO Partition [my-topic-374,1] on broker 1: Shrinking ISR for partition [my-topic-374,1] from 1,2,3 to 1 (kafka.cluster.Partition)
[2014-09-30 16:17:36,656] INFO Partition [my-topic-374,1] on broker 1: Cached zkVersion [0] not equal to that in zookeeper, skip updating ISR (kafka.cluster.Partition)
[2014-09-30 16:17:36,657] INFO Partition [my-topic-549,2] on broker 1: Shrinking ISR for partition [my-topic-549,2] from 1,2,3 to 1 (kafka.cluster.Partition)

I tried reproduce this in a 3 node cluster in vms with 200 topics and with or without producers & consumers running.
But here old controller shutdown goes through fine.

[2014-09-30 14:50:55,193] INFO [SessionExpirationListener on 3], ZK expired; shut down all controller components and try to re-elect (kafka.controller.KafkaController$SessionExpirationListener)
[2014-09-30 14:50:55,196] INFO [delete-topics-thread-3], Shutting down (kafka.controller.TopicDeletionManager$DeleteTopicsThread)
[2014-09-30 14:50:55,200] INFO [delete-topics-thread-3], Stopped  (kafka.controller.TopicDeletionManager$DeleteTopicsThread)
[2014-09-30 14:50:55,200] INFO [delete-topics-thread-3], Shutdown completed (kafka.controller.TopicDeletionManager$DeleteTopicsThread)
[2014-09-30 14:50:55,202] INFO [Partition state machine on Controller 3]: Stopped partition state machine (kafka.controller.PartitionStateMachine)
[2014-09-30 14:50:55,202] INFO [Replica state machine on controller 3]: Stopped replica state machine (kafka.controller.ReplicaStateMachine)
[2014-09-30 14:50:55,202] INFO [Controller-3-to-broker-2-send-thread], Shutting down (kafka.controller.RequestSendThread)
[2014-09-30 14:50:55,202] INFO [Controller-3-to-broker-2-send-thread], Stopped  (kafka.controller.RequestSendThread)
[2014-09-30 14:50:55,202] INFO [Controller-3-to-broker-2-send-thread], Shutdown completed (kafka.controller.RequestSendThread)
[2014-09-30 14:50:55,202] INFO [Controller-3-to-broker-1-send-thread], Shutting down (kafka.controller.RequestSendThread)
[2014-09-30 14:50:55,202] INFO [Controller-3-to-broker-1-send-thread], Stopped  (kafka.controller.RequestSendThread)
[2014-09-30 14:50:55,203] INFO [Controller-3-to-broker-1-send-thread], Shutdown completed (kafka.controller.RequestSendThread)
[2014-09-30 14:50:55,203] INFO [Controller-3-to-broker-3-send-thread], Shutting down (kafka.controller.RequestSendThread)
[2014-09-30 14:50:55,203] INFO [Controller-3-to-broker-3-send-thread], Stopped  (kafka.controller.RequestSendThread)
[2014-09-30 14:50:55,203] INFO [Controller-3-to-broker-3-send-thread], Shutdown completed (kafka.controller.RequestSendThread)


Regarding TopicDeletionManager shouldn't we stop if one of the replicas are offline or atleast have configurable number of retries for topic deletion?



;;;","30/Sep/14 16:51;guozhang;Just wondering if it is caused by KAFKA-1578. Are you testing against trunk?;;;","30/Sep/14 17:02;sriharsha;[~guozhang] yes testing against trunk last updated yesterday.
 ⭠ trunk± ⮀ ~/code/kafka ⮀ 
» git log | grep -i KAFKA-1578 
    kafka-1578; Controller should de-register all listeners upon designation; patched by Guozhang Wang; reviewed by Jun Rao;;;","30/Sep/14 19:34;nehanarkhede;[~sriharsha] Thanks for running more tests. I have a few questions about the latest results you've shared above.
bq.I ran a simple test with 1000 topics in 5 broker cluster . This is without consumers or producers running and no delete topic command issued.

What were you testing here? What didn't work as expected? 
;;;","30/Sep/14 19:53;sriharsha;[~nehanarkhede] I was trying to test if the controller is recovering from  a soft failure successfully and above described behavior is outcome of issuing a delete topic command not necessarily anything to do with controller's ability to come out of soft failure.
Hence I removed the producers/consumers and didn't do delete topic command even than the old controller didn't shutdown properly and I believe this is what causing the delete topic to fail. Since the old controller didn't shutdown topic partition goes to offline which is causing new controller's TopicDeletionManager go into a loop where one of the topic partition replica's are offline and keep retrying to delete it.;;;","01/Oct/14 14:27;nehanarkhede;[~sriharsha] Controller's inability to not shutdown on a soft failure is actually a problem and we need to look into it before fixing that case for delete topic. Would you mind filing another JIRA and stating the steps to reproduce the controller issue and make this depend on that JIRA? Since this JIRA is a blocker for 0.8.2, we are hoping to make progress on it. ;;;","01/Oct/14 14:38;sriharsha;[~nehanarkhede] working on finding the issue behind controller's shutdown. I'll file a JIRA.;;;","03/Oct/14 22:45;guozhang;[~harsha_ch] could we also try one more test case: delete topic while leader rebalance is on-going?;;;","03/Oct/14 22:52;sriharsha;[~guozhang]  I'll work on that test will report the results.  Is there any way to induce the leader rebalance manually?;;;","04/Oct/14 00:36;nehanarkhede;[~sriharsha] Yes. The preferred replica election admin tool can be used to balance the leaders. You would have to have imbalanced leaders before that, though. A broker bounce is the easiest way to end up with leader imbalance.;;;","04/Oct/14 00:43;sriharsha;[~nehanarkhede] [~guozhang] I ran the preferred replica election tool in my tests. But I'll try bouncing the broker and run this tool.;;;","05/Oct/14 01:16;nehanarkhede;[~sriharsha] Now that KAFKA-1663 is pushed to trunk, the previously failing test should now pass right? The only other thing to wait on would be the leader rebalance tests. [~clarkhaskins], [~guozhang]: At this point, many failure tests pass on delete topic. It will be great to try delete topic on 100s of topics on some of LinkedIn's clusters, if possible. Any chance this test can happen in the 0.8.2 timeframe (1-2 weeks) ?;;;","05/Oct/14 01:26;sriharsha;[~nehanarkhede] I ran all the tests except leader rebalance tests with KAFKA-1663 all of them are passed in my tests. With 1000 topics set to partition 3 and replication factor of 3 and producers, consumers running on all of the topics. I am planning on doing the leader rebalance tests tonight will update the results. It will be great to do this testing on LinkedIn's cluster too :).;;;","09/Oct/14 16:51;sriharsha;[~nehanarkhede] [~guozhang] [~junrao] I ran tests for simultaneously running  preferred replica election tool and deleting multiple topics.
I kept running into KAFKA-1305 but by increasing controller.message.queue.size to 1000 I was able to run these tests successfully.
While testing this couple of things caught my eye.
following code in KafkaController.PreferredReplicaElectionListener
{code}
      val partitions = partitionsForPreferredReplicaElection -- controllerContext.partitionsUndergoingPreferredReplicaElection
      val partitionsForTopicsToBeDeleted = partitions.filter(p => controller.deleteTopicManager.isTopicQueuedUpForDeletion(p.topic))
      if(partitionsForTopicsToBeDeleted.size > 0) {
        error(""Skipping preferred replica election for partitions %s since the respective topics are being deleted""
          .format(partitionsForTopicsToBeDeleted))
      }
      else
        controller.onPreferredReplicaElection(partitions -- partitionsForTopicsToBeDeleted)
    }
{code}
It doesn't need a else part there since its calling onPreferredReplicaElection by removing partitionsForTopicsToBeDeleted

In PartitionStateMachine.DeleteTopicListener
{code}
        if(topicsToBeDeleted.size > 0) {
          info(""Starting topic deletion for topics "" + topicsToBeDeleted.mkString("",""))
          // add topic to deletion list
          controller.deleteTopicManager.enqueueTopicsForDeletion(topicsToBeDeleted)
          // mark topic ineligible for deletion if other state changes are in progress
          topicsToBeDeleted.foreach { topic =>
            val preferredReplicaElectionInProgress =
              controllerContext.partitionsUndergoingPreferredReplicaElection.map(_.topic).contains(topic)
            val partitionReassignmentInProgress =
              controllerContext.partitionsBeingReassigned.keySet.map(_.topic).contains(topic)
            if(preferredReplicaElectionInProgress || partitionReassignmentInProgress)
              controller.deleteTopicManager.markTopicIneligibleForDeletion(Set(topic))
          }
        }
{code}

The above code  enqueueTopicsForDeletion which calls resumeTopicDeletionThread() to start the deletion of topics 
mark topic ineligible should be before the enqueueTopicsForDeletion. This way deletion of the topic won't happen if there is preferred replica election or partitions reassignment going for the said topics. I am testing these changes. Let me know what you think of these changes. Thanks.
;;;","09/Oct/14 17:11;junrao;Sriharsha,

I think you are right on both points. Thanks for finding them out.;;;","09/Oct/14 17:40;nehanarkhede;[~sriharsha] Those observations are right, would you like to submit a patch?

bq. I kept running into KAFKA-1305 but by increasing controller.message.queue.size to 1000 I was able to run these tests successfully.

Thanks for running the tests. Did you get a chance to test it by making the queue unbounded?;;;","09/Oct/14 19:26;sriharsha;[~nehanarkhede] I am running above tests with the proposed changes. Will submit a patch shortly on this JIRA.
""Thanks for running the tests. Did you get a chance to test it by making the queue unbounded?""
I can set controller.message.queue.size to Int.MaxValue and run tests. ;;;","09/Oct/14 21:13;sriharsha;Created reviewboard https://reviews.apache.org/r/26521/diff/
 against branch origin/trunk;;;","09/Oct/14 21:15;sriharsha;[~junrao] [~nehanarkhede] re-ran all of the above tests with this patch. Tests passed.
I ran it with controller.message.queue.size set to 1000 and also Int.MaxValue which is what LinkedBlockingQueue does for unbounded queue.;;;","09/Oct/14 21:54;junrao;Thanks for the patch. +1. Committed to both trunk and 0.8.2.

Sriharsha,

Thanks a lot for helping out.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Quickstart refers to wrong port in pastable commands,KAFKA-1552,12728410,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,olleolleolle,olleolleolle,olleolleolle,19/Jul/14 22:20,20/Jul/14 18:06,14/Jul/23 05:39,20/Jul/14 18:06,,,,,,,,,,,website,,,0,documentation,quickstart,,The file http://svn.apache.org/repos/asf/kafka/site/081/quickstart.html mentions  the port localhost:218192 which should be localhost:2181.,,jkreps,olleolleolle,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/14 22:24;olleolleolle;quickstart.patch;https://issues.apache.org/jira/secure/attachment/12656761/quickstart.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,406486,,,Sun Jul 20 18:06:32 UTC 2014,,,,,,,,,,"0|i1xy73:",406506,,,,,,,,,,,,,,,,,,,,"20/Jul/14 18:06;jkreps;Fixed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuration example errors,KAFKA-1551,12728368,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,aozeritsky,aozeritsky,aozeritsky,19/Jul/14 10:39,20/Jul/14 18:07,14/Jul/23 05:39,20/Jul/14 18:07,,,,,,,,,,,website,,,1,,,,"A Production Server Config (http://kafka.apache.org/documentation.html#prodconfig) contains error:
{code}
# ZK configuration
zk.connection.timeout.ms=6000
zk.sync.time.ms=2000
{code}
Should be
{code}
# ZK configuration
zookeeper.connection.timeout.ms=6000
zookeeper.sync.time.ms=2000
{code}
",,aozeritsky,jkreps,ovgolovin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,406444,,,Sun Jul 20 18:07:06 UTC 2014,,,,,,,,,,"0|i1xxxr:",406464,,,,,,,,,,,,,,,,,,,,"20/Jul/14 18:07;jkreps;Fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Patch review tool should use git format-patch to generate patch,KAFKA-1550,12728266,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,lindong,lindong,lindong,18/Jul/14 19:58,20/Aug/14 00:48,14/Jul/23 05:39,05/Aug/14 21:50,,,,,,,,,,,,,18/Jul/14 00:00,0,,,,"1) kafka-patch-review.py uses git-diff to generate patch for jira ticket. The resulting patch includes local uncommitted changes and also maybe different from the patch published to reviewboard.
2) kafka-patch-review.py will still update the jira ticket with the patch even if the reviewboard is not updated.",,jjkoshy,lindong,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/14 20:58;lindong;KAFKA-1550.patch;https://issues.apache.org/jira/secure/attachment/12656570/KAFKA-1550.patch","22/Jul/14 18:42;lindong;KAFKA-1550_2014-07-22_11:41:57.patch;https://issues.apache.org/jira/secure/attachment/12657157/KAFKA-1550_2014-07-22_11%3A41%3A57.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,406371,,,Tue Aug 05 21:50:01 UTC 2014,,,,,,,,,,"0|i1xxhj:",406391,,,,,,,,,,,,,,,,,,,,"18/Jul/14 20:58;lindong;Created reviewboard https://reviews.apache.org/r/23692/diff/
 against branch origin/trunk;;;","22/Jul/14 18:42;lindong;Updated reviewboard https://reviews.apache.org/r/23692/diff/
 against branch origin/transactional_messaging;;;","05/Aug/14 21:50;jjkoshy;Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dead brokers coming in the TopicMetadataResponse,KAFKA-1549,12728242,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nmarasoi,nmarasoi,nmarasoi,18/Jul/14 17:14,27/Jul/14 17:00,14/Jul/23 05:39,27/Jul/14 17:00,0.8.2.0,,,0.8.2.0,,,,,,,,,,0,,,,"JunRao confirming my observation that brokers are only added to the metadataCache, never removed: ""The way that we update liveBrokers in MetadataCache.updateCache() doesn't seem right. We only add newly received live brokers to the list. However, there could be existing brokers in that list that are now dead. Those dead brokers shouldn't be returned to the clients. We should probably just take the new live broker list and cache it.""",trunk,junrao,nmarasoi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/14 19:44;nmarasoi;bringAllBrokers.patch;https://issues.apache.org/jira/secure/attachment/12658006/bringAllBrokers.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,406347,,,Sun Jul 27 17:00:53 UTC 2014,,,,,,,,,,"0|i1xxcn:",406368,,,,,,,,,,,,,,,,,,,,"18/Jul/14 17:15;nmarasoi;test if i receive mails on this jira task;;;","18/Jul/14 19:44;nmarasoi;essentially i will replace the map of brokers every update. this will allow for the structure to be immutable likely. however the code does not depend on it and toMap is called on the read side as well, to convert to immutable if needed in the read locked window;;;","19/Jul/14 11:39;nmarasoi;i created a fix and a tiny refactoring in metadataCache to reuse the read locking snippet and make it available outside to allow atomical and lighter lock use across method calls;;;","21/Jul/14 03:43;junrao;Thanks for the patch. A couple of comments.

1. inReadLock seems like a generally useful util. We probably can add the following to the Utils file where inLock lives, and change all existing usage of readWriteLock to use those utils.
  def inReadLock[T](lock: ReadWriteLock)(fun: => T): T = {
  def inWriteLock[T](lock: ReadWriteLock)(fun: => T): T = {

2. Not sure what the purpose of the change in KafkaApis is. Is that to avoid the overhead of an extra readLock? It's better to limit the usage of the lock inside the MetadataCache.
;;;","21/Jul/14 12:07;nmarasoi;Implemented point 1 and reverted for point 2, attached patch.
I used implicit parameter for the rwLock, so it required to make the lock the second argument-list to compile. I think in most cases there will be a single implicit read write lock reference available.

def inReadLock[T](fun: => T)(lock: ReadWriteLock): T = {..

2. Indeed, to avoid that overhead and make it atomical (i.e. brokers and topic metadata from the same snapshot of metadata). However the atomicity is not required by the functionality.
I understand that this locking encapsulation useful to make it safe and easy to reason on the multithreading behavior, that is the reason right?
;;;","21/Jul/14 20:02;nmarasoi;added an implementation without the implicit: this required a pair of function definitions in each client class (to my current scala knowleadge);;;","22/Jul/14 19:27;nmarasoi;[~jkreps] [~junrao] Hi, can you help me choose from the patches? Thank you, Nicu;;;","22/Jul/14 21:30;junrao;Thanks for the patch. Could we define inReadLock and inWriteLock in the same way as inLock? Since we have code like

inlock(lock) {
 ...
}

It would be consistent if we can just do 

inReadLock(readWriteLock) {
 ...
};;;","26/Jul/14 19:44;nmarasoi;Agreed, makes sense - attached patch.
I wonder why I did not receive an email on gmail with your comment, since I am subscribed (watching) this issue, do you have any idea why?;;;","27/Jul/14 17:00;junrao;Thanks for the patch. +1 and committed to trunk.

Hmm, I am not why you didn't get the email notification. Did you just miss this particular one or you never got any notification?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogCleaner may take a long time to shutdown,KAFKA-1544,12727943,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,omkreddy,junrao,junrao,17/Jul/14 14:47,24/Jul/14 23:35,14/Jul/23 05:39,24/Jul/14 23:35,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,newbie,,,"We have the following code in LogCleaner. Since the cleaner thread is shutdown w/o interrupt. If may take up to backoff time for the cleaner thread to detect the shutdown flag.

    private def cleanOrSleep() {
      cleanerManager.grabFilthiestLog() match {
        case None =>
          // there are no cleanable logs, sleep a while
          time.sleep(config.backOffMs)
 ",,jkreps,junrao,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/14 16:45;omkreddy;KAFKA-1544.patch;https://issues.apache.org/jira/secure/attachment/12657375/KAFKA-1544.patch","24/Jul/14 15:11;omkreddy;KAFKA-1544_2014-07-24_20:38:52.patch;https://issues.apache.org/jira/secure/attachment/12657616/KAFKA-1544_2014-07-24_20%3A38%3A52.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,406048,,,Thu Jul 24 23:35:26 UTC 2014,,,,,,,,,,"0|i1xviv:",406068,,,,,,,,,,,,,,,,,,,,"17/Jul/14 14:59;junrao;One solution is to simply put the sleep in a loop. In each iteration, we sleep for a small amount of time, say 500ms. The loop finishes if the thread is not runnable or if the backoff time is reached.;;;","23/Jul/14 16:45;omkreddy;Created reviewboard https://reviews.apache.org/r/23858/diff/
 against branch origin/trunk;;;","24/Jul/14 15:11;omkreddy;Updated reviewboard https://reviews.apache.org/r/23858/diff/
 against branch origin/trunk;;;","24/Jul/14 23:35;jkreps;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
normal IOException in the new producer is logged as ERROR,KAFKA-1542,12727781,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,heavydawson,junrao,junrao,16/Jul/14 21:38,29/Jul/14 14:53,14/Jul/23 05:39,27/Jul/14 17:56,0.8.2.0,,,0.8.2.0,,,,,,,,,,0,newbie,,,"Saw the following error in the log. It seems this can happen if the broker is down. So, this probably should be logged as WARN, instead ERROR.

2014/07/16 00:12:51.799 [Selector] Error in I/O: 
java.io.IOException: Connection timed out
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
        at sun.nio.ch.IOUtil.read(IOUtil.java:197)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
        at org.apache.kafka.common.network.NetworkReceive.readFrom(NetworkReceive.java:60)
        at org.apache.kafka.common.network.Selector.poll(Selector.java:241)
        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:171)
        at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:174)
        at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:114)
        at java.lang.Thread.run(Thread.java:744)",,guozhang,heavydawson,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/14 07:06;heavydawson;KAFKA-1542.patch;https://issues.apache.org/jira/secure/attachment/12657788/KAFKA-1542.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,405886,,,Tue Jul 29 14:53:49 UTC 2014,,,,,,,,,,"0|i1xuj3:",405906,,,,,,,,,,,,,,,,,,,,"16/Jul/14 22:24;junrao;Also, it would be useful to log the remote ip that caused the IOException.;;;","25/Jul/14 07:06;heavydawson;Attaching patch;;;","25/Jul/14 15:26;guozhang;Thanks for the patch. LGTM.;;;","25/Jul/14 17:00;junrao;Thanks for the patch. Perhaps we can just log the inetaddress. Getting the hostname on the client may not always be possible.;;;","25/Jul/14 17:01;heavydawson;Hey Jun, the current patch returns the IP address. getHostAddress() returns the address as a string, whereas getHostName() would be used if we wanted the hostname;;;","27/Jul/14 17:56;junrao;Thanks for the patch. Committed to trunk.;;;","28/Jul/14 17:59;junrao;This change introduced a bug. Saw the following when running ProducerFailureHandlingTest. The problem is that the remote InetAdress may not always be available (e.g., in connecting mode). Committed a followup patch.

    java.lang.NullPointerException
    	at org.apache.kafka.common.network.Selector.poll(Selector.java:265)
    	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:178)
    	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:175)
    	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:115)
    	at java.lang.Thread.run(Thread.java:695);;;","29/Jul/14 08:19;heavydawson;Hey Jun, the patch looks fine except it's missing the call to {{getHostAddress()}} in the logger. That's need to convert the address object to a string representation for output.;;;","29/Jul/14 14:08;junrao;I think InetAddress.toString gives what we want.;;;","29/Jul/14 14:13;heavydawson;Strictly speaking, toString can return either the hostname and the ipaddress [http://docs.oracle.com/javase/1.5.0/docs/api/java/net/InetAddress.html#toString()], whereas getHostAddress will always be just the IP address. That said, I defer to you guys on this. It was you who request the host info, so happy to run with your suggestion.

;;;","29/Jul/14 14:53;junrao;Yes, I realized that the InetAddress can be null. So, instead of doing another null check, it's simpler to just print out itself.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Due to OS caching Kafka might loose offset files which causes full reset of data,KAFKA-1539,12727045,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,dmitrybugaychenko,dmitrybugaychenko,14/Jul/14 07:38,22/Jul/14 14:21,14/Jul/23 05:39,21/Jul/14 23:57,0.8.1.1,,,0.8.2.0,,,,,,,log,,,0,,,,"Seen this while testing power failure and disk failures. Due to chaching on OS level (eg. XFS can cache data for 30 seconds) after failure we got offset files of zero length. This dramatically slows down broker startup (it have to re-check all segments) and if high watermark offsets lost it simply erases all data and start recovering from other brokers (looks funny - first spending 2-3 hours re-checking logs and then deleting them all due to missing high watermark).

Proposal: introduce offset files rotation. Keep two version of offset file, write to oldest, read from the newest valid. In this case we would be able to configure offset checkpoint time in a way that at least one file is alway flushed and valid.",,aozeritsky,dmitrybugaychenko,jkreps,joestein,junrao,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/14 17:24;jkreps;KAFKA-1539.patch;https://issues.apache.org/jira/secure/attachment/12656897/KAFKA-1539.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,405152,,,Tue Jul 22 13:58:16 UTC 2014,,,,,,,,,,"0|i1xq53:",405187,,,,,,,,,,,,,,,,,,,,"14/Jul/14 07:49;joestein;Did you have the log.flush.interval.messages == 1 when doing this?  If not then you can either do something like that and sacrifice performance and futz with a single broker flush or have (instead) replicas/brokers outside of zones that are sharing power grids for a partition you are working with.  Use replication to achieve your durability with less sacrifice to performance using more than one broker. If you need/want something within a single broker there are lots of toggle to use in the broker configuration https://kafka.apache.org/documentation.html#brokerconfigs. ;;;","14/Jul/14 08:58;dmitrybugaychenko;This is not about log files themselves, but about chekpoint offset files 

{code}
-rw-r--r--  1 root root   158 Jul 14 12:11 recovery-point-offset-checkpoint
-rw-r--r--  1 root root   163 Jul 14 12:11 replication-offset-checkpoint
-rw-r--r--  1 root root     0 May 28 13:09 cleaner-offset-checkpoint
{code}

If recovery-point-offset-checkpoint got corrupted, broker startup slows down dramatically (to hours), if replication-offset-checkpoint got corrupted, then broker removes all the data it has and starts recovering from other replicas. If both got corrupted then you get both - broker spending hours checking log segment files and then removeing them all.
;;;","15/Jul/14 03:23;junrao;Hmm, interesting. The way that we checkpoint those offset files is to first write to a temp file, force a flush, and then do a rename. This is very close to the two versions of the offset file that you are proposing. Not sure how a zero length file is introduced. Is that easily reproducible?;;;","15/Jul/14 06:05;dmitrybugaychenko;It looks like even after flush data are not necesary written to HDD. In XFS by default it could be cached up to 30 secodns, it also can be cached by a disk controller and etc. Wrtiting to temp file is a good idea, but it is better to keep the previous file untouched (do not replace it with the temp one).

On a 20 HDD server with XFS it is pretty easy to reproduce - after power failure we got corrupted offset files on 4-5 disks.;;;","16/Jul/14 01:36;junrao;If flush is not guaranteed, will keeping two versions of the file help? At some point, we will have flushed both versions and neither one is guaranteed to persist.;;;","20/Jul/14 18:29;dmitrybugaychenko;Digged the problem a bit more. It looks like calling flush on new BufferedWriter(new FileWriter(temp)) only forces buffered writer to dump everything into a FileOutputStream under the FileWriter and call flush on it. However, according to http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7u40-b43/java/io/FileOutputStream.java#FileOutputStream it does nothing. In order to really force data to be written to disk you need to call fos.getFD().sync(). According to that the patch could be like that:

{code}
  def write(offsets: Map[TopicAndPartition, Long]) {
    lock synchronized {
      // write to temp file and then swap with the existing file
      val temp = new File(file.getAbsolutePath + "".tmp"")

      val fileOutputStream = new FileOutputStream(temp)
      val writer = new BufferedWriter(new FileWriter(fileOutputStream))
      try {
        // write the current version
        writer.write(0.toString)
        writer.newLine()
      
        // write the number of entries
        writer.write(offsets.size.toString)
        writer.newLine()

        // write the entries
        offsets.foreach { case (topicPart, offset) =>
          writer.write(""%s %d %d"".format(topicPart.topic, topicPart.partition, offset))
          writer.newLine()
        }
      
        // flush and overwrite old file
        writer.flush()
        
        // Force fsync to disk
        fileOutputStream.getFD.sync()
      } finally {
        writer.close()
      }
      
      // swap new offset checkpoint file with previous one
      if(!temp.renameTo(file)) {
        // renameTo() fails on Windows if the destination file exists.
        file.delete()
        if(!temp.renameTo(file))
          throw new IOException(""File rename from %s to %s failed."".format(temp.getAbsolutePath, file.getAbsolutePath))
      }
    }
  }
{code}

Note that the problem is easily reproducable only on XFS, ext3/ext4 seems to handle this case much better. Hope we will be able to try the patch later this week and check if it helps.;;;","21/Jul/14 17:24;jkreps;Created reviewboard https://reviews.apache.org/r/23743/
 against branch trunk;;;","21/Jul/14 17:25;jkreps;This is a really good catch, were clearly thinking flush() meant fsync, which is totally wrong. I uploaded a patch with your fix. If you are doing testing with this let me know that this actually fixes the issue you saw.;;;","21/Jul/14 17:29;sriramsub;I had encountered the same issue in another project and had to explicitly use fsync to fix it.;;;","21/Jul/14 23:57;jkreps;I'm checking this in since it seems to fix a clear problem, but [~arhagnel] it would still be good to get confirmation that the problem you were producing is fixed by this.;;;","22/Jul/14 06:32;dmitrybugaychenko;Going to test power failure again later today, I'll get back with results as soon as we get them.;;;","22/Jul/14 13:58;dmitrybugaychenko;With fileOutputStream.getFD.sync() patch we passed the power failure tests without loosing offset files. So, it seems to work.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Change the status of the JIRA to ""Patch Available"" in the kafka-review-tool",KAFKA-1536,12726838,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,omkreddy,guozhang,guozhang,11/Jul/14 17:38,17/May/16 14:31,14/Jul/23 05:39,15/Aug/14 00:41,,,,,,,,,,,,,,0,,,,"When using the kafka-review-tool to upload a patch to certain jira, the status remains ""OPEN"". It makes searching for JIRAs that needs review a bit hard. Would be better to make the tool also change the status of the jira.",,guozhang,nehanarkhede,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/14 16:56;nehanarkhede;KAFKA-1536.patch;https://issues.apache.org/jira/secure/attachment/12656294/KAFKA-1536.patch","13/Jul/14 01:39;guozhang;KAFKA-1536.patch;https://issues.apache.org/jira/secure/attachment/12655430/KAFKA-1536.patch","06/Aug/14 14:18;omkreddy;KAFKA-1536_2014-08-06_19:44:49.patch;https://issues.apache.org/jira/secure/attachment/12660143/KAFKA-1536_2014-08-06_19%3A44%3A49.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,404945,,,Fri Aug 15 04:46:56 UTC 2014,,,,,,,,,,"0|i1xow7:",404983,,,,,,,,,,,,,,,,,,,,"12/Jul/14 13:52;omkreddy;Created reviewboard https://reviews.apache.org/r/23440/diff/
 against branch origin/trunk;;;","12/Jul/14 13:57;omkreddy;kafka-patch-review.py script updated to set the JIRA status to PATCH AVAILABLE .
Script also updates the assignee field.
;;;","13/Jul/14 01:39;guozhang;Created reviewboard https://reviews.apache.org/r/23444/
 against branch origin/trunk;;;","17/Jul/14 16:51;nehanarkhede;Assigning to myself for review;;;","17/Jul/14 16:56;nehanarkhede;For testing the patch. 

Created reviewboard https://reviews.apache.org/r/23641/
 against branch trunk;;;","17/Jul/14 16:57;nehanarkhede;[~omkreddy] Thanks a lot for improving our tools. Seems to work. Left a minor comment on the rb. Once you address that, I'll check it in.;;;","18/Jul/14 15:08;omkreddy;Updated reviewboard https://reviews.apache.org/r/23440/diff/
 against branch origin/trunk;;;","18/Jul/14 15:12;omkreddy;for testing this patch;;;","18/Jul/14 15:13;omkreddy;Updated reviewboard https://reviews.apache.org/r/23440/diff/
 against branch origin/trunk;;;","06/Aug/14 14:18;omkreddy;Updated reviewboard https://reviews.apache.org/r/23440/diff/
 against branch origin/trunk;;;","06/Aug/14 14:22;omkreddy;[~nehanarkhede] can we check-in this patch?;;;","15/Aug/14 00:41;nehanarkhede;[~omkreddy] Thanks for contributing this patch! Sorry for losing track of it. Trying to find a better way of organizing patch reviews, hoping to do better. +1 and committed to trunk;;;","15/Aug/14 04:19;omkreddy;Thanks for the commit.  looks like commit message is given wrong.

commit a552d4b743c44db0adcb58273b92301b2a4df38b
Author: Manikumar Reddy <manikumar.reddy@gmail.com>
Date:   Wed Aug 13 13:08:57 2014 -0700

    kafka-1567; Metric memory leaking after closing the clients; patched by Jiangjie Qin; reviewed by Guozhang Wang and Jun Rao
;;;","15/Aug/14 04:46;nehanarkhede;[~omkreddy] Wups, my bad. I tried following several blog posts on suggestions of fixing git history, but none worked. Not much of a git expert. Let me know if you can think of a way to fix this, would be happy to help out. Sorry.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient unit test failure in ProducerFailureHandlingTest,KAFKA-1533,12726624,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,junrao,junrao,10/Jul/14 16:35,10/Sep/14 00:43,14/Jul/23 05:39,10/Sep/14 00:43,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"Occasionally, saw the test hang on tear down. The following is the stack trace.

""Test worker"" prio=5 tid=7f9246956000 nid=0x10e078000 in Object.wait() [10e075000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <7f4e69578> (a org.apache.zookeeper.ClientCnxn$Packet)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1344)
        - locked <7f4e69578> (a org.apache.zookeeper.ClientCnxn$Packet)
        at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:732)
        at org.I0Itec.zkclient.ZkConnection.delete(ZkConnection.java:91)
        at org.I0Itec.zkclient.ZkClient$8.call(ZkClient.java:720)
        at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)
        at org.I0Itec.zkclient.ZkClient.delete(ZkClient.java:716)
        at kafka.utils.ZkUtils$.deletePath(ZkUtils.scala:416)
        at kafka.utils.ZkUtils$.deregisterBrokerInZk(ZkUtils.scala:184)
        at kafka.server.KafkaHealthcheck.shutdown(KafkaHealthcheck.scala:50)
        at kafka.server.KafkaServer$$anonfun$shutdown$2.apply$mcV$sp(KafkaServer.scala:243)
        at kafka.utils.Utils$.swallow(Utils.scala:172)
        at kafka.utils.Logging$class.swallowWarn(Logging.scala:92)
        at kafka.utils.Utils$.swallowWarn(Utils.scala:45)
        at kafka.utils.Logging$class.swallow(Logging.scala:94)
        at kafka.utils.Utils$.swallow(Utils.scala:45)
        at kafka.server.KafkaServer.shutdown(KafkaServer.scala:243)
        at kafka.api.ProducerFailureHandlingTest.tearDown(ProducerFailureHandlingTest.scala:90)
",,copester,guozhang,heavydawson,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/14 22:45;guozhang;KAFKA-1533.patch;https://issues.apache.org/jira/secure/attachment/12656600/KAFKA-1533.patch","16/Jul/14 23:38;guozhang;KAFKA-1533.patch;https://issues.apache.org/jira/secure/attachment/12656179/KAFKA-1533.patch","15/Jul/14 20:40;guozhang;KAFKA-1533.patch;https://issues.apache.org/jira/secure/attachment/12655857/KAFKA-1533.patch","21/Jul/14 22:46;guozhang;KAFKA-1533_2014-07-21_15:45:58.patch;https://issues.apache.org/jira/secure/attachment/12656981/KAFKA-1533_2014-07-21_15%3A45%3A58.patch","28/Jul/14 08:35;heavydawson;kafka.threads;https://issues.apache.org/jira/secure/attachment/12658107/kafka.threads","27/Jul/14 17:48;junrao;stack.out;https://issues.apache.org/jira/secure/attachment/12658029/stack.out",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,404731,,,Wed Sep 10 00:43:00 UTC 2014,,,,,,,,,,"0|i1xnkv:",404769,,,,,,,,,,,,,,,,,,,,"10/Jul/14 17:38;guozhang;This error seems not specific to this test case, since it is blocked on submitting the write/delete request to ZK upon shutting down the server, which we do in all test cases.;;;","11/Jul/14 14:53;junrao;Yes, I am not sure what the issue is. Occasionally, I do see the test ran out of memory and this could be the effect of that.;;;","11/Jul/14 17:13;guozhang;Maybe memory usage can be reduced, which test case caused this failure, or all of them in the ProducerFailureHandlingTest has the potential?;;;","11/Jul/14 18:28;junrao;Not sure. The test itself is done and all left is the teardown part.;;;","11/Jul/14 18:33;guozhang;I think at least one thing we can do to improve this test class is to use the KafkaServerHarness instead of writing the setup/teardown code separately as [~jkreps] suggested, and probably further reduce the process length of the testBrokerFailure case. Will try to upload a patch for that.

Guozhang;;;","15/Jul/14 20:40;guozhang;Created reviewboard https://reviews.apache.org/r/23521/
 against branch origin/trunk;;;","16/Jul/14 23:38;guozhang;Created reviewboard https://reviews.apache.org/r/23593/
 against branch origin/trunk;;;","18/Jul/14 22:45;guozhang;Created reviewboard https://reviews.apache.org/r/23697/
 against branch origin/trunk;;;","21/Jul/14 22:46;guozhang;Updated reviewboard https://reviews.apache.org/r/23697/
 against branch origin/trunk;;;","22/Jul/14 21:15;junrao;Thanks for the patch. +1 and committed to trunk.;;;","27/Jul/14 17:48;junrao;I saw this test hang again. Attached is the stacktrace. ""daemon-producer"" seems to hang during shutdown. Not sure why though.;;;","28/Jul/14 08:35;heavydawson;Seeing the same issue. Jun, not sure if you attached your own thread dump or a copy of mine from the mailing list, but attaching here again per your request.;;;","28/Jul/14 18:01;junrao;This seems to be introduced by a bug in KAFKA-1542. I just committed a fix. After that, I don't see the unit test hanging issue any more.

David,

Could you try the test again?
;;;","29/Jul/14 08:17;heavydawson;Hey Jun, I can confirm the test is now passing. However your patch isn't converting the InetAddress to a string representation of the IP. It just needs to be updated to use:
{{getInetAddress().getHostAddress()}}
;;;","04/Sep/14 21:36;guozhang;Hi [~junrao] do you have time to fix this minor thing before 0.8.2 release?;;;","10/Sep/14 00:43;junrao;Yes, logging the InetAddress is fine. To get the remoteAddress, we will have to check if the InetAddress is not null, which makes the code a bit more complicated. So, we can resolve this issue now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
zookeeper.connection.timeout.ms is set to 10000000 in configuration file in Kafka tarball,KAFKA-1531,12726272,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,omkreddy,michalm,michalm,09/Jul/14 09:00,16/Jul/14 15:13,14/Jul/23 05:39,16/Jul/14 15:13,0.8.0,0.8.1.1,,0.8.2.0,,,,,,,config,,,0,,,,"I've noticed that Kafka tarball comes with zookeeper.connection.timeout.ms=1000000 in server.properties while https://kafka.apache.org/08/documentation.html says the default is 6000. This setting was introduced in configuration file in 46b6144a, so quite a long time ago (3 years), which makes it look intentional, but as per Jun Rao's comment on IRC, 6000 sounds more reasonable, so that entry should probably be changed or removed from config at all.",,guozhang,junrao,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/14 17:33;omkreddy;KAFKA-1531.patch;https://issues.apache.org/jira/secure/attachment/12655251/KAFKA-1531.patch","10/Jul/14 17:16;omkreddy;KAFKA-1531.patch;https://issues.apache.org/jira/secure/attachment/12655032/KAFKA-1531.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,404379,,,Wed Jul 16 15:13:36 UTC 2014,,,,,,,,,,"0|i1xlfb:",404418,,,,,,,,,,,,,,,,,,,,"10/Jul/14 16:19;omkreddy;Can I submit a patch by setting all the occurrences of zookeeper.connection.timeout.ms to 6000?;;;","10/Jul/14 16:47;junrao;Yes, I think so. Thanks,;;;","10/Jul/14 17:16;omkreddy;Created reviewboard https://reviews.apache.org/r/23396/diff/
 against branch origin/trunk;;;","10/Jul/14 18:21;michalm;[~junrao] [~omkreddy] If it's a default value, should we set it in config at all? My idea would be rather to remove that line completely.;;;","10/Jul/14 22:48;guozhang;Agreed, I think we should only specify required configs and other configs that we do need to override the default in those config property files.;;;","11/Jul/14 08:39;omkreddy;I feel some of the important tunable parameters can be part of config files with default values. 

The following tunable properties are currently available in server.properties config file. 

num.network.threads=2 
num.io.threads=8

socket.send.buffer.bytes=1048576
socket.receive.buffer.bytes=1048576
socket.request.max.bytes=104857600
num.partitions=2

log.retention.hours=168
log.segment.bytes=536870912
log.retention.check.interval.ms=60000
log.cleaner.enable=false

All the above properties have some defined default value. 
Some of the above property values are different from the default values
given in documentation.

Can we clean some of the above properties also?
;;;","11/Jul/14 15:10;junrao;Yes, it would be good to clean those up too. We can just set all those values with the defaults.;;;","11/Jul/14 17:33;omkreddy;Created reviewboard https://reviews.apache.org/r/23417/diff/
 against branch origin/trunk;;;","11/Jul/14 17:35;omkreddy;The following properties  updated in server.properties.  

zookeeper.connection.timeout.ms=6000
num.network.threads=3
num.partitions=1
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=65536
log.segment.bytes=1073741824
log.retention.check.interval.ms=300000
;;;","16/Jul/14 14:24;junrao;Thanks for the patch. Do you want to include the changes you made in the previous RB?;;;","16/Jul/14 14:34;omkreddy;Previous patch I tried to update all the conf files across all the folders config, various test_suite etc..

Latest patch contains changes related to config/server.properties and config/consumer.properties only. 
I think latest changes are sufficient for preparing kafka tar release.;;;","16/Jul/14 15:13;junrao;Thanks for the patch. Committed the 2nd RB to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient unit test failure in testAutoCreateAfterDeleteTopic,KAFKA-1529,12725639,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,06/Jul/14 18:35,15/Jul/14 21:53,14/Jul/23 05:39,15/Jul/14 21:53,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"Saw the following transient failure.

kafka.admin.DeleteTopicTest > testAutoCreateAfterDeleteTopic FAILED
    org.scalatest.junit.JUnitTestFailedError: Topic should have been auto created
        at org.scalatest.junit.AssertionsForJUnit$class.newAssertionFailedException(AssertionsForJUnit.scala:101)
        at org.scalatest.junit.JUnit3Suite.newAssertionFailedException(JUnit3Suite.scala:149)
        at org.scalatest.Assertions$class.fail(Assertions.scala:711)
        at org.scalatest.junit.JUnit3Suite.fail(JUnit3Suite.scala:149)
        at kafka.admin.DeleteTopicTest.testAutoCreateAfterDeleteTopic(DeleteTopicTest.scala:222)
",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/14 18:40;junrao;KAFKA-1529.patch;https://issues.apache.org/jira/secure/attachment/12654221/KAFKA-1529.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,403797,,,Tue Jul 15 21:53:48 UTC 2014,,,,,,,,,,"0|i1xhw7:",403840,,,,,,,,,,,,,,,,,,,,"06/Jul/14 18:38;junrao;The issue is probably due to not enough retries in the producer. However, testAutoCreateAfterDeleteTopic seems redundant since it should be covered by testRecreateTopicAfterDeletion already.;;;","06/Jul/14 18:40;junrao;Created reviewboard https://reviews.apache.org/r/23294/
 against branch origin/trunk;;;","15/Jul/14 16:31;nehanarkhede;[~junrao], would you like to check this in?;;;","15/Jul/14 21:53;junrao;Thanks for the reviews. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Messages is a required argument to Producer Performance Test,KAFKA-1517,12725168,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,danielcompton,danielcompton,danielcompton,03/Jul/14 03:22,08/Apr/15 05:20,14/Jul/23 05:39,08/Apr/15 05:20,0.8.1.1,,,0.9.0.0,,,,,,,,,,0,newbie,,,"When running the producer performance test without providing a messages argument, you get an error:

{noformat}
$bin/kafka-producer-perf-test.sh --topics mirrormirror --broker-list kafka-dc21:9092
Missing required argument ""[messages]""

Option                                  Description
------                                  -----------
......
--messages <Long: count>                The number of messages to send or
                                          consume (default:
                                          9223372036854775807)
{noformat}

However the [shell command documentation|https://github.com/apache/kafka/blob/c66e408b244de52f1c5c5bbd7627aa1f028f9a87/perf/src/main/scala/kafka/perf/PerfConfig.scala#L25] doesn't say that this is required and implies that [2^63-1|http://en.wikipedia.org/wiki/9223372036854775807] (Long.MaxValue) messages will be sent. It should probably look like the [ConsoleProducer|https://github.com/apache/kafka/blob/c66e408b244de52f1c5c5bbd7627aa1f028f9a87/core/src/main/scala/kafka/producer/ConsoleProducer.scala#L32] and prefix the documentation with REQUIRED. Or should we make this a non-required argument and set the default value to something sane like 100,000 messages.

Which option is preferable for this?",,danielcompton,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/14 01:14;danielcompton;0001-KAFKA-1517-Make-messages-a-required-argument.patch;https://issues.apache.org/jira/secure/attachment/12657021/0001-KAFKA-1517-Make-messages-a-required-argument.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,403351,,,Wed Apr 08 05:20:21 UTC 2015,,,,,,,,,,"0|i1xf7b:",403402,,,,,,,,,,,,,,,,,,,,"06/Jul/14 15:52;junrao;Yes, I think it would be better if we document it as required and get rid of the default.;;;","06/Jul/14 23:20;danielcompton;Great, I'll prepare a patch, documenting it as mandatory and removing the default value.;;;","22/Jul/14 01:15;danielcompton;I've attached a patch for this. Let me know if the patch isn't correct, I wasn't sure I was doing it correctly.;;;","08/Apr/15 05:20;junrao;Sorry for the late review. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Producer Performance Test sends messages with bytes of 0x0,KAFKA-1516,12725161,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,danielcompton,danielcompton,03/Jul/14 02:52,30/Mar/17 12:19,14/Jul/23 05:39,30/Mar/17 12:19,0.8.1.1,,,,,,,,,,,,,0,,,,"The producer performance test in Kafka sends messages with either [0x0 bytes|https://github.com/apache/kafka/blob/0.8.1/perf/src/main/scala/kafka/perf/ProducerPerformance.scala#L237] or messages with [all X's|https://github.com/apache/kafka/blob/0.8.1/perf/src/main/scala/kafka/perf/ProducerPerformance.scala#L225]. This skews the compression ratio massively and probably affects performance in other ways.

We want to create messages which will give a more realistic performance profile. Using random bytes may not be the best solution as these won't compress at all and will skew compression times.

Perhaps using a template which injects random or sequential data into it could work. Or maybe I'm overthinking it and we should just go for random bytes. What other options do we have? Others seem to use random bytes like [cassandra-stress|https://github.com/zznate/cassandra-stress/blob/master/src/main/java/com/riptano/cassandra/stress/InsertCommand.java#L39].",,danielcompton,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-4432,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,403344,,,Thu Mar 30 12:19:50 UTC 2017,,,,,,,,,,"0|i1xf5r:",403395,,,,,,,,,,,,,,,,,,,,"30/Mar/17 12:19;omkreddy;Resolving this as  support for passing custom message payloads is given in KAFKA-4432;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wake-up Sender upon blocked on fetching leader metadata,KAFKA-1515,12725145,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,03/Jul/14 00:41,15/Nov/14 16:31,14/Jul/23 05:39,09/Jul/14 16:48,,,,0.8.2.0,,,,,,,,,,0,,,,"Currently the new KafkaProducer will not wake up the sender thread upon forcing metadata fetch, and hence if the sender is polling with a long timeout (e.g. the metadata.age period) this wait will usually timeout and fail.",,guozhang,jkreps,joestein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/14 16:39;guozhang;KAFKA-1515.patch;https://issues.apache.org/jira/secure/attachment/12655239/KAFKA-1515.patch","03/Jul/14 17:19;guozhang;KAFKA-1515_2014-07-03_10:19:28.patch;https://issues.apache.org/jira/secure/attachment/12653963/KAFKA-1515_2014-07-03_10%3A19%3A28.patch","03/Jul/14 23:43;guozhang;KAFKA-1515_2014-07-03_16:43:05.patch;https://issues.apache.org/jira/secure/attachment/12654010/KAFKA-1515_2014-07-03_16%3A43%3A05.patch","07/Jul/14 17:56;guozhang;KAFKA-1515_2014-07-07_10:55:58.patch;https://issues.apache.org/jira/secure/attachment/12654344/KAFKA-1515_2014-07-07_10%3A55%3A58.patch","08/Jul/14 18:36;guozhang;KAFKA-1515_2014-07-08_11:35:59.patch;https://issues.apache.org/jira/secure/attachment/12654636/KAFKA-1515_2014-07-08_11%3A35%3A59.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,403328,,,Fri Jul 11 20:12:36 UTC 2014,,,,,,,,,,"0|i1xf27:",403379,,,,,,,,,,,,,,,,,,,,"03/Jul/14 17:19;guozhang;Updated reviewboard https://reviews.apache.org/r/23266/
 against branch origin/trunk;;;","03/Jul/14 23:43;guozhang;Updated reviewboard https://reviews.apache.org/r/23266/
 against branch origin/trunk;;;","07/Jul/14 17:56;guozhang;Updated reviewboard https://reviews.apache.org/r/23266/
 against branch origin/trunk;;;","08/Jul/14 18:36;guozhang;Updated reviewboard https://reviews.apache.org/r/23266/
 against branch origin/trunk;;;","08/Jul/14 20:23;jkreps;Committed. Thanks for going through all the versions, I really like the solution we finally ended up with.;;;","08/Jul/14 20:46;guozhang;Thanks Jay!;;;","11/Jul/14 16:39;guozhang;Created reviewboard https://reviews.apache.org/r/23415/
 against branch origin/trunk;;;","11/Jul/14 20:00;jkreps;I am getting transient hangs after ProducerFailureTest.testSendAfterClose, but I will assume that is due to KAFKA-1533.;;;","11/Jul/14 20:12;jkreps;Committed with a very minor change: I renamed the isReadyToSend method to isSendable simply because we had like four methods that were ready/isReady/etc and each meant something slightly different. This hopefully differentiates better: sendable is when you are connected and have room to send, ready is when you are sendable and there is nothing else going on that makes us want to block the request. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Kafka trunk version number,KAFKA-1514,12725115,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jghoman,jghoman,02/Jul/14 22:05,20/Oct/14 21:59,14/Jul/23 05:39,20/Oct/14 21:59,,,,,,,,,,,,,,0,,,,"Right now the 8.1.1 branch has 8.1.1 as its version, while the trunk has 8.1.  Trunk should be 9.0, generally, or at the very least 8.2.",,gmazza,jghoman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,403298,,,Wed Oct 15 21:06:10 UTC 2014,,,,,,,,,,"0|i1xevr:",403350,,,,,,,,,,,,,,,,,,,,"15/Oct/14 21:06;gmazza;Trunk is presently 0.8.3-SNAPSHOT: https://git-wip-us.apache.org/repos/asf?p=kafka.git;a=blob_plain;f=gradle.properties;hb=trunk.  Can this item be closed?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Malformated link and consumer/producer mixup in documentation,KAFKA-1511,12724316,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,junrao,vbernat,vbernat,29/Jun/14 00:12,02/Jul/14 16:00,14/Jul/23 05:39,02/Jul/14 16:00,,,,0.8.2.0,,,,,,,website,,,0,,,,"Hi!

{quote}
Adding such callback functionality is proposed for Kafka 0.9, see [Proposed Producer API](https://cwiki.apache.org/confluence/display/KAFKA/Client+Rewrite#ClientRewrite-ProposedProducerAPI).
{quote}

The link is not formatted as a link (markdown syntax instead of HTML).

{quote}
If the consumer never crashed it could just store this position in memory, but if the producer fails and we want this topic partition to be taken over by another process the new process will need to choose an appropriate position from which to start processing. 
{quote}

I think that ""producer"" should be replaced by ""consumer"".

I do not propose a patch because I am unsure where the sources are. Is that the HTML files in the SVN repository? Or is there some git repository somewhere?",,junrao,vbernat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,402501,,,Wed Jul 02 16:00:44 UTC 2014,,,,,,,,,,"0|i1xa27:",402568,,,,,,,,,,,,,,,,,,,,"02/Jul/14 16:00;junrao;Thanks for pointing this out. Fixed the website.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Force offset commits when migrating consumer offsets from zookeeper to kafka,KAFKA-1510,12724186,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jjkoshy,jjkoshy,jjkoshy,27/Jun/14 18:27,05/Sep/14 19:21,14/Jul/23 05:39,05/Sep/14 19:21,0.8.2.0,,,0.8.2.0,,,,,,,,,,0,newbie,,,"When migrating consumer offsets from ZooKeeper to kafka, we have to turn on dual-commit (i.e., the consumers will commit offsets to both zookeeper and kafka) in addition to setting offsets.storage to kafka. However, when we commit offsets we only commit offsets if they have changed (since the last commit). For low-volume topics or for topics that receive data in bursts offsets may not move for a long period of time. Therefore we may want to force the commit (even if offsets have not changed) when migrating (i.e., when dual-commit is enabled) - we can add a minimum interval threshold (say force commit after every 10 auto-commits) as well as on rebalance and shutdown.

Also, I think it is safe to switch the default for offsets.storage from zookeeper to kafka and set the default to dual-commit (for people who have not migrated yet). We have deployed this to the largest consumers at linkedin and have not seen any issues so far (except for the migration caveat that this jira will resolve).
",,guozhang,jjkoshy,junrao,nmarasoi,nmarasoiu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/14 14:57;nmarasoiu;Patch_to_push_unfiltered_offsets_to_both_Kafka_and_potentially_Zookeeper_when_Kafka_is_con.patch;https://issues.apache.org/jira/secure/attachment/12665351/Patch_to_push_unfiltered_offsets_to_both_Kafka_and_potentially_Zookeeper_when_Kafka_is_con.patch","05/Sep/14 05:07;nmarasoi;Unfiltered_offsets_commit_to_kafka_rebased.patch;https://issues.apache.org/jira/secure/attachment/12666669/Unfiltered_offsets_commit_to_kafka_rebased.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,402371,,,Fri Sep 05 19:21:15 UTC 2014,,,,,,,,,,"0|i1x987:",402433,,jjkoshy,,,,,,,,,,,,,,,,,,"18/Jul/14 12:02;nmarasoiu;Hi, it sounds clear and simple enough, I am going to try this.
I will probably come back with some questions for the low level detail.

Why is the offset management moving from zookeeper to kafka? To ease the consumer and favor language proliferation of consumers ? Is kafka managing them through zookeeper as well, behind the scenes, or is it using its own / other cluster / consensus mechanism to store the offsets in a HA manner?;;;","18/Jul/14 17:06;guozhang;Hi Nicolae,

Thanks for taking this ticket. You can take a look at the offset management design proposal for the motivations of moving it away from ZK.

https://cwiki.apache.org/confluence/display/KAFKA/Inbuilt+Consumer+Offset+Management
;;;","18/Jul/14 19:50;jjkoshy;I think it should be sufficient to force commit only on shutdown while dual-commit is enabled. i.e., no need to force commit at intervals.;;;","23/Jul/14 21:58;jjkoshy;[~nmarasoiu] do you think you will be able to take this on in the next couple days?;;;","24/Jul/14 07:03;nmarasoiu;[~jjkoshy] Yes I am going to tackle this these days, have a first patch proposal in the weekend or sooner.;;;","26/Jul/14 18:03;nmarasoi;Hi,
isAutoCommit argument works exactly the other way around, apparently it is ""false"" from the scheduled auto commit and to ""true"" from zkConsConn.commitOffsets()?

So the migration of offsets from zk to kafka is to : set dual commit and kafka storage, restart consumers, wait for kafka to be copied on the offset commits, and take out dual commit.

So currently kafka is copied with the offsets only when data flows, and for the purpose of this task, we need to add one or 2 more cases when it is getting the offset: when shutting down, or perhaps periodically.

So this task applies only when storage==kafka and dualCommit ==true, right?

I would first ask why the write to zookeeper the new offsets, only if the write to kafka was ok? My assumption is To make sure only one write to zookeeper, even though the process of writing to kafka may involve retries. 

I would write both directions at all time, and perhaps keep 2 checkpoint structures, one kafka one zookeeper.


I create a patch now with: a forceCommit that will make that all offsets are commited to both kafka and zookeeper when shutting down in dual commit mode.

The usefulness of committing all offsets not only to kafka but to zookeeper as well comes at least from one reason: the one I mentioned above, that if kafka offset write fails completely, zookeeper is never copied on that.

Forcing all offsets to zk on shutdown too does indeed have the drawback that it will typically copy the same offsets again, and not only once but potentially several times (if kafka is retried).
However the alternative is to commit to both kafka and zookeeper unconditionally in the normal flow (right now, the commit to zk happens only after a successful commit to kafka if any). That too poses the same risk of committing multiple times to a system (zk) if the other (kafka) needs retries. So a clean way here would be a completely different OffsetDAO implementation, one on kafka , one on zookeeper, and one on dual mode, and read, as now max(both), while write goes to the 2 implementations, each of them doing retries without affecting the other!;;;","27/Jul/14 06:40;nmarasoi;attached patch as per my interpretation and tradeoffs detailed in my comments;;;","30/Jul/14 05:12;nmarasoi;[~jjkoshy] Can you please take a look at my comments+code, it will probably take one more iteration at least to make it.;;;","30/Jul/14 11:58;nmarasoi;[~jkreps] Hi, can you please help me with feedback on my comment + code, or who can I ask, so that I can go in the right direction?;;;","30/Jul/14 23:17;jjkoshy;[~nmarasoi] - sure thing. Will get back to you on this.;;;","01/Aug/14 01:12;jjkoshy;Re: isAutoCommit:
Yes you are right. Can you fix that?

Re: migration steps:
Yes that is correct. ""Wait for kafka to be copied on commits"" would essentially mean do one rolling bounce after your patch (since the shutdown guarantees that the offsets would have moved over).

Re: applies only when storage==kafka and dual-commit==true.
Yes that is correct.

Re:  why the write to zookeeper the new offsets, only if the write to kafka was ok?
As you observed, avoid redundant writes to ZooKeeper, but also for consistency between ZK and Kafka (although this does not matter too much since while transitioning they are inconsistent to start with).

Re: separate DAOs
We probably don't need that since we plan to eventually remove support for ZooKeeper-based offsets from the server-side - i.e., a consumer can choose to store offsets elsewhere (including ZooKeeper) but support for doing that via the OffsetCommitRequest channel will be removed.

Your patch looks good, but I'm unclear on why you need the ""|| forceCommit"" on line 318;;;","01/Aug/14 11:52;nmarasoi;Hi, the ""|| forceCommit"" on line 318 is meant to ensure write to zookeeper on shutdown even in the situation when kafka commits do not work.;;;","02/Aug/14 00:21;jjkoshy;I see - my thinking was since we retry indefinitely until a successful commit it only needs to be done at the end (once) after a successful commit to Kafka.

So to summarize - let me know if you have any comments/questions:

* Can you fix the issue you caught with the isAutoCommit flag?
* Probably unnecessary to have the ""|| forceCommit""
* Also, as mentioned in the summary I think it is reasonable to switch the default offsets.storage to Kafka and set dual.commit to true.
* Can you also run the unit tests and verify? It will be useful to also run the system tests (at least the mirror maker test suite). See https://cwiki.apache.org/confluence/display/KAFKA/Kafka+System+Tests#KafkaSystemTests-RunningSystemTest for more information on this - it should be sufficient to just run the mirror maker tests.
;;;","02/Aug/14 12:30;nmarasoiu;Where is the indefinite retry you mention? I don't think it is indefinite..



;;;","02/Aug/14 12:34;nmarasoiu;It is a limited retry , and I found no easy way to determine if this is the
last attempt, so that I write to zookeeper as well. This can be taken out
to a different jira task too, but I would let the zookeeper ensure-commit
here too, and regard this task as a ""make all efforts to commit all offsets
to both storages at shutdown"" and rename as such.


On Sat, Aug 2, 2014 at 3:28 PM, Nicolae Marasoiu <nicolae.marasoiu@gmail.com

;;;","02/Aug/14 12:57;nmarasoi;Hi,

I have given it more consideration, and indeed to ""||force"" on 318 it is a different concern, which can be taken to another task.
The risk which it would solve, is that when kafka is out for the limited retry count during shutdown, at least zookeeper would get the offsets, and the consumer will not rewind. However it is low probability that both systems are down, so zookeeper would likely be up to date when kafka is down, for instance. The probability that zookeeper will get flooded with all offsets multiple times kafka is retried is comparable to that low probability.

So, for this task, I take out that line 318 part of the patch, test went fine.

I will create another task for isAutoCommit issue and analyze if the meaning is truly reversed, cause I feel it is only partially and perhaps used correctly with the reversed name, and it is mostly diffent thing.

I will do the config changes, no prob - switch the default offsets.storage to Kafka and set dual.commit to true.;;;","03/Aug/14 05:54;nmarasoi;re-uploaded the patch ;;;","08/Aug/14 18:26;jjkoshy;[~nmarasoi] I realized later there is actually a flaw in how we get rid of offset commits from old (non-existent) consumers.

As of now, the offset manager does the following: it periodically goes through its entire cache (i.e., hashtable of offsets) and extracts those entries that have a timestamp earlier than some staleness threshold. It then proceeds to add tombstones for those entries in the offsets commit log.

The problem with this approach as it stands is similar to the original issue that this jira intended to address. A live consumer may be consuming a low volume topic and its offset may change infrequently. i.e., its offset may not move within the staleness threshold. If we delete the offset and a consumer rebalance occurs and fetches that offset, then depending on the auto.offset.reset configuration, it will pick up the new latest offset of the topic (in which case the consumer could lose some messages) or the earliest offset (in which case the consumer will see duplicates).

I think the fix for this is the following and I'm backtracking to what I earlier wrote and later (incorrectly) thought was unnecessary:

A consumer implementation can optionally choose to selectively commit only offsets that have changed since the last commit. HOWEVER, there should be a configurable interval at which the consumer should always commit ALL its offsets regardless of whether it has changed or not.
;;;","08/Aug/14 18:28;jjkoshy;Would you be able to update your patch to take into account the above?;;;","10/Aug/14 21:15;junrao;Thinking about this a bit more, would it be more reliable to do the expiration of an offset based on the last connect time from the client, instead of the last time the offset is modified? In the new consumer, we will be tracking the set of consumers per consumer group on the broker. We can expire an offset if the time since the last time the partition was actively owned by a consumer exceeds the threshold. Handling consumer coordinator failover can be a bit tricky. We can probably just start doing the expiration countdown from the beginning during the failover. This means that the removal of some of the offsets may be delayed. This maybe ok since the consumer coordinator failover should be rare.;;;","12/Aug/14 17:25;nmarasoi;ok, so I have not fully understood, but what I think I did, is that for the moment there is no clear decision on any modifications to the patch as it is now, the way that i understand it;;;","12/Aug/14 17:51;jjkoshy;[~junrao] yes that would make sense. However, we don't have this tracking implemented in the existing consumer and I would rather not add that feature now just to fix an existing bug. i.e., I think we should just fix the current issue by forcing commits (either periodically or always) when using Kafka-based offset storage.;;;","13/Aug/14 19:00;jjkoshy;After some discussion with [~guozhang] and [~junrao] here are some additional comments to help clarify my earlier reasoning:

In order to migrate offsets from ZooKeeper to Kafka, at minimum we need to force an unfiltered commit (regardless of whether offsets have changed or not) at some point - e.g., shut down of the consumer.

An orthogonal issue is that of a consumer that consumes a low-volume topic. i.e., if the offsets don't change within the offset retention threshold on the offset manager (defaults to one day) then those offsets will be deleted. If the consumer fails for any reason and does an offset fetch, it will reset to earliest or latest. We have a couple of options:
* One possible approach to address this is to configure the broker-side offset retention period to a large value - i.e., larger than the maximum retention period of any topic. This is not ideal because: (a) if there are short-lived (say, console-) consumers that come and go often then those offsets can sit around for a long time; (b) in general, you cannot really come up with a retention period for a compacted topics. So I would not want to do this, but I wrote this here for completeness.
* Another approach is to do UN-filtered commits if offsets.storage is set to Kafka. i.e., commit everything always.
* Yet another approach is to do unfiltered commits at a configurable interval.

Thoughts?

My preference after thinking about it is to go with the second approach.
;;;","16/Aug/14 01:11;junrao;Yes, I agree that always do unfiltered commits when offset.storage is used is the simplest and is good enough. ;;;","19/Aug/14 06:42;jjkoshy;[~nmarasoi] can you update your patch to do unfiltered commits if offsets.storage is kafka? Or if you have any other preference in approach, feel free to comment.;;;","25/Aug/14 07:39;jjkoshy;[~nmarasoi] do you think you will have time to wrap this up? If not, let me know or if you need any clarification.;;;","25/Aug/14 07:48;nmarasoiu;Hi, I will attach a patch with unfiltered commits today, or latest tomorrow;

So to clarify the requirement, my understanding is that, when storage =
kafka and dualcommit = enabled (or just one of those?), we are going to
make each and every offsets commit to kafka an unfiltered one i.e. commit
all offsets regardless of change or not; do we do this unfiltered commit
also on zookeeper?

Thanks
Nicu



;;;","25/Aug/14 18:10;jjkoshy;[~nmarasoi] that's right - whenever storage=kafka, we should to unfiltered commits to kafka. In other words, ALL offset commits sent to kafka should be unfiltered.

As for commits to zookeeper: if storage=zookeeper then we can do filtered commits. If storage=kafka and dual.commit=enabled then if the code doesn't get too complicated we should continue to do filtered commits to zookeeper (but unfiltered to kafka).;;;","28/Aug/14 19:55;nmarasoi;Hi, I will do this tomorrow, Friday, so I hope you will have a patch Friday morning your time.

So you say that the only condition to do unfiltered is kafka storage, regardless of dual commit mode or single commit mode, yes?

Thanks,
Nicu;;;","29/Aug/14 14:57;nmarasoiu;Patch to push unfiltered offsets to both Kafka and potentially Zookeeper when Kafka is configured to be the offset storage;;;","29/Aug/14 18:11;nmarasoi;Attached a patch - I am doing unfiltered commits to kafka and using offsets checkpoint Map for zookeeper incremental commits only (in both zk storage and dual commit modes) - its reads and mutations are now part of the commitToZk method exclusively in the suggested approach.

Right now, the rearrangement of topic topicRegistry into offsettsToCommit, a different reified structure with no more filtering in the process of its reification seems a bit futile, but because we got .size if, and we got usage of the structure below, and to minimize changes brought by this task (and leave them for an obvious future need of refactoring on the bigger scale this class), I let it like this.

The other optimization I could do, but not included in the patch, is to keep a state of the commit timestamp for each partition, and use that for filtering commits to kafka, based on a configurable maximum idleness of the partition offset commit for each partition.

A more primitive form of the same optimization, that would only protect from repeatedly committing to good brokers because of the broken ones, I could have such a state in a local structure for the duration of the method, just to make sure we keep retrying only the failed commits.;;;","02/Sep/14 16:06;nmarasoi;[~jjkoshy] Hi, can you check my patch + comments & provide feedback pls?;;;","05/Sep/14 01:12;jjkoshy;[~nmarasoi] Your patch looks good to me - however, it does not cleanly apply on the latest trunk. Would you mind rebasing? If you don't have time I can take care of it as well.;;;","05/Sep/14 05:07;nmarasoi;Attached rebased patch;;;","05/Sep/14 19:21;jjkoshy;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using GetOffsetShell against non-existent topic creates the topic unintentionally,KAFKA-1507,12723388,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,sriharsha,lukeforehand,lukeforehand,24/Jun/14 15:50,15/Jun/18 17:40,14/Jul/23 05:39,15/Jun/18 17:40,0.8.1.1,,,,,,,,,,,,,1,newbie,,,"A typo in using GetOffsetShell command can cause a
topic to be created which cannot be deleted (because deletion is still in
progress)

./kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list
kafka10:9092,kafka11:9092,kafka12:9092,kafka13:9092 --topic typo --time 1

./kafka-topics.sh --zookeeper stormqa1/kafka-prod --describe --topic typo
Topic:typo      PartitionCount:8        ReplicationFactor:1     Configs:
         Topic: typo     Partition: 0    Leader: 10      Replicas: 10
  Isr: 10
...",centos,abraithwaite,bobrik,fullung,gwenshap,jbrosenberg@gmail.com,jkreps,jozi-k,junrao,lukeforehand,nehanarkhede,omkreddy,rmchale,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-2410,,,,,,,,,,,,,,,,,,,KAFKA-1462,,"13/Aug/14 00:05;sriharsha;KAFKA-1507.patch;https://issues.apache.org/jira/secure/attachment/12661346/KAFKA-1507.patch","08/Jul/14 16:21;sriharsha;KAFKA-1507.patch;https://issues.apache.org/jira/secure/attachment/12654607/KAFKA-1507.patch","22/Jul/14 17:28;sriharsha;KAFKA-1507_2014-07-22_10:27:45.patch;https://issues.apache.org/jira/secure/attachment/12657145/KAFKA-1507_2014-07-22_10%3A27%3A45.patch","24/Jul/14 00:07;sriharsha;KAFKA-1507_2014-07-23_17:07:20.patch;https://issues.apache.org/jira/secure/attachment/12657495/KAFKA-1507_2014-07-23_17%3A07%3A20.patch","13/Aug/14 01:09;sriharsha;KAFKA-1507_2014-08-12_18:09:06.patch;https://issues.apache.org/jira/secure/attachment/12661360/KAFKA-1507_2014-08-12_18%3A09%3A06.patch","22/Aug/14 18:06;sriharsha;KAFKA-1507_2014-08-22_11:06:38.patch;https://issues.apache.org/jira/secure/attachment/12663702/KAFKA-1507_2014-08-22_11%3A06%3A38.patch","22/Aug/14 18:09;sriharsha;KAFKA-1507_2014-08-22_11:08:51.patch;https://issues.apache.org/jira/secure/attachment/12663703/KAFKA-1507_2014-08-22_11%3A08%3A51.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,401575,,,Fri Jun 15 17:40:11 UTC 2018,,,,,,,,,,"0|i1x4ev:",401648,,jkreps,,,,,,,,,,,,,,,,,,"27/Jun/14 01:38;sriharsha; [~nehanarkhede] this is the side effect of auto.create.topics.enable set to true by default.
KafkaApis.handleTopicMetadataRequest checks for above config and creates a topic. Do we need to create a topic in TopicMetadataRequest. I don't see anywhere else we are using this but I am not sure if any other outside lib using this api to create a topic.;;;","02/Jul/14 16:11;junrao;Both the produce and the consumer client will issue TopicMetadataRequest. Topics should probably be only auto-created by the writers, not the readers, i.e., it makes sense for topics to be auto-created by the producers, but not by the consumer or offset reader.

One way to do that is to add a field in TopicMetadataRequest to indicate wether topic should be auto-created or not. This will be a wire protocol change though. So, we will have to support both versions on the broker.;;;","08/Jul/14 16:21;sriharsha;Created reviewboard https://reviews.apache.org/r/23339/diff/
 against branch origin/trunk;;;","09/Jul/14 16:28;junrao;Thanks for the patch. I am currently working on KAFKA-1462, which I hope will standardize how we support multiple versions of a request on the server. Perhaps, you can wait until KAFKA-1462 is done, hopefully in a week or so.;;;","22/Jul/14 17:28;sriharsha;Updated reviewboard https://reviews.apache.org/r/23339/diff/
 against branch origin/trunk;;;","24/Jul/14 00:07;sriharsha;Updated reviewboard https://reviews.apache.org/r/23339/diff/
 against branch origin/trunk;;;","29/Jul/14 00:21;jkreps;I kind of feel this is not the right fix.

I think the behavior we have is kind of indefensible. The history was that we previously had an auto-create topic that happened when a message was sent to a broker and that broker didn't host the topic. However when we moved to replication this broke for multiple reasons: the producer needed metadata about the topic to send the message, and not all brokers would have all partitions. However this ""auto-create"" behavior is very useful. So we kind of just grandfathered it in by having the metadata request have the same side effect.

But this is really a silly indefensible behavior. There is no reason that asking for metadata should create topics! This would be like in a database if running DESCRIBE TABLE X would create table X for you if it didn't exist. This just confuses everyone, as it must have confused you.

In any case the auto-creation behavior is very limited because there is no way to specify the number of partitions, the replication factor, or any topic-specific configuration.

Rather than further enshrining this behavior behavior by starting to add topic creation options to the metadata request, I really think we should add a proper create_topic API and have producers use that.

We could even make this a little more general and handle create, alter, and delete. This would give clients full control.

;;;","29/Jul/14 03:40;junrao;Sriharsha,

After thinking about this a bit more, I agree with Jay's points. So, we probably have to abandon the current approach. Sorry for not realizing this earlier.;;;","30/Jul/14 16:16;sriharsha;[~junrao] [~jkreps] Thanks for the details above. Based on the comments by Jay we should be dropping creation of topics from TopicMetaData request and add createTopicRequest to the api along with topic creation properties such partitions , replication etc. 
And in KafkaProducer.send if the metadatarequest comes out empty we should be making a call to createTopic .
In this case should we also have a boolean flag in KafkaProducer for createTopic . If both producer.createTopic and ""auto.create.topics.enable"" on broker set to true we will create a topic with user supplied config or using the defaults.
I think auto creation of topics config should be on the producer side rather than the broker having it on two places might be confusing. 
Please let me know what you think of the above approach. Thanks.;;;","30/Jul/14 16:40;jkreps;Maybe the best plan would be to retain the option we have for compatibility but default it to off, and have the new producer client make use of the new api.;;;","13/Aug/14 00:05;sriharsha;Created reviewboard https://reviews.apache.org/r/24621/diff/
 against branch origin/trunk;;;","13/Aug/14 00:21;sriharsha;[~jkreps] [~junrao] can you please take a look at the latest patch and let me know your thoughts on the approach. I am working on adding more tests. Thanks;;;","13/Aug/14 01:09;sriharsha;Updated reviewboard https://reviews.apache.org/r/24621/diff/
 against branch origin/trunk;;;","13/Aug/14 23:15;nehanarkhede;Reviewed most of the patch, didn't get a chance to look at the changes in NetworkClient closely and the new request formats. Assigning to [~jkreps] for that part of the review. ;;;","22/Aug/14 18:06;sriharsha;Updated reviewboard https://reviews.apache.org/r/24621/diff/
 against branch origin/trunk;;;","22/Aug/14 18:09;sriharsha;Updated reviewboard https://reviews.apache.org/r/24621/diff/
 against branch origin/trunk;;;","02/Sep/14 16:34;sriharsha;2 weeks ping. [~jkreps] [~junrao] Can you please take a look at this patch. Lot of files changed so hard to keep it merge conflict free :).;;;","08/Sep/14 13:36;sriharsha;[~jkreps] [~junrao] [~guozhang] [~nehanarkhede] Could you please review this patch. Thank you.;;;","16/Sep/14 04:48;junrao;Sriharsha,

Sorry for the late reply. Since this patch introduces a new type of request and is relatively big, perhaps it should wait until the 0.8.2 branch is cut.;;;","16/Sep/14 05:02;sriharsha;Thanks [~junrao] sounds good to me.;;;","26/Nov/14 04:38;sriharsha;[~junrao]  I've seen few users request having a create topic ability in producer itself. I can do a up-merge and resend the patch if there is interest in this JIRA.;;;","15/Jan/15 16:32;sriharsha;[~junrao] [~nehanarkhede] Is this JIRA that can be included in 0.8.3 or 0.9.0 . If so I'll do an upmerge and resend the patch. Thanks.;;;","15/Jan/15 16:51;jkreps;I think the right way to do this is have a proper create/delete/alter topic api (which i think is in-flight now). We should make having the get metadata request auto-creating topics optional and disable it by default (e.g. add an option like metadata.requests.auto.create=false). We can retain the auto-create functionality in the producer by having it issue this request in response to errors about a non-existant topic.

I don't think we should change the java api of the producer to expose this (i.e. add a producer.createTopic(name, replication, partitions, etc). Instead I think we should consider a Java admin client that exposes this functionality. This would be where we would expose other operational apis as well. The rationale for this is that creating, deleting, and modifying topics is actually not part of normal application usage so having it directly exposed in the producer is a bit dangerous.

We should definitely do a KIP proposal around this and get the design and API worked out first. I think we could do this in 0.8.3 if you are up to work on it. It would likely depend on some of the changes in KAFKA-1760 so we would want to merge that first.;;;","15/Jan/15 17:51;sriharsha;[~jkreps] thanks for the comments.  
""We should make having the get metadata request auto-creating topics optional and disable it by default (e.g. add an option like metadata.requests.auto.create=false)""
currently meta data request checks for broker config ""auto.create.topics.enable"" . This is disabled in my patch.

""We can retain the auto-create functionality in the producer by having it issue this request in response to errors about a non-existant topic.""
    This is what my current patch does. it sets ""auto.create.topics.enable"" to false on the broker config and when the producer makes TopicMetadataRequest  it returns Unknown_Topic_or_partition.  ProducerConfig has new properties like ""auto.create.topics.enable"" . 
If this property set to true( by default) producer issues a new request for CreateTopicRequest"" upon receiving unknown_topic_or_partition error. which will than issues create topic on broker side with the configured numPartitions and replicationFactor.

""I don't think we should change the java api of the producer to expose this (i.e. add a producer.createTopic(name, replication, partitions, etc). .""

I agree and my patch doesn't change any api of the producer and it doesn't have producer.createTopic but it does introduce createTopicRequest and createTopicResponse.

""Instead I think we should consider a Java admin client that exposes this functionality. This would be where we would expose other operational apis as well. The rationale for this is that creating, deleting, and modifying topics is actually not part of normal application usage so having it directly exposed in the producer is a bit dangerous.""

I am not sure if I understand correctly about the java admin client. We already have AdminUtils , is this about introducing network apis for create/delete/alter topics? . Even in this case I think this patch can be useful as the producer just makes createTopicRequest which I think what you want unless I missed something :). 


;;;","26/Jan/15 18:54;jbrosenberg@gmail.com;I think relegating topic creation to an admin client would be very limitiing.  It's extremely useful to have a self-service system where new applications can just create a new topic on demand (with reasonable defaults), without the need for an admin to come in and prepare topics ahead of a code release (leave that to dba's managing transactional databases!).

I do like the idea of an automatic create topic request from a producer, in response to a topic not found exception, rather than auto-creating topics from meta-data requests (which happens asynchronously and causes the initial meta data request to fail usually!).  Consumers should never create a topic, I should think.;;;","23/Mar/15 15:21;sriharsha;[~jkreps] Since create/update/topic requests are part of KIP-4. Your proposal if the producer is throwing errors like UnknownTopicOrPartition users should catch this error and use AdminClient create a topic?. I still see a benefit of allowing users to pass in their required topic config( partitions, replication etcc) and if there is no topic exists send a createTopicRequest. If this is not desirable as per your suggestion we need to implement AdminClient?. In this case they can use AdminUtils and we should modify the AdminUtils send requests to broker instead of directly sending requests to zookeeper. This will also help KAFKA-1688 as all the create/update/delete requests will go through broker authorizer. Let me know if this what your thinking.;;;","11/Aug/15 22:21;sriharsha;[~jkreps] Since there is interest in the community about moving creation of topics onto client side specifically producer side can this patch be reviewed. There are also other JIRAs filed
https://issues.apache.org/jira/browse/KAFKA-2410 asking for the same feature addressed in the patch here. There is obviously big JIRA to add create topic requests https://issues.apache.org/jira/browse/KAFKA-2229 not sure if this needs to be blocked by that. If there is interest than I can upmerge my patch.;;;","15/Jun/18 17:40;omkreddy;GetOffsetShell to is updated to use java KafkaConsumer. This solves the original problem reported in the JIRA. 

 [https://github.com/apache/kafka/pull/5220];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker failed to start because of corrupted replication-offset-checkpoint file,KAFKA-1505,12723263,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,ttthree,ttthree,24/Jun/14 03:31,04/Sep/17 11:21,14/Jul/23 05:39,04/Sep/17 11:21,0.8.1,,,,,,,,,,replication,,,0,,,,"The replication-offset-checkpoint file seems to have some blank entries where the topic, partition, offset information is expected.

Broker service cannot be started with following error:

[2014-06-18 11:09:51,272] ERROR [KafkaApi-5] error when handling request Name:LeaderAndIsrRequest;Version:0;Controller:2;ControllerEpoch:35;CorrelationId:11;ClientId:id_2-host_10.65.127.89-port_9092;PartitionState:(x,4) -> (LeaderAndIsrInfo:(Leader:14,ISR:14,LeaderEpoch:33,ControllerEpoch:35),ReplicationFactor:3),AllReplicas:14,5,6),(xm,20) -> (LeaderAndIsrInfo:(Leader:11,ISR:11,4,LeaderEpoch:11,ControllerEpoch:32),ReplicationFactor:3),AllReplicas:11,4,5),(xm,0) -> (LeaderAndIsrInfo:(Leader:11,ISR:11,4,LeaderEpoch:28,ControllerEpoch:32),ReplicationFactor:3),AllReplicas:11,4,5),(xm_int,3) -> (LeaderAndIsrInfo:(Leader:17,ISR:17,18,LeaderEpoch:26,ControllerEpoch:30),ReplicationFactor:3),AllReplicas:5,17,18),(x_int,9) -> (LeaderAndIsrInfo:(Leader:11,ISR:11,LeaderEpoch:38,ControllerEpoch:35),ReplicationFactor:3),AllReplicas:11,5,6),(xm_int,11) -> (LeaderAndIsrInfo:(Leader:13,ISR:13,LeaderEpoch:29,ControllerEpoch:35),ReplicationFactor:3),AllReplicas:13,5,6),(x_int,3) -> (LeaderAndIsrInfo:(Leader:19,ISR:19,0,LeaderEpoch:25,ControllerEpoch:32),ReplicationFactor:3),AllReplicas:5,19,0),(xm,1) -> (LeaderAndIsrInfo:(Leader:12,ISR:12,LeaderEpoch:30,ControllerEpoch:35),ReplicationFactor:3),AllReplicas:12,5,6),(x,3) -> (LeaderAndIsrInfo:(Leader:13,ISR:13,4,LeaderEpoch:22,ControllerEpoch:32),ReplicationFactor:3),AllReplicas:13,4,5),(xm,14) -> (LeaderAndIsrInfo:(Leader:18,ISR:18,19,LeaderEpoch:24,ControllerEpoch:31),ReplicationFactor:3),AllReplicas:5,18,19),(xm_int,10) -> (LeaderAndIsrInfo:(Leader:12,ISR:12,4,LeaderEpoch:25,ControllerEpoch:32),ReplicationFactor:3),AllReplicas:12,4,5),(x,15) -> (LeaderAndIsrInfo:(Leader:17,ISR:17,16,LeaderEpoch:24,ControllerEpoch:30),ReplicationFactor:3),AllReplicas:5,16,17),(x_int,8) -> (LeaderAndIsrInfo:(Leader:10,ISR:10,4,LeaderEpoch:26,ControllerEpoch:32),ReplicationFactor:3),AllReplicas:10,4,5),(xm,21) -> (LeaderAndIsrInfo:(Leader:12,ISR:12,LeaderEpoch:8,ControllerEpoch:35),ReplicationFactor:3),AllReplicas:12,5,6);Leaders:id:11,host:25.126.81.157,port:9092,id:14,host:25.126.81.159,port:9092,id:12,host:25.126.81.158,port:9092,id:17,host:10.153.63.196,port:9092,id:18,host:10.153.63.214,port:9092,id:19,host:10.65.127.95,port:9092,id:13,host:10.65.127.93,port:9092,id:10,host:10.65.127.92,port:9092 (kafka.server.KafkaApis)
java.lang.NumberFormatException: For input string: ""                                                                                                                                                                                                                                                        ""
                at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
                at java.lang.Integer.parseInt(Integer.java:481)
                at java.lang.Integer.parseInt(Integer.java:527)
                at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:231)
                at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
                at kafka.server.OffsetCheckpoint.liftedTree2$1(OffsetCheckpoint.scala:77)
                at kafka.server.OffsetCheckpoint.read(OffsetCheckpoint.scala:73)
                at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:91)
                at kafka.server.ReplicaManager$$anonfun$makeFollowers$5.apply(ReplicaManager.scala:347)
                at kafka.server.ReplicaManager$$anonfun$makeFollowers$5.apply(ReplicaManager.scala:346)
                at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
                at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
                at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:95)
                at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:95)
                at scala.collection.Iterator$class.foreach(Iterator.scala:772)
                at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
                at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
                at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:45)
                at scala.collection.mutable.HashMap.foreach(HashMap.scala:95)
                at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
                at scala.collection.mutable.HashMap.map(HashMap.scala:45)
                at kafka.server.ReplicaManager.makeFollowers(ReplicaManager.scala:346)
                at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:248)
                at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:93)
                at kafka.server.KafkaApis.handle(KafkaApis.scala:74)
                at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:42)
                at java.lang.Thread.run(Thread.java:724)
","Window Server 2012;JDK1.7.0",hongyu.bi,omkreddy,ttthree,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,401450,,,Mon Sep 04 11:21:31 UTC 2017,,,,,,,,,,"0|i1x3n3:",401523,,,,,,,,,,,,,,,,,,,,"04/Sep/17 11:21;omkreddy;This was fixed in newer versions.  Pl reopen if you think the issue still exists
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
all partitions are using same broker as their leader after broker is down,KAFKA-1503,12722850,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jackiewang518,jackiewang518,jackiewang518,20/Jun/14 19:02,03/Jul/14 04:56,14/Jul/23 05:39,03/Jul/14 04:56,0.8.0,0.8.1.1,,0.8.2.0,,,,,,,controller,,,0,patch,,,"The current leader selection always pick the first live broker in ISR when the current leader broker is down. Since the list of liveBrokerInIsr is not evenly distributed. As time goes on, all the partitions will use only one broker as its leader. 

I figured out a fix which is to use the first live broker in replica list which is also in ISR list. Since the liveAssignedReplicas is evenly distributed across brokers, all the partitions will be evenly distributed in the live brokers in ISR.
The fix is:
kafka-0.8.1.1-src/core/src/main/scala/kafka/controller/PartitionLeaderSelector.scala


71	71
           case false =>
72	 
-            val newLeader = liveBrokersInIsr.head
 	72
+            val liveReplicasInIsr = liveAssignedReplicas.filter(r => liveBrokersInIsr.contains(r))
 	73
+            val newLeader = liveReplicasInIsr.head",0.8.1.1 ,guozhang,jackiewang518,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/14 16:57;jackiewang518;kafka1503.patch;https://issues.apache.org/jira/secure/attachment/12651996/kafka1503.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,401037,,,Thu Jul 03 04:56:59 UTC 2014,,,,,,,,,,"0|i1x167:",401121,,,,,,,,,,,,,,,,,,,,"20/Jun/14 21:34;guozhang;Hello Jianwen,

Controller does always pick the first alive broker in the ISR list to be the leader, but since our partition assignment logic is round robin, such that if you have three partitions, three brokers, and a replication factor of three, then you should see the ISR lists are

partition1:   ISR {broker-1, broker-2, broker-3}
partition1:   ISR {broker-2, broker-3, broker-1}
partition1:   ISR {broker-3, broker-1, broker-2}

So the leader should still be distributed evenly.;;;","20/Jun/14 21:49;jackiewang518;Hi Guozhang,
   That is not what I experienced in my real cluster environment:
   Given a cluster contains three brokers, when all brokers are live and running, and there is a topic with 10 partitions:
   I just made a query on my cluster and got the topic info as below right now:

   root@kafka-1:~/kafka-0.8.1.1-src# bin/kafka-topics.sh --zookeeper localhost:2181 --topic ruby-p10 --describe
Topic:ruby-p10	PartitionCount:10	ReplicationFactor:3	Configs:
	Topic: ruby-p10	Partition: 0	Leader: 2	Replicas: 1,2,3	Isr: 2,1,3
	Topic: ruby-p10	Partition: 1	Leader: 2	Replicas: 2,3,1	Isr: 2,1,3
	Topic: ruby-p10	Partition: 2	Leader: 3	Replicas: 3,1,2	Isr: 2,1,3
	Topic: ruby-p10	Partition: 3	Leader: 1	Replicas: 1,3,2	Isr: 2,1,3
	Topic: ruby-p10	Partition: 4	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3
	Topic: ruby-p10	Partition: 5	Leader: 3	Replicas: 3,2,1	Isr: 2,1,3
	Topic: ruby-p10	Partition: 6	Leader: 2	Replicas: 1,2,3	Isr: 2,1,3
	Topic: ruby-p10	Partition: 7	Leader: 2	Replicas: 2,3,1	Isr: 2,1,3
	Topic: ruby-p10	Partition: 8	Leader: 3	Replicas: 3,1,2	Isr: 2,1,3
	Topic: ruby-p10	Partition: 9	Leader: 1	Replicas: 1,3,2	Isr: 2,1,3


As you can see the ISR is not evenly distributed, and since current codes always pick the first one. So if broker 1 is down, partition hosted by broker 1 will be changed to be broker 2 instead of evenly distributed to broker 2 and broker 3.

As times goes on, all the partition will be only hosted on one broker.
;;;","20/Jun/14 21:53;jackiewang518;One way to break this loop is to turn on ""auto.leader.rebalance.enable=true"". But the cluster still will be imbalance between two auto rebalances(I used 60 seconds as the interval).
The fix I provided is to make sure the partitions are even distributed within the live brokers between two auto rebalances.;;;","20/Jun/14 21:55;jackiewang518;Also, I experienced another issue is that auto rebalance sometime does not work for very long time and leave the imbalance (all partition on one broker) for hours. I will file a separate ticket for that.;;;","20/Jun/14 22:23;guozhang;Hi Jianwen,

When the topic is firstly created, the leaders should be balanced. Then when there is, say a soft failure on some brokers, its leading partitions will be migrated to others, and even when this broker resumes these partitions will not be migrated back, causing imbalance. And this is probably what you saw before.

Previously we do rebalance using the partition-reassignment-tool from time to time, then we created the auto-rebalance tool to ease this problem. Unfortunately there is an issue with this tool and we are currently fixing it:

https://issues.apache.org/jira/browse/KAFKA-1305

The fix should be released soon with 0.8.2, will that fully-working tool solve your issue then?;;;","20/Jun/14 22:42;jackiewang518;Hi Guozhang,
   The fully working auto rebalance will surely break that imbalance loop. But I like to have a fix to make sure the leader is evenly distributed within live brokers between two rebalances after one broker is down. As you can see, if the auto rebalance interval is set to 5 minutes, the imbalance case (all partitions on one broker) will stay for <= 5 minutes.

   The fix I provided is to solve that particular case.

   Thanks.

-jianwen;;;","20/Jun/14 22:55;guozhang;Cool, could you upload your patch as a file that is appliable to trunk?;;;","23/Jun/14 16:56;jackiewang518;The first live broker in ISR was used as new leader. Since
 the order of liveBrokersInIsr list is not event distributed within live
 brokers, all partitions will be hosted on one broker as time goes on.

Now use the first live replica broker in ISR (liveAssignedReplicas filtered by liveBrokersInIsr) as the replica order is strictly evenly distributed.
;;;","26/Jun/14 17:12;guozhang;Looks good to me. [~junrao] Could you take a look at this?;;;","03/Jul/14 04:56;junrao;Thanks for the patch. It's a nice fix! +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
source jar is empty,KAFKA-1502,12722834,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,20/Jun/14 17:40,09/Mar/15 08:43,14/Jul/23 05:39,30/Aug/14 16:14,0.8.2.0,,,0.8.2.0,,,,,,,build,,,0,newbie,,,"When doing a local publish, kafka_2.8.0-0.8.1.1-sources.jar only contains the following files.
META-INF/
META-INF/MANIFEST.MF
LICENSE
NOTICE
",,heavydawson,joestein,junrao,vanyatka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1362,,,,,,,,,,,,,"29/Aug/14 21:07;junrao;KAFKA-1502.patch;https://issues.apache.org/jira/secure/attachment/12665422/KAFKA-1502.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,401021,,,Sat Aug 30 16:14:47 UTC 2014,,,,,,,,,,"0|i1x12v:",401106,,joestein,,,,,,,,,,,,,,,,,,"25/Jul/14 07:22;heavydawson;Can't reproduce this on trunk or on the 0.8.1 branch. Seems to be resolved.;;;","29/Jul/14 11:39;vanyatka;http://repo1.maven.org/maven2/org/apache/kafka/kafka_2.10/0.8.1.1/kafka_2.10-0.8.1.1-sources.jar
Still empty, any ideas how to fix?

;;;","29/Aug/14 15:38;joestein;I recently did a push to maven for snapshot of 0.8.2 on trunk https://repository.apache.org/content/repositories/snapshots/org/apache/kafka/kafka_2.10/0.8.2-SNAPSHOT/ the source is empty 

[~jjkoshy] i am going to un-assign this from you so if someone wants to take it on, ok? [~edgefox] might have a chance to-do this next week;;;","29/Aug/14 21:07;junrao;Created reviewboard https://reviews.apache.org/r/25200/
 against branch origin/trunk;;;","30/Aug/14 16:14;joestein;Thanks Jun this looks good now

https://repository.apache.org/content/repositories/snapshots/org/apache/kafka/kafka_2.10/0.8.2-SNAPSHOT/

kafka_2.10-0.8.2-20140830.153910-2-sources.jar	Sat Aug 30 15:39:25 UTC 2014	509808	
kafka_2.10-0.8.2-20140830.153910-2-sources.jar.asc	Sat Aug 30 15:39:26 UTC 2014	821	

+1 committed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
new producer performance and bug improvements,KAFKA-1498,12722599,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,junrao,junrao,19/Jun/14 21:31,02/Sep/14 23:05,14/Jul/23 05:39,02/Sep/14 23:05,,,,,,,,,,,core,,,0,,,,"We have seen the following issues with the new producer.

1. The producer request can be significantly larger than the configured batch size.
2. The bottleneck in mirrormaker when there are keyed messages and compression is turned on.
3. The selector is woken up on every message in the new producer.
",,guozhang,junrao,kbanker,ross.black,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/14 21:40;guozhang;KAFKA-1498.patch;https://issues.apache.org/jira/secure/attachment/12653474/KAFKA-1498.patch","23/Jun/14 17:54;guozhang;KAFKA-1498.patch;https://issues.apache.org/jira/secure/attachment/12652011/KAFKA-1498.patch","25/Jun/14 23:45;guozhang;KAFKA-1498_2014-06-25_16:44:51.patch;https://issues.apache.org/jira/secure/attachment/12652522/KAFKA-1498_2014-06-25_16%3A44%3A51.patch","30/Jun/14 17:47;guozhang;KAFKA-1498_2014-06-30_10:47:17.patch;https://issues.apache.org/jira/secure/attachment/12653189/KAFKA-1498_2014-06-30_10%3A47%3A17.patch","30/Jun/14 22:48;guozhang;KAFKA-1498_2014-06-30_15:47:56.patch;https://issues.apache.org/jira/secure/attachment/12653257/KAFKA-1498_2014-06-30_15%3A47%3A56.patch","01/Jul/14 18:12;guozhang;KAFKA-1498_2014-07-01_11:12:41.patch;https://issues.apache.org/jira/secure/attachment/12653432/KAFKA-1498_2014-07-01_11%3A12%3A41.patch","19/Jun/14 21:34;junrao;kafka-1498.patch;https://issues.apache.org/jira/secure/attachment/12651500/kafka-1498.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,400790,,,Tue Jul 01 21:40:38 UTC 2014,,,,,,,,,,"0|i1wzon:",400880,,,,,,,,,,,,,,,,,,,,"19/Jun/14 21:34;junrao;Attach a draft patch of revision dcc88408c98a07cb9a816ab55cd81e55f1d2217d on Jun. 10.

Included in the patch:
1. Address the issue that a batch in the producer request can be significantly larger than the configured batch size.
This is done by patching MemoryRecords.hasRoom() and MemoryRecords.isFull().
2. Address the bottleneck in mirrormaker when there are keyed messages and compression is turned on.
Use a data channel per producer thread.
3. Address the issue that the selector is woken up on every message in the new producer. This is the trickiest part. The fix is the following.
(a) In KafkaProducer.send(), only wake up the selector if the batch becomes full during append.
(b) In Metadata.fetch(), force the selector to wake up if metadata is not available.
(c) In sender, calculate the select time dynamically in each iteration of the selector.poll() call. The select time is the minimal of the remaining linger time of all partitions and the metadata request. The select time is bounded by linger time. This is to handle the case that the selector is doing a long poll, a new messages is produced and no new messages come afterwards. We need to make sure that the message can be processed within the linger time.

This cover the following cases well.
3.1. If linger time is larger and there are lots of messages, the selector won't be woke up too frequently.
3.2. If linger time is small and there are lots of messages, the selector will be busy. However, this is expected.

This doesn't deal with the following case well.
3.3 If linger time is small and there are very few messages, the selector will still wake up every linger time. Not sure what's the best way to deal with this. One thing that I was thinking is to have a min_linger threshold. The selector will use a select time at least of min_linger, say 5ms, if there is nothing to do. In KafkaProducer.send(), if linger is configured to be larger than min_linger, wake up the selector on every message. This way, the selector will only be busy if there are lots of messages.

Not sure that I have thought through other potential timing issues.

4. Added a few missing ingraphs.

Other todos:
1. Metadata.needsUpdate should be renamed properly.
2. Methods with new parameters need new comments accordingly.
3. Metrics
3.1 In addition to record-size-max, add record-size-avg.
3.2 Rename incoming-bytes-rate and outgoing-bytes-rate to network-in-bytes-rate and network-out-bytes-rate
;;;","23/Jun/14 17:54;guozhang;Created reviewboard https://reviews.apache.org/r/22874/
 against branch origin/trunk;;;","25/Jun/14 23:45;guozhang;Updated reviewboard https://reviews.apache.org/r/22874/
 against branch origin/trunk;;;","30/Jun/14 17:47;guozhang;Updated reviewboard https://reviews.apache.org/r/22874/
 against branch origin/trunk;;;","30/Jun/14 22:48;guozhang;Updated reviewboard https://reviews.apache.org/r/22874/
 against branch origin/trunk;;;","01/Jul/14 18:12;guozhang;Updated reviewboard https://reviews.apache.org/r/22874/
 against branch origin/trunk;;;","01/Jul/14 21:40;guozhang;Created reviewboard https://reviews.apache.org/r/23215/
 against branch origin/trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using batch message in sync producer only sends the first message if we use a Scala Stream as the argument ,KAFKA-1496,12721673,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,gliao,gliao,gliao,17/Jun/14 12:58,15/Sep/14 03:41,14/Jul/23 05:39,15/Sep/14 03:41,0.8.1,,,,,,,,,,producer ,,,0,,,,"I am passing a Scala Stream to the producer as followed:

producer.send(events.toSeq:*)

The events was a Scala Stream of KeyedMessages. In sync mode, it will only send one message to Kafka while in async mode it was fine. I tracked down the issue and I'm attaching a fix for it.",Scala 2.10,gliao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/14 16:52;gliao;kafka_1496v2.patch;https://issues.apache.org/jira/secure/attachment/12664169/kafka_1496v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,399869,,,Mon Sep 15 03:41:27 UTC 2014,,,,,,,,,,"0|i1wu4n:",399977,,nehanarkhede,,,,,,,,,,,,,,,,,,"13/Aug/14 23:19;nehanarkhede;Thanks for the patch, [~gliao]. Do you think you can add a unit test to your patch as well?;;;","25/Aug/14 16:52;gliao;This new patch includes a test. Saw that there was one already, so I modified the existing one to test for Stream. Patch was redone on top of latest trunk.;;;","15/Sep/14 03:41;nehanarkhede;+1. Pushed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsumerMetadataResponse is not read completely,KAFKA-1491,12720694,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jjkoshy,jjkoshy,11/Jun/14 21:01,22/Jul/14 14:27,14/Jul/23 05:39,27/Jun/14 18:11,,,,0.8.2.0,,,,,,,,,,0,,,,"This is a regression after KAFKA-1437

The broker always populates the coordinator broker field, but the consumer may do a partial read if error code is non-zero. It should always read the field or we will probably end up with a buffer overflow exception of some sort when reading from a response that has a non-zero error code.",,jjkoshy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/14 23:18;jjkoshy;KAFKA-1491.patch;https://issues.apache.org/jira/secure/attachment/12649942/KAFKA-1491.patch","11/Jun/14 23:17;jjkoshy;KAFKA-1491.patch;https://issues.apache.org/jira/secure/attachment/12649941/KAFKA-1491.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,398893,,,Fri Jun 27 18:11:01 UTC 2014,,,,,,,,,,"0|i1wo8n:",399015,,,,,,,,,,,,,,,,,,,,"11/Jun/14 23:06;jjkoshy;So it turns out this is not as bad as I thought.

We read ConsumerMetadatResponse's through a request channel in ClientUtils.
The entire response is received and we just call readFrom on the underlying
buffer. It's fine if we ignore the remaining since retries create a new
BoundedByteBufferReceive.

I verified this locally.

That said, we may as well change the code to always read the coordinator.

;;;","11/Jun/14 23:17;jjkoshy;Created reviewboard  against branch origin/trunk;;;","11/Jun/14 23:18;jjkoshy;Created reviewboard https://reviews.apache.org/r/22482/diff/
 against branch origin/trunk;;;","27/Jun/14 18:11;jjkoshy;Committed (changed the log level to debug - Neha's review comment).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove gradlew initial setup output from source distribution,KAFKA-1490,12720594,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,edgefox,joestein,joestein,11/Jun/14 14:33,25/Mar/16 16:31,14/Jul/23 05:39,23/Sep/14 16:06,,,,0.8.2.0,,,,,,,,,,0,,,,Our current source releases contains lots of stuff in the gradle folder we do not need,,copester,cos,edgefox,guozhang,jfarrell,jghoman,jimmiebfulton,joestein,junrao,omkreddy,sslavic,szczepiq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SAMZA-283,,,KAFKA-2124,,,,,,KAFKA-1559,,,,"16/Sep/14 18:28;edgefox;KAFKA-1490-2.patch;https://issues.apache.org/jira/secure/attachment/12669124/KAFKA-1490-2.patch","16/Sep/14 16:50;edgefox;KAFKA-1490.patch;https://issues.apache.org/jira/secure/attachment/12669091/KAFKA-1490.patch","23/Sep/14 09:52;edgefox;rb25703.patch;https://issues.apache.org/jira/secure/attachment/12670679/rb25703.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,398793,,,Fri Mar 25 16:31:13 UTC 2016,,,,,,,,,,"0|i1wnmv:",398915,,joestein,,,,,,,,,,,,,,,,,,"11/Jun/14 16:04;jghoman;This is being discussed for other projects, and how Aurora dealt with this problem [on the incubator list|http://mail-archives.apache.org/mod_mbox/incubator-general/201406.mbox/%3CCADiKvVs%3DtKDbp3TWRnxds5dVepqcX4kWeYbj7xUx%2BZoDNM_Lyg%40mail.gmail.com%3E].;;;","11/Jun/14 17:58;jghoman;We're also discussing this in SAMZA-283.;;;","28/Jul/14 07:01;sslavic;SAMZA-283 solution doesn't make sense to me - why would one want to bootstrap gradle wrapper if one already has gradle installed?

I've raised the issue on Gradle forum (see [here|http://forums.gradle.org/gradle/topics/bootstrap_gradle_wrapper_with_gradle_wrapper_scripts]).;;;","04/Sep/14 21:55;guozhang;Should this be done before 0.8.2?;;;","16/Sep/14 16:52;edgefox;I've deleted the gradlew output jar file and left properties file for configuration needs.
If you want to use local gradle distribution use gradle, otherwise ./gradlew will download gradle with version specified in properties file;;;","16/Sep/14 17:24;edgefox;Sorry guys for my previous post. I've confused "".gradle"" and ""gradle"" directories.
So, my method should not work. However, I think that this issue is not related to Kafka project and should be resolved in Gradle instead.

Furthermore, if we eventually resolve this issue, than it could cause gradle version problems in future for Gradle 1.x and 2.x users, since some plugins work under 1.x, but not under 2.x.
;;;","16/Sep/14 17:28;joestein;it is our problem, we need to resolve this we shouldn't be shipping binary in source;;;","16/Sep/14 18:28;edgefox;Ok, I applied the patch which replays the functionality from Samza.;;;","16/Sep/14 18:40;edgefox;https://reviews.apache.org/r/25703/;;;","23/Sep/14 04:25;joestein;[~edgefox] can you rebase please, unless something else causing failing to apply otherwise looks good I can test and commit tomorrow (so for such long delay was traveling last week);;;","23/Sep/14 09:52;edgefox;Rebased to the latest trunk. Reviewboard has been updated accordingly.;;;","23/Sep/14 16:06;joestein;+1 committed to trunk, also minor update to readme 

{code}

diff --git a/README.md b/README.md
index 18a65b1..8e50945 100644
--- a/README.md
+++ b/README.md
@@ -2,6 +2,13 @@ Apache Kafka
 =================
 See our [web site](http://kafka.apache.org) for details on the project.
 
+You need to have [gradle](http://www.gradle.org/installation) installed.
+
+### First bootstrap and download the wrapper ###
+    gradle
+
+Now everything else will work
+
 ### Building a jar and running it ###
     ./gradlew jar  
 

{code};;;","23/Sep/14 16:40;copester;Should there have been a wrapper.gradle file added in that patch?
build.gradle L40:
{code}
apply from: file('wrapper.gradle')
{code}
;;;","23/Sep/14 16:46;joestein;yup, thanks missed the git add, pushed the file now;;;","23/Sep/14 16:47;copester;Ok so the wrapper.gradle was in the attached patch on this issue, just not in the github commit. However, I there may be another change that didn't make it in to the commit?
{noformat}
ubuntu@ip-10-183-61-60:~/kafka$ gradle

FAILURE: Build failed with an exception.

* Where:
Script '/home/ubuntu/kafka/gradle/license.gradle' line: 2

* What went wrong:
A problem occurred evaluating script.
> Could not find method create() for arguments [downloadLicenses, class nl.javadude.gradle.plugins.license.DownloadLicenses] on task set.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED

Total time: 3.303 secs
{noformat};;;","23/Sep/14 16:53;joestein;can you clone latest trunk now please, I did fresh and it is working for me

{code}

new-host:apache_kafka joestein$ git clone https://git-wip-us.apache.org/repos/asf/kafka.git trunk
Cloning into 'trunk'...
remote: Counting objects: 20685, done.
remote: Compressing objects: 100% (10682/10682), done.
Receiving objects: 100% (20685/20685), 14.96 MiB | 620 KiB/s, done.
remote: Total 20685 (delta 12343), reused 11450 (delta 7041)
Resolving deltas: 100% (12343/12343), done.
new-host:apache_kafka joestein$ cd trunk/
new-host:trunk joestein$ gradle
Building project 'core' with Scala version 2.10.1
:downloadWrapper

BUILD SUCCESSFUL

Total time: 11.1 secs
new-host:trunk joestein$ ./gradlew jar
Building project 'core' with Scala version 2.10.1
:clients:compileJava
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:clients:processResources UP-TO-DATE
:clients:classes
:clients:jar
:contrib:compileJava UP-TO-DATE
:contrib:processResources UP-TO-DATE
:contrib:classes UP-TO-DATE
:contrib:jar
:core:compileJava UP-TO-DATE
:core:compileScala
/opt/apache_kafka/trunk/core/src/main/scala/kafka/admin/AdminUtils.scala:259: non-variable type argument String in type pattern scala.collection.Map[String,_] is unchecked since it is eliminated by erasure
        case Some(map: Map[String, _]) => 
                       ^
/opt/apache_kafka/trunk/core/src/main/scala/kafka/admin/AdminUtils.scala:262: non-variable type argument String in type pattern scala.collection.Map[String,String] is unchecked since it is eliminated by erasure
            case Some(config: Map[String, String]) =>
                              ^
/opt/apache_kafka/trunk/core/src/main/scala/kafka/server/KafkaServer.scala:142: a pure expression does nothing in statement position; you may be omitting necessary parentheses
    ControllerStats.uncleanLeaderElectionRate
                    ^
/opt/apache_kafka/trunk/core/src/main/scala/kafka/server/KafkaServer.scala:143: a pure expression does nothing in statement position; you may be omitting necessary parentheses
    ControllerStats.leaderElectionTimer
                    ^
/opt/apache_kafka/trunk/core/src/main/scala/kafka/utils/Utils.scala:81: a pure expression does nothing in statement position; you may be omitting necessary parentheses
    daemonThread(name, runnable(fun))
                                ^
/opt/apache_kafka/trunk/core/src/main/scala/kafka/network/SocketServer.scala:359: Visited SCOPE_EXIT before visiting corresponding SCOPE_ENTER. SI-6049
      maybeCloseOldestConnection
      ^
/opt/apache_kafka/trunk/core/src/main/scala/kafka/network/SocketServer.scala:379: Visited SCOPE_EXIT before visiting corresponding SCOPE_ENTER. SI-6049
      try {
      ^
there were 12 feature warning(s); re-run with -feature for details
8 warnings found
:core:processResources UP-TO-DATE
:core:classes
:core:copyDependantLibs
:core:jar
:examples:compileJava
:examples:processResources UP-TO-DATE
:examples:classes
:examples:jar
:contrib:hadoop-consumer:compileJava
Note: Some input files use or override a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
Note: Some input files use unchecked or unsafe operations.
Note: Recompile with -Xlint:unchecked for details.
:contrib:hadoop-consumer:processResources UP-TO-DATE
:contrib:hadoop-consumer:classes
:contrib:hadoop-consumer:jar
:contrib:hadoop-producer:compileJava
:contrib:hadoop-producer:processResources UP-TO-DATE
:contrib:hadoop-producer:classes
:contrib:hadoop-producer:jar

BUILD SUCCESSFUL

Total time: 2 mins 3.497 secs

{code};;;","23/Sep/14 17:25;copester;I'm still getting that DownloadLicenses error. By default it's building with Scala 2.9.1, but trying Scala 2.10.1 like you also gets the same error. Do you mind listing your dependency versions? There may be something out of date on my end.;;;","23/Sep/14 17:35;joestein;I think your trunk is out of date we bumped the Scala version back in August

{code}

4d075971 (Ivan Lyutov 2014-08-10 21:20:30 -0700 18) scalaVersion=2.10.1

{code}

here are my dependencies for core from a fresh clone of trunk

{code}

new-host:trunk joestein$ ./gradlew core:dependencies
Building project 'core' with Scala version 2.10.1
:core:dependencies

------------------------------------------------------------
Project :core
------------------------------------------------------------

archives - Configuration for archive artifacts.
No dependencies

compile - Compile classpath for source set 'main'.
+--- project :clients
|    +--- org.slf4j:slf4j-api:1.7.6
|    +--- org.xerial.snappy:snappy-java:1.1.1.3
|    \--- net.jpountz.lz4:lz4:1.2.0
+--- org.scala-lang:scala-library:2.10.1
+--- org.apache.zookeeper:zookeeper:3.4.6
|    +--- org.slf4j:slf4j-api:1.6.1 -> 1.7.6
|    +--- org.slf4j:slf4j-log4j12:1.6.1
|    |    +--- org.slf4j:slf4j-api:1.6.1 -> 1.7.6
|    |    \--- log4j:log4j:1.2.16
|    +--- log4j:log4j:1.2.16
|    \--- io.netty:netty:3.7.0.Final
+--- com.101tec:zkclient:0.3
|    +--- org.apache.zookeeper:zookeeper:3.3.1 -> 3.4.6 (*)
|    \--- log4j:log4j:1.2.14 -> 1.2.16
+--- com.yammer.metrics:metrics-core:2.2.0
|    \--- org.slf4j:slf4j-api:1.7.2 -> 1.7.6
\--- net.sf.jopt-simple:jopt-simple:3.2

default - Configuration for default artifacts.
+--- project :clients
|    +--- org.slf4j:slf4j-api:1.7.6
|    +--- org.xerial.snappy:snappy-java:1.1.1.3
|    \--- net.jpountz.lz4:lz4:1.2.0
+--- org.scala-lang:scala-library:2.10.1
+--- org.apache.zookeeper:zookeeper:3.4.6
|    +--- org.slf4j:slf4j-api:1.6.1 -> 1.7.6
|    +--- org.slf4j:slf4j-log4j12:1.6.1
|    |    +--- org.slf4j:slf4j-api:1.6.1 -> 1.7.6
|    |    \--- log4j:log4j:1.2.16
|    +--- log4j:log4j:1.2.16
|    \--- io.netty:netty:3.7.0.Final
+--- com.101tec:zkclient:0.3
|    +--- org.apache.zookeeper:zookeeper:3.3.1 -> 3.4.6 (*)
|    \--- log4j:log4j:1.2.14 -> 1.2.16
+--- com.yammer.metrics:metrics-core:2.2.0
|    \--- org.slf4j:slf4j-api:1.7.2 -> 1.7.6
\--- net.sf.jopt-simple:jopt-simple:3.2

runtime - Runtime classpath for source set 'main'.
+--- project :clients
|    +--- org.slf4j:slf4j-api:1.7.6
|    +--- org.xerial.snappy:snappy-java:1.1.1.3
|    \--- net.jpountz.lz4:lz4:1.2.0
+--- org.scala-lang:scala-library:2.10.1
+--- org.apache.zookeeper:zookeeper:3.4.6
|    +--- org.slf4j:slf4j-api:1.6.1 -> 1.7.6
|    +--- org.slf4j:slf4j-log4j12:1.6.1
|    |    +--- org.slf4j:slf4j-api:1.6.1 -> 1.7.6
|    |    \--- log4j:log4j:1.2.16
|    +--- log4j:log4j:1.2.16
|    \--- io.netty:netty:3.7.0.Final
+--- com.101tec:zkclient:0.3
|    +--- org.apache.zookeeper:zookeeper:3.3.1 -> 3.4.6 (*)
|    \--- log4j:log4j:1.2.14 -> 1.2.16
+--- com.yammer.metrics:metrics-core:2.2.0
|    \--- org.slf4j:slf4j-api:1.7.2 -> 1.7.6
\--- net.sf.jopt-simple:jopt-simple:3.2

signatures
No dependencies

testCompile - Compile classpath for source set 'test'.
+--- project :clients
|    +--- org.slf4j:slf4j-api:1.7.6
|    +--- org.xerial.snappy:snappy-java:1.1.1.3
|    \--- net.jpountz.lz4:lz4:1.2.0
+--- org.scala-lang:scala-library:2.10.1
+--- org.apache.zookeeper:zookeeper:3.4.6
|    +--- org.slf4j:slf4j-api:1.6.1 -> 1.7.6
|    +--- org.slf4j:slf4j-log4j12:1.6.1
|    |    +--- org.slf4j:slf4j-api:1.6.1 -> 1.7.6
|    |    \--- log4j:log4j:1.2.16
|    +--- log4j:log4j:1.2.16
|    \--- io.netty:netty:3.7.0.Final
+--- com.101tec:zkclient:0.3
|    +--- org.apache.zookeeper:zookeeper:3.3.1 -> 3.4.6 (*)
|    \--- log4j:log4j:1.2.14 -> 1.2.16
+--- com.yammer.metrics:metrics-core:2.2.0
|    \--- org.slf4j:slf4j-api:1.7.2 -> 1.7.6
+--- net.sf.jopt-simple:jopt-simple:3.2
+--- junit:junit:4.1
+--- org.easymock:easymock:3.0
|    +--- cglib:cglib-nodep:2.2
|    \--- org.objenesis:objenesis:1.2
+--- org.objenesis:objenesis:1.2
\--- org.scalatest:scalatest_2.10:1.9.1
     +--- org.scala-lang:scala-library:2.10.0 -> 2.10.1
     +--- org.scala-lang:scala-actors:2.10.0
     |    \--- org.scala-lang:scala-library:2.10.0 -> 2.10.1
     \--- org.scala-lang:scala-reflect:2.10.0
          \--- org.scala-lang:scala-library:2.10.0 -> 2.10.1

testRuntime - Runtime classpath for source set 'test'.
+--- project :clients
|    +--- org.slf4j:slf4j-api:1.7.6
|    +--- org.xerial.snappy:snappy-java:1.1.1.3
|    \--- net.jpountz.lz4:lz4:1.2.0
+--- org.scala-lang:scala-library:2.10.1
+--- org.apache.zookeeper:zookeeper:3.4.6
|    +--- org.slf4j:slf4j-api:1.6.1 -> 1.7.6
|    +--- org.slf4j:slf4j-log4j12:1.6.1 -> 1.7.6
|    |    +--- org.slf4j:slf4j-api:1.7.6
|    |    \--- log4j:log4j:1.2.17
|    +--- log4j:log4j:1.2.16 -> 1.2.17
|    \--- io.netty:netty:3.7.0.Final
+--- com.101tec:zkclient:0.3
|    +--- org.apache.zookeeper:zookeeper:3.3.1 -> 3.4.6 (*)
|    \--- log4j:log4j:1.2.14 -> 1.2.17
+--- com.yammer.metrics:metrics-core:2.2.0
|    \--- org.slf4j:slf4j-api:1.7.2 -> 1.7.6
+--- net.sf.jopt-simple:jopt-simple:3.2
+--- junit:junit:4.1
+--- org.easymock:easymock:3.0
|    +--- cglib:cglib-nodep:2.2
|    \--- org.objenesis:objenesis:1.2
+--- org.objenesis:objenesis:1.2
+--- org.scalatest:scalatest_2.10:1.9.1
|    +--- org.scala-lang:scala-library:2.10.0 -> 2.10.1
|    +--- org.scala-lang:scala-actors:2.10.0
|    |    \--- org.scala-lang:scala-library:2.10.0 -> 2.10.1
|    \--- org.scala-lang:scala-reflect:2.10.0
|         \--- org.scala-lang:scala-library:2.10.0 -> 2.10.1
\--- org.slf4j:slf4j-log4j12:1.7.6 (*)

zinc - The Zinc incremental compiler to be used for this Scala project.
\--- com.typesafe.zinc:zinc:0.3.1
     +--- org.scala-lang:scala-library:2.10.3
     +--- com.typesafe.sbt:incremental-compiler:0.13.1
     |    +--- com.typesafe.sbt:sbt-interface:0.13.1
     |    |    \--- org.scala-lang:scala-library:2.10.3
     |    +--- org.scala-lang:scala-library:2.10.3
     |    \--- org.scala-lang:scala-compiler:2.10.3
     |         +--- org.scala-lang:scala-library:2.10.3
     |         \--- org.scala-lang:scala-reflect:2.10.3
     |              \--- org.scala-lang:scala-library:2.10.3
     \--- com.typesafe.sbt:compiler-interface:0.13.1
          \--- org.scala-lang:scala-library:2.10.3

(*) - dependencies omitted (listed previously)

BUILD SUCCESSFUL

Total time: 10.48 secs

{code}

;;;","23/Sep/14 18:52;guozhang;Hey Joe,

After pulling in this patch the gradlew command seems not working for us any more:

{code}
./gradlew jar
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/gradle/wrapper/GradleWrapperMain
Caused by: java.lang.ClassNotFoundException: org.gradle.wrapper.GradleWrapperMain
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
Could not find the main class: org.gradle.wrapper.GradleWrapperMain.  Program will exit.
{code}

Do you know why?;;;","23/Sep/14 18:58;joestein;1) You now need to have gradle installed http://www.gradle.org/installation
2) after that 

{code}
gradle
{code}

That will execute the default task which is to download the wrapper and then everything else works the same. :)

It is kind of like a new bootstrap to get things working required now.

I updated README to explain https://github.com/apache/kafka/blob/trunk/README.md#apache-kafka 

;;;","23/Sep/14 19:00;copester;[~guozhang], you need to run gradle first now. The Readme has been updated.
{noformat}
gradle
./gradlew jar
{noformat};;;","23/Sep/14 22:00;junrao;Joe, Chris,

It wasn't obvious that gradle needs to be run from the kafka source dir. Added that to README to make this clear.

Thanks,;;;","24/Sep/14 11:00;szczepiq;Hey guys,

This is Szczepan from the gradle core team :) This issue was brought to my attention thanks to Guozhang Wang. I wanted to share my feedback about the current solution.

To me, it feels like the current approach defeats the purpose of the wrapper. It looks like samza team has solved the problem by removing the gradle wrapper files (including jar) from the source _distribution_ while keeping the wrapper files (and jar) in the source code. Is this approach not feasible with kafka?;;;","24/Sep/14 13:05;joestein;Thanks [~szczepiq] for jumping in I think the current solution in any form for Apache projects is kludgy. :(  Besides Samza I just checked and Aurora does it that way too (keeping jar files checked in but excluding from source release) but I am not sure that is better though.

For source releases we have to exclude it and we will have to explain to folks they have to bootstrap (and they will ask). I feel like there would be less confusion and questions from folks using a source release if their experience with the repo is the same.

My preference would be to not have to ""bootstrap"" at all.  Maybe the gradle team could modify things so we could pull the wrapper jar first without it being there.  Any chance of that? :)  It seems like a legitimate ask since ""source only releases"" are such a big thing it is is kludgy now no matter where you cut it... this solution just makes it consistency kludgy.

I think this solution favors consistency of how to interact with the repo and source release which leads to less community confusion.

Also, the way we have always done releases https://cwiki.apache.org/confluence/display/KAFKA/Release+Process has been to copy the repository 1:1 for the source release.  This means what is tagged is exactly what is in the source release.  Technically speaking we could change that (of course) and create a source release task but not sure if we would want to-do that. If we decide to keep the jar in the repo we need to make sure the release manager has a way to create a source release differently than has been done before.
;;;","24/Sep/14 14:11;szczepiq;Hey,

Thanks for info! What's the purpose of 'source releases'? E.g. what's the benefit for developer over just cloning the repo?;;;","24/Sep/14 14:26;joestein;A source release is meant for general public consumption and use and not just for developers.  Technically speaking any version we put out is really about the source release the binaries are only an aid for folks that don't have the tools to make their own build from source http://www.apache.org/dev/release.html#what and http://www.apache.org/dev/release.html#what-must-every-release-contain

Any chance of the core gradle team making the gradle-wrapper.jar hosted and signed and then it can get downloaded (and signatures checked) if not found?  I think this would start to help wider adoption within apache projects and create a better uses experience for everyone always using gradle (assuming that approach satisfies apache requirements I believe it would best to get more Apache folks to verify that which we could do of course).  Let me know if you can get that change going and I can kick a discussion off (having a link to your ticketing change would help drive it) to make sure it will be workable within the Apache foundation (prior to you doing the work).;;;","24/Sep/14 14:40;jfarrell;I do not see what the issue is with having a bootstrapping step for the source release, lots of projects have this type of setup requirement. And with a fixed wrapper task it guarantees that everyone will be using the same version and meets the ASF source dist requirements;;;","24/Sep/14 15:42;omkreddy;1.  I am getting following exception after updating my local repo to latest. Do i need to clone new repo to use ./gradlew command?

{noformat}
 gradle

FAILURE: Build failed with an exception.

* Where:
Build file '/safe/KAFKA/kafkaTrunk/build.gradle' line: 40

* What went wrong:
A problem occurred evaluating root project 'kafkaTrunk'.
> Could not read script '/safe/KAFKA/kafkaTrunk/wrapper.gradle' as it does not exist.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

BUILD FAILED

Total time: 3.184 secs

$./gradlew jar
Error: Could not find or load main class org.gradle.wrapper.GradleWrapperMain
{noformat}

2. I cloned the latest trunk and installed gradle. On running ./gradlew, it is downloading gradle.zip. Is it correct?

{noformat}
gradle
Building project 'core' with Scala version 2.10.1
:downloadWrapper UP-TO-DATE

BUILD SUCCESSFUL

Total time: 5.641 secs
$./gradlew jar
Downloading https://services.gradle.org/distributions/gradle-2.0-bin.zip
..........................................................................
{noformat};;;","24/Sep/14 15:52;joestein;In your first step you needed to have gradle installed, yes.

Yes, downloading the distribution for the gradle version we are using is the right thing to see and you should be good to go now.

I confirmed this works with a pull and not requiring a new clone (at least on the setup I have)

{code}

new-host:trunk joestein$ git pull
remote: Counting objects: 19, done.
remote: Compressing objects: 100% (13/13), done.
remote: Total 13 (delta 10), reused 0 (delta 0)
Unpacking objects: 100% (13/13), done.
From http://git-wip-us.apache.org/repos/asf/kafka
   6d70575..27bc372  trunk      -> origin/trunk
Updating 6d70575..27bc372
Fast-forward
 README.md                                |   8 ++++++++
 build.gradle                             |   5 +++++
 gradle/wrapper/gradle-wrapper.jar        | Bin 49875 -> 0 bytes
 gradle/wrapper/gradle-wrapper.properties |   6 ------
 gradlew                                  |   2 +-
 gradlew.bat                              | 180 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++------------------------------------------------------------------------------------------
 wrapper.gradle                           |  25 +++++++++++++++++++++++++
 7 files changed, 129 insertions(+), 97 deletions(-)
 delete mode 100644 gradle/wrapper/gradle-wrapper.jar
 delete mode 100644 gradle/wrapper/gradle-wrapper.properties
 create mode 100644 wrapper.gradle
new-host:trunk joestein$ gradle
Building project 'core' with Scala version 2.10.1
:downloadWrapper

BUILD SUCCESSFUL

Total time: 15.882 secs
new-host:trunk joestein$ ./gradlew clean
Building project 'core' with Scala version 2.10.1
:clients:clean
:contrib:clean
:core:clean
:examples:clean
:contrib:hadoop-consumer:clean
:contrib:hadoop-producer:clean

BUILD SUCCESSFUL

Total time: 14.111 secs

{code}

If there is something else/different you think can be added to the README https://github.com/apache/kafka/blob/trunk/README.md please suggest however; I think Jun's update was good for more clarity and it is makes sense.;;;","24/Sep/14 19:21;szczepiq;Thanks for info. I'll send an email to the gradle dev mailing list. Not sure though if something will happen in near future regarding the wrapper (I believe the priorities are elsewhere atm).

Cheers!;;;","24/Sep/14 22:03;joestein;[~szczepiq] Thanks! Hans just shot me the thread discussion about this http://gradle.1045684.n5.nabble.com/wrapper-jar-td5713148.html so in the future we can ./gradlew jar and if gradle-wrapper.jar is not found it will download it :) exciting!  will continue to bootstrap for now.;;;","03/Oct/14 10:52;szczepiq;Hey

The current bootstrap workflow in kafka is something like. The user:
- clones the repo
- looks into the 'build.gradle' file to find out which Gradle version he needs
- downloads this version of gradle (or maybe already has it)
- sets up this version of gradle, ensures it is on the PATH, etc.
- invokes this version of gradle to grab the wrapper
- now can build kafka OS via ./gradlew

Steps like above increase complexity and offer ways to break stuff (e.g. wrong version of gradle on PATH causing unexpected behavior, etc.). The wrapper was invented to avoid those kind of situations.

It's a bit of a shame that the jar file is involved - I would much prefer if there was no need for any jar to have 'wrapper' functionality available. Hopefully, in future version of Gradle it will be improved!;;;","14/Apr/15 22:36;jimmiebfulton;This breaks the expected gradlew contract, and makes for a poor user experience.  Every other project I've run into containing a gradlew script ""just works"".  If a user has to follow the steps as provided by [~szczepiq@gmail.com] above, there is very little point in gradlew, and having to switch Gradle versions for a specific project is very invasive to an existing developer environment.;;;","14/Apr/15 22:53;jghoman;bq. The current bootstrap workflow in kafka is something like. The user:
Not quite.  For repos, the gradlew script is still included.  However, I just checked it out and it's not running for me right out of the box.  This is a bug and I'll look into it.

Current bootstrap workflow for cloning a repo (assuming gradlew is working...):
* Clone repo
* Do whatever you like with ./gradlew

Current bootstrap workflow for building from a source release:
* Have some version of Gradle installed - yeah, this is annoying but for the reasons discussed above, currently unavoidable...
* Untar source release
* gradle downloadWrapper - This task should probably be moved to a separate boostrap.gradle file, as Samza has done, to be as simple and therefore as compatible with as many versions of Gradle as possible;;;","14/Apr/15 22:56;jghoman;Linking to issue to track current gradlew splat.;;;","25/Mar/16 16:31;cos;I came across this long closed issue because of BIGTOP-2365 and I should say that there's an easy way out of the situation. In Bigtop we simply have a modified version of the wrapper that download all required binaries unless they are present. The wrapper is source only, hence no binaries are getting checked into our repo. Perhaps, something to look at. Cheers.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stop using dashes AND underscores as separators in MBean names,KAFKA-1481,12718028,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,vladimir.tretyakov,otis,otis,03/Jun/14 00:13,18/Mar/15 20:54,14/Jul/23 05:39,20/Nov/14 02:00,0.8.1.1,,,0.8.2.0,0.9.0.0,,,,,,core,,,0,patch,,,"MBeans should not use dashes or underscores as separators because these characters are allowed in hostnames, topics, group and consumer IDs, etc., and these are embedded in MBeans names making it impossible to parse out individual bits from MBeans.

Perhaps a pipe character should be used to avoid the conflict. 

This looks like a major blocker because it means nobody can write Kafka 0.8.x monitoring tools unless they are doing it for themselves AND do not use dashes AND do not use underscores.

See: http://search-hadoop.com/m/4TaT4lonIW",,Bmis13,diederik,guozhang,hakman,jjkoshy,joestein,jonbringhurst,junrao,miguno,nehanarkhede,otis,qwertymaniac,ScottReynolds,vladimir.tretyakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1100,KAFKA-1768,,,,,,,,,,,,"06/Jun/14 10:20;vladimir.tretyakov;KAFKA-1481_2014-06-06_13-06-35.patch;https://issues.apache.org/jira/secure/attachment/12648621/KAFKA-1481_2014-06-06_13-06-35.patch","13/Oct/14 15:28;vladimir.tretyakov;KAFKA-1481_2014-10-13_18-23-35.patch;https://issues.apache.org/jira/secure/attachment/12674516/KAFKA-1481_2014-10-13_18-23-35.patch","14/Oct/14 19:08;vladimir.tretyakov;KAFKA-1481_2014-10-14_21-53-35.patch;https://issues.apache.org/jira/secure/attachment/12674821/KAFKA-1481_2014-10-14_21-53-35.patch","15/Oct/14 07:55;vladimir.tretyakov;KAFKA-1481_2014-10-15_10-23-35.patch;https://issues.apache.org/jira/secure/attachment/12674949/KAFKA-1481_2014-10-15_10-23-35.patch","20/Oct/14 20:31;vladimir.tretyakov;KAFKA-1481_2014-10-20_23-14-35.patch;https://issues.apache.org/jira/secure/attachment/12675934/KAFKA-1481_2014-10-20_23-14-35.patch","21/Oct/14 06:42;vladimir.tretyakov;KAFKA-1481_2014-10-21_09-14-35.patch;https://issues.apache.org/jira/secure/attachment/12676039/KAFKA-1481_2014-10-21_09-14-35.patch","30/Oct/14 18:44;vladimir.tretyakov;KAFKA-1481_2014-10-30_21-35-43.patch;https://issues.apache.org/jira/secure/attachment/12678267/KAFKA-1481_2014-10-30_21-35-43.patch","31/Oct/14 11:14;vladimir.tretyakov;KAFKA-1481_2014-10-31_14-35-43.patch;https://issues.apache.org/jira/secure/attachment/12678454/KAFKA-1481_2014-10-31_14-35-43.patch","03/Nov/14 13:43;vladimir.tretyakov;KAFKA-1481_2014-11-03_16-39-41_doc.patch;https://issues.apache.org/jira/secure/attachment/12678927/KAFKA-1481_2014-11-03_16-39-41_doc.patch","03/Nov/14 14:05;vladimir.tretyakov;KAFKA-1481_2014-11-03_17-02-23.patch;https://issues.apache.org/jira/secure/attachment/12678930/KAFKA-1481_2014-11-03_17-02-23.patch","10/Nov/14 18:14;vladimir.tretyakov;KAFKA-1481_2014-11-10_20-39-41_doc.patch;https://issues.apache.org/jira/secure/attachment/12680615/KAFKA-1481_2014-11-10_20-39-41_doc.patch","10/Nov/14 18:14;vladimir.tretyakov;KAFKA-1481_2014-11-10_21-02-23.patch;https://issues.apache.org/jira/secure/attachment/12680614/KAFKA-1481_2014-11-10_21-02-23.patch","14/Nov/14 13:48;vladimir.tretyakov;KAFKA-1481_2014-11-14_16-33-03.patch;https://issues.apache.org/jira/secure/attachment/12681538/KAFKA-1481_2014-11-14_16-33-03.patch","14/Nov/14 13:48;vladimir.tretyakov;KAFKA-1481_2014-11-14_16-39-41_doc.patch;https://issues.apache.org/jira/secure/attachment/12681539/KAFKA-1481_2014-11-14_16-39-41_doc.patch","17/Nov/14 11:36;vladimir.tretyakov;KAFKA-1481_2014-11-17_14-33-03.patch;https://issues.apache.org/jira/secure/attachment/12681882/KAFKA-1481_2014-11-17_14-33-03.patch","19/Nov/14 13:10;vladimir.tretyakov;KAFKA-1481_2014-11-19_16-03-03_trunk.patch;https://issues.apache.org/jira/secure/attachment/12682416/KAFKA-1481_2014-11-19_16-03-03_trunk.patch","14/Oct/14 19:08;vladimir.tretyakov;KAFKA-1481_IDEA_IDE_2014-10-14_21-53-35.patch;https://issues.apache.org/jira/secure/attachment/12674822/KAFKA-1481_IDEA_IDE_2014-10-14_21-53-35.patch","15/Oct/14 07:55;vladimir.tretyakov;KAFKA-1481_IDEA_IDE_2014-10-15_10-23-35.patch;https://issues.apache.org/jira/secure/attachment/12674948/KAFKA-1481_IDEA_IDE_2014-10-15_10-23-35.patch","20/Oct/14 17:30;vladimir.tretyakov;KAFKA-1481_IDEA_IDE_2014-10-20_20-14-35.patch;https://issues.apache.org/jira/secure/attachment/12675877/KAFKA-1481_IDEA_IDE_2014-10-20_20-14-35.patch","20/Oct/14 20:31;vladimir.tretyakov;KAFKA-1481_IDEA_IDE_2014-10-20_23-14-35.patch;https://issues.apache.org/jira/secure/attachment/12675933/KAFKA-1481_IDEA_IDE_2014-10-20_23-14-35.patch","07/Nov/14 00:02;jjkoshy;alternateLayout1.png;https://issues.apache.org/jira/secure/attachment/12680011/alternateLayout1.png","07/Nov/14 00:02;jjkoshy;alternateLayout2.png;https://issues.apache.org/jira/secure/attachment/12680012/alternateLayout2.png","07/Nov/14 00:02;jjkoshy;diff-for-alternate-layout1.patch;https://issues.apache.org/jira/secure/attachment/12680010/diff-for-alternate-layout1.patch","07/Nov/14 00:02;jjkoshy;diff-for-alternate-layout2.patch;https://issues.apache.org/jira/secure/attachment/12680009/diff-for-alternate-layout2.patch","24/Dec/14 08:43;vladimir.tretyakov;logflushes.png;https://issues.apache.org/jira/secure/attachment/12689019/logflushes.png","07/Nov/14 00:02;jjkoshy;originalLayout.png;https://issues.apache.org/jira/secure/attachment/12680008/originalLayout.png",,26.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,396230,,,Wed Mar 18 20:54:05 UTC 2015,,,,,,,,,,"0|i1w7un:",396353,,jjkoshy,,,,,,,,,,,,,,,,,,"03/Jun/14 00:21;otis;KAFKA-1100 is similar in the sense that it points out a problem with embedding stuff in MBean names that makes it difficult to parse those MBeans.;;;","06/Jun/14 10:19;vladimir.tretyakov;I've made some modifications (cloned 0.8.1 branch), now Kafka use ""|"" instead of ""\-"" or ""\_"" in JMX Bean names where they prevented make parsing (I didn't replace all '\-' and '\_', just replaced in places which were critical for parsing point of view).

I've also noticed that these names Kafka uses not only for JMX Bean names, but for directory names and other things (my changes can be dangerous by this reason). 

I've tested Broker/Producer/Consumer locally, it works, but will be perfect if somebody check everything I am not 100% sure about my changes (I am new in Kafka code).

Best Regards, Vladimir.;;;","20/Jun/14 12:01;otis;Nice and simple patch.  Question about this:
{code}
   def validateChars(prop: String, value: String) {
-    val legalChars = ""[a-zA-Z0-9\\._\\-]""
+    val legalChars = ""[a-zA-Z0-9\\._\\-\\|]
{code}

I didn't check the source code to understand the bigger context, but the above looks like ""|"" is now a valid character, but should it really be valid/allowed?
;;;","20/Jun/14 14:30;junrao;Thanks for the patch. I agree that the current jmx bean names are not ideal. I do have the following concerns.

1. While this may help some users, it may equally affect some other users who now would need to change their parsing of the jmx names.
2. I am not sure how wide-spread the separator issue. If it only helps a small number of users, I'd prefer that we only do that on the new consumer.
3. Other than the separator issue, there are other things that are not ideal. For example, the bean name is just unnecessarily too long. If we do want to change the bean names, perhaps we should fix other problems together as well.
 ;;;","23/Jun/14 11:42;otis;Hi Jun, thanks for having a look.

bq. 1. While this may help some users, it may equally affect some other users who now would need to change their parsing of the jmx names.

Won't they have to do that sooner or later anyway, because dashes as separators obviously can't stay?

bq. 2. I am not sure how wide-spread the separator issue. If it only helps a small number of users, I'd prefer that we only do that on the new consumer.

Nobody knows one way or the other though.  So in absence of info I think it makes sense to at least make a move in the right direction.

bq. Other than the separator issue, there are other things that are not ideal. For example, the bean name is just unnecessarily too long. If we do want to change the bean names, perhaps we should fix other problems together as well.

Long bean names may not be pretty, but they are not blockers.  The dashes are likely blockers for some portion of users and definitely blockers for any Kafka monitoring tool makers.  If long bean names are a problem (are they?) let's open a separate issue, ha?

;;;","11/Aug/14 06:33;otis;Looks like 0.9 has been pushed to December 2014.  Any chance this can go into 0.8.2 then?  It's a blocker for us :(
;;;","11/Aug/14 15:14;junrao;I am wondering if you can get around the problem by just parsing the mbean names a bit differently. For example, for an mbean like the following in which ""topic-1"" is the topic name. When parsing the name part, you can find the last ""-"" to break the name into two parts.

""kafka.server"":type=""BrokerTopicMetrics"",name=""topic-1-BytesInPerSec"";;;","11/Aug/14 15:53;vladimir.tretyakov;Hi Jun Rao, look at http://search-hadoop.com/m/4TaT4lonIW

Example:

kafka.consumer:type=""ZookeeperConsumerConnector"",name=""af_servers-af_servers-spm_new_cluster_topic-af_servers_wawanawna-Dell-1401353748289-fcaaea29-0-FetchQueueSize""

Look at part: ""spm_new_cluster_topic-af_servers_wawanawna-Dell"", what is
host name here ""servers_wawanawna-Dell"" or ""wawanawna-Dell"" ?;;;","12/Aug/14 04:22;junrao;I am not sure exactly what your use cases are, but I was thinking of the following. For the AllTopics metric like below, you can figure out the clientId. Then, you can use that info to parse other beans more reliably.

""kafka.consumer"":type=""ConsumerTopicMetrics"",name=""test-consumer-group-AllTopicsBytesPerSec""

I understand that this is still painful. However, my concern is that this patch may fix the issues for some users, but may force other users to change the way they extract mbeans. ;;;","12/Aug/14 07:01;vladimir.tretyakov;Hi Jun Rao, I understood what you suggest, thx, but I am afraid it will not work everywhere or I've missed something. Look at example from my local env:

1) ""kafka.producer"":type=""ProducerRequestMetrics"",name=""af-servers-AllBrokersProducerRequestRateAndTimeMs""

2) ""kafka.consumer"":type=""ZookeeperConsumerConnector"",name=""af-servers-af-servers-spm-new-cluster-topic-af-servers-wawanawna-Dell-1401353748289-fcaaea29-0-FetchQueueSize""

From 1 I can parse ""af-servers"" as clientId, ok.

From 2 I have to parse: topic ,groupId, consumer host, timestamp, uuid, num_streams.

If I remove ""af-servers"" from 2 I will get ""spm-new-cluster-topic-af-servers-wawanawna-Dell-1401353748289-fcaaea29-0-FetchQueueSize"".

I can extract ""timestamp, uuid, num_streams"" without problems, now let me talk about this part ""spm-new-cluster-topic-af-servers-wawanawna-Dell"" I see ""af-servers""  in the middle but can I be sure that ""clientId"" will be always here?

Why initial bean name contains ""af-servers"" 3 times? I have a feeling sometimes all these 3 parts may be different and I will not able to parse everything in right way. 

It is painful in general as you mentioned above because we have to remember ""context"" for parse each line, I am pretty sure nobody like it.
Even if somebody parse names in way you have described I think they will glad to change its parse mechanism with  more easiest.;;;","12/Aug/14 07:10;joestein;[~junrao] If this patch is going to cause existing users that are not having a problem issues and it is causing existing users issues by not patching it then can we introduce a configuration so we can accommodate both (default to how it is working now)? Then we can get it in 0.8.2. Thoughts?;;;","12/Aug/14 15:13;junrao;The name of ""FetchQueueSize"" is of the following format. If the clientId is not explicitly specified, the groupId will be used. So if you know the clientId, you can parse the rest of the string. You probably don't need to figure out every part of the mbean name. The most important ones are clientId, groupId and topic. 

config.clientId + ""-"" + config.groupId + ""-"" + topicThreadId._1 + ""-"" + topicThreadId._2 + ""-FetchQueueSize""

Joe,

I am not sure having a config is necessarily better. If we want to fix the mbean names, I'd rather that we fix them once into the end state that we want, instead of patching little by little and changing the names multiple times.;;;","28/Aug/14 10:30;otis;bq. if you know the clientId, you can parse the rest of the string. 

I think this makes parsers unnecessarily difficult to write and probably harder to maintain.  Parsers have to remember things this way.

bq. If this patch is going to cause existing users that are not having a problem issues 

I think it won't cause them issues.  I *think* they would just need to change which delimiter character they use to break MBean names into parts.

bq. If it is causing existing users issues by not patching it

I think it is causing issues to anyone who has any of the multiple characters Kafka uses as delimiters in their hostnames or topic names.  So I think these people have issues when they first try parsing stuff from JMX, and then they have to go modify their hostnames, topic names and such.
;;;","04/Sep/14 21:47;guozhang;[~junrao] do you think we can check in this ticket in time for 0.8.2?;;;","10/Sep/14 01:38;otis;Here is a Kafka user who said:

{quote}
 I have topic name with “.”  So, it is hard to distinguish metric name and topic
{quote}
c.f. http://search-hadoop.com/m/4TaT4IRD0t1

That's the sort of problem this patch solves.;;;","12/Sep/14 03:51;nehanarkhede;This issue has popped up enough times on the mailing list that we should pay attention to it and fix it. [~junrao], what plan would you suggest for fixing this so that the change works for everyone?;;;","15/Sep/14 15:54;junrao;Ok, I am fine with taking this patch as a stop-gap solution. Since this is going to affect how people monitor Kafka, we probably should have a discussion in the mailing list.

Otis/Vladimir,
Do you want to start a discussion thread in our dev and user mailing list that describes the problem, the solution and the impact? If there are no serious concerns, we can commit the patch.

Thanks,;;;","22/Sep/14 17:40;otis;This is [~junrao]'s new suggestion for dealing with the problem this issue is meant to address: http://search-hadoop.com/m/4TaT4aHH1S;;;","13/Oct/14 15:28;vladimir.tretyakov;Added another patch which introduce metrics name like pairs (key=value), examples:
{code}
""kafka.consumer"":type=""ConsumerTopicMetrics"",name=""clientId=af_servers,AllTopics,BytesPerSec""
""kafka.cluster"":type=""Partition"",name=""topic=spm_alerts_topic,partitionId=0,UnderReplicated""
""kafka.network"":type=""SocketServer"",name=""NetworkProcessorNum=1,IdlePercent""
""kafka.server"":type=""BrokerTopicMetrics"",name=""topic=spm_topic,MessagesInPerSec""

""kafka.server"":type=""FetcherStats"",name=""clientId=af_servers,ConsumerFetcherThread,groupId=af_servers,consumerHostName=wawanawna,timestamp=1413199781501,uuid=58a5cc70,fetcherId=0,sourceBrokerId=0,brokerHost=wawanawna,brokerPort=9092,BytesPerSec""
""kafka.server"":type=""FetcherLagMetrics"",name=""clientId=af_servers,ConsumerFetcherThread,groupId=af_servers,consumerHostName=wawanawna,timestamp=1413199781501,uuid=58a5cc70,fetcherId=0,sourceBrokerId=0,brokerHost=wawanawna,brokerPort=9092,topic=spm_topic,partitionId=0,ConsumerLag""
""kafka.consumer"":type=""FetchRequestAndResponseMetrics"",name=""clientId=af_servers,ConsumerFetcherThread,groupId=af_servers,consumerHostName=wawanawna,timestamp=1413199781501,uuid=58a5cc70,fetcherId=0,sourceBrokerId=0,brokerHost=wawanawna,brokerPort=9092,FetchResponseSize""
{code};;;","14/Oct/14 00:10;junrao;Vladimir,

Thanks for the patch. My suggestion is actually slightly different from what you did. Instead of using

""kafka.consumer"":type=""ConsumerTopicMetrics"",name=""clientId=af_servers,AllTopics,BytesPerSec""

I was suggesting

""kafka.consumer"":type=""ConsumerTopicMetrics"",clientId=""af_servers"",topic=""AllTopics"",name=""BytesPerSec""

This is probably the more standard mbean name.

We can do that by using the following method to create MetricName and pass in the mBeanName that we want.
    public MetricName(String group, String type, String name, String scope, String mBeanName).

We also need to extend KafkaMetricsGroup by adding new helper functions that take a MetricName explicitly.
  def newMeter(name: MetricName, eventType: String, timeUnit: TimeUnit)

Also, your patch doesn't seem to apply to latest trunk.
git apply ~/Downloads/KAFKA-1481_2014-10-13_18-23-35.patch 
error: core/src/main/scala/kafka/common/ClientIdTopic.scala: No such file or directory



;;;","14/Oct/14 19:07;vladimir.tretyakov;Hi, Jun, 

Thx that you found a time to look at patch, I've added another regarding your suggestion, now in JMX you will see (hope it is what you need):
{code}
""kafka.server"":type=""FetcherStats"",name=""RequestsPerSec"",clientId=""af_servers"",threadName=""ConsumerFetcherThread"",groupId=""af_servers"",consumerHostName=""wawanawna"",timestamp=""1413306414731"",uuid=""4624cb0f"",fetcherId=""0"",sourceBrokerId=""0"",brokerHost=""wawanawna"",brokerPort=""9092""
""kafka.server"":type=""FetcherLagMetrics"",name=""ConsumerLag"",clientId=""af_servers"",threadName=""ConsumerFetcherThread"",groupId=""af_servers"",consumerHostName=""wawanawna"",timestamp=""1413306414731"",uuid=""4624cb0f"",fetcherId=""0"",sourceBrokerId=""0"",brokerHost=""wawanawna"",brokerPort=""9092"",topic=""spm_topic"",partitionId=""0""
""kafka.consumer"":type=""ZookeeperConsumerConnector"",name=""OwnedPartitionsCount"",clientId=""af_servers"",groupId=""af_servers"",topic=""spm_topic""
""kafka.consumer"":type=""FetchRequestAndResponseMetrics"",name=""FetchResponseSize"",clientId=""af_servers"",threadName=""ConsumerFetcherThread"",groupId=""af_servers"",consumerHostName=""wawanawna"",timestamp=""1413306414731"",uuid=""4624cb0f"",fetcherId=""0"",sourceBrokerId=""0"",allBrokers=""true""
{code}

{quote}
Also, your patch doesn't seem to apply to latest trunk.
git apply ~/Downloads/KAFKA-1481_2014-10-13_18-23-35.patch 
error: core/src/main/scala/kafka/common/ClientIdTopic.scala: No such file or directory
{quote}
Interesting...
1. I've worked with 0.8.2 branch, not trunk
2. ClientIdTopic.scala - I've added this file (must be in patch). 

Added 2 equal patches (created them in different way):
1. 'git diff' (KAFKA-1481_2014-10-14_21-53-35.patch)
2. With help from IDEA IDE. (KAFKA-1481_IDEA_IDE_2014-10-14_21-53-35.patch)

Try please, maybe one of them will work

Thx again.
;;;","15/Oct/14 08:00;vladimir.tretyakov;Added 2 patches (created with git diff and from IDEA IDE) with a small refactoring in KafkaMetricsGroup.

Jun can you please double check and change part related to metrics removing (KAFKA-1567) I can't easily test this code locally, thx a lot.

PS: all tests passed after this patch for me locally.;;;","15/Oct/14 23:50;junrao;Thanks for the patch. The IDEA one applies successfully for me. However, it seems to cause compilation failure since in a few cases, two lines are incorrectly merged into a single one. Do you think you can try our patch review tool (https://cwiki.apache.org/confluence/display/KAFKA/Patch+submission+and+review)? Some comments.

1. KafkaMetricsGroup:
1.1 We now have 2
       new MetricName(""kafka.consumer"", ""ConsumerTopicMetrics"", ""MessagesPerSec""),
    and 2
       new MetricName(""kafka.consumer"", ""ConsumerTopicMetrics"", ""BytesPerSec""),
1.2 Could we combine the following two methods
  def newGauge[T](name: String, mBeanName: String, metric: Gauge[T])
 
  def newGauge[T](name: String, metric: Gauge[T]) : Gauge[T]
 
  to
  def newGauge[T](name: String, metric: Gauge[T], tags: Map[String, String] = Map.empty)? We can then general the metric name based on name and the tags. This will consolidate the metric name generation in a single place. In this patch, we have too many places generating the key/value string for the metric name.

2. AbstractFetcherThread: 
2.1 name in this class should be just used for the thread name and it shouldn't be included in the metric name.
2.2. The way that we get the metric name in classes like ClientIdAndBroker, ClientIdBrokerTopicPartition and ClientIdAndTopic is not consistently. Sometimes, we generate the key/value in the toString(). Some other times, we rely on the key/value string to be passed in. It's easier to understand if those classes are constructed by just passing in the clientId string, the broker object, the topic string and the partition number, etc. We can generate the metric name by using what's described in 1.2.
2.3 FetcherLagStats just needs to take clientId, instead of ClientIdAndBroker since the per partition lag metric is unique per client id.
2.4 The metric name for a Broker can just use the brokerId, instead of its host and port.

3. Throttler: Do we need to add mBeanName to the constructor? It seems that nobody is using it at the moment.

4. KafkaMetricsGroup: We need to change the way how removeAllMetricsInList() works. We need to do equal comparison on group, type and name and do regex matching with clientId on MetricName.mBeanName.
;;;","16/Oct/14 06:26;vladimir.tretyakov;Hi Jun, thx for juicy feedback, few comments fro me:

1-2. Did it in this way because I didn't want to change existing code a lot, tried got what we need based on current method signatures. I've thought about your approach with Map too, reject it because of larger changes. Now I see that I will do it.

3. Removed 'mBeanName' param, was legacy code.
4. As I've understood from code this method must be called before shutdown in hook, right? We have to remove only metrics related to particular clientId and we shouldn't touch others, right?;;;","16/Oct/14 14:16;junrao;4. removeAllMetricsInList() will be called when a producer/consumer instance is closed to remove metrics related to a specific client id.;;;","20/Oct/14 17:29;vladimir.tretyakov;Hi, fuf:), created new patch (for 0.8.2 branch).

re 2.1, 2.3 - didn't change this part because we use this part in our monitoring, it is more handy for user to see something like 'localhost:9092' instead of '2 (where 2 is broker id)'. For us it is easiest to see/have this information directly in mBean.

With this patch mBeanNames look like:
{code}
kafka.producer:type=ProducerTopicMetrics,name=MessagesPerSec,topic=spm_alerts_topic
kafka.producer:type=ProducerTopicMetrics,name=MessagesPerSec,allTopics=true
kafka.log:type=Log,name=LogEndOffset,topic=spm_alerts_topic,partitionId=0
kafka.consumer:type=ZookeeperConsumerConnector,name=FetchQueueSize,timestamp=1413817796508,clientId=af_servers,uuid=9f99df40,groupId=af_servers,topicThreadId=spm_topic,consumerHostName=wawanawna,threadId=0
kafka.consumer:type=FetchRequestAndResponseMetrics,name=FetchRequestRateAndTimeMs,clientId=af_servers,threadName=ConsumerFetcherThread,fetcherId=0,sourceBrokerId=0,groupId=af_servers,consumerHostName=wawanawna,timestamp=1413817796508,uuid=9f99df40,brokerHost=wawanawna,brokerPort=9092
{code}

There is not everything clear for me in code, so I've tried to avoid deep refactoring. All tests passed for me locally.

PS: note that if 'value' part is empty (in key->value structure) we will not see this part in attributes.
example: ""key1""->""value1"",""key2""->null,""key3""->""value3"" will be converted to ""key1=value1,key3=value3"".





;;;","20/Oct/14 18:24;junrao;Thanks for the patch. It doesn't seem to apply. Could you rebase?

git apply -p0 ~/Downloads/KAFKA-1481_IDEA_IDE_2014-10-20_20-14-35.patch 
error: core/src/main/scala/kafka/common/ClientIdTopic.scala: No such file or directory
error: patch failed: core/src/main/scala/kafka/server/KafkaApis.scala:285
error: core/src/main/scala/kafka/server/KafkaApis.scala: patch does not apply
error: patch failed: core/src/test/scala/integration/kafka/api/ProducerCompressionTest.scala:74
error: core/src/test/scala/integration/kafka/api/ProducerCompressionTest.scala: patch does not apply
error: patch failed: core/src/main/scala/kafka/consumer/PartitionAssignor.scala:101
error: core/src/main/scala/kafka/consumer/PartitionAssignor.scala: patch does not apply
error: patch failed: core/src/test/scala/integration/kafka/api/ProducerFailureHandlingTest.scala:17
error: core/src/test/scala/integration/kafka/api/ProducerFailureHandlingTest.scala: patch does not apply;;;","20/Oct/14 20:30;vladimir.tretyakov;Hi, thx for quick reply, did rebase, added new patches (1 was created by git, 1 by IDEA IDE).;;;","21/Oct/14 00:38;junrao;It still doesn't apply.

git apply -p0 ~/Downloads/KAFKA-1481_IDEA_IDE_2014-10-20_23-14-35.patch 
/Users/junrao/Downloads/KAFKA-1481_IDEA_IDE_2014-10-20_23-14-35.patch:311: trailing whitespace.
           })  
error: core/src/main/scala/kafka/common/TopicInfo.scala: No such file or directory

Does the patch apply on a fresh checkout for you?;;;","21/Oct/14 06:48;vladimir.tretyakov;Hi, added last patch (KAFKA-1481_2014-10-21_09-14-35.patch). Worked for me locally (previous worked too:( ).
There are commands I've used:
{code}
PATCH CREATION
wawanawna@wawanawna:/home/storage/sematext/src/kfk2/kafka$ git status
On branch 0.8.2
Your branch is up-to-date with 'origin/0.8.2'.

Changes to be committed:
  (use ""git reset HEAD <file>..."" to unstage)

	new file:   core/src/main/scala/kafka/common/BrokerInfo.scala
	new file:   core/src/main/scala/kafka/common/ClientIdTopic.scala
	new file:   core/src/main/scala/kafka/common/Taggable.scala
	renamed:    core/src/main/scala/kafka/common/ClientIdAndTopic.scala -> core/src/main/scala/kafka/common/TopicInfo.scala
	new file:   core/src/main/scala/kafka/consumer/ConsumerFetcherThreadId.scala
	new file:   core/src/main/scala/kafka/consumer/ConsumerId.scala

Changes not staged for commit:
  (use ""git add <file>..."" to update what will be committed)
  (use ""git checkout -- <file>..."" to discard changes in working directory)

	modified:   core/src/main/scala/kafka/admin/AdminUtils.scala
	modified:   core/src/main/scala/kafka/api/HeartbeatRequestAndHeader.scala
	modified:   core/src/main/scala/kafka/api/HeartbeatResponseAndHeader.scala
	modified:   core/src/main/scala/kafka/api/JoinGroupRequestAndHeader.scala
	modified:   core/src/main/scala/kafka/api/JoinGroupResponseAndHeader.scala
	modified:   core/src/main/scala/kafka/api/RequestKeys.scala
	modified:   core/src/main/scala/kafka/cluster/Partition.scala
...
...
...
wawanawna@wawanawna:/home/storage/sematext/src/kfk2/kafka$ git commit -a -m 'kafka-1481; JMX renaming'
[0.8.2 a232af6] kafka-1481; JMX renaming
 73 files changed, 666 insertions(+), 435 deletions(-)
 create mode 100644 core/src/main/scala/kafka/common/BrokerInfo.scala
 create mode 100644 core/src/main/scala/kafka/common/ClientIdTopic.scala
 create mode 100644 core/src/main/scala/kafka/common/Taggable.scala
 rename core/src/main/scala/kafka/common/{ClientIdAndTopic.scala => TopicInfo.scala} (68%)
 create mode 100644 core/src/main/scala/kafka/consumer/ConsumerFetcherThreadId.scala
 create mode 100644 core/src/main/scala/kafka/consumer/ConsumerId.scala
wawanawna@wawanawna:/home/storage/sematext/src/kfk2/kafka$ git log | head -n 20
commit a232af63b8a1f43054994a74520b31ef7b9b347c
Author: wawanawna <sematex@mail.com>
Date:   Tue Oct 21 09:29:31 2014 +0300

    kafka-1481; JMX renaming

commit eb7ac9eb7eebc4e0655b65e07cae594e61a6c05e
Author: Jun Rao <junrao@gmail.com>
Date:   Mon Oct 20 11:09:31 2014 -0700

    kafka-1717; remove netty dependency through ZK 3.4.x; patched by Jun Rao; reviewed by Sriharsha Chintalapani and Neha Narkhede


wawanawna@wawanawna:/home/storage/sematext/src/kfk2/kafka$ git diff eb7ac9eb7eebc4e0655b65e07cae594e61a6c05e a232af63b8a1f43054994a74520b31ef7b9b347c > /tmp/KAFKA-1481_2014-10-21_09-14-35.patch

PATCH APPLYING

wawanawna@wawanawna:/home/storage/sematext/src/kfk3$ git clone https://github.com/apache/kafka.git


wawanawna@wawanawna:/home/storage/sematext/src/kfk3/kafka$ git checkout -b 0.8.2 origin/0.8.2
Branch 0.8.2 set up to track remote branch 0.8.2 from origin.
Switched to a new branch '0.8.2'


wawanawna@wawanawna:/home/storage/sematext/src/kfk3/kafka$ git apply /tmp/KAFKA-1481_2014-10-21_09-14-35.patch
/tmp/KAFKA-1481_2014-10-21_09-14-35.patch:1410: trailing whitespace.
           })  
warning: 1 line adds whitespace errors.
{code}

There is warning, but everything works (compilation/tests passed without errors)

PS: please tell me what I am doing wrong.
wawanawna@wawanawna:/home/storage/sematext/src/kfk3/kafka$ git --version
git version 1.9.1
;;;","21/Oct/14 16:28;otis;[~joestump] can we pleeeeeease have this back in 0.8.2?  The patch is ready, just needs to be merged in.  We've been working very, very hard to get this into 0.8.2.;;;","21/Oct/14 17:53;junrao;Otis,

Yes, we can include this in the 0.8.2 final release. Does that sound good? Thanks,;;;","21/Oct/14 18:42;vladimir.tretyakov;Hi, Jun, will be perfect!;;;","21/Oct/14 20:57;Bmis13;Hi [~junrao],

Can you please let me know if this will also address the [New Java Producer] metrics() method and which client.id or topic has special chars ?  So we have consistent naming across all JMX name bean or metrics() methods ?

Here is background on this:

{code}
Bhavesh,

Yes, allowing dot in clientId and topic makes it a bit harder to define the
JMX bean names. I see a couple of solutions here.

1. Disable dot in clientId and topic names. The issue is that dot may
already be used in existing deployment.

2. We can represent the JMX bean name differently in the new producer.
Instead of
  kafka.producer.myclientid:type=mytopic
we could change it to
  kafka.producer:clientId=myclientid,topic=mytopic

I felt that option 2 is probably better since it doesn't affect existing
users.

Otis,

We probably can also use option 2 to address KAFKA-1481. For topic/clientid
specific metrics, we could explicitly specify the metric name so that it
contains ""topic=mytopic,clientid=myclientid"". That seems to be a much
cleaner way than having all parts included in a single string separated by
'|'.

Thanks,

Jun
{code}

Thanks,

Bhavesh ;;;","21/Oct/14 21:08;junrao;Thanks for the patch. Some comments below.

20. KafkaMetricsGroup:
20.1 In the following, instead of doing map(kv => ), could we do map { case(key, value) => }?
        .filter(_._2 != """").map(kv => ""%s=%s"".format(kv._1, kv._2))
20.2 In the following, shouldn't the pattern be ("".*"" + ""clientId="" + clientId + "".*"").r
      val pattern = ("".*"" + clientId + "".*"").r

21. TaggableInfo: tags should be an immutable hashmap, right?
case class TaggableInfo(tags: mutable.LinkedHashMap[String, String]) extends Taggable {

22. New files:
22.1 The license header should be at the very beginning, before the package and import.
22.2 I am not sure if we really need to have the Taggable trait. For example, in ConsumerTopicMetrics, we can just take the original clientIdAndTopic and explicitly create the tag for clientId and topic when creating those metrics. The tags are really specific to the metrics. So, we probably don't need to expose them in places other than where the metrics are created.

23. ReplicaFetcherManager: The first statement in shutdown() is on the same line as the method. A similar issue happens in a few other classes like RequestChannel and ConsumerFetcherManager. Perhaps you can follow the patch creation process in https://cwiki.apache.org/confluence/display/KAFKA/Patch+submission+and+review#Patchsubmissionandreview-Simplecontributorworkflow

24. FetchRequestAndResponseStatsRegistry.removeConsumerFetchRequestAndResponseStats(): Should we use ""clientId="" + clientId in pattern?

25. SimpleConsumer: Ideally, we shouldn't change the constructor since this will be an api change.

26. Could you add a unit test to make sure that after a producer/consumer is closed, all metrics specific to the producer/consumer are removed?;;;","21/Oct/14 21:22;junrao;Bhavesh,

Created a subtask to track the metric name in the new producer. Thanks,;;;","22/Oct/14 14:08;otis;[~junrao] - in interest of getting this into 0.8.2 can we please just make sure the MBeans look good ""on the outside"", commit that, and then iterate on improving the internals?;;;","22/Oct/14 15:34;junrao;Otis,

I expect that we will have about another 4 weeks before 0.8.2 final is released. That should give us enough time to iterate on this, right? Since the patch touches many files, I'd prefer that we get a clean version upfront.;;;","24/Oct/14 11:05;vladimir.tretyakov;Hi Jun, thx for your feedback again, attached new patch.

20.1 done.
20.2 ""clientId"" is ""Taggable"" here, it will convert to something like ""clientId=XXX"" automatically
21 tags should be Map which iterators iterate in the same order elements were inserted (final string must be stable): from LinkedHashMap docs ""The iterator and all traversal methods of this class visit elements in the order they were inserted."". I am new in scala, if you know better candidate with predictable iterators please let me know
22.1 done
22.2 clientId is not always just ""clientId=XXX"", look at:
{code}
class ConsumerFetcherThread(consumerFetcherThreadId: ConsumerFetcherThreadId,
                            val config: ConsumerConfig,
                            sourceBroker: Broker,
                            partitionMap: Map[TopicAndPartition, PartitionTopicInfo],
                            val consumerFetcherManager: ConsumerFetcherManager)
        extends AbstractFetcherThread(name = consumerFetcherThreadId,
                                      clientId = new TaggableInfo(new TaggableInfo(""clientId"" -> config.clientId), consumerFetcherThreadId),  
                                      sourceBroker = sourceBroker,
                                      socketTimeout = config.socketTimeoutMs,
                                      socketBufferSize = config.socketReceiveBufferBytes,
                                      fetchSize = config.fetchMessageMaxBytes,
                                      fetcherBrokerId = Request.OrdinaryConsumerId,
                                      maxWait = config.fetchWaitMaxMs,
                                      minBytes = config.fetchMinBytes,
                                      isInterruptible = true) {
{code}


this ""clientId = new TaggableInfo(new TaggableInfo(""clientId"" -> config.clientId), consumerFetcherThreadId)"" part was in code before my changes (was manipulation with strings, not ""Taggable"" of course). Yeah, somewhere in code we can use string instead of ""Taggable"", but maybe it is better to has ""Taggable"" everywhere what is related to metrics. At least it was not easy to me decide where I have to use ""Taggable"", but where I can leave string. I'd prefer ""Taggable"" everywhere in this case, sorry:).

23. done everything as described by link, checked how it works/applies on 'origin/0.8.2' from scratch.
24. see 22.2
25. agree, added second constructor with 'String' as clientId, can't remove constructor with ""Taggable"" as clientId because as you can see in 22.2 sometimes clientId is not just ""clientId"" but something more complex.
26. added 'MetricsLeakTest', basic idea is start/stop many producers/consumes and observe count of metrics in Metrics.defaultRegistry(), count should not grow.;;;","28/Oct/14 00:35;junrao;Vladimir,

Thanks for the patch. Really appreciate your help. I realized that this is one of the biggest technical debt that we have accumulated over time. So, it may take some time to sort this out. So, bear with me. Some more comments.

30. About Taggable, I still have mixed feelings. I can see why you created it. However, my reasoning is that for a lot of the case classes (ClientIdTopic, CliendIdAndBroker) that we create, it's weird that some of them are taggable and some of them are not, depending whether they are used for tagging metric names or not. Those classes have no direct relationships with the metrics. Similarly, we only need to be aware of tags when creating metrics. Also, because of this, we change the constructor of SimpleConsumer. Since this is an API change, we should really try to avoid it. 

My feeling is that it's probably simpler if we just create regular case classes as before and generate metric tags explicitly when we create the metric. For example, in AbstractFetcherThread, we can do

class FetcherStats(clientIdAndBroker: ClientIdAndBroker) extends KafkaMetricsGroup {
  val requestRate = newMeter(""RequestsPerSec"", ""requests"", TimeUnit.SECONDS,
                                                Map(""cliendId"" -> clientIdAndBroker.clientId,
                                                        ""brokerHost"" -> clientIdAndBroker.host,
                                                        ""brokerPort"" -> clientIdAndBroker.port))

and just have ClientIdAndBroker be the following case class.

case class ClientIdAndBroker(clientId: String, host: String, port: Int)

This way, the code is a bit cleaner since all the metric tag related stuff are isolated to those places when the metrics are created. So, I'd suggest that we remove Taggable.

31. AbstractFetcherThread:
31.1 You changed the meaning of clientId. clientId is used in the fetch request and we want to leave it as just the clientId string. Since the clientId should be uniquely representing a particular consumer client, we just need to include the clientId in the metric name. We don't need to include the consumer id in either the fetch request or the metric name since it's too long and has redundant info. 
31.2 FetcherLagStats: This is an existing problem. FetcherLagMetrics shouldn't be keyed off ClientIdBrokerTopicPartition. It should be keyed off ClientIdTopicPartition. This way, the metric name remains the same independent of the current leader of those partitions.

32. ZookeeperConsumerConnector:
32.1 FetchQueueSize: I agree that the metric name just needs to be tagged with clientId, topic and threadId. We don't need to include the consumerId since it's too long (note that topicThread._2 includes both the consumerId and the threadId).

33. KafkaMetricsGroup: Duplicate entries.
    // kafka.consumer.ConsumerTopicStats <-- kafka.consumer.{ConsumerIterator, PartitionTopicInfo}
    explicitMetricName(""kafka.consumer"", ""ConsumerTopicMetrics"", ""MessagesPerSec""),
    explicitMetricName(""kafka.consumer"", ""ConsumerTopicMetrics"", ""MessagesPerSec""),

    // kafka.consumer.ConsumerTopicStats
    explicitMetricName(""kafka.consumer"", ""ConsumerTopicMetrics"", ""BytesPerSec""),
    explicitMetricName(""kafka.consumer"", ""ConsumerTopicMetrics"", ""BytesPerSec""),

    // kafka.consumer.FetchRequestAndResponseStats <-- kafka.consumer.SimpleConsumer
    explicitMetricName(""kafka.consumer"", ""FetchRequestAndResponseMetrics"", ""FetchResponseSize""),
    explicitMetricName(""kafka.consumer"", ""FetchRequestAndResponseMetrics"", ""FetchRequestRateAndTimeMs""),
    explicitMetricName(""kafka.consumer"", ""FetchRequestAndResponseMetrics"", ""FetchResponseSize""),
    explicitMetricName(""kafka.consumer"", ""FetchRequestAndResponseMetrics"", ""FetchRequestRateAndTimeMs""),

    /**
     * ProducerRequestStats <-- SyncProducer
     * metric for SyncProducer in fetchTopicMetaData() needs to be removed when consumer is closed.
     */
    explicitMetricName(""kafka.producer"", ""ProducerRequestMetrics"", ""ProducerRequestRateAndTimeMs""),
    explicitMetricName(""kafka.producer"", ""ProducerRequestMetrics"", ""ProducerRequestSize""),
    explicitMetricName(""kafka.producer"", ""ProducerRequestMetrics"", ""ProducerRequestRateAndTimeMs""),
    explicitMetricName(""kafka.producer"", ""ProducerRequestMetrics"", ""ProducerRequestSize"")

34. AbstractFetcherManager: Could you put the followings in 2 separate lines? Similar things happen in a few other files. Perhaps you need to change the formatting in your IDE?

   }, metricPrefix.toTags

  private def getFetcherId(topic: String, partitionId: Int) : Int = {    Utils.abs(31 * topic.hashCode() + partitionId) % numFetchers

;;;","29/Oct/14 16:46;vladimir.tretyakov;Hi Jun, thx for juicy feedback again, one question:
{quote}
31. AbstractFetcherThread:
31.1 You changed the meaning of clientId. clientId is used in the fetch request and we want to leave it as just the clientId string. Since the clientId should be uniquely representing a particular consumer client, we just need to include the clientId in the metric name. We don't need to include the consumer id in either the fetch request or the metric name since it's too long and has redundant info. 
{quote}

I didn't change meaning of clientId here, look (all code without my changes):

consumerIdString string is:
{code}
val consumerIdString = {
    var consumerUuid : String = null
    config.consumerId match {
      case Some(consumerId) // for testing only
      => consumerUuid = consumerId
      case None // generate unique consumerId automatically
      => val uuid = UUID.randomUUID()
      consumerUuid = ""%s-%d-%s"".format(
        InetAddress.getLocalHost.getHostName, System.currentTimeMillis,
        uuid.getMostSignificantBits().toHexString.substring(0,8))
    }
    config.groupId + ""_"" + consumerUuid
  }
{code}

thread name is (consumerIdString + fetcherId + sourceBroker.id):
{code}
 override def createFetcherThread(fetcherId: Int, sourceBroker: Broker): AbstractFetcherThread = {
    new ConsumerFetcherThread(
      ""ConsumerFetcherThread-%s-%d-%d"".format(consumerIdString, fetcherId, sourceBroker.id),
      config, sourceBroker, partitionMap, this)
  }
{code}

clientId inside AbstractFetcherThread is: config.clientId + consumerIdString + fetcherId + sourceBroker.id
{code}
class ConsumerFetcherThread(name: String,
                            val config: ConsumerConfig,
                            sourceBroker: Broker,
                            partitionMap: Map[TopicAndPartition, PartitionTopicInfo],
                            val consumerFetcherManager: ConsumerFetcherManager)
        extends AbstractFetcherThread(name = name, 
                                      clientId = config.clientId + ""-"" + name,
                                      sourceBroker = sourceBroker,
                                      socketTimeout = config.socketTimeoutMs,
                                      socketBufferSize = config.socketReceiveBufferBytes,
                                      fetchSize = config.fetchMessageMaxBytes,
                                      fetcherBrokerId = Request.OrdinaryConsumerId,
                                      maxWait = config.fetchWaitMaxMs,
                                      minBytes = config.fetchMinBytes,
                                      isInterruptible = true) {
{code}

As you see there is no clean clientId inside AbstractFetcherThread class and it is not unique situation, and this is main goal why I added Taggable.
Now I am trying to remove Taggable, but I have no idea what to do with such cases I've described. Can I add new 'clientId' parameter to all these classes and use only this new/clean clientId as part of matric name? Any other suggestions? Thx.

PS: there is no way t get 'clean' parameters in such classes and build map for metrics just here, I must stretching parameters during all classes as Taggable. 
PSS: Can we use more interactive way for communication, Jira is not a bets way for discussion/explanation, not fast at least:)
;;;","30/Oct/14 18:40;vladimir.tretyakov;Hi Jun, added new patch, removed Taggable, did everything as you have suggested.;;;","31/Oct/14 00:42;junrao;Thanks for the patch. Looks good overall. Some minor comments below.

40. KafkaMetricsGroup:
40.1 Since we already match the metric name directly, shouldn't the following pattern
           val pattern = ("".*"" + metric.getName + "".*"" + clientId + "".*"").r
       be
           val pattern = ("".*clientId="" + clientId + "".*"").r
40.2 Can we make toMBeanName() private?
40.3 According to http://docs.oracle.com/javase/7/docs/api/javax/management/ObjectName.html,  the key properties in an Mbean objectname are a set of unordered keys. So, we don't need the toMap() method. The caller can just create the Map using Map(""tag1""->""val1"", ""tag1""->va2""). 

41. FetchRequestAndResponseMetrics: The following two lines should be in the same line.
  val tags = metricId
  match {

42. Log, Partition,AbstractFetcherThread,ProducerRequestPurgatory: ""partitionId"" => ""partition""

43. TopicPartitionRequestKey: We don't need keyLabel() any more. Instead, we can override the toString() method to get the string in the same way as that in TopicAndPartition.toString().

44. RequestChannel: Lower case ""processor"" in the following.
      toMap(""Processor"" -> i.toString)

45. SocketServer: ""NetworkProcessor"" =>  ""networkProcessor""

46. ZookeeperConsumerConnector: Could we put the new Gauge and the toMap() into a separate line?
    newGauge(""OwnedPartitionsCount"", new Gauge[Int] {
      def value() = allTopicsOwnedPartitionsCount
    }, toMap(""clientId"" -> config.clientId, ""groupId"" -> config.groupId, ""allTopics"" -> ""true""))

              newGauge(""OwnedPartitionsCount"", new Gauge[Int] {
                def value() = partitionThreadPairs.size
              }, ownedPartitionsCountMetricName(topic));;;","31/Oct/14 11:16;vladimir.tretyakov;Hi Jun, added new one, changed according to your last comments.;;;","31/Oct/14 16:44;junrao;Thanks for the patch. A few more comments.

50. ClientIdBroker: Instead of having 2 subclasses, would it be better to have just one class
  case class ClientIdAndBroker(clientId: String, brokerHost: Option[String], brokerPort: Option[Int])?
Ditto to ClientIdTopic.

51. TopicPartitionRequestKey: Can this just be TopicAndPartition?

52. MetricsTest:
52.1 Could we remove the extra empty lines after the class?
52.2 remove unnecessary {} in the following (a few other files have a similar issue)
import java.util.{Properties}
52.3 In testMetricsLeak(), you don't need to create a new zkClient. You can get one from the base class in ZooKeeperTestHarness.
52.4 Instead of duplicating the createAndShutdownStep() calls, could we use a loop instead?
52.5 Instead of duplicating sendMessages() and getMessages() from ZookeeperConsumerConnectorTest, could we extract those methods to TestUtils and add comments to describe what they do? Then, we can reuse those methods.

53. Could you also include a patch to the 0.8.2 doc (https://svn.apache.org/repos/asf/kafka/site/082/ops.html) with the metric name changes?




;;;","03/Nov/14 14:08;vladimir.tretyakov;Hi, added new patch

50. I think it is better to have separate case class for 'all' things. It is real 'other' case.
51. Done
52.1 Done
52.2 Done
52.3 Done
52.4 Done
52.5 Done
53. https://issues.apache.org/jira/secure/attachment/12678927/KAFKA-1481_2014-11-03_16-39-41_doc.patch;;;","04/Nov/14 01:46;junrao;Thanks for the patch. Just some minor comments below. Otherwise, +1 from me.

60. AbstractFetcherManager: In the following, we don't need to wrap new Gauge in {}.
  ""MinFetchRate"", {
    new Gauge[Double] {
      // current min fetch rate across all fetchers/topics/partitions
      def value = {
        val headRate: Double =
          fetcherThreadMap.headOption.map(_._2.fetcherStats.requestRate.oneMinuteRate).getOrElse(0)

        fetcherThreadMap.foldLeft(headRate)((curMinAll, fetcherThreadMapEntry) => {
          fetcherThreadMapEntry._2.fetcherStats.requestRate.oneMinuteRate.min(curMinAll)
        })
      }
    }
  },

61. AbstractFetcherThread: Could we align ""topic"" and ""partition"" to the same column as ""clientId""? Ditto in a few other places.
    Map(""clientId"" -> metricId.clientId,
      ""topic"" -> metricId.topic,
      ""partition"" -> metricId.partitionId.toString)
  )

62. FetchRequestAndResponseMetrics: In the following, could we put Map in a separate line?
  val tags = metricId match {
    case ClientIdAndBroker(clientId, brokerHost, brokerPort) => Map(""clientId"" -> clientId, ""brokerHost"" -> brokerHost,
      ""brokerPort"" -> brokerPort.toString)
    case ClientIdAllBrokers(clientId) => Map(""clientId"" -> clientId, ""allBrokers"" -> ""true"")
  }

62. TestUtils: It's a bit weird that sendMessagesToBrokerPartition() has both a config and configs. We actually don't need the config any more. When sending a message to a partition, we actually don't know which broker the partition is on. So, the message can just be of the form test + ""-"" + partition + ""-"" + x. We can also rename the method to sendMessagesToPartition().
;;;","04/Nov/14 01:47;junrao;[~jjkoshy], do you want to take another look at the patch?;;;","04/Nov/14 02:08;jjkoshy;Yes thanks - I'll take a look at this tomorrow.;;;","05/Nov/14 02:53;jjkoshy;Sorry I ran out of time - will look tomorrow.;;;","07/Nov/14 00:02;jjkoshy;Can you rebase? Sorry I know you have rebased a couple times already. 
Hopefully this should be the last time as these are minor comments.

KafkaMetricsGroup: 64: foreach

KafkaMetricsGroup: toMbeanName: 150/153: can you use filter { case(tagKey, tagValue) =>
...}


For aggregate topic metrics, since allTopics=true appears at the end it is a 
bit weird when browsing mbeans in jvisualvm/other tools. i.e., the mbean is
listed as ""true"". I understand why - it is just a bit weird. I'm referring 
to (for example)
kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec,allTopics=true
See the attached originalLayout.png

Personally I prefer aggregate=true to allTopics=true. A further improvement 
with aggregate=true is the following: in KafkaMetricsGroup.metricName you
can check in the tags map if aggregate=true. If so, then modify the typeName 
by pre-pending Aggregate to it and then strip off the aggregate=true tag. So
you will end up with:
kafka.server:type=BrokerTopicMetrics,name=AggregateBytesOutPerSec 

See alternateLayout1.png 

Another alternative is to modify the name (not the typeName). See 
alternateLayout2.png
                                                                 
The aggregate=true approach seems generic enough to apply to any other
all-topic, all-request, or all-broker level mbeans. What do you think?
;;;","10/Nov/14 07:37;vladimir.tretyakov;Hi [~jjkoshy] thx for your feedback, will change, one question from me:

What if I include Kafka version in any of the MBeans?  It will easy for external tools to detect Kafka version and use parsing rules for particular version. Something like Elasticsearch provides by http (Kafka will provide by JMX).

{code}
curl localhost:9200 
{
...
  ""version"" : {
    ""number"" : ""1.3.2"",
    ""build_hash"" : ""dee175dbe2f254f3f26992f5d7591939aaefd12f"",
    ""build_timestamp"" : ""2014-08-13T14:29:30Z""
....
  }
}
{code}

Maybe you can point out code place where I can get current Kafka version? ;;;","10/Nov/14 18:16;vladimir.tretyakov;Hi again, added new patches (for code and for docs). New patch was done according to 'Alternative 1'.
How about Kafka version publishing in JMX?
Thx!;;;","11/Nov/14 04:34;junrao;[~vladimir.tretyakov], we could probably just add a new MBean to expose the Kafka version number. Any value in exposing other things like build hash and build timestamp?

Also, could you address my last few comments? For example, it seem #62 is still not addressed. A few more comments on the new patch.

64. ConsumerTopicMetrics: Could you merge the following into a single line?
  val tags = metricId
  match {

65. About the change to the aggregate metric name. It seems that we now have the following MBean name. A couple of comments on this.
kafka.consumer:type=AggregateFetchRequestAndResponseMetrics,name=FetchRequestRateAndTimeMs,clientId=console-consumer-50964
65.1 We only include the following in KafkaMetricsGroup.consumerMetricNameList. That won't match the above aggregate metric's name. So, I am not sure how this metric is removed when closing the consumer. The unit test does pass. So, I am not sure if it's testing the right thing.
    new MetricName(""kafka.consumer"", ""FetchRequestAndResponseMetrics"", ""FetchResponseSize""),
65.1 I think adding Aggregate in front of the class name is a bit weird. The way that we add it in KafkaMetricsGroup is also a bit hacky since it essentially changed the typeName for aggregate metrics w/o actually changing it. I was thinking for aggregate metrics, would it be simpler just to have the following? The fact that it doesn't have any broker level labels is enough an indication that it's an aggregate across all brokers.
kafka.consumer:type=FetchRequestAndResponseMetrics,name=FetchRequestRateAndTimeMs,clientId=console-consumer-50964
;;;","11/Nov/14 11:13;vladimir.tretyakov;Hi [~junrao], 
{quote}
Vladimir Tretyakov, we could probably just add a new MBean to expose the Kafka version number. Any value in exposing other things like build hash and build timestamp?
{quote}
Version will be ok I think, but my question was also about where I can get this Version? Should I hardcoded this Version in MBean? What is the best place for such information? Maybe you already have some special property file?;;;","11/Nov/14 11:41;vladimir.tretyakov;Hi again, 
65. Honestly I'd prefer allBrokers=true and allTopics=true or maybe brokers=aggregated, topics=aggregated.
From first view user will not know what this is about:
{code}
kafka.consumer:type=FetchRequestAndResponseMetrics,name=FetchRequestRateAndTimeMs,clientId=console-consumer-50964
{code}

and only after user will see something like:

{code}
kafka.consumer:type=FetchRequestAndResponseMetrics,name=FetchRequestRateAndTimeMs,clientId=console-consumer-50964, broker=0
kafka.consumer:type=FetchRequestAndResponseMetrics,name=FetchRequestRateAndTimeMs,clientId=console-consumer-50964, broker=1
{code}

user can understand (not sure) that name without 'broker=..' is aggregated value, I'd prefer explicit definition.

[~jjkoshy] what do you think about:
""brokers=aggregated, topics=aggregated"" ? In jconsole user will see ""aggregated"" instead of ""true"", handy from my point of view.
;;;","12/Nov/14 17:22;vladimir.tretyakov;Maybe somebody can answer my last questions? Have to finish with this patch and moving forward! Thx.

Will extract Kafka version like ""Gwen Shapira
"" has suggested in http://search-hadoop.com/m/4TaT4xtk36/Programmatic+Kafka+version+detection%252Fextraction&subj=Programmatic+Kafka+version+detection+extraction+ 

{quote}
So it looks like we can use Gradle to add properties to manifest file and
then use getResourceAsStream to read the file and parse it.

The Gradle part would be something like:
jar.manifest {
            attributes('Implementation-Title': project.name,
            'Implementation-Version': project.version,
            'Built-By': System.getProperty('user.name'),
            'Built-JDK': System.getProperty('java.version'),
            'Built-Host': getHostname(),
            'Source-Compatibility': project.sourceCompatibility,
            'Target-Compatibility': project.targetCompatibility
            )
        }

The code part would be:
this.getClass().getClassLoader().getResourceAsStream(""/META-INF/MANIFEST.MF"")

Does that look like the right approach?
{quote}

What do you think?

What about 65?
{quote}
{quote};;;","12/Nov/14 19:19;junrao;
65. The following are some of the choices that we have.
(1) kafka.server:type=BrokerTopicMetrics,name=AggregateBytesOutPerSec
(2) kafka.server:type=AggregateBrokerTopicMetrics,name=BytesOutPerSec
(3) kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec
(4) kafka.server:type=BrokerTopicMetrics,name=AllTopicsBytesOutPerSec
(5) kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec,allTopics=true
(6) kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec,topics=aggregate
The following is my take. The issue with (1), (2) and (3) is that it's not obvious which dimension is being aggregated upon. I also don't quite like (2) since it breaks the convention that type is the class name. If we do go with this route, I'd prefer that we explicitly create an AggregateBrokerTopicMetrics class instead of sneaking in the prefix in KafkaMetricsGroup. (4), (5) and (6) will all make it clear which dimension is being aggregated upon. (4) is a bit weird now that we support tags since the main purpose of tags is that we don't have to squeeze everything into a single name. So, either (5) and (6) looks reasonable to me. Also, I am not sure how jconsole displays mbeans, but the key/value pairs in the mbean name are supposed to be unordered.

[~jjkoshy], what's your take?

As for the mbean for the Kafka version, could we do that in a separate jira? The approach seems reasonable.;;;","13/Nov/14 01:22;jjkoshy;(1) and (4) seem equivalent (i.e., AllTopics vs Aggregate) - or are you saying that (4) will be AllTopics or AllBrokers as appropriate?

I'm +0 on (5) for the reason I stated above. i.e., it is odd to see ""true"" when browsing mbeans

I'm +0 on (6) as well as ""topics=aggregate"" is a bit odd. The field name suggests it is a list of topics but it is more like a boolean. Between this and (5) I prefer (5).

(3) seems reasonable to me although it is not as clear as having an explicit aggregate term in the type. However, I think (1), (2) and (3) do make it clear enough what is being aggregated: i.e., bytes-out-per-sec aggregated on topic. I actually think ""Broker"" should not be there since this is a broker-side mbean already. i.e., if we had kafka.server:type=TopicMetrics,name=BytesOutPerSec (wouldn't it be clear that the dimension of aggregation is across topics?) i.e., I think we can just make the dimension clear from the typename.

Likewise, it should be clear (for consumers) that FetchRequestAndResponseMetrics is really a broker-level aggregation.
;;;","13/Nov/14 18:50;junrao;Thinking about his a bit more, I agree with [~jjkoshy] that (3) is probably the best choice. It's the simplest. Also, if you look at a metric like the following, it should be clear that the request rate is for a particular client, aggregated. There are different ways that this metrics can be broken down. However, that information shouldn't need to be included in the aggregate metrics.

kafka.consumer:type=FetchRequestAndResponseMetrics,name=FetchRequestRateAndTimeMs,clientId=console-consumer-50964

We can probably keep the name BrokerTopicMetrics so that it can be distinguished from ConsumerTopicMetrics and ProducerTopicMetrics.

[~vladimir.tretyakov], are you ok with this approach?;;;","13/Nov/14 21:44;jjkoshy;Re: BrokerTopicMetrics so that it can be distinguished from ConsumerTopicMetrics and ProducerTopicMetrics

Why do we need to distinguish it? i.e., BrokerTopicMetrics would only be on the server right? Or are you concerned about the possibility of a producer or consumer that happens to run in the same VM as a broker?;;;","14/Nov/14 00:15;junrao;My concern is that in the code, if you have a class TopicMetrics, it's not clear what it is for (broker, consumer or producer). Also, if you have a centralized monitoring system that pulls all Kafka metrics (for both the client and the broker), TopicMetrics will be a bit out of context.;;;","14/Nov/14 13:55;vladimir.tretyakov;Hi, added new patches (code + doc), go by way 3:
(3) kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec


Also added Kafka version MBean, it exposes only Kafka version now (from gradle.properties file). I didn't find easy way where I can get build hash, so only version for now.

I hope it will be my last patches, it is a time consumption to change things many times and test everything each time and prepare patched, so I really hope these patches are good enough and I will not do additional iterations, thx.;;;","14/Nov/14 17:49;junrao;Thanks for the patch. Appreciate your persistence. A few comments below.

80. AppInfo.registerInfo()
80.1 On the server side, this needs to be called in KafkaServerStartable.startup(). Some users will start up a Kafka broker using KafkaServerStartable in a container and not from the command line. 
80.2 On the client side, if there are multiple instances of clients running in the same jvm, registerInfo() will be called multiple times. It would be good if we make sure registerInfo() only register the mbean once no matter how many times it's called. We can maintain an isRegistered flag internally and only register the mbean if the flag is not set. We can also make this a synchronized method.
80.3 There is no need to call registerInfo() in ConsoleConsumer and ProducerPerformance since the mbean will be registered by the consumer/producer client.
80.4 We will need to add the same version mbean in the new java client. We don't need to do that in this jira. Could you file a separate jira to track that?

81. KafkaServer: remove unused import AppInfo

82. TestUtils: Could you fix the indentation in the following?
  def sendMessagesToPartition(configs: Seq[KafkaConfig],
                                    topic: String,
                                    partition: Int,
                                    numMessages: Int,
                                    compression: CompressionCodec = NoCompressionCodec): List[String] = {

83. As I was reviewing KAFKA-1684, I realized that in the future, a broker may have multiple ports: plain text, SSL, SASL, etc. In this patch, the broker-specific mbeans have the tag of brokerHost and brokerPort. This is going to be inconvenient once the broker has more than one port. I was thinking it's simpler if we just add the brokerId tag or both the brokerId and the brokerHost tag. What do you think?;;;","14/Nov/14 18:09;junrao;83. On another thought, the port tag may be ok since a client is only going to connect to one port any way.;;;","14/Nov/14 18:36;vladimir.tretyakov;Thx Jun, will try to fix everything according your last comments.
re 83, yeah host:port is unique pair, so it will work even with KAFKA-1684;;;","14/Nov/14 21:38;jjkoshy;[~junrao] For 80.2, I believe the additional registration will not create any new mbeans. i.e., it should be a no-op.
For 83, we could have one set of mbeans per port right? Or do you think that would be too much? Your suggestion is to drop the port and just unify right? That should also be good.

Also, [~vladimir.tretyakov] your patch needs a rebase as mentioned earlier.;;;","14/Nov/14 22:39;junrao;[~jjkoshy], yes, re-registering an existing mbean is a no-op. However, it would probably be good not to depend on this and to avoid the unnecessary checks on resources.

For 83, chances are a given application is going to use one type of port. So, we can leave this as it is.

The patch is actually intended for 0.8.2 and it applies.;;;","15/Nov/14 06:18;otis;bq. 80.4 We will need to add the same version mbean in the new java client. We don't need to do that in this jira. Could you file a separate jira to track that?

I created KAFKA-1768 a few days ago and have linked it to this issue.;;;","17/Nov/14 11:40;vladimir.tretyakov;Hi, provided new patch, last one?:)


80.1 Done
80.2 Done
80.3 Done
80.4 Do you mean 'new producer'? Can I just reformulate KAFKA-1768? Something like: ""Expose version via JMX in new client""?
81. Done
82. Done
83. Left as is.

PS: I'll be traveling for 3 week starting 19.11 (Wednesday) and won't Internet access.  Can we get this committed before I go pleaase?;;;","17/Nov/14 22:26;junrao;Thanks for the patch. +1 from me. Just a few minor comments from me.

90. Kafka: No need to call AppInfo.registerInfo().

91. ZookeeperConsumerConnector: Could we rename the following method to ownedPartitionsCountMetricTags?
def ownedPartitionsCountMetricName

Could you also provide a patch for trunk?

[~jjkoshy], do you want to look at the patch again?;;;","17/Nov/14 23:19;jjkoshy;+1  (for 0.8.2);;;","18/Nov/14 02:39;junrao;[~vladimir.tretyakov], thanks a lot for your work. Committed to 0.8.2 after fixing 90 and 91.

Will leave the jira open until trunk is patched too.;;;","18/Nov/14 08:05;vladimir.tretyakov;Thx a lot guys, will try prepare patch for trunk today.;;;","19/Nov/14 13:17;vladimir.tretyakov;Added patch for trunk - KAFKA-1481_2014-11-19_16-03-03_trunk.patch;;;","20/Nov/14 02:00;junrao;Thanks for the patch for trunk. +1 and committed to trunk.;;;","22/Dec/14 13:27;vladimir.tretyakov;Hi guys, maybe it is not a best place for this question, but it is related to metric, so I will try:

In code ( ""kafka/core/src/main/scala/kafka/log/FileMessageSet.scala"" ) I see:
 {code}
object LogFlushStats extends KafkaMetricsGroup {
  val logFlushTimer = new KafkaTimer(newTimer(""LogFlushRateAndTimeMs"", TimeUnit.MILLISECONDS, TimeUnit.SECONDS))
}
 {code}

But I don't see this metric in JMX when I attach JConsole to my broker.

Is it ok?
What I have to do to see this metric?
Maybe this bean/metric exists short time and recreates each time after/during log flushes?
Can somebody explain me?

Best regards. Vladimir.
;;;","22/Dec/14 15:41;junrao;The JMX will be registered the first time that the flush stat is recorded. So, try producing some messages to a topic.;;;","22/Dec/14 16:06;vladimir.tretyakov;Hi Jun, thx for answer, already did (added many messages), waited more than 1 day (on our test env), nothing, no bean with such name is in broker process.
;;;","22/Dec/14 17:28;junrao;Ok, the metric does show up. By default, the flush is only done in a background thread and it only flushes old log segments. So, a flush is only called when a new log segment is rolled. If you want to see the metric quicker, you can try setting log.flush.interval.messages to a small value.;;;","24/Dec/14 08:40;vladimir.tretyakov;Hi Jun, thx for answer, it helps (""log.flush.interval.messages"" property), I see data on charts
!https://issues.apache.org/jira/secure/attachment/12689019/logflushes.png!

Maybe you also have a receipt how to force displaying of these metrics kafka.log.LogCleaner:
{code}
newGauge(""max-buffer-utilization-percent"", 
           new Gauge[Int] {
             def value: Int = cleaners.map(_.lastStats).map(100 * _.bufferUtilization).max.toInt
           })
  /* a metric to track the recopy rate of each thread's last cleaning */
  newGauge(""cleaner-recopy-percent"", 
           new Gauge[Int] {
             def value: Int = {
               val stats = cleaners.map(_.lastStats)
               val recopyRate = stats.map(_.bytesWritten).sum.toDouble / math.max(stats.map(_.bytesRead).sum, 1)
               (100 * recopyRate).toInt
             }
           })
  /* a metric to track the maximum cleaning time for the last cleaning from each thread */
  newGauge(""max-clean-time-secs"",
           new Gauge[Int] {
             def value: Int = cleaners.map(_.lastStats).map(_.elapsedSecs).max.toInt
           })
{code}

?

Thx again.;;;","09/Jan/15 23:23;junrao;Also committed the doc change to 0.8.2 documentation.;;;","18/Mar/15 20:54;guozhang;We hit an issue related to this ticket, which adds the ""brokerHost"" / ""brokerPort"" into FetchRequestAndResponseMetrics. The root cause is that when server starts up, it gets local host string by calling:

{code}
InetAddress.getLocalHost.getCanonicalHostName
{code}

which, will possibly just return the textual representation of the IP address if somehow accessing local hostname is not allowed:

http://docs.oracle.com/javase/7/docs/api/java/net/InetAddress.html#getCanonicalHostName%28%29

In our case, the IPV6 address string is returned, which is registered in ZK, read by controller and propogated to brokers through metadata update, and eventually read by consumers. And when that happens, we got the following error:

{code}
2015-03-18 09:46:30 JmxReporter [WARN] Error processing kafka.consumer:type=FetchRequestAndResponseMetrics,name=FetchRequestRateAndTimeMs,clientId=samza_checkpoint_manager-wikipedia_parser-1-1426697189628-0,brokerHost=fe80:0:0:0:7ed1:c3ff:fee0:c60f%4,brokerPort=9092
javax.management.MalformedObjectNameException: Invalid character ':' in value part of property
at javax.management.ObjectName.construct(ObjectName.java:618)
at javax.management.ObjectName.<init>(ObjectName.java:1382)
at com.yammer.metrics.reporting.JmxReporter.onMetricAdded(JmxReporter.java:395)
at com.yammer.metrics.core.MetricsRegistry.notifyMetricAdded(MetricsRegistry.java:516)
at com.yammer.metrics.core.MetricsRegistry.getOrAdd(MetricsRegistry.java:491)
at com.yammer.metrics.core.MetricsRegistry.newTimer(MetricsRegistry.java:320)
at kafka.metrics.KafkaMetricsGroup$class.newTimer(KafkaMetricsGroup.scala:85)
at kafka.consumer.FetchRequestAndResponseMetrics.newTimer(FetchRequestAndResponseStats.scala:26)
at kafka.consumer.FetchRequestAndResponseMetrics.<init>(FetchRequestAndResponseStats.scala:35)
at kafka.consumer.FetchRequestAndResponseStats$$anonfun$1.apply(FetchRequestAndResponseStats.scala:44)
at kafka.consumer.FetchRequestAndResponseStats$$anonfun$1.apply(FetchRequestAndResponseStats.scala:44)
at kafka.utils.Pool.getAndMaybePut(Pool.scala:61)
at kafka.consumer.FetchRequestAndResponseStats.getFetchRequestAndResponseStats(FetchRequestAndResponseStats.scala:51)
at kafka.consumer.SimpleConsumer.fetch(SimpleConsumer.scala:108)
at org.apache.samza.checkpoint.kafka.KafkaCheckpointManager$$anonfun$readLog$1.apply(KafkaCheckpointManager.scala:283)
at org.apache.samza.checkpoint.kafka.KafkaCheckpointManager$$anonfun$readLog$1.apply(KafkaCheckpointManager.scala:256)
at org.apache.samza.util.ExponentialSleepStrategy.run(ExponentialSleepStrategy.scala:82)
at org.apache.samza.checkpoint.kafka.KafkaCheckpointManager.readLog(KafkaCheckpointManager.scala:255)
at org.apache.samza.checkpoint.kafka.KafkaCheckpointManager.readChangeLogPartitionMapping(KafkaCheckpointManager.scala:242)
at org.apache.samza.coordinator.JobCoordinator$.buildJobModel(JobCoordinator.scala:136)
at org.apache.samza.coordinator.JobCoordinator.buildJobModel(JobCoordinator.scala)
at org.apache.samza.job.standalone.controller.StandaloneZkCoordinatorController.refreshOwnership(StandaloneZkCoordinatorController.java:161)
at org.apache.samza.job.standalone.controller.StandaloneZkCoordinatorController.access$900(StandaloneZkCoordinatorController.java:49)
at org.apache.samza.job.standalone.controller.StandaloneZkCoordinatorController$ContainerPathListener.handleChildChange(StandaloneZkCoordinatorController.java:256)
at org.I0Itec.zkclient.ZkClient$7.run(ZkClient.java:568)
at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
{code}

I think the right solution here is that BrokerHost string should also be canonized before adding to sensor tags. [~vladimir.tretyakov] [~junrao] what do you think?;;;"
Remove spam comment from wiki,KAFKA-1478,12717682,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jonbringhurst,jonbringhurst,30/May/14 23:02,30/May/14 23:55,14/Jul/23 05:39,30/May/14 23:55,,,,,,,,,,,,,,0,,,,"A spam comment exists on the wiki at:

https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Replication?focusedCommentId=33294292#comment-33294292",,joestein,jonbringhurst,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,395886,,,Fri May 30 23:55:13 UTC 2014,,,,,,,,,,"0|i1w5p3:",396003,,,,,,,,,,,,,,,,,,,,"30/May/14 23:55;joestein;I took care of this just now. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix a document typo ""in in"" --> ""in""",KAFKA-1474,12717224,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,chengwei-yang,chengwei-yang,chengwei-yang,29/May/14 06:50,29/May/14 15:02,14/Jul/23 05:39,29/May/14 15:02,0.8.0,0.8.1.1,,,,,,,,,website,,,0,,,,"In fact the only metadata retained on a per-consumer basis is the position of the consumer in in the log, called the ""offset"".

In the above line, *in in* should be *in*.",,chengwei-yang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/14 06:51;chengwei-yang;KAFKA-1474.patch;https://issues.apache.org/jira/secure/attachment/12647307/KAFKA-1474.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,395428,,,Thu May 29 15:02:21 UTC 2014,,,,,,,,,,"0|i1w2xr:",395556,,,,,,,,,,,,,,,,,,,,"29/May/14 06:51;chengwei-yang;The patch to fix this typo;;;","29/May/14 15:02;junrao;Thanks for the patch. Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient unit test failure in testRequestHandlingDuringDeleteTopic,KAFKA-1473,12717083,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,junrao,junrao,28/May/14 16:31,04/Jun/14 20:34,14/Jul/23 05:39,04/Jun/14 20:34,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"kafka.admin.DeleteTopicTest > testRequestHandlingDuringDeleteTopic FAILED
    org.scalatest.junit.JUnitTestFailedError: fails with exception
        at org.scalatest.junit.AssertionsForJUnit$class.newAssertionFailedException(AssertionsForJUnit.scala:102)
        at org.scalatest.junit.JUnit3Suite.newAssertionFailedException(JUnit3Suite.scala:149)
        at org.scalatest.Assertions$class.fail(Assertions.scala:731)
        at org.scalatest.junit.JUnit3Suite.fail(JUnit3Suite.scala:149)
        at kafka.admin.DeleteTopicTest.testRequestHandlingDuringDeleteTopic(DeleteTopicTest.scala:118)

        Caused by:
        org.scalatest.junit.JUnitTestFailedError: Test should fail because the topic is being deleted
            at org.scalatest.junit.AssertionsForJUnit$class.newAssertionFailedException(AssertionsForJUnit.scala:101)
            at org.scalatest.junit.JUnit3Suite.newAssertionFailedException(JUnit3Suite.scala:149)
            at org.scalatest.Assertions$class.fail(Assertions.scala:711)
            at org.scalatest.junit.JUnit3Suite.fail(JUnit3Suite.scala:149)
            at kafka.admin.DeleteTopicTest.testRequestHandlingDuringDeleteTopic(DeleteTopicTest.scala:120)
",,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/14 21:38;guozhang;KAFKA-1473.patch;https://issues.apache.org/jira/secure/attachment/12648237/KAFKA-1473.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,395291,,,Wed Jun 04 20:34:09 UTC 2014,,,,,,,,,,"0|i1w247:",395422,,,,,,,,,,,,,,,,,,,,"28/May/14 16:33;junrao;I think we probably need to make sure the metadata cache in the broker no longer has the deleted partitions before testing the failure of the producer requests.;;;","03/Jun/14 21:38;guozhang;Created reviewboard https://reviews.apache.org/r/22220/
 against branch origin/trunk;;;","04/Jun/14 20:34;junrao;Thanks for the patch. +1. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Quickstart documentation contains a minor typo,KAFKA-1470,12716546,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,barisdaylik,barisdaylik,barisdaylik,25/May/14 14:14,28/May/14 04:18,14/Jul/23 05:39,28/May/14 04:18,0.8.1.1,,,,,,,,,,,,,0,,,,Attaching the patch.,,barisdaylik,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/14 14:15;barisdaylik;fix_typo_on_quickstart.diff;https://issues.apache.org/jira/secure/attachment/12646712/fix_typo_on_quickstart.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,394754,,,Wed May 28 04:18:33 UTC 2014,,,,,,,,,,"0|i1vytz:",394889,,,,,,,,,,,,,,,,,,,,"25/May/14 14:15;barisdaylik;Changed number of partitions from two to one, matching the example above.;;;","28/May/14 04:18;junrao;Thanks for the patch. Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Util.abs function does not return correct absolute values for negative values,KAFKA-1469,12716451,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jjkoshy,jjkoshy,23/May/14 21:54,22/Sep/14 22:42,14/Jul/23 05:39,06/Jun/14 17:04,,,,0.8.2.0,,,,,,,,,,0,newbie,patch,,"Reported by Russell Melick. [edit1: I don't think this affects correctness of
the places that use the abs utility since we just need it to return a
consistent positive value, but we should fix this nonetheless]
[edit 2: actually it affects correctness in places that depend on consistent
values across the fix. e.g., the offset manager is determined based on
abs(hash(consumer group)). So after an upgrade that can change]

{code}
     /**
      * Get the absolute value of the given number. If the number is
   Int.MinValue return 0.
      * This is different from java.lang.Math.abs or scala.math.abs in that
   they return Int.MinValue (!).
      */
     def abs(n: Int) = n & 0x7fffffff
{code}

For negative integers, it does not return the absolute value.  It does
appear to do what the comment says for Int.MinValue though.  For example,

{code}
   scala> -1 & 0x7fffffff
   res8: Int = 2147483647

   scala> -2 & 0x7fffffff
   res9: Int = 2147483646

   scala> -2147483647 & 0x7fffffff
   res11: Int = 1

   scala> -2147483648 & 0x7fffffff
   res12: Int = 0
{code}
",,guozhang,jjkoshy,nehanarkhede,sgeller,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/14 20:31;sgeller;KAFKA-1469.patch;https://issues.apache.org/jira/secure/attachment/12648224/KAFKA-1469.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,394659,,,Fri Jun 06 17:04:06 UTC 2014,,,,,,,,,,"0|i1vy9r:",394798,,,,,,,,,,,,,,,,,,,,"03/Jun/14 20:33;sgeller;This patch handles MinValue as described in the comment aswell as handling negative integers. Added unit tests nevertheless.;;;","04/Jun/14 16:38;guozhang;Thanks for the patch, I am +1 on this one.;;;","06/Jun/14 17:02;nehanarkhede;+1. Thanks for the patch.;;;","06/Jun/14 17:04;nehanarkhede;Pushed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve perf tests,KAFKA-1468,12716397,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,23/May/14 18:22,07/Oct/14 00:25,14/Jul/23 05:39,06/Oct/14 23:22,,,,0.8.2.0,,,,,,,,,,0,,,,This is issue is a placeholder for a bunch of improvements that came out of a round of benchmarking.,,guozhang,jkreps,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/14 23:31;guozhang;KAFKA-1468.patch;https://issues.apache.org/jira/secure/attachment/12673236/KAFKA-1468.patch","23/May/14 21:08;jkreps;KAFKA-1468.patch;https://issues.apache.org/jira/secure/attachment/12646606/KAFKA-1468.patch","28/May/14 23:15;jkreps;KAFKA-1468_2014-05-28_16:15:01.patch;https://issues.apache.org/jira/secure/attachment/12647248/KAFKA-1468_2014-05-28_16%3A15%3A01.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,394605,,,Tue Oct 07 00:25:53 UTC 2014,,,,,,,,,,"0|i1vxy7:",394746,,,,,,,,,,,,,,,,,,,,"23/May/14 21:08;jkreps;Created reviewboard https://reviews.apache.org/r/21878/
 against branch trunk;;;","28/May/14 23:15;jkreps;Updated reviewboard https://reviews.apache.org/r/21878/
 against branch trunk;;;","06/Oct/14 23:22;junrao;Resolving the jira since it's already committed.;;;","06/Oct/14 23:31;guozhang;Created reviewboard https://reviews.apache.org/r/26393/diff/
 against branch origin/trunk;;;","07/Oct/14 00:25;junrao;Thanks for the followup patch. +1. Committed to trunk and 0.8.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fixes small documentation typos,KAFKA-1467,12716370,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,,gumchum,gumchum,23/May/14 16:52,23/May/14 18:03,14/Jul/23 05:39,23/May/14 18:03,0.8.1,,,,,,,,,,website,,,0,documentation,,,"Fixes a minor typo in the log compaction section of the documentation.

Also includes second a patch to changes various letter cases used for ZooKeeper (e.g., ""zookeeper"" and ""Zookeeper"") into just ""ZooKeeper."" There's nothing to ensure that newer documentation will adhere to the letter case convention, however.",,gumchum,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/14 16:56;gumchum;KAFKA-1467-consistent-zookeeper-letter-cases.diff;https://issues.apache.org/jira/secure/attachment/12646544/KAFKA-1467-consistent-zookeeper-letter-cases.diff","23/May/14 16:56;gumchum;KAFKA-1467-log-compaction-typo.diff;https://issues.apache.org/jira/secure/attachment/12646545/KAFKA-1467-log-compaction-typo.diff",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,394578,,,Fri May 23 18:03:37 UTC 2014,,,,,,,,,,"0|i1vxs7:",394719,,,,,,,,,,,,,,,,,,,,"23/May/14 18:03;jkreps;Thanks for the patch! Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-reassign-partitions.sh fails when topic name contains dash/hyphen,KAFKA-1465,12716041,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,amuraru,amuraru,22/May/14 10:54,19/Jun/15 06:53,14/Jul/23 05:39,19/Jun/15 06:53,0.8.0,,,0.9.0.0,,,,,,,tools,,,0,,,,"{{./bin/kafka-reassign-partitions.sh --topics-to-move-json-file ~/rebalance-topic.json --broker-list ""18,19"" --zookeeper $ZK_QUORUM --execute}}

{code}
Partitions reassignment failed due to Can't parse json string: null
kafka.common.KafkaException: Can't parse json string: null
	at kafka.utils.Json$.liftedTree1$1(Json.scala:36)
	at kafka.utils.Json$.parseFull(Json.scala:32)
	at kafka.utils.ZkUtils$$anonfun$getReplicaAssignmentForTopics$1.apply(ZkUtils.scala:529)
	at kafka.utils.ZkUtils$$anonfun$getReplicaAssignmentForTopics$1.apply(ZkUtils.scala:525)
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
	at scala.collection.immutable.List.foreach(List.scala:45)
	at kafka.utils.ZkUtils$.getReplicaAssignmentForTopics(ZkUtils.scala:525)
	at kafka.admin.ReassignPartitionsCommand$.main(ReassignPartitionsCommand.scala:112)
	at kafka.admin.ReassignPartitionsCommand.main(ReassignPartitionsCommand.scala)
Caused by: java.lang.NullPointerException
	at scala.util.parsing.combinator.lexical.Scanners$Scanner.<init>(Scanners.scala:52)
	at scala.util.parsing.json.JSON$.parseRaw(JSON.scala:71)
	at scala.util.parsing.json.JSON$.parseFull(JSON.scala:85)
	at kafka.utils.Json$.liftedTree1$1(Json.scala:33)
	... 8 more

{code}

*rebalance-topic.json*
{code}
{""topics"":
     [{""topic"": ""metrics-logs""}],
     ""version"":1
}
{code}",,amuraru,junrao,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,394245,,,Fri Jun 19 06:53:24 UTC 2015,,,,,,,,,,"0|i1vvr3:",394383,,,,,,,,,,,,,,,,,,,,"22/May/14 14:55;junrao;Could you paste the content in rebalance-topic.json? Thanks,;;;","28/May/14 14:11;amuraru;[~junrao] Updated the description with the json content;;;","28/May/14 16:21;junrao;Could you try this on 0.8.1.1? The reassignment tool is only stable in 0.8.1.x.

Thanks,;;;","19/Jun/15 06:53;omkreddy;Not able to reproduce this issue on trunk. Might have fixed in previous commits. So closing this issue. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka.tools.ConsumerOffsetChecker throws NoNodeException,KAFKA-1459,12714793,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriharsha,t1ckt0ck,t1ckt0ck,16/May/14 15:58,22/Jul/14 14:31,14/Jul/23 05:39,27/May/14 20:44,0.8.1.1,,,0.8.2.0,,,,,,,tools,,,0,newbie,,,"When using the kafka.tools.ConsumerOffsetChecker to check offsets for consumers that are doing manual offset management, and offsets for some but not all partitions have been stored, the offset checker will throw a no node exception.  It should probably return 0 for partitions that dont have an offset recorded yet.

In this case I was using github.com/pinterest/secor, which may read thousands or millions of messages from a partition before committing an offset.",,nehanarkhede,sriharsha,t1ckt0ck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/14 02:05;sriharsha;KAFKA-1459.patch;https://issues.apache.org/jira/secure/attachment/12646653/KAFKA-1459.patch","28/May/14 03:20;sriharsha;KAFKA-1459_2014-05-27_20:19:55.patch;https://issues.apache.org/jira/secure/attachment/12647041/KAFKA-1459_2014-05-27_20%3A19%3A55.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,393106,,,Thu May 29 16:46:00 UTC 2014,,,,,,,,,,"0|i1voyv:",393273,,,,,,,,,,,,,,,,,,,,"24/May/14 02:05;sriharsha;Created reviewboard https://reviews.apache.org/r/21883/diff/
 against branch origin/trunk;;;","27/May/14 20:44;nehanarkhede;Thanks for the patch, pushed to trunk;;;","28/May/14 03:20;sriharsha;Updated reviewboard https://reviews.apache.org/r/21883/diff/
 against branch origin/trunk;;;","29/May/14 16:46;nehanarkhede;Checked in the follow up patch as well;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a channel queue jmx in Mirror Maker,KAFKA-1453,12714528,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,15/May/14 16:57,22/May/14 21:53,14/Jul/23 05:39,16/May/14 04:27,,,,0.8.2.0,,,,,,,,,,0,,,,This metric would be very helpful identifying which side (consumer or producer) is the bottleneck for slowness.,,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/14 21:34;guozhang;KAFKA-1453.patch;https://issues.apache.org/jira/secure/attachment/12645332/KAFKA-1453.patch","15/May/14 17:12;guozhang;KAFKA-1453.patch;https://issues.apache.org/jira/secure/attachment/12645040/KAFKA-1453.patch","15/May/14 23:58;guozhang;KAFKA-1453_2014-05-15_16:57:56.patch;https://issues.apache.org/jira/secure/attachment/12645125/KAFKA-1453_2014-05-15_16%3A57%3A56.patch","16/May/14 00:32;guozhang;KAFKA-1453_2014-05-15_17:31:57.patch;https://issues.apache.org/jira/secure/attachment/12645135/KAFKA-1453_2014-05-15_17%3A31%3A57.patch","16/May/14 00:33;guozhang;KAFKA-1453_2014-05-15_17:33:04.patch;https://issues.apache.org/jira/secure/attachment/12645137/KAFKA-1453_2014-05-15_17%3A33%3A04.patch","16/May/14 02:05;guozhang;KAFKA-1453_2014-05-15_19:05:16.patch;https://issues.apache.org/jira/secure/attachment/12645161/KAFKA-1453_2014-05-15_19%3A05%3A16.patch","16/May/14 16:56;guozhang;KAFKA-1453_2014-05-16_09:56:51.patch;https://issues.apache.org/jira/secure/attachment/12645265/KAFKA-1453_2014-05-16_09%3A56%3A51.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,392841,,,Fri May 16 22:32:29 UTC 2014,,,,,,,,,,"0|i1vndr:",393015,,,,,,,,,,,,,,,,,,,,"15/May/14 17:12;guozhang;Created reviewboard https://reviews.apache.org/r/21491/
 against branch origin/trunk;;;","15/May/14 23:58;guozhang;Updated reviewboard https://reviews.apache.org/r/21491/
 against branch origin/trunk;;;","16/May/14 00:32;guozhang;Updated reviewboard  against branch origin/trunk;;;","16/May/14 00:33;guozhang;Updated reviewboard https://reviews.apache.org/r/21491/
 against branch origin/trunk;;;","16/May/14 02:05;guozhang;Updated reviewboard https://reviews.apache.org/r/21491/
 against branch origin/trunk;;;","16/May/14 04:27;junrao;Thanks for the patch. +1 and committed to trunk.;;;","16/May/14 16:56;guozhang;Updated reviewboard https://reviews.apache.org/r/21491/
 against branch origin/trunk;;;","16/May/14 17:53;junrao;Thanks for the follow-up patch. +1. Committed to trunk after a minor modification of the comment.;;;","16/May/14 21:34;guozhang;Created reviewboard https://reviews.apache.org/r/21582/
 against branch origin/trunk;;;","16/May/14 22:32;junrao;Thanks for the second follow-up patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker stuck due to leader election race ,KAFKA-1451,12714147,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,omkreddy,mmakowski,mmakowski,14/May/14 10:52,19/Nov/15 17:43,14/Jul/23 05:39,30/Jul/14 15:15,0.8.1.1,,,0.8.2.0,,,,,,,core,,,2,newbie,,,"h3. Symptoms

The broker does not become available due to being stuck in an infinite loop while electing leader. This can be recognised by the following line being repeatedly written to server.log:

{code}
[2014-05-14 04:35:09,187] INFO I wrote this conflicted ephemeral node [{""version"":1,""brokerid"":1,""timestamp"":""1400060079108""}] at /controller a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
{code}

h3. Steps to Reproduce

In a single kafka 0.8.1.1 node, single zookeeper 3.4.6 (but will likely behave the same with the ZK version included in Kafka distribution) node setup:

# start both zookeeper and kafka (in any order)
# stop zookeeper
# stop kafka
# start kafka
# start zookeeper

h3. Likely Cause

{{ZookeeperLeaderElector}} subscribes to data changes on startup, and then triggers an election. if the deletion of ephemeral {{/controller}} node associated with previous zookeeper session of the broker happens after subscription to changes in new session, election will be invoked twice, once from {{startup}} and once from {{handleDataDeleted}}:

* {{startup}}: acquire {{controllerLock}}
* {{startup}}: subscribe to data changes
* zookeeper: delete {{/controller}} since the session that created it timed out
* {{handleDataDeleted}}: {{/controller}} was deleted
* {{handleDataDeleted}}: wait on {{controllerLock}}
* {{startup}}: elect -- writes {{/controller}}
* {{startup}}: release {{controllerLock}}
* {{handleDataDeleted}}: acquire {{controllerLock}}
* {{handleDataDeleted}}: elect -- attempts to write {{/controller}} and then gets into infinite loop as a result of conflict

{{createEphemeralPathExpectConflictHandleZKBug}} assumes that the existing znode was written from different session, which is not true in this case; it was written from the same session. That adds to the confusion.

h3. Suggested Fix

In {{ZookeeperLeaderElector.startup}} first run {{elect}} and then subscribe to data changes.",,aseychell,becket_qin,chrisbeach,dude9527,fpj,joestein,junrao,kenmacd,laxpio,longtimer,marcusai,mazhar.shaikh.in,mmakowski,nehanarkhede,noxis,omkreddy,rusty@2211,sinewy,zcox,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1585,KAFKA-1387,,,,,,,,,"26/Jul/14 09:48;omkreddy;KAFKA-1451.patch;https://issues.apache.org/jira/secure/attachment/12657983/KAFKA-1451.patch","28/Jul/14 15:00;omkreddy;KAFKA-1451_2014-07-28_20:27:32.patch;https://issues.apache.org/jira/secure/attachment/12658141/KAFKA-1451_2014-07-28_20%3A27%3A32.patch","29/Jul/14 04:46;omkreddy;KAFKA-1451_2014-07-29_10:13:23.patch;https://issues.apache.org/jira/secure/attachment/12658354/KAFKA-1451_2014-07-29_10%3A13%3A23.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,392460,,,Thu Nov 19 17:43:57 UTC 2015,,,,,,,,,,"0|i1vl5b:",392645,,,,,,,,,,,,,,,,,,,,"17/Jul/14 14:47;kenmacd;This can also be caused by restarting Kafka quickly after a sigkill. I had a supervisord config file with 'stopwaitsecs=1' and it would pretty reliably create a hung Kafka process.;;;","17/Jul/14 16:04;junrao;Thanks for reporting this. Very interesting. That does sound like a potential problem. The problem is that ZookeeperLeaderElector.elect assumes that no controller exists. However, this may not be true. One possible solution is to first check the existence of the controller from ZK before creating the ephemeral node. ;;;","18/Jul/14 15:46;nehanarkhede;Just checking the existence is not enough since there is a risk of not electing a controller at all if all brokers do the same and the node disappears. Following will work
1. Register watch
2. Check existence and elect if one does not exist

#1 ensures that if the node disappears, an election will take place;;;","21/Jul/14 04:40;junrao;Neha, I am not sure if #1 is need. We can get into elect from two paths (1) from startup or (2) from handleDeleted. If it's from startup, we already register the watcher before calling elect. If it's from handleDeleted, it means that the watcher must have already been registered. So, once in elect, we know the watcher is already registered. So if after we check the existence of the controller node and the controller node goes away immediately afterward, the watcher is guaranteed to be triggered.;;;","26/Jul/14 09:48;omkreddy;Created reviewboard https://reviews.apache.org/r/23962/diff/
 against branch origin/trunk;;;","26/Jul/14 10:02;omkreddy;Uploaded a patch which checks controller existence in leader election process.
With this patch i am not able to reproduce the issue.
;;;","28/Jul/14 14:51;omkreddy;Updated reviewboard https://reviews.apache.org/r/23962/diff/
 against branch origin/trunk;;;","28/Jul/14 14:57;omkreddy;Created reviewboard https://reviews.apache.org/r/23983/diff/
 against branch origin/trunk;;;","28/Jul/14 15:00;omkreddy;Updated reviewboard https://reviews.apache.org/r/23962/diff/
 against branch origin/trunk;;;","29/Jul/14 04:46;omkreddy;Updated reviewboard https://reviews.apache.org/r/23962/diff/
 against branch origin/trunk;;;","30/Jul/14 15:15;junrao;Thanks for the latest patch. +1 and committed to trunk.;;;","10/Aug/14 08:09;joestein;Hi, two issues so far where found with leader election https://issues.apache.org/jira/browse/KAFKA-1387?focusedCommentId=14087063&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14087063 I don't know if the issues are related to each other or even to this just yet... the issues found were not happening on the 0.8.1 branch.... could be another 0.8.2 patch I supose but before I started trying to test on a 0.8.2 version without this patch (to isolate the root cause) I wanted to see if this type of scenario was tested or what thoughts were in general to this patch and how it might be affecting either of the two issues found in 0.8.2 trunk?  ;;;","10/Aug/14 22:46;junrao;Joe,

KAFKA-1387 seems to be related to broker registration and this jira only fixes how the controller is registered in ZK. So, I am not sure if they are related.;;;","05/Feb/15 06:49;sinewy;I think the patch dose not RESOLVE the problem,it will be happen again when zk event notification request arrive long after the /controller   has  deleted.
kafkaController.onControllerResignation method has executed,but elect action is return when it find that ""/controller"" znode has already been created;;;","04/Mar/15 10:47;aseychell;I just encountered this issue on version 0.8.2.0 after a period of slow PC performance and perhaps zookeeper and kafka were slow to communicate between each other (possibly the same issue highlighted by [~sinewy].  This resulted in an infinite loop in attempting to created the ephemeral node.  The logs that were continously being written are as follows:

[2015-03-03 13:48:19,831] INFO conflict in /brokers/ids/0 data: {""jmx_port"":-1,""timestamp"":""1425386833617"",""host"":""MTDKP119.ix.com"",""version"":1,""port"":9092} stored data: {""jmx_port"":-1,""timestamp"":""1425380575230"",""host"":""MTDKP119.ix.com"",""version"":1,""port"":9092} (kafka.utils.ZkUtils$)

[2015-03-03 13:48:19,832] INFO I wrote this conflicted ephemeral node [{""jmx_port"":-1,""timestamp"":""1425386833617"",""host"":""MTDKP119.ix.com"",""version"":1,""port"":9092}] at /brokers/ids/0 a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)

[2015-03-03 13:48:25,844] INFO conflict in /brokers/ids/0 data: {""jmx_port"":-1,""timestamp"":""1425386833617"",""host"":""MTDKP119.ix.com"",""version"":1,""port"":9092} stored data: {""jmx_port"":-1,""timestamp"":""1425380575230"",""host"":""MTDKP119.ix.com"",""version"":1,""port"":9092} (kafka.utils.ZkUtils$);;;","28/Apr/15 06:48;marcusai;I have also hit this issue on version 0.8.2.0. It occurred directly after Zookeeper got restarted:

[2015-04-27 03:47:03,291] INFO conflict in /brokers/ids/2 data: {""jmx_port"":-1,""timestamp"":""1430038275477"",""host"":""ams5mdppdmsbacmq01b.markit.partners"",""version"":1,""port"":9092} stored data: {""jmx_port"":-1,""timestamp"":""1430036480690"",""host"":""ams5mdppdmsbacmq01b.markit.partners"",""version"":1,""port"":9092} (kafka.utils.ZkUtils$)
[2015-04-27 03:47:03,292] INFO I wrote this conflicted ephemeral node [{""jmx_port"":-1,""timestamp"":""1430038275477"",""host"":""ams5mdppdmsbacmq01b.markit.partners"",""version"":1,""port"":9092}] at /brokers/ids/2 a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$);;;","19/Jun/15 15:26;rusty@2211;Hit this issue on version 0.8.2.1 when twiiterstream generate the large data i have one topic with two broker and two partition


[2015-06-19 20:35:10,141] INFO I wrote this conflicted ephemeral node [{""jmx_port"":10000,""timestamp"":""1434726183806"",""host"":""localhost.localdomain"",""version"":1,""port"":9093}] at /brokers/ids/2 a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
[2015-06-19 20:35:16,246] INFO conflict in /brokers/ids/2 data: {""jmx_port"":10000,""timestamp"":""1434726183806"",""host"":""localhost.localdomain"",""version"":1,""port"":9093} stored data: {""jmx_port"":10000,""timestamp"":""1434726044184"",""host"":""localhost.localdomain"",""version"":1,""port"":9093} (kafka.utils.ZkUtils$)
[2015-06-19 20:35:16,796] INFO I wrote this conflicted ephemeral node [{""jmx_port"":10000,""timestamp"":""1434726183806"",""host"":""localhost.localdomain"",""version"":1,""port"":9093}] at /brokers/ids/2 a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
[2015-06-19 20:35:22,965] INFO conflict in /brokers/ids/2 data: {""jmx_port"":10000,""timestamp"":""1434726183806"",""host"":""localhost.localdomain"",""version"":1,""port"":9093} stored data: {""jmx_port"":10000,""timestamp"":""1434726044184"",""host"":""localhost.localdomain"",""version"":1,""port"":9093} (kafka.utils.ZkUtils$)
[2015-06-19 20:35:22,967] INFO I wrote this conflicted ephemeral node [{""jmx_port"":10000,""timestamp"":""1434726183806"",""host"":""localhost.localdomain"",""version"":1,""port"":9093}] at /brokers/ids/2 a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
[2015-06-19 20:35:29,159] INFO conflict in /brokers/ids/2 data: {""jmx_port"":10000,""timestamp"":""1434726183806"",""host"":""localhost.localdomain"",""version"":1,""port"":9093} stored data: {""jmx_port"":10000,""timestamp"":""1434726044184"",""host"":""localhost.localdomain"",""version"":1,""port"":9093} (kafka.utils.ZkUtils$)
[2015-06-19 20:35:29,161] INFO I wrote this conflicted ephemeral node [{""jmx_port"":10000,""timestamp"":""1434726183806"",""host"":""localhost.localdomain"",""version"":1,""port"":9093}] at /brokers/ids/2 a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
[2015-06-19 20:35:35,219] INFO conflict in /brokers/ids/2 data: {""jmx_port"":10000,""timestamp"":""1434726183806"",""host"":""localhost.localdomain"",""version"":1,""port"":9093} stored data: {""jmx_port"":10000,""timestamp"":""1434726044184"",""host"":""localhost.localdomain"",""version"":1,""port"":9093} (kafka.utils.ZkUtils$)
[2015-06-19 20:35:35,221] INFO I wrote this conflicted ephemeral node [{""jmx_port"":10000,""timestamp"":""1434726183806"",""host"":""localhost.localdomain"",""version"":1,""port"":9093}] at /brokers/ids/2 a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
[2015-06-19 20:35:41,338] INFO conflict in /brokers/ids/2 data: {""jmx_port"":10000,""timestamp"":""1434726183806"",""host"":""localhost.localdomain"",""version"":1,""port"":9093} stored data: {""jmx_port"":10000,""timestamp"":""1434726044184"",""host"":""localhost.localdomain"",""version"":1,""port"":9093} (kafka.utils.ZkUtils$)
[2015-06-19 20:35:42,208] INFO I wrote this conflicted ephemeral node [{""jmx_port"":10000,""timestamp"":""1434726183806"",""host"":""localhost.localdomain"",""version"":1,""port"":9093}] at /brokers/ids/2 a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
;;;","26/Aug/15 13:27;longtimer;I too am seeing this issue in 0.8.2.1.;;;","17/Sep/15 08:15;dude9527;Also occurred in 3 node kafka 0.8.2.1 cluster;;;","06/Oct/15 08:50;laxpio;also hit in 0.8.2.1,and the /controller node in zk is lost.;;;","06/Oct/15 17:15;becket_qin;[~laxpio] May be related to KAFKA-2437.;;;","06/Oct/15 17:18;fpj;Maybe related to KAFKA-1387?;;;","19/Nov/15 17:01;zcox;We experienced this yesterday on a 3-node 0.8.2.1 cluster, which caused a major outage for several hours. Restarting Kafka brokers several times, along with restarting Zookeeper nodes, did not resolve the issue. We identified one of the brokers that seemed to be going in/out of ISRs repeatedly, and ended up deleting all of its state on disk & restarting it. This was the only thing that finally resolved the issue. Maybe there was some corrupt state on that broker's disk? We still have that broker's state (moved its data dir, didn't actually delete) if that is helpful at all.;;;","19/Nov/15 17:33;fpj;[~zcox] if you observed messages like the ones in this comment above

https://issues.apache.org/jira/browse/KAFKA-1451?focusedCommentId=14593515&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14593515

then I suspect this will be resolved with the fix of KAFKA-1387, which will be available in 0.9.;;;","19/Nov/15 17:43;zcox;[~fpj] Yes we saw the ""I wrote this conflicted ephemeral node"" error messages, we saw lots of partitions in/out of ISRs and a lot of this too:

{code}
[2015-11-19 01:05:51,685] INFO Opening socket connection to server ip-10-10-1-35.ec2.internal/10.10.1.35:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2015-11-19 01:05:51,685] INFO Socket connection established to ip-10-10-1-35.ec2.internal/10.10.1.35:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2015-11-19 01:05:51,687] INFO Unable to reconnect to ZooKeeper service, session 0x54a0e5799a8195d has expired, closing socket connection (org.apache.zookeeper.ClientCnxn)
[2015-11-19 01:05:51,687] INFO zookeeper state changed (Expired) (org.I0Itec.zkclient.ZkClient)
[2015-11-19 01:05:51,687] INFO Initiating client connection, connectString=zookeeper1.production.redacted.com:2181,zookeeper2.production.redacted.com:2181,zookeeper3.production.redacted.com:2181/kafka sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@ace1333 (org.apache.zookeeper.ZooKeeper)
[2015-11-19 01:05:51,701] INFO EventThread shut down (org.apache.zookeeper.ClientCnxn)
[2015-11-19 01:05:51,701] ERROR Error handling event ZkEvent[New session event sent to kafka.controller.KafkaController$SessionExpirationListener@2261adb8] (org.I0Itec.zkclient.ZkEventThread)
java.lang.IllegalStateException: Kafka scheduler has not been started
  at kafka.utils.KafkaScheduler.ensureStarted(KafkaScheduler.scala:114)
  at kafka.utils.KafkaScheduler.shutdown(KafkaScheduler.scala:86)
  at kafka.controller.KafkaController.onControllerResignation(KafkaController.scala:350)
  at kafka.controller.KafkaController$SessionExpirationListener$$anonfun$handleNewSession$1.apply$mcZ$sp(KafkaController.scala:1108)
  at kafka.controller.KafkaController$SessionExpirationListener$$anonfun$handleNewSession$1.apply(KafkaController.scala:1107)
  at kafka.controller.KafkaController$SessionExpirationListener$$anonfun$handleNewSession$1.apply(KafkaController.scala:1107)
  at kafka.utils.Utils$.inLock(Utils.scala:535)
  at kafka.controller.KafkaController$SessionExpirationListener.handleNewSession(KafkaController.scala:1107)
  at org.I0Itec.zkclient.ZkClient$4.run(ZkClient.java:472)
  at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[2015-11-19 01:05:51,701] INFO re-registering broker info in ZK for broker 3 (kafka.server.KafkaHealthcheck)
[2015-11-19 01:05:51,701] INFO Opening socket connection to server ip-10-10-1-104.ec2.internal/10.10.1.104:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2015-11-19 01:05:51,702] INFO Socket connection established to ip-10-10-1-104.ec2.internal/10.10.1.104:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2015-11-19 01:05:51,713] INFO Session establishment complete on server ip-10-10-1-104.ec2.internal/10.10.1.104:2181, sessionid = 0x64a0e57972a1a85, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2015-11-19 01:05:51,713] INFO zookeeper state changed (SyncConnected) (org.I0Itec.zkclient.ZkClient)
[2015-11-19 01:05:51,718] INFO Registered broker 3 at path /brokers/ids/3 with address mesos-slave3.production.redacted.com:9092. (kafka.utils.ZkUtils$)
[2015-11-19 01:05:51,718] INFO done re-registering broker (kafka.server.KafkaHealthcheck)
[2015-11-19 01:05:51,718] INFO Subscribing to /brokers/topics path to watch for new topics (kafka.server.KafkaHealthcheck)
[2015-11-19 01:05:51,721] INFO New leader is 1 (kafka.server.ZookeeperLeaderElector$LeaderChangeListener)
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controlled shutdown deadlock when trying to send state updates,KAFKA-1447,12713799,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,smeder,smeder,13/May/14 00:28,31/Aug/17 17:01,14/Jul/23 05:39,31/Aug/17 17:01,0.8.0,,,,,,,,,,controller,,,1,newbie++,,,"We're seeing controlled shutdown indefinitely stuck on trying to send out state change messages to the other brokers:

[2014-05-03 04:01:30,580] INFO [Socket Server on Broker 4], Shutdown completed (kafka.network.SocketServer)
[2014-05-03 04:01:30,581] INFO [Kafka Request Handler on Broker 4], shutting down (kafka.server.KafkaRequestHandlerPool)

and stuck on:

""kafka-request-handler-12"" daemon prio=10 tid=0x00007f1f04a66800 nid=0x6e79 waiting on condition [0x00007f1ad5767000]
java.lang.Thread.State: WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
parking to wait for <0x000000078e91dc20> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:349)
at kafka.controller.ControllerChannelManager.sendRequest(ControllerChannelManager.scala:57)
locked <0x000000078e91dc38> (a java.lang.Object)
at kafka.controller.KafkaController.sendRequest(KafkaController.scala:655)
at kafka.controller.ControllerBrokerRequestBatch$$anonfun$sendRequestsToBrokers$2.apply(ControllerChannelManager.scala:298)
at kafkler.ControllerBrokerRequestBatch$$anonfun$sendRequestsToBrokers$2.apply(ControllerChannelManager.scala:290)
at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:95)
at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:95)
at scala.collection.Iterator$class.foreach(Iterator.scala:772)
at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:45)
at scala.collection.mutable.HashMap.foreach(HashMap.scala:95)
at kafka.controller.ControllerBrokerRequestBatch.sendRequestsToBrokers(ControllerChannelManager.scala:290)
at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:97)
at kafka.controller.KafkaController$$anonfun$shutdownBroker$3$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(KafkaController.scala:269)
at kafka.controller.KafkaController$$anonfun$shutdownBroker$3$$anonfun$apply$1$$anonfun$apply$mcV$sp$3.apply(KafkaController.scala:253)
at scala.Option.foreach(Option.scala:197)
at kafka.controller.KafkaController$$anonfun$shutdownBroker$3$$anonfun$apply$1.apply$mcV$sp(KafkaController.scala:253)
at kafka.controller.KafkaController$$anonfun$shutdownBroker$3$$anonfun$apply$1.apply(KafkaController.scala:253)
at kafka.controller.KafkaController$$anonfun$shutdownBroker$3$$anonfun$apply$1.apply(KafkaController.scala:253)
at kafka.utils.Utils$.inLock(Utils.scala:538)
at kafka.controller.KafkaController$$anonfun$shutdownBroker$3.apply(KafkaController.scala:252)
at kafka.controller.KafkaController$$anonfun$shutdownBroker$3.apply(KafkaController.scala:249)
at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:130)
at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:275)
at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:275)
at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:275)
at kafka.controller.KafkaController.shutdownBroker(KafkaController.scala:249)
locked <0x000000078b495af0> (a java.lang.Object)
at kafka.server.KafkaApis.handleControlledShutdownRequest(KafkaApis.scala:264)
at kafka.server.KafkaApis.handle(KafkaApis.scala:192)
at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:42)
at java.lang.Thread.run(Thread.java:722)",,becket_qin,guozhang,jjkoshy,jkreps,jozi-k,junrao,nehanarkhede,omkreddy,rudolf.sima,smeder,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,392115,,,Thu Aug 31 17:01:45 UTC 2017,,,,,,,,,,"0|i1vj1j:",392309,,,,,,,,,,,,,,,,,,,,"13/May/14 15:32;guozhang;Hi Sam, which version are you using?;;;","13/May/14 16:05;smeder;0.8.0;;;","13/May/14 20:41;guozhang;This may be due a known issue in 0.8.0, could you try with 0.8.1.1?;;;","14/May/14 13:04;smeder;We'll be rolling out 0.8.1.1 soon, but since we can't reproduce this easily it is going to be hard to validate. I did look through any of the Jira issues related to controlled shutdown to see if this was already addressed and didn't see any that seemed to match this situation. I'll report back once we have run with 0.8.1.1 for a while.;;;","15/May/14 22:17;jjkoshy;Do you still have the preceding log and see if controlled shutdown actually succeeded or it ran out of retries?
Was broker 4 the controller at the time it was shut down? It could be that it did not finish sending all the state change requests to itself and will never finish because the socket server has been shut down.
Also, do you have the full stack trace?;;;","19/May/14 19:00;smeder;I don't have those logs anymore, should have grabbed all of it - my bad.

I do believe broker 4 was the controller at the time and yes, my guess is that it had something to do with sending requests to itself.

That is the full stack trace (from the thread dump), there were no stack-traces in the logs.;;;","27/Aug/14 15:57;rudolf.sima;The bug seems to be still present in 0.8.2. We ran into the issue when bouncing 18 brokers at once with controlled shutdown enabled, which led to this kind of deadlock. As a workaround, we have increased controller.message.queue.size to 10000 (10 is default). Are there any pitfalls of using large controller message queue sizes?;;;","15/Sep/14 01:58;nehanarkhede;[~rudolf.sima], It would help immensely if you can share the controller logs and the entire thread dump when you observe this issue. ;;;","16/Sep/14 04:58;junrao;Rudolf,

Controlled shutdown is designed for rolling bounces. ;;;","07/Feb/15 21:56;jkreps;Does this problem still exist?;;;","08/Feb/15 22:38;becket_qin;I think KAFKA-1305 solved this issue.;;;","31/Aug/17 17:01;omkreddy; Pl reopen if you think the issue still exists
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New Producer should send all partitions that have non-empty batches when on of them is ready,KAFKA-1445,12713405,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,guozhang,guozhang,09/May/14 22:30,17/May/16 14:31,14/Jul/23 05:39,15/May/14 23:58,,,,0.8.2.0,,,,,,,,,,0,,,,"One difference between the new producer and the old producer is that on the new producer the linger time is per partition, instead of global. Therefore, when the traffic is low, the sender will likely expire partitions one-by-one and send lots of small request containing only a few partitions with a few data, resulting largely increased request rate.

One solution of it would be to let senders select all partitions that have non-empty batches when on of them is ready.",,guozhang,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/May/14 18:18;guozhang;KAFKA-1445.patch;https://issues.apache.org/jira/secure/attachment/12644669/KAFKA-1445.patch","10/May/14 21:39;guozhang;KAFKA-1445.patch;https://issues.apache.org/jira/secure/attachment/12644302/KAFKA-1445.patch","13/May/14 18:25;guozhang;KAFKA-1445_2014-05-13_11:25:13.patch;https://issues.apache.org/jira/secure/attachment/12644672/KAFKA-1445_2014-05-13_11%3A25%3A13.patch","14/May/14 23:24;guozhang;KAFKA-1445_2014-05-14_16:24:25.patch;https://issues.apache.org/jira/secure/attachment/12644906/KAFKA-1445_2014-05-14_16%3A24%3A25.patch","14/May/14 23:28;guozhang;KAFKA-1445_2014-05-14_16:28:06.patch;https://issues.apache.org/jira/secure/attachment/12644907/KAFKA-1445_2014-05-14_16%3A28%3A06.patch","15/May/14 22:15;guozhang;KAFKA-1445_2014-05-15_15:15:37.patch;https://issues.apache.org/jira/secure/attachment/12645099/KAFKA-1445_2014-05-15_15%3A15%3A37.patch","15/May/14 22:19;guozhang;KAFKA-1445_2014-05-15_15:19:10.patch;https://issues.apache.org/jira/secure/attachment/12645100/KAFKA-1445_2014-05-15_15%3A19%3A10.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,391721,,,Thu May 15 23:58:13 UTC 2014,,,,,,,,,,"0|i1vgpz:",391929,,,,,,,,,,,,,,,,,,,,"10/May/14 21:39;guozhang;Created reviewboard https://reviews.apache.org/r/21304/
 against branch origin/trunk;;;","12/May/14 16:58;guozhang;[~jkreps] Could you give this a look?;;;","13/May/14 18:18;guozhang;Created reviewboard https://reviews.apache.org/r/21398/
 against branch origin/trunk;;;","13/May/14 18:25;guozhang;Updated reviewboard https://reviews.apache.org/r/21398/
 against branch origin/trunk;;;","14/May/14 23:24;guozhang;Updated reviewboard https://reviews.apache.org/r/21398/
 against branch origin/trunk;;;","14/May/14 23:28;guozhang;Updated reviewboard https://reviews.apache.org/r/21398/
 against branch origin/trunk;;;","15/May/14 22:15;guozhang;Updated reviewboard https://reviews.apache.org/r/21398/
 against branch origin/trunk;;;","15/May/14 22:19;guozhang;Updated reviewboard https://reviews.apache.org/r/21398/
 against branch origin/trunk;;;","15/May/14 22:27;jkreps;Looks good to me, if no comments from others I will apply this.;;;","15/May/14 23:58;jkreps;Applied.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka.javaapi.TopicMetadata and PartitionMetadata doesn't forward the toString method,KAFKA-1444,12713298,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,sriharsha,thecoop1984,thecoop1984,09/May/14 12:24,22/Jul/14 14:38,14/Jul/23 05:39,27/May/14 17:21,,,,0.8.2.0,,,,,,,,,,0,newbie,,,the kafka.javaapi.PartitionMetadata and TopicMetadata classes don't forward the toString method to the underlying kafka.api.PartitionMetadata/TopicMetadata classes along with the other methods. This means toString on these classes doesn't work properly.,,nehanarkhede,sriharsha,thecoop1984,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/14 17:06;sriharsha;KAFKA-1444.patch;https://issues.apache.org/jira/secure/attachment/12645820/KAFKA-1444.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,391614,,,Tue May 27 17:21:27 UTC 2014,,,,,,,,,,"0|i1vg33:",391826,,,,,,,,,,,,,,,,,,,,"20/May/14 17:06;sriharsha;Created reviewboard https://reviews.apache.org/r/21712/diff/
 against branch origin/trunk;;;","27/May/14 17:21;nehanarkhede;Thanks for the patch, pushed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RBTools post-review is deprecated,KAFKA-1442,12713204,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,sriharsha,sriharsha,sriharsha,08/May/14 22:34,22/Jul/14 14:40,14/Jul/23 05:39,08/May/14 23:06,,,,0.8.2.0,,,,,,,,,,0,,,,"kafka-patch-review.py uses RBTools post-review for sending patch reviews. post-review command is deprecated and replaced by rbt post in their latest release. 
kafka-patch-review.py updates the jira ticket with attaching patch even if the RBTools are not found.
",,nehanarkhede,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/May/14 22:56;nehanarkhede;KAFKA-1442.patch;https://issues.apache.org/jira/secure/attachment/12644039/KAFKA-1442.patch","08/May/14 22:36;sriharsha;KAFKA-1442.patch;https://issues.apache.org/jira/secure/attachment/12644033/KAFKA-1442.patch","08/May/14 23:01;nehanarkhede;KAFKA-1442_2014-05-08_16:01:09.patch;https://issues.apache.org/jira/secure/attachment/12644041/KAFKA-1442_2014-05-08_16%3A01%3A09.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,391520,,,Thu May 08 23:06:43 UTC 2014,,,,,,,,,,"0|i1vfif:",391733,,,,,,,,,,,,,,,,,,,,"08/May/14 22:36;sriharsha;Created reviewboard https://reviews.apache.org/r/21243/diff/
 against branch origin/trunk;;;","08/May/14 23:06;nehanarkhede;Thanks for the patch. Committed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Purgatory purge causes latency spikes,KAFKA-1441,12713191,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jkreps,jkreps,08/May/14 21:30,03/Oct/14 23:06,14/Jul/23 05:39,03/Oct/14 23:06,,,,,,,,,,,,,,0,,,,"The request purgatory has a funky thing where it periodically loops over all watches and purges them. If you have a fair number of partitions you can accumulate lots of watches and purging them can take a long time. During this time all expiry is halted.

Here is an example log:
[2014-05-08 21:07:41,950] INFO ExpiredRequestReaper-2 Expired request after 10ms: 5829 (kafka.server.RequestPurgatory$ExpiredRequestReaper)
[2014-05-08 21:07:41,952] INFO ExpiredRequestReaper-2 Expired request after 10ms: 5882 (kafka.server.RequestPurgatory$ExpiredRequestReaper)
[2014-05-08 21:07:41,967] INFO ExpiredRequestReaper-2 Expired request after 11ms: 5884 (kafka.server.RequestPurgatory$ExpiredRequestReaper)
[2014-05-08 21:07:41,968] INFO ExpiredRequestReaper-2 Purging purgatory (kafka.server.RequestPurgatory$ExpiredRequestReaper)
[2014-05-08 21:07:41,969] INFO ExpiredRequestReaper-2 Purged 0 requests from delay queue. (kafka.server.RequestPurgatory$ExpiredRequestReaper)
[2014-05-08 21:07:42,305] INFO ExpiredRequestReaper-2 Purged 340809 (watcher) requests. (kafka.server.RequestPurgatory$ExpiredRequestReaper)
[2014-05-08 21:07:42,305] INFO ExpiredRequestReaper-2 Expired request after 106ms: 5847 (kafka.server.RequestPurgatory$ExpiredRequestReaper)
[2014-05-08 21:07:42,305] INFO ExpiredRequestReaper-2 Expired request after 106ms: 5904 (kafka.server.RequestPurgatory$ExpiredRequestReaper)
[2014-05-08 21:07:42,328] INFO ExpiredRequestReaper-2 Expired request after 10ms: 5908 (kafka.server.RequestPurgatory$ExpiredRequestReaper)
[2014-05-08 21:07:42,329] INFO ExpiredRequestReaper-2 Expired request after 10ms: 5852 (kafka.server.RequestPurgatory$ExpiredRequestReaper)
[2014-05-08 21:07:42,343] INFO ExpiredRequestReaper-2 Expired request after 11ms: 5854 (kafka.server.RequestPurgatory$ExpiredRequestReaper)

Combined with our buggy purgatory request impls that can sometimes hit their expiration this can lead to huge latency spikes.",,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1430,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,391507,,,2014-05-08 21:30:59.0,,,,,,,,,,"0|i1vffj:",391720,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate kafka client tools,KAFKA-1438,12712614,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriharsha,guozhang,guozhang,06/May/14 16:18,14/Sep/14 17:47,14/Jul/23 05:39,05/Jun/14 05:21,,,,0.8.2.0,,,,,,,,,,0,newbie,tools,usability,"Currently the console/perf client tools scatter across different packages, we'd better to:

1. Move Consumer/ProducerPerformance and SimpleConsumerPerformance to tools and remove the perf sub-project.
2. Move ConsoleConsumer from kafka.consumer to kafka.tools.
3. Move other consumer related tools from kafka.consumer to kafka.tools.",,guozhang,junrao,nehanarkhede,sriharsha,sslavic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/14 14:10;sslavic;KAFKA-1438-windows_bat.patch;https://issues.apache.org/jira/secure/attachment/12663657/KAFKA-1438-windows_bat.patch","07/Jun/14 23:15;sriharsha;KAFKA-1438.patch;https://issues.apache.org/jira/secure/attachment/12648838/KAFKA-1438.patch","23/May/14 15:52;sriharsha;KAFKA-1438.patch;https://issues.apache.org/jira/secure/attachment/12646534/KAFKA-1438.patch","27/May/14 18:45;sriharsha;KAFKA-1438_2014-05-27_11:45:29.patch;https://issues.apache.org/jira/secure/attachment/12646961/KAFKA-1438_2014-05-27_11%3A45%3A29.patch","27/May/14 19:16;sriharsha;KAFKA-1438_2014-05-27_12:16:00.patch;https://issues.apache.org/jira/secure/attachment/12646967/KAFKA-1438_2014-05-27_12%3A16%3A00.patch","28/May/14 00:09;sriharsha;KAFKA-1438_2014-05-27_17:08:59.patch;https://issues.apache.org/jira/secure/attachment/12647015/KAFKA-1438_2014-05-27_17%3A08%3A59.patch","28/May/14 15:32;sriharsha;KAFKA-1438_2014-05-28_08:32:46.patch;https://issues.apache.org/jira/secure/attachment/12647147/KAFKA-1438_2014-05-28_08%3A32%3A46.patch","28/May/14 15:36;sriharsha;KAFKA-1438_2014-05-28_08:36:28.patch;https://issues.apache.org/jira/secure/attachment/12647148/KAFKA-1438_2014-05-28_08%3A36%3A28.patch","28/May/14 15:40;sriharsha;KAFKA-1438_2014-05-28_08:40:22.patch;https://issues.apache.org/jira/secure/attachment/12647149/KAFKA-1438_2014-05-28_08%3A40%3A22.patch","30/May/14 18:36;sriharsha;KAFKA-1438_2014-05-30_11:36:01.patch;https://issues.apache.org/jira/secure/attachment/12647661/KAFKA-1438_2014-05-30_11%3A36%3A01.patch","30/May/14 18:38;sriharsha;KAFKA-1438_2014-05-30_11:38:46.patch;https://issues.apache.org/jira/secure/attachment/12647662/KAFKA-1438_2014-05-30_11%3A38%3A46.patch","30/May/14 18:42;sriharsha;KAFKA-1438_2014-05-30_11:42:32.patch;https://issues.apache.org/jira/secure/attachment/12647664/KAFKA-1438_2014-05-30_11%3A42%3A32.patch",,,,,,,,,,,,,,,,12.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,390930,,,Sun Sep 14 17:47:47 UTC 2014,,,,,,,,,,"0|i1vc0n:",391165,,,,,,,,,,,,,,,,,,,,"23/May/14 15:52;sriharsha;Created reviewboard https://reviews.apache.org/r/21865/diff/
 against branch origin/trunk;;;","27/May/14 18:45;sriharsha;Updated reviewboard https://reviews.apache.org/r/21865/diff/
 against branch origin/trunk;;;","27/May/14 19:16;sriharsha;Updated reviewboard https://reviews.apache.org/r/21865/diff/
 against branch origin/trunk;;;","28/May/14 00:09;sriharsha;Updated reviewboard https://reviews.apache.org/r/21865/diff/
 against branch origin/trunk;;;","28/May/14 15:32;sriharsha;Updated reviewboard https://reviews.apache.org/r/21977/diff/
 against branch origin/trunk;;;","28/May/14 15:36;sriharsha;Updated reviewboard https://reviews.apache.org/r/21977/diff/
 against branch origin/trunk;;;","28/May/14 15:40;sriharsha;Updated reviewboard https://reviews.apache.org/r/21865/diff/
 against branch origin/trunk;;;","28/May/14 18:50;nehanarkhede;[~sriharsha] I think you need to do the following for all tools under perf/

git mv tool-file core/src/main/scala/kafka/tools
git rm -r perf

Once you've done that, I recommend you try to run system_test/run_sanity.sh. This will verify if the system tests still function after removing the perf subproject.;;;","30/May/14 18:36;sriharsha;Updated reviewboard https://reviews.apache.org/r/21865/diff/
 against branch origin/trunk;;;","30/May/14 18:38;sriharsha;Updated reviewboard https://reviews.apache.org/r/21865/diff/
 against branch origin/trunk;;;","30/May/14 18:42;sriharsha;Updated reviewboard https://reviews.apache.org/r/21865/diff/
 against branch origin/trunk;;;","30/May/14 20:55;sriharsha;[~nehanarkhede]   Sorry about the spam with reviewboard uploads. I tried what you suggested but generating an upload to reviewboard and applying that patch it doesn't delete the perf dir. I just uploaded git diff origin/trunk (which is what kafka-patch-review.py does) and tested from the reviewboard it works in applying the patch and deletes perf dir.
I ran run_sanity.sh it shows 1 test failed which is related KAFKA-924.
Thanks,
Harsha;;;","05/Jun/14 04:37;nehanarkhede;[~sriharsha] So could you point me to the patch that I can review? ;;;","05/Jun/14 05:06;nehanarkhede;I just reviewed the last reviewboard. Looks good to me. I think we can check it in. The only change I had to do on top of your changes is to remove the perf subproject directory (git rm -r perf);;;","05/Jun/14 05:21;nehanarkhede;Thanks for the patches. Pushed to trunk;;;","06/Jun/14 22:29;junrao;Thanks for the patch. It seems that after the patch, the message count reported for the producer is always 0 in system tests.

_test_case_name  :  testcase_0001
_test_class_name  :  ReplicaBasicTest
arg : bounce_broker  :  false
arg : broker_type  :  leader
arg : message_producing_free_time_sec  :  15
arg : num_iteration  :  1
arg : num_messages_to_produce_per_producer_call  :  50
arg : num_partition  :  1
arg : replica_factor  :  3
arg : sleep_seconds_between_producer_calls  :  1
validation_status  : 
     No. of messages from consumer on [test_1] at simple_consumer_test_1-0_r1.log  :  15500
     No. of messages from consumer on [test_1] at simple_consumer_test_1-0_r2.log  :  15500
     No. of messages from consumer on [test_1] at simple_consumer_test_1-0_r3.log  :  15500
     Unique messages from consumer on [test_1]  :  15500
     Unique messages from producer on [test_1]  :  0
     Validate for data matched on topic [test_1] across replicas  :  PASSED
     Validate for merged log segment checksum in cluster [source]  :  PASSED

;;;","07/Jun/14 05:05;nehanarkhede;[~sriharsha] To rephrase Jun's finding, the way the system test relies on determining # of unique messages is by turning on DEBUG in ProducerPerformance and the parsing the log for the unique IDs that it outputs. Maybe there is a minor change required to the default log4j.properties that got moved during the patch?;;;","07/Jun/14 05:17;sriharsha;[~junrao] [~nehanarkhede] I'll take a look and update the patch. I might have missed it in changing system test. Thanks.;;;","07/Jun/14 23:15;sriharsha;Created reviewboard https://reviews.apache.org/r/22344/diff/
 against branch origin/trunk;;;","07/Jun/14 23:17;sriharsha;[~junrao] [~nehanarkhede] 
I needed to update the tools-log4j.properties I missed it in the earlier patch. 
I ran the system tests 
_test_case_name  :  testcase_0001
_test_class_name  :  ReplicaBasicTest
arg : bounce_broker  :  false
arg : broker_type  :  leader
arg : message_producing_free_time_sec  :  15
arg : num_iteration  :  1
arg : num_messages_to_produce_per_producer_call  :  50
arg : num_partition  :  1
arg : replica_factor  :  3
arg : sleep_seconds_between_producer_calls  :  1
validation_status  :
     No. of messages from consumer on [test_1] at simple_consumer_test_1-0_r1.log  :  25000
     No. of messages from consumer on [test_1] at simple_consumer_test_1-0_r2.log  :  25000
     No. of messages from consumer on [test_1] at simple_consumer_test_1-0_r3.log  :  25000
     Unique messages from consumer on [test_1]  :  25000
     Unique messages from producer on [test_1]  :  25000
     Validate for data matched on topic [test_1]  :  PASSED
     Validate for data matched on topic [test_1] across replicas  :  PASSED
     Validate for merged log segment checksum in cluster [source]  :  PASSED
     Validate index log in cluster [source]  :  PASSED;;;","09/Jun/14 15:19;junrao;Thanks for the followup patch. Committed to trunk.;;;","22/Aug/14 14:06;sslavic;Windows batch scripts, {{kafka-console-consumer.bat}} and {{kafka-console-producer.bat}}, haven't been updated with new {{ConsoleConsumer}} and {{ConsoleProducer}} path/package.;;;","22/Aug/14 14:10;sslavic;Attached [^KAFKA-1438-windows_bat.patch] which fixes windows batch files issue. The patch assumes that another patch, in KAFKA-1419, for similar windows batch files issue has been applied first - otherwise this change cannot be tested.;;;","26/Aug/14 07:32;sslavic;Can some of the committers please reopen this ticket?;;;","14/Sep/14 17:47;junrao;The followup patch has been committed as part of kafka-1419. Thanks for the help.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsumerMetadataResponse should always include coordinator information,KAFKA-1437,12712611,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jjkoshy,jjkoshy,jjkoshy,06/May/14 16:02,19/May/14 17:51,14/Jul/23 05:39,19/May/14 17:51,,,,0.8.2.0,,,,,,,,,,0,,,,"The consumer metadata response currently includes coordinator broker information only on NoError. We need to avoid data-driven protocol specifications - the error code field being the data in this case. This won't work with the protocol schema utilities in the client package which does not (and should not) look at the data.

Also, need to update the wire protocol wiki page after the above is fixed.",,jjkoshy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/May/14 17:57;jjkoshy;KAFKA-1437.patch;https://issues.apache.org/jira/secure/attachment/12643599/KAFKA-1437.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,390927,,,Mon May 19 17:51:29 UTC 2014,,,,,,,,,,"0|i1vbzz:",391162,,,,,,,,,,,,,,,,,,,,"06/May/14 17:57;jjkoshy;Created reviewboard https://reviews.apache.org/r/21123/
 against branch origin/trunk;;;","06/May/14 18:56;jjkoshy;Can someone review? (Not sure why the patch update did not send an email to the list.);;;","15/May/14 21:56;jjkoshy;Thanks for the reviews - checked in, but will leave it open until I update the protocol wiki (wiki is currently down);;;","19/May/14 17:51;jjkoshy;Updated the wiki.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka.admin.TopicCommand missing --delete topic command,KAFKA-1434,12711720,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,mcz,mcz,01/May/14 15:13,20/Jan/15 09:35,14/Jul/23 05:39,20/Jan/15 09:35,0.8.1.1,,,0.8.2.0,,,,,,,tools,,,0,,,,"It is not possible to delete topic as --delete command is not available:
~/kafka_2.10-0.8.1.1/bin# ./kafka-run-class.sh kafka.admin.TopicCommand
Command must include exactly one action: --list, --describe, --create or --alter
Option                                  Description
------                                  -----------
--alter                                 Alter the configuration for the topic.
--config <name=value>                   A topic configuration override for the
                                          topic being created or altered.
--create                                Create a new topic.
--deleteConfig <name>                   A topic configuration override to be
                                          removed for an existing topic
--describe                              List details for the given topics.
--help                                  Print usage information.
--list                                  List all available topics.
--partitions <Integer: # of partitions> The number of partitions for the topic
                                          being created or altered (WARNING:
                                          If partitions are increased for a
                                          topic that has a key, the partition
                                          logic or ordering of the messages
                                          will be affected
--replica-assignment                    A list of manual partition-to-broker
  <broker_id_for_part1_replica1 :         assignments for the topic being
  broker_id_for_part1_replica2 ,          created or altered.
  broker_id_for_part2_replica1 :
  broker_id_for_part2_replica2 , ...>
--replication-factor <Integer:          The replication factor for each
  replication factor>                     partition in the topic being created.
--topic <topic>                         The topic to be create, alter or
                                          describe. Can also accept a regular
                                          expression except for --create option
--topics-with-overrides                 if set when describing topics, only
                                          show topics that have overridden
                                          configs
--unavailable-partitions                if set when describing topics, only
                                          show partitions whose leader is not
                                          available
--under-replicated-partitions           if set when describing topics, only
                                          show under replicated partitions
--zookeeper <urls>                      REQUIRED: The connection string for
                                          the zookeeper connection in the form
                                          host:port. Multiple URLS can be
                                          given to allow fail-over.",,guozhang,mcz,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,390041,,,Tue Jan 20 09:35:59 UTC 2015,,,,,,,,,,"0|i1v6lz:",390279,,,,,,,,,,,,,,,,,,,,"01/May/14 15:39;guozhang;We disabled the delete-topic functionality in 0.8.1.1 as it has some regression that we are fixing now. It is tracked in KAFKA-1397.;;;","20/Jan/15 09:35;omkreddy;This issue is fixed in 0.8.2. Pl enable ""delete.topic.enable""  broker config property  to enable topic deletion feature.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient unit test failure in ZookeeperConsumerConnectorTest,KAFKA-1433,12711641,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,01/May/14 00:13,01/May/14 15:54,14/Jul/23 05:39,01/May/14 15:54,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"Saw the following transient unit test failure.

kafka.consumer.ZookeeperConsumerConnectorTest > testBasic FAILED
    kafka.common.FailedToSendMessageException: Failed to send messages after 3 tries.
        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:90)
        at kafka.producer.Producer.send(Producer.scala:76)
        at kafka.consumer.ZookeeperConsumerConnectorTest.sendMessagesToBrokerPartition(ZookeeperConsumerConnectorTest.scala:353)
        at kafka.consumer.ZookeeperConsumerConnectorTest.testBasic(ZookeeperConsumerConnectorTest.scala:92)
",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/14 00:17;junrao;KAFKA-1433.patch;https://issues.apache.org/jira/secure/attachment/12642774/KAFKA-1433.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,389962,,,Thu May 01 15:54:26 UTC 2014,,,,,,,,,,"0|i1v653:",390203,,,,,,,,,,,,,,,,,,,,"01/May/14 00:17;junrao;Created reviewboard https://reviews.apache.org/r/20930/
 against branch origin/trunk;;;","01/May/14 15:54;junrao;Thanks for the review. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make num.producerThreads configurable on new MirrrorMaker,KAFKA-1432,12711634,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,30/Apr/14 23:44,20/May/14 06:23,14/Jul/23 05:39,05/May/14 23:57,,,,0.8.2.0,,,,,,,,,,0,,,,"Originally we make the num.producerThreads coupled with num.consumerThreads since we thought with the new producer, a single or couple of instances are sufficient for throughput. However, with compression turned on, which is now executed in the caller thread we still need configurable number of producer threads.",,guozhang,joliver,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/14 20:47;guozhang;KAFKA-1432.patch;https://issues.apache.org/jira/secure/attachment/12643844/KAFKA-1432.patch","06/May/14 22:56;guozhang;KAFKA-1432.patch;https://issues.apache.org/jira/secure/attachment/12643653/KAFKA-1432.patch","06/May/14 22:54;guozhang;KAFKA-1432.patch;https://issues.apache.org/jira/secure/attachment/12643652/KAFKA-1432.patch","01/May/14 23:44;guozhang;KAFKA-1432.patch;https://issues.apache.org/jira/secure/attachment/12642957/KAFKA-1432.patch","05/May/14 17:31;guozhang;KAFKA-1432_2014-05-05_10:31:42.patch;https://issues.apache.org/jira/secure/attachment/12643394/KAFKA-1432_2014-05-05_10%3A31%3A42.patch","05/May/14 21:33;guozhang;KAFKA-1432_2014-05-05_14:33:01.patch;https://issues.apache.org/jira/secure/attachment/12643433/KAFKA-1432_2014-05-05_14%3A33%3A01.patch","05/May/14 22:24;guozhang;KAFKA-1432_2014-05-05_15:24:08.patch;https://issues.apache.org/jira/secure/attachment/12643442/KAFKA-1432_2014-05-05_15%3A24%3A08.patch","06/May/14 23:05;guozhang;KAFKA-1432_2014-05-06_16:05:28.patch;https://issues.apache.org/jira/secure/attachment/12643654/KAFKA-1432_2014-05-06_16%3A05%3A28.patch",,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,389955,,,Thu May 08 04:03:21 UTC 2014,,,,,,,,,,"0|i1v63j:",390196,,,,,,,,,,,,,,,,,,,,"01/May/14 23:44;guozhang;Created reviewboard https://reviews.apache.org/r/20997/
 against branch origin/trunk;;;","05/May/14 17:31;guozhang;Updated reviewboard https://reviews.apache.org/r/20997/
 against branch origin/trunk;;;","05/May/14 21:33;guozhang;Updated reviewboard https://reviews.apache.org/r/20997/
 against branch origin/trunk;;;","05/May/14 22:24;guozhang;Updated reviewboard https://reviews.apache.org/r/20997/
 against branch origin/trunk;;;","05/May/14 23:57;nehanarkhede;Thanks for the patches, Guozhang. Pushed to trunk;;;","06/May/14 22:54;guozhang;Created reviewboard  against branch origin/trunk;;;","06/May/14 22:56;guozhang;Created reviewboard https://reviews.apache.org/r/21134/
 against branch origin/trunk;;;","06/May/14 23:05;guozhang;Updated reviewboard https://reviews.apache.org/r/21134/
 against branch origin/trunk;;;","07/May/14 17:08;nehanarkhede;Thanks for the followup patch, pushed to trunk;;;","07/May/14 20:47;guozhang;Created reviewboard https://reviews.apache.org/r/21173/
 against branch origin/trunk;;;","08/May/14 04:03;junrao;Thanks for the followup patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsoleConsumer - Option to clean zk consumer path,KAFKA-1431,12711417,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,sriharsha,JezzaLaycock,JezzaLaycock,30/Apr/14 08:20,22/Jul/14 14:40,14/Jul/23 05:39,15/May/14 23:02,0.8.1,,,0.8.2.0,,,,,,,consumer,,,0,newbie,,,"Raised in response to KAFKA-1426. Currently option ""from-beginning"" auto deletes the zk consumer path. This is confusing and un-expected behaviour. Suggest a separate option to clean the console consumer path.",All,JezzaLaycock,nehanarkhede,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1426,,,,,"08/May/14 21:58;sriharsha;KAFKA-1431.patch;https://issues.apache.org/jira/secure/attachment/12644020/KAFKA-1431.patch","14/May/14 18:34;sriharsha;KAFKA-1431_2014-05-14_11:34:00.patch;https://issues.apache.org/jira/secure/attachment/12644864/KAFKA-1431_2014-05-14_11%3A34%3A00.patch","14/May/14 19:09;sriharsha;KAFKA-1431_2014-05-14_12:08:47.patch;https://issues.apache.org/jira/secure/attachment/12644870/KAFKA-1431_2014-05-14_12%3A08%3A47.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,389738,,,Thu May 15 22:48:53 UTC 2014,,,,,,,,,,"0|i1v4rr:",389980,,,,,,,,,,,,,,,,,,,,"08/May/14 21:58;sriharsha;Created reviewboard https://reviews.apache.org/r/21239/diff/
 against branch origin/trunk;;;","14/May/14 18:34;sriharsha;Updated reviewboard https://reviews.apache.org/r/21239/diff/
 against branch origin/trunk;;;","14/May/14 19:09;sriharsha;Updated reviewboard https://reviews.apache.org/r/21239/diff/
 against branch origin/trunk;;;","15/May/14 22:48;nehanarkhede;Fixed the minor comment to convert error->System.err.println and checked in to trunk. Thanks for the patches!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FIS not closed after Properties.load,KAFKA-1428,12710972,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jonbringhurst,jonbringhurst,jonbringhurst,28/Apr/14 17:16,29/Apr/14 04:19,14/Jul/23 05:39,28/Apr/14 23:49,,,,,,,,,,,,,,0,,,,A FileInputStream is not being closed after using it for a Properties.load.,,jkreps,jonbringhurst,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/14 17:17;jonbringhurst;KAFKA-1428.patch;https://issues.apache.org/jira/secure/attachment/12642278/KAFKA-1428.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,389293,,,Mon Apr 28 23:49:42 UTC 2014,,,,,,,,,,"0|i1v21r:",389539,,,,,,,,,,,,,,,,,,,,"28/Apr/14 23:49;jkreps;Applied. Nice catch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient unit test failure in testSendWithDeadBroker,KAFKA-1424,12710562,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,25/Apr/14 14:51,28/Apr/14 22:00,14/Jul/23 05:39,28/Apr/14 22:00,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"Saw the following transient unit test failure.

kafka.producer.ProducerTest > testSendWithDeadBroker FAILED
    java.lang.AssertionError: Message set should have 1 message
        at org.junit.Assert.fail(Assert.java:69)
        at org.junit.Assert.assertTrue(Assert.java:32)
        at kafka.producer.ProducerTest.testSendWithDeadBroker(ProducerTest.scala:245)
",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/14 17:16;junrao;KAFKA-1424.patch;https://issues.apache.org/jira/secure/attachment/12642277/KAFKA-1424.patch","25/Apr/14 14:54;junrao;KAFKA-1424.patch;https://issues.apache.org/jira/secure/attachment/12641929/KAFKA-1424.patch","28/Apr/14 17:24;junrao;KAFKA-1424_2014-04-28_10:24:23.patch;https://issues.apache.org/jira/secure/attachment/12642281/KAFKA-1424_2014-04-28_10%3A24%3A23.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,388883,,,Mon Apr 28 22:00:05 UTC 2014,,,,,,,,,,"0|i1uzj3:",389129,,,,,,,,,,,,,,,,,,,,"25/Apr/14 14:54;junrao;Created reviewboard https://reviews.apache.org/r/20713/
 against branch origin/trunk;;;","25/Apr/14 15:23;junrao;Thanks for the review. Committed to trunk.;;;","28/Apr/14 17:15;junrao;The fix is not complete. If there is only a single broker in the cluster, on broker restart, the controller sends the following metadata requests to the broker. The first few requests will have leader as -1. We need to wait until a valid leader is propagated in the metadata cache in the test. 

    [2014-04-25 14:06:26,712] TRACE Controller 0 epoch 2 sending UpdateMetadata request (Leader:-1,ISR:,LeaderEpoch:1,ControllerEpoch:1) with correlationId 3 to broker 0 for partition [test,0] (state.change.logger:36)
    [2014-04-25 14:06:26,714] TRACE Controller 0 epoch 2 sending become-follower LeaderAndIsr request (Leader:-1,ISR:,LeaderEpoch:1,ControllerEpoch:1) with correlationId 4 to broker 0 for partition [test,0] (state.change.logger:36)
    [2014-04-25 14:06:26,714] TRACE Controller 0 epoch 2 sending UpdateMetadata request (Leader:-1,ISR:,LeaderEpoch:1,ControllerEpoch:1) with correlationId 4 to broker 0 for partition [test,0] (state.change.logger:36)
    [2014-04-25 14:06:26,745] TRACE Controller 0 epoch 2 sending become-leader LeaderAndIsr request (Leader:0,ISR:0,LeaderEpoch:2,ControllerEpoch:2) with correlationId 5 to broker 0 for partition [test,0] (state.change.logger:36)
    [2014-04-25 14:06:26,745] TRACE Controller 0 epoch 2 sending UpdateMetadata request (Leader:0,ISR:0,LeaderEpoch:2,ControllerEpoch:2) with correlationId 5 to broker 0 for partition [test,0] (state.change.logger:36)
;;;","28/Apr/14 17:16;junrao;Created reviewboard https://reviews.apache.org/r/20783/
 against branch origin/trunk;;;","28/Apr/14 17:24;junrao;Updated reviewboard https://reviews.apache.org/r/20783/
 against branch origin/trunk;;;","28/Apr/14 22:00;junrao;Thanks for the reviews. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updating partition count clears topic configuration and any further alters done to a topics config do not stick,KAFKA-1423,12710488,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,nbrownus,nbrownus,25/Apr/14 03:52,07/Feb/15 22:47,14/Jul/23 05:39,07/Feb/15 22:47,0.8.1,,,0.8.2.0,,,,,,,,,,0,,,,"After creating a topic with custom retention configs, I tried to change the number of partitions. This succeeded but it cleared the config for the topic. I then tried to add the configs back in, which appeared to succeed, but the config is still reporting empty when using `kafka-topics.sh --describe`.
",,nbrownus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,388809,,,Fri Apr 25 04:04:17 UTC 2014,,,,,,,,,,"0|i1uz33:",389057,,,,,,,,,,,,,,,,,,,,"25/Apr/14 04:04;nbrownus;If I wait about a minute or so the config reports back as it should, better than I thought but still not ideal.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace AdminUtils.createOrUpdateTopicPartitionAssignmentPathInZK with TestUtils.createTopic in unit tests,KAFKA-1420,12710396,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,natty,guozhang,guozhang,24/Apr/14 18:50,11/Sep/18 10:46,14/Jul/23 05:39,19/Jul/18 17:43,,,,2.1.0,,,,,,,,,,0,newbie,,,"This is a follow-up JIRA from KAFKA-1389.

There are a bunch of places in the unit tests where we misuse AdminUtils.createOrUpdateTopicPartitionAssignmentPathInZK to create topics, where TestUtils.createTopic needs to be used instead.",,guozhang,gwenshap,jozi-k,natty,nehanarkhede,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1389,DRILL-6739,,,,,,,,,,,,"28/Jul/14 20:54;natty;KAFKA-1420.patch;https://issues.apache.org/jira/secure/attachment/12658242/KAFKA-1420.patch","30/Jul/14 18:18;natty;KAFKA-1420_2014-07-30_11:18:26.patch;https://issues.apache.org/jira/secure/attachment/12658694/KAFKA-1420_2014-07-30_11%3A18%3A26.patch","30/Jul/14 18:25;natty;KAFKA-1420_2014-07-30_11:24:55.patch;https://issues.apache.org/jira/secure/attachment/12658698/KAFKA-1420_2014-07-30_11%3A24%3A55.patch","02/Aug/14 18:04;natty;KAFKA-1420_2014-08-02_11:04:15.patch;https://issues.apache.org/jira/secure/attachment/12659489/KAFKA-1420_2014-08-02_11%3A04%3A15.patch","10/Aug/14 21:12;natty;KAFKA-1420_2014-08-10_14:12:05.patch;https://issues.apache.org/jira/secure/attachment/12660874/KAFKA-1420_2014-08-10_14%3A12%3A05.patch","11/Aug/14 06:04;natty;KAFKA-1420_2014-08-10_23:03:46.patch;https://issues.apache.org/jira/secure/attachment/12660930/KAFKA-1420_2014-08-10_23%3A03%3A46.patch",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,388718,,,Thu Jul 19 17:43:51 UTC 2018,,,,,,,,,,"0|i1uyjr:",388968,,junrao,,,,,,,,,,,,,,,,,,"21/Jul/14 22:03;natty;I was starting to work on this JIRA, but I'm hitting a small stumbling block. I've noticed some tests that create brokers using TestUtils.createBrokerConfigs() and mapping over the configs with TestUtils.createServer(), and other tests that create brokers using TestUtils.createBrokersInZk().

Where this becomes a little confusing is that both implementations of createTopic require a Seq[KafkaServer], but createServer() returns a KafkaServer via a Properties object and createBrokersInZk returns a Seq[Broker], and I don't see a particularly obvious way to go from a Broker to a KakfaServer.

Am I missing something obvious?;;;","21/Jul/14 22:57;guozhang;Hi Jonathan,

TestUtils.createBrokerConfigs() is usually used when the test class is inheriting from KafkaServerTestHarness, which already handles server creation and shutdown at setUp() and tearDown() time; TestUtils.createServer() should then be used otherwise, i.e. when a broker is just needed to be created on the fly.

TestUtils.createBrokersInZk() is different, though, in that it does not actually create a running server, but just create the registration znode in ZK, the Broker object is just a placeholder for the broker metadata, like broker id, address, etc. They should only be used when we just need to test some ZK-based utilities but do not necessarily need to really create a running server.;;;","22/Jul/14 00:55;natty;Thanks for the context, Guozhang. So in the cases where createBrokersInZk is used, should AdminUtils.createOrUpdateTopicPartitionAssignmentPathInZK still be used? In particular, I encountered this in AdminTest.testManualReplicaAssignment.;;;","22/Jul/14 02:13;guozhang;TestUtils.createTopic can be used here also.;;;","22/Jul/14 19:30;natty;Hi Guozhang,

The particular case I'm confused by is a test like this:

{code}
  @Test
  def testManualReplicaAssignment() {
    val brokers = List(0, 1, 2, 3, 4)
    TestUtils.createBrokersInZk(zkClient, brokers)

    // duplicate brokers
    intercept[IllegalArgumentException] {
      AdminUtils.createOrUpdateTopicPartitionAssignmentPathInZK(zkClient, ""test"", Map(0->Seq(0,0)))
    }

    // inconsistent replication factor
    intercept[IllegalArgumentException] {
      AdminUtils.createOrUpdateTopicPartitionAssignmentPathInZK(zkClient, ""test"", Map(0->Seq(0,1), 1->Seq(0)))
    }

    // good assignment
    val assignment = Map(0 -> List(0, 1, 2),
                         1 -> List(1, 2, 3))
    AdminUtils.createOrUpdateTopicPartitionAssignmentPathInZK(zkClient, ""test"", assignment)
    val found = ZkUtils.getPartitionAssignmentForTopics(zkClient, Seq(""test""))
    assertEquals(assignment, found(""test""))
  }
{code}

This test uses createOrUpdate, but creates brokers via createBrokersInZk, which doesn't return a reference to a KafkaServer.

The createTopic call, on the other hand, has two implementations:

{code}
def createTopic(zkClient: ZkClient, topic: String, numPartitions: Int = 1, replicationFactor: Int = 1,
                  servers: Seq[KafkaServer])

def createTopic(zkClient: ZkClient, topic: String, partitionReplicaAssignment: collection.Map[Int, Seq[Int]],
                  servers: Seq[KafkaServer])
{code}

In both cases, I need a Seq[KafkaServer]. It's unclear to me how to convert the aforementioned test to use createTopic, without materially changing the semantics of the test. Currently it doesn't use a real Kafka server, but to use createTopic would require me to get a hold of some real servers.

How would a conversion like this be done? Hopefully this clarifies why I'm confused.;;;","22/Jul/14 23:39;guozhang;Hello Jonathan,

Thanks for the comment. You can note that there are two createTopic functions, one in TestUtils and one in AdminUtils, and TestUtils.createTopic just calls AdminUtils.createTopics and then use the Seq[KafkaServer] to check if the topic metadata has been propagated to these servers.

For the AdminTest, where no Seq[KafkaServer] is used, we actually do not need the real running servers but just test some ZK-based admin utilities. In this case we can use AdminUtils.createTopic to replace the TestUtils.createOrUpdateTopicPartitionAssignmentPathInZK.;;;","28/Jul/14 20:52;natty;Patch available at https://reviews.apache.org/r/24006/;;;","28/Jul/14 20:58;guozhang;Thanks for the patch, will review it asap.;;;","30/Jul/14 18:18;natty;Updated reviewboard https://reviews.apache.org/r/24006/diff/
 against branch origin/trunk;;;","30/Jul/14 18:25;natty;Updated reviewboard https://reviews.apache.org/r/24006/diff/
 against branch origin/trunk;;;","02/Aug/14 18:04;natty;Updated reviewboard https://reviews.apache.org/r/24006/diff/
 against branch origin/trunk;;;","07/Aug/14 21:05;natty;Wanted to try to make sure this doesn't fall too far off the radar, since the review is mostly done.;;;","07/Aug/14 21:18;guozhang;Sorry Jonathan, was completely swamped the past week. Will get back to you asap.;;;","10/Aug/14 21:12;natty;Updated reviewboard https://reviews.apache.org/r/24006/diff/
 against branch origin/trunk;;;","11/Aug/14 06:04;natty;Updated reviewboard https://reviews.apache.org/r/24006/diff/
 against branch origin/trunk;;;","14/Aug/14 14:50;nehanarkhede;Assigning to myself for review;;;","15/Aug/14 00:36;nehanarkhede;Doesn't look like I'm going to be able to get to this in the next couple days. Reassigning to [~junrao] to see if he has some time to review this.;;;","28/Aug/14 18:08;natty;Wanted to bump this ticket so the patch doesn't get irreparably stale.;;;","04/Sep/14 21:44;guozhang;Can some committer take another look and commit this one? It looks good to me now.;;;","14/Sep/14 17:10;nehanarkhede;[~junrao], please feel free to reassign for review.;;;","08/Apr/15 01:31;gwenshap;This probably needs rebase.

[~natty] would you have time to rebase?
[~guozhang] will you be ok committing this once Natty rebases?;;;","10/Feb/17 14:50;jozi-k;I see the patch is available for this issue, would you mind if I open PR with it?;;;","19/Jul/18 17:43;omkreddy;Fixed as part of https://github.com/apache/kafka/pull/5303;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient unit test failure in ProducerFailureHandlingTest,KAFKA-1418,12710355,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,24/Apr/14 16:29,28/Apr/14 19:55,14/Jul/23 05:39,24/Apr/14 22:24,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"Saw the following transient failure.

kafka.api.ProducerFailureHandlingTest > testWrongBrokerList PASSED

kafka.api.ProducerFailureHandlingTest > testNoResponse FAILED
    kafka.common.KafkaException: Socket server failed to bind to localhost:49013: Address already in use.
        at kafka.network.Acceptor.openServerSocket(SocketServer.scala:195)
        at kafka.network.Acceptor.<init>(SocketServer.scala:141)
        at kafka.network.SocketServer.startup(SocketServer.scala:68)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:84)
        at kafka.utils.TestUtils$.createServer(TestUtils.scala:120)
        at kafka.api.ProducerFailureHandlingTest.setUp(ProducerFailureHandlingTest.scala:80)

        Caused by:
        java.net.BindException: Address already in use
            at sun.nio.ch.Net.bind(Native Method)
            at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:124)
            at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)
            at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:52)
            at kafka.network.Acceptor.openServerSocket(SocketServer.scala:191)
            ... 5 more
",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/14 16:34;junrao;KAFKA-1418.patch;https://issues.apache.org/jira/secure/attachment/12641748/KAFKA-1418.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,388677,,,Thu Apr 24 22:24:11 UTC 2014,,,,,,,,,,"0|i1uyav:",388928,,,,,,,,,,,,,,,,,,,,"24/Apr/14 16:33;junrao;We actually do close the broker in each of the test. The issue may be related to the following article. Basically, the normal socket close procedure should be client.close first, followed by server close. Otherwise, the port may not be reusable after the process is gone since it has to wait for the timeout. So, in unit tests, we should try to close all clients before shutting down the broker.

http://hea-www.harvard.edu/~fine/Tech/addrinuse.html;;;","24/Apr/14 16:34;junrao;Created reviewboard https://reviews.apache.org/r/20669/
 against branch origin/trunk;;;","24/Apr/14 22:24;junrao;Thanks for the review. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Very slow initial high-level consumer startup in low traffic/blocking fetch scenario,KAFKA-1417,12710208,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,smeder,smeder,24/Apr/14 02:44,26/Apr/14 01:21,14/Jul/23 05:39,26/Apr/14 01:21,0.8.0,,,0.8.1,,,,,,,consumer,,,0,,,,"We're seeing very slow startup times when starting a high level consumer in a low traffic/blocking fetch type setup. The example we've come across has a consumer that is set up to use 3 topics and uses a 20s/1 byte fetch timeout. What happens is that the leader finder thread adds partitions one by one and since the offset is not know this causes a call to figure out the offset. This call uses the fetcher threads simple consumer instance and locks around the call. Initially this is not a problem, but as soon as the fetcher thread has some partitions it will start fetching and since this is a low traffic situation the fetch will at least sometimes take up to 20s (again locking around the simple consumer). This leads to behavior like:

# Finder thread adds a partition
# Data thread notices it has partitions to fetch data for, locks the consumer for 20s
# Finder thread tries to add a partition, tries to lock consumer and blocks for 20s
# Rinse, repeat for each partition",,guozhang,junrao,smeder,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,388530,,,Sat Apr 26 01:21:15 UTC 2014,,,,,,,,,,"0|i1uxe7:",388781,,,,,,,,,,,,,,,,,,,,"24/Apr/14 02:45;smeder;The simple, although not the most efficient solution would be to add another simple consumer instance in each fetcher...;;;","24/Apr/14 04:20;junrao;Interesting. The problem is that the leaderFinderThread uses the same SimpleConsumer (used by the fetcher thread) when issuing the OffsetBefore request. We could somehow let them use different SimpleConsumer instances. Not sure if this is the best solution though. 

Also, is there a particular reason that you use a 20s maxwait in the fetch request?;;;","24/Apr/14 04:26;smeder;I think the timeout is somewhat arbitrary, but since we react to any data (1 byte requirement) we don't want to be be doing a whole bunch of unnecessary fetches if there is not data. I'm going to implement the simple second consumer approach and attach a patch.;;;","25/Apr/14 01:12;guozhang;In 0.8.1, the leader finder thread would not add partition one-by-one but in batches. Would this help your case?;;;","25/Apr/14 02:55;smeder;It should, let me take a look at the 0.8.1 code.;;;","26/Apr/14 01:21;smeder;Looks fine in 0.8.1 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unify sendMessages/getMessages in unit tests,KAFKA-1416,12710165,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,futtre,guozhang,guozhang,23/Apr/14 22:04,14/Apr/15 21:44,14/Jul/23 05:39,14/Apr/15 21:44,,,,,,,,,,,,,,0,newbie,,,"Multiple unit tests have its own internal function to send/get messages from the brokers. For example:

sendMessages in ZookeeperConsumerConnectorTest
produceMessage in UncleanLeaderElectionTest
sendMessages in FetcherTest

etc

It is better to unify them in TestUtils.",,alanlee,futtre,guozhang,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/15 00:32;futtre;KAFKA-1416.patch;https://issues.apache.org/jira/secure/attachment/12701750/KAFKA-1416.patch","02/Mar/15 01:25;futtre;KAFKA-1416_2015-03-01_17:24:55.patch;https://issues.apache.org/jira/secure/attachment/12701758/KAFKA-1416_2015-03-01_17%3A24%3A55.patch","26/Mar/15 07:21;futtre;KAFKA-1416_2015-03-26_00:20:36.patch;https://issues.apache.org/jira/secure/attachment/12707451/KAFKA-1416_2015-03-26_00%3A20%3A36.patch","11/Apr/15 01:36;futtre;KAFKA-1416_2015-04-10_18:36:10.patch;https://issues.apache.org/jira/secure/attachment/12724722/KAFKA-1416_2015-04-10_18%3A36%3A10.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,388487,,,Tue Apr 14 21:43:57 UTC 2015,,,,,,,,,,"0|i1ux53:",388740,,guozhang,,,,,,,,,,,,,,,,,,"19/Feb/15 22:08;futtre;May I (or how do I) assign this task to myself?

I'd like to take a stab at it.;;;","22/Feb/15 23:56;guozhang;[~futtre] I have assigned the ticket yo you, thanks.;;;","02/Mar/15 00:32;futtre;Created reviewboard https://reviews.apache.org/r/31606/diff/
 against branch origin/trunk;;;","02/Mar/15 00:59;futtre;Thank you. I just committed a patch.
;;;","02/Mar/15 01:25;futtre;Updated reviewboard https://reviews.apache.org/r/31606/diff/
 against branch origin/trunk;;;","08/Mar/15 21:13;nehanarkhede;[~guozhang] Since you were helping out...;;;","26/Mar/15 07:21;futtre;Updated reviewboard https://reviews.apache.org/r/31606/diff/
 against branch origin/trunk;;;","11/Apr/15 01:36;futtre;Updated reviewboard https://reviews.apache.org/r/31606/diff/
 against branch origin/trunk;;;","14/Apr/15 21:43;guozhang;Thanks for the patch. +1 and pushed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient unit test failure in ProducerSendTest.testAutoCreateTopic,KAFKA-1412,12709808,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,22/Apr/14 16:47,01/May/14 23:36,14/Jul/23 05:39,01/May/14 23:36,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"Saw the following transient failure. 

kafka.api.test.ProducerSendTest > testAutoCreateTopic FAILED
    java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
        at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.valueOrError(FutureRecordMetadata.java:56)
        at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:43)
        at org.apache.kafka.clients.producer.internals.FutureRecordMetadata.get(FutureRecordMetadata.java:25)
        at kafka.api.test.ProducerSendTest.testAutoCreateTopic(ProducerSendTest.scala:254)

        Caused by:
        org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
",,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/14 16:53;junrao;KAFKA-1412.patch;https://issues.apache.org/jira/secure/attachment/12642868/KAFKA-1412.patch","01/May/14 16:52;junrao;KAFKA-1412.patch;https://issues.apache.org/jira/secure/attachment/12642867/KAFKA-1412.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,388130,,,Thu May 01 23:36:08 UTC 2014,,,,,,,,,,"0|i1uuzb:",388388,,,,,,,,,,,,,,,,,,,,"22/Apr/14 16:49;junrao;The issue seems to be there is no resend in the test. We should configure enough retries (default to 0) and retry backoff time to allow metadata to be propagated to the broker after auto topic creation.;;;","22/Apr/14 17:26;guozhang;I think this is a duplicate of KAFKA-1395. My proposal is to unify createProducer to ensure acks = -1 and type = sync, in which after the send call if no exception is thrown for failed message sending we are assured that the topic is created, hence no need to check waitUntilMetadataIsPropagated either.;;;","01/May/14 16:52;junrao;Created reviewboard  against branch origin/trunk;;;","01/May/14 16:53;junrao;Created reviewboard https://reviews.apache.org/r/20954/
 against branch origin/trunk;;;","01/May/14 23:36;junrao;Thanks for the reviews. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetadataCache cleanup,KAFKA-1410,12709641,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,21/Apr/14 23:05,23/Apr/14 00:23,14/Jul/23 05:39,23/Apr/14 00:23,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"There are a few remaining cleanup items from KAFKA-1356 for trunk.

1. There is no need to call ensureTopicExists(). A consistent check would require holding the read lock of metadata cache on every fetch/producer request, which means a metadata update could block all fetch/producer requests. Also, this call is just an optimization. When leaderAndIsrRequest is propagated after a topic deletion, fetch/producer requests will be rejected by the ReplicaManager.

2. The update of the controller epoc when handling metadata request needs to be done inside the replicaStateChangeLock.",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/14 23:16;junrao;KAFKA-1410.patch;https://issues.apache.org/jira/secure/attachment/12641139/KAFKA-1410.patch","22/Apr/14 16:37;junrao;KAFKA-1410_2014-04-22_09:37:36.patch;https://issues.apache.org/jira/secure/attachment/12641271/KAFKA-1410_2014-04-22_09%3A37%3A36.patch","22/Apr/14 20:51;junrao;KAFKA-1410_2014-04-22_13:51:26.patch;https://issues.apache.org/jira/secure/attachment/12641324/KAFKA-1410_2014-04-22_13%3A51%3A26.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,387963,,,Wed Apr 23 00:23:03 UTC 2014,,,,,,,,,,"0|i1utyv:",388222,,,,,,,,,,,,,,,,,,,,"21/Apr/14 23:16;junrao;Created reviewboard https://reviews.apache.org/r/20540/
 against branch origin/trunk;;;","22/Apr/14 16:37;junrao;Updated reviewboard https://reviews.apache.org/r/20540/
 against branch origin/trunk;;;","22/Apr/14 20:51;junrao;Updated reviewboard https://reviews.apache.org/r/20540/
 against branch origin/trunk;;;","23/Apr/14 00:23;junrao;Thanks for the reviews. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
oversized messages can slow down the brokers,KAFKA-1409,12709576,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,guozhang,junrao,junrao,21/Apr/14 16:50,25/Apr/14 18:49,14/Jul/23 05:39,25/Apr/14 18:49,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"There are 2 main issues.

1. The broker first decompresses and then recompresses each message (to assign new offsets) before validating the message size (we have to do the validation after recompression since the message size could change). So, it can spend many secs to decompress/recompress an oversized message, only to be dropped later. While this was happening, a request thread was tied up, which reduced the capacity on the broker.

2. Both the fetch and producer requests need to hold a leader lock (per partition). So, if the producer is slow in appending the log, it will block other producer/fetch requests on the same partition.
",,guozhang,jthakrar,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/14 17:29;guozhang;KAFKA-1409.patch;https://issues.apache.org/jira/secure/attachment/12641514/KAFKA-1409.patch","23/Apr/14 21:10;guozhang;KAFKA-1409_2014-04-23_14:10:03.patch;https://issues.apache.org/jira/secure/attachment/12641576/KAFKA-1409_2014-04-23_14%3A10%3A03.patch","23/Apr/14 21:26;guozhang;KAFKA-1409_2014-04-23_14:25:48.patch;https://issues.apache.org/jira/secure/attachment/12641580/KAFKA-1409_2014-04-23_14%3A25%3A48.patch","23/Apr/14 22:20;guozhang;KAFKA-1409_2014-04-23_15:20:43.patch;https://issues.apache.org/jira/secure/attachment/12641598/KAFKA-1409_2014-04-23_15%3A20%3A43.patch","24/Apr/14 23:26;guozhang;KAFKA-1409_2014-04-24_16:26:06.patch;https://issues.apache.org/jira/secure/attachment/12641827/KAFKA-1409_2014-04-24_16%3A26%3A06.patch","25/Apr/14 17:23;guozhang;KAFKA-1409_2014-04-25_10:22:54.patch;https://issues.apache.org/jira/secure/attachment/12641953/KAFKA-1409_2014-04-25_10%3A22%3A54.patch","25/Apr/14 17:44;guozhang;KAFKA-1409_2014-04-25_10:44:27.patch;https://issues.apache.org/jira/secure/attachment/12641965/KAFKA-1409_2014-04-25_10%3A44%3A27.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,387898,,,Fri Apr 25 18:49:24 UTC 2014,,,,,,,,,,"0|i1utkv:",388159,,,,,,,,,,,,,,,,,,,,"21/Apr/14 17:00;junrao;The fix would be:
1. add an extra verification of the message size before decompression;
2. change the leader lock to be a read/write lock (only the leader updater will hold the write lock) so that slow producer requests don't block the fetch requests.
3. the byte-in rate includes over-sized messages, which is not consistent with message-in rate.;;;","23/Apr/14 17:29;guozhang;Created reviewboard https://reviews.apache.org/r/20616/
 against branch origin/trunk;;;","23/Apr/14 21:11;guozhang;Updated reviewboard https://reviews.apache.org/r/20616/
 against branch origin/trunk;;;","23/Apr/14 21:26;guozhang;Updated reviewboard https://reviews.apache.org/r/20616/
 against branch origin/trunk;;;","23/Apr/14 22:20;guozhang;Updated reviewboard https://reviews.apache.org/r/20616/
 against branch origin/trunk;;;","24/Apr/14 23:26;guozhang;Updated reviewboard https://reviews.apache.org/r/20616/
 against branch origin/trunk;;;","25/Apr/14 17:23;guozhang;Updated reviewboard https://reviews.apache.org/r/20616/
 against branch origin/trunk;;;","25/Apr/14 17:44;guozhang;Updated reviewboard https://reviews.apache.org/r/20616/
 against branch origin/trunk;;;","25/Apr/14 18:49;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix scaladoc/javadoc warnings,KAFKA-1406,12709337,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,alanlee,jjkoshy,jjkoshy,18/Apr/14 20:28,11/Jul/14 15:10,14/Jul/23 05:39,10/Jul/14 16:46,,,,0.8.2.0,,,,,,,packaging,,,1,build,,,"./gradlew docsJarAll

You will see a bunch of warnings mainly due to typos/incorrect use of javadoc/scaladoc",,alanlee,jjkoshy,junrao,sslavic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/14 15:27;alanlee;kafka-1406-v1.patch;https://issues.apache.org/jira/secure/attachment/12654817/kafka-1406-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,387659,,,Fri Jul 11 15:10:45 UTC 2014,,,,,,,,,,"0|i1us3b:",387921,,,,,,,,,,,,,,,,,,,,"09/Jul/14 12:00;alanlee;Would it be okay if I work on this?;;;","09/Jul/14 14:44;junrao;Alan,

That would be great. Thanks for your help.;;;","09/Jul/14 15:27;alanlee;I've attached a patch file for the fix.
There are still few (non variable type-argument ...) warnings during scaladoc build but it does not seem to be related to documentation.;;;","10/Jul/14 16:46;junrao;Thanks for the patch. +1 and committed to trunk.;;;","11/Jul/14 15:10;alanlee;Great. Thanks for handling that quickly. Now I'll go and find something bit more challenging to work on.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Global JSON.globalNumberParser screws up other libraries,KAFKA-1405,12709180,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,vchekan,vchekan,18/Apr/14 00:04,12/Apr/18 10:46,14/Jul/23 05:39,12/Apr/18 10:46,0.8.0,,,,,,,,,,,,,1,json,,,"I'm getting exception ""kafka.common.ConsumerRebalanceFailedException"" but it only happens when I do a call to ""scala/pickling"" serialization library. What the connection you might ask? The underly exception is 

""ZookeeperConsumerConnector:76, exception during rebalance 
kafka.common.KafkaException: Failed to parse the broker info from zookeeper: {""jmx_port"":-1,""timestamp"":""1397514497053"",""host"":""xxx"",""version"":1,""port"":9092}
Caused by: java.lang.ClassCastException: java.lang.Double cannot be cast to java.lang.Integer""

A little bit looking at  kafka code lead me to this line:
In https://github.com/apache/kafka/blob/0.8.0/core/src/main/scala/kafka/utils/Json.scala#L27

there is JSON.globalNumberParser redefined. It is terrible idea to change global variable. This JSON library is used by other libraries and this global assignment messes it up.

My 5-minutes research shows that scala's JSON library was considered almost of demo quality and most people prefer ligt-json implementation.
https://groups.google.com/forum/#!topic/scala-user/P7-8PEUUj6A
Also it is my understanding, that scala JSON is deprecated in scala-2.11, so this change is needed anyway.

If no objections to this ticket in general, I can work on a patch to use 3rd party JSON library usage in kafka. Pleas let me know...",,doctapp,junrao,omkreddy,vchekan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,387503,,,Thu Apr 12 10:46:26 UTC 2018,,,,,,,,,,"0|i1ur4n:",387765,,,,,,,,,,,,,,,,,,,,"18/Apr/14 17:17;junrao;Vadim,

Thanks for reporting this. Yes, overriding JSON.globalNumberParser is not ideal. Are you sure JSON is going to be deprecated in scala 2.11? The api is still there.

http://www.scala-lang.org/api/2.11.0-M3/index.html#scala.util.parsing.json.JSON$

If JSON is still supported in scala 2.11, maybe we can see if there is a way to fix JSON.globalNumberParser. Could we just override the global value each time we do parsing and reset it back to the original value when done?

As for dragging in lift-json, this is also possible, but we have to be a bit careful. So, we need to know that the library is well maintained and has a good history of providing backward compatible releases. Currently, the consumer client depends on json parsing. Any jar compatibility issue could affect the consumer applications.;;;","21/Apr/14 18:22;vchekan;Hi Jun,

Thanks for quick response. 
Seems JSON has been moved out from scala core:
https://github.com/scala/scala-parser-combinators
""As of Scala 2.11, this library is a separate jar that can be omitted from Scala projects that do not use Parser Combinators.""
""The first 2.11 milestone for which this is true is 2.11.0-M4""

So M3, which you tried, is the last release which has JSON in the core. But even if you decide to use ""scala-parser-combinators.jar"", JSON is deprecated there:
https://github.com/scala/scala-parser-combinators/blob/master/src/main/scala/scala/util/parsing/json/JSON.scala#L31

And here is related discussion:
https://groups.google.com/forum/#!topic/scala-internals/axzOQ6fUNQg/discussion
""The JSON classes feel a bit like they were meant as a code example for the parser combinator library. Compared to hand-tuned libraries, they seem to be too slow for any serious usage.""
;;;","12/Apr/18 10:46;omkreddy;Old Scala JSON  usage is removed in new versions of Kafka;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create unit test helper that stops and starts a cluster,KAFKA-1402,12709147,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jkreps,jkreps,17/Apr/14 20:50,31/Aug/17 17:42,14/Jul/23 05:39,31/Aug/17 17:42,,,,,,,,,,,,,,0,,,,"We have a zookeeper test harness. We used to have a kafka server test harness, but it looks like it has been deleted.

Each test that wants to manage a server starts and stops it itself. This is a little problematic as it may not do a good job of cleaning up and there are lots of details to get right.

We should have an EmbeddedKafkaCluster class like the EmbeddedZookeeper we have that starts N brokers and convert the existing full server classes to use this.",,alanlee,guozhang,jkreps,omkreddy,ScottReynolds,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1954,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,387470,,,Thu Aug 31 17:42:12 UTC 2017,,,,,,,,,,"0|i1uqxb:",387732,,,,,,,,,,,,,,,,,,,,"17/Apr/14 21:31;guozhang;I think we still have the kafka server test harness, but I agree that not all tests that need to deploy the server are actually using it. This is causing un-clean unit tests, maybe not the only cause though. One thing we can do is to enforce using server test harness, and then run it once and see if there are any other temporary kafka log dirs left.;;;","17/Apr/14 23:34;jkreps;Ah, yes, weirdly I couldn't find that. In that case I think all we need to do is generalize that so it can support an arbitrary number of servers and then actually convert the existing tests to use it.;;;","31/Aug/17 17:42;omkreddy;We have KafkaServerTestHarness, EmbeddedKafkaCluster, EmbeddedZookeeper helper classes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient unit test failure in SocketServerTest,KAFKA-1400,12708817,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,16/Apr/14 16:09,21/May/18 23:21,14/Jul/23 05:39,27/Feb/15 19:54,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"Saw the following transient failure.
kafka.network.SocketServerTest > testSocketsCloseOnShutdown FAILED java.lang.AssertionError: Expected exception: java.net.SocketException 
",,githubbot,gwenshap,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/14 22:27;nehanarkhede;KAFKA-1400.patch;https://issues.apache.org/jira/secure/attachment/12641131/KAFKA-1400.patch","27/Feb/15 00:15;junrao;kafka-1400.patch;https://issues.apache.org/jira/secure/attachment/12701217/kafka-1400.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,387140,,,Mon May 21 23:21:26 UTC 2018,,,,,,,,,,"0|i1uowf:",387403,,,,,,,,,,,,,,,,,,,,"16/Apr/14 16:09;junrao;Perhaps we should expect IOException instead of SocketException.;;;","21/Apr/14 22:27;nehanarkhede;Created reviewboard https://reviews.apache.org/r/20537/
 against branch trunk;;;","29/Apr/14 00:03;junrao;Thanks for the patch. Committed to trunk.;;;","19/Aug/14 01:12;gwenshap;Still seeing the same issue (occasionally).  Except now the Expected Exception is IOException. (no pun intended...)

I'm wondering if this is a race condition. Perhaps the server doesn't complete shutdown before we are sending the request? 

Should we re-open this jira? or open a new one?;;;","19/Aug/14 04:23;junrao;Gwen,

Not sure what the problem is. We can reopen this one. Do you know what exception is thrown, if it's not IOException?;;;","19/Aug/14 17:07;gwenshap;No exception is thrown. Thats the problem :)

I believe that server.shutdown() returns before shutdown is complete, and the next send() command (the one that should fail with an IOException) actually succeeds. 

I hate adding ""sleep"" to unit tests, but I'm not sure how else to avoid the send() succeeding and the test failing.;;;","19/Aug/14 18:18;nehanarkhede;[~gwenshap], to see if you are observing the same problem, it will be helpful to have the log4j output from the failed test. Could you paste it here?;;;","20/Aug/14 02:41;gwenshap;Good idea. Will do that next time the issue reproduces.;;;","25/Feb/15 23:26;gwenshap;.... and here are the logs :

{code}
[2015-02-25 15:11:10,002] INFO Awaiting socket connections on 0.0.0.0:52503. (kafka.network.Acceptor:68)
[2015-02-25 15:11:10,007] INFO [Socket Server on Broker 0], Started (kafka.network.SocketServer:68)
[2015-02-25 15:11:10,096] INFO Awaiting socket connections on 0.0.0.0:52504. (kafka.network.Acceptor:68)
[2015-02-25 15:11:10,096] INFO [Socket Server on Broker 0], Started (kafka.network.SocketServer:68)
wrote data to socket Socket[addr=localhost/127.0.0.1,port=52504,localport=52505]
[2015-02-25 15:11:10,111] DEBUG Accepted connection from /127.0.0.1 on /127.0.0.1:52504. sendBufferSize [actual|requested]: [300000|300000] recvBufferSize [actual|requested]: [310308|300000] (kafka.network.Acceptor:52)
[2015-02-25 15:11:10,114] TRACE Processor id 0 selection time = 17218000 ns (kafka.network.Processor:36)
[2015-02-25 15:11:10,115] DEBUG Processor 0 listening to new connection from /127.0.0.1:52505 (kafka.network.Processor:52)
[2015-02-25 15:11:10,115] TRACE Processor id 0 selection time = 36000 ns (kafka.network.Processor:36)
[2015-02-25 15:11:10,121] TRACE 42 bytes read from /127.0.0.1:52505 (kafka.network.Processor:36)
[2015-02-25 15:11:10,160] TRACE Processor 1 received request : Name: ProducerRequest; Version: 0; CorrelationId: 0; ClientId: ; RequiredAcks: 0; AckTimeoutMs: 0 ms; TopicAndPartition:  (kafka.network.RequestChannel$:36)
[2015-02-25 15:11:10,161] TRACE Processor 0 received request : Name: ProducerRequest; Version: 0; CorrelationId: 0; ClientId: ; RequiredAcks: 0; AckTimeoutMs: 0 ms; TopicAndPartition:  (kafka.network.RequestChannel$:36)
[2015-02-25 15:11:10,165] TRACE Processor id 0 selection time = 3599000 ns (kafka.network.Processor:36)
[2015-02-25 15:11:10,165] INFO [Socket Server on Broker 0], Shutting down (kafka.network.SocketServer:68)
[2015-02-25 15:11:10,166] DEBUG Closing server socket and selector. (kafka.network.Acceptor:52)
[2015-02-25 15:11:10,166] TRACE Socket server received response to send, registering for write: Response(0,Request(0,sun.nio.ch.SelectionKeyImpl@4328493c,null,1424905870123,/127.0.0.1:52505),kafka.network.BoundedByteBufferSend@318b1420,SendAction) (kafka.network.Processor:36)
[2015-02-25 15:11:10,167] TRACE Processor id 0 selection time = 26000 ns (kafka.network.Processor:36)
[2015-02-25 15:11:10,170] TRACE 22 bytes written to /127.0.0.1:52505 using key sun.nio.ch.SelectionKeyImpl@4328493c (kafka.network.Processor:36)
[2015-02-25 15:11:10,170] DEBUG is socket ServerSocket[addr=/0:0:0:0:0:0:0:0,localport=52504] closed? true (kafka.network.Acceptor:52)
[2015-02-25 15:11:10,171] DEBUG Done shutting down acceptor. (kafka.network.Acceptor:52)
[2015-02-25 15:11:10,231] TRACE Completed request:Name: ProducerRequest; Version: 0; CorrelationId: 0; ClientId: ; RequiredAcks: 0; AckTimeoutMs: 0 ms; TopicAndPartition:  from client /127.0.0.1:52505;totalTime:47,requestQueueTime:0,localTime:1424905870165,remoteTime:0,responseQueueTime:1,sendTime:5 (kafka.request.logger:85)
[2015-02-25 15:11:10,232] TRACE Finished writing, registering for read on connection /127.0.0.1:52505 (kafka.network.Processor:36)
[2015-02-25 15:11:10,232] DEBUG Closing selector. (kafka.network.Processor:52)
[2015-02-25 15:11:10,233] DEBUG Closing connection from /127.0.0.1:52505 (kafka.network.Processor:52)
[2015-02-25 15:11:10,237] DEBUG done shutting down processor (kafka.network.Processor:52)
[2015-02-25 15:11:10,237] INFO [Socket Server on Broker 0], SocketServer: Shutdown completed (kafka.network.SocketServer:68)
wrote data to socket Socket[addr=localhost/127.0.0.1,port=52504,localport=52505]
- testSocketsCloseOnShutdown
{code}

Note that we successfully wrote to the socket (... the ""wrote data"" line is logged after calling flush() and sending the request). This is with trunk code (I added few extra log lines for clarity).

Are we sure that writing a single packet (we are not sending a lot of data) to a server that did socket.close() is actually expected to fail?

Because it looks like this may not be the case:
http://stackoverflow.com/questions/11436013/writing-to-a-closed-local-tcp-socket-not-failing
;;;","26/Feb/15 01:00;gwenshap;Kinda strange, the TCPDUMP looks the same whether I get an exception or not:

{code}
16:52:17.249207 IP localhost.55538 > localhost.9095: Flags [S], seq 3746193634, win 65535, options [mss 16344,nop,wscale 5,nop,nop,TS val 1314712608 ecr 0,sackOK,eol], length 0
16:52:17.249257 IP localhost.9095 > localhost.55538: Flags [S.], seq 999508519, ack 3746193635, win 65535, options [mss 16344,nop,wscale 3,nop,nop,TS val 1314712608 ecr 1314712608,sackOK,eol], length 0
16:52:17.249264 IP localhost.55538 > localhost.9095: Flags [.], ack 1, win 12759, options [nop,nop,TS val 1314712608 ecr 1314712608], length 0
16:52:17.249270 IP localhost.9095 > localhost.55538: Flags [.], ack 1, win 38788, options [nop,nop,TS val 1314712608 ecr 1314712608], length 0
16:52:22.436127 IP localhost.55538 > localhost.9095: Flags [P.], seq 1:2, ack 1, win 12759, options [nop,nop,TS val 1314717768 ecr 1314712608], length 1
16:52:22.436162 IP localhost.9095 > localhost.55538: Flags [.], ack 2, win 38788, options [nop,nop,TS val 1314717768 ecr 1314717768], length 0
16:52:22.436169 IP localhost.55538 > localhost.9095: Flags [P.], seq 2:7, ack 1, win 12759, options [nop,nop,TS val 1314717768 ecr 1314717768], length 5
16:52:22.436173 IP localhost.9095 > localhost.55538: Flags [.], ack 7, win 38787, options [nop,nop,TS val 1314717768 ecr 1314717768], length 0
16:52:22.436177 IP localhost.55538 > localhost.9095: Flags [P.], seq 7:47, ack 1, win 12759, options [nop,nop,TS val 1314717768 ecr 1314717768], length 40
16:52:22.436180 IP localhost.9095 > localhost.55538: Flags [.], ack 47, win 38782, options [nop,nop,TS val 1314717768 ecr 1314717768], length 0
16:52:22.484320 IP localhost.9095 > localhost.55538: Flags [P.], seq 1:23, ack 47, win 38782, options [nop,nop,TS val 1314717808 ecr 1314717768], length 22
16:52:22.484353 IP localhost.55538 > localhost.9095: Flags [.], ack 23, win 12758, options [nop,nop,TS val 1314717808 ecr 1314717808], length 0
16:52:22.552365 IP localhost.9095 > localhost.55538: Flags [F.], seq 23, ack 47, win 38782, options [nop,nop,TS val 1314717872 ecr 1314717808], length 0
16:52:22.552372 IP localhost.55538 > localhost.9095: Flags [.], ack 24, win 12758, options [nop,nop,TS val 1314717872 ecr 1314717872], length 0
16:52:22.552376 IP localhost.9095 > localhost.55538: Flags [.], ack 47, win 38782, options [nop,nop,TS val 1314717872 ecr 1314717872], length 0
16:52:28.632196 IP localhost.55538 > localhost.9095: Flags [P.], seq 47:48, ack 24, win 12758, options [nop,nop,TS val 1314723916 ecr 1314717872], length 1
16:52:28.632235 IP localhost.9095 > localhost.55538: Flags [R], seq 999508543, win 0, length 0
{code}

I get a FIN, I ack it. I send one extra packet and I get a RST. Pretty much as expected.

Whether I get an exception or not, OTOH is completely random. I'm guessing this is because the .write() and .flush() methods are async.

As documented for output stream: The <code>flush</code> method of <code>OutputStream</code> does nothing.

I *think* that if I force a sleep after some bytes were written, I'll always get an exception. I'm not too happy with this approach.

Any thoughts?

;;;","26/Feb/15 01:27;gwenshap;On second thought, why do we even try to detect from the client whether the server closed the connection? 

We have the server right there... we can just:
Assert(server.acceptor.socket.isClosed)

Any reason not to do this?;;;","27/Feb/15 00:15;junrao;Created reviewboard https://reviews.apache.org/r/31510/diff/
 against branch origin/trunk;;;","27/Feb/15 00:16;junrao;Provide a simple fix to send enough bytes to trigger a socket flush. Without the patch, the unit test failed 3 times on 10 tries. With the patch, it didn't fail in 20 tries.;;;","27/Feb/15 01:30;gwenshap;Any reason you prefer to try and detect a closed socket from the client side, rather than check the state of the socket on the server?;;;","27/Feb/15 01:35;junrao;Well, this test is supposed to test the client behavior. On the server side, we know the socket will be closed on shutdown.;;;","27/Feb/15 02:01;gwenshap;I was under the impression that this was there to validate that the SocketServer actually closes the sockets as intended.

Isn't the client behavior when writing to a closed socket part of Java's NIO implementation (and the OS socket implementation, and TCP specs...) and largely outside our control?

Your fix is obviously valid, so I'm not really arguing against your patch. Just trying to clarify my own understanding.;;;","27/Feb/15 19:47;junrao;Yes, that's probably the goal of this test. The checking on the socket server may be a bit involved though since in addition to checking the socket for the acceptor, we probably need to check the sockets in each of the processors. Testing from the client seems simpler to me.

How about this: let me check in my patch. I will resolve this jira, but not close it. If you have a better patch, feel free to reopen the jira.;;;","27/Feb/15 19:54;junrao;Thanks for the review. Committed to trunk.;;;","27/Feb/15 20:48;gwenshap;Awesome. I'm just happy to see this fixed :)

It drove me mad, especially since I wasn't sure if its just my multi-port work, or a more general problem...;;;","21/May/18 23:15;githubbot;JimGalasyn opened a new pull request #5056: KAFKA-1400: Fix link in num.standby.replicas section
URL: https://github.com/apache/kafka/pull/5056
 
 
   Replace broken link with link to [State restoration during workload rebalance](https://docs.confluent.io/current/streams/developer-guide/running-app.html#state-restoration-during-workload-rebalance). Fixes [KSTREAMS-1400: AK docs: broken link in section on num.standby.replicas](https://confluentinc.atlassian.net/browse/KSTREAMS-1400).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;","21/May/18 23:21;githubbot;JimGalasyn opened a new pull request #142: KAFKA-1400: Fix link in num.standby.replicas section
URL: https://github.com/apache/kafka-site/pull/142
 
 
   Replace broken link with link to [State restoration during workload rebalance](https://docs.confluent.io/current/streams/developer-guide/running-app.html#state-restoration-during-workload-rebalance). Fixes [KSTREAMS-1400: AK docs: broken link in section on num.standby.replicas](https://confluentinc.atlassian.net/browse/KSTREAMS-1400).

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Topic config changes can be lost and cause fatal exceptions on broker restarts,KAFKA-1398,12708658,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,jkreps,jjkoshy,jjkoshy,16/Apr/14 01:35,27/May/14 18:42,14/Jul/23 05:39,18/Apr/14 23:23,0.8.1,,,0.8.1.1,,,,,,,,,,0,,,,"Our topic config cleanup policy seems to be broken. When a broker is
bounced and starting up:
1 - Read all the children of the config change path
2 - For each, if the change id is greater than the last executed change,
  then extract the topic information.
3 - If there is a log for that topic on this broker, then apply the change.
  However, if there is no log, then delete the config change.

In step 3, a delete triggers a child change watch firing on all the other
brokers. The other brokers currently take all the children of the config
path but will ignore those config changes that are less than the last
executed change. At least one issue here is that if a broker does not have
partitions for a topic then the lastExecutedChange is not updated (for
that topic).

Consider this scenario:
- Three brokers 0, 1, 2
- Topic A has partitions only assigned to broker 0
- Topic B has partitions only assigned to broker 1
- Topic C has partitions only assigned to broker 2
- Change 0: topic A
- Change 1: topic B
- Change 2: topic C
- lastExecutedChange on broker 0 is 0
- lastExecutedChange on broker 1 is 1
- lastExecutedChange on broker 2 is 2
- Bounce broker 1
- The above bounce will cause Change 0 and Change 2 to get deleted.
- Watch fires on broker 0 and 1
- Broker 0 will try and read the topic corresponding to change 1 (since its
  lastExecutedChange is 0) and then change 2. That read will fail:

2014/04/15 19:35:34.236 INFO [TopicConfigManager] [main] [kafka-server] [] Processed topic config change 25 for topic xyz, setting new config to
 {retention.ms=3600000, segment.ms=3600000}.
2014/04/15 19:35:34.238 FATAL [KafkaServerStartable] [main] [kafka-server] [] Fatal error during KafkaServerStable startup. Prepare to shutdown
org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /config/changes/config_change_0000000026
        at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47)
        at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685)
        at org.I0Itec.zkclient.ZkClient.readData(ZkClient.java:766)
        at org.I0Itec.zkclient.ZkClient.readData(ZkClient.java:761)
        at kafka.utils.ZkUtils$.readData(ZkUtils.scala:467)
        at kafka.server.TopicConfigManager$$anonfun$kafka$server$TopicConfigManager$$processConfigChanges$2.apply(TopicConfigManager.scala:97)
        at kafka.server.TopicConfigManager$$anonfun$kafka$server$TopicConfigManager$$processConfigChanges$2.apply(TopicConfigManager.scala:93)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.server.TopicConfigManager.kafka$server$TopicConfigManager$$processConfigChanges(TopicConfigManager.scala:93)
        at kafka.server.TopicConfigManager.processAllConfigChanges(TopicConfigManager.scala:81)
        at kafka.server.TopicConfigManager.startup(TopicConfigManager.scala:72)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:104)
        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:34)
        ...
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /config/changes/config_change_0000000026
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:102)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
        at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:927)
        at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:956)
        at org.I0Itec.zkclient.ZkConnection.readData(ZkConnection.java:103)
        at org.I0Itec.zkclient.ZkClient$9.call(ZkClient.java:770)
        at org.I0Itec.zkclient.ZkClient$9.call(ZkClient.java:766)
        at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)
        ... 39 more


Another issue is that there are two logging statements with incorrect
qualifiers which makes things a little harder to debug. E.g.,

2014/04/15 19:35:34.223 ERROR [TopicConfigManager] [kafka-server] [] Ignoring topic config change %d for topic %s since the change has expired

",,jjkoshy,jkreps,jonbringhurst,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1380,,,,"18/Apr/14 19:59;jkreps;KAFKA-1398.patch;https://issues.apache.org/jira/secure/attachment/12640877/KAFKA-1398.patch","18/Apr/14 18:05;jkreps;KAFKA-1398.patch;https://issues.apache.org/jira/secure/attachment/12640860/KAFKA-1398.patch","18/Apr/14 00:36;jkreps;KAFKA-1398.patch;https://issues.apache.org/jira/secure/attachment/12640749/KAFKA-1398.patch","18/Apr/14 20:03;jkreps;KAFKA-1398_2014-04-18_13:03:03.patch;https://issues.apache.org/jira/secure/attachment/12640879/KAFKA-1398_2014-04-18_13%3A03%3A03.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,386981,,,Fri Apr 18 23:23:25 UTC 2014,,,,,,,,,,"0|i1unx3:",387244,,,,,,,,,,,,,,,,,,,,"18/Apr/14 00:36;jkreps;Created reviewboard https://reviews.apache.org/r/20471/
 against branch trunk;;;","18/Apr/14 00:39;jkreps;Looks like this got broken with the delete topic work.

I refactored a bit
- Added a unit test for the simple case of changing config
- Moved purge logic into another method
- Changed the zk ops to handle the case where the znode is purged in between the notification and the read.;;;","18/Apr/14 18:05;jkreps;Created reviewboard https://reviews.apache.org/r/20492/
 against branch trunk;;;","18/Apr/14 19:59;jkreps;Created reviewboard https://reviews.apache.org/r/20498/
 against branch trunk;;;","18/Apr/14 20:03;jkreps;Updated reviewboard https://reviews.apache.org/r/20471/
 against branch trunk;;;","18/Apr/14 20:41;jjkoshy;Reopening to keep track of the follow-up. Also, I need to commit to 0.8.1.;;;","18/Apr/14 23:23;jjkoshy;Committed to 0.8.1 as well;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
delete topic is not working ,KAFKA-1397,12708621,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tnachen,junrao,junrao,15/Apr/14 20:57,12/Feb/16 21:46,14/Jul/23 05:39,06/May/14 17:39,0.8.2.0,,,0.8.2.0,,,,,,,,,,2,,,,All unit tests are disabled since they hang transiently (see details in KAFKA-1391).,,aozeritsky,av11du@gmail.com,fullung,harisekhon,huasanyelao,ijuma,jimplush,junrao,mgharat,mrlabbe,nehanarkhede,smiklosovic,spidaman,sriharsha,stevenz3wu,tnachen,vivumail@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SAMZA-374,,,,,,,,,,,,,"27/Apr/14 06:43;tnachen;KAFKA-1397.patch;https://issues.apache.org/jira/secure/attachment/12642115/KAFKA-1397.patch","28/Apr/14 21:48;tnachen;KAFKA-1397_2014-04-28_14:48:32.patch;https://issues.apache.org/jira/secure/attachment/12642340/KAFKA-1397_2014-04-28_14%3A48%3A32.patch","29/Apr/14 00:08;tnachen;KAFKA-1397_2014-04-28_17:08:49.patch;https://issues.apache.org/jira/secure/attachment/12642365/KAFKA-1397_2014-04-28_17%3A08%3A49.patch","30/Apr/14 21:55;tnachen;KAFKA-1397_2014-04-30_14:55:28.patch;https://issues.apache.org/jira/secure/attachment/12642734/KAFKA-1397_2014-04-30_14%3A55%3A28.patch","01/May/14 22:54;tnachen;KAFKA-1397_2014-05-01_15:53:57.patch;https://issues.apache.org/jira/secure/attachment/12642945/KAFKA-1397_2014-05-01_15%3A53%3A57.patch","02/May/14 01:12;tnachen;KAFKA-1397_2014-05-01_18:12:24.patch;https://issues.apache.org/jira/secure/attachment/12642979/KAFKA-1397_2014-05-01_18%3A12%3A24.patch","02/May/14 20:38;tnachen;KAFKA-1397_2014-05-02_13:38:02.patch;https://issues.apache.org/jira/secure/attachment/12643112/KAFKA-1397_2014-05-02_13%3A38%3A02.patch","05/May/14 18:18;tnachen;KAFKA-1397_2014-05-05_11:17:59.patch;https://issues.apache.org/jira/secure/attachment/12643398/KAFKA-1397_2014-05-05_11%3A17%3A59.patch","05/May/14 21:00;tnachen;KAFKA-1397_2014-05-05_14:00:29.patch;https://issues.apache.org/jira/secure/attachment/12643426/KAFKA-1397_2014-05-05_14%3A00%3A29.patch",,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,386944,,,Fri Feb 12 21:46:55 UTC 2016,,,,,,,,,,"0|i1unov:",387207,,,,,,,,,,,,,,,,,,,,"16/Apr/14 21:41;jimplush;what was the regression that caused topic deletion to stop working?

;;;","16/Apr/14 22:30;junrao;The original implementation never fully worked.;;;","27/Apr/14 06:43;tnachen;Created reviewboard https://reviews.apache.org/r/20745/
 against branch origin/trunk;;;","28/Apr/14 19:09;nehanarkhede;[~tnachen] Can you please explain the issue that your patch attempts to fix? I couldn't find anything on the JIRA or reviewboard.;;;","28/Apr/14 21:48;tnachen;Updated reviewboard https://reviews.apache.org/r/20745/
 against branch origin/trunk;;;","29/Apr/14 00:08;tnachen;Updated reviewboard https://reviews.apache.org/r/20745/
 against branch origin/trunk;;;","30/Apr/14 21:55;tnachen;Updated reviewboard https://reviews.apache.org/r/20745/
 against branch origin/trunk;;;","01/May/14 22:54;tnachen;Updated reviewboard https://reviews.apache.org/r/20745/
 against branch origin/trunk;;;","02/May/14 01:12;tnachen;Updated reviewboard https://reviews.apache.org/r/20745/
 against branch origin/trunk;;;","02/May/14 20:38;tnachen;Updated reviewboard https://reviews.apache.org/r/20745/
 against branch origin/trunk;;;","05/May/14 18:18;tnachen;Updated reviewboard https://reviews.apache.org/r/20745/
 against branch origin/trunk;;;","05/May/14 21:00;tnachen;Updated reviewboard https://reviews.apache.org/r/20745/
 against branch origin/trunk;;;","06/May/14 17:39;junrao;Thanks for the patch. +1 and committed to trunk.;;;","12/Apr/15 12:22;smiklosovic;I am not sure what I am doing wrong but it seems to me this does not work in 0.8.2.1

I am doing this

$ bin/zookeeper-server-start.sh config/zookeeper.properties
$ bin/kafka-server-start.sh config/server.properties
$ bin/kafka-topics.sh --zookeeper 127.0.0.1:2181 --create --topic someTopic --partitions 10 --replication-factor 1 
$ bin/kafka-topics.sh --list // gives me ""someTopic""
$ bin/kafka-topics.sh --zookeeper 127.0.0.1:2181 --delete --topic someTopic
$ bin/kafka-topics.sh --list // gives me ""someTopic - marked for deletion""

and this is there forever, even when I stop broker and zookeeper and start it once again, nothing changes, it is there forever.

I set delete.topics.enable=true in server.properties for broker;;;","12/Apr/15 13:58;sriharsha;[~smiklosovic]
It looks like you are setting the wrong property , its not ""delete.topics.enable"" . Can you try setting ""delete.topic.enable"" (topic is not plural) to true ;;;","12/Apr/15 14:22;smiklosovic;Yes it is ""delete.topic.enable"", that was my typo in the original post, the behaviour is the same. Could you confirm this?;;;","12/Apr/15 14:38;sriharsha;[~smiklosovic] I did a quick test on a single kafka 0.8.2.1 . I am not able to reproduce this it takes a bit of time but the topic gets deleted.
Just to check make sure you don't have any producer or consumer sending any requests while the delete topic is going on or just set auto.creation.topic.enable to false in server.properties

./bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic test-topic                                               
Topic:test-topic        PartitionCount:10       ReplicationFactor:1     Configs:
        Topic: test-topic       Partition: 0    Leader: 0       Replicas: 0     Isr: 0
        Topic: test-topic       Partition: 1    Leader: 0       Replicas: 0     Isr: 0
        Topic: test-topic       Partition: 2    Leader: 0       Replicas: 0     Isr: 0
        Topic: test-topic       Partition: 3    Leader: 0       Replicas: 0     Isr: 0
        Topic: test-topic       Partition: 4    Leader: 0       Replicas: 0     Isr: 0
        Topic: test-topic       Partition: 5    Leader: 0       Replicas: 0     Isr: 0
        Topic: test-topic       Partition: 6    Leader: 0       Replicas: 0     Isr: 0
        Topic: test-topic       Partition: 7    Leader: 0       Replicas: 0     Isr: 0
        Topic: test-topic       Partition: 8    Leader: 0       Replicas: 0     Isr: 0
        Topic: test-topic       Partition: 9    Leader: 0       Replicas: 0     Isr: 0
 ⚙ ⮀ ~/build/kafka_2.10-0.8.2.1 ⮀ 
» ./bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic test-topic                                                 
Topic test-topic is marked for deletion.
Note: This will have no impact if delete.topic.enable is not set to true.
 ⚙ ⮀ ~/build/kafka_2.10-0.8.2.1 ⮀ 
» ./bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic test-topic                                              
Topic:test-topic        PartitionCount:10       ReplicationFactor:1     Configs:
        Topic: test-topic       Partition: 0    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 1    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 2    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 3    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 4    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 5    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 6    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 7    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 8    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 9    Leader: -1      Replicas: 0     Isr: 
                                                                                     
 ⚙ ⮀ ~/build/kafka_2.10-0.8.2.1 ⮀ 
» ./bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic test-topic                                               
Topic:test-topic        PartitionCount:10       ReplicationFactor:1     Configs:
        Topic: test-topic       Partition: 0    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 1    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 2    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 3    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 4    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 5    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 6    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 7    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 8    Leader: -1      Replicas: 0     Isr: 
        Topic: test-topic       Partition: 9    Leader: -1      Replicas: 0     Isr: 
 ⚙ ⮀ ~/build/kafka_2.10-0.8.2.1 ⮀ 
» ./bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic test-topic 

Last command doesn't show the topic .;;;","10/Sep/15 08:36;harisekhon;I've got this same issue with 0.8.2.2 and there are no consumers or producers using the topic that is remaining as marked for deletion but not disappearing from --list or --describe. I even checked I had set the correct variable delete.topic.enable=true and restarted Kafka. This is on HDP 2.3 fully kerberized.;;;","15/Oct/15 22:09;spidaman;I'm using the ASF release of 0.8.2.2 and I can confirm, topic deletion does not work.  The only way to ditch a topic is to stop the brokers, remove the directories on disk, remove the topic from zookeeper and start the brokers back up;;;","16/Oct/15 00:59;junrao;One thing that you want to make sure is that the delete.topic.enable=true property is indeed set. If you start the broker, we log all overridden properties, could you check if delete.topic.enable is there?;;;","20/Jan/16 13:50;vivumail@gmail.com;I have 0.8.2.2 version kafka and the delete topic was working fine till today(i am running this version past 1 month). I used to delete the topics at times and it was working fine. Suddenly it stopped working. After multiple restart (and delete from zookeeper client) helped me to remove few topics but 3 of them remains in the server and after a restart of both kafka and zookeeper they are back to active topic. I have ensured all the consumers and producers stopped while removing and restarting the kafka. but still this issue is still there.;;;","20/Jan/16 18:01;mgharat;Can you paste the server side logs?;;;","12/Feb/16 21:39;av11du@gmail.com;I would just like to add that I am still seeing issues deleting topics in 0.9.0. I will delete topics with kafka-topics.sh and it is set as ""Marked for Deletion"" but never gets deleted. Is this still a known issue?

On server.log I see lot's of these messages:

[2016-02-12 21:33:29,480] WARN [Replica Manager on Broker 1]: While recording the replica LEO, the partition [spiderman-pricegrabber-prices,5] hasn't been created. (kafka.server.ReplicaManager)

On state-change.log, these are samples of the llog entries for this topic after I tried to delete with kafka-topics.sh. For each of these entries, there is typically one per partition... I've just summarized.

[2016-02-12 21:02:19,585] TRACE Controller 1 epoch 234 changed state of replica 2 for partition [spiderman-pricegrabber-prices,3] from ReplicaDeletionIneligible to OfflineReplica (state.change.logger)
[2016-02-12 21:02:19,766] TRACE Controller 1 epoch 234 sending UpdateMetadata request (Leader:-2,ISR:1,3,LeaderEpoch:0,ControllerEpoch:234) to broker 1 for partition spiderman-pricegrabber-prices-5 (state.change.logger)
[2016-02-12 21:02:19,824] ERROR Controller 1 epoch 234 initiated state change of replica 2 for partition [spiderman-pricegrabber-prices,6] from OfflineReplica to ReplicaDeletionIneligible failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=spiderman-pricegrabber-prices,Partition=6,Replica=2] should be in the ReplicaDeletionStarted states before moving to ReplicaDeletionIneligible state. Instead it is in OfflineReplica state
[2016-02-12 21:02:20,117] TRACE Controller 1 epoch 234 changed state of replica 1 for partition [spiderman-pricegrabber-prices,4] from OfflineReplica to ReplicaDeletionStarted (state.change.logger)
[2016-02-12 21:02:20,131] TRACE Broker 1 deleted partition [spiderman-pricegrabber-prices,3] from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 234 with correlation id 1013 (state.change.logger)
[2016-02-12 21:02:20,132] TRACE Controller 1 epoch 234 received response {error_code=0,partitions=[{topic=spiderman-pricegrabber-prices,partition=0,error_code=0}]} for a request sent to broker Node(3, 10.108.0.105, 9092) (state.change.logger)
[2016-02-12 21:02:20,139] TRACE Broker 1 handling stop replica (delete=false) for partition [spiderman-pricegrabber-prices,5] (state.change.logger)
[2016-02-12 21:02:20,139] TRACE Broker 1 finished handling stop replica (delete=false) for partition [spiderman-pricegrabber-prices,5] (state.change.logger)
[2016-02-12 21:02:20,140] TRACE Controller 1 epoch 234 received response {error_code=0,partitions=[{topic=spiderman-pricegrabber-prices,partition=7,error_code=0}]} for a request sent to broker Node(3, 10.108.0.105, 9092) (state.change.logger)
[2016-02-12 21:02:20,151] TRACE Broker 1 handling stop replica (delete=false) for partition [spiderman-pricegrabber-prices,3] (state.change.logger)
[2016-02-12 21:02:20,151] TRACE Broker 1 finished handling stop replica (delete=false) for partition [spiderman-pricegrabber-prices,3] (state.change.logger)
[2016-02-12 21:02:20,152] TRACE Controller 1 epoch 234 received response {error_code=0,partitions=[{topic=spiderman-pricegrabber-prices,partition=3,error_code=0}]} for a request sent to broker Node(1, 10.108.0.122, 9092) (state.change.logger)
[2016-02-12 21:02:20,153] TRACE Broker 1 handling stop replica (delete=false) for partition [spiderman-pricegrabber-prices,1] (state.change.logger)
[2016-02-12 21:02:20,838] TRACE Controller 1 epoch 234 received response {error_code=0,partitions=[{topic=spiderman-pricegrabber-prices,partition=4,error_code=0}]} for a request sent to broker Node(1, 10.108.0.122, 9092) (state.change.logger)
[2016-02-12 21:02:20,838] TRACE Controller 1 epoch 234 changed state of replica 1 for partition [spiderman-pricegrabber-prices,4] from ReplicaDeletionStarted to ReplicaDeletionSuccessful (state.change.logger)
[2016-02-12 21:02:21,090] TRACE Controller 1 epoch 234 received response {error_code=0,partitions=[{topic=spiderman-pricegrabber-prices,partition=6,error_code=0}]} for a request sent to broker Node(3, 10.108.0.105, 9092) (state.change.logger)
[2016-02-12 21:02:21,090] TRACE Controller 1 epoch 234 changed state of replica 3 for partition [spiderman-pricegrabber-prices,6] from ReplicaDeletionStarted to ReplicaDeletionSuccessful (state.change.logger);;;","12/Feb/16 21:46;ijuma;There is also KAFKA-2937 which was fixed for 0.9.0.1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix transient unit test ProducerFailureHandlingTest.testBrokerFailure,KAFKA-1396,12708619,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,junrao,junrao,15/Apr/14 20:55,14/May/14 00:34,14/Jul/23 05:39,14/May/14 00:34,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,Currently disabled after kafka-1390.,,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/May/14 20:48;guozhang;KAFKA-1396.patch;https://issues.apache.org/jira/secure/attachment/12644694/KAFKA-1396.patch","07/May/14 21:16;guozhang;KAFKA-1396.patch;https://issues.apache.org/jira/secure/attachment/12643847/KAFKA-1396.patch","07/May/14 21:55;guozhang;KAFKA-1396_2014-05-07_14:55:09.patch;https://issues.apache.org/jira/secure/attachment/12643856/KAFKA-1396_2014-05-07_14%3A55%3A09.patch","08/May/14 22:57;guozhang;KAFKA-1396_2014-05-08_15:57:11.patch;https://issues.apache.org/jira/secure/attachment/12644040/KAFKA-1396_2014-05-08_15%3A57%3A11.patch","09/May/14 17:35;guozhang;KAFKA-1396_2014-05-09_10:35:07.patch;https://issues.apache.org/jira/secure/attachment/12644135/KAFKA-1396_2014-05-09_10%3A35%3A07.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,386942,,,Wed May 14 00:34:08 UTC 2014,,,,,,,,,,"0|i1unon:",387206,,,,,,,,,,,,,,,,,,,,"15/Apr/14 21:42;guozhang;1. I tried the test several times but cannot re-produce the hanging locally. So I can only debug based on the stack trace in the comments.

2. Following the stack trace, there is no ""synchronized"" function call along the trace; also it is not clear if a real dead-lock happens during the hanging.

3. It is suggested that the JVM GC could block any threads:

http://stackoverflow.com/questions/7067058/java-thread-dump-blocked-thread-without-waiting-to-lock
http://stackoverflow.com/questions/4016356/java-blocking-issue-why-would-jvm-block-threads-in-many-different-classes-metho

I agree this test takes long, but it is not a unit test but rather an integration test. I am wondering if we are sure there is occasional forever-hangs there or just occasional taking-too-long-due-to-GC? ;;;","16/Apr/14 03:31;junrao;Basically what I did was to disable the rest of the tests in this file and ran it 3 times on a mac laptop, in 2 times, the test didn't finish after more than 1 minute.;;;","16/Apr/14 17:48;guozhang;I did the same on a mac laptop and on a desktop, each for 10 times. All of them finished within 1 minute, some is close to 1 minute. So I think the problem is this integration test being long not having transient failures. One thing we can do is to reduce the number of server failure iterations (currently it is 5).

--- Desktop ---

Total time: 40.028 secs

real    0m40.122s
user    0m48.018s
sys    0m3.791s

Total time: 40.411 secs

real    0m40.501s
user    0m44.771s
sys    0m3.517s

Total time: 41.76 secs

real    0m41.863s
user    0m47.275s
sys    0m3.953s

Total time: 41.096 secs

real    0m41.192s
user    0m47.258s
sys    0m3.942s

Total time: 42.378 secs

real    0m42.478s
user    0m52.630s
sys    0m5.211s

Total time: 41.79 secs

real    0m41.893s
user    0m50.208s
sys    0m4.515s


--- Laptop ---

Total time: 49.44 secs

real    0m49.589s
user    1m6.507s
sys    0m5.650s

Total time: 48.087 secs

real    0m48.264s
user    1m2.772s
sys    0m5.352s

Total time: 48.157 secs

real    0m48.312s
user    1m1.393s
sys    0m5.156s

Total time: 47.686 secs

real    0m47.854s
user    0m57.030s
sys    0m4.664s

Total time: 48.67 secs

real    0m48.862s
user    1m4.015s
sys    0m5.774s

Total time: 48.875 secs

real    0m49.054s
user    1m0.465s
sys    0m5.154s

Total time: 47.233 secs

real    0m47.414s
user    0m59.274s
sys    0m4.835s

Total time: 48.635 secs

real    0m48.826s
user    1m1.937s
sys    0m5.646s

Total time: 49.261 secs

real    0m49.420s
user    1m3.514s
sys    0m5.081s

Total time: 49.883 secs

real    0m50.040s
user    1m4.248s
sys    0m5.244s
 ;;;","28/Apr/14 17:54;junrao;I suggest that we do the following.

1. Reduce the # iterations from 5 to 2 to reduce the execution time.

2. Wait for metadata propagation before start fetching.

3. There could be messages that get committed by the broker, but whose ack to the producer are lost, depending on when the brokers are bounced. So, the test needs to be more resilient. Perhaps we can make sure that uniqueProducedMessageSet is equal to or a subset of uniqueFetchedMessageSet.;;;","28/Apr/14 18:02;guozhang;+1 on 1,2.

I am a little unclear about 3, is this case not covered by only counting uniqueFetchedMessageSet and compare with the sent count on scheduler, which does not include duplicates either?;;;","29/Apr/14 00:09;junrao;On second thought, the existing test does handle #3.;;;","07/May/14 21:16;guozhang;Created reviewboard https://reviews.apache.org/r/21174/
 against branch origin/trunk;;;","07/May/14 21:55;guozhang;Updated reviewboard https://reviews.apache.org/r/21174/
 against branch origin/trunk;;;","08/May/14 22:57;guozhang;Updated reviewboard https://reviews.apache.org/r/21174/
 against branch origin/trunk;;;","09/May/14 17:35;guozhang;Updated reviewboard https://reviews.apache.org/r/21174/
 against branch origin/trunk;;;","13/May/14 20:48;guozhang;Created reviewboard https://reviews.apache.org/r/21406/
 against branch origin/trunk;;;","14/May/14 00:34;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix unit tests in AutoOffsetResetTest,KAFKA-1395,12708618,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,junrao,junrao,15/Apr/14 20:53,25/Apr/14 21:32,14/Jul/23 05:39,25/Apr/14 21:32,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,It's currently disabled after kafka-1390.,,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/14 22:07;guozhang;KAFKA-1395.patch;https://issues.apache.org/jira/secure/attachment/12640341/KAFKA-1395.patch","16/Apr/14 17:11;guozhang;KAFKA-1395_2014-04-16_10:11:30.patch;https://issues.apache.org/jira/secure/attachment/12640493/KAFKA-1395_2014-04-16_10%3A11%3A30.patch","22/Apr/14 17:44;guozhang;KAFKA-1395_2014-04-22_10:43:54.patch;https://issues.apache.org/jira/secure/attachment/12641287/KAFKA-1395_2014-04-22_10%3A43%3A54.patch","24/Apr/14 23:22;guozhang;KAFKA-1395_2014-04-24_16:22:18.patch;https://issues.apache.org/jira/secure/attachment/12641825/KAFKA-1395_2014-04-24_16%3A22%3A18.patch","25/Apr/14 18:03;guozhang;KAFKA-1395_2014-04-25_11:03:13.patch;https://issues.apache.org/jira/secure/attachment/12641973/KAFKA-1395_2014-04-25_11%3A03%3A13.patch","25/Apr/14 18:54;guozhang;KAFKA-1395_2014-04-25_11:54:08.patch;https://issues.apache.org/jira/secure/attachment/12641987/KAFKA-1395_2014-04-25_11%3A54%3A08.patch","25/Apr/14 18:57;guozhang;KAFKA-1395_2014-04-25_11:56:47.patch;https://issues.apache.org/jira/secure/attachment/12641988/KAFKA-1395_2014-04-25_11%3A56%3A47.patch","25/Apr/14 21:09;guozhang;KAFKA-1395_2014-04-25_14:09:21.patch;https://issues.apache.org/jira/secure/attachment/12642012/KAFKA-1395_2014-04-25_14%3A09%3A21.patch",,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,386941,,,Fri Apr 25 21:32:03 UTC 2014,,,,,,,,,,"0|i1unof:",387205,,,,,,,,,,,,,,,,,,,,"15/Apr/14 22:07;guozhang;Created reviewboard https://reviews.apache.org/r/20380/
 against branch origin/trunk;;;","16/Apr/14 17:11;guozhang;Updated reviewboard https://reviews.apache.org/r/20380/
 against branch origin/trunk;;;","22/Apr/14 17:44;guozhang;Updated reviewboard https://reviews.apache.org/r/20380/
 against branch origin/trunk;;;","24/Apr/14 23:22;guozhang;Updated reviewboard https://reviews.apache.org/r/20380/
 against branch origin/trunk;;;","25/Apr/14 18:03;guozhang;Updated reviewboard https://reviews.apache.org/r/20380/
 against branch origin/trunk;;;","25/Apr/14 18:54;guozhang;Updated reviewboard  against branch origin/trunk;;;","25/Apr/14 18:57;guozhang;Updated reviewboard  against branch origin/trunk;;;","25/Apr/14 21:09;guozhang;Updated reviewboard https://reviews.apache.org/r/20380/
 against branch origin/trunk;;;","25/Apr/14 21:32;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient unit test failure in DeleteTopicTest.testPreferredReplicaElectionDuringDeleteTopic,KAFKA-1391,12708165,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,junrao,junrao,13/Apr/14 22:11,16/Apr/14 15:55,14/Jul/23 05:39,16/Apr/14 15:55,0.8.2.0,,,0.8.2.0,,,,,,,,,,0,,,,"The test hang due to the following deadlock.

""Test worker"" prio=5 tid=7fd40c0b2800 nid=0x114ebd000 waiting on condition [114eb9000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <7f40b2aa0> (a java.util.concurrent.CountDownLatch$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)
        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:207)
        at kafka.utils.ShutdownableThread.shutdown(ShutdownableThread.scala:36)
        at kafka.controller.TopicDeletionManager.shutdown(TopicDeletionManager.scala:105)
        at kafka.controller.KafkaController$$anonfun$onControllerResignation$1.apply$mcV$sp(KafkaController.scala:344)
        at kafka.controller.KafkaController$$anonfun$onControllerResignation$1.apply(KafkaController.scala:340)
        at kafka.controller.KafkaController$$anonfun$onControllerResignation$1.apply(KafkaController.scala:340)
        at kafka.utils.Utils$.inLock(Utils.scala:537)
        at kafka.controller.KafkaController.onControllerResignation(KafkaController.scala:340)
        at kafka.controller.KafkaController$$anonfun$shutdown$1.apply$mcV$sp(KafkaController.scala:647)
        at kafka.controller.KafkaController$$anonfun$shutdown$1.apply(KafkaController.scala:645)
        at kafka.controller.KafkaController$$anonfun$shutdown$1.apply(KafkaController.scala:645)
        at kafka.utils.Utils$.inLock(Utils.scala:537)
        at kafka.controller.KafkaController.shutdown(KafkaController.scala:645)
        at kafka.server.KafkaServer$$anonfun$shutdown$9.apply$mcV$sp(KafkaServer.scala:242)
        at kafka.utils.Utils$.swallow(Utils.scala:166)
        at kafka.utils.Logging$class.swallowWarn(Logging.scala:92)
        at kafka.utils.Utils$.swallowWarn(Utils.scala:45)
        at kafka.utils.Logging$class.swallow(Logging.scala:94)
        at kafka.utils.Utils$.swallow(Utils.scala:45)
        at kafka.server.KafkaServer.shutdown(KafkaServer.scala:242)
        at kafka.admin.DeleteTopicTest.testPreferredReplicaElectionDuringDeleteTopic(DeleteTopicTest.scala:163)

""delete-topics-thread"" prio=5 tid=7fd409ad2000 nid=0x11b0c2000 waiting on condition [11b0c1000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <7f40a7048> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
        at kafka.utils.Utils$.inLock(Utils.scala:535)
        at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:376)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)

",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,386488,,,Wed Apr 16 15:55:54 UTC 2014,,,,,,,,,,"0|i1ukvz:",386752,,,,,,,,,,,,,,,,,,,,"13/Apr/14 22:15;junrao;KAFKA-1363 doesn't quite fix the hanging issue when shutting down deleteTopicsThread.  This thread does the following. During the shutdown, if the shutdown logic will first unblock awaitTopicDeletionNotification() and then mark isRunning as false. However, if the marking happens after the checking of isRunning in the deleteTopicsThread, it will block forever. 

    override def doWork() {
      awaitTopicDeletionNotification()

      if(!isRunning.get)
        return

      inLock(controllerContext.controllerLock) {
;;;","13/Apr/14 22:17;junrao;At this moment, I am not if it's worth keeping patching the unit tests in DeleteTopicTest. We know that we have to make another pass of them when we actually fix the delete topic logic itself. We probably can just comment out all tests in DeleteTopicTest for now and fix them when we actual fix delete topic.;;;","16/Apr/14 15:55;junrao;The relevant unit tests are commented out for now in KAFKA-1390. The real fix will be done in KAFKA-1397.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient unit test failure in ProducerFailureHandlingTest,KAFKA-1389,12708155,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,13/Apr/14 20:09,24/Apr/14 18:50,14/Jul/23 05:39,24/Apr/14 00:30,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"Saw the following transient unit test failure.

kafka.api.ProducerFailureHandlingTest > testTooLargeRecordWithAckZero FAILED
    junit.framework.AssertionFailedError: Partition [topic-1,0] metadata not propagated after timeout
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.assertTrue(Assert.java:20)
        at kafka.utils.TestUtils$.waitUntilMetadataIsPropagated(TestUtils.scala:532)
        at kafka.utils.TestUtils$$anonfun$createTopic$1.apply(TestUtils.scala:151)
        at kafka.utils.TestUtils$$anonfun$createTopic$1.apply(TestUtils.scala:150)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.immutable.Range.foreach(Range.scala:141)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at kafka.utils.TestUtils$.createTopic(TestUtils.scala:150)
        at kafka.api.ProducerFailureHandlingTest.testTooLargeRecordWithAckZero(ProducerFailureHandlingTest.scala:115)
",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1420,,,,,,,,,,"13/Apr/14 20:22;junrao;KAFKA-1389.patch;https://issues.apache.org/jira/secure/attachment/12639992/KAFKA-1389.patch","23/Apr/14 17:41;junrao;KAFKA-1389_2014-04-23_10:41:08.patch;https://issues.apache.org/jira/secure/attachment/12641519/KAFKA-1389_2014-04-23_10%3A41%3A08.patch","23/Apr/14 21:36;junrao;KAFKA-1389_2014-04-23_14:36:00.patch;https://issues.apache.org/jira/secure/attachment/12641584/KAFKA-1389_2014-04-23_14%3A36%3A00.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,386478,,,Thu Apr 24 00:30:54 UTC 2014,,,,,,,,,,"0|i1uktr:",386742,,,,,,,,,,,,,,,,,,,,"13/Apr/14 20:22;junrao;Created reviewboard https://reviews.apache.org/r/20290/
 against branch origin/trunk;;;","16/Apr/14 16:07;junrao;We can use this opportunity to standardize the timeout in TestUtils.waitUntilMetadataIsPropagated() to sth like 5 secs.;;;","23/Apr/14 17:41;junrao;Updated reviewboard https://reviews.apache.org/r/20290/
 against branch origin/trunk;;;","23/Apr/14 21:36;junrao;Updated reviewboard https://reviews.apache.org/r/20290/
 against branch origin/trunk;;;","24/Apr/14 00:30;junrao;Thanks for the review. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka getting stuck creating ephemeral node it has already created when two zookeeper sessions are established in a very short period of time,KAFKA-1387,12707778,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,fpj,slon,slon,10/Apr/14 19:48,24/Sep/15 17:15,14/Jul/23 05:39,24/Sep/15 17:15,0.8.1.1,,,0.9.0.0,,,,,,,,,,12,newbie,patch,zkclient-problems,"Kafka broker re-registers itself in zookeeper every time handleNewSession() callback is invoked.

https://github.com/apache/kafka/blob/0.8.1/core/src/main/scala/kafka/server/KafkaHealthcheck.scala 

Now imagine the following sequence of events.
1) Zookeeper session reestablishes. handleNewSession() callback is queued by the zkClient, but not invoked yet.
2) Zookeeper session reestablishes again, queueing callback second time.
3) First callback is invoked, creating /broker/[id] ephemeral path.
4) Second callback is invoked and it tries to create /broker/[id] path using createEphemeralPathExpectConflictHandleZKBug() function. But the path is already exists, so createEphemeralPathExpectConflictHandleZKBug() is getting stuck in the infinite loop.

Seems like controller election code have the same issue.

I'am able to reproduce this issue on the 0.8.1 branch from github using the following configs.

# zookeeper
tickTime=10
dataDir=/tmp/zk/
clientPort=2101
maxClientCnxns=0

# kafka
broker.id=1
log.dir=/tmp/kafka
zookeeper.connect=localhost:2101

zookeeper.connection.timeout.ms=100
zookeeper.sessiontimeout.ms=100

Just start kafka and zookeeper and then pause zookeeper several times using Ctrl-Z.",,aauradkar,anigam,blokecom,"chris|",clarkhaskins,CurryGaifan,darion,eggsby,fpj,githubbot,guozhang,gwenshap,hakman,jeremypinkham,joestein,junrao,jwlent@gmail.com,jwlent55,kenmacd,kzadorozhny-tubemogul,marcusai,mazhar.shaikh.in,mgharat,mrlabbe,nehanarkhede,ryanwi,slon,twbecker,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1451,,,,,,,,,,,ZOOKEEPER-1740,,"24/Aug/15 16:06;fpj;KAFKA-1387.patch;https://issues.apache.org/jira/secure/attachment/12752026/KAFKA-1387.patch","30/Sep/14 16:27;jwlent55;kafka-1387.patch;https://issues.apache.org/jira/secure/attachment/12672056/kafka-1387.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,386101,,,Thu Sep 24 17:14:44 UTC 2015,,,,,,,,,,"0|i1uii7:",386366,,guozhang,,,,,,,,,,,,,,,,,,"11/Apr/14 15:45;guozhang;Hi Fedor, do you think this is caused by the same issue described in https://issues.apache.org/jira/browse/KAFKA-1382 ?;;;","13/Apr/14 14:14;slon;I think it's a different issue.;;;","13/Apr/14 19:54;guozhang;I think the main issue here is when there is a zookeeper session timeout, the zkClient will re-try write the data which could be already committed to ZK and failed. This issue is the same as the one causing KAFKA-1382. But I think their fixes would be different.;;;","05/Aug/14 04:40;joestein;Here is another way to reproduce this issue.  I have seen it a few times now with folks getting going with their clusters.

steps to reproduce.  install a 3 node zk ensemble with 3 brokers cluster

e.g. 

git clone https://github.com/stealthly/scala-kafka
git checkout -b zkbk3 origin/zkbk3
vagrant up provider=virtualbox

now setup each node in the cluster as you would broker 1,2,3 and the ensemble

e.g.

vagrant ssh zkbkOne
sudo su
cd /vagrant/vagrant/ && ./up.sh
vagrant ssh zkbkTwo
sudo su
cd /vagrant/vagrant/ && ./up.sh
vagrant ssh zkbkThree
sudo su
cd /vagrant/vagrant/ && ./up.sh

start up zookeeper on all 3 nodes
cd /opt/apache/kafka && bin/zookeeper-server-start.sh config/zookeeper.properties 1>>/tmp/zk.log 2>>/tmp/zk.log &

now, start up broker on node 2 only
cd /opt/apache/kafka && bin/kafka-server-start.sh config/server.properties 1>>/tmp/bk.log 2>>/tmp/bk.log &

ok, now here is where it gets wonky

on server 3 change from broker.id=3 to broker.id=2 
now you need to start up server 1 and 3 (even though it is broker.id=2) at the same time

cd /opt/apache/kafka && bin/kafka-server-start.sh config/server.properties 1>>/tmp/bk.log 2>>/tmp/bk.log &
cd /opt/apache/kafka && bin/kafka-server-start.sh config/server.properties 1>>/tmp/bk.log 2>>/tmp/bk.log &
( you can have two tabs, hit enter in one switch to other tab and hit enter is close enough to same time)

and you get this looping forever

2014-08-05 04:34:38,591] INFO I wrote this conflicted ephemeral node [{""version"":1,""brokerid"":2,""timestamp"":""1407212148186""}] at /controller a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
[2014-08-05 04:34:44,598] INFO conflict in /controller data: {""version"":1,""brokerid"":2,""timestamp"":""1407212148186""} stored data: {""version"":1,""brokerid"":2,""timestamp"":""1407211911014""} (kafka.utils.ZkUtils$)
[2014-08-05 04:34:44,601] INFO I wrote this conflicted ephemeral node [{""version"":1,""brokerid"":2,""timestamp"":""1407212148186""}] at /controller a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
[2014-08-05 04:34:50,610] INFO conflict in /controller data: {""version"":1,""brokerid"":2,""timestamp"":""1407212148186""} stored data: {""version"":1,""brokerid"":2,""timestamp"":""1407211911014""} (kafka.utils.ZkUtils$)
[2014-08-05 04:34:50,614] INFO I wrote this conflicted ephemeral node [{""version"":1,""brokerid"":2,""timestamp"":""1407212148186""}] at /controller a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
[2014-08-05 04:34:56,621] INFO conflict in /controller data: {""version"":1,""brokerid"":2,""timestamp"":""1407212148186""} stored data: {""version"":1,""brokerid"":2,""timestamp"":""1407211911014""} (kafka.utils.ZkUtils$)

the expected result that you get should be

[2014-08-05 04:07:20,917] INFO conflict in /brokers/ids/2 data: {""jmx_port"":-1,""timestamp"":""1407211640900"",""host"":""192.168.30.3"",""version"":1,""port"":9092} stored data: {""jmx_port"":-1,""timestamp"":""140721119
9464"",""host"":""192.168.30.2"",""version"":1,""port"":9092} (kafka.utils.ZkUtils$)
[2014-08-05 04:07:20,949] FATAL Fatal error during KafkaServerStable startup. Prepare to shutdown (kafka.server.KafkaServerStartable)
java.lang.RuntimeException: A broker is already registered on the path /brokers/ids/2. This probably indicates that you either have configured a brokerid that is already in use, or else you have shutdown 
this broker and restarted it faster than the zookeeper timeout so it appears to be re-registering.
        at kafka.utils.ZkUtils$.registerBrokerInZk(ZkUtils.scala:205)
        at kafka.server.KafkaHealthcheck.register(KafkaHealthcheck.scala:57)
        at kafka.server.KafkaHealthcheck.startup(KafkaHealthcheck.scala:44)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:103)
        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:34)
        at kafka.Kafka$.main(Kafka.scala:46)
        at kafka.Kafka.main(Kafka.scala)
[2014-08-05 04:07:20,952] INFO [Kafka Server 2], shutting down (kafka.server.KafkaServer)
[2014-08-05 04:07:20,954] INFO [Socket Server on Broker 2], Shutting down (kafka.network.SocketServer)
[2014-08-05 04:07:20,959] INFO [Socket Server on Broker 2], Shutdown completed (kafka.network.SocketServer)
[2014-08-05 04:07:20,960] INFO [Kafka Request Handler on Broker 2], shutting down (kafka.server.KafkaRequestHandlerPool)
[2014-08-05 04:07:20,992] INFO [Kafka Request Handler on Broker 2], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2014-08-05 04:07:21,263] INFO [Replica Manager on Broker 2]: Shut down (kafka.server.ReplicaManager)
[2014-08-05 04:07:21,263] INFO [ReplicaFetcherManager on broker 2] shutting down (kafka.server.ReplicaFetcherManager)

which is what you get if you just start server 3 on its one configured with broker.id=2

I originally saw this on a 12 node AWS cluster same kafka 0.8.1.1 zk 3.3.4

I don't know how very critical this is, someone just brought up something similar but with /brokers/ids/x I don't know if they are related but something wonky is going on with the ZkUtils.createEphemeralPathExpectConflictHandleZKBug code paths.


;;;","05/Aug/14 15:48;junrao;Joe,

The issue that you described is probably fixed in KAFKA-1451?;;;","06/Aug/14 01:08;joestein;[~junrao] I tested on trunk and it is much worse now.

instead of looping on the /controller node (like it was before) ... node 3 actually overwrote/stole the /brokers/ids/2 (doing a get before had it as 192.168.30.1 and after it is 192.168.30.3)

so now i have a situation where I have two running broker servers, each with the same broker id running (2), server 3 is the (""active"") broker with all the topics being created on it and failing requests for producing and consuming (because all the data is on server 2 but that is not advertised).... and server 2 is still the controller handling preferred leader election, etc.

what is weird is broker.id = 2 was running already.  I started up broker.id=1 and another broker.id=2 at the same time



;;;","06/Aug/14 23:44;gwenshap;Attempted to reproduce with trunk as well.

I'm not seeing the same behavior as [~joestein]. 
In my experiment the new broker 2 fails with the correct error message. The old broker 2, OTOH, goes into a loop, printing:
""[2014-08-06 16:37:01,884] INFO Partition [test1,0] on broker 2: Cached zkVersion [89] not equal to that in zookeeper, skip updating ISR (kafka.cluster.Partition)""

Not a good behavior either. ;;;","10/Aug/14 22:48;junrao;Hmm, this seems really weird. Not sure why starting two brokers at the same time will affect the ZK registration. Is this reproducible by running multiple brokers on the same machine?;;;","28/Sep/14 02:19;jwlent55;I have seen the issue reported in the original problem description in our QA environment (3 ZooKeeper, 3 Kafka and several application specific nodes) several times now.  I have not tested any configurations where 2 nodes try to claim the same broker id.  The problem is triggered when the system is under stress (high I/O and CPU load) and the ZooKeeper connections become unstable.  When this happens Kafka threads can get stuck trying to register Brokers nodes and Application threads get stuck trying to register Consumer nodes. One way to recover is to restart the impacted nodes.  As an experiment I aslo tried deleting the blocking ZooKeeper nodes (hours later when the system was under no stress).  When I did so the createEphemeralPathExpectConflictHandleZKBug would finally complete processing the current Expire message (i.e. add the node), break out of its loop, but, then immediately reenter the loop when it tired to process the next expire message.  The few times I tested this approach I had to delete the node dozens of times before the problem would clear itself - in other words there were dozens of Expire messages wating to be processed. Obvoisuly I am looking into this issue from a configuration point of view (avoid the unstable connection issue), but, this Kafka error behavior concerns me.

I have reproduced it (somewhat artificially) in a dev environment as follows:

1) Start one ZooKeeper and one Kafka node.
2) Set a thread breakpoint in KafkaHealthCheck.java. 
{noformat}
    def handleNewSession() {
      info(""re-registering broker info in ZK for broker "" + brokerId)
-->   register()
      info(""done re-registering broker"")
      info(""Subscribing to %s path to watch for new topics"".format(ZkUtils.BrokerTopicsPath))
    }
{noformat}
3) Pause Kafka.
4) Wait for ZooKeeper to expire the first session and drop the ephemeral node.
5) Unpause Kafka.
6) Kafka reconnects with ZooKeeper, receives an Expire, and establishes a second session.
7) Breakpoint hit and event thread paused before handling the first Expire.
8) Pause Kafka again.
9) Wait for ZooKeeper to expire the second session and delete the ephemeral node (again).
10) Remove breakpoint, unpause Kafka, and finally release the event thread.
11) Kafka reconnects with ZooKeeper, receives a second Expire, and establishes a third session.
12) Kafka registers an ephemeral triggered by the first expire (which triggerd the second session), but, ZooKeeper associates it with the third Session. 
13) Kafka tries to register an an ephemeral triggered by the second expire, but, ZooKeeper already has a stable node.
14) Kafka assumes this node will go away soon, sleeps, and then retries.
15) The node is associcated with a valid session and threfore does not go away so Kafka remains stuck in the retry loop.

I have tested this with the latest code in trunk and noted the same behavior (the code looks pretty similar).

I have coded up a potential 0.8.1.1 patch for this issue based on the following principles:

# Ensure that when the node starts stale nodes are removed in main
#* For Brokers this means remove nodes with the same host name and port otherwise fail to start (the existing checker logic)
#* For Consumer nodes don't worry about stale nodes - the way they are named should prevent this from ever happening.
# In main add the initial node which should now always work with no looping required - direct call to createEphemeralPath
# Create a EphemeralNodeMonitor class that contains:
#* IZkDataListener
#* IZkStateListener
# The users of this class provide a path to monitor and a closure that defines what to do when the node is not found
# When the state listener is notifed about a new session it checks to see if the node is already gone:
#* Yes, call the provided function
#* No, ignore the event
# When the data listener is notified of a deletion it does the same thing
# Both the Broker and Comsumer registation use this new class in the same way they curently use their individual state listeners.  There only change in behavior is to call createEphemeralPath directly (and avoid the looping code).

Since all this work should be done in the event thread I don't think there are any race conditions and no other nodes should be adding these nodes (or we have a serious configuration issue that should have been detected at startup).

One assumption is that we will always recieve at least one more event (expire and/or delete) after the node is really deleted by ZooKeeper.  I think that is a valid assumption (ZooKeeper can't send the delete until the node is gone).  If the node was present when when we process the final Expire message then we should get notified of a delete if that node was actually related to a previous session.  I wonder if we could get away with just monitoring node deletions, but, that seems risky.  The only change in behavior should be that if the expire is recieved before the node is actually deleted then event loop is not blocked and could process other messages while waiting for the delete event.

Note: I have not touched the leader election / contoller node code (the third user of the createEphemeralPathExpectConflictHandleZKBug code).  That still uses the looping code.  I did apply the KAFKA-1451 patch to our 0.8.1.1 build.

If there is any interest in the code I can provide a patch of what I have so far.  I would very much like to get feedback.  I was not sure of the protocol for submitting patches for comment.


;;;","28/Sep/14 23:23;junrao;James,

Thanks for reporting this. Yes, what you discovered is a real problem. The fix is going to be tricky though. The issue is the following. When a client lose an ephemeral node in ZK due to session expiration, that ephemeral node is not removed exactly at expiration time, but a short time after (ZOOKEEPER-1740). When the client tries to recreate the ephemeral node and get a NodeExistException, one of the two things could happen: (1) the existing node is from the expired session and is on its way to be deleted, (2) the node is actually created on the latest session (The reason is what you discovered:  the client gets multiple handleNewSession() calls due to multiple session expiration events, but the node is created on the latest session). I am not sure if there is an easy way to distinguish the two cases though.

Overall, it seems to me that there are so many corner cases that one has to deal with during ZK session expiration. The simplest approach is probably to prevent session expiration from happening at all (e.g., set a larger session timeout).;;;","28/Sep/14 23:41;gwenshap;AFAIK the ZK bug was never reproduced in newer versions of ZK. I'm wondering if at some point we can say that ZK 3.3 is no longer supported and remove the work-around (which is creating few issues of its own).
;;;","28/Sep/14 23:48;junrao;Gwen,

From ZOOKEEPER-1809, it seems the design of not deleting ephemeral node immediately on session expiration still exists on ZK 3.4.x and beyond?;;;","29/Sep/14 00:33;gwenshap;ZOOKEEPER-1809 was closed because the re-creation of the issue was buggy (the test app was actually creating two sessions at same time). 

I agree that Flavio indicated that ZNodes can hang around after expiration, but he also indicated the opposite in the email thread for ZOOKEEPER-1740.

Its important to get this right, so I'll do more research on the expected ZooKeeper behavior here.

One thing I'm not sure about is why does createEphemeralPathExpectConflictHandleZKBug loop indefinitely? 
If ZK indeed takes a bit of extra time to clean up, we can loop for specific amount of time (number of retries), like Curator typically does. After few seconds, the probability that the ZNode belongs to an active session and not an expired one is very high.;;;","29/Sep/14 13:11;jwlent55;As background we are using ZooKeeper 3.4.5.

When trying to come up with a fix for this I did consider limiting the loop to 2 to 3 tries.  My concerns with this approach were:

# Slow to recover if there are lots of Expire messages to process and each of these could trigger redundant rebalance events until you get to the last one.
# What happens if you don't loop quite long enough?  You are again stuck in a bad state when the ephemeral does go away.

I also considered trying to access the Session Id and storing that value instead of (or in addition to) the timestamp in the node's data.  That appraoch looked difficult to implement, error prone, and had the application doing what I would consider ZooKeeper work.

I agree there are a lot of corner cases to consider, but, I think we are going to pursue the approach I outlined above.  I would be happy to post the proposed solution for your review, but, again I am not sure about the protocol around patch submission.  I would not want this to be mistaken by someone as any kind of offical patch without a lot more review.

When working on this appraoch I looked at the curator PersistentEphemeralNode for ideas:

https://github.com/bazaarvoice/curator-extensions/blob/master/recipes/src/main/java/com/bazaarvoice/curator/recipes/PersistentEphemeralNode.java

This is curator based so does not directly apply to Kafka (yet), but, it also keys off nodeDelete to restore the node.

In the end I went with the simple idea that:

""If when we process an Expire event the node still exists then ZooKeeper will inform us if that node later goes away.""

If we can't trust ZooKeeper/ZkClient to do that then ...

{noformat}
  class StateListener() extends IZkStateListener {

    def handleStateChanged(state: KeeperState) {}

    def handleNewSession() {
      if (zkClient.exists(path)) {
        info(""New session started, but, ephemeral %s already/still exists"".format(path))
      }
      else {
        info(""New session started, recreate ephemeral node %s"".format(path))
        recreateNode()
      }
    }
  }
{noformat};;;","29/Sep/14 13:20;jwlent55;In case anyone is interested in the complete code for the new class I am testing with:

{noformat}
class EphemeralNodeMonitor(zkClient: ZkClient, path: String, recreateNode: () => Unit) extends Logging {

  val dataListener = new DataListener()
  val stateListener = new StateListener()
  
  def start() {
    zkClient.subscribeStateChanges(stateListener)
    zkClient.subscribeDataChanges(path, dataListener)
  }
  
  def close() {
    zkClient.unsubscribeStateChanges(stateListener)
    zkClient.unsubscribeDataChanges(path, dataListener)
  }

  class DataListener() extends IZkDataListener {
    
    var oldData: scala.Any = null

    def handleDataChange(dataPath: String, newData: scala.Any) {
      if (newData != oldData) {
        oldData = newData
        info(""Ephemeral node %s has new data [%s]"".format(dataPath, newData))
      }
    }

    def handleDataDeleted(dataPath: String) {
      if (zkClient.exists(path)) {
        info(""Ephemeral node %s was deleted, but, has already been recreated"".format(dataPath))
      }
      else {
        info(""Ephemeral node %s was deleted, recreate it"".format(dataPath))
        recreateNode()
      }
    }
  }

  class StateListener() extends IZkStateListener {

    def handleStateChanged(state: KeeperState) {}

    def handleNewSession() {
      if (zkClient.exists(path)) {
        info(""New session started, but, ephemeral %s already/still exists"".format(path))
      }
      else {
        info(""New session started, recreate ephemeral node %s"".format(path))
        recreateNode()
      }
    }
  }
{noformat};;;","29/Sep/14 16:35;junrao;James,

Contributing code to Kafka is pretty simple. You just need to attach a patch to the jira.

As for your solution, we probably need to verify the following: will a watcher fire if it's registered on a path created by an already expired session and the path will be deleted soon.;;;","29/Sep/14 21:37;jwlent55;I aplogize in advance for my ignorance, but, I have one newbie question.  My starting point is the 0.8.1.1 tag (really the 0.8.1.1 source distribution).  Would it be OK for me to submit a patch against that baseline or would it be better for me to first merge the code to trunk and then create the patch?;;;","29/Sep/14 22:43;jwlent55;As for your question (which I agree is one of the key questions) I have the following comments:

* The ZooKeeper documentation states there is one case where a watch may be missed which I do not think applies to the situation I am trying to address:

""Watches are maintained locally at the ZooKeeper server to which the client is connected. This allows watches to be lightweight to set, maintain, and dispatch. When a client connects to a new server, the watch will be triggered for any session events. Watches will not be received while disconnected from a server. When a client reconnects, any previously registered watches will be reregistered and triggered if needed. In general this all occurs transparently. There is one case where a watch may be missed: a watch for the existence of a znode not yet created will be missed if the znode is created and deleted while disconnected.""

* In my testing the node is normally gone by the time the New Session event is handled which recreates the node. In that case I do not see a Delete message (I log that arrival of a delete event even if the node is already gone):

{noformat}
[2014-09-29 18:23:43,071] INFO zookeeper state changed (Expired) (org.I0Itec.zkclient.ZkClient)
[2014-09-29 18:23:43,071] INFO Unable to reconnect to ZooKeeper service, session 0x148c36a0a94000f has expired, closing socket connection (org.apache.zookeeper.ClientCnxn)
[2014-09-29 18:23:43,071] INFO Initiating client connection, connectString=localhost:2181/kafka/0.8 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@56404645 (org.apache.zookeeper.ZooKeeper)
[2014-09-29 18:23:43,072] INFO Opening socket connection to server localhost/127.0.0.1:2181 (org.apache.zookeeper.ClientCnxn)
[2014-09-29 18:23:43,073] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2014-09-29 18:23:43,074] INFO EventThread shut down (org.apache.zookeeper.ClientCnxn)
[2014-09-29 18:23:43,082] INFO Closing socket connection to /10.210.10.165. (kafka.network.Processor)
[2014-09-29 18:23:43,087] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x148c36a0a940010, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2014-09-29 18:23:43,087] INFO zookeeper state changed (SyncConnected) (org.I0Itec.zkclient.ZkClient)
[2014-09-29 18:23:43,099] INFO 0 successfully elected as leader (kafka.server.ZookeeperLeaderElector)
[2014-09-29 18:23:43,143] INFO New session started, recreate ephemeral node /brokers/ids/0 (kafka.utils.EphemeralNodeMonitor)
[2014-09-29 18:23:43,144] INFO Start registering broker 0 in ZooKeeper (kafka.server.KafkaHealthcheck)
[2014-09-29 18:23:43,161] INFO Registered broker 0 at path /brokers/ids/0 with address jlent.digitalsmiths.com:9092. (kafka.utils.ZkUtils$)
[2014-09-29 18:23:43,218] INFO Ephemeral node /brokers/ids/0 has new data [{""jmx_port"":10001,""timestamp"":""1412029423148"",""host"":""jlent.digitalsmiths.com"",""version"":1,""port"":9092}] (kafka.utils.EphemeralNodeMonitor)
[2014-09-29 18:23:43,237] INFO New leader is 0 (kafka.server.ZookeeperLeaderElector$LeaderChangeListener)
{noformat}

* I have seen cases where the node is still present when the New Session is handled and in that case I do see a Delete event a short while later.  I don't have the logs that document that (don't ask me why I don't have logs to document the most important scenario).  I will try to recreate that situation.
* As an alternative I modified the New Session handling code to do nothing (except log the arrival of the new session event).  In that case I do see the Delete event.  This could perhaps be viewed a more severe test.  In this case we get notified of a Delete that actually occured before we even handled the New Seesion event.  That was actually how I did some of my original testing.

{noformat}
[2014-09-29 18:14:31,414] INFO zookeeper state changed (Expired) (org.I0Itec.zkclient.ZkClient)
[2014-09-29 18:14:31,414] INFO Unable to reconnect to ZooKeeper service, session 0x148c36a0a94000c has expired, closing socket connection (org.apache.zookeeper.ClientCnxn)
[2014-09-29 18:14:31,414] INFO Initiating client connection, connectString=localhost:2181/kafka/0.8 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@15c58840 (org.apache.zookeeper.ZooKeeper)
[2014-09-29 18:14:31,414] INFO Opening socket connection to server localhost/127.0.0.1:2181 (org.apache.zookeeper.ClientCnxn)
[2014-09-29 18:14:31,415] INFO EventThread shut down (org.apache.zookeeper.ClientCnxn)
[2014-09-29 18:14:31,415] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2014-09-29 18:14:31,420] INFO Closing socket connection to /10.210.10.165. (kafka.network.Processor)
[2014-09-29 18:14:31,459] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x148c36a0a94000d, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2014-09-29 18:14:31,459] INFO zookeeper state changed (SyncConnected) (org.I0Itec.zkclient.ZkClient)
[2014-09-29 18:14:31,587] INFO 0 successfully elected as leader (kafka.server.ZookeeperLeaderElector)
[2014-09-29 18:14:31,683] INFO New session started, DO NOT recreate ephemeral node /brokers/ids/0 (kafka.utils.EphemeralNodeMonitor)
[2014-09-29 18:14:31,701] INFO Ephemeral node /brokers/ids/0 was deleted, recreate it (kafka.utils.EphemeralNodeMonitor)
[2014-09-29 18:14:31,702] INFO Start registering broker 0 in ZooKeeper (kafka.server.KafkaHealthcheck)
[2014-09-29 18:14:31,722] INFO Registered broker 0 at path /brokers/ids/0 with address jlent.digitalsmiths.com:9092. (kafka.utils.ZkUtils$)
[2014-09-29 18:14:31,744] INFO New leader is 0 (kafka.server.ZookeeperLeaderElector$LeaderChangeListener)
[2014-09-29 18:14:31,746] INFO Ephemeral node /brokers/ids/0 has new data [{""jmx_port"":10001,""timestamp"":""1412028871704"",""host"":""jlent.digitalsmiths.com"",""version"":1,""port"":9092}] (kafka.utils.EphemeralNodeMonitor)
{noformat}
;;;","30/Sep/14 16:16;jwlent55;Here is what I have so far.  Comments welcomed.;;;","30/Sep/14 16:41;jwlent55;I have messed things up.  I tried to use the Submit Patch option.  I filled out the fields in the form, but, it never asked me for a file.  I also specifed labels that I assumed were related to the patch, but, instead are associated with the issue itself.  I then directly attached the file to the issue.  That seemed to go OK.  Now the Submit Patch option is gone and the Status is Patch Available.  I don't think that is correct.  I decided it is best if I stop messing with the issue for now.  I have done enough damage.

I apologize for my ignorance of the process.;;;","02/Oct/14 14:55;junrao;James,

For my question, could you ask the ZK mailing list and get your understanding confirmed by their developers? Thanks,;;;","02/Oct/14 16:53;jwlent55;Good idea and done:

http://mail-archives.apache.org/mod_mbox/zookeeper-user/201410.mbox/browser;;;","16/Feb/15 20:11;twbecker;Can a project member comment on what it is going to take to get this patch accepted?  We have been running 0.8.1 with it for months, and I guess we'll have to apply it to 0.8.2 as well, but it would be nice to get it into the official tree...;;;","28/Apr/15 01:07;eggsby;I am seeing similar behavior in my consumer, using kafka 0.8.2.1 and zookeeper 3.4.6

In an infinite loop:

{code}
15/04/27 17:44:31 INFO utils.ZkUtils$: conflict in /consumers/******************
15/04/27 17:44:31 INFO utils.ZkUtils$: I wrote this conflicted ephemeral node ************** a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry
15/04/27 17:45:01 INFO INFO utils.ZkUtils$: conflict in /consumers/******************
15/04/27 17:45:01 INFO utils.ZkUtils$: I wrote this conflicted ephemeral node ************** a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry
{code};;;","29/Apr/15 11:57;marcusai;I've also encountered this issue running Kafka 0.8.2.0 and Zookeeper 3.4.6 in a three node cluster. The error occured after two zookeeper nodes got restarted at the same time. The error below repeatedly appeared in the Kafka logs. I resolved the issue by restarting Kafka.

{panel}
[2015-04-27 03:47:03,292] INFO I wrote this conflicted ephemeral node [""jmx_port"":-1,""timestamp"":""1430038275477"",""host"":""ams5mdppdmsbacmq01b.markit.partners"",""version"":1,""port"":9092] at /brokers/ids/2 a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
{panel}
;;;","30/Apr/15 05:02;eggsby;It looks like this ""infinite retry"" behavior is only in kafka to accomodate another strange issue where zookeeper was deleting ephemeral nodes out from under it:

https://github.com/apache/kafka/blob/0.8.2.1/core/src/main/scala/kafka/utils/ZkUtils.scala#L272
https://issues.apache.org/jira/browse/ZOOKEEPER-1740

It seems the simplest thing to do would be to just delete the conflicted node and write the truth about the process environment it knows.

I see that my issue appeared in the consumer code, where this issue is occurring in the kafka brokers themselves, but the bug appears to be the same:

There are two exceptional cases in ephemeral nodes that I can see, either the ZOOKEEPER-1740 bug was hit in which case our ephemeral node mysteriously was lost out from under us, but our session is still active and we can just create a new one. The other bug I believe we are seeing is that the session is long gone but the ephemeral node is still hanging around until the consumer process exits.

Currently the first case is handled, but I the second case is not.;;;","04/May/15 17:58;nehanarkhede;When this happens, there isn't a way to get out of this without killing the broker. Marking it as a blocker.;;;","07/May/15 21:17;anigam;I have seen the ephemeral node issue before and the fix made there was exactly what Thomas mentioned:
""It seems the simplest thing to do would be to just delete the conflicted node and write the truth about the process environment it knows.""

Is there a reason why the approach outlined by Thomas does not work for kafka?;;;","30/Jul/15 21:54;clarkhaskins;This patch is listed as a blocker. Can the existing patch be committed? Is anyone actively working on it? 

This has been a problem for us recently and we would like to see this fixed soon.

Thanks,
-Clark;;;","11/Aug/15 18:39;mgharat;Can the person who uploaded the patch submit a testcase on how to reproduce this? 
We are hitting this in production but are not able to reproduce this locally.

;;;","11/Aug/15 18:43;slon;Have you tried steps from issue description?;;;","11/Aug/15 20:19;guozhang;[~fpj] Could you help taking a look at this issue?;;;","12/Aug/15 00:07;jwlent@gmail.com;It has been a while since I investigated this issue. I will take another look at it tomorrow and get back to you. 

Sent from my iPhone

;;;","12/Aug/15 19:32;jwlent55;After refreshing my memory of this issue I was unable to come up with any new ideas for how to create an automated test case for the issue.  I was only able to reproduce this issue in my dev environment using the cumbersome manual process I outlined in my Sept 27 comment.

My question posted to the zookeeper-user mailing list regarding the validity of the key assumption of the patch logic generated no feedback.

We have been using the patch I provided with Kafka 0.8.1.1 for almost a year now.  We have not seen a re-occurrence of the hung ephemeral connection issue since then.  Since the problem was intermittent and only triggered when the system was unstable, this may or may not be due to the presence of the patch.

There was one an NPE issue found during test in March when our application code changed and in certain cases tried to close a Connector that had never been fully started.  That was fixed as follows:

{noformat}
Index: core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala
===================================================================
--- core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala	(revision 73668)
+++ core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala	(revision 73669)
@@ -162,7 +162,9 @@
       if (canShutdown) {
         info(""ZKConsumerConnector shutting down"")
 
-        consumerNodeMonitor.close()
+        if (consumerNodeMonitor != null) {
+          consumerNodeMonitor.close()
+        }
         
         if (wildcardTopicWatcher != null)
           wildcardTopicWatcher.shutdown()
{noformat}

Not sure any of this was of much help, but, I would be happy to try to answer any questions regarding the patch logic and/or update it based on your comments.;;;","14/Aug/15 13:34;fpj;I'm actually really sorry that this issue has been around for so long, I didn't realize it was going on and that I was even indirectly participating in it. Let me start by giving a sort of general overview of what to expect.

If a client has received a session expiration event, it means that the leader has expired the session and has broadcast the closeSession event to the followers. If the same client creates a new session successfully, then the server it connects to must have applied the previous closeSession, which deletes the ephemeral znodes, because ZK guarantees that txns are totally ordered. Consequently, the client shouldn't observe an ephemeral from an old session of its own. Note that another client could still observe the ephemeral znode after the session expiration if it is connected to a server that is a bit behind, but that's fine.

What I'm thinking is that one problem that could happen is that a client creates a new session before receiving the session expiration for an earlier session. In that case the ephemerals will still be there because the session still exists.

The bottom line is that if the client has seen the session expiration event, then it seems fine to go ahead and create new ephemerals without having to check whether ephemerals are stale or not. If the session creation isn't clean, then there are a few options like waiting for the timeout period, storing and recovering the session id.

I'll dig into the code to see how we can fix this, have a closer look at the patch, and will reopen the associated ZOOKEEPER-1740 issue until we sort this out. let me know if the explanation above makes sense in the meanwhile. ;;;","14/Aug/15 17:32;anigam;Thanks a lot for digging into this. Not sure if it helps but in the past
when I saw this issue it went like this:
a) Say session time out is 30 seconds.
b) If we kill the instance which create the zookeeper ephemeral node and
bring it back up quickly (less than 30 seconds) we would find the previous
session data (ephemeral node) still exists.

The solution was to assume the existing data was from an old session,
delete and re-create it during startup. However, we were processing the
zookeeper events on a single thread.

On Fri, Aug 14, 2015 at 6:34 AM, Flavio Junqueira (JIRA) <jira@apache.org>

;;;","14/Aug/15 22:50;guozhang;Thanks [~fpj], this is very helpful.

Just to add some more context regarding this issue, we have seen issues when ephemeral nodes were not deleted when brokers / consumers try to re-register themselves in ZK upon a session timeout event (details can be found in KAFKA-992). We tried to fix it via adding a registration timestamp into the registration node's data, and checking if the timestamp is different upon seeing it, and if not backing off to wait for this node to be removed.

However people have been also reporting a couple of times that the backing-off is never ending, i.e. the node has a different timestamp, but was never deleted. The suspicion was that there were multiple consequent session creation at a very short period of time, and the node with a different timestamp is created by a session that was not actually expired, and hence will never be gone. But no one has validated if this is the case though.

The logic of re-registration can be found in ZookeeperConsumerConnector.scala and KafkaHealthcheck.scala.;;;","17/Aug/15 13:24;fpj;There are two problems at a high level described here: zk losing ephemerals and ephemerals not going away. I haven't been able to reproduce the former, but I've been able to find one potential problem that could be causing it.

I started by finding suspicious that the ZK listeners were not dealing with session events at all:

{code}
def handleStateChanged(state: KeeperState) {
      // do nothing, since zkclient will do reconnect for us.
}
{code}

 It is quite typical with ZK that you wait for the connected event before making progress. Looking at the ZkClient implementation, I realized that it retries operations in the case of connection loss or session expiration until they go through. There is a race here, though. Say you submit a create, but instead of getting OK as a response, you get connection loss. ZkClient in this case will say ""well, need to retry"" and will get a node exists exception, which the code currently treats as a znode from a previous session. This znode will never go away because it belongs to the current session!

Now let's say we get rid of such corner cases. It is still possible that when the client recovers it finds a znode from a previous session. It can happen because the lease (session) corresponding to the znode is still valid, so ZK can't get rid of it. Revoking leases in general is a bit complicated, but it sounds ok in this case if there is no risky of having multiple incarnations of the same element (a broker) running concurrently.;;;","17/Aug/15 19:47;guozhang;I thought that when the previous session has ended (e.g. expired), its ephemeral node will be ""eventually"" removed? Does ZooKeeper itself have a leasing mechanism?;;;","17/Aug/15 20:44;fpj;bq. I thought that when the previous session has ended (e.g. expired), its ephemeral node will be ""eventually"" removed?

If the session ends cleanly, by the client submitting a closeSession request, then the session closes and the ephemerals are deleted with the request. But, if the client crashes and the server simply stops hearing from the client, then the session has to time out and expire so it takes some time.

bq. Does ZooKeeper itself have a leasing mechanism?

I'm referring to the fact that the ephemeral represents a lease that is revoked when the session times out.

I'm not sure if this is clear, but one of the problems I'm pointing out is that zkclient might end up creating the ephemeral znode in your *current* session. In this case, the znode won't go away. Here is actually another problem I found along the same lines. The createEphemeral call in ZkClient ends up calling retryUntilConnected, which retries even when the session expires:

{code}
            try {
                return callable.call();
            } catch (ConnectionLossException e) {
                // we give the event thread some time to update the status to 'Disconnected'
                Thread.yield();
                waitForRetry();
            } catch (SessionExpiredException e) {
                // we give the event thread some time to update the status to 'Expired'
                Thread.yield();
                waitForRetry();
            }
{code}

In this case, say that one call to createEphemeral via handleNewSession happens during a given session, but the session expires before the operation goes through. The client will retry with the new session. When the consumer tries again, it will fail because the znode is there and won't go away. This is another case in which the znode won't go away because it has been created in the current session.;;;","18/Aug/15 00:03;guozhang;[~fpj] That makes sense. So it seems the right resolution should be at the ZkClient layer, not on Kafka's layer?;;;","18/Aug/15 15:19;fpj;It doesn't look like it 'd be a small change to zkclient to fix this. We essentially need it to expose zk events as they occur. In the way it currently does it, the events are serialized and the operations are retried transparently so I don't know if the znode already exists because of a connection loss or if the session actually expired and there is a new one now. 

The simplest way around this seems to be to just re-register the consumer directly (delete and create) upon a node exists exception. This should work because of the following argument.

There are three possibilities when we get a node exists exception:

# The znode exists from a previous session and hasn't been reclaimed yet
# The znode exists because of a connection loss event while the znode was being created, so the second time we get an exception (event)
# The previous session has expired, a new one was created, and the registration was occurring around this transition, so when we execute handleNewSession for the new session, we get a node exists exception. 

In all these three cases, deleting and recreating seems fine. It is clearly conservative and more expensive than necessary, but at least it doesn't require changes to zkclient. Does it sound a reasonable? Do you see any problem? 

CC [~guozhang] [~jwlent@gmail.com];;;","18/Aug/15 16:16;guozhang;Thanks [~fpj], that makes sense to me. [~jwlent55] do you want to submit a new patch following this approach?;;;","18/Aug/15 16:51;fpj;[~guozhang] it looks like [~jwlent@gmail.com] isn't in the list of contributors, could you add him so that we can assign the jira to him?;;;","19/Aug/15 13:16;jwlent55;Your approach sounds much simpler than mine (which I like).  Similar to what I proposed doing only at startup (ensureNodeDoesNotExist method).  I am however not sure I understand the exact change you propose.  As I remember the createEphemeralPathExpectConflictHandleZKBug is called by three code paths:

- Register Broker
- Register Consumer
- Leadership election  

In my change I specifically tried avoid changing the Leadership election logic.

Is your change basically to implement your new logic (delete if already exists) instead of calling createEphemeralPathExpectConflictHandleZKBug in the first two cases?  If so I agree it sounds reasonable.  I suppose in a misconfiguration case two nodes might get into a registration war over the Broker node, but, that could (perhaps) be handled at startup (second one fails to start up).

If your propose replacing the createEphemeralPathExpectConflictHandleZKBug for the Leadership election case too then I am less comfortable making (and testing) that change.  I have never really dug into that logic too much.

One other factor to consider is that I am a bit backed up a work right now and this will not be issue will not be my highest priority.
;;;","19/Aug/15 18:38;guozhang;[~jwlent55] I agree that this fix may be just for broker / consumer registration, i.e. ZK should not be used to detect mis-configuration that two brokers / clients use the same Id. Hence for that case, in the new approach they may end-up in a delete-and-write war. We should consider fixing such mis-operation in a different manner which is orthogonal to this JIRA. For leader election, one should not simply delete the path upon conflict, we should leave it as is. In the future, we should either fix the root cause in ZkClient or move on to use a different client as KIP-30 is current discussing about.

If you do not have time this week and feel it is OK, [~fpj] could you help taking it over?;;;","19/Aug/15 21:22;fpj;I'm indeed proposing to get rid of createEphemeralPathExpectConflictHandleZKBug. I can investigate the impact to leadership election.;;;","24/Aug/15 16:06;fpj;Given that it isn't clear that we will be getting curator as a dependency, I started a fix that pretty much relies on the ZK handle that ZkClient creates. Here is a preliminary patch that fixes the issues we have been discussing for the consumer registration by simply not retrying the creation of the registration znode across sessions. Given that I'm not using the ZkClient API, there is a bit of wiring to be done, but I hope it is ok.;;;","25/Aug/15 19:37;guozhang;Thanks [~fpj], thanks for the patch. Here are some high-level comments:

1. Will the mixing usage of ZK directly and ZkClient together violate ordering? AFAIK ZkClient orders all events fired by watchers and hand them to the user callbacks one-by-one, if we use ZK's Watcher directly will its callback be called out-of-order with other events?

2. If we get a Code.OK in CreateCallback, do we still need to trigger a ZooKeeper.exist with ExistsCallback again?

3. For the consumer / server registration case particularly, we tries to handle parent path creation in ZkUtils.makeSurePersistentPathExists, so I feel we should expose the problem that parent path does not exist yet instead trying to hide it in createRecursive.;;;","29/Aug/15 15:17;githubbot;GitHub user fpj opened a pull request:

    https://github.com/apache/kafka/pull/178

    KAFKA-1387: Kafka getting stuck creating ephemeral node it has already created when two zookeeper sessions are established in a very short period of time

    This is a patch to get around the problem discussed in the KAFKA-1387 jira. The tests are not passing in my box when I run them all, but they do pass when I run them individually, which indicates that there is something leaking from a test to the next. I still need to work this out and also work on further testing this. I wanted to open this PR now so that it can start getting reviewed.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/fpj/kafka 1387

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/178.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #178
    
----
commit f8be8657e649d0490e9ed1f1ef52234b3c31435e
Author: flavio junqueira <fpj@apache.org>
Date:   2015-08-23T13:55:11Z

    KAFKA-1387: First cut, node dependency on curator

commit b8f901b6478d4ac9c961e899d702e6fc11cfee07
Author: flavio junqueira <fpj@apache.org>
Date:   2015-08-23T13:55:11Z

    KAFKA-1387: First cut, node dependency on curator

commit 2369e66921f88b2ee1b24ddeff2bf2d050015447
Author: flavio junqueira <fpj@apache.org>
Date:   2015-08-23T14:07:41Z

    Merge branch '1387' of https://github.com/fpj/kafka into 1387

commit f03c301d5d919d9c05c6837de508b4f383906fdb
Author: flavio junqueira <fpj@apache.org>
Date:   2015-08-23T13:55:11Z

    KAFKA-1387: First cut, node dependency on curator

commit d8eab9e0f569eaaecb4afda4d486d00600ad1e6f
Author: flavio junqueira <fpj@apache.org>
Date:   2015-08-24T14:56:01Z

    KAFKA-1387: Some polishing

commit b7cbe5dbecbc28a564b99209114f39db785c73dd
Author: flavio junqueira <fpj@apache.org>
Date:   2015-08-24T15:50:58Z

    KAFKA-1387: Style fixes

commit 336f67c641c44b73ac1dbb66cdde4ff97f2fcd9a
Author: flavio junqueira <fpj@apache.org>
Date:   2015-08-24T15:53:18Z

    KAFKA-1387: More style fixes

commit 201ab2dcc33ba10a19c51f7452ce40497d3fcf83
Author: flavio junqueira <fpj@apache.org>
Date:   2015-08-24T15:59:32Z

    Merge branch '1387' of https://github.com/fpj/kafka into 1387

commit 9961665230e04331f7767d8aa8aaac0a14f46cd8
Author: flavio junqueira <fpj@apache.org>
Date:   2015-08-23T13:55:11Z

    KAFKA-1387: First cut, node dependency on curator

commit b52c12422f7a831137d8659f14779eaad1972217
Author: flavio junqueira <fpj@apache.org>
Date:   2015-08-24T14:56:01Z

    KAFKA-1387: Some polishing

commit b2400a0a37555250d50b1f1abfdda2c4d00b03ac
Author: flavio junqueira <fpj@apache.org>
Date:   2015-08-24T15:50:58Z

    KAFKA-1387: Style fixes

commit 888f6e0cf17d6a3a8d6b8dd46f8099731ba36511
Author: flavio junqueira <fpj@apache.org>
Date:   2015-08-24T15:53:18Z

    KAFKA-1387: More style fixes

commit d675b024b0e8627c4c2c9c113c07527851e81f7a
Author: flavio junqueira <fpj@apache.org>
Date:   2015-08-29T15:00:07Z

    KAFKA-1387

commit 4c83ac2609ed29a0f1887bf5087dab50e3e93488
Author: flavio junqueira <fpj@apache.org>
Date:   2015-08-29T15:07:23Z

    KAFKA-1387: Removing whitespaces.

commit 240b51a77715c53db784d5932702318ff28468c2
Author: flavio junqueira <fpj@apache.org>
Date:   2015-08-29T15:11:30Z

    Merge branch '1387' of https://github.com/fpj/kafka into 1387

----
;;;","22/Sep/15 21:09;fpj;hey [~guozhang]

bq. Will the mixing usage of ZK directly and ZkClient together violate ordering? AFAIK ZkClient orders all events fired by watchers and hand them to the user callbacks one-by-one, if we use ZK's Watcher directly will its callback be called out-of-order with other events?

ZkClient indeed handles the processing to a separate thread. To avoid blocking the dispatcher thread, it uses a separate thread to deliver events. This can be a problem if the events here and events handled directly by ZkClient are correlated. I tried to confine the ZK processing for this feature in the same class to avoid ordering issues. I don't see a problem concretely, but if you do, let me know. Right now it sounds like you're just speculating that it could be a problem, yes?

bq. If we get a Code.OK in CreateCallback, do we still need to trigger a ZooKeeper.exist with ExistsCallback again?

Right, that exists call is to set a watch.

bq. For the consumer / server registration case particularly, we tries to handle parent path creation in ZkUtils.makeSurePersistentPathExists, so I feel we should expose the problem that parent path does not exist yet instead trying to hide it in createRecursive.

I've commented on the PR about this. What's your specific concern here? If you could elaborate a bit more, I'd appreciate.  ;;;","24/Sep/15 17:14;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/178
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mirrormaker hangs during shutdown if no topic is consumed,KAFKA-1385,12707739,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,junrao,junrao,10/Apr/14 16:34,05/Oct/14 03:17,14/Jul/23 05:39,05/Oct/14 03:17,0.8.2.0,,,,,,,,,,core,,,0,,,,"Couldn't do clean shutdown when running the following command.

bin/kafka-run-class.sh kafka.tools.MirrorMaker --producer.config config/producer.properties --consumer.config config/consumer.properties --blacklist="".*""

Saw the following stacktrace.

""Thread-6"" prio=5 tid=7f94120af800 nid=0x113e16000 waiting on condition [113e15000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <7ec0eb870> (a java.util.concurrent.CountDownLatch$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)
        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:207)
        at kafka.tools.MirrorMaker$MirrorMakerThread.awaitShutdown(MirrorMaker.scala:216)
        at kafka.tools.MirrorMaker$$anonfun$cleanShutdown$2.apply(MirrorMaker.scala:167)
        at kafka.tools.MirrorMaker$$anonfun$cleanShutdown$2.apply(MirrorMaker.scala:167)
        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
        at scala.collection.immutable.List.foreach(List.scala:45)
        at kafka.tools.MirrorMaker$.cleanShutdown(MirrorMaker.scala:167)
        at kafka.tools.MirrorMaker$$anon$2.run(MirrorMaker.scala:144)


""mirrormaker-0"" prio=5 tid=7f9414a90800 nid=0x113804000 waiting on condition [113803000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <7ec0b0298> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
        at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:63)
        at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:33)
        at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:66)
        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:58)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:32)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
        at kafka.consumer.KafkaStream.foreach(KafkaStream.scala:25)
        at kafka.tools.MirrorMaker$MirrorMakerThread.run(MirrorMaker.scala:190)
",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,386062,,,Sun Oct 05 03:17:02 UTC 2014,,,,,,,,,,"0|i1ui9j:",386326,,,,,,,,,,,,,,,,,,,,"05/Oct/14 03:17;nehanarkhede;Fixed by patch for KAFKA-1650
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient unit test failure in SocketServerTest,KAFKA-1383,12707620,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,10/Apr/14 01:55,28/Apr/14 22:03,14/Jul/23 05:39,28/Apr/14 22:03,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"Saw the following transient unit test failure.

kafka.network.SocketServerTest > testNullResponse FAILED
    java.lang.AssertionError: null
        at org.junit.Assert.fail(Assert.java:69)
        at org.junit.Assert.assertTrue(Assert.java:32)
        at org.junit.Assert.assertFalse(Assert.java:51)
        at org.junit.Assert.assertFalse(Assert.java:60)
        at kafka.network.SocketServerTest.testNullResponse(SocketServerTest.scala:117)
",,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1422,,,,,,,,,,,,,,,,,,,,,"28/Apr/14 17:44;junrao;KAFKA-1383.patch;https://issues.apache.org/jira/secure/attachment/12642287/KAFKA-1383.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385943,,,Mon Apr 28 22:03:56 UTC 2014,,,,,,,,,,"0|i1uhjb:",386207,,,,,,,,,,,,,,,,,,,,"22/Apr/14 21:23;guozhang;[~junrao] Is this fixed by KAFKA-1400 or a different issue?;;;","22/Apr/14 21:28;junrao;This seems to be a different issue. Not sure what the cause is.;;;","28/Apr/14 17:43;junrao;The problem is the following. The interestOps is updated by the processor thread after the request is put in the request queue. There is no guarantee that after a request is dequeued, the read bit in interestOps is turned off. So need to wait a bit.;;;","28/Apr/14 17:44;junrao;Created reviewboard https://reviews.apache.org/r/20787/
 against branch origin/trunk;;;","28/Apr/14 22:03;junrao;Thanks for the reviews. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update zkVersion on partition state update failures,KAFKA-1382,12707613,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriharsha,jjkoshy,jjkoshy,10/Apr/14 01:00,27/Apr/16 23:25,14/Jul/23 05:39,10/Jun/14 21:56,,,,0.8.1.2,0.8.2.0,,,,,,,,,0,,,,"Our updateIsr code is currently:

  private def updateIsr(newIsr: Set[Replica]) {
    debug(""Updated ISR for partition [%s,%d] to %s"".format(topic, partitionId, newIsr.mkString("","")))
    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.map(r => r.brokerId).toList, zkVersion)
    // use the epoch of the controller that made the leadership decision, instead of the current controller epoch
    val (updateSucceeded, newVersion) = ZkUtils.conditionalUpdatePersistentPath(zkClient,
      ZkUtils.getTopicPartitionLeaderAndIsrPath(topic, partitionId),
      ZkUtils.leaderAndIsrZkData(newLeaderAndIsr, controllerEpoch), zkVersion)
    if (updateSucceeded){
      inSyncReplicas = newIsr
      zkVersion = newVersion
      trace(""ISR updated to [%s] and zkVersion updated to [%d]"".format(newIsr.mkString("",""), zkVersion))
    } else {
      info(""Cached zkVersion [%d] not equal to that in zookeeper, skip updating ISR"".format(zkVersion))
    }

We encountered an interesting scenario recently when a large producer fully
saturated the broker's NIC for over an hour. The large volume of data led to
a number of ISR shrinks (and subsequent expands). The NIC saturation
affected the zookeeper client heartbeats and led to a session timeout. The
timeline was roughly as follows:

- Attempt to expand ISR
- Expansion written to zookeeper (confirmed in zookeeper transaction logs)
- Session timeout after around 13 seconds (the configured timeout is 20
  seconds) so that lines up.
- zkclient reconnects to zookeeper (with the same session ID) and retries
  the write - but uses the old zkVersion. This fails because the zkVersion
  has already been updated (above).
- The ISR expand keeps failing after that and the only way to get out of it
  is to bounce the broker.

In the above code, if the zkVersion is different we should probably update
the cached version and even retry the expansion until it succeeds.

",,DEvil0000,donnchadh,dude9527,fpj,guozhang,jblackburn,jhooda,jjkoshy,junrao,KaneK,mazhar.shaikh.in,mck,nehanarkhede,nyetter,sriharsha,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-3042,,,,,,,,,,"26/May/14 05:08;sriharsha;KAFKA-1382.patch;https://issues.apache.org/jira/secure/attachment/12646741/KAFKA-1382.patch","31/May/14 04:19;sriharsha;KAFKA-1382_2014-05-30_21:19:21.patch;https://issues.apache.org/jira/secure/attachment/12647750/KAFKA-1382_2014-05-30_21%3A19%3A21.patch","31/May/14 22:50;sriharsha;KAFKA-1382_2014-05-31_15:50:25.patch;https://issues.apache.org/jira/secure/attachment/12647797/KAFKA-1382_2014-05-31_15%3A50%3A25.patch","04/Jun/14 19:30;sriharsha;KAFKA-1382_2014-06-04_12:30:40.patch;https://issues.apache.org/jira/secure/attachment/12648374/KAFKA-1382_2014-06-04_12%3A30%3A40.patch","07/Jun/14 16:01;sriharsha;KAFKA-1382_2014-06-07_09:00:56.patch;https://issues.apache.org/jira/secure/attachment/12648820/KAFKA-1382_2014-06-07_09%3A00%3A56.patch","10/Jun/14 01:23;sriharsha;KAFKA-1382_2014-06-09_18:23:42.patch;https://issues.apache.org/jira/secure/attachment/12649503/KAFKA-1382_2014-06-09_18%3A23%3A42.patch","11/Jun/14 16:37;sriharsha;KAFKA-1382_2014-06-11_09:37:22.patch;https://issues.apache.org/jira/secure/attachment/12649821/KAFKA-1382_2014-06-11_09%3A37%3A22.patch","16/Jun/14 20:50;sriharsha;KAFKA-1382_2014-06-16_13:50:16.patch;https://issues.apache.org/jira/secure/attachment/12650644/KAFKA-1382_2014-06-16_13%3A50%3A16.patch","16/Jun/14 21:19;sriharsha;KAFKA-1382_2014-06-16_14:19:27.patch;https://issues.apache.org/jira/secure/attachment/12650654/KAFKA-1382_2014-06-16_14%3A19%3A27.patch",,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385936,,,Wed Apr 27 23:25:52 UTC 2016,,,,,,,,,,"0|i1uhhr:",386200,,,,,,,,,,,,,,,,,,,,"10/Apr/14 05:47;guozhang;When the session timeout happens, would the controller's session timeout listener be fired? If yes, would we just increment the zkVersion in the callback function?;;;","10/Apr/14 15:15;junrao;It sounds like we get a ZK connectionLossException, instead of a session expiration. If it's the latter, the session ID would be different. ConnectionLossException is handled in zkclient by simply retrying the operation. This doesn't quite work for conditional updates since if the previous operation succeeded, the underlying ZK version would have changed. We can't just blindly update the cached zkVersion either since the zkVersion could be different because the controller has changed the zk path at the same time, which takes precedence.

Not sure what's the best way to fix this. One way is to add some metadata in the zk value to indicate the id of the writer. Then, we can use that information to verify if the last (potential conflicting) update is made by the caller itself or not. We will have to add such metadata in a backward compatible way.;;;","10/Apr/14 15:26;guozhang;Could we patch controller such that when such an ISR write has failed, read the stored value, and if it is the same as the value trying to write, let it pass just like the broker registration function?;;;","10/Apr/14 15:34;junrao;Controller's update to isr will always succeed. It keeps re-reading the current value in ZK and then does the conditional update. The leader's update to isr has to fail if the previous update was made by the controller. Checking the value itself will work for most cases, but may not cover 100%. It can happen that the controller updated isr to the same value that the leader wants to write. In this case, we still want to fail the update made by the leader.;;;","10/Apr/14 15:36;guozhang;Yeah I meant to say ""broker"" in the last comment.

Why in this corner case we still want to fail the update instead of just letting it pass?;;;","10/Apr/14 17:25;junrao;Basically once a controller has updated isr, the leader can't touch it until it has received the latest info from the controller. Say a leader had a zk version conflict when updating isr and it sees the latest value (written by the controller) is the same as the value it's trying the write, and we allow it. What could happen is that the leader could successfully shrink the isr again before the controller's decision is propagated to all brokers. Once the leader shrinks the isr, it can commit new messages, which may not be in the new leader that the controller selected. We have lost committed data at this point.;;;","10/Apr/14 19:08;jjkoshy;We do have the leader epoch maintained for each partition - so we won't blindly update the zkVersion but only update it if the leaderEpoch stored in zookeeper is what we currently know.;;;","11/Apr/14 16:36;junrao;Joel,

Yes, that's a good point. When the leader tries to update the isr and sees a zk version conflict, we could read the data back and make sure the content is the same, including the leader epoch. If so, we can assume the previous update actually succeeded. This should be easy to patch.;;;","29/Apr/14 15:37;jblackburn;The other thing about this is that it can quickly churn through log.  There are log lines every few milliseconds at INFO level leading to  GBs of log in a very short time, e.g.:

{code}
[2014-04-28 00:01:37,010] INFO Partition [RSF_OPTIONS,10] on broker 1: Cached zkVersion [21] not equal to that in zookeeper, ski
p updating ISR (kafka.cluster.Partition)
[2014-04-28 00:01:37,017] INFO Partition [RSF_OPTIONS,10] on broker 1: Expanding ISR for partition [RSF_OPTIONS,10] from 1 to 1,
0 (kafka.cluster.Partition)
[2014-04-28 00:01:37,019] ERROR Conditional update of path /brokers/topics/RSF_OPTIONS/partitions/10/state with data {""controlle
r_epoch"":19,""leader"":1,""version"":1,""leader_epoch"":6,""isr"":[1,0]} and expected version 21 failed due to org.apache.zookeeper.Keep
erException$BadVersionException: KeeperErrorCode = BadVersion for /brokers/topics/RSF_OPTIONS/partitions/10/state (kafka.utils.Z
kUtils$)
[2014-04-28 00:01:37,019] INFO Partition [RSF_OPTIONS,10] on broker 1: Cached zkVersion [21] not equal to that in zookeeper, ski
p updating ISR (kafka.cluster.Partition)
[2014-04-28 00:01:37,019] INFO Partition [RSF,14] on broker 1: Expanding ISR for partition [RSF,14] from 1 to 1,0 (kafka.cluster
.Partition)
[2014-04-28 00:01:37,020] ERROR Conditional update of path /brokers/topics/RSF/partitions/14/state with data {""controller_epoch""
:19,""leader"":1,""version"":1,""leader_epoch"":6,""isr"":[1,0]} and expected version 21 failed due to org.apache.zookeeper.KeeperExcept
ion$BadVersionException: KeeperErrorCode = BadVersion for /brokers/topics/RSF/partitions/14/state (kafka.utils.ZkUtils$)
[2014-04-28 00:01:37,020] INFO Partition [RSF,14] on broker 1: Cached zkVersion [21] not equal to that in zookeeper, skip updati
ng ISR (kafka.cluster.Partition)
[2014-04-28 00:01:37,035] INFO Partition [RSF_OPTIONS,10] on broker 1: Expanding ISR for partition [RSF_OPTIONS,10] from 1 to 1,
0 (kafka.cluster.Partition)
[2014-04-28 00:01:37,037] ERROR Conditional update of path /brokers/topics/RSF_OPTIONS/partitions/10/state with data {""controlle
r_epoch"":19,""leader"":1,""version"":1,""leader_epoch"":6,""isr"":[1,0]} and expected version 21 failed due to org.apache.zookeeper.Keep
erException$BadVersionException: KeeperErrorCode = BadVersion for /brokers/topics/RSF_OPTIONS/partitions/10/state (kafka.utils.Z
kUtils$)
[2014-04-28 00:01:37,037] INFO Partition [RSF_OPTIONS,10] on broker 1: Cached zkVersion [21] not equal to that in zookeeper, ski
p updating ISR (kafka.cluster.Partition)
[2014-04-28 00:01:37,037] INFO Partition [RSF,14] on broker 1: Expanding ISR for partition [RSF,14] from 1 to 1,0 (kafka.cluster
.Partition)
{code};;;","26/May/14 05:08;sriharsha;Created reviewboard https://reviews.apache.org/r/21899/diff/
 against branch origin/trunk;;;","31/May/14 04:19;sriharsha;Updated reviewboard https://reviews.apache.org/r/21899/diff/
 against branch origin/trunk;;;","31/May/14 22:50;sriharsha;Updated reviewboard https://reviews.apache.org/r/21899/diff/
 against branch origin/trunk;;;","04/Jun/14 19:30;sriharsha;Updated reviewboard https://reviews.apache.org/r/21899/diff/
 against branch origin/trunk;;;","07/Jun/14 16:01;sriharsha;Updated reviewboard https://reviews.apache.org/r/21899/diff/
 against branch origin/trunk;;;","10/Jun/14 01:23;sriharsha;Updated reviewboard https://reviews.apache.org/r/21899/diff/
 against branch origin/trunk;;;","10/Jun/14 21:56;nehanarkhede;Thanks for the patches, pushed to trunk.;;;","10/Jun/14 21:59;nehanarkhede;Oops, I'm sorry, I didn't see Jun's latest comments before committing. I guess we need a follow up patch with Jun's suggestions addressed.;;;","10/Jun/14 22:06;sriharsha;[~nehanarkhede] I am working on patch based on Jun's comments. Will send updated patch.;;;","11/Jun/14 16:37;sriharsha;Updated reviewboard https://reviews.apache.org/r/21899/diff/
 against branch origin/trunk;;;","16/Jun/14 20:50;sriharsha;Updated reviewboard https://reviews.apache.org/r/21899/diff/
 against branch origin/trunk;;;","16/Jun/14 21:19;sriharsha;Updated reviewboard https://reviews.apache.org/r/21899/diff/
 against branch origin/trunk;;;","18/Jun/14 18:18;sriharsha;[~junrao] Can you please take a look at the latest patch and let me know if it looks good or not.
Thanks,
Harsha;;;","19/Jun/14 04:14;nehanarkhede;Thanks for the follow up patch. Pushed to trunk;;;","19/Jun/14 04:58;junrao;Thanks for the patch. This looks good to me.;;;","03/Sep/14 16:20;nyetter;Is it possible to apply this patch to 0.8.1.1?  Or better yet, can we get a 0.8.1.2 release with this patch included?

We have experienced this bug multiple times in production, causing data loss.  We had been taking the approach of waiting for 0.8.2 and crossing our fingers, but that release no longer has even a target release date on the roadmap.;;;","12/Sep/14 02:01;jhooda;We are in same fix. Can you please comment if this can be patched safely on 0.8.1.1?;;;","12/Sep/14 02:31;nehanarkhede;[~nyetter] There is an ongoing discussion on the mailing list regarding 0.8.1.2. Marking this JIRA to be considered for 0.8.1.2;;;","14/Sep/14 19:08;jhooda;Not sure if this is related. We applied the final patch on 0.8.1.1 and noticed a null pointer that we haven't noticed earlier

----------------------8<--------------------------------------------------------
[2014-09-13 15:26:36,254] ERROR Error processing config change: (kafka.server.TopicConfigManager)
java.lang.NullPointerException
        at scala.collection.convert.Wrappers$JListWrapper.length(Wrappers.scala:85)
        at scala.collection.SeqLike$class.size(SeqLike.scala:106)
        at scala.collection.AbstractSeq.size(Seq.scala:40)
        at kafka.server.TopicConfigManager.kafka$server$TopicConfigManager$$processConfigChanges(TopicConfigManager.scala:89)
        at kafka.server.TopicConfigManager$ConfigChangeListener$.handleChildChange(TopicConfigManager.scala:144)
        at org.I0Itec.zkclient.ZkClient$7.run(ZkClient.java:570)
        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[2014-09-13 15:26:36,273] INFO New leader is 1 (kafka.server.ZookeeperLeaderElector$LeaderChangeListener)
----------------------8<--------------------------------------------------------

Our configuration is 3 brokers 3 zookeepers and topic replication set to 3.

Thanks,
jagbir;;;","14/Sep/14 21:10;sriharsha;[~jhooda]
 can you share how did you applied this patch against 0.8.1.1. I tried doing the following 0.8.1 branch
git apply --check  ../KAFKA-1382_2014-06-16_14:19:27.patch 
I get 
error: patch failed: core/src/main/scala/kafka/cluster/Partition.scala:18
error: core/src/main/scala/kafka/cluster/Partition.scala: patch does not apply

I tried it on http://kafka.apache.org/downloads.html kafka-0.8.1.1-src.tgz it doesn't apply cleanly.
;;;","14/Sep/14 23:43;jhooda;Hi Sriharsha,

This is what I did

prompt> git clone http://git-wip-us.apache.org/repos/asf/kafka.git
prompt> git checkout refs/tags/0.8.1.1 -b kafka-1382
prompt> wget https://issues.apache.org/jira/secure/attachment/12646741/KAFKA-1382.patch
prompt> patch -p1 < KAFKA-1382.patch

Thanks,
Jagbir;;;","15/Sep/14 16:23;sriharsha;[~jhooda]
Thanks for the info. I tried to reproduce this by running a 3 node cluster with 3 zookeeper.
Ran simple tests like creating a topic, altering the topic config , producing data into the topic and reading off the topic, shutting down one of the brokers and also kill -9 a broker. None of the above cases produced the error. Could you please provide the logs and also when did this error happened.;;;","16/Sep/14 00:03;jhooda;Hi Sriharsha,

Thanks for looking into the issue. We are trying to consistently replicate the behavior and will get back.

Jagbir;;;","17/Sep/14 23:22;nyetter;We put a decent amount of effort at replicating this scenario and were unable to.  It has only happened to us in the wild, and never for any apparent reason.;;;","08/Jan/16 14:16;dude9527;we hit this bug in kafka0.8.2.1, three nodes. zookeeper version is 3.4.6. the log is :


[2016-01-05 08:49:27,047] INFO Partition [error-signatureId-956a8fd7-a3ec-4718-bb77-45b3a97eb0cd,0] on broker 3: Shrinking ISR for partition [error-signatureId-956a8fd
7-a3ec-4718-bb77-45b3a97eb0cd,0] from 3,1 to 3 (kafka.cluster.Partition)
[2016-01-05 08:49:27,227] ERROR Uncaught exception in thread 'kafka-network-thread-39091-0': (kafka.utils.Utils$)
java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:347)
        at scala.None$.get(Option.scala:345)
        at kafka.network.ConnectionQuotas.dec(SocketServer.scala:524)
        at kafka.network.AbstractServerThread.close(SocketServer.scala:165)
        at kafka.network.AbstractServerThread.close(SocketServer.scala:157)
        at kafka.network.Processor.close(SocketServer.scala:374)
        at kafka.network.Processor.processNewResponses(SocketServer.scala:406)
        at kafka.network.Processor.run(SocketServer.scala:318)
        at java.lang.Thread.run(Thread.java:745)
[2016-01-05 08:49:27,248] INFO Reconnect due to socket error: java.io.IOException: connection timeout (kafka.consumer.SimpleConsumer)
[2016-01-05 08:49:27,248] INFO Reconnect due to socket error: java.io.IOException: Connection reset by peer (kafka.consumer.SimpleConsumer)
[2016-01-05 08:49:27,278] ERROR Uncaught exception in thread 'kafka-network-thread-39091-1': (kafka.utils.Utils$)
java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:347)
        at scala.None$.get(Option.scala:345)
        at kafka.network.ConnectionQuotas.dec(SocketServer.scala:524)
        at kafka.network.AbstractServerThread.close(SocketServer.scala:165)
        at kafka.network.AbstractServerThread.close(SocketServer.scala:157)
        at kafka.network.Processor.close(SocketServer.scala:374)
        at kafka.network.Processor.processNewResponses(SocketServer.scala:406)
        at kafka.network.Processor.run(SocketServer.scala:318)
        at java.lang.Thread.run(Thread.java:745)
[2016-01-05 08:49:27,918] INFO re-registering broker info in ZK for broker 3 (kafka.server.KafkaHealthcheck)
[2016-01-05 08:49:36,312] INFO Registered broker 3 at path /brokers/ids/3 with address AI-iPaaS-ATS03:39091. (kafka.utils.ZkUtils$)
[2016-01-05 08:49:36,312] INFO done re-registering broker (kafka.server.KafkaHealthcheck)
[2016-01-05 08:49:36,313] INFO Subscribing to /brokers/topics path to watch for new topics (kafka.server.KafkaHealthcheck)
[2016-01-05 08:49:36,332] INFO New leader is 1 (kafka.server.ZookeeperLeaderElector$LeaderChangeListener)
[2016-01-05 08:49:36,343] INFO Partition [error-signatureId-956a8fd7-a3ec-4718-bb77-45b3a97eb0cd,0] on broker 3: Cached zkVersion [0] not equal to that in zookeeper, s
kip updating ISR (kafka.cluster.Partition)
[2016-01-05 08:49:36,343] INFO Partition [error-signatureId-e8c1c145-4109-48d8-a46f-4eca92143594,0] on broker 3: Shrinking ISR for partition [error-signatureId-e8c1c14
5-4109-48d8-a46f-4eca92143594,0] from 3,2 to 3 (kafka.cluster.Partition)
[2016-01-05 08:49:36,372] INFO Partition [error-signatureId-e8c1c145-4109-48d8-a46f-4eca92143594,0] on broker 3: Cached zkVersion [0] not equal to that in zookeeper, s
kip updating ISR (kafka.cluster.Partition)
[2016-01-05 08:49:36,373] INFO Partition [error-signatureId-59206ee6-e9b7-470d-9b1d-638e2cc7ebad,0] on broker 3: Shrinking ISR for partition [error-signatureId-59206ee
6-e9b7-470d-9b1d-638e2cc7ebad,0] from 3,2 to 3 (kafka.cluster.Partition)
[2016-01-05 08:49:36,402] INFO Partition [error-signatureId-59206ee6-e9b7-470d-9b1d-638e2cc7ebad,0] on broker 3: Cached zkVersion [0] not equal to that in zookeeper, s
kip updating ISR (kafka.cluster.Partition)
[2016-01-05 08:49:36,402] INFO Partition [error-signatureId-be6798c3-57d8-4ddc-a155-04983987b160,0] on broker 3: Shrinking ISR for partition [error-signatureId-be6798c
3-57d8-4ddc-a155-04983987b160,0] from 3,1 to 3 (kafka.cluster.Partition)
[2016-01-05 08:49:36,426] INFO Partition [error-signatureId-be6798c3-57d8-4ddc-a155-04983987b160,0] on broker 3: Cached zkVersion [0] not equal to that in zookeeper, s
kip updating ISR (kafka.cluster.Partition)
[2016-01-05 08:49:36,426] INFO Partition [error-signatureId-38fd31e8-3a0a-4b06-b278-a8f10bab232f,0] on broker 3: Shrinking ISR for partition [error-signatureId-38fd31e
;;;","23/Feb/16 12:38;fpj;hi there, it isn't clear to me if you've hit this bug or if your broker simply got stale versions. even with the fix proposed here, you can still get conflicting versions, which will lead to those messages. was your cluster able to recover after you've seen those messages? what's the precise behavior you have observed?;;;","24/Feb/16 01:38;dude9527;we hit this when there is a network problem, which the kakfa broker 3 can not connect to zookeeper.  After the network work normal, the broker can not update the zk and will loop the cached zk version not equal the zookeeper infinitely and the brokercan not recover until restart it.;;;","27/Apr/16 23:25;KaneK;We've been hitting this bug too on kafka 0.8.2.1:

[2016-04-27 23:21:13,310]  4843558914 [kafka-scheduler-4] INFO  kafka.cluster.Partition  - Partition [mp-unknown,220] on broker 104224875: Shrinking ISR for partition [mp-unknown,220] from 104224875,104224876 to 104224875
[2016-04-27 23:21:13,312]  4843558916 [kafka-scheduler-4] INFO  kafka.cluster.Partition  - Partition [mp-unknown,220] on broker 104224875: Cached zkVersion [5] not equal to that in zookeeper, skip updating ISR

Looks like we had some network problems when this happened. Network restored but broker never rejoined ISR set.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient unit test failure in AddPartitionsTest,KAFKA-1381,12707601,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,09/Apr/14 23:43,12/Apr/14 20:51,14/Jul/23 05:39,12/Apr/14 20:51,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"Saw the following transient unit test failure.

kafka.admin.AddPartitionsTest > testReplicaPlacement FAILED
    java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:313)
        at scala.None$.get(Option.scala:311)
        at kafka.admin.AddPartitionsTest.testReplicaPlacement(AddPartitionsTest.scala:189)
",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 01:16;junrao;KAFKA-1381.patch;https://issues.apache.org/jira/secure/attachment/12639500/KAFKA-1381.patch","12/Apr/14 01:39;junrao;KAFKA-1381_2014-04-11_18:39:42.patch;https://issues.apache.org/jira/secure/attachment/12639908/KAFKA-1381_2014-04-11_18%3A39%3A42.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385924,,,Sat Apr 12 20:51:54 UTC 2014,,,,,,,,,,"0|i1uhf3:",386188,,,,,,,,,,,,,,,,,,,,"10/Apr/14 01:16;junrao;Created reviewboard https://reviews.apache.org/r/20185/
 against branch origin/trunk;;;","12/Apr/14 01:39;junrao;Updated reviewboard https://reviews.apache.org/r/20185/
 against branch origin/trunk;;;","12/Apr/14 20:51;junrao;Thanks for the review. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition reassignment resets clock for time-based retention,KAFKA-1379,12707581,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,becket_qin,jjkoshy,jjkoshy,09/Apr/14 22:25,11/Aug/17 21:20,14/Jul/23 05:39,11/Aug/17 21:20,,,,0.10.1.0,,,,,,,log,,,5,,,,"Since retention is driven off mod-times reassigned partitions will result in
data that has been on a leader to be retained for another full retention
cycle. E.g., if retention is seven days and you reassign partitions on the
sixth day then those partitions will remain on the replicas for another
seven days.

",,elevy,elukey,jeffwidman,jjkoshy,Karolis,kzakee,moritz,noslowerdna,xvrl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-3802,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385904,,,Fri Aug 11 21:20:49 UTC 2017,,,,,,,,,,"0|i1uhan:",386168,,,,,,,,,,,,,,,,,,,,"23/Feb/15 14:21;moritz;This also happens when a broker dies and loses it's data. 

When the broker comes back without any data it will use more and more disk space until it doubles the used disk space until the retention kicks in and the usage drops to normal.

IMHO this is pretty bad for disaster scenarios, so I would like to see a higher prio on this.
;;;","24/Feb/15 02:05;jjkoshy;We have been thinking through various alternatives and this is included in a proposal here: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Enriched+Message+Metadata;;;","22/Sep/15 18:55;xvrl;This is a huge issue for us as well, since it requires we keep double the disk capacity on hand, in case one of our brokers or disks fails, which happens relatively often at our scale.

Alternatively, we have to go in and remove expired segments by hand, by comparing replicated segments with the partition leader, before disks run out of space.
;;;","26/May/16 10:54;moritz;From the user-mailinglist:

{quote}
We’ve recently upgraded to 0.9.  In 0.8, when we restarted a broker, data
log file mtimes were not changed.  In 0.9, any data log file that was on
disk before the broker has it’s mtime modified to the time of the broker
restart.
{quote}

A workaround can be to set {{retention.bytes}} on a topic level, like this:

{noformat}
./bin/kafka-topics.sh --zookeeper X.X.X.X:2181/kafka -alter --config retention.bytes=5000000 –topic my_topic
{noformat}

The settings controls the max size in bytes of a partition oft he specified topic. So you can find a good size by checking the size of a partition with {{du -b}} and use this value.;;;","01/Jun/16 10:08;elukey;Hi Moritz,

thanks a lot for pointing us to this Jira in users@. At the moment we use a similar trick to resolve disk partitions filling up (retention.ms):
https://wikitech.wikimedia.org/wiki/Analytics/Cluster/Kafka/Administration#Temporarily_Modify_Per_Topic_Retention_Settings

I also opened a Phabricator task to track this problem https://phabricator.wikimedia.org/T136690

retention.bytes is definitely worth to try, but is there anything else that can mitigate this issue?;;;","20/Jan/17 17:38;noslowerdna;[~jjkoshy] / [~becket_qin] should this Jira now be closed as a duplicate of KAFKA-3163?

https://cwiki.apache.org/confluence/display/KAFKA/KIP-33+-+Add+a+time+based+log+index#KIP-33-Addatimebasedlogindex-Enforcetimebasedlogretention;;;","27/Jun/17 21:11;noslowerdna;[~hachikuji] Jason, could you confirm if this bug has been fixed? According to http://kafka.apache.org/documentation.html#upgrade_10_1_breaking it appears so.;;;","11/Aug/17 21:20;noslowerdna;Marking this bug as resolved in 0.10.1.0 based on the statement ""The log retention time is no longer based on last modified time of the log segments. Instead it will be based on the largest timestamp of the messages in a log segment."" in http://kafka.apache.org/documentation.html#upgrade_10_1_breaking;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient unit test failure in LogRecoveryTest,KAFKA-1378,12707502,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,09/Apr/14 17:14,12/Apr/14 20:49,14/Jul/23 05:39,12/Apr/14 20:49,,,,0.8.2.0,,,,,,,core,,,0,,,,"Saw the following transient unit test failure.

kafka.server.LogRecoveryTest > testHWCheckpointNoFailuresMultipleLogSegments FAILED
    java.lang.AssertionError: Failed to update highwatermark for follower after 1000 ms
        at org.junit.Assert.fail(Assert.java:69)
        at org.junit.Assert.assertTrue(Assert.java:32)
        at kafka.server.LogRecoveryTest.testHWCheckpointNoFailuresMultipleLogSegments(LogRecoveryTest.scala:182)
",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 01:29;junrao;KAFKA-1378.patch;https://issues.apache.org/jira/secure/attachment/12639507/KAFKA-1378.patch","12/Apr/14 01:21;junrao;KAFKA-1378_2014-04-11_18:21:34.patch;https://issues.apache.org/jira/secure/attachment/12639905/KAFKA-1378_2014-04-11_18%3A21%3A34.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385825,,,Sat Apr 12 20:49:24 UTC 2014,,,,,,,,,,"0|i1ugt3:",386089,,,,,,,,,,,,,,,,,,,,"10/Apr/14 01:29;junrao;Created reviewboard https://reviews.apache.org/r/20189/
 against branch origin/trunk;;;","12/Apr/14 01:21;junrao;Updated reviewboard https://reviews.apache.org/r/20189/
 against branch origin/trunk;;;","12/Apr/14 20:49;junrao;Thanks for the review. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient unit test failure in LogOffsetTest,KAFKA-1377,12707499,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,09/Apr/14 17:10,23/Nov/16 19:25,14/Jul/23 05:39,23/Nov/16 19:25,,,,,,,,,,,core,,,0,newbie,,,"Saw the following transient unit test failure.

kafka.server.LogOffsetTest > testGetOffsetsBeforeEarliestTime FAILED
    junit.framework.AssertionFailedError: expected:<List(0)> but was:<Vector()>
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.failNotEquals(Assert.java:277)
        at junit.framework.Assert.assertEquals(Assert.java:64)
        at junit.framework.Assert.assertEquals(Assert.java:71)
        at kafka.server.LogOffsetTest.testGetOffsetsBeforeEarliestTime(LogOffsetTest.scala:198)
",,guozhang,ijuma,junrao,nehanarkhede,omkreddy,pyritschard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 01:22;junrao;KAFKA-1377.patch;https://issues.apache.org/jira/secure/attachment/12639504/KAFKA-1377.patch","12/Apr/14 00:42;junrao;KAFKA-1377_2014-04-11_17:42:13.patch;https://issues.apache.org/jira/secure/attachment/12639899/KAFKA-1377_2014-04-11_17%3A42%3A13.patch","12/Apr/14 01:14;junrao;KAFKA-1377_2014-04-11_18:14:45.patch;https://issues.apache.org/jira/secure/attachment/12639903/KAFKA-1377_2014-04-11_18%3A14%3A45.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385822,,,Wed Nov 23 19:25:38 UTC 2016,,,,,,,,,,"0|i1ugsf:",386086,,,,,,,,,,,,,,,,,,,,"09/Apr/14 17:12;junrao;Not sure the exact cause. One potential issue that I saw is the following. We call
    waitUntilTrue(() => isLeaderLocalOnBroker(topic, part, server), 1000)
before the assertion. However, if waitUntilTrue returns false. We proceed. We should add the assertion on waitUntilTrue and probably increase the wait time.;;;","10/Apr/14 01:22;junrao;Created reviewboard https://reviews.apache.org/r/20186/
 against branch origin/trunk;;;","12/Apr/14 00:42;junrao;Updated reviewboard https://reviews.apache.org/r/20186/
 against branch origin/trunk;;;","12/Apr/14 01:14;junrao;Updated reviewboard https://reviews.apache.org/r/20186/
 against branch origin/trunk;;;","12/Apr/14 20:47;junrao;Thanks for the review. Committed to trunk.;;;","12/Jul/14 10:09;omkreddy;LogOffsetTests are consistently failing on my machine.

kafka.server.LogOffsetTest > testGetOffsetsBeforeLatestTime FAILED
junit.framework.AssertionFailedError: Log for partition [topic,0] should be created
at junit.framework.Assert.fail(Assert.java:47)
at kafka.utils.TestUtils$.waitUntilTrue(TestUtils.scala:589)
at kafka.server.LogOffsetTest.testGetOffsetsBeforeLatestTime(LogOffsetTest.scala:85)

kafka.server.LogOffsetTest > testGetOffsetsBeforeEarliestTime FAILED
    junit.framework.AssertionFailedError: Leader should be elected
        at junit.framework.Assert.fail(Assert.java:47)
        at kafka.utils.TestUtils$.waitUntilTrue(TestUtils.scala:589)
        at kafka.server.LogOffsetTest.testGetOffsetsBeforeEarliestTime(LogOffsetTest.scala:188)

kafka.server.LogOffsetTest > testEmptyLogsGetOffsets FAILED
    junit.framework.AssertionFailedError: Partition [kafka,0] metadata not propagated after 5000 ms
        at junit.framework.Assert.fail(Assert.java:47)
        at kafka.utils.TestUtils$.waitUntilTrue(TestUtils.scala:589)
        at kafka.utils.TestUtils$.waitUntilMetadataIsPropagated(TestUtils.scala:629)
        at kafka.utils.TestUtils$$anonfun$createTopic$1.apply(TestUtils.scala:174)
        at kafka.utils.TestUtils$$anonfun$createTopic$1.apply(TestUtils.scala:173)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.immutable.Range$ByOne$class.foreach(Range.scala:285)
        at scala.collection.immutable.Range$$anon$2.foreach(Range.scala:265)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.immutable.Range.map(Range.scala:39)
        at kafka.utils.TestUtils$.createTopic(TestUtils.scala:173)
        at kafka.server.LogOffsetTest.testEmptyLogsGetOffsets(LogOffsetTest.scala:122)

kafka.server.LogOffsetTest > testGetOffsetsBeforeNow FAILED
    junit.framework.AssertionFailedError: Leader should be elected
        at junit.framework.Assert.fail(Assert.java:47)
        at kafka.utils.TestUtils$.waitUntilTrue(TestUtils.scala:589)
        at kafka.server.LogOffsetTest.testGetOffsetsBeforeNow(LogOffsetTest.scala:160)
;;;","14/Jul/14 14:58;junrao;Is this on trunk? Thanks,;;;","14/Jul/14 15:25;omkreddy;Yes. I am observing these failures on trunk.;;;","17/Jul/14 18:37;nehanarkhede;[~junrao], reopening this as per [~omkreddy]'s observation;;;","04/Sep/14 21:40;guozhang;Pushing to 0.9 as for now, [~omkreddy] is it still a consistently reproducible issue on your side?;;;","05/Sep/14 15:25;omkreddy;Yes, these failures are consistent on my machine.

My machine configuration: 32 bit Ubuntu OS, i5 processor, 4GB
;;;","11/Feb/15 08:41;omkreddy;now i am not getting this exception..so closing the issue.;;;","21/Dec/15 10:14;pyritschard;I am getting these errors consistently.
This is against trunk on Linux. 64 bit, i7 processor 16G. JDK version:

java version ""1.8.0_60""
Java(TM) SE Runtime Environment (build 1.8.0_60-b27)
Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)
;;;","21/Dec/15 11:26;pyritschard;FWIW, bumping the waitTime parameter in TestUtils.scala does not change the behavior, so this is not timing related (waiting for 15s instead of 5s still exhibits the same behavior).;;;","21/Dec/15 18:55;guozhang;[~pyritschard] Which Kafka version are you running for these tests?;;;","21/Dec/15 20:59;pyritschard;[~guozhang] I'm testing against trunk.
The failure to propagater results is confined to the Sasl tests.;;;","21/Dec/15 21:35;ijuma;[~pyritschard], this JIRA is about LogOffsetTest, if you are seeing other failures (ie Sasl related), please file a separate issue (in case it hasn't been filed already).;;;","21/Dec/15 22:00;pyritschard;[~ijuma] will do. it looked to me as a generalization of the previous problem.;;;","23/Nov/16 19:20;guozhang;[~ijuma] Is this still an issue? I cannot remember seeing this failure in recent Jenkins builds.;;;","23/Nov/16 19:25;ijuma;I haven't seen it, so closing it since the original reporter said it was fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient test failure in UncleanLeaderElectionTest,KAFKA-1376,12707493,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,09/Apr/14 17:00,09/Apr/14 21:53,14/Jul/23 05:39,09/Apr/14 21:53,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"We have the following transient unit test failure in trunk.

kafka.integration.UncleanLeaderElectionTest > testUncleanLeaderElectionEnabled FAILED
    org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server within timeout: 400
        at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:880)
        at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
        at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
        at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:169)
        at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:125)
        at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:141)
        at kafka.consumer.Consumer$.create(ConsumerConnector.scala:89)
        at kafka.integration.UncleanLeaderElectionTest.consumeAllMessages(UncleanLeaderElectionTest.scala:273)
        at kafka.integration.UncleanLeaderElectionTest.verifyUncleanLeaderElectionEnabled(UncleanLeaderElectionTest.scala:197)
        at kafka.integration.UncleanLeaderElectionTest.testUncleanLeaderElectionEnabled(UncleanLeaderElectionTest.scala:106)

kafka.integration.UncleanLeaderElectionTest > testUncleanLeaderElectionDisabled PASSED

kafka.integration.UncleanLeaderElectionTest > testUncleanLeaderElectionEnabledByTopicOverride PASSED

kafka.integration.UncleanLeaderElectionTest > testCleanLeaderElectionDisabledByTopicOverride FAILED
    org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server within timeout: 400
        at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:880)
        at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
        at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
        at kafka.consumer.ZookeeperConsumerConnector.connectZk(ZookeeperConsumerConnector.scala:169)
        at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:125)
        at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:141)
        at kafka.consumer.Consumer$.create(ConsumerConnector.scala:89)
        at kafka.integration.UncleanLeaderElectionTest.consumeAllMessages(UncleanLeaderElectionTest.scala:273)
        at kafka.integration.UncleanLeaderElectionTest.verifyUncleanLeaderElectionDisabled(UncleanLeaderElectionTest.scala:214)
        at kafka.integration.UncleanLeaderElectionTest.testCleanLeaderElectionDisabledByTopicOverride(UncleanLeaderElectionTest.scala:148)
",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 21:44;junrao;KAFKA-1376.patch;https://issues.apache.org/jira/secure/attachment/12639471/KAFKA-1376.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385816,,,Wed Apr 09 21:53:49 UTC 2014,,,,,,,,,,"0|i1ugr3:",386080,,,,,,,,,,,,,,,,,,,,"09/Apr/14 17:02;junrao;The issue seems to be the zk connection/sessesion timeout are too small (400ms) as configured in TestUtils.createConsumerProperties.;;;","09/Apr/14 21:44;junrao;Created reviewboard https://reviews.apache.org/r/20181/
 against branch origin/trunk;;;","09/Apr/14 21:53;junrao;Thanks for the review. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Formatting for ""Running a task on a particular version of Scala"" paragraph in README.md is broken",KAFKA-1375,12707439,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,sslavic,sslavic,sslavic,09/Apr/14 13:38,29/Jul/15 23:22,14/Jul/23 05:39,09/Apr/14 15:24,0.8.1,,,0.8.2.0,,,,,,,website,,,0,documentation,,,See commit which broke formatting at https://github.com/apache/kafka/commit/879e3e770ebc49f916137e8416df74373fa26a74#diff-04c6e90faac2675aa89e2176d2eec7d8,,githubbot,junrao,sslavic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/14 13:54;sslavic;0001-KAFKA-1375-Fixed-formatting-of-instructions-for-usin.patch;https://issues.apache.org/jira/secure/attachment/12639397/0001-KAFKA-1375-Fixed-formatting-of-instructions-for-usin.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385762,,,Wed Jul 29 23:22:31 UTC 2015,,,,,,,,,,"0|i1ugf3:",386026,,,,,,,,,,,,,,,,,,,,"09/Apr/14 13:52;githubbot;GitHub user sslavic opened a pull request:

    https://github.com/apache/kafka/pull/24

    KAFKA-1375: Fix formatting in README.md

    This patch fixes formatting of instructions for using particular version of Scala when running a Gradle build task.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/sslavic/kafka KAFKA-1375

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/24.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #24
    
----
commit f3cfc76185d17b8a0970bb19bbb0d4979beec0dc
Author: Stevo Slavic <sslavic@gmail.com>
Date:   2014-04-09T13:50:13Z

    KAFKA-1375: Fixed formatting of instructions for using particular version of Scala when running a Gradle build task

----
;;;","09/Apr/14 13:54;sslavic;Attaching [^0001-KAFKA-1375-Fixed-formatting-of-instructions-for-usin.patch];;;","09/Apr/14 13:54;sslavic;Submitted [^0001-KAFKA-1375-Fixed-formatting-of-instructions-for-usin.patch];;;","09/Apr/14 15:24;junrao;Thanks for the patch. +1 and committed to trunk.;;;","29/Jul/15 23:22;githubbot;Github user sslavic closed the pull request at:

    https://github.com/apache/kafka/pull/24
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogCleaner (compaction) does not support compressed topics,KAFKA-1374,12707263,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,omkreddy,jjkoshy,jjkoshy,08/Apr/14 21:25,15/Dec/15 00:26,14/Jul/23 05:39,20/May/15 18:07,,,,0.9.0.0,,,,,,,,,,0,newbie++,,,"This is a known issue, but opening a ticket to track.

If you try to compact a topic that has compressed messages you will run into
various exceptions - typically because during iteration we advance the
position based on the decompressed size of the message. I have a bunch of
stack traces, but it should be straightforward to reproduce.

",,diederik,guozhang,jjkoshy,jkreps,nehanarkhede,nmarasoiu,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SAMZA-586,KAFKA-1499,SAMZA-398,KAFKA-1581,,,,,,KAFKA-1944,SAMZA-388,,,"19/May/15 07:54;jjkoshy;KAFKA-1374.patch;https://issues.apache.org/jira/secure/attachment/12733742/KAFKA-1374.patch","03/Aug/14 17:41;omkreddy;KAFKA-1374.patch;https://issues.apache.org/jira/secure/attachment/12659556/KAFKA-1374.patch","09/Aug/14 10:52;omkreddy;KAFKA-1374_2014-08-09_16:18:55.patch;https://issues.apache.org/jira/secure/attachment/12660810/KAFKA-1374_2014-08-09_16%3A18%3A55.patch","12/Aug/14 16:56;omkreddy;KAFKA-1374_2014-08-12_22:23:06.patch;https://issues.apache.org/jira/secure/attachment/12661240/KAFKA-1374_2014-08-12_22%3A23%3A06.patch","23/Sep/14 16:20;omkreddy;KAFKA-1374_2014-09-23_21:47:12.patch;https://issues.apache.org/jira/secure/attachment/12670732/KAFKA-1374_2014-09-23_21%3A47%3A12.patch","03/Oct/14 13:22;omkreddy;KAFKA-1374_2014-10-03_18:49:16.patch;https://issues.apache.org/jira/secure/attachment/12672767/KAFKA-1374_2014-10-03_18%3A49%3A16.patch","03/Oct/14 13:50;omkreddy;KAFKA-1374_2014-10-03_19:17:17.patch;https://issues.apache.org/jira/secure/attachment/12672770/KAFKA-1374_2014-10-03_19%3A17%3A17.patch","17/Jan/15 18:52;omkreddy;KAFKA-1374_2015-01-18_00:19:21.patch;https://issues.apache.org/jira/secure/attachment/12692926/KAFKA-1374_2015-01-18_00%3A19%3A21.patch","18/May/15 17:30;omkreddy;KAFKA-1374_2015-05-18_22:55:48.patch;https://issues.apache.org/jira/secure/attachment/12733584/KAFKA-1374_2015-05-18_22%3A55%3A48.patch","20/May/15 00:20;jjkoshy;KAFKA-1374_2015-05-19_17:20:44.patch;https://issues.apache.org/jira/secure/attachment/12734003/KAFKA-1374_2015-05-19_17%3A20%3A44.patch",,,,,,,,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385586,,,Tue Dec 15 00:26:37 UTC 2015,,,,,,,,,,"0|i1ufcf:",385852,,guozhang,,,,,,,,,,,,,,,,,,"19/Jun/14 18:53;jjkoshy;I had started on this a while ago, but did not finish. Here's a WIP patch
that doesn't quite work yet:
https://gist.github.com/jjkoshy/4657a44e52e3f88be1c1

Another nuance with compression and compaction is what compression-codec do
we use when writing out the compacted data?

We could adopt a broker-side compression config that can be overridden on a
per-topic basis. This would not only enable a consistent compression codec
for each topic, but it will also make the above decision more
straightforward - i.e., write out compacted messages in the configured
compression codec for that topic (or broker-default if the topic does not
have any override).

Will file a separate jira for the above.

;;;","30/Jul/14 04:03;omkreddy; I am trying to look in to the issue and WIP Patch.

In WIP patch,  the following code is used to traverse segment offsets .
 
{code}
    var currOffset = segment.baseOffset
     while (currOffset < segment.index.lastOffset) {
        currOffset = entry.nextOffset
     }
{code}

As per my observation,  segment.index.lastOffset is not giving the last offset of a given segment. 
I have a segment with startingOffset=0 and lastOffset=7140. I am getting segment.index.lastOffset=7118.
This is creating some issue in the code.

Any  idea on why segment.index.lastOffset is not returning proper lastOffset.?
;;;","03/Aug/14 17:41;omkreddy;Created reviewboard https://reviews.apache.org/r/24214/diff/
 against branch origin/trunk;;;","03/Aug/14 17:42;omkreddy;In this patch, LogCleaner decompress the compressed messages and writes back the retained messages in compressed form.

I tested the following scenarios
1. Topic with non-compressed messages
2. Topic with compressed messages
3. Topic with both compressed and non-compressed messages.;;;","06/Aug/14 13:45;nehanarkhede;Assigning to [~jjkoshy] for review;;;","09/Aug/14 10:38;omkreddy;Updated reviewboard https://reviews.apache.org/r/24214/diff/
 against branch origin/trunk;;;","09/Aug/14 10:52;omkreddy;Updated reviewboard https://reviews.apache.org/r/24214/diff/
 against branch origin/trunk;;;","12/Aug/14 16:56;omkreddy;Updated reviewboard https://reviews.apache.org/r/24214/diff/
 against branch origin/trunk;;;","15/Sep/14 02:05;nehanarkhede;Bump. This is marked for 0.8.2. Feel free to reassign for review.;;;","23/Sep/14 16:20;omkreddy;Updated reviewboard https://reviews.apache.org/r/24214/diff/
 against branch origin/trunk;;;","03/Oct/14 13:22;omkreddy;Updated reviewboard https://reviews.apache.org/r/24214/diff/
 against branch origin/trunk;;;","03/Oct/14 13:50;omkreddy;Updated reviewboard https://reviews.apache.org/r/24214/diff/
 against branch origin/trunk;;;","17/Jan/15 18:52;omkreddy;Updated reviewboard https://reviews.apache.org/r/24214/diff/
 against branch origin/trunk;;;","11/Feb/15 16:58;jkreps;Hey guys, the test kafka.tools.TestLogCleaning is a very aggressive test that runs against a kafka cluster configured for log compaction. It produces a bunch of messages and compacts them continuously and then does an out of band comparison of the two. It would be good to ensure that stills works on really large cleaner runs with deletes with this patch.;;;","11/Feb/15 18:16;omkreddy;yes, we are using TestLogCleaning  tool to test the changes. 

TestLogCleaning stress test output for compressed messages

{code}
Producing 100000 messages...
Logging produce requests to /tmp/kafka-log-cleaner-produced-6014466306002699464.txt
Sleeping for 120 seconds...
Consuming messages...
Logging consumed messages to /tmp/kafka-log-cleaner-consumed-177538909590644701.txt
100000 rows of data produced, 13165 rows of data consumed (86.8% reduction).
De-duplicating and validating output files...
Validated 9005 values, 0 mismatches.

Producing 1000000 messages...
Logging produce requests to /tmp/kafka-log-cleaner-produced-3298578695475992991.txt
Sleeping for 120 seconds...
Consuming messages...
Logging consumed messages to /tmp/kafka-log-cleaner-consumed-7192293977610206930.txt
1000000 rows of data produced, 119926 rows of data consumed (88.0% reduction).
De-duplicating and validating output files...
Validated 89947 values, 0 mismatches.

Producing 10000000 messages...
Logging produce requests to /tmp/kafka-log-cleaner-produced-3336255463347572934.txt
Sleeping for 120 seconds...
Consuming messages...
Logging consumed messages to /tmp/kafka-log-cleaner-consumed-9149188270705707725.txt
10000000 rows of data produced, 1645281 rows of data consumed (83.5% reduction).
De-duplicating and validating output files...
Validated 899853 values, 0 mismatches.
{code};;;","11/Feb/15 18:26;jjkoshy;I can review this next week. However, as far as checking in is concerned I would strongly prefer to get KAFKA-1755 done first (for which I have a patch almost ready). The reason for that is that this patch is a significant change to the log cleaner and I would rather get some defensive code in first since the log cleaner health is critical for offset management as well as for Samza use-cases.;;;","11/Feb/15 18:39;jkreps;Great!;;;","12/May/15 14:03;jjkoshy;Sorry I dropped this. I just reviewed the patch. I think it looks good, but needs a rebase. Let me know if you are swamped though and we can help with it.;;;","18/May/15 17:30;omkreddy;Updated reviewboard https://reviews.apache.org/r/24214/diff/
 against branch origin/trunk;;;","19/May/15 07:54;jjkoshy;Created reviewboard https://reviews.apache.org/r/34397/diff/
 against branch origin/trunk;;;","20/May/15 00:20;jjkoshy;Updated reviewboard https://reviews.apache.org/r/34397/diff/
 against branch origin/trunk;;;","20/May/15 17:30;guozhang;Committed to trunk.

I think we need some integration tests before feeling confident about this issue being resolved, so leave it open just for now.;;;","20/May/15 18:07;jjkoshy;Please file a separate jira for that. Also, I'm going to amend this commit with the right author information.;;;","20/May/15 18:37;jjkoshy;I just wanted the commit to record Manikumar as author. Unfortunately our git repo does not seem to let me force push an amended commit so nm.;;;","01/Dec/15 16:53;granthenke;With this being resolved, should we update this section of the docs? 
http://kafka.apache.org/documentation.html#design_compactionlimitations;;;","15/Dec/15 00:26;omkreddy;[~granthenke] Yes, I will update the docs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogCleaner assumes first dirty offset zero if there is no cleaner checkpoint,KAFKA-1373,12707262,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jjkoshy,jjkoshy,jjkoshy,08/Apr/14 21:20,10/Apr/14 23:19,14/Jul/23 05:39,10/Apr/14 23:19,0.8.1,,,0.8.1.1,,,,,,,,,,1,,,,"If you try enabling the compaction policy on a topic that already exists and if its first segment starts with anything other than zero you will see something like this:

java.lang.IllegalArgumentException: requirement failed: Last clean offset is 0 but segment base offset is 2722629 for log test-0.
        at scala.Predef$.require(Predef.scala:145)
        at kafka.log.Cleaner.buildOffsetMap(LogCleaner.scala:489)
        at kafka.log.Cleaner.clean(LogCleaner.scala:287)
        at kafka.log.LogCleaner$CleanerThread.cleanOrSleep(LogCleaner.scala:203)
        at kafka.log.LogCleaner$CleanerThread.doWork(LogCleaner.scala:188)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)
",,adenysenko,jjkoshy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1380,,,,"10/Apr/14 18:55;jjkoshy;KAFKA-1373.patch;https://issues.apache.org/jira/secure/attachment/12639627/KAFKA-1373.patch","08/Apr/14 21:22;jjkoshy;KAFKA-1373.patch;https://issues.apache.org/jira/secure/attachment/12639269/KAFKA-1373.patch","08/Apr/14 23:18;jjkoshy;KAFKA-1373_2014-04-08_16:18:22.patch;https://issues.apache.org/jira/secure/attachment/12639297/KAFKA-1373_2014-04-08_16%3A18%3A22.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385585,,,Thu Apr 10 23:19:52 UTC 2014,,,,,,,,,,"0|i1ufc7:",385851,,,,,,,,,,,,,,,,,,,,"08/Apr/14 21:22;jjkoshy;Created reviewboard https://reviews.apache.org/r/20130/
 against branch origin/trunk;;;","08/Apr/14 23:18;jjkoshy;Updated reviewboard https://reviews.apache.org/r/20130/
 against branch origin/trunk;;;","08/Apr/14 23:20;jjkoshy;Updated the RB with an additional fix: KAFKA-1289 changed the compaction config from dedupe to compact. That needed to be updated in a couple of other places as well. Ideally it should have been some global constant string but this is fine for now.;;;","09/Apr/14 17:51;jjkoshy;Committed to trunk, but need to submit a separate patch for 0.8.1 branch.;;;","10/Apr/14 18:55;jjkoshy;Created reviewboard https://reviews.apache.org/r/20227/
 against branch origin/0.8.1;;;","10/Apr/14 23:19;jjkoshy;Committed to 0.8.1 also.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
snappy version update 1.1.x,KAFKA-1369,12707159,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,theduderog,thinker0,thinker0,08/Apr/14 11:34,02/Feb/16 23:09,14/Jul/23 05:39,14/Sep/14 18:16,0.8.0,0.8.1.1,,0.8.2.0,,,,,,,core,,,1,,,,"https://github.com/xerial/snappy-java/issues/38 issue

snappy version 1.1.x

{code}
org.xerial.snappy.SnappyError: [FAILED_TO_LOAD_NATIVE_LIBRARY] null
    at org.xerial.snappy.SnappyLoader.load(SnappyLoader.java:239)
    at org.xerial.snappy.Snappy.<clinit>(Snappy.java:48)
    at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:351)
    at org.xerial.snappy.SnappyInputStream.rawRead(SnappyInputStream.java:159)
    at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:142)
    at java.io.InputStream.read(InputStream.java:101)
    at kafka.message.ByteBufferMessageSet$$anonfun$decompress$1.apply$mcI$sp(ByteBufferMessageSet.scala:68)
    at kafka.message.ByteBufferMessageSet$$anonfun$decompress$1.apply(ByteBufferMessageSet.scala:68)
    at kafka.message.ByteBufferMessageSet$$anonfun$decompress$1.apply(ByteBufferMessageSet.scala:68)
    at scala.collection.immutable.Stream$.continually(Stream.scala:1129)
    at kafka.message.ByteBufferMessageSet$.decompress(ByteBufferMessageSet.scala:68)
    at kafka.message.ByteBufferMessageSet$$anon$1.makeNextOuter(ByteBufferMessageSet.scala:178)
    at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:191)
    at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:145)
    at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:66)
    at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:58)
{code}

{code}
/tmp] ldd snappy-1.0.5-libsnappyjava.so
./snappy-1.0.5-libsnappyjava.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.9' not found (required by ./snappy-1.0.5-libsnappyjava.so)
./snappy-1.0.5-libsnappyjava.so: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.11' not found (required by ./snappy-1.0.5-libsnappyjava.so)
	linux-vdso.so.1 =>  (0x00007fff81dfc000)
	libstdc++.so.6 => /usr/lib64/libstdc++.so.6 (0x00002b554b430000)
	libm.so.6 => /lib64/libm.so.6 (0x00002b554b731000)
	libc.so.6 => /lib64/libc.so.6 (0x00002b554b9b4000)
	libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00002b554bd0c000)
	/lib64/ld-linux-x86-64.so.2 (0x00000033e2a00000)
{code}

{code}
/tmp] ldd snappy-1.1.1M1-be6ba593-9ac7-488e-953e-ba5fd9530ee1-libsnappyjava.so
ldd: warning: you do not have execution permission for `./snappy-1.1.1M1-be6ba593-9ac7-488e-953e-ba5fd9530ee1-libsnappyjava.so'
	linux-vdso.so.1 =>  (0x00007fff1c132000)
	libstdc++.so.6 => /usr/lib64/libstdc++.so.6 (0x00002b9548319000)
	libm.so.6 => /lib64/libm.so.6 (0x00002b954861a000)
	libc.so.6 => /lib64/libc.so.6 (0x00002b954889d000)
	libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00002b9548bf5000)
	/lib64/ld-linux-x86-64.so.2 (0x00000033e2a00000)
{code}","Red Hat Enterprise Linux Server release 5.8 (Tikanga)
- x64 ",githubbot,junrao,theduderog,thinker0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/14 01:18;thinker0;patch.diff;https://issues.apache.org/jira/secure/attachment/12662940/patch.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385482,,,Tue Feb 02 23:09:56 UTC 2016,,,,,,,,,,"0|i1uepj:",385749,,,,,,,,,,,,,,,,,,,,"19/Aug/14 10:23;githubbot;GitHub user thinker0 opened a pull request:

    https://github.com/apache/kafka/pull/31

    KAFKA-1369 - snappy version update 1.1.x

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/thinker0/kafka snappy-update

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/31.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #31
    
----
commit 214aa1a1cdcc89a73d9304d874d1074f63e78211
Author: thinker0 <thinker0@daumcorp.com>
Date:   2014-08-19T10:17:14Z

    KAFKA-1369 - snappy version update 1.1.x

----
;;;","19/Aug/14 10:23;thinker0;https://github.com/apache/kafka/pull/31;;;","19/Aug/14 14:21;junrao;Could you attach the changes as a patch to this jira? This will take care of Apache licensing stuff.;;;","12/Sep/14 22:32;theduderog;I also ran into this issue on 64-bit Centos 5.  The snappy-java maintainer said that 1.1.1.3 is API compatible with 1.0.5.3 and has libstdc++ statically compiled into the object file so that it doesn't rely on the OS to have a new enough version.

https://github.com/xerial/snappy-java/issues/17;;;","14/Sep/14 18:16;junrao;Thanks for the patch. +1 and committed to trunk.;;;","25/Aug/15 08:49;githubbot;Github user thinker0 closed the pull request at:

    https://github.com/apache/kafka/pull/31
;;;","02/Feb/16 23:09;githubbot;Github user stumped2 closed the pull request at:

    https://github.com/apache/kafka/pull/30
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker topic metadata not kept in sync with ZooKeeper,KAFKA-1367,12707041,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,singhashish,rberdeen,rberdeen,07/Apr/14 22:23,05/Aug/15 05:50,14/Jul/23 05:39,07/Jul/15 16:49,0.8.0,0.8.1,,0.9.0.0,,,,,,,,,,5,newbie++,,,"When a broker is restarted, the topic metadata responses from the brokers will be incorrect (different from ZooKeeper) until a preferred replica leader election.

In the metadata, it looks like leaders are correctly removed from the ISR when a broker disappears, but followers are not. Then, when a broker reappears, the ISR is never updated.


I used a variation of the Vagrant setup created by Joe Stein to reproduce this with latest from the 0.8.1 branch: https://github.com/also/kafka/commit/dba36a503a5e22ea039df0f9852560b4fb1e067c",,aauradkar,abiletskyi,aozeritsky,ataraxer,becket_qin,daisuke.kobayashi,fsaintjacques,fullung,guozhang,gwenshap,jeffwidman,jjkoshy,jkreps,jonbringhurst,junrao,mazhar.shaikh.in,nehanarkhede,ottomata,rberdeen,ScottReynolds,singhashish,Skandragon,smeder,sslavic,vanyatka,Wallrat2000,winbatch,,,,,,,,,,,,,,,,,,KAFKA-1557,,,,,,,,,,,KAFKA-972,,,,,,,,,,"24/Jun/15 05:09;singhashish;KAFKA-1367.patch;https://issues.apache.org/jira/secure/attachment/12741448/KAFKA-1367.patch","07/Apr/14 22:26;rberdeen;KAFKA-1367.txt;https://issues.apache.org/jira/secure/attachment/12639080/KAFKA-1367.txt","02/Jul/15 00:23;singhashish;KAFKA-1367_2015-07-01_17:23:14.patch;https://issues.apache.org/jira/secure/attachment/12743187/KAFKA-1367_2015-07-01_17%3A23%3A14.patch","07/Jul/15 05:04;singhashish;KAFKA-1367_2015-07-06_22:04:06.patch;https://issues.apache.org/jira/secure/attachment/12743887/KAFKA-1367_2015-07-06_22%3A04%3A06.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385364,,,Wed Aug 05 05:50:26 UTC 2015,,,,,,,,,,"0|i1udzj:",385631,,,,,,,,,,,,,,,,,,,,"07/Apr/14 22:26;rberdeen;JIRA ate my whitespace. See Attached for a description of the steps.;;;","19/Sep/14 18:06;ottomata;This happens to me as well.  See:  https://github.com/edenhill/librdkafka/issues/147
;;;","19/Sep/14 18:15;ottomata;I just updated the librdkafka issue, pasting it here as well:

I noticed that in my case, only 1 of the 4 brokers was ever missing in the ISRs reported by Kafka Brokers (via librdkafka). This JIRA indicated that a preferred-replica-election should fix the problem. I did this:

controlled-shutdown of offending broker 21. Then actual shutdown of broker 21. Once this was done, librdkafka metadata showed the correct ISRs, since this offending broker really was not in any ISRs. I then restarted broker 21 and let its replicas catch back up. Once it caught up, zookeeper reported that all ISRs were in sync. I then checked librdkafka's metadata, and broker 21 was not listed in any ISR. I then ran a preferred-replica-election. broker 21 was then promoted to leader of some partitions. librdkafka then only showed broker 21 being in the ISRs for which it was also the leader. Any partition that has a replica on broker 21 does not show up in the ISR unless broker 21 is the leader.

  $ kafkacat -L -b analytics1022.eqiad.wmnet  -t webrequest_upload
  Metadata for webrequest_upload (from broker -1: analytics1022.eqiad.wmnet:9092/bootstrap):
   4 brokers:
    broker 12 at analytics1012.eqiad.wmnet:9092
    broker 21 at analytics1021.eqiad.wmnet:9092
    broker 22 at analytics1022.eqiad.wmnet:9092
    broker 18 at analytics1018.eqiad.wmnet:9092
   1 topics:
    topic ""webrequest_upload"" with 12 partitions:
      partition 11, leader 12, replicas: 12,21,22, isrs: 12,22
      partition 5, leader 21, replicas: 21,22,12, isrs: 22,12,21
      partition 10, leader 22, replicas: 22,18,21, isrs: 18,22
      partition 7, leader 12, replicas: 12,18,21, isrs: 12,18
      partition 8, leader 18, replicas: 18,22,12, isrs: 12,18,22
      partition 3, leader 12, replicas: 12,22,18, isrs: 12,18,22
      partition 4, leader 18, replicas: 18,21,22, isrs: 18,22
      partition 1, leader 21, replicas: 21,18,22, isrs: 18,22,21
      partition 6, leader 22, replicas: 22,12,18, isrs: 12,18,22
      partition 2, leader 22, replicas: 22,21,12, isrs: 12,22
      partition 9, leader 21, replicas: 21,12,18, isrs: 12,18,21
      partition 0, leader 18, replicas: 18,12,21, isrs: 12,18



vs kafka-topic.sh --describe

  Topic:webrequest_upload	PartitionCount:12	ReplicationFactor:3	Configs:
  	Topic: webrequest_upload	Partition: 0	Leader: 18	Replicas: 18,12,21	Isr: 12,18,21
  	Topic: webrequest_upload	Partition: 1	Leader: 21	Replicas: 21,18,22	Isr: 18,22,21
  	Topic: webrequest_upload	Partition: 2	Leader: 22	Replicas: 22,21,12	Isr: 12,22,21
  	Topic: webrequest_upload	Partition: 3	Leader: 12	Replicas: 12,22,18	Isr: 12,18,22
  	Topic: webrequest_upload	Partition: 4	Leader: 18	Replicas: 18,21,22	Isr: 18,22,21
  	Topic: webrequest_upload	Partition: 5	Leader: 21	Replicas: 21,22,12	Isr: 22,12,21
  	Topic: webrequest_upload	Partition: 6	Leader: 22	Replicas: 22,12,18	Isr: 12,18,22
  	Topic: webrequest_upload	Partition: 7	Leader: 12	Replicas: 12,18,21	Isr: 12,18,21
  	Topic: webrequest_upload	Partition: 8	Leader: 18	Replicas: 18,22,12	Isr: 12,18,22
  	Topic: webrequest_upload	Partition: 9	Leader: 21	Replicas: 21,12,18	Isr: 12,18,21
  	Topic: webrequest_upload	Partition: 10	Leader: 22	Replicas: 22,18,21	Isr: 18,22,21
  	Topic: webrequest_upload	Partition: 11	Leader: 12	Replicas: 12,21,22	Isr: 12,22,21;;;","20/Sep/14 05:01;junrao;Yes, in the current implementation, ISR returned from metadata requests can be inconsistent with what's stored in ZK. This is because metadata is only propagated from the controller to the brokers when the controller changes the leader or the ISR. However, when a follower catches up, the leader (not the controller) adds it back to ISR and updates ZK. That info is not propagated to all brokers.

Currently, the ISR part in a metadata response is not really used by the clients. Do you have a usage for this?;;;","22/Sep/14 13:53;edenhill;Apart from supplying the ISR count and list in its metadata API, librdkafka also provides an `enforce.isr.cnt` configuration property that fails
produce requests locally before transmission if the currently known ISR count is smaller than the configured value.
This is a workaround for the broker not fully honoring `request.required.acks`, i.e., if `request.required.acks=3` and only one broker is available the produce request will not fail.
More info in the original issue here: https://github.com/edenhill/librdkafka/issues/91

Generally I would assume that information provided by the broker is correct, otherwise it should not be included at all since it can't be used (reliably).;;;","22/Sep/14 16:57;guozhang;Hello [~edenhill], I think your use case may be supported in a new feature that is currently developed: KAFKA-1555. Would you like to take a look at it and see if your case is really covered?;;;","23/Sep/14 00:28;winbatch;I'd also add that having the broker being able to serve up (accurate) metadata allows client applications to build custom dashboards, etc.  As I understand it, there is the idea to move away from zookeeper (or at least for somethings) within the kafka infrastructure - so having the broker be able to provide this would be good.;;;","23/Sep/14 06:48;jjkoshy;I definitely agree with [~edenhill] that if such a field exists in the response then the information populated in the field should be accurate (or we may as well not include the field) - so we should fix this.;;;","25/Sep/14 05:08;nehanarkhede;bq. I definitely agree with Magnus Edenhill that if such a field exists in the response then the information populated in the field should be accurate (or we may as well not include the field) - so we should fix this.

+1;;;","25/Sep/14 18:04;edenhill;[~guozhang] Yes, KAFKA-1555 looks like a good match.;;;","06/Oct/14 17:26;guozhang;Regarding the fix to this issue, we can either 1) remove the ISR field from the metadata response and hence enforce people to use the admin tool (with ZK dependency) for such usages, which would also require a protocol change between client / server; or 2) let the controller to also watch for ISR change and propagate that information to brokers, this will not introduce protocol change to clients but will likely add a lot of burden on controllers since ISR change is more frequent than leader migrations.

[~jjkoshy][~junrao] any other thoughts?;;;","06/Oct/14 19:02;guozhang;Just talked to Joel offline. I think since ISR (and also Leader) info in broker is just a cached snapshot and cannot be really used in a scenario like this (i.e. depending on the ISR list to determine if the ack received with -1 setting is reliable or not, since the ISR can shrink while the ack is sent back), we could remove the ISR cache from the brokers and also remove it from the metadata response, unless there is a clear use case of this information.;;;","06/Oct/14 19:20;nehanarkhede;The ISR cache on the broker was added only because we had to expose that information through the topic metadata response. I don't think we gave a lot of thought, back then, on why the ISR information is useful in the topic metadata response (especially since it's stale and effectively inaccurate). I am not entirely sure if having the controller be aware of all ISR changes is terrible even though it's true that the # of watches it has to add is proportional to the # of partitions in a cluster. But it's not worth doing that if we don't find a use for the ISR information in the topic metadata response. So I'd vote for removing ISR from topic metadata and also from the broker's metadata cache.;;;","06/Oct/14 20:57;edenhill;May I suggest not to change the protocol but to only send an empty ISR vector in the MetadataResponse?;;;","06/Feb/15 21:35;fsaintjacques;Instead of silently removing the field, could the controller force a cache refresh on a metadata request?;;;","04/Apr/15 21:37;jkreps;[~nehanarkhede] Does this problem still exist?;;;","09/Apr/15 19:24;Skandragon;There is another issue that no one seems to be discussing.  This ISR data is AFAIK the only way to know when a broker is ""Back in service""

Consider this scenario.  I have 5 brokers, and want to upgrade them.  I want to know when a broker has caught up so I can take down the next one in sequence to upgrade it.  How can I know this if the reported state of the world is different than what is actually in use by the brokers themselves?

This seems to be very much an operational issue.;;;","09/Apr/15 20:33;junrao;[~Skandragon], for the operational issue that you pointed out, there are jmx such as the underReplicatedCount that one can leverage.

That said, it's probably useful for an admin client to know the accurate ISR. As Guozhang has suggested, one approach is to register a watcher of the state path for each partition. We probably need to do a bit experiments to see how much overhead this adds. Another approach is to have the controller periodically read the latest ISR for each partition. This is probably a bit simpler to implement, but may not reflect ISR timely and may add unnecessary load to ZK.;;;","14/Apr/15 20:31;jonbringhurst;Hey [~jkreps], I can confirm that the metadata response is still out of sync with ZK in a recent trunk build (about a month old).

Btw, it's not much effort for me to simply look at ZK or JMX -- it was just confusing when I initially ran into this.;;;","26/Apr/15 21:10;nehanarkhede;[~jkreps] Yup, issue still exists and the solution I still recommend is to have the controller register watches and know the latest ISR for all partitions. This change isn't big if someone wants to take a stab. ;;;","27/Apr/15 01:23;singhashish;[~nehanarkhede] If no one has already started working on this I would like to take a stab at it.;;;","27/Apr/15 17:26;jjkoshy;I'm a bit wary of the watch approach. I believe at the last google hangout we decided against doing this (as part of the KIP-4 discussion). A number of people had dropped off at that point - I believe we were going with a broker-level metadata request that can return ISR information for partitions that it leads. [~abiletskyi] can you confirm? I didn't see it in the summary notes so I could be wrong.;;;","27/Apr/15 19:24;abiletskyi;[~jjkoshy] Yes, it appears that topic commands don't require ISR information so it was proposed to remove it at all from the TopicMetadataRequest. There was an idea to create some sort of BrokerMetadataRequest which will include correct topic metadata but since it's not related to KIP-4 directly it won't be a part of it. So everyone is welcome to create a separate KIP for it. Atleast to this conclusion we came last time.;;;","28/Apr/15 04:02;nehanarkhede;[~jjkoshy] That would work too but looks like [~abiletskyi] is suggesting that it is not included as part of KIP-4. Maybe we can have whoever picks this JIRA discuss this change as part of a separate KIP?;;;","09/May/15 00:05;singhashish;[~nehanarkhede]/ [~jjkoshy] I have put together [KIP-24|https://cwiki.apache.org/confluence/display/KAFKA/KIP-24+-+Remove+ISR+information+from+TopicMetadataRequest+and+add+broker+level+metadata+request]. I will need a bit more information on what we decided for BrokerMetadataRequest, before I can update ""Public Interfaces"" section of the KIP.

Is my understanding correct that below is the plan we agreed upon. Below excerpt is actually from the KIP.

{quote}
It is proposed to remove ISR information from the TopicMetadataRequest. However, a new request, BrokerMetadataRequest, is proposed to be added. The new request will include ISR information for all the partitions that the broker leads.
{quote};;;","28/May/15 22:24;singhashish;[~jjkoshy] pinging you for your confirmation on this.;;;","28/May/15 23:10;jjkoshy;Hi [~singhashish] - sorry I missed your pings. Yes that is the approach we are planning to take. i.e., remove ISR from TMR. As mentioned in KIP-4 the ISR will be removed in v1 of TMR.;;;","28/May/15 23:38;singhashish;Thanks [~jjkoshy]! I will update the KIP accordingly.;;;","28/May/15 23:56;junrao;There is actually a reasonable use case of ISR in KAFKA-2225. Basically, for economical reasons, we may want to let a consumer fetch from a replica in ISR that's in the same zone. In order to support that, it will be convenient to have TMR return the correct ISR for the consumer to choose.

Implementation wise, one way to address the concern with too many watchers is to do sth similar to changing topic configs. Basically, when the leader changes the isr, in addition to writing the new isr in the partition state in ZK, it also writes the change as a sequential node under a new isrChangeNotification path in ZK. The controller listens to child changes in the isrChangeNotification path. On child change, the controller reads the new isr and broadcasts it through an UpdateMetadataRequest to every broker.;;;","29/May/15 00:14;jjkoshy;Jun - that is a good point. That sounds like a good approach to address the concern with watcher counts. Another way is to just allow brokers to send an equivalent update metadata (or similar) request to the controller to notify it of an ISR change - or even allow leaders to broadcast update metadata requests for ISR changes. We currently don't allow this, but maybe we should consider a generic broker-broker communication component. Given the use-case that Theo raised on the list yesterday, it appears we may want to keep the ISR even in TMR v1. It may make sense to discuss this at an upcoming hangout especially since it affects KIP-4.;;;","29/May/15 00:42;junrao;It's probably better to always let the controller propagate metadata changes to the brokers. If the metadata change can be sent from both the controller and other brokers, we need additional logic to reason about ordering.

Having the broker send the change to the controller is possible. The implication is that there is another thread that can exercise the controller logic, instead of just the ZK watcher thread. So, we may need to deal with more concurrency issues.;;;","29/May/15 00:55;jjkoshy;One minor issue with depending on the TMR for KAKFA-2225 even if we fix this is that the consumer would need to periodically refresh its metadata in case the ISR changes after it starts reading from a follower in ISR.

Another approach for KAFKA-2225 is to add the ISR information to the fetch response. The followers will then have the current ISR information and so will the consumers. There are at least two concerns though: first, it depends on a live replica fetcher thread; second, it's a bit hacky to add ISR to fetch response as it is more associated with metadata.
;;;","29/May/15 01:08;junrao;Well, in that approach, you still have the problem on the very first fetch request. If ISR is not returned in TMR, the first fetch request has to go to the leader. Then the consumer has to switch to another broker on a subsequent request, which seems more complicated.

I am not sure if we need to rely on periodic metadata refresh to detect whether a replica is out of sync. Basically, as long as the fetch offset is less than HW, the replica can serve the request. If the fetch offset is larger than HW (an indication that the replica is out of sync), the consumer will get an OffsetOutOfRangeException and has to refresh the metadata and pick a new broker to fetch from.;;;","29/May/15 15:40;singhashish;[~junrao] can we add this to the agenda of next KIP hangout?;;;","29/May/15 17:11;junrao;Yes.;;;","02/Jun/15 17:02;jjkoshy;You may still want to switch back to a replica in the same (or nearer) availability zone right after it catches up and rejoins the ISR. Also, I'm not sure about the offset going out of range while fetching from a given replica. i.e., the fetcher will just fetch with an offset larger than the last fetched chunk. It may occur if you were switching between replicas though but that would only be if you were switching to a replica out of the ISR.;;;","02/Jun/15 17:46;junrao;[~joel koshy], yes, that's a good point. If we want to switch back to the closest replica for consumption, we do need to refresh the metadata periodically to check if the closest replica is back in ISR. We also need to handle OffsetOutOfRangeException a bit differently. If the consumer gets OffsetOutOfRangeException because the replica is out of sync, we want to switch to another in-sync replica instead of resetting the offset. One way to do that is the following protocol.

1. Get topic metadata.
2. Pick the ""closest"" in-sync replica to issue fetch requests.
3. On an OffsetOutOfRangeException, get the smallest/largest offset. If the fetch offset is within the range, go back to step 1 to switch to a different in-sync replica. Otherwise, go through the offset reset logic.
4. Periodically refresh the metadata. Switch to the ""closest"" in-sync replica for fetching, if needed.

;;;","02/Jun/15 21:36;guozhang;One note is that with the new consumer, one has to periodically refresh metadata anyways for wildcard subscriptions.;;;","03/Jun/15 16:13;jjkoshy;Follow-up from the KIP hangout. Side-note: this and most of the above comments are actually implementation details for KAFKA-2225. This is relevant here only because we are considering keeping vs. removing the ISR field.

I do think it is possible to implement KAFKA-2225 without ISR support either in metadata or the fetch response. The fetch response already contains HW. So the consumer can watch its fetch offset and the current HW (from the last fetch response). If the fetchOffset << HW but if the fetch response size is smaller than the requested bytes and the highest offset in the response is << HW then the consumer knows that the follower that it is fetching from is lagging behind (especially if this difference increases in successive fetches). The main caveat with this is that it depends on the replica having a live replica fetcher. The other issue is that the consumer needs to have its own definition of what it takes to deem a replica as out of sync (since the replica lag time config is server-side). The other observation is that ISR is a highly relevant and useful field in the topic metadata response. I would be in favor of keeping it in the TMR and just having the consumer refresh the topic metadata periodically to keep itself informed of ISR changes.;;;","08/Jun/15 17:31;aauradkar;[~jjkoshy] [~junrao] KAFKA-2225, even if we leave the ISR in the TopicMetadataRequest, how do the consumers detect which of the replicas in ISR to fetch from right? The consumers need to know which ""zone"" each of the brokers live in and their own in order to fetch from the closest replica (which mitigates with the bandwidth issues described in 2225).

Couple of options:
1. Return it in BrokerMetadataRequest (KIP-24)
2. Piggyback it along with the ISR field in TMR. i.e. isr : {0: ""zone1"", 1: ""zone2""}

If we choose to do (2), then the TMR will evolve anyway.;;;","08/Jun/15 18:05;junrao;Yes, we need some kind of notion of zones for both the brokers and the clients. Each broker and each client (producer/consumer) need a configuration for which zone it belongs to. It's probably simpler to just return the zone info in TMR. We will need to evolve TMR, but that can probably be done separately from fixing the ISR in TMR. We probably should move these design discussions to KAFKA-2225 itself.;;;","08/Jun/15 19:47;gwenshap;By ""zones"" do we mean rack-awareness? Or more general locality notion?
Sounds like something that may need its own JIRA and design.;;;","08/Jun/15 20:10;becket_qin;I agree with [~gwenshap], it sounds this deserves a KIP.;;;","08/Jun/15 23:27;junrao;Yes, perhaps some kind of more general locality could be useful. That can be done in a separate jira.

Here, we just want to figure out whether it's useful to maintain ISR in TMR.

[~jjkoshy], another issue without ISR is that initially a client will have no idea which replica is in sync and can only guess.;;;","11/Jun/15 00:45;singhashish;[~junrao] and [~jjkoshy], correct me if my understanding is wrong, but I think we agreed on keeping ISR info in TMR and below mentioned approach is our preference.

{quote}
When the leader changes the isr, in addition to writing the new isr in the partition state in ZK, it also writes the change as a sequential node under a new isrChangeNotification path in ZK. The controller listens to child changes in the isrChangeNotification path. On child change, the controller reads the new isr and broadcasts it through an UpdateMetadataRequest to every broker.
{quote}

Now that we want to keep ISR as part of TMR, do we still need a new BrokerMetadataRequest?;;;","11/Jun/15 19:57;jjkoshy;[~singhashish] - yes that is a good summary. BrokerMetadataRequest - probably yes, but that is now orthogonal.;;;","11/Jun/15 21:20;singhashish;[~jjkoshy] thanks for confirming. I will get started on the suggested solution for this issue. We will probably need a separate JIRA for KIP-24.;;;","24/Jun/15 05:09;singhashish;Created reviewboard https://reviews.apache.org/r/35820/
 against branch trunk;;;","24/Jun/15 05:15;singhashish;[~junrao], [~jjkoshy], [~nehanarkhede], [~gwenshap] just uploaded a patch to fix this. Apart from the changes suggested above, I just had to update controller's leader and isr cache before sending update metadata request. Tested it on a 3 node kafka cluster and the patch resolves the issue.;;;","02/Jul/15 00:23;singhashish;Updated reviewboard https://reviews.apache.org/r/35820/
 against branch trunk;;;","07/Jul/15 05:04;singhashish;Updated reviewboard https://reviews.apache.org/r/35820/
 against branch trunk;;;","07/Jul/15 16:49;junrao;Thanks for the latest patch. +1. Committed to trunk after fixing the last minor issue in the RB.;;;","04/Aug/15 22:26;junrao;[~singhashish], forgot to mention this earlier. Could you include the ZK structure change in https://cwiki.apache.org/confluence/display/KAFKA/Kafka+data+structures+in+Zookeeper ? Thanks,;;;","05/Aug/15 04:45;singhashish;[~junrao] done!;;;","05/Aug/15 05:31;becket_qin;[~ashishujjain] [~junrao] [~jjkoshy], I found some issue when trying to deploy the latest trunk with this patch. 

The problem is that during a rolling bounce in a cluster the ISR state propagation flooded the controller to broker traffic, the controlled shutdown of one broker takes about 1.5 hour to finish for a 30 node cluster.

Also, even during a clean cluster startup, there are more than 70,000 ISR change ephemeral path got created.

I feel we need to either find another way to propagate ISR change other than using zookeeper, or at very least we need to throttle the propagation rate to, say, once every 10 seconds.

I'm going to create another ticket to address the issue. Hopefully the fix should be quick. If the fix takes some time, do we consider reverting this patch and rework on it?;;;","05/Aug/15 05:44;singhashish;[~becket_qin] that is a valid concern. I guess what you are suggesting makes sense, controller should wait for a configurable time before sending out Update Metadata Request to brokers.;;;","05/Aug/15 05:50;becket_qin;Thanks for the quick response [~ashishujjain]. I created KAFKA-2406 and will submit a patch tonight. This has become a blocker for us now so I want to get it fixed ASAP. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple Unit Test failures with new producer,KAFKA-1366,12707029,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,07/Apr/14 21:18,08/Apr/14 17:15,14/Jul/23 05:39,08/Apr/14 17:15,,,,,,,,,,,,,,0,,,,"Current known failed tests include

1. Log4jAppenderTest
2. ProducerFailureHandlingTest

These tests failed mainly due to the config changes in KAFKA-1337, some others due to the async mode of the new producer.",,guozhang,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/14 23:00;guozhang;KAFKA-1366.patch;https://issues.apache.org/jira/secure/attachment/12639087/KAFKA-1366.patch","07/Apr/14 23:12;guozhang;KAFKA-1366_2014-04-07_16:12:00.patch;https://issues.apache.org/jira/secure/attachment/12639089/KAFKA-1366_2014-04-07_16%3A12%3A00.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385352,,,Tue Apr 08 17:15:33 UTC 2014,,,,,,,,,,"0|i1udwv:",385619,,,,,,,,,,,,,,,,,,,,"07/Apr/14 23:00;guozhang;Created reviewboard https://reviews.apache.org/r/20109/
 against branch origin/trunk;;;","07/Apr/14 23:12;guozhang;Updated reviewboard https://reviews.apache.org/r/20109/
 against branch origin/trunk;;;","08/Apr/14 17:15;nehanarkhede;Committed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Second Manual preferred replica leader election command always fails,KAFKA-1365,12707026,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,nehanarkhede,rberdeen,rberdeen,07/Apr/14 20:42,27/May/14 18:42,14/Jul/23 05:39,18/Apr/14 01:32,0.8.1,,,0.8.1.1,,,,,,,controller,tools,,2,,,,"After running kafka-preferred-replica-election.sh once, a second run will fail with ""Preferred replica leader election currently in progress for ..."".

The /admin/preferred_replica_election key is never deleted from ZooKeeper, because the ""isTriggeredByAutoRebalance"" parameter to onPreferredReplicaElection (https://github.com/apache/kafka/blob/0ffec142a991849833d9767be07e895428ccaea1/core/src/main/scala/kafka/controller/KafkaController.scala#L614) is used incorrectly. In the automatic case (https://github.com/apache/kafka/blob/0ffec142a991849833d9767be07e895428ccaea1/core/src/main/scala/kafka/controller/KafkaController.scala#L1119), it is set to false. In the manual case (https://github.com/apache/kafka/blob/0ffec142a991849833d9767be07e895428ccaea1/core/src/main/scala/kafka/controller/KafkaController.scala#L1266) the parameter is not passed, so it defaults to true.",,adenysenko,balaji.seshadri,balaji.seshadri@dish.com,guozhang,jjkoshy,rberdeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1380,,,,"16/Apr/14 18:23;guozhang;KAFKA-1365.patch;https://issues.apache.org/jira/secure/attachment/12640515/KAFKA-1365.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385349,,,Fri Apr 18 15:14:15 UTC 2014,,,,,,,,,,"0|i1udw7:",385616,,,,,,,,,,,,,,,,,,,,"14/Apr/14 21:29;jjkoshy;Thanks for filing this issue and for debugging it.  We will fix this in
0.8.2. The automatic preferred replica leader election feature has other
known issues (e.g., KAFKA-1305) that we won't get to in 0.8.1.1 so it should
turned off (default) until 0.8.2.

;;;","15/Apr/14 19:01;balaji.seshadri;I can help fix this as its happening for us in DISH Upgrade,please direct me accordingly.

Please see email from Bob.

-----Original Message-----
From: Bello, Bob [mailto:Bob.Bello@dish.com] 
Sent: Tuesday, April 15, 2014 10:00 AM
To: users@kafka.apache.org
Cc: Bello, Bob
Subject: RE: Kafka upgrade 0.8.0 to 0.8.1 - kafka-preferred-replica-election failure

I performed another test. I build a single Kafka 0.8.1 Broker with a single ZK instance. Brand new, no topics.

Upon start up of the Kafka broker, the zookeeper /admin node only contains ""/admin/delete_topics"".

Even without creating a topic, I run a perfered replica election, and it's successful. After the run, I check the node for /admin/preferred_replica_election and it exists.

[zk: tm1mwwm001:2181(CONNECTED) 0] get /admin/preferred_replica_election {""version"":1,""partitions"":[]} cZxid = 0x19 ctime = Tue Apr 15 09:53:22 MDT 2014 mZxid = 0x19 mtime = Tue Apr 15 09:53:22 MDT 2014 pZxid = 0x19 cversion = 0 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 29 numChildren = 0

If I run the election again, I get the following error (the same error as my original post).


Failed to start preferred replica election
kafka.common.AdminCommandFailedException: Admin command failed
        at kafka.admin.PreferredReplicaLeaderElectionCommand.moveLeaderToPreferredReplica(PreferredReplicaLeaderElectionCommand.scala:115)
        at kafka.admin.PreferredReplicaLeaderElectionCommand$.main(PreferredReplicaLeaderElectionCommand.scala:60)
        at kafka.admin.PreferredReplicaLeaderElectionCommand.main(PreferredReplicaLeaderElectionCommand.scala)
Caused by: kafka.admin.AdminOperationException: Preferred replica leader election currently in progress for Set(). Aborting operation
        at kafka.admin.PreferredReplicaLeaderElectionCommand$.writePreferredReplicaElectionData(PreferredReplicaLeaderElectionCommand.scala:101)
        at kafka.admin.PreferredReplicaLeaderElectionCommand.moveLeaderToPreferredReplica(PreferredReplicaLeaderElectionCommand.scala:113)
        ... 2 more


The zookeeper log shows the following:

2014-04-15 09:53:44 INFO server.PrepRequestProcessor - Got user-level KeeperException when processing sessionid:0x1456615e7770006 type:create cxid:0x2 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NodeExists for /admin/preferred_replica_election

It looks like the preferred election process is not removing the node after completion.



Bob Bello
Middleware Applications Administrator
Direct: 720-514-6605
email: bob.bello@dish.com

;;;","16/Apr/14 18:06;jjkoshy;Adding back 0.8.1.1;;;","16/Apr/14 18:23;guozhang;Created reviewboard https://reviews.apache.org/r/20424/
 against branch origin/0.8.1;;;","16/Apr/14 18:24;guozhang;Has double checked that this issue has been fixed in trunk. Submitted a patch for 0.8.1 only.;;;","18/Apr/14 01:32;jjkoshy;Thanks for the patch. Committed to 0.8.1

I tested locally as well, but [~balaji.seshadri] do you want to give this a spin and confirm?;;;","18/Apr/14 03:06;balaji.seshadri@dish.com;We stopped the upgrade so we will not be able to test this.

Thanks,

Balaji

;;;","18/Apr/14 15:14;balaji.seshadri@dish.com;We will test the controller issue one in dev ,we will test this also at that time.

Sorry for that.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReplicaManagerTest hard-codes log dir,KAFKA-1364,12707003,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,junrao,junrao,07/Apr/14 18:47,09/Apr/14 21:50,14/Jul/23 05:39,09/Apr/14 21:50,,,,0.8.2.0,,,,,,,core,,,1,,,,"Saw the following unit test failure. This is probably due to that the hard-coded log.dir conflicts with the default config when running a standalone sack.

kafka.server.ReplicaManagerTest > testHighwaterMarkDirectoryMapping FAILED
    java.lang.IllegalArgumentException: requirement failed: Corrupt index found, index file (/tmp/kafka-logs/test-topic-1/00000000000000000000.index) has non-zero size but the last offset is 0 and the base offset is 0
        at scala.Predef$.require(Predef.scala:145)
        at kafka.log.OffsetIndex.sanityCheck(OffsetIndex.scala:352)
        at kafka.log.Log$$anonfun$loadSegments$5.apply(Log.scala:159)
        at kafka.log.Log$$anonfun$loadSegments$5.apply(Log.scala:158)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:474)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
        at scala.collection.JavaConversions$JCollectionWrapper.foreach(JavaConversions.scala:495)
        at kafka.log.Log.loadSegments(Log.scala:158)
        at kafka.log.Log.<init>(Log.scala:64)
        at kafka.server.ReplicaManagerTest.testHighwaterMarkDirectoryMapping(ReplicaManagerTest.scala:43)
",,adenysenko,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/14 20:56;guozhang;KAFKA-1364.patch;https://issues.apache.org/jira/secure/attachment/12639262/KAFKA-1364.patch","09/Apr/14 17:24;guozhang;KAFKA-1364_2014-04-09_10:24:24.patch;https://issues.apache.org/jira/secure/attachment/12639436/KAFKA-1364_2014-04-09_10%3A24%3A24.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385326,,,Wed Apr 09 21:50:23 UTC 2014,,,,,,,,,,"0|i1udr3:",385593,,,,,,,,,,,,,,,,,,,,"08/Apr/14 20:56;guozhang;Created reviewboard https://reviews.apache.org/r/20128/
 against branch origin/trunk;;;","09/Apr/14 17:24;guozhang;Updated reviewboard https://reviews.apache.org/r/20128/
 against branch origin/trunk;;;","09/Apr/14 21:50;junrao;Thanks for the patch. +1 Committed to trunk. I don't think this needs to be in 0.8.1.1 since it's not a blocker.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testTopicConfigChangesDuringDeleteTopic hangs,KAFKA-1363,12706999,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tnachen,junrao,junrao,07/Apr/14 18:14,12/Apr/14 20:09,14/Jul/23 05:39,12/Apr/14 20:08,0.8.1,,,0.8.1.1,,,,,,,core,,,0,,,,"Saw the following deadlock during shutting down the delete topic manager.

""delete-topics-thread"" prio=10 tid=0x00007fd50c003800 nid=0x7d9 waiting on condition [0x00007fd53d160000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000006b41d6318> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
        at kafka.utils.Utils$.inLock(Utils.scala:535)
        at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:363)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)

""Test worker"" prio=10 tid=0x00007fd578928800 nid=0x763d waiting on condition [0x00007fd570a87000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000006b5b6f580> (a java.util.concurrent.CountDownLatch$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)
        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:207)
        at kafka.utils.ShutdownableThread.shutdown(ShutdownableThread.scala:36)
        at kafka.controller.TopicDeletionManager.shutdown(TopicDeletionManager.scala:100)
        at kafka.controller.KafkaController$$anonfun$onControllerResignation$1.apply$mcV$sp(KafkaController.scala:345)
        at kafka.controller.KafkaController$$anonfun$onControllerResignation$1.apply(KafkaController.scala:341)
        at kafka.controller.KafkaController$$anonfun$onControllerResignation$1.apply(KafkaController.scala:341)
        at kafka.utils.Utils$.inLock(Utils.scala:537)
        at kafka.controller.KafkaController.onControllerResignation(KafkaController.scala:341)
        at kafka.controller.KafkaController$$anonfun$shutdown$1.apply$mcV$sp(KafkaController.scala:648)
        at kafka.controller.KafkaController$$anonfun$shutdown$1.apply(KafkaController.scala:646)
        at kafka.controller.KafkaController$$anonfun$shutdown$1.apply(KafkaController.scala:646)
        at kafka.utils.Utils$.inLock(Utils.scala:537)
        at kafka.controller.KafkaController.shutdown(KafkaController.scala:646)
        at kafka.server.KafkaServer$$anonfun$shutdown$9.apply$mcV$sp(KafkaServer.scala:242)
        at kafka.utils.Utils$.swallow(Utils.scala:166)
        at kafka.utils.Logging$class.swallowWarn(Logging.scala:92)
        at kafka.utils.Utils$.swallowWarn(Utils.scala:45)
        at kafka.utils.Logging$class.swallow(Logging.scala:94)
        at kafka.utils.Utils$.swallow(Utils.scala:45)
        at kafka.server.KafkaServer.shutdown(KafkaServer.scala:242)
        at kafka.admin.DeleteTopicTest$$anonfun$testTopicConfigChangesDuringDeleteTopic$1.apply(DeleteTopicTest.scala:362)
        at kafka.admin.DeleteTopicTest$$anonfun$testTopicConfigChangesDuringDeleteTopic$1.apply(DeleteTopicTest.scala:362)
        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
        at scala.collection.immutable.List.foreach(List.scala:45)
        at kafka.admin.DeleteTopicTest.testTopicConfigChangesDuringDeleteTopic(DeleteTopicTest.scala:362)
",,junrao,nehanarkhede,tnachen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/14 01:26;tnachen;KAFKA-1363.patch;https://issues.apache.org/jira/secure/attachment/12639506/KAFKA-1363.patch","09/Apr/14 00:50;tnachen;KAFKA-1363.patch;https://issues.apache.org/jira/secure/attachment/12639315/KAFKA-1363.patch","09/Apr/14 00:52;tnachen;KAFKA-1363_2014-04-08_17:52:17.patch;https://issues.apache.org/jira/secure/attachment/12639316/KAFKA-1363_2014-04-08_17%3A52%3A17.patch","09/Apr/14 18:38;tnachen;KAFKA-1363_2014-04-09_11:38:09.patch;https://issues.apache.org/jira/secure/attachment/12639446/KAFKA-1363_2014-04-09_11%3A38%3A09.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385322,,,Sat Apr 12 20:08:47 UTC 2014,,,,,,,,,,"0|i1udq7:",385589,,,,,,,,,,,,,,,,,,,,"09/Apr/14 00:50;tnachen;Created reviewboard https://reviews.apache.org/r/20143/
 against branch origin/trunk;;;","09/Apr/14 00:52;tnachen;Updated reviewboard https://reviews.apache.org/r/20143/
 against branch origin/trunk;;;","09/Apr/14 18:38;tnachen;Updated reviewboard https://reviews.apache.org/r/20143/
 against branch origin/trunk;;;","10/Apr/14 01:26;tnachen;Created reviewboard https://reviews.apache.org/r/20187/
 against branch origin/0.8.1;;;","12/Apr/14 03:32;junrao;Thanks for the patch. +1 and committed to trunk.;;;","12/Apr/14 20:08;nehanarkhede;Checked into 0.8.1 as well;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Publish sources and javadoc jars,KAFKA-1362,12706941,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jjkoshy,sslavic,sslavic,07/Apr/14 12:37,09/Mar/15 08:43,14/Jul/23 05:39,18/Apr/14 20:41,0.8.1,,,0.8.1.1,,,,,,,packaging,,,1,build,,,Currently just binaries jars get published on Maven Central (see http://repo1.maven.org/maven2/org/apache/kafka/kafka_2.10/0.8.1/ ). Please also publish sources and javadoc jars.,,bcalmac,dwegener,ianfriedman,jjkoshy,joestein,junrao,sandris,sslavic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1388,,,,,,,,,,,KAFKA-1502,,KAFKA-1171,,,,KAFKA-1380,,KAFKA-1399,,"15/Apr/14 19:39;jjkoshy;KAFKA-1362.patch;https://issues.apache.org/jira/secure/attachment/12640319/KAFKA-1362.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,385264,,,Mon Sep 15 03:48:20 UTC 2014,,,,,,,,,,"0|i1uddb:",385531,,,,,,,,,,,,,,,,,,,,"07/Apr/14 14:26;sslavic;Assigned issue to packaging component;;;","07/Apr/14 14:30;sslavic;This bug is caused by new and incomplete migration to gradle build, introduced in 0.8.1 through KAFKA-1171.
Before this 0.8.0 had sources and javadoc jars published (see [here|http://repo1.maven.org/maven2/org/apache/kafka/kafka_2.10/0.8.0/]);;;","15/Apr/14 16:59;jjkoshy;Will upload a patch today.;;;","15/Apr/14 19:39;jjkoshy;Created reviewboard https://reviews.apache.org/r/20376/
 against branch origin/0.8.1;;;","15/Apr/14 19:40;jjkoshy;The above patch should apply to trunk as well, but I will rebase if necessary. The docsJar_2_8_2 task fails for some reason, but all the other tasks pass.;;;","16/Apr/14 02:42;joestein;./gradlew srcJar worked great 

however the docsJar_2_8_2  error is stopping ./gradlew releaseTarGzAll from working :( trying to get a stacktrace out of it



;;;","16/Apr/14 03:40;joestein;i ran with stacktrace, debug and info https://gist.github.com/joestein/10803377#file-kafka-1362-L296 googled around a bit looks like a bug in 2.8.1 scaladoc itself https://issues.scala-lang.org/browse/SI-4284 but fixed in 2.9.0 not sure yet if there is a workaround or what exactly in the code is causing this that we can change to subvert it.;;;","16/Apr/14 07:34;sslavic;Consider dropping Scala 2.8.x support. I've just created a separate request for that (see KAFKA-1399);;;","16/Apr/14 17:13;jjkoshy;[~joestein] I think I ran across a similar bug report earlier, but couldn't explain why that would affect the 2.8.2 scaladoc build.;;;","16/Apr/14 17:14;jjkoshy;We could just get rid of 2_8_2. We would still have 2_8_0;;;","17/Apr/14 22:03;joestein;2.8.0 is so much older and worse (arguably) and not used if we drop support for 2.8.2 we might as well do it for 2.8.0, we should have some consensus to-do that though IMHO it falls under ""release plan"" and would require a Lazy majority to pass.  ;;;","17/Apr/14 22:48;jjkoshy;We could, but it's just that there are a couple of users on 2.8 still - including LinkedIn. Since only the docs jar task is failing, I thought it would be sufficient to just remove 2.8.2 from docsJarAll.;;;","17/Apr/14 22:58;joestein;ok;;;","18/Apr/14 20:40;jjkoshy;Committed to both 0.8.1 and trunk

Filed KAFKA-1406 to address the warnings when generating javadocs/scaladocs.
;;;","07/Jun/14 12:45;dwegener;kafka_2.10-0.8.1.1-sources.jar in maven central ist empty (see http://search.maven.org/#artifactdetails%7Corg.apache.kafka%7Ckafka_2.10%7C0.8.1.1%7Cjar). Is this intentional?;;;","21/Aug/14 17:03;ianfriedman;Echoing the comment about the sources jar being empty. Can we get a resolution on this please? [~jjkoshy]?;;;","25/Aug/14 17:35;bcalmac;This issue should be reopened. The sources jar for 0.8.1.1 on Maven Central is empty.;;;","15/Sep/14 03:48;junrao;The maven issue should be fixed in kafka-1502.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker throws exception when reconnecting to zookeeper,KAFKA-1358,12706332,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tnachen,tnachen,tnachen,03/Apr/14 00:20,27/May/14 18:43,14/Jul/23 05:39,04/Apr/14 15:13,0.8.1,,,0.8.1.1,,,,,,,,,,0,,,,"A non-controller broker currently if zk session expires and re-established calls onControllerResignation even though it may not be the controller.

The result is that the broker gets exception like this: 

java.lang.NullPointerException
	at kafka.controller.KafkaController$$anonfun$onControllerResignation$1.apply$mcV$sp(KafkaController.scala:340)
	at kafka.controller.KafkaController$$anonfun$onControllerResignation$1.apply(KafkaController.scala:337)
	at kafka.controller.KafkaController$$anonfun$onControllerResignation$1.apply(KafkaController.scala:337)
	at kafka.utils.Utils$.inLock(Utils.scala:538)
	at kafka.controller.KafkaController.onControllerResignation(KafkaController.scala:337)
	at kafka.controller.KafkaController$SessionExpirationListener$$anonfun$handleNewSession$1.apply$mcZ$sp(KafkaController.scala:1068)
	at kafka.controller.KafkaController$SessionExpirationListener$$anonfun$handleNewSession$1.apply(KafkaController.scala:1067)
	at kafka.controller.KafkaController$SessionExpirationListener$$anonfun$handleNewSession$1.apply(KafkaController.scala:1067)
	at kafka.utils.Utils$.inLock(Utils.scala:538)
	at kafka.controller.KafkaController$SessionExpirationListener.handleNewSession(KafkaController.scala:1067)
	at org.I0Itec.zkclient.ZkClient$4.run(ZkClient.java:472)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
",,junrao,tnachen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1380,,,,"03/Apr/14 00:34;tnachen;KAFKA-1358.patch;https://issues.apache.org/jira/secure/attachment/12638383/KAFKA-1358.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,384655,,,Fri Apr 04 15:13:45 UTC 2014,,,,,,,,,,"0|i1u9mn:",384923,,,,,,,,,,,,,,,,,,,,"03/Apr/14 00:34;tnachen;Created reviewboard https://reviews.apache.org/r/19972/
 against branch origin/0.8.1;;;","03/Apr/14 00:36;tnachen;Looks like we already have this guard in trunk, adding this to the 0.8.1 branch;;;","04/Apr/14 15:13;junrao;Committed to 0.8.1 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Topic metadata requests takes too long to process,KAFKA-1356,12706066,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tnachen,tnachen,tnachen,02/Apr/14 00:09,21/Apr/14 17:57,14/Jul/23 05:39,08/Apr/14 17:49,0.8.1,,,,,,,,,,,,,0,,,,"Currently we're seeing slow response times in handling get topic metadata requests.

Local testing shows that even locally it takes 300 avg ms to respond, even though it's not doing any IO operations.",,jjkoshy,junrao,nehanarkhede,tnachen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1380,,,,"17/Apr/14 17:46;jjkoshy;KAFKA-1356.patch;https://issues.apache.org/jira/secure/attachment/12640661/KAFKA-1356.patch","17/Apr/14 17:41;jjkoshy;KAFKA-1356.patch;https://issues.apache.org/jira/secure/attachment/12640660/KAFKA-1356.patch","17/Apr/14 17:39;jjkoshy;KAFKA-1356.patch;https://issues.apache.org/jira/secure/attachment/12640659/KAFKA-1356.patch","11/Apr/14 21:03;junrao;KAFKA-1356.patch;https://issues.apache.org/jira/secure/attachment/12639866/KAFKA-1356.patch","11/Apr/14 07:41;tnachen;KAFKA-1356.patch;https://issues.apache.org/jira/secure/attachment/12639751/KAFKA-1356.patch","10/Apr/14 01:48;tnachen;KAFKA-1356.patch;https://issues.apache.org/jira/secure/attachment/12639509/KAFKA-1356.patch","02/Apr/14 20:44;tnachen;KAFKA-1356.patch;https://issues.apache.org/jira/secure/attachment/12638331/KAFKA-1356.patch","03/Apr/14 01:39;tnachen;KAFKA-1356_2014-04-02_18:39:36.patch;https://issues.apache.org/jira/secure/attachment/12638394/KAFKA-1356_2014-04-02_18%3A39%3A36.patch","04/Apr/14 21:40;tnachen;KAFKA-1356_2014-04-04_14:40:18.patch;https://issues.apache.org/jira/secure/attachment/12638773/KAFKA-1356_2014-04-04_14%3A40%3A18.patch","05/Apr/14 00:45;tnachen;KAFKA-1356_2014-04-04_17:45:37.patch;https://issues.apache.org/jira/secure/attachment/12638812/KAFKA-1356_2014-04-04_17%3A45%3A37.patch","06/Apr/14 08:46;tnachen;KAFKA-1356_2014-04-06_01:45:47.patch;https://issues.apache.org/jira/secure/attachment/12638903/KAFKA-1356_2014-04-06_01%3A45%3A47.patch","08/Apr/14 08:38;tnachen;KAFKA-1356_2014-04-08_01:38:23.patch;https://issues.apache.org/jira/secure/attachment/12639159/KAFKA-1356_2014-04-08_01%3A38%3A23.patch","11/Apr/14 18:39;junrao;KAFKA-1356_2014-04-11_11:39:10.patch;https://issues.apache.org/jira/secure/attachment/12639851/KAFKA-1356_2014-04-11_11%3A39%3A10.patch",,,,,,,,,,,,,,,13.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,384389,,,Mon Apr 21 17:57:49 UTC 2014,,,,,,,,,,"0|i1u7zz:",384657,,,,,,,,,,,,,,,,,,,,"02/Apr/14 20:44;tnachen;Created reviewboard https://reviews.apache.org/r/19957/
 against branch origin/trunk;;;","03/Apr/14 01:39;tnachen;Updated reviewboard https://reviews.apache.org/r/19957/
 against branch origin/trunk;;;","04/Apr/14 21:40;tnachen;Updated reviewboard https://reviews.apache.org/r/19957/
 against branch origin/trunk;;;","05/Apr/14 00:45;tnachen;Updated reviewboard https://reviews.apache.org/r/19957/
 against branch origin/trunk;;;","06/Apr/14 08:46;tnachen;Updated reviewboard https://reviews.apache.org/r/19957/
 against branch origin/trunk;;;","08/Apr/14 01:31;jjkoshy;Would you mind uploading a rebased patch? (Sorry I think the conflict is due to my patch in KAFKA-1355).;;;","08/Apr/14 08:38;tnachen;Updated reviewboard https://reviews.apache.org/r/19957/
 against branch origin/trunk;;;","08/Apr/14 17:49;nehanarkhede;Thanks for the patches! Committed to trunk;;;","10/Apr/14 01:48;tnachen;Created reviewboard https://reviews.apache.org/r/20190/
 against branch origin/trunk;;;","11/Apr/14 07:41;tnachen;Created reviewboard https://reviews.apache.org/r/20252/
 against branch origin/0.8.1;;;","11/Apr/14 18:39;junrao;Updated reviewboard  against branch origin/trunk;;;","11/Apr/14 18:41;junrao;Upload another patch for the following fix. RB doesn't work since I wasn't the original creator.

Fix the way we update controller epoch during UpdateMetadataRequest since the check and set is not atomic.;;;","11/Apr/14 21:03;junrao;Created reviewboard https://reviews.apache.org/r/20272/
 against branch origin/trunk;;;","17/Apr/14 17:39;jjkoshy;Created reviewboard  against branch origin/0.8.1;;;","17/Apr/14 17:41;jjkoshy;Created reviewboard  against branch origin/0.8.1;;;","17/Apr/14 17:46;jjkoshy;(For some reason, the patch-review tool isn't letting me upload via RB - so I'm attaching an incremental patch for review);;;","17/Apr/14 17:47;jjkoshy;Well - it did attach it to the jira: https://issues.apache.org/jira/secure/attachment/12640660/KAFKA-1356.patch (but no RB);;;","18/Apr/14 18:23;junrao;Joel,

+1 on  https://issues.apache.org/jira/secure/attachment/12640660/KAFKA-1356.patch 

Thanks,;;;","18/Apr/14 18:37;jjkoshy;Thanks for the review. Committed the follow-up patch to 0.8.1;;;","21/Apr/14 17:57;junrao;Joel,

I overlooked one issue. With your latest change, if a topic doesn't exist and auto topic creation is disabled, we won't return any result for the requested topic. Instead, we should return a TopicNotExist error code.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce/optimize update metadata requests sent during leader election,KAFKA-1355,12705833,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,jjkoshy,jjkoshy,01/Apr/14 01:25,27/May/14 18:42,14/Jul/23 05:39,18/Apr/14 21:04,0.8.1,,,0.8.1.1,,,,,,,,,,1,,,,"This is part of the investigation into slow shutdowns in 0.8.1. While
logging contributes to bulk of the regression, this one also adds
quite a bit of overhead:

In addLeaderAndIsrRequest (called for every partition that is led by the
broker being shut down) we also add an UpdateMetadataRequest - each call to
addUpdateMetadataRequests does two traversals over _all_ (global)
partitions. I think it should be straightforward to optimize this a bit.

Marking as critical, since it is not as big an overhead as the logging.
",,adenysenko,aozeritsky,jjkoshy,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1380,,,,"10/Apr/14 21:02;jjkoshy;KAFKA-1355.patch;https://issues.apache.org/jira/secure/attachment/12639653/KAFKA-1355.patch","04/Apr/14 20:48;jjkoshy;KAFKA-1355_2014-04-04_13:48:34.patch;https://issues.apache.org/jira/secure/attachment/12638761/KAFKA-1355_2014-04-04_13%3A48%3A34.patch","04/Apr/14 20:51;jjkoshy;KAFKA-1355_2014-04-04_13:51:22.patch;https://issues.apache.org/jira/secure/attachment/12638762/KAFKA-1355_2014-04-04_13%3A51%3A22.patch","17/Apr/14 21:49;jjkoshy;KAFKA-1355_2014-04-17_14:48:57.patch;https://issues.apache.org/jira/secure/attachment/12640714/KAFKA-1355_2014-04-17_14%3A48%3A57.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,384157,,,Fri Apr 18 21:04:46 UTC 2014,,,,,,,,,,"0|i1u6kn:",384425,,,,,,,,,,,,,,,,,,,,"01/Apr/14 01:45;jjkoshy;BTW, here are the configs and steps I used for this, KAFKA-1342 and KAFKA-1350:

Four brokers, 100 topics, eight partitions each.

Log4j and server configs by broker:
https://gist.github.com/anonymous/9906088
https://gist.github.com/anonymous/9906092
https://gist.github.com/anonymous/9906096
https://gist.github.com/anonymous/9906102
https://gist.github.com/anonymous/9906144
https://gist.github.com/anonymous/9906148
https://gist.github.com/anonymous/9906153
https://gist.github.com/anonymous/9906157

Producer performance: https://gist.github.com/anonymous/9906163

(At the end, just grep in the controller's request log and extract local time)
grep ControlledShutdownRequest logs/kafka-request*
;;;","04/Apr/14 18:23;jjkoshy;https://reviews.apache.org/r/20038;;;","04/Apr/14 20:48;jjkoshy;Updated reviewboard https://reviews.apache.org/r/20038/
 against branch origin/trunk;;;","04/Apr/14 20:51;jjkoshy;Updated reviewboard https://reviews.apache.org/r/20038/
 against branch origin/trunk;;;","09/Apr/14 18:01;jjkoshy;Committed to trunk (including the comment fix in Jun's follow-up review).

Need patch for 0.8.1;;;","10/Apr/14 21:02;jjkoshy;Created reviewboard https://reviews.apache.org/r/20232/
 against branch origin/0.8.1;;;","12/Apr/14 20:10;nehanarkhede;[~jjkoshy] Should we also check in this patch to 0.8.1. I'm not sure if we waiting on something?;;;","12/Apr/14 22:18;jjkoshy;Tim's patch in 1363 conflicts with this. So we can get 1363 in first, and I will rebase this one.;;;","12/Apr/14 23:46;nehanarkhede;KAFKA-1363 is in 0.8.1 as well as trunk now. ;;;","14/Apr/14 17:47;jjkoshy;Sorry - I meant KAFKA-1356, not 1363 - will check that in first after reviewing and rebase this.;;;","14/Apr/14 17:48;jjkoshy;That has been marked as closed, but https://reviews.apache.org/r/20252/ has not been checked into 0.8.1;;;","17/Apr/14 21:49;jjkoshy;Updated reviewboard https://reviews.apache.org/r/20232/
 against branch origin/0.8.1;;;","18/Apr/14 21:04;jjkoshy;Thanks for the review. Committed to 0.8.1 as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Failed to load class ""org.slf4j.impl.StaticLoggerBinder""",KAFKA-1354,12705825,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,rakeshacharya,rakeshacharya,01/Apr/14 00:14,27/Nov/21 08:33,14/Jul/23 05:39,13/Sep/17 12:36,0.8.1,,,,,,,,,,log,,,1,newbie,patch,usability,"Getting below errors during Kafka startup

SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
[2014-03-31 18:55:36,488] INFO Will not load MX4J, mx4j-tools.jar is not in the classpath (kafka.utils.Mx4jLoader$)",RHEL,daisuke.kobayashi,guozhang,jjkoshy,junrao,omkreddy,rakeshacharya,RakeshManiyoor@fico.com,saden1,srikrishna,tnachen,,,,,,,,,,,,,,,,,,,2419200,2419200,,0%,2419200,2419200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/21 08:32;srikrishna;image-2021-11-27-14-02-14-795.png;https://issues.apache.org/jira/secure/attachment/13036686/image-2021-11-27-14-02-14-795.png","27/Nov/21 08:33;srikrishna;image-2021-11-27-14-03-52-371.png;https://issues.apache.org/jira/secure/attachment/13036687/image-2021-11-27-14-03-52-371.png",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,384149,,,Sat Nov 27 08:10:00 UTC 2021,,,,,,,,,,"0|i1u6iv:",384417,,,,,,,,,,,,,,,,,,,,"02/Apr/14 16:51;rakeshacharya;Any update?;;;","03/Apr/14 06:49;tnachen;Hi Rakesh, this is actually a very common message you get usually when Kafka doesn't find a log4j.properties file to configure a logger to use. How did you start the broker and do you have log4j.properties in the config folder where you read your server.properties?;;;","03/Apr/14 13:44;rakeshacharya;Yes the log4j.properties file is in same folder as server. Properties.

I have replaced the LOG_DIR in the kafka run class file to a location where I want logs to be stored, the same value exists in the log4j.properties file too.

;;;","03/Apr/14 13:50;rakeshacharya;Also when I start I am seeing the correct log directory being picked up along with log4j.properties

-Dlog4j.configuration, -Dkafka.logs.dir,-Xloggc

;;;","04/Apr/14 17:09;junrao;The issue is related to slf4j. slf4j only provides the api. The user is expected to provide the actual binding implementation. You can include either slf4j-simple.jar or slf4j-log4j12.jar in the classpath. Then the warning will go away.;;;","09/Apr/14 18:04;jjkoshy;Should we include a default binding that can be overridden through an environment variable in our start-up scripts?;;;","10/Apr/14 01:36;junrao;I am not sure. The problem is that you can't put more than one bindings in the classpath. Otherwise, slf4j is confused. If we include a default binding and a user's environment includes another binding, we will introduce a new problem.;;;","29/Apr/14 03:32;saden1;Kafka should definitely avoid forcing end-users to mess with logging libraries.  Currently Kafka distributes log4j and slf4j-api so it stands to reason to also include log4j binding in the distribution! ;;;","29/Apr/14 03:35;RakeshManiyoor@fico.com;hi

I will be out of the office until July 10th , with little access to E-mail or voicemail. I will respond to any queries upon my return.If there is crtical issues then please contact Rivera Jose.

Thanks.
Rakesh
This email and any files transmitted with it are confidential, proprietary and intended solely for the individual or entity to whom they are addressed. If you have received this email in error please delete it immediately.
;;;","29/Apr/14 04:03;junrao;This probably makes sense for Kafka brokers. However, the client dependency will be a tricky. I am wondering how other projects using slf4j specify their dependencies.;;;","29/Apr/14 15:55;saden1;As long as the client library uses slf4j api to log messages it only needs to include the slf4j-api jar. This way the user has the freedom to choose their own logging framework and add their slf4j bridges or bindings to their classpath.

Broker Dependencies:
slf4j-api.jar
slf4j-log4j12.jar
log4j.jar

Client Dependencies:
slf4j-api

For third party dependencies that use other logging frameworks the client and the broker will also need to include slf4j bridge dependencies. For example, if a third party library broker/client dependency uses commons logging then you should also distribute jcl-over-slf4j.jar.

;;;","29/Apr/14 17:20;junrao;That makes sense. The problem is that currently the kafka jar is for both the sever and the client. We are creating the new clients in a separate jar. Perhaps we can revisit this when the old clients are phased out.;;;","04/Sep/14 22:12;guozhang;Moving to 0.9 for tracking.;;;","23/Jan/17 01:15;RakeshManiyoor@fico.com;HI

I am currently OOO and have limited access to mails , So will respond once am back.


Cheers

Rakesh

This email and any files transmitted with it are confidential, proprietary and intended solely for the individual or entity to whom they are addressed. If you have received this email in error please delete it immediately.
;;;","13/Sep/17 12:36;omkreddy;This was fixed in 0.8.2  by adding slf4j-log4j12 binding to Kafka libs.;;;","27/Nov/21 08:10;srikrishna;slf4j-log4j12-1.7.30 version is already present in the libs.

What is the solution for this issue?

!image-2021-11-27-14-03-52-371.png!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix excessive state change logging,KAFKA-1350,12704394,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,jjkoshy,jjkoshy,28/Mar/14 23:47,09/Apr/14 22:32,14/Jul/23 05:39,01/Apr/14 05:29,0.8.1,,,0.8.1.1,,,,,,,,,,0,,,,"I can provide steps to reproduce this issue.  The state change logger needs
to be guarded (to check if trace logging is turned on or not).

The delete topic patch significantly increased the amount of logging that we
do both on the controller. This results in higher latencies in state
transitions and can slow down the controller (as well as the broker).  This
slow-down was how we ran into KAFKA-1342.",,guozhang,jjkoshy,nehanarkhede,tnachen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1380,,,,"30/Mar/14 06:22;nehanarkhede;KAFKA-1350.patch;https://issues.apache.org/jira/secure/attachment/12637693/KAFKA-1350.patch","30/Mar/14 06:28;nehanarkhede;KAFKA-1350_2014-03-29_23:28:07.patch;https://issues.apache.org/jira/secure/attachment/12637694/KAFKA-1350_2014-03-29_23%3A28%3A07.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,382728,,,Tue Apr 01 05:29:41 UTC 2014,,,,,,,,,,"0|i1txr3:",382996,,,,,,,,,,,,,,,,,,,,"29/Mar/14 20:43;guozhang;[~jjkoshy] Could you provide some more context? I thought the state change logging is guarded by log4j?;;;","30/Mar/14 06:22;nehanarkhede;Created reviewboard https://reviews.apache.org/r/19828/
 against branch 0.8.1;;;","30/Mar/14 06:28;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/19828/
 against branch 0.8.1;;;","30/Mar/14 06:29;nehanarkhede;Uploaded a patch against the 0.8.1 branch. Once it is reviewed, will upload a patch against trunk;;;","30/Mar/14 22:17;tnachen;[~guozhang] the state change logger log call itself isn't guarded, and although it might not output once it goes into the method, the most expensive call was actually the string.format that forms the log message overall takes a long time to process.;;;","30/Mar/14 23:41;nehanarkhede;[~tnachen] Filed KAFKA-1351 to convert the String.format uses in our logging.;;;","31/Mar/14 18:49;guozhang;I think if the corresponding level is not triggered, it should not invoke the string.format function. But if we use the getLogger() and use that instance function, then the string.format call will be triggered no matter what. This is a small test I did:

{code:title=TestMisc.java|borderStyle=solid}
object TestMisc extends Logging {
  def main(args: Array[String]) {

    var count = 0

    info(""INFO Printed %d"".format( {count+=1;count} ) )
    debug(""DEBUG Printed %d"".format( {count+=1;count} ) )

    println(""Final Count %d"".format(count))

    val dummyLogger = Logger.getLogger(KafkaController.stateChangeLogger)
    count = 0

    dummyLogger.info(""INFO Printed %d"".format( {count+=1;count} ) )
    dummyLogger.debug(""DEBUG Printed %d"".format( {count+=1;count} ) )

    println(""Final Count %d"".format(count))
  }
}
{code}

with log4j at INFO the output:
{quote}
[2014-03-31 12:47:34,207] INFO INFO Printed 1 (kafka.TestMisc$)
Final Count 1
[2014-03-31 12:47:34,213] INFO INFO Printed 1 (state.change.logger)
Final Count 2
{quote}

with log4j at DEBUG the output:
{quote}
[2014-03-31 12:50:54,211] INFO INFO Printed 1 (kafka.TestMisc$)
[2014-03-31 12:50:54,213] DEBUG DEBUG Printed 2 (kafka.TestMisc$)
Final Count 2
[2014-03-31 12:50:54,219] INFO INFO Printed 1 (state.change.logger)
[2014-03-31 12:50:54,219] DEBUG DEBUG Printed 2 (state.change.logger)
Final Count 2
{quote}
;;;","31/Mar/14 20:45;tnachen;Ah got it, just learned that Scala has a call-by-name / call-by-value difference, and our logging uses call-by-name. 
You're right the format won't be evaluated with our trait.;;;","31/Mar/14 21:33;jjkoshy;[~guozhang] I'm not sure I follow your question. I meant that since we use the logger API directly for state-change.log, we need to check if (stateChangeLogger.is*Enabled).

Also, are you sure you recompiled your test - the final count at the INFO level should be 3. (I tried that locally as well.)
;;;","31/Mar/14 21:47;guozhang;Hi Joel,

I think we are on the same page as for your first question, if we use getLogger(), the logger API itself will always evaluate the string.format; but as long as we use the Logging as Neha's patch did, it will call-by-name and hence effectively do lazy evaluation.

For the second question, since my test code reset count to 0, it would either be 1 or 2 for any cases, right?;;;","31/Mar/14 22:27;jjkoshy;Ack yes - I missed the reset.;;;","01/Apr/14 05:29;nehanarkhede;Thanks for the reviews. Committed to 0.8.1 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revisit sample start date in metrics under when there is no activity,KAFKA-1346,12704186,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,28/Mar/14 03:06,20/May/15 18:08,14/Jul/23 05:39,20/May/15 18:08,,,,,,,,,,,,,,0,,,,"The current logic rolls over a new sample when the current window expires and either an event occurs or a read occurs. In either of these the starting time stamp of the new event window is set to the current time.

However this introduces a subtle bias in the case of very low volume sensors following a period of complete inactivity.",,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-2191,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,382520,,,2014-03-28 03:06:17.0,,,,,,,,,,"0|i1twgn:",382788,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Slow controlled shutdowns can result in stale shutdown requests,KAFKA-1342,12703878,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,jjkoshy,jjkoshy,jjkoshy,27/Mar/14 01:35,30/Dec/19 22:19,14/Jul/23 05:39,30/Dec/19 22:19,0.8.1,,,,,,,,,,,,,3,newbie++,reliability,,"I don't think this is a bug introduced in 0.8.1., but triggered by the fact
that controlled shutdown seems to have become slower in 0.8.1 (will file a
separate ticket to investigate that). When doing a rolling bounce, it is
possible for a bounced broker to stop all its replica fetchers since the
previous PID's shutdown requests are still being shutdown.

- 515 is the controller
- Controlled shutdown initiated for 503
- Controller starts controlled shutdown for 503
- The controlled shutdown takes a long time in moving leaders and moving
  follower replicas on 503 to the offline state.
- So 503's read from the shutdown channel times out and a new channel is
  created. It issues another shutdown request.  This request (since it is a
  new channel) is accepted at the controller's socket server but then waits
  on the broker shutdown lock held by the previous controlled shutdown which
  is still in progress.
- The above step repeats for the remaining retries (six more requests).
- 503 hits SocketTimeout exception on reading the response of the last
  shutdown request and proceeds to do an unclean shutdown.
- The controller's onBrokerFailure call-back fires and moves 503's replicas
  to offline (not too important in this sequence).
- 503 is brought back up.
- The controller's onBrokerStartup call-back fires and moves its replicas
  (and partitions) to online state. 503 starts its replica fetchers.
- Unfortunately, the (phantom) shutdown requests are still being handled and
  the controller sends StopReplica requests to 503.
- The first shutdown request finally finishes (after 76 minutes in my case!).
- The remaining shutdown requests also execute and do the same thing (sends
  StopReplica requests for all partitions to
  503).
- The remaining requests complete quickly because they end up not having to
  touch zookeeper paths - no leaders left on the broker and no need to
  shrink ISR in zookeeper since it has already been done by the first
  shutdown request.
- So in the end-state 503 is up, but effectively idle due to the previous
  PID's shutdown requests.

There are some obvious fixes that can be made to controlled shutdown to help
address the above issue. E.g., we don't really need to move follower
partitions to Offline. We did that as an ""optimization"" so the broker falls
out of ISR sooner - which is helpful when producers set required.acks to -1.
However it adds a lot of latency to controlled shutdown. Also, (more
importantly) we should have a mechanism to abort any stale shutdown process.
",,adenysenko,boniek,cotedm,donnchadh,futtre,guozhang,hachikuji,ijuma,jazemek,jeffwidman,jjkoshy,junrao,khsibr,mrlabbe,toddpalino,umesh9794@gmail.com,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-4207,,,,KAFKA-7235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,382212,,,Sat Jul 07 06:11:02 UTC 2018,,,,,,,,,,"0|i1tulb:",382483,,,,,,,,,,,,,,,,,,,,"28/Mar/14 23:55;jjkoshy;KAFKA-1350 is what triggered this issue. Although we will fix that, we
should note that controlled shutdowns can take time to complete if there are
several partitions.  It is unsafe to increase the number of shutdown retries
on the broker because each retry will hold up a request-handler thread on
the controller.  i.e., it is better to set a very high shutdown timeout. (It
is right now hard-coded to 30 seconds, but it really should be
configurable.) It may also help to add a controlled shutdown request
purgatory in the controller.

;;;","09/Apr/14 22:04;jjkoshy;Moving this out to 0.8.2 since the slow shutdown regression has been resolved. The socket timeout is in fact configurable (via controller.socket.timeout.ms);;;","30/Apr/14 14:59;junrao;If it's just a matter of configuring the timeout, could we just bump up the default timeout?;;;","30/Apr/14 19:05;jjkoshy;We could, but I think the underlying issue is serious. i.e., if we allow concurrent (and redundant) shutdown request we tie up request handlers. We should allow max one shutdown request per broker (or even across the cluster).;;;","04/Sep/14 21:50;guozhang;Moving out of 0.8.2 for now.;;;","07/Apr/15 22:56;toddpalino;Bump

I think we need to revive this. We have a ""safe shutdown"" bit of wrapper code we use which relies on an external resource that we should eliminate. It would be better to provide a safe shutdown option within Kafka itself without that wrapper (i.e. do not shut down unless your under replicated count is 0). However, this is not possible without serialized shutdown at the controller level. We can't allow a second broker to shut down until the first broker has completed its shutdown process. Then the second broker can check the URP count and be allowed to proceed.;;;","26/Sep/16 22:24;junrao;A similar incident was reported in https://issues.apache.org/jira/browse/KAFKA-4207.;;;","27/Sep/16 04:21;hachikuji;Downgrading this to ""Critical"" since it has not actually blocked the previous few releases and after discussion with [~junrao], it seems unlikely to be fixed for 0.10.1.;;;","25/Feb/17 07:10;wushujames;[~jjkoshy], [~toddpalino], is it still true that it is unsafe to increase the number of controlled shutdown requests? We currently have brokers with 10,000 partitions each, and there is no way they can effectively shutdown within the shutdown timeout of 30 seconds, even with the current default of controlled.shutdown.max.retries=3. If the brokers aren't able to shutdown within the 90 seconds (30 seconds * 3), then when we bounce them and they start back up too quickly, we end up with a broker with all of its replica fetchers stopped (as described in this JIRA). This also seems like a specific instance of KAFKA-1120

We have increased that to 40 or so, to allow brokers up to 20 minutes to shutdown. Usually, it takes them 8 minutes.

Is it better to increase the value of controller.socket.timeout.ms? If we increase this to 25 minutes for example, doesn't that impact much more than just the shutdown request? Won't normal controller->broker communication like LeaderAndIsr and MetadataUpdate requests also be subject to an 25 minute timeout?
;;;","03/Mar/17 04:30;toddpalino;[~wushujames], I'm not sure about increasing the number of requests, but a lot of this should have been resolved recently with some updates that I believe [~becket_qin] made. To make it better, we had bumped controller.socket.timeout.ms significantly (to 300000). We didn't see any side effects from doing that.;;;","07/Jul/18 04:22;jazemek;[~wushujames] do you have TRACE logging enabled on the controller? (Apparently that's the default??) We noticed that turning this off allowed shutdown to happen much more quickly. We were running into the same issue you described with ~6k partitions per broker, but after turning off TRACE logging, we are able to shutdown cleanly;;;","07/Jul/18 05:16;ijuma;[~jazemek] in what Kafka version did you see that?;;;","07/Jul/18 05:21;jazemek;[~ijuma] We're running 0.10.2;;;","07/Jul/18 05:24;ijuma;If you upgrade to 0.11.0.3 or newer, you shouldn't have to disable the trace logging in the Controller. 1.1.1, in particular, has a number of optimizations.;;;","07/Jul/18 05:27;jazemek;[~ijuma] Would you be able to provide some details around the changes in those versions which would remove the need to disable trace logging? Thanks!;;;","07/Jul/18 06:08;wushujames;In an older release, there was a logging “bug” in the controller where upon any leadership change in a single partition, it would log the state of *all partitions in the cluster*. That was probably the cause of the slowness you were seeing.

It got fixed at one point. I don’t remember which release it was fixed in, but I’m pretty sure that the “bug” existed in 0.10.2;;;","07/Jul/18 06:11;wushujames;Found it: 

https://github.com/apache/kafka/pull/4075

And

https://issues.apache.org/jira/browse/KAFKA-6116



;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client Selector doesn't check connection id properly,KAFKA-1341,12703768,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,tnachen,tnachen,tnachen,26/Mar/14 17:55,01/Apr/14 05:37,14/Jul/23 05:39,01/Apr/14 05:37,,,,,,,,,,,producer ,,,0,,,,"Reviewing the new producer code I found that we're not checking connection id properly in the Selector, in result connecting using the same id over and over will result in sockets leaking.",,jkreps,nehanarkhede,tnachen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/14 17:55;tnachen;KAFKA-1341.patch;https://issues.apache.org/jira/secure/attachment/12636959/KAFKA-1341.patch","31/Mar/14 21:44;tnachen;KAFKA-1341_2014-03-31_14:44:27.patch;https://issues.apache.org/jira/secure/attachment/12637928/KAFKA-1341_2014-03-31_14%3A44%3A27.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,382102,,,Tue Apr 01 05:37:14 UTC 2014,,,,,,,,,,"0|i1ttxr:",382377,,,,,,,,,,,,,,,,,,,,"26/Mar/14 17:55;tnachen;Created reviewboard https://reviews.apache.org/r/19690/
 against branch origin/trunk;;;","27/Mar/14 12:41;nehanarkhede;+1, LGTM. [~jkreps], would you like to take a quick look?;;;","27/Mar/14 19:13;jkreps;I'm +1, though is the change in scala version in the gradle file intentional?;;;","31/Mar/14 21:41;tnachen;Sorry that was a mistake, uploading a new patch.;;;","31/Mar/14 21:43;tnachen;Updated reviewboard https://reviews.apache.org/r/19690/
 against branch origin/trunk;;;","31/Mar/14 21:44;tnachen;Updated reviewboard https://reviews.apache.org/r/19690/
 against branch origin/trunk;;;","01/Apr/14 05:37;nehanarkhede;Thanks for the patch, pushed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Time based offset retrieval seems broken,KAFKA-1339,12703641,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,fvarnavas,fvarnavas,26/Mar/14 01:30,24/Aug/17 14:32,14/Jul/23 05:39,24/Aug/17 14:32,0.8.0,,,,,,,,,,core,,,1,,,,"The kafka PartitionOffsetRequest takes a time parameter.  It seems broken to me.

There are two magic values

  -2 returns the oldest  available offset
  -1 returns the newest available offset
  Otherwise the value is time since epoch in millisecs (System.currentTimeMillis())

The granularity is limited to the granularity of the log files
These are the log segments for the partition I tested

  Time now is about 17:07
  Time shown is last modify time
  File name has the starting offset number
  You can see that the current one started about 13:40

1073742047 Mar 24 02:52 00000000000004740823.log
1073759588 Mar 24 11:25 00000000000004831581.log
1073782532 Mar 24 16:31 00000000000004916313.log
1073741985 Mar 25 09:11 00000000000005066939.log
1073743756 Mar 25 13:39 00000000000005158529.log
 778424349 Mar 25 17:07 00000000000005214225.log

The below shows the returned offset for an input time = (current time - [0..23] hours)
Even 1 second less than the current time returns the previous segment, even though that segment ended 2.5 hours earlier.

I think the result is off by 1 log segment. i.e. offset 1-3 should have been from 5214225, 4-7 should have been from 5158529

0 -> 5214225
1 -> 5158529
2 -> 5158529
3 -> 5158529
4 -> 5066939
5 -> 5066939
6 -> 5066939
7 -> 5066939
8 -> 4973490
9 -> 4973490
10 -> 4973490
11 -> 4973490
12 -> 4973490
13 -> 4973490
14 -> 4973490
15 -> 4973490
16 -> 4916313
17 -> 4916313
18 -> 4916313
19 -> 4916313
20 -> 4916313
21 -> 4916313
22 -> 4916313
23 -> 4916313
",Linux,chikim79,fvarnavas,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,381975,,,Thu Aug 24 14:32:28 UTC 2017,,,,,,,,,,"0|i1tt5j:",382250,,,,,,,,,,,,,,,,,,,,"24/Aug/17 14:32;omkreddy;Time-based offset retrieval is improved with the introduction of message timestamp.  Pl reopen if you think the issue still exists
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log.dirs server property no longer supports relative directories,KAFKA-1323,12703267,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,jjkoshy,jjkoshy,24/Mar/14 18:19,27/May/14 18:42,14/Jul/23 05:39,16/Apr/14 20:31,,,,0.8.1.1,,,,,,,,,,1,,,,"This seems to have been caused by KAFKA-1315 - we now don't support relative directories.

Steps to reproduce:
* Set a relative directory for log.dirs. E.g., {{log.dirs=data/kafka-logs}}
* Bring up the broker and produce some messages: {{./bin/kafka-producer-perf-test.sh --broker-list localhost:9092 --messages 1000 --topic test}}
",,adenysenko,guozhang,jjkoshy,nehanarkhede,tnachen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1380,,,,"15/Apr/14 23:25;guozhang;KAFKA-1323.patch;https://issues.apache.org/jira/secure/attachment/12640361/KAFKA-1323.patch","15/Apr/14 23:09;guozhang;KAFKA-1323.patch;https://issues.apache.org/jira/secure/attachment/12640354/KAFKA-1323.patch","15/Apr/14 16:36;guozhang;KAFKA-1323.patch;https://issues.apache.org/jira/secure/attachment/12640291/KAFKA-1323.patch","25/Mar/14 18:54;tnachen;KAFKA-1323.patch;https://issues.apache.org/jira/secure/attachment/12636754/KAFKA-1323.patch","27/Mar/14 23:17;tnachen;KAFKA-1323_2014-03-27_16:17:26.patch;https://issues.apache.org/jira/secure/attachment/12637277/KAFKA-1323_2014-03-27_16%3A17%3A26.patch","31/Mar/14 18:57;tnachen;KAFKA-1323_2014-03-31_11:57:37.patch;https://issues.apache.org/jira/secure/attachment/12637895/KAFKA-1323_2014-03-31_11%3A57%3A37.patch","02/Apr/14 19:18;tnachen;KAFKA-1323_2014-04-02_12:17:57.patch;https://issues.apache.org/jira/secure/attachment/12638314/KAFKA-1323_2014-04-02_12%3A17%3A57.patch","14/Apr/14 23:13;guozhang;KAFKA-1323_2014-04-14_16:13:07.patch;https://issues.apache.org/jira/secure/attachment/12640165/KAFKA-1323_2014-04-14_16%3A13%3A07.patch","16/Apr/14 18:53;guozhang;KAFKA-1323_2014-04-16_11:53:39.patch;https://issues.apache.org/jira/secure/attachment/12640521/KAFKA-1323_2014-04-16_11%3A53%3A39.patch","16/Apr/14 19:05;guozhang;KAFKA-1323_2014-04-16_12:05:40.patch;https://issues.apache.org/jira/secure/attachment/12640524/KAFKA-1323_2014-04-16_12%3A05%3A40.patch","28/Mar/14 04:26;nehanarkhede;kafka-1323-trunk-test-failures.png;https://issues.apache.org/jira/secure/attachment/12637330/kafka-1323-trunk-test-failures.png",,,,,,,,,,,,,,,,,11.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,381605,,,Wed Apr 16 20:31:01 UTC 2014,,,,,,,,,,"0|i1tqvr:",381880,,,,,,,,,,,,,,,,,,,,"24/Mar/14 18:22;jjkoshy;{code}
java.util.NoSuchElementException: key not found: data/kafka-logs
        at scala.collection.MapLike$class.default(MapLike.scala:223)
        at scala.collection.immutable.Map$Map1.default(Map.scala:93)
        at scala.collection.MapLike$class.apply(MapLike.scala:134)
        at scala.collection.immutable.Map$Map1.apply(Map.scala:93)
        at kafka.cluster.Partition.getOrCreateReplica(Partition.scala:93)
        at kafka.cluster.Partition$$anonfun$makeLeader$2.apply(Partition.scala:178)
        at kafka.cluster.Partition$$anonfun$makeLeader$2.apply(Partition.scala:178)
        at scala.collection.immutable.Set$Set1.foreach(Set.scala:81)
        at kafka.cluster.Partition.makeLeader(Partition.scala:178)
        at kafka.server.ReplicaManager$$anonfun$makeLeaders$5.apply(ReplicaManager.scala:309)
        at kafka.server.ReplicaManager$$anonfun$makeLeaders$5.apply(ReplicaManager.scala:308)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
        at kafka.server.ReplicaManager.makeLeaders(ReplicaManager.scala:308)
        at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:260)
        at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:100)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:71)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:42)
        at java.lang.Thread.run(Thread.java:662)
{code}

I think we just need to update the look-up key in getOrCreateReplica with
the absolute path, but we should check if there are other accesses to the
same map which needs an update.
;;;","24/Mar/14 18:54;nehanarkhede;oops, this is my bad. Missed this during the review. Which points out that we need a unit test to cover this as well.;;;","25/Mar/14 18:54;tnachen;Created reviewboard https://reviews.apache.org/r/19626/
 against branch origin/trunk;;;","27/Mar/14 23:17;tnachen;Updated reviewboard https://reviews.apache.org/r/19626/
 against branch origin/trunk;;;","28/Mar/14 04:26;nehanarkhede;I ran into quite a few unit test failures with the patch on trunk -

!kafka-1323-trunk-test-failures.png!;;;","31/Mar/14 18:57;tnachen;Updated reviewboard https://reviews.apache.org/r/19626/
 against branch origin/trunk;;;","01/Apr/14 17:28;nehanarkhede;The tests seem to pass now. I'm a +1 once [~junrao]'s comments are addressed.;;;","02/Apr/14 19:18;tnachen;Updated reviewboard https://reviews.apache.org/r/19626/
 against branch origin/trunk;;;","04/Apr/14 16:24;nehanarkhede;The same tests (mentioned above) fail for me on trunk with the latest patch.;;;","11/Apr/14 00:08;nehanarkhede;Hey [~tnachen], I think the trunk patch needs to be rebased. ;;;","14/Apr/14 23:13;guozhang;Updated reviewboard  against branch origin/trunk;;;","15/Apr/14 16:36;guozhang;Created reviewboard https://reviews.apache.org/r/20370/
 against branch origin/trunk;;;","15/Apr/14 23:09;guozhang;Created reviewboard https://reviews.apache.org/r/20392/
 against branch origin/0.8.1;;;","15/Apr/14 23:25;guozhang;Created reviewboard https://reviews.apache.org/r/20393/
 against branch origin/0.8.1;;;","15/Apr/14 23:27;guozhang;Please ignore the RB 20392, that was a messed one.;;;","15/Apr/14 23:52;jjkoshy;[~guozhang] You can mark that RB as discarded;;;","16/Apr/14 00:09;guozhang;Thanks Joel. Done.;;;","16/Apr/14 18:53;guozhang;Updated reviewboard https://reviews.apache.org/r/20393/
 against branch origin/0.8.1;;;","16/Apr/14 19:05;guozhang;Updated reviewboard https://reviews.apache.org/r/20393/
 against branch origin/0.8.1;;;","16/Apr/14 20:31;jjkoshy;Thanks for the patch. Committed to trunk and 0.8.1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change compression.codec to compression.type in new producer configs of system tests,KAFKA-1320,12702957,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,21/Mar/14 19:53,17/May/16 14:14,14/Jul/23 05:39,31/Mar/14 23:06,,,,0.8.2.0,,,,,,,,,,0,,,,"In the new producer, the compression config is no longer specified by compression.codec but compression.type.",,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/14 23:43;guozhang;KAFKA-1320.patch;https://issues.apache.org/jira/secure/attachment/12637574/KAFKA-1320.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,381294,,,Mon Mar 31 23:06:48 UTC 2014,,,,,,,,,,"0|i1tozj:",381570,,,,,,,,,,,,,,,,,,,,"28/Mar/14 23:43;guozhang;Created reviewboard https://reviews.apache.org/r/19811/
 against branch origin/trunk;;;","31/Mar/14 23:06;junrao;Thanks for the patch. +1. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka jar doesn't depend on metrics-annotation any more,KAFKA-1319,12702778,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,21/Mar/14 01:03,27/May/14 18:43,14/Jul/23 05:39,24/Mar/14 01:16,,,,0.8.1,0.8.2.0,,,,,,,,,0,,,,,,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1380,,,,"21/Mar/14 01:04;junrao;KAFKA-1319.patch;https://issues.apache.org/jira/secure/attachment/12635931/KAFKA-1319.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,381116,,,Mon Mar 24 01:16:24 UTC 2014,,,,,,,,,,"0|i1tnvz:",381392,,,,,,,,,,,,,,,,,,,,"21/Mar/14 01:04;junrao;Created reviewboard https://reviews.apache.org/r/19511/
 against branch origin/trunk;;;","21/Mar/14 01:06;junrao;If we do want to release 0.8.1.1, we probably should include this change too to clean up the dependencies in maven.;;;","24/Mar/14 01:16;junrao;Thanks for the review. Double committed to trunk and 0.8.1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
waiting for producer to stop is not reliable in system tests,KAFKA-1318,12702775,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,21/Mar/14 00:50,01/Apr/14 18:40,14/Jul/23 05:39,01/Apr/14 18:40,,,,0.8.2.0,,,,,,,,,,0,,,,"Occasionally, system hangs because the producer already stopped, but the script was still waiting for the producer to stop.",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/14 01:01;junrao;KAFKA-1318.patch;https://issues.apache.org/jira/secure/attachment/12635930/KAFKA-1318.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,381113,,,Tue Apr 01 18:40:42 UTC 2014,,,,,,,,,,"0|i1tnvb:",381389,,,,,,,,,,,,,,,,,,,,"21/Mar/14 01:01;junrao;Created reviewboard https://reviews.apache.org/r/19510/
 against branch origin/trunk;;;","01/Apr/14 18:40;junrao;Thanks for the reviews. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"KafkaServer 0.8.1 not responding to .shutdown() cleanly, possibly related to TopicDeletionManager or MetricsMeter state",KAFKA-1317,12702700,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,tnachen,brentbradbury,brentbradbury,20/Mar/14 19:43,27/May/14 18:41,14/Jul/23 05:39,29/Mar/14 01:22,0.8.1,,,0.8.1.1,,,,,,,,,,0,newbie,,,"When I run an in-process instance of KafkaServer, send a message through it, then call shutdown(), some threads never exit and the process hangs until the process is killed manually. The same scenario does not result in a hang on 0.8.0. The hang happens when calling both shutdown() by itself as well as shutdown() and awaitShutdown() together. I have seen similar behavior shutting down a deployed kafka server as well, but haven't had time to diagnose whether or not it is the same symptom.


I suspect either the metrics-meter-tick-thread-1 & 2 or delete-topics-thread
 (waiting in kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$awaitTopicDeletionNotification(TopicDeletionManager.scala:178) is to blame. Since the TopicDeletionManager is new, it seems more suspicious to me. A complete thread dump is attached; the suspect threads are below.

""delete-topics-thread"" prio=5 tid=0x00007fb3e31d2800 nid=0x6b03 waiting on condition [0x000000013c3b3000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000012e6e6920> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$awaitTopicDeletionNotification(TopicDeletionManager.scala:178)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:334)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:333)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:333)
	at kafka.utils.Utils$.inLock(Utils.scala:538)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:333)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)

   Locked ownable synchronizers:
	- None

""metrics-meter-tick-thread-2"" daemon prio=5 tid=0x00007fb3e31c1000 nid=0x5f03 runnable [0x000000013ab8f000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000012e7d05d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1090)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)

   Locked ownable synchronizers:
	- None

""metrics-meter-tick-thread-1"" daemon prio=5 tid=0x00007fb3e31ef800 nid=0x5e03 waiting on condition [0x000000013a98c000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000012e7d05d8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1085)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:807)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)

   Locked ownable synchronizers:
	- None

",,brentbradbury,jkreps,nehanarkhede,tnachen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1380,,,,"26/Mar/14 18:56;tnachen;KAFKA-1317.patch;https://issues.apache.org/jira/secure/attachment/12636973/KAFKA-1317.patch","24/Mar/14 06:45;tnachen;KAFKA-1317.patch;https://issues.apache.org/jira/secure/attachment/12636302/KAFKA-1317.patch","24/Mar/14 06:48;tnachen;KAFKA-1317_2014-03-23_23:48:28.patch;https://issues.apache.org/jira/secure/attachment/12636303/KAFKA-1317_2014-03-23_23%3A48%3A28.patch","24/Mar/14 18:06;tnachen;KAFKA-1317_2014-03-24_11:06:15.patch;https://issues.apache.org/jira/secure/attachment/12636393/KAFKA-1317_2014-03-24_11%3A06%3A15.patch","25/Mar/14 22:20;tnachen;KAFKA-1317_2014-03-25_15:20:14.patch;https://issues.apache.org/jira/secure/attachment/12636796/KAFKA-1317_2014-03-25_15%3A20%3A14.patch","26/Mar/14 16:48;tnachen;KAFKA-1317_2014-03-26_09:48:03.patch;https://issues.apache.org/jira/secure/attachment/12636944/KAFKA-1317_2014-03-26_09%3A48%3A03.patch","26/Mar/14 18:31;tnachen;KAFKA-1317_2014-03-26_11:30:57.patch;https://issues.apache.org/jira/secure/attachment/12636968/KAFKA-1317_2014-03-26_11%3A30%3A57.patch","26/Mar/14 22:09;tnachen;KAFKA-1317_2014-03-26_15:09:48.patch;https://issues.apache.org/jira/secure/attachment/12637023/KAFKA-1317_2014-03-26_15%3A09%3A48.patch","26/Mar/14 22:19;tnachen;KAFKA-1317_2014-03-26_15:18:52.patch;https://issues.apache.org/jira/secure/attachment/12637026/KAFKA-1317_2014-03-26_15%3A18%3A52.patch","27/Mar/14 22:15;tnachen;KAFKA-1317_2014-03-27_15:15:05.patch;https://issues.apache.org/jira/secure/attachment/12637257/KAFKA-1317_2014-03-27_15%3A15%3A05.patch","28/Mar/14 16:34;tnachen;KAFKA-1317_2014-03-28_09:34:02.patch;https://issues.apache.org/jira/secure/attachment/12637446/KAFKA-1317_2014-03-28_09%3A34%3A02.patch","20/Mar/14 19:47;brentbradbury;threaddump.txt;https://issues.apache.org/jira/secure/attachment/12635852/threaddump.txt",,,,,,,,,,,,,,,,12.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,381038,,,Sat Mar 29 01:22:10 UTC 2014,,,,,,,,,,"0|i1tnfb:",381316,,,,,,,,,,,,,,,,,,,,"20/Mar/14 19:47;brentbradbury;Thread dump of hung process;;;","20/Mar/14 19:49;brentbradbury;Maybe related to KAFKA-1298 ?;;;","20/Mar/14 20:25;nehanarkhede;Thanks for filing the bug. I wonder why I don't see the thread that calls shutdown() in the thread dump. From the description above, it seems that shutdown() hangs, right? Or is this a problem of the process not exiting due to a non-daemon live thread?;;;","20/Mar/14 20:42;brentbradbury;Ah, sorry not to make that clear Neha. I believe it's the latter case you describe-- a non-daemon thread lives past the end of the .shutdown() call. (shutdown() returns, and I see logging from the KafkaServer object indicating that it has completed the shutdown process, but the java process does not return). Checking the attached dump, it does appear that delete-topics-thread is not a daemon thread and is waiting on a lock.;;;","20/Mar/14 20:55;brentbradbury;I should add- I call awaitShutdown() as well, and this thread is still alive after that returns. ;;;","20/Mar/14 21:02;nehanarkhede;This is a bug, a lot of our threads for some reason are non-daemon threads, which is something we need to fix. What is the best way to reproduce this bug? ;;;","20/Mar/14 21:14;brentbradbury;Simplest way is to create a KafkaServer object, then start it and stop it. 

Forgiving the JRuby, I put an example here: https://gist.github.com/bbradbury/9673942;;;","20/Mar/14 21:22;jkreps;Actually non-daemon is not wrong. There are two cases. There are a few things which are pure ""nice to have"" background activities that can be killed at any time. But actually most of our threads may be doing I/O or other things so they go through an explicit shutdown rather than just terminating in the middle of whatever they are doing. For these we actually don't want them to be daemon threads, we just need to call shutdown. If we don't call shutdown the hang is a good indication that there is a problem--making them daemon would just mask the problem.;;;","20/Mar/14 21:29;nehanarkhede;Yes, that is what I thought the convention was - any thread doing I/O should be a managed non-daemon thread. Everything else can be a daemon thread. But looks like there are a some places where we have non-daemon thread even if the thread doesn't do I/O - e.g. LeaderFinderThread.;;;","20/Mar/14 21:35;jkreps;Gotcha.;;;","24/Mar/14 06:45;tnachen;Created reviewboard https://reviews.apache.org/r/19577/
 against branch origin/0.8.1;;;","24/Mar/14 06:48;tnachen;Updated reviewboard https://reviews.apache.org/r/19577/
 against branch origin/0.8.1;;;","24/Mar/14 18:06;tnachen;Updated reviewboard https://reviews.apache.org/r/19577/
 against branch origin/0.8.1;;;","25/Mar/14 22:20;tnachen;Updated reviewboard https://reviews.apache.org/r/19577/
 against branch origin/0.8.1;;;","26/Mar/14 16:48;tnachen;Updated reviewboard https://reviews.apache.org/r/19577/
 against branch origin/0.8.1;;;","26/Mar/14 18:31;tnachen;Updated reviewboard https://reviews.apache.org/r/19577/
 against branch origin/0.8.1;;;","26/Mar/14 18:56;tnachen;Created reviewboard https://reviews.apache.org/r/19696/
 against branch origin/trunk;;;","26/Mar/14 22:09;tnachen;Updated reviewboard https://reviews.apache.org/r/19577/
 against branch origin/0.8.1;;;","26/Mar/14 22:19;tnachen;Updated reviewboard https://reviews.apache.org/r/19696/
 against branch origin/trunk;;;","27/Mar/14 22:15;tnachen;Updated reviewboard https://reviews.apache.org/r/19696/
 against branch origin/trunk;;;","28/Mar/14 16:34;tnachen;Updated reviewboard https://reviews.apache.org/r/19696/
 against branch origin/trunk;;;","29/Mar/14 01:22;nehanarkhede;Thanks for the patches, Tim! Committed to trunk and 0.8.1 branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log.dirs property in KafkaServer intolerant of trailing slash,KAFKA-1315,12702522,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,tnachen,brentbradbury,brentbradbury,20/Mar/14 00:41,27/May/14 18:41,14/Jul/23 05:39,16/Apr/14 20:31,0.8.0,0.8.1,,0.8.1.1,,,,,,,,,,0,newbie,,,"A trailing slash in log.dirs causes a java.util.NoSuchElementException on the producer and a kafka.common.NotLeaderForPartitionException on the consumer. 

Per this thread: http://mail-archives.apache.org/mod_mbox/kafka-users/201307.mbox/%3CCAFbh0Q18PLacokCBy8+JG6Ef3N8+ysGKfqGhW4YB2Up18H-Eew@mail.gmail.com%3E

This is because we populate the key in
ReplicaManager.highWatermarkCheckpoints using the ""dirs"" config, but look
up the key using log.dir.getParent. So, if you have a trailing slash in the
config, they won't match. This seems a bug that we should fix. Could you
file a jira?

Thanks,

Jun

Still occuring for me, using org.apache.kafka:kafka_2.10:0.8.1",,brentbradbury,jjkoshy,jkreps,nehanarkhede,tnachen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1380,,,,"20/Mar/14 19:49;tnachen;KAFKA-1315.patch;https://issues.apache.org/jira/secure/attachment/12635854/KAFKA-1315.patch","20/Mar/14 20:28;tnachen;KAFKA-1315_2014-03-20_13:28:06.patch;https://issues.apache.org/jira/secure/attachment/12635862/KAFKA-1315_2014-03-20_13%3A28%3A06.patch","20/Mar/14 21:03;tnachen;KAFKA-1315_2014-03-20_14:03:34.patch;https://issues.apache.org/jira/secure/attachment/12635867/KAFKA-1315_2014-03-20_14%3A03%3A34.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,380861,,,Mon Mar 24 18:19:49 UTC 2014,,,,,,,,,,"0|i1tmcn:",381140,,,,,,,,,,,,,,,,,,,,"20/Mar/14 03:29;jkreps;This seems pretty bad since many people always add a slash to indicate that it is a directory. Fix should be easy too, right? Maybe we should consider this for 0.8.1.1?;;;","20/Mar/14 19:49;tnachen;Created reviewboard https://reviews.apache.org/r/19490/
 against branch origin/trunk;;;","20/Mar/14 20:28;tnachen;Updated reviewboard https://reviews.apache.org/r/19490/
 against branch origin/trunk;;;","20/Mar/14 21:03;tnachen;Updated reviewboard https://reviews.apache.org/r/19490/
 against branch origin/trunk;;;","21/Mar/14 00:36;nehanarkhede;Thanks for the patch! Committed to 0.8.1 and trunk.;;;","24/Mar/14 18:19;jjkoshy;This patch seems to have resulted in KAFKA-1323;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add a flag to turn off delete topic until it is stable,KAFKA-1311,12702180,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,18/Mar/14 16:01,27/May/14 18:41,14/Jul/23 05:39,19/Mar/14 00:44,0.8.1,,,0.8.1.1,,,,,,,,,,0,,,,"Currently delete topic is checked in as beta but is turned on by default. Since it is not well tested right now, I'm proposing turning the feature off until it is. This will introduce a config temporarily that we can get rid of once we are confident that delete topic is stable",,mazhar.shaikh.in,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1380,,,,"18/Mar/14 22:30;nehanarkhede;KAFKA-1311.patch;https://issues.apache.org/jira/secure/attachment/12635419/KAFKA-1311.patch","19/Mar/14 00:14;nehanarkhede;KAFKA-1311_2014-03-18_17:14:30.patch;https://issues.apache.org/jira/secure/attachment/12635442/KAFKA-1311_2014-03-18_17%3A14%3A30.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,380520,,,Wed Mar 19 00:44:11 UTC 2014,,,,,,,,,,"0|i1tk9z:",380801,,,,,,,,,,,,,,,,,,,,"18/Mar/14 22:30;nehanarkhede;Created reviewboard https://reviews.apache.org/r/19379/
 against branch trunk;;;","19/Mar/14 00:14;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/19379/
 against branch trunk;;;","19/Mar/14 00:44;nehanarkhede;Thanks for the review, committed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zookeeper timeout causes deadlock in Controller,KAFKA-1310,12702139,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,slon,slon,18/Mar/14 13:03,04/Aug/16 09:33,14/Jul/23 05:39,29/Mar/14 01:22,0.8.1,,,0.8.1.1,,,,,,,,,,1,,,,"Steps to reproduce:

1. Checkout and build 0.8.1 branch from github:
git clone git@github.com:apache/kafka.git && cd kafka && git checkout origin/0.8.1 && ./gradlew jar

2. Start zookeeper server:
./bin/zookeeper-server-start.sh config/zookeeper.properties

3. Start kafka server:
./bin/kafka-server-start.sh config/server.properties

4. Suspend zookeeper process for 10 seconds (ctrl-Z, then %1).

5. And kafka hasn't been re-registered in zookeeper.
./bin/zookeeper-shell.sh
ls /brokers/ids
>> []

Root cause of the problem seems to be the deadlock between DeleteTopicsThread and SessionExpirationListener in KafkaController.

1. DeleteTopicsThread acquires controllerLock and await()-s on deleteTopicsCond in awaitTopicDeletionNotification()

2. SessionExpirationListener fires. It acquires controllerLock and tries to shutdown deleteTopicManager(in onControllerResignation()). This interrupts DeleteTopicsThread.

3. DeleteTopicsThread can't return from deleteTopicsCond.await() because controllerLock is taken. We got a deadlock.",,jjkoshy,Joker.D.,miguno,nehanarkhede,slon,tnachen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1380,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,380479,,,Thu Aug 04 09:33:08 UTC 2016,,,,,,,,,,"0|i1tk0v:",380760,,,,,,,,,,,,,,,,,,,,"21/Mar/14 10:14;miguno;I can confirm this issue, using Kafka 0.8.1.

Here are the error messages when trying to create a topic:

{code}
$ bin/kafka-topics.sh --create --zookeeper zookeeper1:2181 --topic testing --partitions 1 --replication-factor 1
Error while executing topic command org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids
org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids
	at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47)
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685)
	at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:413)
	at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:409)
	at kafka.utils.ZkUtils$.getChildren(ZkUtils.scala:480)
	at kafka.utils.ZkUtils$.getSortedBrokerList(ZkUtils.scala:81)
	at kafka.admin.AdminUtils$.createTopic(AdminUtils.scala:154)
	at kafka.admin.TopicCommand$.createTopic(TopicCommand.scala:88)
	at kafka.admin.TopicCommand$.main(TopicCommand.scala:50)
	at kafka.admin.TopicCommand.main(TopicCommand.scala)
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:102)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1249)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1277)
	at org.I0Itec.zkclient.ZkConnection.getChildren(ZkConnection.java:99)
	at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:416)
	at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:413)
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)
	... 8 more
{code}

If you use the ZK CLI you will sometimes see a znode under {{/brokers/ids}}, sometimes not.  In my limited testing I could, for instance, create a topic (partitions=1, replicas=1) and then list/describe it.  But at least when I reached the point to try sending messages to it, it would fail.  See next example.

When trying to use the console producer to sent a test message ""foo"" (String) to the topic/broker:

{code}
$ bin/kafka-console-producer.sh --topic testing --broker-list localhost:9092
foo   <<< This is the test message, manually entered in the console/terminal
[2014-03-20 09:45:32,223] WARN Error while fetching metadata [{TopicMetadata for topic testing ->
No partition metadata for topic testing due to kafka.common.LeaderNotAvailableException}] for topic [testing]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)
[2014-03-20 09:45:32,233] WARN Error while fetching metadata [{TopicMetadata for topic testing ->
No partition metadata for topic testing due to kafka.common.LeaderNotAvailableException}] for topic [testing]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)
[2014-03-20 09:45:32,234] ERROR Failed to collate messages by topic, partition due to: Failed to fetch topic metadata for topic: testing (kafka.producer.async.DefaultEventHandler)
{code}

*How to reproduce*

Using Wirbelsturm you can reproduce this error as follow.  This assumes you have Vagrant 1.4.x and VirtualBox already installed on your host machine.

{code}
$ git clone https://github.com/miguno/wirbelsturm.git
$ cd wirbelsturm
$ ./bootstrap     # <<< May take a while depending on how fast your Internet connection is.

# Then uncomment the `kafka_broker` section in `wirbelsturm.yaml`.
# Only remove the leading `#` character in each line -- the remaining leading whitespace is significant.
$ vagrant up zookeeper1 kafka1     # <<< May take a while (boots VMs, downloads RPMs from the Internet to provision the VMs, etc.)
{code}

Now you can ssh into the VM {{kafka1}} via {{vagrant ssh kafka1}} and run the commands above from within the {{/opt/kafka}} directory.;;;","21/Mar/14 10:23;miguno;Also, I can confirm the errors above do not occur with Kafka 0.8.0, using the following test commands:

{code}
$ bin/kafka-create-topic.sh --topic testing --zookeeper zookeeper1:2181 --partition 1 --replica 1
creation succeeded!
$ bin/kafka-list-topic.sh --zookeeper zookeeper1:2181
topic: testing	partition: 0	leader: 0	replicas: 0	isr: 0

# Trying to produce data works!
$ bin/kafka-console-producer.sh --topic testing --broker-list localhost:9092
foo
^C

$ bin/kafka-console-consumer.sh --topic testing --zookeeper zookeeper1:2181 --from-beginning
foo
{code};;;","28/Mar/14 21:58;tnachen;I tried out the repro scenario described in the latest 0.8.1 branch, and with latest commit 39a5607 I see that after pausing zookeeper for 10 seconds the broker successfully registers itself afterwards.;;;","29/Mar/14 01:22;nehanarkhede;Very cool. Thanks for verifying that [~tnachen]!;;;","29/Mar/14 01:22;nehanarkhede;Fixed by KAFKA-1310;;;","09/Apr/14 18:35;jjkoshy;Fixed by KAFKA-1317;;;","04/Aug/16 09:33;Joker.D.;Excuse me, appear this error is how to solve?
What principle, each too see?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jarAll (cross-compilation) is broken,KAFKA-1309,12702043,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jjkoshy,jjkoshy,18/Mar/14 01:26,18/Mar/14 19:10,14/Jul/23 05:39,18/Mar/14 19:10,,,,,,,,,,,,,,0,,,,"This one is my bad - happened after the offset management check-in. The
issue is that that commit made the OffsetCommitRequest's request info map
mutable. The JavaConversions API for converting a Java map to scala mutable
map is different between 2.8.x and 2.9.x onwards.

Will upload a fix shortly.",,jjkoshy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/14 01:37;jjkoshy;KAFKA-1309.patch;https://issues.apache.org/jira/secure/attachment/12635219/KAFKA-1309.patch","18/Mar/14 07:53;jjkoshy;KAFKA-1309_2014-03-18_00:53:21.patch;https://issues.apache.org/jira/secure/attachment/12635254/KAFKA-1309_2014-03-18_00%3A53%3A21.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,380383,,,Tue Mar 18 19:10:14 UTC 2014,,,,,,,,,,"0|i1tjfr:",380664,,,,,,,,,,,,,,,,,,,,"18/Mar/14 01:37;jjkoshy;Created reviewboard https://reviews.apache.org/r/19338/
 against branch origin/trunk;;;","18/Mar/14 07:53;jjkoshy;Updated reviewboard https://reviews.apache.org/r/19338/
 against branch origin/trunk;;;","18/Mar/14 19:10;jjkoshy;Thanks for the review. This has been committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
potential socket leak in new producer and clean up,KAFKA-1307,12701930,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,17/Mar/14 17:06,18/Mar/14 17:56,14/Jul/23 05:39,18/Mar/14 17:56,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"Address the delta review comments from kafka-1227.

20. Selector and Sender: need to close socket channel on IOException in addition to UnresolvedHostnameException.
21. AbstractConfig:
21.1 To be consistent, we probably should change the return value of the following api from Long to long.
public Long getLong(String key)
21.1 Could we also add getDouble(String key)?
22. Metadata.fetch(): The wait time is incorrect when there is an InterruptedException.
23. Add comments on BufferPool.allocate() and Struct.instanceOf() for clarification.
24. Various other minor fixes for mis-spelling.
",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/14 17:11;junrao;KAFKA-1307.patch;https://issues.apache.org/jira/secure/attachment/12635118/KAFKA-1307.patch","18/Mar/14 16:53;junrao;KAFKA-1307_2014-03-18_09:53:00.patch;https://issues.apache.org/jira/secure/attachment/12635343/KAFKA-1307_2014-03-18_09%3A53%3A00.patch","18/Mar/14 17:03;junrao;KAFKA-1307_2014-03-18_10:03:01.patch;https://issues.apache.org/jira/secure/attachment/12635344/KAFKA-1307_2014-03-18_10%3A03%3A01.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,380270,,,Tue Mar 18 17:56:19 UTC 2014,,,,,,,,,,"0|i1tir3:",380553,,,,,,,,,,,,,,,,,,,,"17/Mar/14 17:11;junrao;Created reviewboard https://reviews.apache.org/r/19311/
 against branch origin/trunk;;;","17/Mar/14 17:14;nehanarkhede;[~junrao] You mentioned that you also have a couple of logging changes that you found useful while troubleshooting this bug. Does this patch include all of those changes as well?;;;","17/Mar/14 20:05;junrao;The logging improvement is already committed in KAFKA-1302.;;;","18/Mar/14 16:53;junrao;Updated reviewboard https://reviews.apache.org/r/19311/
 against branch origin/trunk;;;","18/Mar/14 17:03;junrao;Updated reviewboard https://reviews.apache.org/r/19311/
 against branch origin/trunk;;;","18/Mar/14 17:56;junrao;Thanks for the review. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Offset commit API, does it work?",KAFKA-1306,12701792,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,korebantic2,korebantic2,17/Mar/14 03:25,19/Mar/14 19:33,14/Jul/23 05:39,19/Mar/14 19:33,0.8.1,,,,,,,,,,,,,0,,,,"I followed this guide very carefully https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol and patched the ruby client Poseidon. Whenever I submit an OffsetCommitRequest, I receive the following error in the kafka logs:

{noformat}
23:07:33 kafka-b0.1  | java.nio.BufferUnderflowException
23:07:33 kafka-b0.1  | 	at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:145)
23:07:33 kafka-b0.1  | 	at java.nio.ByteBuffer.get(ByteBuffer.java:694)
23:07:33 kafka-b0.1  | 	at kafka.api.ApiUtils$.readShortString(ApiUtils.scala:38)
23:07:33 kafka-b0.1  | 	at kafka.api.UpdateMetadataRequest$$anonfun$readFrom$1.apply(UpdateMetadataRequest.scala:43)
23:07:33 kafka-b0.1  | 	at kafka.api.UpdateMetadataRequest$$anonfun$readFrom$1.apply(UpdateMetadataRequest.scala:42)
23:07:33 kafka-b0.1  | 	at scala.collection.immutable.Range.foreach(Range.scala:78)
23:07:33 kafka-b0.1  | 	at kafka.api.UpdateMetadataRequest$.readFrom(UpdateMetadataRequest.scala:42)
23:07:33 kafka-b0.1  | 	at kafka.api.RequestKeys$$anonfun$7.apply(RequestKeys.scala:42)
23:07:33 kafka-b0.1  | 	at kafka.api.RequestKeys$$anonfun$7.apply(RequestKeys.scala:42)
23:07:33 kafka-b0.1  | 	at kafka.network.RequestChannel$Request.<init>(RequestChannel.scala:50)
23:07:33 kafka-b0.1  | 	at kafka.network.Processor.read(SocketServer.scala:353)
23:07:33 kafka-b0.1  | 	at kafka.network.Processor.run(SocketServer.scala:245)
23:07:33 kafka-b0.1  | 	at java.lang.Thread.run(Thread.java:724)
{/noformat}

The binary data for the request looks as follows:

{noformat}
\x00\x06\x00\x00\x00\x00\x00\x02\x00\x02p1\x00\x03cg1\x00\x00\x00\x01\x00\x05repl5\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x04meta
{/noformat}

Thinking that I failed to understand how to formulate the binary request correctly, I downloaded sarama, the go kafka library. Using its API to make the request, I also received the same error. Here's an example:

https://github.com/talbright/saramit


",,junrao,korebantic2,zhangzs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,380137,,,Wed Mar 19 19:32:07 UTC 2014,,,,,,,,,,"0|i1thxr:",380421,,,,,,,,,,,,,,,,,,,,"19/Mar/14 04:11;junrao;It seems that you put in the wrong request id. The id for OffsetCommit is 8, instead of 6.;;;","19/Mar/14 04:21;junrao;Actually, just realized that the request id for OffsetCommit is documented incorrectly in the wiki. Updated the wiki.;;;","19/Mar/14 16:31;korebantic2;Man Jimi had it all wrong, it should have been if a 6 was an 8.

http://www.youtube.com/watch?v=VNXWMHu9An0

Thanks for the update, I'll work on testing it out on my end and update the issue once I verify. ;;;","19/Mar/14 19:32;korebantic2;Yup. As you said, it wasn't documented correctly. I change the ID to the request as you indicated above, and I was able to submit an OffsetCommit request. I also confirmed the data was stored in zookeeper. So it looks good.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller can hang on controlled shutdown with auto leader balance enabled,KAFKA-1305,12701385,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriharsha,jjkoshy,jjkoshy,14/Mar/14 00:08,17/May/16 13:51,14/Jul/23 05:39,13/Oct/14 19:32,,,,0.8.2.0,,,,,,,,,,1,,,,"This is relatively easy to reproduce especially when doing a rolling bounce.
What happened here is as follows:

1. The previous controller was bounced and broker 265 became the new controller.
2. I went on to do a controlled shutdown of broker 265 (the new controller).
3. In the mean time the automatically scheduled preferred replica leader election process started doing its thing and starts sending LeaderAndIsrRequests/UpdateMetadataRequests to itself (and other brokers).  (t@113 below).
4. While that's happening, the controlled shutdown process on 265 succeeds and proceeds to deregister itself from ZooKeeper and shuts down the socket server.
5. (ReplicaStateMachine actually removes deregistered brokers from the controller channel manager's list of brokers to send requests to.  However, that removal cannot take place (t@18 below) because preferred replica leader election task owns the controller lock.)
6. So the request thread to broker 265 gets into infinite retries.
7. The entire broker shutdown process is blocked on controller shutdown for the same reason (it needs to acquire the controller lock).

Relevant portions from the thread-dump:

""Controller-265-to-broker-265-send-thread"" - Thread t@113
   java.lang.Thread.State: TIMED_WAITING
	at java.lang.Thread.sleep(Native Method)
	at kafka.controller.RequestSendThread$$anonfun$liftedTree1$1$1.apply$mcV$sp(ControllerChannelManager.scala:143)
	at kafka.utils.Utils$.swallow(Utils.scala:167)
	at kafka.utils.Logging$class.swallowWarn(Logging.scala:92)
	at kafka.utils.Utils$.swallowWarn(Utils.scala:46)
	at kafka.utils.Logging$class.swallow(Logging.scala:94)
	at kafka.utils.Utils$.swallow(Utils.scala:46)
	at kafka.controller.RequestSendThread.liftedTree1$1(ControllerChannelManager.scala:143)
	at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:131)
	- locked java.lang.Object@6dbf14a7
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)

   Locked ownable synchronizers:
	- None

...

""Thread-4"" - Thread t@17
   java.lang.Thread.State: WAITING on java.util.concurrent.locks.ReentrantLock$NonfairSync@4836840 owned by: kafka-scheduler-0
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
	at kafka.utils.Utils$.inLock(Utils.scala:536)
	at kafka.controller.KafkaController.shutdown(KafkaController.scala:642)
	at kafka.server.KafkaServer$$anonfun$shutdown$9.apply$mcV$sp(KafkaServer.scala:242)
	at kafka.utils.Utils$.swallow(Utils.scala:167)
	at kafka.utils.Logging$class.swallowWarn(Logging.scala:92)
	at kafka.utils.Utils$.swallowWarn(Utils.scala:46)
	at kafka.utils.Logging$class.swallow(Logging.scala:94)
	at kafka.utils.Utils$.swallow(Utils.scala:46)
	at kafka.server.KafkaServer.shutdown(KafkaServer.scala:242)
	at kafka.server.KafkaServerStartable.shutdown(KafkaServerStartable.scala:46)
	at kafka.Kafka$$anon$1.run(Kafka.scala:42)

...

""kafka-scheduler-0"" - Thread t@117
   java.lang.Thread.State: WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1dc407fc
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:306)
	at kafka.controller.ControllerChannelManager.sendRequest(ControllerChannelManager.scala:57)
	- locked java.lang.Object@578b748f
	at kafka.controller.KafkaController.sendRequest(KafkaController.scala:657)
	at kafka.controller.ControllerBrokerRequestBatch$$anonfun$sendRequestsToBrokers$2.apply(ControllerChannelManager.scala:290)
	at kafka.controller.ControllerBrokerRequestBatch$$anonfun$sendRequestsToBrokers$2.apply(ControllerChannelManager.scala:282)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
	at kafka.controller.ControllerBrokerRequestBatch.sendRequestsToBrokers(ControllerChannelManager.scala:282)
	at kafka.controller.PartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:126)
	at kafka.controller.KafkaController.onPreferredReplicaElection(KafkaController.scala:612)
	at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4$$anonfun$apply$17$$anonfun$apply$5.apply$mcV$sp(KafkaController.scala:1119)
	at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4$$anonfun$apply$17$$anonfun$apply$5.apply(KafkaController.scala:1114)
	at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4$$anonfun$apply$17$$anonfun$apply$5.apply(KafkaController.scala:1114)
	at kafka.utils.Utils$.inLock(Utils.scala:538)
	at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4$$anonfun$apply$17.apply(KafkaController.scala:1111)
	at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4$$anonfun$apply$17.apply(KafkaController.scala:1109)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
	at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4.apply(KafkaController.scala:1109)
	at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4.apply(KafkaController.scala:1088)
	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:125)
	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)
	at kafka.controller.KafkaController.kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance(KafkaController.scala:1088)
	at kafka.controller.KafkaController$$anonfun$onControllerFailover$1.apply$mcV$sp(KafkaController.scala:323)
	at kafka.utils.KafkaScheduler$$anon$1.run(KafkaScheduler.scala:100)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

   Locked ownable synchronizers:
	- locked java.util.concurrent.locks.ReentrantLock$NonfairSync@4836840

	- locked java.util.concurrent.locks.ReentrantLock$NonfairSync@4918530

...

""ZkClient-EventThread-18-/kafka-shadow"" - Thread t@18
   java.lang.Thread.State: WAITING on java.util.concurrent.locks.ReentrantLock$NonfairSync@4836840 owned by: kafka-scheduler-0
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
	at kafka.utils.Utils$.inLock(Utils.scala:536)
	at kafka.controller.ReplicaStateMachine$BrokerChangeListener.handleChildChange(ReplicaStateMachine.scala:328)
	at org.I0Itec.zkclient.ZkClient$7.run(ZkClient.java:568)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
",,becket_qin,dmitrybugaychenko,guozhang,gwenshap,jbrosenberg@gmail.com,jjkoshy,jkreps,junrao,nehanarkhede,noslowerdna,sriharsha,stevenz3wu,TaoFeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1699,,,,,,,,,,"12/Oct/14 23:49;sriharsha;KAFKA-1305.patch;https://issues.apache.org/jira/secure/attachment/12674439/KAFKA-1305.patch","10/Oct/14 15:51;sriharsha;KAFKA-1305.patch;https://issues.apache.org/jira/secure/attachment/12674192/KAFKA-1305.patch","13/Oct/14 14:30;sriharsha;KAFKA-1305_2014-10-13_07:30:45.patch;https://issues.apache.org/jira/secure/attachment/12674511/KAFKA-1305_2014-10-13_07%3A30%3A45.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,379731,,,Mon Mar 16 12:25:12 UTC 2015,,,,,,,,,,"0|i1tfg7:",380016,,junrao,,,,,,,,,,,,,,,,,,"14/Mar/14 21:13;guozhang;This is a similar issue as for KAFKA-1235. One alternative solution to kill both birds is to allow the sender thread jump out of the infinite retry if it realize that the destination broker is shutting down.;;;","30/Apr/14 21:10;noslowerdna;Is it recommended to not set both auto.leader.rebalance.enable=true and controlled.shutdown.enable=true?

If this issue is encountered, killing the hung broker process sounds like the only resolution. Would that cause any issues for the other brokers in the cluster?  Also, would it be problematic if the hung broker is not killed in a timely manner?;;;","01/May/14 14:34;junrao;Right, until this is fixed, don't turn set auto.leader.rebalance.enable and controlled.shutdown.enable to be true.;;;","04/Sep/14 21:50;guozhang;Moving out of  0.8.2 for now.;;;","21/Sep/14 00:45;becket_qin;I looked into this problem and it seems to me the issue is mainly because the default controller queue size was too small.
The problem flow is as below:
1. Controller 265 received controlled shutdown request
2. Controller 265 put leaderAndIsrRequest into controller message queue and responded to broker 265.
3. Broker 265 received respond from Controller 265, shutdown successfully and de-registerred itself form zk.
4. Controller 265 request send thread started to send leaderAndIsrRequests which are put in step 2 to the brokers. Since broker 265 has already shutdown, it will start infinite retry. At this moment, the controller message queue size will never decrease. (Thread t@113)
5. Scheduled preferred leader election started, grabbed the controller lock and was trying to put the LeaderAndIsr request into controller message queue. However, because the queue size is only 10, it could not finish but just blocking on the put method while still holding the controller lock. (Thread t@117)
6. Broker change listener on controller 265 was triggered because broker path change in step 3, it was trying to grab the controller lock and stop thread t@113, but failed to do that because thread t@117 was holding controller lock and waiting on the controller message queue.

Currently the controller message queue size is 10. IMO if we can increase the number to be 100 or even bigger, this problem won't happen again. Actually, in most time, the number of messages in the queue will be small even empty because there should not be too many controller messages. So increasing the queue size won't cause memory consumption to increase.;;;","22/Sep/14 16:38;guozhang;Thanks [~becket_qin] for the great findings. It seems to me that as long as the controller's channel manager is async, no matter how large is its queue the corner-case issue can still happen in (i.e. request blocked in the queue for brokers that is already shutdown but the ZK watcher not fired yet), and causing some chain of lock conflicts.

Currently the controller has multiple threads for admin commands, ZK listeners, scheduled operations (leader electioner), etc, which complicates the locking mechanism inside controller. After going through the code I think it would be better to refactor the controller as following:

1. Besides the async channel manager's sender thread, we use only a single controller thread and have a single working queue for the controller thread.
3. ZK fire handling logic determines the event (topic/partition/broker change, admin operation, etc), and put the task into the queue.
4. Scheduled task is also created periodically and put into the queue.
5. The controller did one task at a time, which do not need to compete locks on controller metadata.
6. Make the channel manager's queue size infinite and add a metric on monitoring its size.

With this the controller logic would be easier to read / debug, may also help KAFKA-1558. The downside is that since a single thread is used, it loses parallelism for controller task handling, and the unbounded channel queue may also be an issue (when there is a bug). But since controller tasks are usually rare in practice, this should not be an issue.;;;","29/Sep/14 00:35;junrao;Guozhang,

Yes, I agree that the controller code is getting complicated and it's probably worth a refactoring. A single threaded controller perhaps will make the logic simpler and easier to understand. There are operations like reassigning partitions that are time consuming though. Instead of blocking on those operations, do we want to use a purgatory to track them?

Since it may take time to completely fix this issue, I suggest that we just disable this feature in 0.8.2. Otherwise, people may turn it on and hit this issue. ;;;","30/Sep/14 01:00;guozhang;Yeah. Reassigning partitions would take time and hence better be handled in a purgatory, but then there are a couple more subtle issues we need to be careful with:

1. Upon condition (new replica caught up, etc) satisfied, shall we execute the rest of the logic in the satisfaction checking thread, which will then be a different thread, or just put the rest of the job back into the queue?

2. Related to 1), as a topic is undergoing partition reassignment or any other ""delayable"" operations, we need to disable other operations under these topics, right?

I agree that this is a rather big change and maybe we should push after 0.8.2.;;;","30/Sep/14 02:48;nehanarkhede;This is a good but potentially very large change. Basically controller functionality would need to be modeled as multi-step actions that require need arbitrary delay between each of the steps that constitute an action. As such the rest of the logic definitely needs to go back into the queue.  ;;;","03/Oct/14 03:59;sriharsha;[~nehanarkhede] Is this still planned for 0.8.2 release. ;;;","04/Oct/14 00:56;nehanarkhede;[~sriharsha] Depends :) Basically, my perspective is that if doing this correctly requires delaying 0.8.2 by a month, then let's push it to 0.8.3. If there is a small fix for the issue, then let's include it. IIRC, [~junrao] was going to take a stab at thinking if there is a small fix or not.;;;","06/Oct/14 03:14;junrao;Thinking about this a bit more. As a short term fix, I think Jiangjie's suggestion actually has a point. Basically, if a channel queue is full (since the target broker is down), a thread that tries to put a request into the queue will block while holding the controller lock. This essentially will stall the controller since it can't process any other events, which is bad. Now, imagine what if we make the channel queue unbounded. In this case, no thread will block on putting requests into the queue. So, if a broker is down, the controller will always be able to act on it, which will clear up the queue and remove the channel to the dead broker. The only down side is that if there are outstanding requests in a queue, new important requests may be delayed. This is not a big concern because in the common case, the channel queue shouldn't build up.

Sriharsha,

Could you do a bit of testing on this? Basically, set the default value of controller.message.queue.size to sth large (e.g., 10K). Create a cluster with a few K partitions per broker. Enable auto leader balancing and keep doing rolling bounces of the cluster (with controlled shutdown enabled) and see there is any issue. Ideally, we want to hit the case that the auto leader balancing happens concurrently with the controlled shutdown in the controller. So, you may want to play with leader.imbalance.check.interval.seconds.;;;","06/Oct/14 19:28;jkreps;FWIW, from my perspective being able to enable auto leader balancing would be a huge win. This is arguably the biggest operational ""gotcha"" today...;;;","06/Oct/14 19:48;noslowerdna;> from my perspective being able to enable auto leader balancing would be a huge win

We are running with auto leader balance enabled and controlled shutdown disabled. Given that they're currently mutually exclusive options, is controlled shutdown generally considered more valuable than auto leader balancing? If so, why is that?;;;","06/Oct/14 19:54;jkreps;Well I guess the point is that they shouldn't be mutually exclusive. So hopefully we can make them both be enabled by default.;;;","09/Oct/14 14:50;sriharsha;[~junrao] Ran into this issue when I am testing delete topics along while simultaneously running preferred replica leader election tool. I am running the above test you suggested will update with the results.;;;","09/Oct/14 22:45;sriharsha;[~junrao]  I ran the above test you suggested with leader.imbalance.check.interval.seconds set to 30 and controller.message.queue.size set 10000. With 5 brokers and 1500 topics with 3 partitons and 3 replication factor. I am able to run into a case where a broker prints ""Shutdown completed"" but the process still hangs. Running the test by setting controller.message.queue.size  to higher number.
Here is thread dump

Full thread dump Java HotSpot(TM) 64-Bit Server VM (24.65-b04 mixed mode):

""Attach Listener"" daemon prio=10 tid=0x00007f8860003000 nid=0x26cc waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Controller-4-to-broker-3-send-thread"" prio=10 tid=0x00007f884c049000 nid=0x26b2 waiting on condition [0x00007f83c99e0000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000d2be8008> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at kafka.controller.RequestSendThread.doWork(ControllerChannelManager.scala:121)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)

""Thread-2"" prio=10 tid=0x00007f8868005800 nid=0x26b1 waiting on condition [0x00007f83c98df000]
   java.lang.Thread.State: TIMED_WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000d2a34508> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
        at java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1468)
        at kafka.utils.KafkaScheduler.shutdown(KafkaScheduler.scala:88)
        at kafka.controller.KafkaController$$anonfun$onControllerResignation$1.apply$mcV$sp(KafkaController.scala:353)
        at kafka.controller.KafkaController$$anonfun$onControllerResignation$1.apply(KafkaController.scala:348)
        at kafka.controller.KafkaController$$anonfun$onControllerResignation$1.apply(KafkaController.scala:348)
        at kafka.utils.Utils$.inLock(Utils.scala:535)
        at kafka.controller.KafkaController.onControllerResignation(KafkaController.scala:348)
        at kafka.controller.KafkaController.shutdown(KafkaController.scala:663)
        at kafka.server.KafkaServer$$anonfun$shutdown$9.apply$mcV$sp(KafkaServer.scala:287)
        at kafka.utils.Utils$.swallow(Utils.scala:172)
        at kafka.utils.Logging$class.swallowWarn(Logging.scala:92)
        at kafka.utils.Utils$.swallowWarn(Utils.scala:45)
        at kafka.utils.Logging$class.swallow(Logging.scala:94)
        at kafka.utils.Utils$.swallow(Utils.scala:45)
        at kafka.server.KafkaServer.shutdown(KafkaServer.scala:287)
        at kafka.server.KafkaServerStartable.shutdown(KafkaServerStartable.scala:40)
        at kafka.Kafka$$anon$1.run(Kafka.scala:42)

""SIGTERM handler"" daemon prio=10 tid=0x00007f8860002000 nid=0x26ae in Object.wait() [0x00007f83c9be2000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0x00000000d018bda8> (a kafka.Kafka$$anon$1)
        at java.lang.Thread.join(Thread.java:1281)
        - locked <0x00000000d018bda8> (a kafka.Kafka$$anon$1)
        at java.lang.Thread.join(Thread.java:1355)
        at java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:106)
        at java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46)
        at java.lang.Shutdown.runHooks(Shutdown.java:123)
        at java.lang.Shutdown.sequence(Shutdown.java:167)
        at java.lang.Shutdown.exit(Shutdown.java:212)
        - locked <0x00000000d0080660> (a java.lang.Class for java.lang.Shutdown)
        at java.lang.Terminator$1.handle(Terminator.java:52)
        at sun.misc.Signal$1.run(Signal.java:212)
        at java.lang.Thread.run(Thread.java:745)

""kafka-scheduler-0"" daemon prio=10 tid=0x00007f884c045000 nid=0x26aa waiting on condition [0x00007f83c9ee5000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
 - parking to wait for  <0x00000000cfb1b800> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
        at kafka.utils.Utils$.inLock(Utils.scala:533)
        at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4$$anonfun$apply$17.apply(KafkaController.scala:1149)
        at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4$$anonfun$apply$17.apply(KafkaController.scala:1147)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
        at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4.apply(KafkaController.scala:1147)
        at kafka.controller.KafkaController$$anonfun$kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance$4.apply(KafkaController.scala:1126)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:224)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)
        at kafka.controller.KafkaController.kafka$controller$KafkaController$$checkAndTriggerPartitionRebalance(KafkaController.scala:1126)        at kafka.controller.KafkaController$$anonfun$onControllerFailover$1.apply$mcV$sp(KafkaController.scala:326)
        at kafka.utils.KafkaScheduler$$anonfun$1.apply$mcV$sp(KafkaScheduler.scala:99)
        at kafka.utils.Utils$$anon$1.run(Utils.scala:54)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

""Controller-4-to-broker-1-send-thread"" prio=10 tid=0x00007f884c019800 nid=0x26a9 waiting on condition [0x00007f83c9fe6000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000d2828b58> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442);;;","09/Oct/14 23:17;junrao;Sriharsha,

It seems that you found a deadlock.

In KafkaController.onControllerResignation(), it holds the controller lock while calling autoRebalanceScheduler.shutdown(). The auto rebalance scheduler couldn't shutdown since it's blocked waiting for the controller lock in checkAndTriggerPartitionRebalance().

To break the deadlock, we can move autoRebalanceScheduler.shutdown() in onControllerResignation() to before acquiring the controller lock. We don't need to hold on the controller lock while shutting down the scheduler.;;;","09/Oct/14 23:20;sriharsha;Thanks [~junrao]. Testing with above change.;;;","10/Oct/14 01:47;sriharsha;[~junrao] ran tests with your suggested change. All of the controller shutdown went through without any issue.
I'll file a separate JIRA for autoRebalanceScheduler.shutdown deadlock.;;;","10/Oct/14 04:17;junrao;Sriharsha,

Thanks for testing this out. Based on your result, it seems that auto leader balancing is stable with a large controller channel queue. Could you create a patch by changing the default value of controller.message.queue.size to 10000 and auto.leader.rebalance.enable to true? Once that's done, we can resolve this jira. We can file a separate jira for controller refactoring.

;;;","10/Oct/14 15:51;sriharsha;Created reviewboard https://reviews.apache.org/r/26560/diff/
 against branch origin/trunk;;;","10/Oct/14 16:10;nehanarkhede;[~junrao], [~sriharsha] What's the value in changing it from something to 10K vs unbounded?;;;","10/Oct/14 17:19;sriharsha;[~nehanarkhede] [~junrao] my understanding is that we created more room for KafkaController not to get into any of the above mentioned issues by setting to 10k but yes making unbounded is a better option as there could be a chance of exhausting 10k bounded queue and run into issues. We can get rid off  controller.message.queue.size as config option and make the LinkedBlockingQueue unbounded.  
;;;","10/Oct/14 17:44;junrao;Yes, in theory, we can make the queue unbounded. However, in practice, the queue shouldn't build up. I was a bit concerned that if we make the queue unbounded and another issue that causes the queue to build up, we may hit OOME. Then, we may not be able to take a thread dump to diagnose the issue.;;;","12/Oct/14 22:25;nehanarkhede;Increasing the queue size by a little doesn't really solve the problem. We should conduct more tests on an unbounded controller queue, if we have any doubt whether or not it will work. [~sriharsha] I will help you review changes to the controller, if you are up for updating your patch.;;;","12/Oct/14 23:20;sriharsha;[~nehanarkhede] so the changes you are looking for are remove the config option and make the LinkedBlockingQueue to unbounded?;;;","12/Oct/14 23:49;sriharsha;Created reviewboard https://reviews.apache.org/r/26633/diff/
 against branch origin/trunk;;;","13/Oct/14 04:20;junrao;Perhaps we can just set the default queue to max int for now. If we don't see any issue with this, we can make LinkedBlockingQueue to unbounded later. Could you also change the default value for auto.leader.rebalance.enable?;;;","13/Oct/14 14:30;sriharsha;Updated reviewboard https://reviews.apache.org/r/26633/diff/
 against branch origin/trunk;;;","13/Oct/14 18:25;nehanarkhede;[~sriharsha] Latest patch looks good. Pushing it to trunk and 0.8.2;;;","16/Nov/14 16:55;jbrosenberg@gmail.com;Is it safe to say then, if we are not yet on 0.8.2 (e.g. still on 0.8.1.1), we should not enable automatic preferred leader election?;;;","16/Nov/14 18:29;sriharsha;[~jbrosenberg@gmail.com] The fix is in config. So if you are using 0.8.1.1 you can enable automatic preferred leader election but make sure you set controller.message.queue.size to Int.MaxValue by default this is set to 10 in 0.8.1.1.;;;","13/Mar/15 12:01;dmitrybugaychenko;With a large controller queues size we see a significant datalos during prefered replica election for a brokers leading a lot of partitions (100+). The problem is that the prefered replica handles its requests fast, empties its request queue and stop following others, but the old leader hadles request much slower and it takse few minutes before it stop considering itself as a leader for all re-elected partitions. During these few minutes old leader continue to acknowledge produce requests and at the end it recognize its not longer a leader and truncates its logs deleting all data received...;;;","13/Mar/15 16:57;becket_qin;That's a good point. But I kind of think the right solution to that problem does not lie in the queue size. Because there will be data loss in leader migration today, more or less. The amount is actually non-deterministic. So my understanding is either user can tolerate data loss or user needs to use acks=-1.;;;","13/Mar/15 21:42;dmitrybugaychenko;Yes, data loss is tolerated to some extend. With small queue it was about lossing less then a second or even none of data and were considered fine, but with extended queue it is about few minutes - using acks in this case will simply cause producers to crash, denie their service or drop messages (because for few minutes they basically can not produce).  In the end we decided to reduce the queue to default and to apply three patches:

# Add throttling to prefered replica elections and controlled shuttdown leadership reasignement
# Add timeout for adding messages to queue in order to avoid locked controller
# Add separate timeout for sending controlled shutdown message - in our setup it takes about 10 minutes and this value is meaningless and dangerous for other kind of controller-to-broker communication

Things seems to work and data are not lost, but shutdown and rebalance are slow. Instead of throttling it could be better to wait for previous leader movement to be completed by all the participatnts before moving to next one. It is also possible to do leader movements in batches (at least api seems to support that).;;;","13/Mar/15 22:13;becket_qin;It is not clear to me how adding timeout when put messages to broker queue in controller would help. This operation is done in the controller lock and have to be infinitely retry. I would guess in a parallel shutdown, you might still see deadlock.;;;","14/Mar/15 05:53;dmitrybugaychenko;Retry itself is in the cahnnel manager independent of controller lock. Deadlock happens because one of the threads owning controller lock trying to put message to channel manager - it waits for free space but won't evere get it. With timeout it won't wait forever and eventually fail the operation given controller a chance to handle broker failure (which includes closing corresponding channel and emptying its queue).;;;","14/Mar/15 08:10;becket_qin;I see. The risk of this approach is that controller or broker could potentially be in a inconsistent state. Because it is not necessarily the case that timeout occurs on broker shutdown. In that case, some controller to broker messages are sent while some might not.
I think the key problem of current approach is that we mix the data plain and control plain, i.e. the controller message and user data are handled by same request handlers on Kafka server. So controller messages usually sitting in the queue behind many user requests. That could cause the handling of controller messages to delay for almost arbitrary time (the more leader a broker has, the worse the situation will be). The right solution is probably having a separate thread handling controller message or prioritize controller message handling. Giving priority to controller message probably has less change because we just need to insert the controller message to the head of the queue instead of the tail.;;;","16/Mar/15 12:25;dmitrybugaychenko;Even with a fast dedicated channel there will be a race condition in switching leadership. It could be removed either by complicating the protocol (eg. the new leader shoul take leadership only after getting ""not a leader"" respone in fetcher thread from the old one, while the old leader should stop handling produce request allowing fetches only from the new leader untill it gets everything), or, may be, it is worth to consider getting rid of controller in partition leader election and use distributed elections in ZK.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cleanup logging in new producer,KAFKA-1302,12701011,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,12/Mar/14 16:29,03/Apr/14 16:15,14/Jul/23 05:39,13/Mar/14 21:27,0.8.2.0,,,0.8.2.0,,,,,,,core,,,0,,,,"1. When we hit an error in producer, we call the callback with a null RecordMetadata. If the callback tries to get the topic/partition, it will hit a NullPointerException. It's probably better to create a RecordMetadata with -1 as the offset.
2. When printing out a Struct, we don't print out the content wrapped in an array properly. So, we will see sth like the following.
[2014-03-09 11:56:24,364] INFO Created 1 requests: [InFlightRequest(expectResponse=true, batches={test-0=RecordBatch(topicPartition=test-0, recordCount=1)}, request=RequestSend(header={api_key=0,api_version=0,correlation_id=1,client_id=perf-test}, body={acks=-1,timeout=3000,topic_data=[Ljava.lang.Object;@700a4488}))] (org.apache.kafka.clients.producer.internals.Sender)
3. Need to override the toString() in ProduceResponse.
4. Sender.run(now): It would be good to log metadata request too.
5. Sender.handleDisconnects(): It would be useful to log the correlation id of cancelled inflight requests
",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1296,,,,,,,,,,,,,,,,,,,"12/Mar/14 16:31;junrao;KAFKA-1302.patch;https://issues.apache.org/jira/secure/attachment/12634199/KAFKA-1302.patch","13/Mar/14 18:00;junrao;KAFKA-1302_2014-03-13_11:00:33.patch;https://issues.apache.org/jira/secure/attachment/12634497/KAFKA-1302_2014-03-13_11%3A00%3A33.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,379357,,,Thu Mar 13 21:27:16 UTC 2014,,,,,,,,,,"0|i1td6f:",379648,,,,,,,,,,,,,,,,,,,,"12/Mar/14 16:31;junrao;Created reviewboard https://reviews.apache.org/r/19132/
 against branch origin/trunk;;;","13/Mar/14 18:00;junrao;Updated reviewboard https://reviews.apache.org/r/19132/
 against branch origin/trunk;;;","13/Mar/14 21:27;junrao;Thanks for the reviews. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system testcase_0206 fails using the new producer,KAFKA-1301,12699810,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,10/Mar/14 17:38,12/Mar/14 03:58,14/Jul/23 05:39,12/Mar/14 03:58,0.8.2.0,,,0.8.2.0,,,,,,,,,,0,,,,"The problem is that the producer doesn't drain the unsent data properly on close. The problem is in the following code in Sender.run(). It's possible for this loop to exit with unfinished requests.

        // okay we stopped accepting requests but there may still be
        // requests in the accumulator or waiting for acknowledgment,
        // wait until these are completed.
        int unsent = 0;
        do {
            try {
                unsent = run(time.milliseconds());
            } catch (Exception e) {
                log.error(""Uncaught error in kafka producer I/O thread: "", e);
            }
        } while (unsent > 0 || this.inFlightRequests.totalInFlightRequests() > 0);

Suppose that all produce requests are being sent, but the sender is waiting for responses. Then the broker failed. In handling disconnects, we cleared all inflight requests. When we check the condition in the while clause, there is no unsent data and no in flight requests. However, failed records have been added to RecordAccumulator and are ready to be sent in the next iteration.",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/14 01:50;junrao;KAFKA-1301.patch;https://issues.apache.org/jira/secure/attachment/12634079/KAFKA-1301.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,378156,,,Wed Mar 12 03:58:08 UTC 2014,,,,,,,,,,"0|i1t5sf:",378448,,,,,,,,,,,,,,,,,,,,"12/Mar/14 01:50;junrao;Created reviewboard https://reviews.apache.org/r/19088/
 against branch origin/trunk;;;","12/Mar/14 03:58;junrao;Thanks for the review. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
releaseTarGz target needs signing task,KAFKA-1297,12699308,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,07/Mar/14 01:05,14/Jun/14 15:23,14/Jul/23 05:39,20/Mar/14 16:08,0.8.2.0,,,0.8.2.0,,,,,,,,,,0,,,,"./gradlew releaseTarGz

three warnings found
:core:processResources UP-TO-DATE
:core:classes
:core:copyDependantLibs UP-TO-DATE
:core:jar
:core:signArchives FAILED

FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':core:signArchives'.
> Cannot perform signing task ':core:signArchives' because it has no configured signatory

",,coderplay,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,377655,,,Sat Jun 14 15:23:00 UTC 2014,,,,,,,,,,"0|i1t2pj:",377947,,,,,,,,,,,,,,,,,,,,"07/Mar/14 01:06;junrao;Joe Stein,

Do you know how we can run the releaseTarGz target w/o setting the signing key?;;;","20/Mar/14 15:22;junrao;We can use the following workaround. Then, we don't need to set up the signing key. Will check in a trivial change in README.

./gradlew releaseTarGz -x signArchives;;;","20/Mar/14 16:08;junrao;Committed to trunk.;;;","14/Jun/14 00:20;coderplay;Hi Jun,

Seems still can't work when building kafka like below

./gradlew releaseTarGzAll -x signArchives ;;;","14/Jun/14 15:23;junrao;Not sure why it doesn't work. However, if you run the individual command, it works.
./gradlew -PscalaVersion=2.10.1 releaseTarGz  -x signArchives;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLONE (0.8.1) - Minor typos in documentation,KAFKA-1295,12699193,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,jkreps,zackse,zackse,06/Mar/14 16:58,06/Mar/14 17:17,14/Jul/23 05:39,06/Mar/14 17:17,0.8.1,,,,,,,,,,website,,,0,documentation,,,"Several typos (misspellings, missing punctuation) appear in the introduction, configuration, design, and ops pages. For example:

it's -> its
consumers -> consumer's
chose -> choose
consumers instances -> consumer instances
then -> than
",,jkreps,zackse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1269,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 17:05;zackse;KAFKA-1295-doc-typos.patch;https://issues.apache.org/jira/secure/attachment/12633175/KAFKA-1295-doc-typos.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,377540,,,Thu Mar 06 17:17:11 UTC 2014,,,,,,,,,,"0|i1t20f:",377834,,,,,,,,,,,,,,,,,,,,"06/Mar/14 16:59;zackse;Updated patch for 0.8.1 per [~jkreps] update in BNMA-1275.;;;","06/Mar/14 17:05;zackse;Adapted from patch for BNMA-1269.;;;","06/Mar/14 17:17;jkreps;Thanks! Applied.

BTW since you seem to have strong writing skills if you see anything where the explanation is unclear or could be improved feel free to point those out too. Those are often the hardest to spot in ones own writing so I greatly appreciate any feedback.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mirror maker housecleaning,KAFKA-1293,12699189,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,jkreps,jkreps,06/Mar/14 16:35,26/Apr/15 15:47,14/Jul/23 05:39,26/Apr/15 15:47,0.8.1,,,,,,,,,,tools,,,0,usability,,,"Mirror maker uses it's own convention for command-line arguments, e.g. --num.producers, where everywhere else follows the unix convention like --num-producers. This is annoying because when running different tools you have to constantly remember whatever quirks of the person who wrote that tool.

Mirror maker should also have a top-level wrapper script in bin/ to make tab completion work and so you don't have to remember the fully qualified class name.",,becket_qin,jkreps,junrao,mwarhaftig,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/15 20:10;mwarhaftig;KAFKA-1293.patch;https://issues.apache.org/jira/secure/attachment/12706411/KAFKA-1293.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,377536,,,Sun Apr 26 15:47:23 UTC 2015,,,,,,,,,,"0|i1t1zr:",377830,,,,,,,,,,,,,,,,,,,,"22/Mar/15 20:10;mwarhaftig;Created reviewboard https://reviews.apache.org/r/32378/diff/
 against branch origin/trunk;;;","22/Mar/15 20:21;mwarhaftig;In addition to Mirror Maker other tools don't follow argument naming conventions.   So, going a step further than the original request I have submitted a patch with updates to:
* Standardize tool argument names.
* Properly format argument descriptions (into sentences) and add any missing ""REQUIRED"" notes.
* Add 'help' argument to any top-level tool scripts that were missing it.

Since argument name changes would have the most impact here is the list:
|*Command*|*Old Argument Name*|*New Argument Name*|
|kafka-console-consumer.sh|consumer.config|consumer-config|
|kafka-consumer-offset-checker.sh|socket.timeout.ms|socket-timeout-ms|
|kafka-consumer-offset-checker.sh|retry.backoff.ms|retry-backoff-ms |
|ExportZkOffsets.scala|zkconnect|zookeeper|
|ImportZkOffsets.scala|zkconnect|zookeeper|
|KafkaMigrationTool.java|consumer.config|consumer-config|
|KafkaMigrationTool.java|producer.config|producer-config|
|KafkaMigrationTool.java|num.producers|num-producers|
|KafkaMigrationTool.java|num.streams|num-streams|
|KafkaMigrationTool.java|queue.size|queue-size|
|kafka-mirror-maker.sh|consumer.config|consumer-config|
|kafka-mirror-maker.sh|producer.config|producer-config|
|kafka-mirror-maker.sh|num.streams|num-streams|
|kafka-producer-perf-test.sh|request-num-acks|request-required-acks|
|kafka-replay-log-producer.sh|inputtopic|input-topic|
|kafka-replay-log-producer.sh|outputtopic|output-topic|
|kafka-replica-verification.sh|topic-white-list|whitelist|
|kafka-replica-verification.sh|report-interval-ms|reporting-interval|
|kafka-simple-consumer-shell.sh|fetchsize|fetch-size|
|TestLogCleaning.scala|zk|zookeeper|
|VerifyConsumerRebalance.scala|zookeeper.connect|zookeeper|
|TestLinearWriteSpeed.scala|compression|compression-codec|
|TestOffsetManager.scala|thread-count|threads|
|TestOffsetManager.scala|reporting-interval-ms|reporting-interval|

No worries if some of these changes are too dramatic to make it to the codebase.;;;","25/Mar/15 00:43;becket_qin;[~mwarhaftig] It looks very closely related to the work in KIP-14. Please feel free to own that KIP if you want to.
https://cwiki.apache.org/confluence/display/KAFKA/KIP-14+-+Tools+Standardization
Since this is a public interface change, we need to go through the KIP process. You can find the KIP process here:
https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals;;;","26/Mar/15 00:47;mwarhaftig;Thanks for pointing out KIP-14 [~becket_qin], it is a very similar proposal.

I would be happy to update KIP-14 with change details and continue its KIPs process - can you give me wiki edit access (username mwarhaftig) or point me towards an admin who can?;;;","30/Mar/15 00:15;nehanarkhede;cc [~junrao] who can help with the access to the wiki.;;;","04/Apr/15 23:02;jkreps;[~mwarhaftig] Did we get you wiki access?;;;","05/Apr/15 00:02;mwarhaftig;[~jkreps] No, still don't have edit permissions.;;;","05/Apr/15 15:42;junrao;[~mwarhaftig], granted wiki permission to you.;;;","08/Apr/15 01:14;mwarhaftig;Thanks everyone!  Moving forward on KIP-14.;;;","09/Apr/15 03:25;mwarhaftig;[~jkreps] I broke out command line argument standardization to KIP-14 for items that require discussion and KAFKA-2111 for the less invasive changes that don't require a vote.  This ticket can be closed and Ill submit an updated ReviewBoard request for KAFKA-2111.;;;","26/Apr/15 15:47;nehanarkhede;Closing based on [~mwarhaftig]'s latest comment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misc. nitpicks in log cleaner,KAFKA-1289,12698565,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jkreps,jkreps,04/Mar/14 04:08,27/May/14 18:43,14/Jul/23 05:39,04/Mar/14 19:41,0.8.1,,,0.8.1,,,,,,,,,,0,,,,"There are a couple of minor annoyances in the log cleaner in 0.8.1. Since this is one of the major features it would be nice to address these.

Problems:
1. Logging is no longer going to the kafka-cleaner.log 
2. Shutdown when the log cleaner is enabled is very slow
3. TestLogCleaner uses obsolete properties for the producer and consumer

In addition I want to change the configuration from ""dedupe"" to ""compact"" as we don't use the terminology dedupe anywhere else and I think it is less intuitive.",,jkreps,joestein,junrao,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1380,,,,"04/Mar/14 04:46;jkreps;KAFKA-1289-v1.patch;https://issues.apache.org/jira/secure/attachment/12632439/KAFKA-1289-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,376923,,,Tue Mar 04 20:31:09 UTC 2014,,,,,,,,,,"0|i1sy87:",377218,,,,,,,,,,,,,,,,,,,,"04/Mar/14 04:46;jkreps;1. Fix logging by keying everything off the same class
2. Decrease backoff time slightly to make shutdown a little faster
3. Fix TestLogCleaner properties
4. Expose the setting to enable the log cleaner in server.properties since we are defaulting it off for now
5. Change the default number of i/o threads to 8;;;","04/Mar/14 05:08;sriramsub;+1;;;","04/Mar/14 05:35;junrao;+1;;;","04/Mar/14 19:41;joestein;applying to 0.8.1 branch;;;","04/Mar/14 20:31;joestein;committed to 0.8.1 and trunk, thanks!!!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add enclosing dir in release tar gz,KAFKA-1288,12698530,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,03/Mar/14 23:45,27/May/14 18:42,14/Jul/23 05:39,04/Mar/14 20:31,0.8.1,,,0.8.1,,,,,,,packaging,,,0,,,,,,joestein,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1380,,,,"03/Mar/14 23:47;junrao;KAFKA-1288.patch;https://issues.apache.org/jira/secure/attachment/12632387/KAFKA-1288.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,376888,,,Tue Mar 04 20:31:54 UTC 2014,,,,,,,,,,"0|i1sy0f:",377183,,,,,,,,,,,,,,,,,,,,"03/Mar/14 23:47;junrao;Created reviewboard https://reviews.apache.org/r/18716/
 against branch origin/trunk;;;","04/Mar/14 20:31;joestein;applying to 0.8.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log4jAppender is unable to send the message.,KAFKA-1283,12697603,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriharsha,style95,style95,27/Feb/14 05:51,20/Jun/14 20:24,14/Jul/23 05:39,20/Jun/14 20:24,0.8.0,,,0.8.0,,,,,,,producer ,,,0,newbie,,,"User application can`t send any messages via KafkaLog4jAppender.

Here is log4j.properties.
----------------------------------------------------------------------------------------------
log4j.rootLogger=INFO, stdout, KAFKA

log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%5p [%t] (%F:%L) - %m%n


log4j.appender.KAFKA=kafka.producer.KafkaLog4jAppender
log4j.appender.KAFKA.layout=org.apache.log4j.PatternLayout
log4j.appender.KAFKA.layout.ConversionPattern=%-5p: %c - %m%n
log4j.appender.KAFKA.BrokerList=hnode01:9092
log4j.appender.KAFKA.Topic=DKTestEvent

#log4j.appender.KAFKA.SerializerClass=kafka.log4j.AppenderStringEncoder
----------------------------------------------------------------------------------------------


And this is a sample application.
----------------------------------------------------------------------------------------------
import org.apache.log4j.Logger;
import org.apache.log4j.BasicConfigurator;
import org.apache.log4j.PropertyConfigurator;

public class HelloWorld {

	static Logger logger = Logger.getLogger(HelloWorld.class.getName());

	public static void main(String[] args) {
		PropertyConfigurator.configure(args[0]);

		logger.info(""Entering application."");
		logger.debug(""Debugging!."");
		logger.info(""Exiting application."");
	}
}
----------------------------------------------------------------------------------------------

Since my project is maven project, I attached pom.xml also.

----------------------------------------------------------------------------------------------
<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
	xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"">
	<modelVersion>4.0.0</modelVersion>
	<groupId>com.my.app</groupId>
	<artifactId>log4-appender</artifactId>
	<version>0.0.1-SNAPSHOT</version>

	<dependencies>
		<dependency>
			<groupId>org.apache.kafka</groupId>
			<artifactId>kafka_2.8.2</artifactId>
			<version>0.8.0</version>
		</dependency>

		<dependency>
			<groupId>log4j</groupId>
			<artifactId>log4j</artifactId>
			<version>1.2.17</version>
		</dependency>
	</dependencies>

</project>
----------------------------------------------------------------------------------------------------------


And I am getting these error:
----------------------------------------------------------------------------------------------
INFO [main] (Logging.scala:67) - Verifying properties
 INFO [main] (Logging.scala:67) - Property metadata.broker.list is overridden to hnode01:9092
 INFO [main] (Logging.scala:67) - Property serializer.class is overridden to kafka.serializer.StringEncoder
 INFO [main] (HelloWorld.java:14) - Entering application.
 INFO [main] (HelloWorld.java:14) - Fetching metadata from broker id:0,host:hnode01,port:9092 with correlation id 0 for 1 topic(s) Set(DKTestEvent)
 INFO [main] (HelloWorld.java:14) - Fetching metadata from broker id:0,host:hnode01,port:9092 with correlation id 1 for 1 topic(s) Set(DKTestEvent)
 INFO [main] (HelloWorld.java:14) - Fetching metadata from broker id:0,host:hnode01,port:9092 with correlation id 2 for 1 topic(s) Set(DKTestEvent)
 INFO [main] (HelloWorld.java:14) - Fetching metadata from broker id:0,host:hnode01,port:9092 with correlation id 3 for 1 topic(s) Set(DKTestEvent)
 INFO [main] (HelloWorld.java:14) - Fetching metadata from broker id:0,host:hnode01,port:9092 with correlation id 4 for 1 topic(s) Set(DKTestEvent)
 INFO [main] (HelloWorld.java:14) - Fetching metadata from broker id:0,host:hnode01,port:9092 with correlation id 5 for 1 topic(s) Set(DKTestEvent)
.
.
.
INFO [main] (HelloWorld.java:14) - Fetching metadata from broker id:0,host:hnode01,port:9092 with correlation id 60 for 1 topic(s) Set(DKTestEvent)
 INFO [main] (HelloWorld.java:14) - Fetching metadata from broker id:0,host:hnode01,port:9092 with correlation id 61 for 1 topic(s) Set(DKTestEvent)
 INFO [main] (HelloWorld.java:14) - Fetching metadata from broker id:0,host:hnode01,port:9092 with correlation id 62 for 1 topic(s) Set(DKTestEvent)
 INFO [main] (Logging.scala:67) - Fetching metadata from broker id:0,host:hnode01,port:9092 with correlation id 63 for 1 topic(s) Set(DKTestEvent)
 INFO [main] (Logging.scala:67) - Fetching metadata from broker id:0,host:hnode01,port:9092 with correlation id 64 for 1 topic(s) Set(DKTestEvent)
 INFO [main] (Logging.scala:67) - Fetching metadata from broker id:0,host:hnode01,port:9092 with correlation id 65 for 1 topic(s) Set(DKTestEvent)
 INFO [main] (Logging.scala:67) - Fetching metadata from broker id:0,host:hnode01,port:9092 with correlation id 66 for 1 topic(s) Set(DKTestEvent)
 INFO [main] (Logging.scala:67) - Fetching metadata from broker id:0,host:hnode01,port:9092 with correlation id 67 for 1 topic(s) Set(DKTestEvent)
.
.
.
 INFO [main] (Logging.scala:67) - Fetching metadata from broker id:0,host:hnode01,port:9092 with correlation id 534 for 1 topic(s) Set(DKTestEvent)
ERROR [main] (Logging.scala:67) - 
ERROR [main] (Logging.scala:67) - 
ERROR [main] (Logging.scala:67) - 
ERROR [main] (Logging.scala:67) - 
ERROR [main] (Logging.scala:67) - 
ERROR [main] (Logging.scala:67) - 
java.lang.StackOverflowError
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:643)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:277)
	at java.net.URLClassLoader.access$000(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:212)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:643)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:277)
	at java.net.URLClassLoader.access$000(URLClassLoader.java:73)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:212)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
	at org.apache.log4j.spi.ThrowableInformation.getThrowableStrRep(ThrowableInformation.java:87)
	at org.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:413)
	at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:313)
	at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)
	at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
	at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
	at org.apache.log4j.Category.callAppenders(Category.java:206)
	at org.apache.log4j.Category.forcedLog(Category.java:391)
	at org.apache.log4j.Category.error(Category.java:322)
	at kafka.utils.Logging$$anonfun$swallowError$1.apply(Logging.scala:105)
	at kafka.utils.Logging$$anonfun$swallowError$1.apply(Logging.scala:105)
	at kafka.utils.Utils$.swallow(Utils.scala:189)
	at kafka.utils.Logging$class.swallowError(Logging.scala:105)
	at kafka.utils.Utils$.swallowError(Utils.scala:46)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:67)
	at kafka.producer.Producer.send(Producer.scala:76)
	at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:96)
	at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
	at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
	at org.apache.log4j.Category.callAppenders(Category.java:206)
	at org.apache.log4j.Category.forcedLog(Category.java:391)
	at org.apache.log4j.Category.info(Category.java:666)
	at kafka.utils.Logging$class.info(Logging.scala:67)
	at kafka.client.ClientUtils$.info(ClientUtils.scala:31)
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:51)
	at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82)
	at kafka.producer.async.DefaultEventHandler$$anonfun$handle$1.apply$mcV$sp(DefaultEventHandler.scala:67)
	at kafka.utils.Utils$.swallow(Utils.scala:187)
	at kafka.utils.Logging$class.swallowError(Logging.scala:105)
	at kafka.utils.Utils$.swallowError(Utils.scala:46)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:67)
	at kafka.producer.Producer.send(Producer.scala:76)
	at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:96)
	at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
	at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
.
.
.
----------------------------------------------------------------------------------------------

I am getting above error continuously if i don`t terminate the program.",ubuntu. eclipse.,babakbehzad,cwimmer,nehanarkhede,sriharsha,style95,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,376077,,,Fri Jun 20 20:24:36 UTC 2014,,,,,,,,,,"0|i1st0n:",376373,,,,,,,,,,,,,,,,,,,,"04/Jun/14 22:17;babakbehzad;Any progress on this? I am having the same issue!;;;","05/Jun/14 05:32;nehanarkhede;[~babakbehzad] Not really so far. We are up for reviewing your patch, if you'd like to take a stab at the fix.;;;","19/Jun/14 05:13;babakbehzad;[~nehanarkhede]: I came up with a patch for this today! I am not sure if it's a general one, but it works for our case. Should I sync up with  [~harsha_ch]?;;;","20/Jun/14 15:38;sriharsha;[~nehanarkhede] [~babakbehzad] It looks like this issue is fixed in the trunk but I am able to reproduce this in 0.8.1.
Do we need to fix this in 0.8.1 branch.
;;;","20/Jun/14 15:50;babakbehzad;Great! I can test this soon. I will let you know about the results.;;;","20/Jun/14 20:19;babakbehzad;Thanks [~sriharsha]. I pulled the trunk and the issue is fixed. I can see that current correct Log4j appender is using KafkaProducer client rather than a Kafka Producer which makes more sense. There's however some lack of features (such as not handling Log4j ConversionPattern, etc.). [~nehanarkhede], should I create a new Jira for this and you want to close this one?;;;","20/Jun/14 20:24;nehanarkhede;bq.  Neha Narkhede, should I create a new Jira for this and you want to close this one?

Sure. That makes sense.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disconnect idle socket connection in Selector,KAFKA-1282,12697266,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nmarasoi,junrao,junrao,26/Feb/14 00:34,16/Oct/15 01:54,14/Jul/23 05:39,16/Oct/15 01:54,0.8.2.0,,,0.9.0.0,,,,,,,producer ,,,0,newbie++,,,"To reduce # socket connections, it would be useful for the new producer to close socket connections that are idle. We can introduce a new producer config for the idle time.",,donnchadh,jjkoshy,jkreps,junrao,me.venkatr,nehanarkhede,nmarasoi,nmarasoiu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1941,,,,,,,,,,,,,,,,,,,KAFKA-1928,,"01/Jun/15 13:57;nmarasoi;1282_access-order_+_test_(same_class).patch;https://issues.apache.org/jira/secure/attachment/12736556/1282_access-order_%2B_test_%28same_class%29.patch","22/Aug/14 14:48;nmarasoiu;KAFKA-1282_Disconnect_idle_socket_connection_in_Selector.patch;https://issues.apache.org/jira/secure/attachment/12663665/KAFKA-1282_Disconnect_idle_socket_connection_in_Selector.patch","01/Jun/15 14:01;nmarasoi;access-order_+_test.patch;https://issues.apache.org/jira/secure/attachment/12736558/access-order_%2B_test.patch","01/Jun/15 14:24;nmarasoi;access_order_+_test_waiting_from_350ms_to_1100ms.patch;https://issues.apache.org/jira/secure/attachment/12736567/access_order_%2B_test_waiting_from_350ms_to_1100ms.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,375740,,,Fri Oct 16 01:54:45 UTC 2015,,,,,,,,,,"0|i1sqxz:",376036,,nehanarkhede,,,,,,,,,,,,,,,,,,"18/Jul/14 13:07;nmarasoiu;Right, the limitation is more critical on the client side of a client-server connection due to port count limitation, and/or socket/file count restrictions of the client env.

On the other hand, the brokers could close the connections too on such condition, rather than relying on the clients(producers) to protect it.

However, what is any other reason to reduce the socket connections count? To make the NIO select lighter on the server, on a lesser number of connections? I think epoll is quite relaxed on this.

I would like to work on this, but also understand the original problem(s) / concern(s) to see if we can also see any more suitable solutions to the particular concern?;;;","18/Jul/14 15:54;jkreps;The goal is just to reduce server connection count. In our environment there might be a single Kafka producer in each process we run publishing to a small Kafka cluster (say ~20 servers). However there are tens of thousands of client processes. Connections can end up going unused when leadership migrates and we should eventually close these out rather than retaining them indefinitely.

As you say it is not critical as the server seems to do a good job of dealing with high connection counts, but it seems like a good thing to do.

I agree that doing this on the server might be better. This does mean it is possible that the server will attempt to close the socket while the client is attempting to send something. But if the timeout is 10 mins, it is unlikely that this will happen often (i.e. if nothing was sent in the last 10 mins, it will not likely happen in the 0.5 ms it takes to do the close). The advantage of doing it on the server is that it will work for all clients.

This change would be in core/.../kafka/network/SocketServer.scala.

The only gotcha is that we likely need to avoid iterating over all connections to avoid latency impact (there could be 100k connections). One way to do this would be to use java.util.LinkedHashMap to implement an LRU hash map of the SelectionKeys, and access this every time the selection key comes up in a select operation. (There are a ton of details in LinkedHashMap--needs to be ""access order"", etc). Then every 5-10 select loop iterations we would iterate the map expiring connections until we come to a connection that doesn't need expiring, then stop.;;;","18/Jul/14 16:15;nmarasoiu;Beautiful, I can't wait to work this out, so I take this to code right?:)
;;;","23/Jul/14 13:49;nmarasoiu;[~junrao] You agree with the approach, do you?;;;","23/Jul/14 14:48;junrao;Yes. Thanks for picking it up.;;;","06/Aug/14 13:46;nehanarkhede;Hey [~nmarasoiu], are you actively working on this patch yet? If not, do you mind if we have someone else pick it up?;;;","08/Aug/14 14:52;nmarasoi;Hi,

I will spend up to 4 hours per day the next week (11-15 august), when I have this time.
So I would like to keep this nice task.
My estimate, I will have a first working solution to put up for review in ~3 days, so Thursday.

Does that sound good?;;;","12/Aug/14 21:11;nmarasoi;I attached a first version of the patch.
I am still thinking on any other implications, but wanted to share a first draft to collect some feedback already.
Thanks;;;","14/Aug/14 14:33;nehanarkhede;Thanks for picking this up [~nmarasoi]. Assigning to myself for review. ;;;","14/Aug/14 14:49;nehanarkhede;Took a look at the patch. How about the following -
1. Limit the LRU cache size to the number of active connections that should be supported by the Kafka server. I'm guessing this should be a config. 
2. Override removeEldestEntry to evict the oldest entry if the cache size exceeds the configured number of LRU connections.

That way, we don't have to traverse the map several times in the main loop, which can be expensive.;;;","14/Aug/14 15:05;nmarasoi;Traversing is quite cheap (it is traversing a linked list underneath, and only a prefix of it) and can be done every 1000 selects.
The intent of your suggestion is to optimize, I understand, but the effects is a different behavior as I feel it (changes the expiration by time and switches it to an expiration by connection count), and to a low performance benefit (I think traversing is much cheaper than blocking close on each channel, that would happen either way).
The idea of limited connection count can be used complementary to the existing traversing, but if you mean to take out the traversing every n selects, that changes the expiration by time and switches it to an expiration by connection count - is it an agreed requirements change with [~junrao]? I must warn that it is dangerous in my view to configure a maximum connection count per broker, because in event many brokers go down, and many clients need to use the system, this connection thrashing would not help anybody, and be a worse effect than not having this connection expiration at all, in such a scenario, relevant to a highly available system.;;;","14/Aug/14 18:56;nmarasoi;To make the ~O(1) cost of ""traversing"" more clear, typically only the first element in the linked list is accessed, and it will typically be used in the last 10 minutes, and in this case nothing happens anymore. Of course, this is if the low volume topics do not generate many connections, which they won't, with this cleaning up in place. And I am checking now that map() and the rest are lazy, or else for sure I can make so that only the relevant ""prefix/first"" part of the collection is iterated, typically first element only.;;;","15/Aug/14 00:31;nehanarkhede;My suggestion was not just to address the performance concern which is somewhat of an issue nevertheless. The motivation was that there is an upper bound on the number of open connections you can support on the broker. That number is the # of open file handles configured on the box. Since that number is known anyway, you probably would want to configure your server so that the connections never exceed a certain percentage of that upper limit. Currently, if the server runs out of open file handles, it effectively stays alive, but is unable to serve any data and becomes a 'zombie'. 

But a downside of the expiration based on the connection count is that it doesn't necessarily achieve the goal of expiring really old connections. Instead it tries to solve the problem of preventing the broker from running out of available file handles, in which case we probably need a fairer strategy for expiring connections. 

Thinking more, I think it might be sufficient to override removeEldestEntry and check if the oldest entry is older than the threshold and let the map remove it. If the oldest entry is not above the threshold, traversing the map doesn't buy you anything. The downside is that if no new activity takes place on any of the connections all of a sudden, the server wouldn't proactively drop all connections, which is less of a concern. 

The advantage is that you will still get the same benefit of expiring older connections and it removes the need to traverse.
;;;","15/Aug/14 08:08;nmarasoiu;Hi, I am sorry, but traversing will be limited to the connections that will actually be expired, so there is no traversing of non-expiring connections (please see the detailed example below). 

I do agree on the other hand that there will be a polling on the first entry until it expires, but this is how we can implement the requirement exactly as intended (expiration taking into account just time as per stated ""stale connections"" issue, not connection count or activity as well), and it can be done every 1000 selects. 

If we want to protect brokers from becoming zombies, this is a different concern I feel. However, I completely agree that we can do the LRU limiting as well to avoid zombeing (as part of this jira or another one). Both mechanisms to expire can be at work and solve both problems with no overhead in doing so (there would just be 2 contexts in which an evict+close would be performed, if we do not count the evict done in a normal close call).

[~junrao], [~jkreps], what do you think?

Say the server hold 100K connections. Say 100 connections are not used in the last 10 minutes.

What the program does (or I will make sure it does) is just iterate through the first 101 connections, the first 100 will be expired and it will stop at number 101.
I think this is an exact achievement of expected behavior of the jira task, as intended, and there is no performance penalty to that really!

I will rewrite with a loop /(tail-)recursive function, to check the first entry, and if stale call close (which also does a remove on the map anyways), and retry the next entry. This would be to avoid copying of the first 100 selectionKeys as well as to avoid any overhead/eagerness in map function.;;;","21/Aug/14 17:09;nmarasoi;After discussion with Neha, we agreed that using the removeEldestEntry approach works better in the sense that avoids disruption caused by potentially many connections being up for close at once, and evens out that overhead. The disadvantage remains that an inactive server will not close connections but seems less than the advantage of closing overhead leveling and of performance plus of not traversing and of not polling the oldest entry.;;;","25/Aug/14 04:24;junrao;Thanks for the patch. Looks good to me overall. Some minor comments below.

1. Could we make connectionsLruTimeout a broker side configuration?

2. Do we need to insert the key to lruConnections in write()? It seems to me doing that in read() (for incoming requests) is enough.

3. The patch doesn't seem to apply for me. Could you rebase?

git  apply -p0 ~/Downloads/KAFKA-1282_Disconnect_idle_socket_connection_in_Selector.patch 
/Users/jrao/Downloads/KAFKA-1282_Disconnect_idle_socket_connection_in_Selector.patch:13: trailing whitespace.
import java.util
/Users/jrao/Downloads/KAFKA-1282_Disconnect_idle_socket_connection_in_Selector.patch:21: trailing whitespace.
import java.util.Map.Entry
/Users/jrao/Downloads/KAFKA-1282_Disconnect_idle_socket_connection_in_Selector.patch:30: trailing whitespace.
  private val connectionsLruTimeout: Long = TimeUnit.MINUTES.toNanos(10)
/Users/jrao/Downloads/KAFKA-1282_Disconnect_idle_socket_connection_in_Selector.patch:31: trailing whitespace.
  private var currentTime: Long = SystemTime.nanoseconds
/Users/jrao/Downloads/KAFKA-1282_Disconnect_idle_socket_connection_in_Selector.patch:32: trailing whitespace.
  private val lruConnections = new util.LinkedHashMap[SelectionKey, Long](100, .75F, true) {
error: patch failed: core/src/main/scala/kafka/network/SocketServer.scala:16
error: core/src/main/scala/kafka/network/SocketServer.scala: patch does not apply
;;;","26/Aug/14 04:35;nmarasoi;Hi, Thank you, for 2. I agree for producers but I am not sure if the same SocketServer is used to serve consumers as well, and in this case, for consumers, the read/write ratio may be well in favor of writes making it risky perhaps to account just the reads?;;;","26/Aug/14 14:30;junrao;Nicu,

Similar to producers, consumers just issue fetch requests. The SocketServer first reads the fetch request from the network and then writes the fetch response to the network once the fetch request is served by the broker. So, there is a 1-to-1 mapping btw reads and writes and writes typically happen within a second after the reads. ;;;","27/Aug/14 19:20;nmarasoi;uploaded with parametrization and no more access-touch from write;;;","28/Aug/14 19:59;nmarasoi;[~nehanarkhede] Hi, I implemented our discussion and applied Jun Rao suggestions, can you check and perhaps commit it if looks good? Hope for more tasks like this, do you have any suggestions?:);;;","28/Aug/14 21:36;junrao;Thanks for the patch. The following should be * 1000000, right?

  private val connectionsLruTimeout: Long = connectionsMaxIdleMs * 1000;;;","29/Aug/14 18:26;nmarasoi;Patch updated. Configurable max idleness of a connection since the last read on it. On creating new N connections, the server will be Closing at most N idle connections too, if they are idle for more than the mentioned threshold, default 10 minutes.;;;","01/Sep/14 18:24;junrao;Looking at the patch again, in removeEldestEntry(), shouldn't we close the socket for eldest if the entry is to be removed? Right now, it seems that we only remove the entry from LRU w/o actually closing the idle socket connection.;;;","01/Sep/14 18:56;nmarasoi;I am sorry, Yes, that was the intent! I will write unit tests from now on to avoid such slips.

Moreover, the removeEldestEntry will return false all the time, because it keeps the responsability of mutating the map for itself, as part of calling the close method. 

Attached the patch, tests pass.;;;","02/Sep/14 16:30;junrao;Thanks for the latest patch. I was trying to do some local testing. The following are my observations.

1. I first started a local ZK and broker (setting connections.max.idle.ms 10secs). I then started a console-producer and a console-consumer. Then, I typed in sth in console-producer every 15 secs. However, I don't see the producer connection gets killed. I added sth instrumentation. It doesn't seem that removeEldestEntry() is called on every fetch request.

2. As I was debugging this, I realized that it's kind of weird to kill idle connections only when there is another non-idle connection. This makes debugging harder since one can't just test this out with a single connection. It's much simpler to understand if the idle connection can just be killed after the connection idle time, independent of other connections to the broker. To address the concern of closing many sockets in one iteration of the selector, we can calculate the time that a socket entry is expected to be killed (this is the access time of the oldest entry + maxIdleTime, or maxIdleTime if no entry exists). When that time comes during the iteration of the selector, we can just check the oldest entry and see if it needs to be closed.

3. It would be good to check if our clients (especially the producer, both old and new) can handle a closed idle connection properly. For example, when detecting an already closed socket, the producer should be able to resend the message and therefore we shouldn't see any data loss.;;;","05/Sep/14 05:25;nmarasoi;Hi, I am not completely sure I fully understood your solution in point 2: 

Do you mean to close at most one connection per iteration, right? This is ok, the worst case scenario is closing 100K old connections in 10 hours, one per select.

On storing the time to close in a local variable, the access of the oldest entry every iteration is O(1) super cheap so I would skip this optimization. ;;;","08/Sep/14 12:47;nmarasoi;[~junrao], hi, can you answer please? I agree with what you say if I understood all of it, I am doing a small patch right now;;;","08/Sep/14 12:49;nmarasoi;[~nehanarkhede] Hi, can you also check the new idea? It is consistent with my initial approach and solves the potential overhead of closing too many connections on a single iteration.;;;","14/Sep/14 16:11;nehanarkhede;Thanks for the patch, [~nmarasoi]! Looks good overall. Few review comments -

1. Do we really need connectionsLruTimeout in addition to connectionsMaxIdleMs? It seems to me that we are translating the idle connection timeout plugged in by the user to 1000000x times more than what is configured. That's probably why Jun saw the behavior he reported earlier. 
2. I don't really share Jun's concern in #2 and we can state that more clearly in the comment that describes the new config in KafkaConfig. Connections that are idle for more than connections.max.idle.ms *may* get killed. I don't think the users particularly care about a hard guarantee of their connections getting killed here. So the simplicity of this approach is well justified.
3. I do think that adding a produce and fetch test where the connections get killed will be great ;;;","16/Sep/14 04:45;junrao;Nicu,

On #2, I wasn't worried about any performance optimization. My concern is mostly on testing and ease of understanding. Since removeEldestEntry is only called on update, you can't test the logic on a single connection to the broker. It's a bit weird that if there is only a single idle connection, that connection is never killed. But as soon as a second connection is added, the idle connection will be killed. For the user's perspective, it's simpler to understand how idle connections are killed if they are not tied to # of connection.

Also, could you explain how you fixed #1 in the latest patch? It wasn't obvious to me.;;;","16/Sep/14 14:53;nmarasoiu;Hi, 

I have understood what you say and I agree it is highly unintuitive and we should change that. I just saw you propose a solution which included a precomputation of the time to close, and it was bit confusion, looked like an attempt of micro optimization.

I have not made any patch yet, I waited for feedback from Neha too, but I will do the patch today: it looks ok to me the idea of closing at most one old connection per selector iteration.

So the solution will look more like the previous patch, but instead of traversing n+1 entries to close n old connections, it will just pick the oldest and check if it is time to close.

For #1, the way Neha and me discussed, and the way you understood it works (for the latest patch), is that an old connection is taken into consideration for close only when a new connection is being opened up (or activity exists on an existing connection too). But this will no longer be the case.;;;","16/Sep/14 19:56;nmarasoi;Attached patch: every select iteration, zero or one connections are closed for being idle for too long.
The units pass well, but
For the moment I am blocked by:
./kafka-console-producer.sh
Error: Could not find or load main class kafka.tools.ConsoleProducer;;;","17/Sep/14 04:36;junrao;Did you do ""./gradlew jar"" first?;;;","17/Sep/14 09:47;nmarasoi;Hi,

Unfortunately the client used in console-producer is not very robust with respect to disconnections, as will detail below. Is this the ""old"" scala producer, and can we hope for a resilient behaviour that I can test with the new java producer?

More specifically, the connection is closed from the broker side, but the producer is unaware of this. The first message after the close is lost (and is not retried later). The second message sees the broken channel, outputs the exception below, and reconnects and is succesfully retried, I can see it consumed.

[2014-09-17 12:44:12,009] WARN Failed to send producer request with correlation id 15 to broker 0 with data for partitions [topi,0] (kafka.producer.async.DefaultEventHandler)
java.io.IOException: Broken pipe
	at sun.nio.ch.FileDispatcherImpl.writev0(Native Method)
	at sun.nio.ch.SocketDispatcher.writev(SocketDispatcher.java:51)
	at sun.nio.ch.IOUtil.write(IOUtil.java:149)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:483)
	at java.nio.channels.SocketChannel.write(SocketChannel.java:493)
	at kafka.network.BoundedByteBufferSend.writeTo(BoundedByteBufferSend.scala:56)
	at kafka.network.Send$class.writeCompletely(Transmission.scala:75)
	at kafka.network.BoundedByteBufferSend.writeCompletely(BoundedByteBufferSend.scala:26)
	at kafka.network.BlockingChannel.send(BlockingChannel.scala:100)
;;;","17/Sep/14 10:00;nmarasoi;re-attached fixed patch, but we may have a blocker to the whole solution on the broker side, pls see comment above/below (first message after disconnect is lost on the client used in console-prod);;;","17/Sep/14 12:08;nmarasoi;here is a time line:

he -> produced
he -> consumed
[ wait beyond timeout here, connection got closed underneath by the other side]
[2014-09-17 15:02:28,689] INFO Got user-level KeeperException when processing sessionid:0x148837ce1800001 type:setData cxid:0x24 zxid:0xec txntype:-1 reqpath:n/a Error Path:/consumers/console-consumer-87959/offsets/topi/0 Error:KeeperErrorCode = NoNode for /consumers/console-consumer-87959/offsets/topi/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[2014-09-17 15:02:28,691] INFO Got user-level KeeperException when processing sessionid:0x148837ce1800001 type:create cxid:0x25 zxid:0xed txntype:-1 reqpath:n/a Error Path:/consumers/console-consumer-87959/offsets Error:KeeperErrorCode = NoNode for /consumers/console-consumer-87959/offsets (org.apache.zookeeper.server.PrepRequestProcessor)
dddddddddddddd --> produce attempt (never retried, or never reached the broker or at least never reached the consumer)
[ many seconds wait, to see if the message is being retried, apparently not, even though the default retry is 3 times]
wwwwwwwwwwwwwwwww --> new attempt (immediattely I see the message below with the stack trace, and reconnect + retry is instantly sucesfull)
[2014-09-17 15:03:12,599] WARN Failed to send producer request with correlation id 9 to broker 0 with data for partitions [topi,0] (kafka.producer.async.DefaultEventHandler)
java.io.IOException: Broken pipe
	at sun.nio.ch.FileDispatcherImpl.writev0(Native Method)
	at sun.nio.ch.SocketDispatcher.writev(SocketDispatcher.java:51)
	at sun.nio.ch.IOUtil.write(IOUtil.java:149)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:483)
	at java.nio.channels.SocketChannel.write(SocketChannel.java:493)
	at kafka.network.BoundedByteBufferSend.writeTo(BoundedByteBufferSend.scala:56)
	at kafka.network.Send$class.writeCompletely(Transmission.scala:75)
	at kafka.network.BoundedByteBufferSend.writeCompletely(BoundedByteBufferSend.scala:26)
	at kafka.network.BlockingChannel.send(BlockingChannel.scala:100)
	at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:72)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:71)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SyncProducer.scala:102)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:102)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:102)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply$mcV$sp(SyncProducer.scala:101)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply(SyncProducer.scala:101)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply(SyncProducer.scala:101)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:100)
	at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$send(DefaultEventHandler.scala:255)
	at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$2.apply(DefaultEventHandler.scala:106)
	at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$2.apply(DefaultEventHandler.scala:100)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:100)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:72)
	at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:104)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:87)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:67)
	at scala.collection.immutable.Stream.foreach(Stream.scala:547)
	at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:66)
	at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)
[2014-09-17 15:03:12,712] INFO Closing socket connection to /127.0.0.1. (kafka.network.Processor)
wwwwwwwwwwwwwwwww;;;","17/Sep/14 12:18;nmarasoiu;in fact, this is something that needs fixing in the producer(s) anyway, but the issue is with the currently deployed producers.
One of the main reasons to go with a broker side close of the idle connections was that it is easier to redeploy brokers then producers.
But if this is indeed a bug in the producer(s) as I reproduced, those producers would need redeploy.
So moving this to the producer side as a configuration may again be an option on the table.;;;","17/Sep/14 15:36;junrao;Interesting. The data loss may have to do with ack=0, which is the default in console producer. Could you try ack=1?;;;","17/Sep/14 19:33;nmarasoi;Indeed, ack=1 solves it for most times but not for all:
- in 6 of 7 tests it gets a reset by peer and a socket timeout on fetch meta, than re connects and sends message.
- in one test, after leaving one night the laptop, I entered:
sdfgsdfgdsfg --> that never returned, no exception, nothing at all reported
aaaaaaaaaaa
aaaaaaaaaaa
ff
ff

The ""ok"" flow, which reproduces most of the time with ack=1 is (sometimes with just one of the 2 expcetions):
gffhgfhgfjfgjhfhjfgjhf
[2014-09-18 08:22:35,057] WARN Failed to send producer request with correlation id 43 to broker 0 with data for partitions [topi,0] (kafka.producer.async.DefaultEventHandler)
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
..
	at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)
[2014-09-18 08:22:36,663] WARN Fetching topic metadata with correlation id 44 for topics [Set(topi)] from broker [id:0,host:localhost,port:9092] failed (kafka.client.ClientUtils$)
java.net.SocketTimeoutException
	at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:226)
..
[2014-09-18 08:22:36,664] ERROR fetching topic metadata for topics [Set(topi)] from broker [ArrayBuffer(id:0,host:localhost,port:9092)] failed (kafka.utils.Utils$)
kafka.common.KafkaException: fetching topic metadata for topics [Set(topi)] from broker [ArrayBuffer(id:0,host:localhost,port:9092)] failed
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:71)
..
Caused by: java.net.SocketTimeoutException
	at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:226)
	.. kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29)
	at kafka.network.BlockingChannel.receive(BlockingChannel.scala:108)
	at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:74)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:71)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:112)
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:57)
	... 12 more
gffhgfhgfjfgjhfhjfgjhf;;;","18/Sep/14 14:02;nehanarkhede;Thanks for the updated patch. Overall, looks great. Few comments -
1. Can you rename initialNextIdleCloseCheckTimeValue to nextIdleCloseCheckTimeValue?
2. It will be easier to understand the code if we rename currentTime to currentTimeNanos.
;;;","18/Sep/14 19:13;nmarasoi;attached, renamed time and for the ""initial/reset value of the nextIdleCheck"", i just inlined the function, the code is more clear like this i think;;;","18/Sep/14 21:04;junrao;Do you think you can reproduce that data loss issue in 1 out of your 7 tests? With ack=1 and retries, this shouldn't happen. Perhaps it's useful to enable the trace logging in the producer to see what's exactly happening there.

Could you also do the same test by enabling the new producer in console producer?;;;","20/Sep/14 19:48;nehanarkhede;+1 on your latest patch. I'm leaning towards accepting the patch since the test above points to an issue that seems unrelated to the patch. [~nmarasoi], it will be great if you can follow Jun's suggestion to reproduce the issue. Then file a JIRA to track it. I'm guessing killing idle connections shouldn't lead to data loss.;;;","20/Sep/14 20:52;nehanarkhede;Pushed the latest patch to trunk.;;;","07/Nov/14 02:37;junrao;Nicu,

I was doing some manual testing of this feature. What I observed is that sometimes, the idle connections are not closed. The following was what I did.

1. Configure a small connections.max.idle.ms = 10000.
2. start ZK and Kafka broker
3. start a console consumer
bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic topic1 --from-beginning
4. start a console producer and type in sth every 15 secs or so. 
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic topic1 --request-required-acks 1

What I observed was that initially, the producer connections kept getting killed by the broker correctly after being idle for 10 secs. The next producer send would hit an IOException and trigger a resend. However, after typing in 10 or so messages, at some point, no idle connections were killed by the broker any more and the producer send always succeeded.;;;","08/Nov/14 14:54;nmarasoi;Indeed, I can reproduce this. I did saw an instance where no exception was thrown by the producer but still the broker mentioned new connection being listened to suggesting close took place. However, checking with required-acks 0 I can see that after some time the connection does not close anymore.;;;","08/Nov/14 16:41;nmarasoi;Fixed it - I have mistakenly deleted at some point the fact that the linked hash map needs to be in access order :( 
I tested with your scenario and looks ok now.;;;","10/Nov/14 02:15;nehanarkhede;good catch [~nmarasoi]. +1 on your change;;;","10/Nov/14 18:56;junrao;Nicu,

Thanks for the patch. Do you think it's easy to add a unit test on Processor?;;;","12/Nov/14 22:25;nmarasoi;I want, yes, I will add a few tests this week.;;;","18/Nov/14 21:36;jjkoshy;This is already in 0.8.2 so we should incorporate the follow-ups there as well I think.;;;","29/Dec/14 22:21;nehanarkhede;[~nmarasoi], [~junrao] This is marked for 0.8.2. Is anyone working or planning to work on this?;;;","29/Dec/14 22:59;nmarasoiu;I will do unit tests tommorow / day after. The fix should be ok otherwise,
and ready to be pushed on trunk and 0.8.2. I will announce when done with
units.

On Tue, Dec 30, 2014 at 12:21 AM, Neha Narkhede (JIRA) <jira@apache.org>

;;;","01/Jun/15 13:57;nmarasoi;Hi [~junrao], [~nehanarkhede], I added a test, please review. The patch has 2 variations (latest 2 patches), explained at point 2 below, while the latest implements 1' below.

1. I wanted to sleep on MockTime, but here we actually need to physically wait at leat one epoll/select cycle. Since I have put 10ms idle time & it works, mocked time would not bring benefits, i.e. only the select time needs to be waited over. 

1'. Because of potentially large & not deterministically bounded select times, I implemented a mechanism to try a few times, waiting 50% more time every time.

2. Seems to work with low (10ms) idle timeout for all current test methods. However, I attach a patch with separate test class for this (and yet another utils class for reuse), to isolate configuration between group of test methods.

3. Shall I do a multiple connections test?;;;","01/Jun/15 23:59;junrao;[~nmarasoi], thanks for the patch. We are changing SocketServer to reuse Selector right now in KAFKA-1928. Once that's done, the idle connection logic will be moved into Selector and should be easier to test since Selector supports mock time. That patch is almost ready. Perhaps you can wait until it's committed and submit a new patch. ;;;","20/Aug/15 15:00;nmarasoi;Hi,

I noticed that the dependencies are done and I will resume this task.
The task contributions had been:
- a fix
- unit test(s)

As far as the fix is concerned, I noticed that it is already fixed in the current Selector, namely the lruConnections is a LinkedHashMap with accessOrder=true. This was the only fix needed, and I am 100% convinced that the fix is already done.

I already have a unit test too, I will try to put a patch here this week.

Just wanted to mention that the old connections should be closed by the kafka installations using the new reusable network code.

Thanks
Nicu;;;","16/Oct/15 01:54;junrao;[~nmarasoi], yes, the actual problem is now fixed in trunk. We just need to add a unit test. I created a followup jira KAFKA-2661 for that. Resolving this jira.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
exclude kafka-clients jar from dependant-libs dir,KAFKA-1280,12697240,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,25/Feb/14 22:48,26/Feb/14 02:10,14/Jul/23 05:39,26/Feb/14 02:10,0.8.2.0,,,0.8.2.0,,,,,,,,,,0,,,,We already include the built kafka-clients jar in the kafka-run-class path. There is no need to copy it over to dependant-libs. Duplicates add confusion.,,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/14 22:50;junrao;KAFKA-1280.patch;https://issues.apache.org/jira/secure/attachment/12631083/KAFKA-1280.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,375714,,,Wed Feb 26 02:10:45 UTC 2014,,,,,,,,,,"0|i1sqs7:",376010,,,,,,,,,,,,,,,,,,,,"25/Feb/14 22:50;junrao;Created reviewboard https://reviews.apache.org/r/18486/
 against branch origin/trunk;;;","26/Feb/14 02:10;junrao;Thanks for the review. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SocketServer.shutdown() doesn't close open connections,KAFKA-1279,12696986,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,24/Feb/14 19:08,25/Feb/14 03:31,14/Jul/23 05:39,25/Feb/14 03:31,,,,,,,,,,,,,,0,,,,"SocketServer.shutdown() stops the selector thread but doesn't actually close all the existing connections.

In normal operations this doesn't matter much because right after shutting down the socket server the process exits which closes all the connections. However we found this issue during unit testing--essentially we are leaking all the connections so shutdown() doesn't actually cause any error at all on the client which is still able to write to the socket and just never receives a response.",,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/14 22:47;jkreps;KAFKA-1279.patch;https://issues.apache.org/jira/secure/attachment/12630820/KAFKA-1279.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,375461,,,Mon Feb 24 22:47:52 UTC 2014,,,,,,,,,,"0|i1sp87:",375757,,,,,,,,,,,,,,,,,,,,"24/Feb/14 22:47;jkreps;Created reviewboard https://reviews.apache.org/r/18441/
 against branch trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a list of config overrides available when running kafka.topics,KAFKA-1276,12696309,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,toddpalino,toddpalino,20/Feb/14 17:56,25/Feb/14 21:01,14/Jul/23 05:39,25/Feb/14 21:01,,,,0.8.2.0,,,,,,,tools,,,0,,,,It would be helpful to have the help for kafka-topics enumerate a list of the per-topic configuration overrides that are available with the --config option.,,jkreps,junrao,toddpalino,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/14 05:36;jkreps;KAFKA-1276.patch;https://issues.apache.org/jira/secure/attachment/12630254/KAFKA-1276.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,374785,,,Tue Feb 25 21:01:50 UTC 2014,,,,,,,,,,"0|i1sl2v:",375085,,,,,,,,,,,,,,,,,,,,"20/Feb/14 18:42;jkreps;Good idea! This is actually pretty easy to do. The idea would be that the --help option (or running with no args) would print something like

jkreps-mn:kafka jkreps$ bin/kafka-topics.sh 
Command must include exactly one action: --list, --describe, --create, --delete, or --alter
Option                                  Description                            
------                                  -----------                            
--alter                                 Alter the configuration for the topic. 
--config <name=value>     A topic configuration override for the 
                                          topic being created or altered. The following are valid topic configurations:
                                              segment.bytes
                                              segment.ms
                                              segment.index.bytes
                                          For details on these configs see http://kafka.apache.org/documentation.html#configuration
--create                                Create a new topic.                    
...

This can be done automatically off the code. I can't automatically include documentation on the configs (yet) hence the annoying link to the main docs. We could potentially improve this with the changes to the config code in 0.9 and I'd prefer to avoid trying to keep the docs in two places up to date in the interim.

Does this seem reasonable?;;;","21/Feb/14 05:36;jkreps;Created reviewboard https://reviews.apache.org/r/18344/
 against branch trunk;;;","25/Feb/14 21:01;junrao;This is already committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fixes for quickstart documentation,KAFKA-1275,12696029,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jkreps,zackse,zackse,19/Feb/14 18:05,30/Oct/14 18:30,14/Jul/23 05:39,05/Mar/14 21:22,0.8.1,,,0.8.1,,,,,,,website,,,0,documentation,,,"The quickstart guide refers to commands that no longer exist in the master git branch per changes in KAFKA-554.

If changes for the documentation to match 0.8.1 are already in development elsewhere, please feel free to discard this issue.",,guozhang,jjkoshy,jkreps,joestein,junrao,sachingoyal,zackse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/14 18:10;zackse;KAFKA-1275-quickstart-doc.patch;https://issues.apache.org/jira/secure/attachment/12629827/KAFKA-1275-quickstart-doc.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,374506,,,Thu Oct 30 18:30:29 UTC 2014,,,,,,,,,,"0|i1sjcv:",374806,,,,,,,,,,,,,,,,,,,,"19/Feb/14 18:10;zackse;Patch against 08/quickstart.html;;;","19/Feb/14 18:12;zackse;Includes some fixes to minor typos and inconsistent command formatting.;;;","19/Feb/14 22:01;guozhang; +1. Thanks for the patch.;;;","20/Feb/14 04:36;junrao;Evan,

Thanks for the patch. We will have to wait until 0.8.1 is released.;;;","20/Feb/14 05:08;jkreps;I guess the question is whether we want to fork the documentation on a more minor release like this. Our plan had been to basically leave the existing release docs as-is so they continue to work with that release and add changes in the new doc. So for 0.8.1 I had planned to copy the existing doc and add some details about log compaction and the new config. If no objection I'll apply this patch on that.;;;","20/Feb/14 05:28;joestein;+1;;;","05/Mar/14 21:22;jkreps;Hey Evan,

I took you patch and added a bunch of additional docs. They are live now under the /081 directory and I will make them the main docs when the release occurs. Take a look if you have a chance and let me know if I goofed anything:

 http://kafka.apache.org/081/documentation.html;;;","05/Mar/14 21:56;zackse;Thanks, Jay.

There were a few inconsistencies in the command samples around whether bold-face text was used for the prompt indicator at the start of each line, those were addressed in my patch but not applied, but not a big deal.

Another minor change that wasn't applied was to make the reference to ""messages"" (plural) consistent in the sample commands of Section 1.3 / Step 6: Setting up a multi-broker cluster: ""Let's publish a few messages to our new topic"", followed by output, then ""Now consume this message"" (should be ""Now consume these messages"").

In the same section, the output for the ""Let's publish a few messages..."" is in bold-face whereas all other command output on the page is not.

There is a typo in the new documentation towards the end of the Section 1.3 /  Step 6: Setting up a multi-broker cluster section: {{kafka-topics.sh}} is misspelled {{kafktopics.sh}} in the second-to-last command.

Hope this helps.;;;","05/Mar/14 22:43;jkreps;Ah, thanks for the careful attention to detail. Yeah I goofed the patch application as I had already created the new copy of the docs for 0.8.1 and tried to apply the changes by hand which I obviously didn't get right.

1. I fixed the prompt so it is now never bolded.
2. Fixed the message/messages typo.
3. Ah, this is actually deliberate. The intended convention was that things you type are in bold, things printed out are non-bold. So for the producer you are typing the messages and for the consumer it is printing them. This may be more confusing then helpful, though. In any case your pointing this out made my realize I had been inconsistent even in this and missed some of the bolding for the other producer example. Hopefully it is now fully consistent.
4. Ack, fixed typo.;;;","06/Mar/14 01:38;junrao;Some more comments on the doc:

20. Quickstart still references sbt.
21. unruly consumer will be able to publish messages too large for consumers to consume ==> unruly producer
22. All text after the end of ""Step 4: Send some messages"" are bold.
23. In the config table, the row for max.message.bytes is not aligned properly.
24. so multiple by the number  => so multiply
25. For topic level config, could you give an example of removing an existing config?
26. In the log compaction section.
26.1No sure how to pass this sentence. ""In the case that one is only handling the real-time updates""
26.2 delete markers are special in they will => in that they will

;;;","06/Mar/14 03:51;jkreps;20. Yes, I plan to fix this when we have the release done. The quickstart should reference the binary distribution not the source.
21. Fixed
22. Yes, the idea is that text you type is in bold. Text that is output from some command is not. Hopefully that makes sense.
23. Ack, fixed.
24. Fixed.
26. Fixed.

Overall did you think the log compaction section made sense? I struggled a bit to explain that feature.;;;","06/Mar/14 05:19;junrao;The log compaction section makes sense to me. Maybe other people can provide some feedback too.;;;","06/Mar/14 18:46;jjkoshy;I had skimmed through the log compaction write-up yesterday - looked good to
me, although I have read the code before. So it would help to have a user's
perspective.

In the first diagram, I would consider shading the boxes under log tail
differently from the log head (just to make it clearer).

Also, there is a typo under guarantees: 4) Any read from offset 0 will
contain see ...

Typo apart, I think I understand what you are trying to convey in that
guarantee but I don't know what you mean by ""provided it completes in less
than a configurable time period"".
;;;","06/Mar/14 19:24;jjkoshy;Also, may want to make a note of the difference between partitionKey and key. Just realized this after a thread on the user mailing list - I'm also hoping that user can review the compaction documentation.;;;","06/Mar/14 19:36;joestein;I am a bit confused by 

""3.The offset for a message never changes, it is the permanent identifier for a position in the log.
4. Any read from offset 0 will contain see the final state of all records provided it completes in less than a configurable time period (even if further compaction occurs during this read).

The third guarantee seems somewhat esoteric but this ensures that a reader will end in a coherent state (equivalent to reading the full uncompacted log) even though the underlying log may be compacted while this read occurs""

shouldn't it be the fourth guarantee?  I am not sure I understand number 4 (haven't looked at the code) does it mean you are reading the log and not the copy of the log since you started prior to compaction ending? 

Also does this mean now that the compaction could mean you might need ~ twice ~ as much disk space (assuming a scenario where almost the entire log is copied (nothing not held back from writing)... 

also the word ""Offset"" is missing from the diagram Log After Compaction.

lastly it is not entirely clear exactly what will happen when you turn log.cleanup.policy=compact on for a topic, is there a specific class that gets executed ? what gets deleted? how do you control it?  or do you have to build a compaction strategy or use an existing one?;;;","06/Mar/14 20:33;jkreps;Hey Joe/Joel,

I clarified the guarantees and fixed the typos. Thanks.

Log compaction will take more space than just the raw compacted data in the time between compactions. This is bounded by min.cleanable.dirty.ratio (i.e. a dirty ratio of 50% implies 2x space usage).

The part about the compaction strategy is interesting. The compaction is always by key--older entries for the same primary key are discarded. This implies a usage strategy where the new value contains the full information. You can think of this like physical logging in databases where the full after-image of the row might be logged after an update. You are describing the possibility of making this pluggable so that fancier things could be done (aggregation or what have you). I am skeptical of the operational feasability of this in an ""as a service"" deployment, though: basically I don't want a bunch of custom user code running on a centrally managed cluster and I don't want to be deploying new aggregation jars each time this logic changes. So I am a little scared of that. :-);;;","06/Mar/14 20:46;joestein;ah ha! got it now :) VERY COOL.

So the KeyedMessage val key:K is the primaryKey key (which is what is used to compact all older matching primary key messages leaving only the latest value (which for the use case is the only thing that would matter)) and the val partKey: Any is the partition key thrown away for deciding what partition to go on (same as < 0.8.1)

Thanks!!!!;;;","06/Mar/14 20:50;jkreps;[~charmalloc] Yeah exactly. Now the question is how can I make that more clear from the docs?;;;","06/Mar/14 21:04;joestein;Maybe an example after the paragraph

""Log compaction is a mechanism to give finer-grained per-record retention, rather than the coarser-grained time-based retention. The idea is to selectively remove records where we have a more recent update with the same primary key. This way the log is guaranteed to have at least the last state for each key.""

Something like... 

Lets say you have a topic that every time inventory information for a product changes it is sent to a topic

Message1 with Primary Key == PRODUCTID99876 is produced

the inventory for PRODUCTID99876 changes

Message 2 with Primary Key == PRODUCTID99876 is produced

the inventory changes again for PRODUCTID99876

Message 3 with Primary Key == PRODUCTID99876

Now, since this data stream is only having the inventory information as a snapshot every old one is likely to not have relevance (since for this example we are only interested in the latest).  Compaction would get rid of message 1 and 2 only leaving 3 as it is the current state... or pick something from the uses cases you mentioned above in the docs (my example is not very realistic just trying to express the thought though)

I think it looks good (but now that I understand it I may no longer be very objective).

Another option (instead of example or in addition too, dunno) is after the log compaction diagram (the before and after compaction) maybe explain that 0,2 are not copied because the primary K1 is latest state is 3 (so that is kept), 1,5 are not copied because the primary key K2 has the latest state as offset 9, etc... ;;;","06/Mar/14 23:37;jkreps;[~charmalloc] Cool, that makes sense. I added an example. Let me know if you think that helped.;;;","07/Mar/14 01:15;guozhang;One more minor thing: in the quick start, we named the release as

tar xzf kafka-<VERSION>.tgz

but ./gradlew releaseTarGz actually generates

kafka_<VERSION>.tgz;;;","07/Mar/14 01:23;jkreps;Yeah, I was waiting to see the final binary release package and then update that rather than having them compile.;;;","08/Mar/14 02:07;joestein;[~jkreps] looks fantastic! ;;;","30/Oct/14 18:30;sachingoyal;[~joestein], [~jkreps], We are using Kafka 0.8.1.1 and we are thoroughly confused by the following statement:
{quote}
So the KeyedMessage val key:K is the primaryKey key (which is what is used to compact all older matching primary key messages leaving only the latest value (which for the use case is the only thing that would matter)) and the val partKey: Any is the partition key thrown away for deciding what partition to go on (same as < 0.8.1)
{quote}

The code we are using to send message to Kafka is:
{code}
String payload = ....;
String topic = ...;
String key = """" + System.currentTimeMillis();
KeyedMessage<String, String> data = new KeyedMessage<String, String>(topic, key, payload);
{code}

*Our concern is*:
Is it possible that when multiple threads are writing, some of them (with different payload) write with same partition key and because of that Kafka overwrites these messages (due to same partitionKey)?


Note that I am not talking about log compaction here.
I am talking from consumers' perspective.
*Is there a possibility that consumers can see missing payloads because their +key+ was same?*

If you can point me to the source code or the documentation, it would be much appreciated.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
controller logs exceptions during ZK session expiration,KAFKA-1271,12695902,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,19/Feb/14 02:31,03/Apr/14 16:27,14/Jul/23 05:39,20/Feb/14 17:52,0.8.1,,,0.8.1,,,,,,,,,,0,,,,"Saw the following issues when there is ZK session expiration in the controller.

1. 
 ERROR Error handling event ZkEvent[Children of
/admin/delete_topics changed sent to
kafka.controller.PartitionStateMachine$DeleteTopicsListener@39abdac9]
(org.I0Itec.zkclient.ZkEventThread)
java.lang.NullPointerException
at
scala.collection.JavaConversions$JListWrapper.iterator(JavaConversions.scala:524)
at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
at
scala.collection.JavaConversions$JListWrapper.foreach(JavaConversions.scala:521)
at
scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:176)
at
scala.collection.JavaConversions$JListWrapper.foldLeft(JavaConversions.scala:521)
at
scala.collection.TraversableOnce$class.$div$colon(TraversableOnce.scala:139)
at
scala.collection.JavaConversions$JListWrapper.$div$colon(JavaConversions.scala:521)
at scala.collection.generic.Addable$class.$plus$plus(Addable.scala:54)
at scala.collection.immutable.Set$EmptySet$.$plus$plus(Set.scala:47)
at scala.collection.TraversableOnce$class.toSet(TraversableOnce.scala:436)
at
scala.collection.JavaConversions$JListWrapper.toSet(JavaConversions.scala:521)
at
kafka.controller.PartitionStateMachine$DeleteTopicsListener$$anonfun$handleChildChange$2.apply$mcV$sp(PartitionStateMachine.scala:448)
at
kafka.controller.PartitionStateMachine$DeleteTopicsListener$$anonfun$handleChildChange$2.apply(PartitionStateMachine.scala:445)
at
kafka.controller.PartitionStateMachine$DeleteTopicsListener$$anonfun$handleChildChange$2.apply(PartitionStateMachine.scala:445)
at kafka.utils.Utils$.inLock(Utils.scala:538)
at
kafka.controller.PartitionStateMachine$DeleteTopicsListener.handleChildChange(PartitionStateMachine.scala:445)
at org.I0Itec.zkclient.ZkClient$7.run(ZkClient.java:570)
at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71) 

2. IllegalStateException due to ""Kafka scheduler has not been started"".",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1272,KAFKA-1270,,,,,,,,,,,,,,,,,,"19/Feb/14 15:57;junrao;KAFKA-1271.patch;https://issues.apache.org/jira/secure/attachment/12629794/KAFKA-1271.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,374383,,,Thu Feb 20 17:52:34 UTC 2014,,,,,,,,,,"0|i1silj:",374683,,,,,,,,,,,,,,,,,,,,"19/Feb/14 15:57;junrao;Created reviewboard https://reviews.apache.org/r/18273/
 against branch origin/trunk;;;","20/Feb/14 17:52;junrao;Thanks for the review. Double committed to trunk and 0.8.1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor typos in documentation,KAFKA-1269,12695801,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,zackse,zackse,zackse,18/Feb/14 22:10,06/Mar/14 16:58,14/Jul/23 05:39,24/Feb/14 18:12,0.8.0,,,,,,,,,,website,,,0,documentation,,,"Several typos (misspellings, missing punctuation) appear in the introduction and configuration pages. For example:

chose -> choose
multiple -> multiply
then -> than
",,junrao,zackse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1295,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/14 17:10;zackse;KAFKA-1269-documentation-typos-2.patch;https://issues.apache.org/jira/secure/attachment/12630715/KAFKA-1269-documentation-typos-2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,374308,,,Mon Feb 24 18:12:19 UTC 2014,,,,,,,,,,"0|i1si4v:",374608,,,,,,,,,,,,,,,,,,,,"18/Feb/14 22:12;zackse;Attached patch to 08/configuration.html and 08/introduction.html;;;","18/Feb/14 22:17;zackse;Attached patch without git prefix (git diff --no-prefix), please let me know if this needs to be generated with svn tools instead.;;;","24/Feb/14 17:10;zackse;Updating with additional fixes.;;;","24/Feb/14 18:12;junrao;Thanks for the patch. +1 and committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snazzy up the README markdown for better visibility on github,KAFKA-1263,12695063,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,joestein,joestein,13/Feb/14 19:39,14/Feb/14 03:46,14/Jul/23 05:39,14/Feb/14 03:46,,,,0.8.1,,,,,,,,,,0,,,,Here are my suggested changes so you can see them on github how they look https://github.com/stealthly/samplereadmekafka/blob/master/README.md,,charmalloc,joestein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/14 19:46;charmalloc;KAFKA-1263.patch;https://issues.apache.org/jira/secure/attachment/12628826/KAFKA-1263.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,373571,,,Fri Feb 14 03:46:46 UTC 2014,,,,,,,,,,"0|i1sdlj:",373871,,,,,,,,,,,,,,,,,,,,"13/Feb/14 19:46;charmalloc;Created reviewboard https://reviews.apache.org/r/18092/
 against branch origin/0.8.1;;;","14/Feb/14 03:46;joestein;double committed to 0.8.1 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delete temporary data directory after unit test finishes,KAFKA-1258,12694579,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,omkreddy,guozhang,guozhang,11/Feb/14 19:59,17/May/16 14:14,14/Jul/23 05:39,12/Jul/14 19:50,,,,0.8.2.0,,,,,,,,,,0,newbie,,,"Today in unit testsuite most of the time when a test case is setup a temporary directory will be created with a random int as suffix, and will not be deleted after the test. After a few unit tests this will create tons of directories in java.io.tmpdir (/tmp for Linux). Would be better to remove them for clean unit tests.",,guozhang,jkreps,omkreddy,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/14 10:38;omkreddy;KAFKA-1258.patch;https://issues.apache.org/jira/secure/attachment/12655377/KAFKA-1258.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,373087,,,Sat Jul 12 19:50:39 UTC 2014,,,,,,,,,,"0|i1samv:",373388,,,,,,,,,,,,,,,,,,,,"11/Feb/14 21:00;jkreps;Yeah this annoys me too. The core problem is that java provides File.deleteOnExit but this doesn't work for directories. Instead what we should do is change TestUtils.createTempDir to add a shutdown hook that deletes the directory recursively.;;;","12/Feb/14 06:43;sriramsub;Look into http://junit.org/javadoc/4.9/org/junit/rules/TemporaryFolder.html. Helps to manage temp folders in junit. It may be supported only in java 7. ;;;","12/Jul/14 10:38;omkreddy;Created reviewboard https://reviews.apache.org/r/23439/diff/
 against branch origin/trunk;;;","12/Jul/14 10:46;omkreddy;Junit 4.7 can be used to manage temporary folders. But this  requires
changes in all the test cases and some base test classes.

Submitted patch by adding shutdown hook to TestUtils.tempDir method.;;;","12/Jul/14 19:50;jkreps;Committed. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gradle issues for release,KAFKA-1243,12694113,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,joestein,joestein,08/Feb/14 17:59,20/Feb/14 05:15,14/Jul/23 05:39,20/Feb/14 05:15,0.8.1,,,0.8.1,,,,,,,,,,0,,,,This is the parent issue for all of the sub tasks found for the release that are required to update on gradle.  I think some of the changes are going to be minor and in some cases not blockers but there are a bunch of them so we can identify each minuscule item but have it a bit organized.,,clarkbreyman,joestein,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/14 23:25;junrao;KAFKA-1243.patch;https://issues.apache.org/jira/secure/attachment/12628105/KAFKA-1243.patch","11/Feb/14 21:37;junrao;KAFKA-1243_2014-02-11_13:37:25.patch;https://issues.apache.org/jira/secure/attachment/12628340/KAFKA-1243_2014-02-11_13%3A37%3A25.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,372622,,,Thu Feb 20 05:15:19 UTC 2014,,,,,,,,,,"0|i1s7sv:",372926,,,,,,,,,,,,,,,,,,,,"10/Feb/14 23:25;junrao;Created reviewboard https://reviews.apache.org/r/17923/
 against branch origin/trunk;;;","10/Feb/14 23:40;junrao;I made a pass of this jira and fixed the following.
1. added LICENSE and NOTICE file to all built jars (KAFKA-1244)
2. removed ""-dist"" from release tgz file (KAFKA-1249)
3. removed the step ""./gradlew copyDependantLibs"" by making it a dependency of ""./gradlew core:jar""

KAFKA-1246: There is no scala 2.10 release. The released ones are 2.10.1, 2.10.2, 2.10.3, etc. So, there shouldn't be a kafka jar for scala 2.10. What we are doing now in gradle seems correct.

KAFKA-1248: I don't think those jars (include the test jar) need to be in maven. We can build a test jar for local testing though.

Joe,

I am not so sure about KAFKA-121. The pom file generated by gradle does have license, url and group id. What else do you think are needed?

KAFKA-1254: Once we addressed all sub-jiras, we can remove sbt.;;;","11/Feb/14 21:37;junrao;Updated reviewboard https://reviews.apache.org/r/17923/
 against branch origin/trunk;;;","11/Feb/14 21:49;junrao;KAFKA-1246: It seems that the sbt way is correct. The reason is that all scala 2.10.* versions are both forward and backward compatible. So, we just need to publish one version of artifact with suffix 2.10 (i.e., w/o minor revision #).

Attach another patch to address this issue.;;;","12/Feb/14 16:33;junrao;Double committed my patch to 0.8.1 and trunk. Will close kafka-1244, kafka-1246 and kafka-1249.

Joe,

Could you rebase your patch kafka-1245 and comment on what we should do for kafka-121?;;;","12/Feb/14 17:02;joestein;Thanks Jun! Sorry I have been swamped the last few days.  KAFKA-121 was there just so folks could review the POM as it is now and I agree with you it looks good so I resolved that ticket.  I will rebase 1245 and give another release dry run through hopefully later today/tonight/tomorrow.  Thanks!;;;","14/Feb/14 00:03;clarkbreyman;Perhaps this is worthy of a separate ticket, but the generated jar files do not include a META-INF/maven/ directory tree or pom. This is different from what would be expected if the artifact were generated from maven. ;;;","14/Feb/14 19:06;junrao;Clark,

Is this needed to 0.8.1? If not, could you open a separate jira to track it?

Thanks,;;;","14/Feb/14 20:27;clarkbreyman;Jun - Nice to have but if it doesn't make the train that's ok. The SBT/IVY jars had the same problem. I'll file a separate ticket. ;;;","14/Feb/14 20:55;clarkbreyman;KAFKA-1265 has the addMavenDescriptor compatibility issues. ;;;","20/Feb/14 05:15;joestein;everything related to this has been taken care of and is committed to 0.8.1 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cryptic serde error messages in new producer,KAFKA-1241,12693189,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,jkreps,nehanarkhede,nehanarkhede,04/Feb/14 19:11,20/Mar/14 21:56,14/Jul/23 05:39,20/Mar/14 21:56,0.10.1.0,,,,,,,,,,producer ,,,0,,,,"One of the motivations for the new serde format is better error reporting. I was running a test on the new mirror maker when I saw this exception -

java.nio.BufferUnderflowException
	at java.nio.Buffer.nextGetIndex(Buffer.java:480)
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:336)
	at kafka.common.protocol.types.Type$3.read(Type.java:88)
	at kafka.common.protocol.types.Schema.read(Schema.java:50)
	at kafka.common.protocol.types.ArrayOf.read(ArrayOf.java:30)
	at kafka.common.protocol.types.Schema.read(Schema.java:50)
	at kafka.common.protocol.types.ArrayOf.read(ArrayOf.java:30)
	at kafka.common.protocol.types.Schema.read(Schema.java:50)
	at kafka.clients.producer.internals.Sender.handleResponses(Sender.java:273)
	at kafka.clients.producer.internals.Sender.run(Sender.java:144)
	at kafka.clients.producer.internals.Sender.run(Sender.java:84)
	at java.lang.Thread.run(Thread.java:619)


I was expecting to see a precise error message about the Request type that serde failed for. Instead it says Type$3 and no information on which field.",,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/14 04:23;jkreps;KAFKA-1241.patch;https://issues.apache.org/jira/secure/attachment/12627551/KAFKA-1241.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,371775,,,Thu Mar 20 21:56:30 UTC 2014,,,,,,,,,,"0|i1s2lb:",372074,,,,,,,,,,,,,,,,,,,,"07/Feb/14 04:23;jkreps;Created reviewboard https://reviews.apache.org/r/17836/
 against branch trunk;;;","07/Feb/14 04:29;jkreps;I improved the error for that case. I think the rationale was that we CAN have good error messages because all parsing logic is in a single place not that we DO have good errors. Once we fix all the issues like this then we will have good errors :-)

We can't give the request name as the serialization doesn't know about requests but we can give the field being read and how much it was off by which I think is an improvement.

All that said this sounds like there is a bug, no? I mean the fact that we get an error is itself an issue irrespective of how good the error message is, right?;;;","07/Feb/14 05:29;nehanarkhede;Yes, it is a bug I hit during a test with large set of topics on the mirror maker. However, due to absence of logging and the incomplete error message, it is difficult to say what happened. Here is the rest of the stack trace -

java.lang.IllegalStateException: Attempt to begin a send operation with prior send operation still in progress.
	at kafka.common.network.Selector.poll(Selector.java:171)
	at kafka.clients.producer.internals.Sender.run(Sender.java:137)
	at kafka.clients.producer.internals.Sender.run(Sender.java:84)
	at java.lang.Thread.run(Thread.java:619)
java.lang.IllegalStateException: Correlation id for response (7401215) does not match request (7401214)
	at kafka.clients.producer.internals.Sender.correlate(Sender.java:313)
	at kafka.clients.producer.internals.Sender.handleResponses(Sender.java:274)
	at kafka.clients.producer.internals.Sender.run(Sender.java:144)
	at kafka.clients.producer.internals.Sender.run(Sender.java:84)
	at java.lang.Thread.run(Thread.java:619)
java.nio.BufferUnderflowException
	at java.nio.Buffer.nextGetIndex(Buffer.java:480)
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:336)
	at kafka.common.protocol.types.Type$3.read(Type.java:88)
	at kafka.common.protocol.types.Schema.read(Schema.java:50)
	at kafka.common.protocol.types.ArrayOf.read(ArrayOf.java:30)
	at kafka.common.protocol.types.Schema.read(Schema.java:50)
	at kafka.common.protocol.types.ArrayOf.read(ArrayOf.java:30)
	at kafka.common.protocol.types.Schema.read(Schema.java:50)
	at kafka.clients.producer.internals.Sender.handleResponses(Sender.java:273)
	at kafka.clients.producer.internals.Sender.run(Sender.java:144)
	at kafka.clients.producer.internals.Sender.run(Sender.java:84)
	at java.lang.Thread.run(Thread.java:619)


;;;","09/Feb/14 23:57;jkreps;Hey [~nehanarkhede] I think that is a real bug that someone pointed out in the code review, namely that when the metadata refresh happens if it goes to a broker that also gets a produce request in the same iteration it is possible to have two unsent requests at the same time (which we don't allow). The fix is fairly straight-forward but let me break it into a seperate issue from the error message bug.

In any case I think the patch I posted fixes the logging in the serialization. I'll fix up the sender logging when we do the slf4j. So can I get a +1 on this?;;;","10/Feb/14 05:48;nehanarkhede;Got it. Do you want to fix the issue of better error messages for request types in the Sender in another JIRA then? If so, I'm a +1 on this patch. ;;;","20/Mar/14 21:56;jkreps;I believe this is fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New producer hangs in a loop detecting metadata for auto created topics,KAFKA-1238,12692756,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,jkreps,nehanarkhede,nehanarkhede,03/Feb/14 00:10,22/Jul/14 14:38,14/Jul/23 05:39,11/Feb/14 05:01,,,,0.8.2.0,,,,,,,producer ,,,0,,,,"New producer hangs in a loop detecting metadata for auto created topics -

java.lang.IllegalStateException: No known nodes.
	at kafka.common.Cluster.nextNode(Cluster.java:97)
	at kafka.clients.producer.internals.Sender.maybeMetadataRequest(Sender.java:154)
	at kafka.clients.producer.internals.Sender.run(Sender.java:120)
	at kafka.clients.producer.internals.Sender.run(Sender.java:84)
	at java.lang.Thread.run(Thread.java:695)
java.lang.IllegalStateException: No known nodes.
	at kafka.common.Cluster.nextNode(Cluster.java:97)
	at kafka.clients.producer.internals.Sender.maybeMetadataRequest(Sender.java:154)
	at kafka.clients.producer.internals.Sender.run(Sender.java:120)
	at kafka.clients.producer.internals.Sender.run(Sender.java:84)
	at java.lang.Thread.run(Thread.java:695)
java.lang.IllegalStateException: No known nodes.
	at kafka.common.Cluster.nextNode(Cluster.java:97)
	at kafka.clients.producer.internals.Sender.maybeMetadataRequest(Sender.java:154)
	at kafka.clients.producer.internals.Sender.run(Sender.java:120)
	at kafka.clients.producer.internals.Sender.run(Sender.java:84)
	at java.lang.Thread.run(Thread.java:695)
java.lang.IllegalStateException: No known nodes.
	at kafka.common.Cluster.nextNode(Cluster.java:97)
	at kafka.clients.producer.internals.Sender.maybeMetadataRequest(Sender.java:154)
	at kafka.clients.producer.internals.Sender.run(Sender.java:120)
	at kafka.clients.producer.internals.Sender.run(Sender.java:84)
	at java.lang.Thread.run(Thread.java:695)
java.lang.IllegalStateException: No known nodes.
	at kafka.common.Cluster.nextNode(Cluster.java:97)
	at kafka.clients.producer.internals.Sender.maybeMetadataRequest(Sender.java:154)
	at kafka.clients.producer.internals.Sender.run(Sender.java:120)
	at kafka.clients.producer.internals.Sender.run(Sender.java:84)
	at java.lang.Thread.run(Thread.java:695)
java.lang.IllegalStateException: No known nodes.
	at kafka.common.Cluster.nextNode(Cluster.java:97)
	at kafka.clients.producer.internals.Sender.maybeMetadataRequest(Sender.java:154)
	at kafka.clients.producer.internals.Sender.run(Sender.java:120)
	at kafka.clients.producer.internals.Sender.run(Sender.java:84)
	at java.lang.Thread.run(Thread.java:695)

The producer needs to be restarted to start sending data to the auto created topic",,guozhang,jkreps,junrao,nehanarkhede,tcollinsworth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/14 17:24;jkreps;KAFKA-1238-v1.patch;https://issues.apache.org/jira/secure/attachment/12627654/KAFKA-1238-v1.patch","03/Feb/14 18:28;jkreps;KAFKA-1238.patch;https://issues.apache.org/jira/secure/attachment/12626705/KAFKA-1238.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,371351,,,Fri May 23 04:00:28 UTC 2014,,,,,,,,,,"0|i1s007:",371654,,,,,,,,,,,,,,,,,,,,"03/Feb/14 18:28;jkreps;Fix attached.;;;","03/Feb/14 19:05;nehanarkhede;Thanks for the patch, Jay! 

Review comments-

        // the server can give back an empty metadata response (!) if the topic doesn't exist and that is our only
        // topic, don't use that

The comment is a bit misleading. The server returns the LeaderNotAvailable error code in the response. The handleMetadataResponse doesn't seem to check for error codes and retry based on that. But retries based on cluster.nodes().size() > 0? ;;;","04/Feb/14 01:54;jkreps;Ack, this is clearly the wrong fix, I was misinterpreting the response. Right one is to check the error code as you say. That will require some refactoring.;;;","07/Feb/14 17:24;jkreps;Here is an updated patch that has the safety check but also breaks the metadata parsing into a separate class to retain errors. This refactoring didn't end up being strictly necessary but is still good.;;;","07/Feb/14 17:48;nehanarkhede;Thanks for the patch. Few review comments

1. Using review board saves review time. Please can we use it :)
2. Sender.handleMetadataResponse
I'm not sure if I understand how the metadata retry happens based on error codes in the metadata response. Also, the comment here is confusing. We do return the list of all brokers even if the topic is being auto created. During auto creation, we create the topic and return the LeaderNotAvailable error code back. In this case, we should retry getting topic metadata, similar to when any error code is received in the response
3. MetadataResponse
Is there a way to define the fields in a central place instead of hardcoding it in 2 places - one where the fields are defined (Protocol) and the other where those will be parsed?;;;","07/Feb/14 18:59;jkreps;1. Yeah I have been but due to git accident this ended up on the wrong branch so the tool doesn't work. I figured this was a pretty trivial one.
2. Actually after considering it I think the error code is irrelevant. The logic is ""always update the cluster metadata unless doing so would remove all known nodes"". For any topic we don't get metadata retry will occur automatically after the backoff. This makes sense as the set of topics may contain multiple new topics, some of these may have metadata and some may not (because creation is in process). So the correct logic is to update with whatever you get, but in the special case where there are no brokers don't update.
3. I would rather have the name be the contract rather than a variable. My rationale is that if we define 300 static variables in the protocol definition it becomes very hard to read (and obviously the protocol can't refer to the request object).;;;","09/Feb/14 23:52;jkreps;Hey Neha, can you let me know if you agree with my rationale on these?;;;","10/Feb/14 06:06;nehanarkhede;Regarding #2, I'm not so sure I understood the logic behind updating metadata only where there are no brokers. When do you expect topic metadata to return no brokers? Also, if the metadata request doesn't have an error code (which means it succeeded for all topics it asked for), then why would we retry? Also, if it failed only for a subset of topics, why not just retry with the subset of topics that failed instead of all topics. And, the bug is still not fixed with the latest patch, still hangs in an infinite loop for a new topic that is auto created. Will go through the code in more detail, as I may be missing something obvious here.;;;","10/Feb/14 17:22;jkreps;The way metadata works is the following:
1. When someone requests metadata using Metadata.java that we don't have we we flag that we need to update metadata.
2. Once that flag is set the sender will update metadata until the flag is unset. The flag is unset whenever the cluster is updated. There is a metadata fetch backoff to prevent polling too quickly.
3. The metadata update described in (1) and (2) will repeat until we get metadata for the topic we requested or we time out.

So if we don't update the cluster metadata then the metadata request just repeats as soon as the backoff expires.

So that is why it works. But why is this the right logic? Why don't we check the error code? The reason is simple, if we ever remove ALL the nodes from the cluster metadata then we lose the ability to continue to update metadata because we update metadata using a random node in our metadata. So if we update our metadata to have no nodes, then we have no one to update metadata with.

The server's behavior is that it will always include whatever brokers are referenced by the metadata in the response. So if you give a single topic and that topic doesn't exist you will get no brokers back and an error code for that topic. But regardless of the reason if the cluster metadata comes back with no brokers we should not use it (the only reason I know this can happen is an error, but it basically doesn't matter what error and perhaps there are other conditions).;;;","11/Feb/14 00:40;guozhang;Added a testcase in integration test for auto-created topic and it passed. I think the patch works.

My understanding is that without the patch we can ran into the deadend where the cluster is updated to have zero brokers, and hence loop indefinitely there. I think longer-term we should probably return all the brokers in the cluster no matter of the TopicMetadataRequest content.

A few comments besides my non-committer +1 :

1. The ""throw new IllegalStateException(""No known nodes."");"" in nextNode should be fatal since once we got here there would be not way out. Shall we make this exception a special one and force producer to stop?

2. The re-factoring seems much like the Request Object, if we are going to live with it shall we have a RequestOrResponse interface with toStruct and fromStruct APIs?

Guozhang;;;","11/Feb/14 00:46;jkreps;Yeah arguably the server should just return all the brokers. I think the only argument against that was supporting very very large clusters.

1. I guess I meant that more as a ""this should not happen"" exception, rather than handle it I kind of just want to proceed with the testing and fix these cases.

2. Yeah I was going to wait on the outcome of the discussion on request protocol serialization to establish a pattern there. Not many people chimed in on that one.;;;","11/Feb/14 01:23;nehanarkhede;Got it. Thanks for the explanation. +1;;;","22/May/14 20:05;tcollinsworth;What release is this expected to be included in? The patch is not applied to 0.8.1.1. Don't see where any release info is stated on the bug for what it applies to or which it is scheduled.

I'm trying out 0.8.1.1 and when I try to publish to a new non-existent partition the publisher ends up in a loop logging exceptions. If the test/publisher is stopped and restarted it works fine since it successfully created the partition, but couldn't detect it until restarted.

java.lang.IllegalStateException: No known nodes.
	at org.apache.kafka.common.Cluster.nextNode(Cluster.java:135)
	at org.apache.kafka.clients.producer.internals.Sender.maybeMetadataRequest(Sender.java:171)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:137)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:101)
	at java.lang.Thread.run(Thread.java:745);;;","23/May/14 04:00;junrao;The new producer is only stable in trunk and we expect to release in the next release 0.8.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable server to indefinitely retry on controlled shutdown,KAFKA-1235,12692642,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,01/Feb/14 03:24,28/Jan/15 06:20,14/Jul/23 05:39,28/Jan/15 06:20,,,,0.8.2.0,,,,,,,,,,0,,,,"Today the kafka server can exit silently if it hits an exception that is swallowed during controlled shut down or ""controlled.shutdown.max.retries"" has been exhausted. It is better to add an option to let it retry indefinitely.

Also will fix some other loose-check bugs on socket-closing logic.",,guozhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/14 20:02;guozhang;KAFKA-1235.patch;https://issues.apache.org/jira/secure/attachment/12626728/KAFKA-1235.patch","20/Feb/14 19:28;guozhang;KAFKA-1235_2014-02-20_11:28:36.patch;https://issues.apache.org/jira/secure/attachment/12630127/KAFKA-1235_2014-02-20_11%3A28%3A36.patch","20/Feb/14 21:08;guozhang;KAFKA-1235_2014-02-20_13:08:23.patch;https://issues.apache.org/jira/secure/attachment/12630151/KAFKA-1235_2014-02-20_13%3A08%3A23.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,371237,,,Wed Jan 28 06:20:08 UTC 2015,,,,,,,,,,"0|i1rzb3:",371540,,,,,,,,,,,,,,,,,,,,"03/Feb/14 20:02;guozhang;Created reviewboard https://reviews.apache.org/r/17671/
 against branch origin/trunk;;;","19/Feb/14 23:57;guozhang;Updated reviewboard https://reviews.apache.org/r/17671/
against branch origin/trunk;;;","20/Feb/14 19:28;guozhang;Updated reviewboard https://reviews.apache.org/r/17671/
 against branch origin/trunk;;;","20/Feb/14 21:08;guozhang;Updated reviewboard https://reviews.apache.org/r/17671/
 against branch origin/trunk;;;","04/Sep/14 22:22;guozhang;Moving to 0.9.;;;","28/Jan/15 06:20;guozhang;The patch is actually already committed on Feb.20th, 2014. Closing it now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Socket Leak on ReplicaFetcherThread,KAFKA-1228,12691067,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,yulrizka,yulrizka,yulrizka,24/Jan/14 16:19,30/Jan/14 18:18,14/Jul/23 05:39,30/Jan/14 18:18,0.8.0,,,0.8.1,,,,,,,consumer,,,0,,,,"Follow up bug on the user mailing list

http://search-hadoop.com/m/4TaT4XrcE71/possibly+leaking&subj=Possibly+leaking+socket+on+ReplicaFetcherThread

uncaught ""java.nio.channels.UnresolvedAddressException"" when trying to resolve brokers domain lead to unclose socket
","Ubuntu 12.04 64 bit
java version ""1.6.0_27""
OpenJDK Runtime Environment (IcedTea6 1.12.6) (6b27-1.12.6-1ubuntu0.12.04.4)
OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)",junrao,pasthelod,yulrizka,,,,,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/14 16:36;yulrizka;KAFKA-1228-2014-01-24.patch;https://issues.apache.org/jira/secure/attachment/12625067/KAFKA-1228-2014-01-24.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,369810,,,Thu Jan 30 18:18:06 UTC 2014,,,,,,,,,,"0|i1rql3:",370112,,,,,,,,,,,,,,,,,,,,"24/Jan/14 16:34;yulrizka;Catch all exception and close the socket;;;","24/Jan/14 16:37;yulrizka;Close socket connection on any Exception;;;","24/Jan/14 17:59;junrao;Thanks for the patch. We probably should do the same on the first try, instead of just on retry. ;;;","30/Jan/14 18:18;junrao;Thanks for the patch. Made some minor changes and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support arguments to zookeeper-shell.sh script,KAFKA-1214,12690324,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,pkwarren,pkwarren,pkwarren,22/Jan/14 17:35,24/Jan/14 17:56,14/Jul/23 05:39,24/Jan/14 17:56,0.8.0,,,0.8.1,,,,,,,,,,0,,,,"It would be useful to support arguments to zookeeper-shell.sh script (for example, in provisioning Kafka to use a chroot this could be used to pre-create the chroot ZNode).",,junrao,pkwarren,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/14 17:36;pkwarren;KAFKA-1214.patch;https://issues.apache.org/jira/secure/attachment/12624375/KAFKA-1214.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,369277,,,Fri Jan 24 17:56:23 UTC 2014,,,,,,,,,,"0|i1rnbz:",369582,,,,,,,,,,,,,,,,,,,,"22/Jan/14 17:36;pkwarren;Patch to enable additional arguments to the ZooKeeper shell script.;;;","24/Jan/14 17:56;junrao;Thanks for the patch. +1. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System test exception handling does not stop background producer threads,KAFKA-1212,12689409,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,17/Jan/14 00:49,27/Feb/14 22:17,14/Jul/23 05:39,27/Feb/14 22:17,,,,0.8.2.0,,,,,,,,,,0,,,,"When exception is thrown, the system test script stops all known entities. However, the background producer thread cannot be stopped since it does not register its pid in the testcase environment. We need to specifically stop them.",,guozhang,jfung,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/14 01:08;guozhang;KAFKA-1212.patch;https://issues.apache.org/jira/secure/attachment/12623554/KAFKA-1212.patch","21/Jan/14 21:21;guozhang;KAFKA-1212_2014-01-21_13:20:59.patch;https://issues.apache.org/jira/secure/attachment/12624190/KAFKA-1212_2014-01-21_13%3A20%3A59.patch","22/Jan/14 19:47;guozhang;KAFKA-1212_2014-01-22_11:47:22.patch;https://issues.apache.org/jira/secure/attachment/12624406/KAFKA-1212_2014-01-22_11%3A47%3A22.patch","22/Jan/14 19:52;guozhang;KAFKA-1212_2014-01-22_11:51:46.patch;https://issues.apache.org/jira/secure/attachment/12624408/KAFKA-1212_2014-01-22_11%3A51%3A46.patch","24/Jan/14 19:44;guozhang;KAFKA-1212_2014-01-24_11:43:53.patch;https://issues.apache.org/jira/secure/attachment/12625098/KAFKA-1212_2014-01-24_11%3A43%3A53.patch","27/Jan/14 18:55;guozhang;KAFKA-1212_2014-01-27_10:55:11.patch;https://issues.apache.org/jira/secure/attachment/12625401/KAFKA-1212_2014-01-27_10%3A55%3A11.patch","27/Jan/14 18:59;guozhang;KAFKA-1212_2014-01-27_10:59:24.patch;https://issues.apache.org/jira/secure/attachment/12625404/KAFKA-1212_2014-01-27_10%3A59%3A24.patch","28/Jan/14 02:24;guozhang;KAFKA-1212_2014-01-27_18:24:03.patch;https://issues.apache.org/jira/secure/attachment/12625503/KAFKA-1212_2014-01-27_18%3A24%3A03.patch","28/Jan/14 02:55;guozhang;KAFKA-1212_2014-01-27_18:54:53.patch;https://issues.apache.org/jira/secure/attachment/12625510/KAFKA-1212_2014-01-27_18%3A54%3A53.patch","28/Jan/14 17:26;guozhang;KAFKA-1212_2014-01-28_09:25:56.patch;https://issues.apache.org/jira/secure/attachment/12625605/KAFKA-1212_2014-01-28_09%3A25%3A56.patch",,,,,,,,,,,,,,,,,,10.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,368376,,,Thu Feb 27 22:17:04 UTC 2014,,,,,,,,,,"0|i1rhrz:",368680,,,,,,,,,,,,,,,,,,,,"17/Jan/14 01:08;guozhang;Created reviewboard https://reviews.apache.org/r/17006/
 against branch origin/trunk;;;","21/Jan/14 21:21;guozhang;Updated reviewboard https://reviews.apache.org/r/17006/
 against branch origin/trunk;;;","22/Jan/14 19:47;guozhang;Updated reviewboard  against branch origin/trunk;;;","22/Jan/14 19:52;guozhang;Updated reviewboard https://reviews.apache.org/r/17006/
 against branch origin/trunk;;;","23/Jan/14 21:19;jfung;Commented in the rb. thanks.;;;","24/Jan/14 19:44;guozhang;Updated reviewboard https://reviews.apache.org/r/17006/
 against branch origin/trunk;;;","27/Jan/14 18:55;guozhang;Updated reviewboard https://reviews.apache.org/r/17006/
 against branch origin/trunk;;;","27/Jan/14 18:59;guozhang;Updated reviewboard https://reviews.apache.org/r/17006/
 against branch origin/trunk;;;","28/Jan/14 02:24;guozhang;Updated reviewboard https://reviews.apache.org/r/17006/
 against branch origin/trunk;;;","28/Jan/14 02:55;guozhang;Updated reviewboard https://reviews.apache.org/r/17006/
 against branch origin/trunk;;;","28/Jan/14 17:26;guozhang;Updated reviewboard https://reviews.apache.org/r/17006/
 against branch origin/trunk;;;","27/Feb/14 22:17;junrao;Thanks for the patch. +1. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hold the produce request with ack > 1 in purgatory until replicas' HW has larger than the produce offset (KIP-101),KAFKA-1211,12689354,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,benstopford,guozhang,guozhang,16/Jan/14 21:13,07/Apr/17 14:33,14/Jul/23 05:39,06/Apr/17 21:53,,,,0.11.0.0,,,,,,,,,,1,reliability,,,"Today during leader failover we will have a weakness period when the followers truncate their data before fetching from the new leader, i.e., number of in-sync replicas is just 1. If during this time the leader has also failed then produce requests with ack >1 that have get responded will still be lost. To avoid this scenario we would prefer to hold the produce request in purgatory until replica's HW has larger than the offset instead of just their end-of-log offsets.",,abhishek.agarwal,aozeritsky,astubbs,BigAndy,cagatayk,cmolter,easyfmxu,fpj,guozhang,gwenshap,ijuma,jthakrar,junrao,kzadorozhny-tubemogul,mazhar.shaikh.in,mkizner,nehanarkhede,noslowerdna,sam.nguyen@sendgrid.com,wushujames,,,,,,,,,,,,,,,,,KAFKA-1188,,,,,,,,,,,,,,,,KAFKA-3919,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,368321,,,Thu Apr 06 21:53:00 UTC 2017,,,,,,,,,,"0|i1rhfz:",368626,,,,,,,,,,,,,,,,,,,,"24/Jul/14 16:23;junrao;Yes, this is a potential problem. Waiting for HW to be propagated to the followers will introduce another round of network delay on every message to be committed though. The following is another potential solution that avoid this overhead.

Note that the follower in ISR always has all committed messages. On follower startup, if we can figure out accurately which messages are committed and which ones are not, we won't unnecessarily truncate committed messages. Not that when a follower takes over as the new leader, it always tries to commit all existing messages that are obtained from the previous generation of the leader. After that, it will start committing new messages received in its own generation. If we can track the leader generation of each message, we can do the truncation accurately. To do that, in each replica, we maintain a leader generation vector that contains the leader generation id and its starting offset (the offset of the first message written by the leader in that generation) and we persist that vector in a LeaderGeneration file locally.

If a replica becomes a leader, before it accepts any new message, it first appends the current leader generation id and its current log end offset to the LeaderGeneration file. If a replica becomes a follower, it first gets the leader generation vector from the leader and then determines the offset where its highest leader generation ends in the leader. It will then truncate its log up to that offset (if there are messages beyond that offset). After that, the follower will store the leader generation vector obtained from the leader in its local LeaderGeneration file and starts fetching messages from the leader from its log end offset.

Let's consider a couple of examples. 

Example 1. Suppose that we have two replicas A and B and A is the leader. At some point, we have the following messages in A and B.

{noformat}
offset   A    B
1       m1  m1
2       m2
{noformat}
 
Let's assume that message m1 is committed, but m2 is not. At this point, A dies and B takes over as the leader. Let's say B then commits 2 more messages m3 and m4.

{noformat}
offset    A    B
0        m1  m1
1        m2  m3
2            m4
{noformat}

When replica A comes back, it's important for A to get rid of m2 from offset 1 since m2 is never committed. In this case, the leader generation vector in A and B will look like the following.

{noformat}
                 A                                        B
leaderGenId   startOffset                leaderGenId   startOffset
1                     0                    1           0
                                           2           1
{noformat}

By comparing A's leader generation vector with that from the current leader B, A knows that its latest messages are produced by the leader in generation 1, which ends at offset 0. So any message in its local log after offset 0 are not committed and can be truncated. Any message at or before offset 0 is guaranteed to be committed. So, replica A will remove m2 from offset 1 and get m3 and m4 from B afterward. At that point, A's log is consistent with that of B. All committed messages are preserved and all uncommitted messages are removed.

Example 2. Suppose that we have two replicas A and B and A is the leader. At some point, we have the following messages in A and B.

{noformat}
offset    A    B
1          m1  m1
2          m2  m2
{noformat}
 
Let's assume that both message m1 and m2 are committed. At this point, A dies and B takes over as the leader. Let's say B then commits 2 more messages m3 and m4.

{noformat}
offset    A    B
0          m1  m1
1          m2  m2
2              m3
3              m4
{noformat}

In this case, the leader generation vector in A and B will look like the following.

{noformat}
                 A                                       B
leaderGenId   startOffset                leaderGenId   startOffset
1                     0                    1                0
                                           2                2
{noformat}

When A comes back, by comparing its leader generation vector with that from the current leader B, A knows that its latest messages are produced by the leader in generation 1, which ends at offset 1. So, it will keep m2 at offset 1 and get m3 and m4 from B. Again, this will make A's log consistent with B.

This approach doesn't pay the extra network roundtrip to commit a message. The becoming follower process will be a bit slower since It now needs to issue a new request to get the leader vector before it can start fetching from the leader. However, since leader changes are rare, this probably provides a better tradeoff. There are also other details that need to be worked out.

1. We need to deal with the case that the leader generation vector may have a gap, i.e., no messages are produced in a leader generation.
2. We probably need to remove old leader generations from the LeaderGeneration file so that it won't grow forever. Perhaps we need to configure a max # of generations to keep.

Since this problem is relatively rare and the fix is a bit involved, we can probably put it off until 0.9 or beyond.


;;;","24/Jul/14 17:43;guozhang;Jun, I think in your review of KAFKA-1430's patch, you are already suggesting to wait for leader HW to be larger than the produce offset instead of just log end offset for ack=-1.

So as for ack > 1, but not = to num.replicas, since data loss may happen anyways because of the leader election logic may choose a follower which does not have all the committed data, this issue would just potentially increase the data loss by a bit under such scenarios. For its complexity and the benefit maybe it is not an optimization worth doing?;;;","24/Jul/14 18:06;junrao;The issue is that this problem not only affects ack>1, but only affects ack=-1. Suppose that you have 3 replicas A, B, and C and A is the leader initially. If A fails and B takes over as the new leader, C will first truncate its log, which could include committed data. Now, if immediately after the truncation, B fails, C has to be the new leader. Now, we may have lost previously committed messages, even though we had only 2 failures.;;;","24/Jul/14 18:24;guozhang;What I meant is that the ack=-1 should be already handled in KAFKA-1430, as we are not wait for leader HW. Right?;;;","25/Jul/14 04:47;junrao;Not quite. The case that I described above could happen with ack = -1 too.;;;","24/May/16 20:58;gwenshap;[~junrao] is this still an issue?;;;","07/Jun/16 02:58;ijuma;[~guozhang], is this still an issue?;;;","07/Jun/16 16:25;junrao;This is still an issue. It can cause data loss if the leader of a partition changes too quickly. This is less likely to happen with the fix in KAFKA-3670, but could still happen in theory. Fixing this is a bit involved since it would require the leader and the follower to keep track of and communicate additional information about leader generations, and may potentially require a change in message format.;;;","07/Jun/16 16:32;ijuma;Thanks Jun, I changed the fix version since I don't think we are working on this for 0.10.1.0.;;;","01/Aug/16 18:51;junrao;The following is a draft proposal. [~fpj], does that look reasonable to you?

1. In every log directory, we create a new leader-generation-checkpoint file, where we store the sequence (LGS) of leader generation and the start offset of messages produced in that generation.
2. When a replica becomes a leader, it first adds the leader generation and the log end offset of the replica to the end of leader-generation-checkpoint file and flushes the file. It then remembers its last leader generation (LLG) and becomes the leader.
3. When a replica becomes a follower, it does the following steps.
  3.1 Send a new RetreiveLeaderGeneration request for the partition to the leader.
  3.2 The leader responds with its LGS in the RetreiveLeaderGeneration response
  3.3 The follower finds the first leader generation whose start offset differs between its local LGS and the leader's LGS. It then truncates its local log to the smaller of the start offset of the identified leader generation, if needed.
  3.4 The follower flushes the LGS from the leader to its local leader-generation-checkpoint file and also remembers the expected LLG from the leader's LGS.
  3.5 The follower starts fetching from the leader from its log end offset.
  3.5.1 During fetching, we extend the FetchResponse to add a new field per partition for the LLG in the leader.
  3.5.2 If the follower sees the returned LLG in the FetchResponse not matching its expected LLG, go back to 3.1. (This can only happen if the leader changes more than once between 2 consecutive fetch requests and should be rare. We could also just stop the follower and wait for the next becoming follower request from the controller.)
  3.5.3 Otherwise, the follower proceeds to append the fetched data to its local log in the normal way.

Implementation wise. We probably need to extend ReplicaFetchThread to maintain an additional state per partition. When a partition is added to a ReplicaFetchThread, it needs to go through steps 3.1 to 3.4 first before starting fetching the data.;;;","02/Aug/16 16:00;fpj;[~junrao] let me ask a few clarification questions.

# Is it right that the scenarios described here do not affect the cases in which min isr > 1 and unclean leader election is disabled? If min isr is greater than 1 and the leader is always coming from the latest isr, then the leader can either truncate the followers or have them fetch the missing log suffix.
# The main goal of the proposal is to have replicas in a lossy configuration (e.g. min isr = 1, unclean leader election enabled) a leader and a follower converging to a common prefix by choosing an offset based on a common generation. The chosen generation is the largest generation in common between the two replicas. Is it right?
# How do we guarantee that the generation id is unique, by using zookeeper versions?
# I think there is a potential race between updating the leader-generation-checkpoint file and appending the first message of the generation. We might be better off rolling the log segment file and having the generation being part of the log segment file name. This way when we start a new generation, we also start a new file and we know precisely when a message from that generation has been appended.
# Let's consider a scenario with 3 servers A B C. I'm again assuming that it is ok to have a single server up to ack requests. Say we have the following execution:

||Generation||A||B||C||
|1| |m1|m1|
| | |m2|m2|
|2|m3| | |
| |m4| | |

Say that now A and B start generation 3. They have no generation in common, so they start from zero, dropping m1 and m2. Is that right? If later on C joins A and B, then it will also drop m1 and m2, right? Given that the configuration is lossy, it doesn't wrong to do it as all we are trying to do is to converge to a consistent state. ;;;","02/Aug/16 17:10;junrao;[~fpj], for #1 and #2, there are a couple scenarios that this proposal can fix.
a. The first one is what's described in the original jira. Currently, when the follower does truncation, it can truncate some previously committed messages. If the follower immediately becomes the leader after truncation, we will lose some previously committed messages. This is rare, but if it happens, it's bad. The proposal fixes this case by preventing the follower from unnecessarily truncating previously committed messages.
b. Another issue is that a portion of the log in different replicas may not match in certain failure cases. This can happen when unclean leader election is enabled. However, even if unclean leader election is disabled, mis-matching can still happen when messages are lost due to power outage (see KAFKA-3919). The proposal fixes this issue by making sure that the replicas are always identical.

For #3, the controller increases the leader generation every time the leader changes. The latest leader generation is persisted in ZK.

For #4, putting the leader generation in the segment file name is another possibility. One concern I had on that approach is dealing with compacted topics. After compaction, it's possible there is only a small number (or even just a single) messages left in a particular generation. Putting the generation id in the segment file name will force us to have tiny segments, which is not ideal. About the race condition, even with a separate checkpoint file, we can avoid that. The sequencing will be (1) broker receives LeaderAndIsrRequest to become leader; (2) broker stops fetching from current leader; (3) no new writes can happen to this replica at this point; (4) broker writes the new leader generation and log end offset to checkpoint file; (5) broker marks replica as leader; (6) new writes can happen to this replica now.

For #5, it depends on who becomes the new leader in that case. If A becomes the new leader (generation 3), then B and C will remove m1 and m2 and copy m3 and m4 over from A. If B becomes the new leader, A will remove m3 and m4 and copy m1 and m2 over from B. In either case, the replicas will be identical.;;;","03/Aug/16 15:30;fpj;Thanks for the clarification, [~junrao]. There are a couple of specific points that still aren't entirely clear to me:

# We are trying to preserve the generation when we copy messages to a follower, correct? In step 3.4, when we say that the follower flushes the LGS, we are more specifically trying to replicate the leader LGS, is that right? What happens if either the follower crashes or the leader changes between persisting the new LGS and fetching the new messages from the leader? I'm concerned that we will leave the LGS and the log of the broker in an inconsistent state.
# When we say in step 3.4 that the follower needs to remember the LLG, I suppose this is just during this recovery period. Otherwise, once we have completed the sync up, the follower knows that the latest generation is the LLG. During sync up, there is the question I'm raising above, but it is also not super clear whether we need to persist the LLG independently to make sure that we don't have a situation in which the follower crashes, comes back, and accepts messages from a different generation.;;;","04/Aug/16 00:51;junrao;[~fpj], very good questions.

1. Yes, the idea is for the follower to copy LGS from the leader. About the possibility of leading to an inconsistent state. We just need to make sure the log is consistent with respect to the local leader-generation-checkpoint file up to the log end offset. One potential issue with the current proposal is when the follower truncates the file and then flushes the checkpoint file. If the follower crashes at this point and the truncation hasn't been flushed, we may treat some of the messages after the truncation point  to be in a wrong leader generation. To fix that, we can change the protocol a bit. The basic idea is that the follower will never flush the checkpoint ahead of the log. Specially, when the follower gets the LGS from the leader, it stores it in memory. After truncation, the follower only flushes the prefix of LGS whose start offset is up to the log end offset. As the follower starts fetching data, everytime the fetched messages cross the leader generation boundary (according to the cached LGS), the follower will add a new lead generation entry to the checkpoint file and flushes it.

2. LLG doesn't have to be persisted and only needs to be cached in memory. The idea of LLG is really to detect any leader generation changes since the follower issued the RetreiveLeaderGeneration request. Once this is detected, the follower can handle it properly. If the follower crashes and restarts, it can always re-get the LLG from the current leader.;;;","06/Aug/16 18:05;fpj;[~junrao] I get (2), but (1) might need a bit more tweaking because of the following. Say the follower executes the steps in the order you describe:

# Truncate log
# Update LGS to reflect the truncated log (flush the prefix of LGS whose start offset is up to the log end offset)
# Fetch and update the LGS as messages cross leader boundaries

If the follower crashes in the middle of fetching and updating the LGS, it may leave the LGS in an inconsistent state. For example, let's say that it crossed the boundary of a generation, it writes to the log, and crashes before updating the LGS. I'm actually thinking that it might be better to have the update in the LGS first because in the worst case we point to an offset that is not in the log, so we know that the LGS entry is invalid. In any case, it sounds like there is some work to be done to make sure the LGS and the log are consistent.

;;;","06/Aug/16 18:47;fpj;[~junrao] I was reading point (a) in your answer again, and there is something I don't understand. You say that the follower truncates and then become leader. This is fine, I understand it can happen. The bit I don't understand is how it can truncate committed messages. 

Let's say that we are talking about servers A and B, min ISR is 2 (the replica set can be larger than 2, but it doesn't really matter for this example):

# A leads initially and B follows A.
# B truncates
# B becomes leader

If A leads, the it means that it was previously in the ISR (assuming unclean leader election disabled) and it contains all committed messages. If B was also part of the previous ISR, then both A and B it will also have all committed and B won't truncate committed messages.

The situation you describe can only happen if either A or B lose committed messages on their own and not because of the truncation, e.g., if the messages didn't make it from the page cache to disk before a crash.

Is my understanding correct?;;;","08/Aug/16 17:23;junrao;[~fpj], yes, the idea is to always first write the new LGS before writing any messages in the new leader generation to the follower's log. So, if the follower fetches a chunk of messages that cross leader generation. We will have to split the messages into subsets of the same leader generation. We append subsets belonging to the current leader generation, update LGS to reflect to new leader generation, then append subsets belonging to the new leader generation.;;;","08/Aug/16 17:35;junrao;[~fpj], for (a), this is because the leader needs to first wait for the follower to receive a message before it can advance the last committed offset. After that, it can propagate the last committed offset to the follower. So, the last committed offset in the follower is always behind that in the leader. Since the follower truncates based on the local last committed offset, it's possible for the follower to truncate messages that are already committed by the leader.;;;","13/Aug/16 16:09;fpj;[~junrao]

bq. the leader needs to first wait for the follower to receive a message before it can advance the last committed offset.

makes sense

bq. it can propagate the last committed offset to the follower

makes sense

bq. the last committed offset in the follower is always behind that in the leader

makes sense, it is either equal or behind, never ahead.

bq. Since the follower truncates based on the local last committed offset, it's possible for the follower to truncate messages that are already committed by the leader.

I'm not sure why we are doing this. A follower can't truncate until it hears from the leader upon recovery, it shouldn't truncate based on its local last committed offset.;;;","05/Apr/17 22:41;junrao;The PR for this issue is https://github.com/apache/kafka/pull/2808;;;","06/Apr/17 21:53;junrao;The PR has been merged. Thanks, [~benstopford]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows Bat files are not working properly,KAFKA-1210,12689253,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,sslavic,HCanber,HCanber,16/Jan/14 14:43,13/Apr/14 15:44,14/Jul/23 05:39,13/Apr/14 15:44,0.8.1,,,0.8.2.0,,,,,,,,,,2,,,,"The bat files are not working properly.
The paths in them are invalid.
They have not been updated to reflect the changes made to the shell scripts.",Windows,HCanber,johnarnold,junrao,reinhard.maier,sslavic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1195,,,,,,,,,KAFKA-581,,,,,,,,,,"16/Jan/14 14:59;HCanber;1210-v1.patch;https://issues.apache.org/jira/secure/attachment/12623399/1210-v1.patch","10/Apr/14 12:54;sslavic;KAFKA-1210-v2.patch;https://issues.apache.org/jira/secure/attachment/12639569/KAFKA-1210-v2.patch","09/Apr/14 08:31;sslavic;KAFKA-1210-v2.patch;https://issues.apache.org/jira/secure/attachment/12639365/KAFKA-1210-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,368220,,,Sun Apr 13 15:44:32 UTC 2014,,,,,,,,,,"0|i1rgtj:",368525,,,,,,,,,,,,,,,,,,,,"16/Jan/14 14:56;HCanber;Patch: Updated windows bat files
The patch do not include -daemon -loggc or name <name> options since shifting command arguments are not working properly in bat files (when shifting the argument isn't removed from the all-arguments-variable).;;;","24/Feb/14 18:22;junrao;Thanks for the patch. We have moved to gradle now. Could you double check that the patch still applies?;;;","08/Apr/14 10:04;sslavic;kafka-run-class.bat changes from the patch unfortunatelly do not apply to current 0.8.1 branch head.
Befor trying to apply patch file, on Windows, I had to first convert patch file encoding to ANSI and EOL to Unix format, like explained in [this SO question|http://stackoverflow.com/q/13675782/381140].;;;","08/Apr/14 17:59;johnarnold;Is there a branch that ""just works"" on windows?

I tried the 0.8.1 tarball with windows batch file fixes and had issues with loading the classes, even after setting static paths in the batch files.

PS C:\kafka> .\bin\windows\kafka-create-topic.bat --zookeeper localhost:2181 --replica 1 --partition 1 --topic test
C:\kafka
C:\kafka\libs
classpath=C:\kafka\libs;""C:\kafka\target\scala-2.9.2\kafka_2.9.2-0.1-SNAPSHOT.jar"";""C:\kafka\target\scala-2.9.2\kafka_2.9.2-0.1-SNAPSHOT.jar"";""C:\kafka\libs\jopt-simple-3.2.jar"";""C:\kafka\libs\kafka_2.9.2-0.8.1.jar"";""C:\kafka\libs\log4j-1.2.15.jar"";""C:\kafka\libs\metrics-annotation-2.2.0.jar"";""C:\kafka\libs\metrics-core-2.2.0.jar"";""C:\kafka\libs\scala-library-2.9.2.jar"";""C:\kafka\libs\slf4j-api-1.7.2.jar"";""C:\kafka\libs\slf4j-log4j12-1.7.6.jar"";""C:\kafka\libs\snappy-java-1.0.5.jar"";""C:\kafka\libs\zkclient-0.3.jar"";""C:\kafka\libs\zookeeper-3.3.4.jar""

""C:\java\bin\java"" -Xmx256M -server -XX:+UseCompressedOops -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC -Djava.awt.headless=true -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false  -Dcom.sun.management.jmxremote.ssl=false -Dlog4j.configuration=file:C:\kafka\config\tools-log4j.properties -cp C:\kafka\libs;""C:\kafka\target\scala-2.9.2\kafka_2.9.2-0.1-SNAPSHOT.jar"";""C:\kafka\target\scala-2.9.2\kafka_2.9.2-0.1-SNAPSHOT.jar"";""C:\kafka\libs\jopt-simple-3.2.jar"";""C:\kafka\libs\kafka_2.9.2-0.8.1
.jar"";""C:\kafka\libs\log4j-1.2.15.jar"";""C:\kafka\libs\metrics-annotation-2.2.0.jar"";""C:\kafka\libs\metrics-core-2.2.0.jar"";""C:\kafka\libs\scala-library-2.9.2.jar"";""C:\kafka\libs\slf4j-api-1.7.2.jar"";""C:\kafka\libs\slf4j-log4j12-1.7.6.jar"";""C:\kafka\libs\snappy-java-1.0.5.jar"";""C:\kafka\libs\zkclient-0.3.jar"";""C:\kafka\libs\zookeeper-3.3.4.jar""  kafka.admin.CreateTopicCommand --zookeeper localhost:2181 --replica 1 --partition 1 --topic test

Error: Could not find or load main class kafka.admin.CreateTopicCommand
PS C:\kafka>

Help?;;;","09/Apr/14 08:31;sslavic;Attaching updated [^KAFKA-1210-v2.patch], based on [^1210-v1.patch] by [~HCanber], with following changes:
- adjusted libs (classpath element) references, changed due to migration from SBT to Gradle
- In {{windows/kafka-run-class.bat}} implemented error handling, when empty classpath is detected, giving feedback that user/developer likely has not built the project before trying to run a given class;;;","09/Apr/14 15:20;junrao;Thanks for the patch. Could another window user confirm that the script works?;;;","10/Apr/14 12:54;sslavic;Attaching updated [^KAFKA-1210-v2.patch] with extra fixes and little bit of fine tuning after feedback from a colleague.;;;","10/Apr/14 13:01;reinhard.maier;verified the patch on Windows 7 by  starting Zookeeper and Kafka and changing the log levels in log4j.properties;;;","13/Apr/14 15:44;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update system test still to use kafka-topics instead of kafka-create-topics shell,KAFKA-1208,12689120,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,15/Jan/14 23:29,16/Jan/14 04:54,14/Jul/23 05:39,16/Jan/14 04:54,,,,0.8.1,,,,,,,,,,0,,,,,,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1209,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/14 03:48;guozhang;KAFKA-1208.patch;https://issues.apache.org/jira/secure/attachment/12623301/KAFKA-1208.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,368087,,,Thu Jan 16 04:54:25 UTC 2014,,,,,,,,,,"0|i1rg07:",368392,,,,,,,,,,,,,,,,,,,,"16/Jan/14 03:48;guozhang;Created reviewboard https://reviews.apache.org/r/16944/
 against branch origin/trunk;;;","16/Jan/14 04:54;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
README in examples not update,KAFKA-1205,12688941,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ailzhang,ailzhang,ailzhang,15/Jan/14 05:26,17/Jan/14 16:27,14/Jul/23 05:39,17/Jan/14 16:27,0.8.0,,,0.8.1,,,,,,,,,,0,patch,,,The commands in the examples/README are not working. The name of the projects and the order of them have been changed.,,ailzhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/14 17:49;ailzhang;2.patch;https://issues.apache.org/jira/secure/attachment/12623428/2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,367908,,,Fri Jan 17 16:27:53 UTC 2014,,,,,,,,,,"0|i1rexz:",368215,,,,,,,,,,,,,,,,,,,,"16/Jan/14 16:22;junrao;Thanks for the patch. It doesn't seem to apply to trunk though.

git apply ~/Downloads/1.patch 
error: patch failed: examples/README:6
error: examples/README: patch does not apply
;;;","16/Jan/14 17:49;ailzhang;Sorry for it. This 2.patch should be working. Thanks a lot! ;;;","16/Jan/14 17:57;ailzhang;Hi Jun Rao,
Thanks a lot for your reply! I'm new to kafka and it's my first time to use
patch to commit. Have updated a new 2.patch in the issue. It should be
working.
Have a nice day!

Ailing



;;;","17/Jan/14 16:27;junrao;Thanks for patch v2. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
optimize ZK access in KafkaController,KAFKA-1202,12687897,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,08/Jan/14 17:14,15/Jan/14 16:36,14/Jul/23 05:39,15/Jan/14 16:36,0.8.1,,,0.8.1,,,,,,,core,,,0,,,,"In KafkaController, we access ZK in the following places. Those accesses are not necessary since we can read from the cache in the controller.

In onBrokerFailure(deadBrokers: Seq[Int]),
replicaStateMachine.handleStateChanges(getAllReplicasOnBroker(zkClient, controllerContext.allTopics.toSeq, deadBrokers), OfflineReplica)
  }

In onBrokerStartup(newBrokers: Seq[Int])
replicaStateMachine.handleStateChanges(getAllReplicasOnBroker(zkClient, controllerContext.allTopics.toSeq, newBrokers), OnlineReplica)
  }

In  shutdownBroker(),
getPartitionsAssignedToBroker(zkClient, controllerContext.allTopics.toSeq, id).map {
  }
",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/14 05:57;junrao;KAFKA-1202.patch;https://issues.apache.org/jira/secure/attachment/12622586/KAFKA-1202.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,366904,,,Wed Jan 15 16:36:05 UTC 2014,,,,,,,,,,"0|i1r8sv:",367215,,,,,,,,,,,,,,,,,,,,"13/Jan/14 05:58;junrao;Created reviewboard https://reviews.apache.org/r/16814/
 against branch origin/trunk;;;","15/Jan/14 16:36;junrao;Thanks for the review. Committed to trunk. Also, incorporated changes in kafka-1020.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inconsistent log levels when consumed offset is reset,KAFKA-1200,12687802,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,jbrosenberg@gmail.com,jbrosenberg@gmail.com,08/Jan/14 04:53,14/Jan/14 02:16,14/Jul/23 05:39,14/Jan/14 02:16,0.8.0,,,0.8.1,,,,,,,consumer,,,0,,,,"I've recently been dealing with the issue where my consumer falls behind and essentially loses data when the broker deletes data, due to it's retention policy.

On the broker, this is logged as an ERROR:

2013-12-23 05:02:08,456 ERROR [kafka-request-handler-2] server.KafkaApis - [KafkaApi-45] Error when processing fetch request for partition [mytopic,0] offset 204243601 from consumer with correlation id 130341
kafka.common.OffsetOutOfRangeException: Request for offset 204243601 but we only have log segments in the range 204343397 to 207423640.

But on the consumer, this same event is logged as a WARN:

2013-12-23 05:02:08,797  WARN [ConsumerFetcherThread-myconsumergroup-1387353494862-7aa0c61d-0-45] consumer.ConsumerFetcherThread - [ConsumerFetcherThread-myconsumergroup-1387353494862-7aa0c61d-0-45], Current offset 204243601 for partition [mytopic,0] out of range; reset offset to 204343397

It seems this should also be an ERROR condition (it would seem the consumer would care more about this than the broker, at least!).

Also, sometimes (but not always) there is also this log message on the consumer, which does log as an ERROR (I'm not sure why this log line doesn't always appear after the above WARN?):

2014-01-08 02:31:47,681 ERROR [myconsumerthread-0]
consumer.ConsumerIterator - consumed offset: 16163904970 doesn't match
fetch offset: 16175326044 for mytopic:0: fetched offset = 16175330598:
consumed offset = 16163904970;
 Consumer may lose data

In this message, there is the ""Consumer may lose data"" message, which makes sense.  Seems the fetcher thread above should also log something like that, and be an ERROR.

This would allow for more consistent alerting, in this case.",,Dima Pekar,jbrosenberg@gmail.com,joestein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/14 12:58;Dima Pekar;KAFKA-1200.patch;https://issues.apache.org/jira/secure/attachment/12622618/KAFKA-1200.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,366806,,,Tue Jan 14 02:16:24 UTC 2014,,,,,,,,,,"0|i1r873:",367117,,,,,,,,,,,,,,,,,,,,"08/Jan/14 15:31;joestein;the consumer should log the error too, yeah.  should be able to patch this too if no one else is already working on it;;;","13/Jan/14 12:58;Dima Pekar;Provided patch that changes log level (several occurrences) to from warn to error inside kafka.server.AbstractFetcherThread#processFetchRequest method.
;;;","14/Jan/14 02:16;joestein;+1, committing to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in describe topic,KAFKA-1198,12687012,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,junrao,junrao,02/Jan/14 18:17,13/Jan/14 04:38,14/Jul/23 05:39,13/Jan/14 04:38,0.8.1,,,0.8.1,,,,,,,,,,0,,,,"If topic is not specified, we get the following.

bin/kafka-topics.sh --zookeeper localhost:2181 --describe
(Error while executing topic command,java.lang.NullPointerException)

Also, list topic seems to always list all topics even when topics are specified.
",,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jan/14 23:10;guozhang;KAFKA-1198.patch;https://issues.apache.org/jira/secure/attachment/12621179/KAFKA-1198.patch","03/Jan/14 18:18;guozhang;KAFKA-1198_2014-01-03_10:17:46.patch;https://issues.apache.org/jira/secure/attachment/12621354/KAFKA-1198_2014-01-03_10%3A17%3A46.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,366007,,,Fri Jan 03 20:19:13 UTC 2014,,,,,,,,,,"0|i1r393:",366314,,,,,,,,,,,,,,,,,,,,"02/Jan/14 19:55;guozhang;1) List option will always get all the topics, only describe option will take the topic specification.

2) the getTopics function does not check if opts.topicOpt is specified, and hence throws the NullPointerException.

Proposed solution: check topicOpt unless it is the list option.;;;","02/Jan/14 21:38;guozhang;I tried the following with the current trunk HEAD and did not hit the Exception:

1. start ZK
2. start one broker
3. bin/kafka-topics.sh --zookeeper localhost:2181 --describe returns empty.
4. create a topic
5. bin/kafka-topics.sh --zookeeper localhost:2181 --list returns the topic
6. bin/kafka-topics.sh --zookeeper localhost:2181 --describe returns the description of the topic.;;;","02/Jan/14 22:49;guozhang;I can re-produce this issue now, was having an older jar before.;;;","02/Jan/14 23:10;guozhang;Created reviewboard https://reviews.apache.org/r/16579/
 against branch origin/trunk;;;","03/Jan/14 18:19;guozhang;Updated reviewboard https://reviews.apache.org/r/16579/
 against branch origin/trunk;;;","03/Jan/14 20:19;junrao;Thanks for the patch. +1 and committed trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.IllegalArgumentException Buffer.limit on FetchResponse.scala + 33,KAFKA-1196,12686852,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,ewencp,gerritjvv,gerritjvv,31/Dec/13 16:15,26/Sep/16 03:48,14/Jul/23 05:39,26/Sep/16 03:48,0.8.0,,,0.10.1.0,,,,,,,consumer,,,1,newbie,,,"I have 6 topics each with 8 partitions spread over 4 kafka servers.
the servers are 24 core 72 gig ram.

While consuming from the topics I get an IlegalArgumentException and all consumption stops, the error keeps on throwing.

I've tracked it down to FectchResponse.scala line 33

The error happens when the FetchResponsePartitionData object's readFrom method calls:
messageSetBuffer.limit(messageSetSize)

I put in some debug code the the messageSetSize is 671758648, while the buffer.capacity() gives 155733313, for some reason the buffer is smaller than the required message size.

I don't know the consumer code enough to debug this. It doesn't matter if compression is used or not.




","running java 1.7, linux and kafka compiled against scala 2.9.2",antonymayi,becket_qin,dashengju,donnchadh,ewencp,gerritjvv,ijuma,junrao,kbanker,kostassoid,nehanarkhede,solon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-2063,,,,,,,,,,,,,"16/Oct/14 16:55;ewencp;KAFKA-1196.patch;https://issues.apache.org/jira/secure/attachment/12675308/KAFKA-1196.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,365845,,,Mon Sep 26 03:48:21 UTC 2016,,,,,,,,,,"0|i1r293:",366152,,,,,,,,,,,,,,,,,,,,"06/Jan/14 18:27;nehanarkhede;As suggested by Jun -

If a broker is the leader of multiple partitions of a topic, the high level
consumer will fetch all those partitions in a single fetch request. Then
the aggregate of the fetched data from multiple partitions could be more
than 2GB.

You can try using more consumers in the same consumer group to reduce #
partitions fetched per consumer.

Thanks,

Jun;;;","06/Jan/14 18:39;gerritjvv;Hi, thanks for the response, but this is not really a solution.

Its obvious that how the api does the call is then flawed that or the broker should take care of it. I cannot add more consumers just for this issue because my data will grow maybe it doubles in a year, then I'd run again into the same problem, which is in the end an integer overflow problem and not a resource problem.

i.e. beyond a certain amount of data the api fails and I should add more consumers? I've opted for writing my own api that does calls on a topic,partition basis to each broker, so far its working.


 
;;;","13/Jan/14 04:44;junrao;To really support fetching more than 2GB of data in a single fetch response requires wire protocol change. One simple thing that we can do immediately is to sanity check the response size. If it's more than 2GB, just throw an error to the client.;;;","13/Jan/14 09:33;gerritjvv;If I throw an error then what? How can I consume the data in the first place? I'm using all the defaults and the provided stock consumer/producer.

The problem here is that I've not setup anywhere in the configuration to consume more than 2GB, its the way that the consumer does its fetch that causes the data to go over 2GB.

So it means that: If you use the broker and consumer and for some reason the consumer does a fetch over 2GB at any time, I'll be unable to consume the data ever, even though no single message is even over the 100-200 mb. The only solution left to me is then either write my own client, or delete the topic data every time I see this, which is about 5 seconds after I removed and recreated the topic.

;;;","15/Jan/14 15:54;junrao;Yes, I wasn't thinking of fixing the issue since I can't think of an easy way. I was just thinking of giving the client the right error message so that the client is not confused  btw this case and an actual log corruption.;;;","15/Jan/14 15:59;gerritjvv;:) , it doesn't seem like there is an easy fix, 
 I'm trying to write my own consumer that would try and work around this error, i.e. ignore the initial message length, and still read the message sets, even though the actual topic/partition/messageset sequences might go over the initial 4 byte int message size. will comment on how this goes.;;;","15/Jan/14 17:08;gerritjvv;I've written a consumer https://github.com/gerritjvv/kafka-fast, its running on the same dataset implemented what I said in the previous comment, I do not see any errors yet.
;;;","02/Jul/14 09:09;dashengju;I have encounter this problem.

I have many topics and partitions, every topic have 2 replicas. Then one broker needs to consumer data from another broker. It seems the broker to fetch many replication,  and the sync was failed, because of the error above. So the ISR list is always just the ""preferred replica"". 

I think this is a big problem.

============= below is the error log ====================================
[2014-06-28 10:00:03,434] ERROR [ReplicaFetcherThread-0-1], Error in fetch Name: FetchRequest; Version: 0; CorrelationId: 710; ClientId: ReplicaFetcherThread-0-1; ReplicaId: 5; MaxWait: 3000 ms; MinBytes: 1 bytes; RequestInfo: [org.wasp_card_log,12] -> PartitionFetchInfo(0,67108864),[org.mobile_push_netevent,1] -> PartitionFetchInfo(492788,67108864),[org.mobile,8] -> PartitionFetchInfo(59767,67108864),[app.newbuyer,7] -> PartitionFetchInfo(42,67108864),[binlog.wwwdeal,6] -> PartitionFetchInfo(1718,67108864),[org.b,6] -> PartitionFetchInfo(150548,67108864),[org.filestorage,14] -> PartitionFetchInfo(11,67108864),[org.feedbackchange,6] -> PartitionFetchInfo(277,67108864),[org.eventlog,6] -> PartitionFetchInfo(101049,67108864),[org.bizappapi,10] -> PartitionFetchInfo(0,67108864),[org.data_resys_routerres,5] -> PartitionFetchInfo(606925,67108864),[log.mobile,5] -> PartitionFetchInfo(0,67108864),[org.mtrace,0] -> PartitionFetchInfo(75581,67108864),[org.mobile,3] -> PartitionFetchInfo(2062782,67108864),[org.data_resys_geo,10] -> PartitionFetchInfo(678,67108864),[org.data_asyncquerier_tablecolumn,12] -> PartitionFetchInfo(1,67108864),[org.mobile_push_applist,10] -> PartitionFetchInfo(2438334,67108864),[org.mobile_hotellog,12] -> PartitionFetchInfo(623,67108864),[org.mobile,15] -> PartitionFetchInfo(2376788,67108864),[org.stormlog,0] -> PartitionFetchInfo(0,67108864),[org.mtcrm,14] -> PartitionFetchInfo(250,67108864),[binlog.coupon,4] -> PartitionFetchInfo(149681,67108864),[org.mtrace,1] -> PartitionFetchInfo(1025277,67108864),[org.cos_sso,15] -> PartitionFetchInfo(3301,67108864),[org.data_resys_router,6] -> PartitionFetchInfo(1031,67108864),[log.mobile,3] -> PartitionFetchInfo(17453,67108864),[log.loginlog,7] -> PartitionFetchInfo(5932,67108864),[org.data_search_queryserver,13] -> PartitionFetchInfo(891,67108864),[org.mobile_push_applist,5] -> PartitionFetchInfo(180204,67108864),[org.data_resys_hotquery,4] -> PartitionFetchInfo(1153,67108864),[org.wasp_device,0] -> PartitionFetchInfo(10,67108864),[org.mobilerpc,9] -> PartitionFetchInfo(42137,67108864),[org.mtrace,9] -> PartitionFetchInfo(1783580,67108864),[org.mobile_push_applist,8] -> PartitionFetchInfo(2471375,67108864),[org.mobile_eventlog,1] -> PartitionFetchInfo(311414,67108864),[org.extendapply,15] -> PartitionFetchInfo(0,67108864),[app.newbuyer,14] -> PartitionFetchInfo(0,67108864),[org.mobile_push_record,13] -> PartitionFetchInfo(11,67108864),[org.mtrace,3] -> PartitionFetchInfo(1578588,67108864),[log.orderlog,8] -> PartitionFetchInfo(0,67108864),[org.mtrace,4] -> PartitionFetchInfo(122482,67108864),[org.tmplog,13] -> PartitionFetchInfo(0,67108864),[org.ecomlogin,0] -> PartitionFetchInfo(2,67108864),[org.payrequest,6] -> PartitionFetchInfo(6207,67108864),[org.dealrank,14] -> PartitionFetchInfo(3699,67108864),[org.wm_submitorder,15] -> PartitionFetchInfo(0,67108864),[org.web_seckill_ticketlog,2] -> PartitionFetchInfo(0,67108864),[org.nginx,4] -> PartitionFetchInfo(3122673,67108864),[org.mobile_group_push,11] -> PartitionFetchInfo(0,67108864),[org.data_resys_routerreq,11] -> PartitionFetchInfo(162226,67108864),[binlog.coupon,10] -> PartitionFetchInfo(32224,67108864),[log.eventlog,10] -> PartitionFetchInfo(31378,67108864),[log.intranet_nginx,2] -> PartitionFetchInfo(0,67108864),[org.extendapply,13] -> PartitionFetchInfo(1,67108864),[log.accesslog,0] -> PartitionFetchInfo(149156,67108864),[org.mobile,4] -> PartitionFetchInfo(51119,67108864),[org.ecomlogin,14] -> PartitionFetchInfo(655,67108864),[org.mobileapp,4] -> PartitionFetchInfo(150715,67108864),[org.mobile_push_netevent,14] -> PartitionFetchInfo(12863099,67108864),[org.wasp_card_log,4] -> PartitionFetchInfo(11,67108864),[org.data_resys_router,4] -> PartitionFetchInfo(35192,67108864),[org.nginx,15] -> PartitionFetchInfo(92880,67108864),[org.mobile_groupapi_search,6] -> PartitionFetchInfo(408,67108864),[org.wasp_page_visit_record,0] -> PartitionFetchInfo(0,67108864),
[log.couponverify,4] -> PartitionFetchInfo(0,67108864),[org.nginx,9] -> PartitionFetchInfo(100270,67108864),[org.mtcrm,7] -> PartitionFetchInfo(5078,67108864),[org.data_resys_hotquery,6] -> PartitionFetchInfo(83107,67108864),[org.mobile_nginx,6] -> PartitionFetchInfo(157869,67108864),[org.mobile,9] -> PartitionFetchInfo(1831700,67108864),[org.intranet_nginx,9] -> PartitionFetchInfo(132779,67108864),[org.mobile_push_applist,4] -> PartitionFetchInfo(2279570,67108864),[org.dealrank,0] -> PartitionFetchInfo(103,67108864),[org.mobile_networklog,3] -> PartitionFetchInfo(1723358,67108864),[org.mobile_movie_statistics,5] -> PartitionFetchInfo(6309,67108864),[org.mtrace,2] -> PartitionFetchInfo(94002,67108864),[org.data_asyncquerier_sql,7] -> PartitionFetchInfo(1,67108864),[org.spamlogin,12] -> PartitionFetchInfo(2480,67108864),[org.search_dealrank,8] -> PartitionFetchInfo(24118,67108864),[log.orderlog,1] -> PartitionFetchInfo(7231,67108864),[log.b,12] -> PartitionFetchInfo(0,67108864),[org.mobile_push_netevent,11] -> PartitionFetchInfo(659425,67108864),[org.mobile_networklog,9] -> PartitionFetchInfo(1251021,67108864),[org.data_resys_combinerec,11] -> PartitionFetchInfo(21870,67108864),[org.paytobiznotify,9] -> PartitionFetchInfo(0,67108864),[org.mobile_push_applist,12] -> PartitionFetchInfo(3099746,67108864),[org.mtrace,7] -> PartitionFetchInfo(1190072,67108864),[org.mobile_nginx,0] -> PartitionFetchInfo(174905,67108864),[org.web_user_message_op,8] -> PartitionFetchInfo(4529,67108864),[binlog.coupon,12] -> PartitionFetchInfo(23996,67108864),[org.tmplog,15] -> PartitionFetchInfo(0,67108864),[org.paylog,1] -> PartitionFetchInfo(489,67108864),[org.data_resys_routerres,7] -> PartitionFetchInfo(10502,67108864),[binlog.coupon,0] -> PartitionFetchInfo(25215,67108864),[org.nginxerrorlog,9] -> PartitionFetchInfo(29483,67108864),[org.mobile_push_applist,11] -> PartitionFetchInfo(103347,67108864),[org.basync,7] -> PartitionFetchInfo(299174,67108864),[org.mobile_push_netevent,0] -> PartitionFetchInfo(12720799,67108864),[org.basync,5] -> PartitionFetchInfo(4184,67108864),[org.mobile_nginx,12] -> PartitionFetchInfo(181939,67108864),[org.cos_errorlog,0] -> PartitionFetchInfo(189,67108864),[org.nginxerrorlog,15] -> PartitionFetchInfo(27555,67108864),[org.captcha,4] -> PartitionFetchInfo(14233,67108864),[org.data_search_smartbox,11] -> PartitionFetchInfo(0,67108864),[org.mobile_push_applist,2] -> PartitionFetchInfo(2371719,67108864),[org.mobile,5] -> PartitionFetchInfo(1906756,67108864),[org.mobile_movielog,3] -> PartitionFetchInfo(126780,67108864),[org.mobile_push_netevent,8] -> PartitionFetchInfo(10731244,67108864),[org.nginxerrorlog,14] -> PartitionFetchInfo(1525,67108864),[org.bdecomaction,2] -> PartitionFetchInfo(0,67108864),[org.cos_errorlog,12] -> PartitionFetchInfo(123,67108864),[org.mobile_networklog,2] -> PartitionFetchInfo(62332,67108864),[org.mobile_xmlog,8] -> PartitionFetchInfo(478,67108864),[org.mobile_eventlog,15] -> PartitionFetchInfo(8429,67108864),[org.cos_errorlog,10] -> PartitionFetchInfo(2569,67108864),[org.eventlog,4] -> PartitionFetchInfo(935,67108864),[org.data_asyncquerier_sql,14] -> PartitionFetchInfo(0,67108864),[org.dealrank,12] -> PartitionFetchInfo(93,67108864),[org.search_queryserver,1] -> PartitionFetchInfo(0,67108864),[binlog.user,5] -> PartitionFetchInfo(5950,67108864),[org.mobile_group_push,9] -> PartitionFetchInfo(25002,67108864),[org.mobile,13] -> PartitionFetchInfo(2117661,67108864),[org.nginx,11] -> PartitionFetchInfo(140093,67108864),[org.mobile_push_applist,7] -> PartitionFetchInfo(171384,67108864),[org.mobile_push_netevent,13] -> PartitionFetchInfo(531815,67108864),[org.mobile_eventlog,3] -> PartitionFetchInfo(14430,67108864),[org.captcha,6] -> PartitionFetchInfo(88,67108864),[org.mobile_push_netevent,5] -> PartitionFetchInfo(460960,67108864),[org.mobile_order_nomsg,4] -> PartitionFetchInfo(3,67108864),[org.data_xmltableview_access,2] -> PartitionFetchInfo(7,67108864),[org.payrequest,13] -> PartitionFetchInfo(101,67108864),[org.mtrace,8] -> Partiti
...
4),[org.mobile_mars_report,10] -> PartitionFetchInfo(2926,67108864),[org.mobile,12] -> PartitionFetchInfo(55550,67108864),[org.mobile_movielog,2] -> PartitionFetchInfo(1519,67108864),[org.commonsubscribe,3] -> PartitionFetchInfo(0,67108864),[org.apacheerrorlog,14] -> PartitionFetchInfo(1366,67108864),[org.wm_submitorder,13] -> PartitionFetchInfo(3,67108864),[org.data_resys_routerreq,9] -> PartitionFetchInfo(1582,67108864),[org.mobile_push_applist,1] -> PartitionFetchInfo(228829,67108864),[org.mtrace,11] -> PartitionFetchInfo(1232471,67108864),[org.404log,12] -> PartitionFetchInfo(0,67108864),[org.mtsg,4] -> PartitionFetchInfo(4,67108864),[org.applog,4] -> PartitionFetchInfo(607,67108864),[org.mobile_movielog,8] -> PartitionFetchInfo(3223,67108864),[org.mobile_push_netevent,6] -> PartitionFetchInfo(13081128,67108864),[org.cmsdealapi,11] -> PartitionFetchInfo(0,67108864),[org.mobile,2] -> PartitionFetchInfo(53029,67108864),[org.mobilerebindinfo,6] -> PartitionFetchInfo(32,67108864),[org.nginx,3] -> PartitionFetchInfo(105261,67108864),[org.mobile_push_applist,3] -> PartitionFetchInfo(162462,67108864),[org.mobile_xmlog,10] -> PartitionFetchInfo(21754,67108864),[org.mobile,6] -> PartitionFetchInfo(65015,67108864),[org.mobile_movielog,9] -> PartitionFetchInfo(132672,67108864),[org.intranet_nginx,11] -> PartitionFetchInfo(4738,67108864),[binlog.user,0] -> PartitionFetchInfo(225872,67108864) (kafka.server.ReplicaFetcherThread)
java.lang.IllegalArgumentException
at java.nio.Buffer.limit(Buffer.java:267)
at kafka.api.FetchResponsePartitionData$.readFrom(FetchResponse.scala:33)
at kafka.api.TopicData$$anonfun$1.apply(FetchResponse.scala:83)
at kafka.api.TopicData$$anonfun$1.apply(FetchResponse.scala:81)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at scala.collection.immutable.Range.foreach(Range.scala:141)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
at scala.collection.AbstractTraversable.map(Traversable.scala:105)
at kafka.api.TopicData$.readFrom(FetchResponse.scala:81)
at kafka.api.FetchResponse$$anonfun$3.apply(FetchResponse.scala:142)
at kafka.api.FetchResponse$$anonfun$3.apply(FetchResponse.scala:141)
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
at scala.collection.immutable.Range.foreach(Range.scala:141)
at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
at kafka.api.FetchResponse$.readFrom(FetchResponse.scala:141)
at kafka.consumer.SimpleConsumer.fetch(SimpleConsumer.scala:112)
at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:96)
at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:88)
at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51);;;","02/Jul/14 15:44;junrao;You can do one or more of the following to get around this.

1. Increase num.replica.fetchers (http://kafka.apache.org/documentation.html#brokerconfigs)
2. Reduce replica.fetch.max.bytes, but make sure it's still >= message.max.bytes.;;;","03/Jul/14 02:24;dashengju;Thanks for your reply. 
I will try to increase num.replica.fetchers in our production environment.
do you have any plan to fixing the issue?;;;","16/Oct/14 16:55;ewencp;Created reviewboard https://reviews.apache.org/r/26811/diff/
 against branch origin/trunk;;;","16/Oct/14 16:57;ewencp;This is a wip patch to fix this issue, which previous discussion suggests was due to the FetchResponse exceeding 2GB. My approach to triggering the issue, however, doesn't exhibit exactly the same issue but does cause an unrecoverable error that causes the consumer connection to terminate. (For reference, it causes the server to fail when FetchResponseSend.writeTo calls expectIncomplete and sendSize is negative due to overflow. This confuses the server since it looks like the message is already done sending and the server forcibly closes the consumer's connection.)

The patch addresses the core issue by ensuring the returned message doesn't exceed 2GB by dropping parts of it in a way that otherwise shouldn't affect the consumer. But there are a lot of points that still need to be addressed:

* I started by building an integration test to trigger the issue, included in PrimitiveApiTest. However, since we necessarily need to have > 2GB data to trigger the issue, it's probably too expensive to include in this way. Offline discussion suggests maybe a system test would be a better place to include this. It's still included here for completeness.
* The implementation filters to a subset of the data in FetchResponse. The main reason for this is that this process needs to know the exact (or at least conservative estimate) size of serialized data, which only FetchResponse knows. But it's also a bit weird compared to other message classes, which are case classes and don't modify those inputs.
* Algorithm for choosing subset to return: initial approach is to remove random elements until we get below the limit. This is simple to understand and avoids starvation of specific TopicAndPartitions. Any concerns with this basic approach?
* I'm pretty sure I've managed to keep the < 2GB case to effectively the same computational cost (computing the serialized size, grouped data, etc. exactly once as before). However, for the > 2GB case I've only ensured correctness. In particular, the progressive removal and reevaluation of serialized size could potentially be very bad for very large data sets (e.g. starting a mirror maker against a large data set with large # of partitions from scratch).
* Note that the algorithm never deals with the actual message data, only metadata about what messages are available. This is relevant since this is what suggested the approach in the patch could still be performant -- ReplicaManager.readMessageSets processes the entire FetchRequest and filters it down because the metadata involved is relatively small.
* Based on the previous two points, this really needs some more realistic large scale system tests to make sure this approach is not only correct, but provides reasonable performance (or indicates we need to revise the algorithm for selecting a subset of the data).
* Testing isn't really complete -- I triggered the issue with 4 topics * 600 MB/topic, which is > 2GB. Another obvious case to check is when some partitions contain > 2GB on their own.
* I'd like someone to help sanity check the exact maximum FetchResponse serialized size we limit messages to. It's not Int.MaxValue because the FetchResponseSend class adds 4 + FetchResponse.sizeInBytes for it's own size. I'd like a sanity check that the extra 4 bytes is enough -- is there any additional wrapping we might need to account for? Getting a test to hit exactly that narrow range could be tricky.
* The tests include both immediate-response and purgatory paths, but the purgatory version requires a timeout in the test, which could end up being flaky + wasting time, but it doesn't look like there's a great way to mock that right now. Maybe this doesn't matter if it moves to a system test?
* One case this doesn't handle yet is when the data reaches > 2GB after it's in the purgatory. The result is correct, but the response is not sent as soon as that condition is satisfied. This is because it looks like evaluating this exactly would require calling readMessageSets and evaluating the size of the message for every DelayedFetch.isSatisifed call. This sounds like it could end up being pretty expensive. Maybe there's a better way, perhaps an approximate scheme?
* The test requires some extra bytes in the fetchSize for each partition, presumably for overhead in encoding. I haven't tracked down exactly how big that should be, but I'm guessing it could end up affecting the results of more comprehensive tests.;;;","24/Feb/16 05:39;ijuma;[~ewencp], this is marked as ""Blocker"" with a ""Fix Version"" of 0.10.0. Is that really the case?;;;","24/Feb/16 05:55;ewencp;[~ijuma] Pretty sure I didn't set the fix version on this (it's a throwback, and I doubt I would have known what fix version to set back then anyway). Probably was set incorrectly w/ the initial report and can be changed.;;;","26/Sep/16 03:48;becket_qin;This issue should have been resolved by KIP-74 (KAFKA-2063).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data loss if broker is killed using kill -9,KAFKA-1193,12686344,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,hanish.bansal.agarwal,hanish.bansal.agarwal,25/Dec/13 09:49,04/Sep/14 22:02,14/Jul/23 05:37,04/Sep/14 22:02,0.8.0,0.8.1,,0.8.2.0,,,,,,,replication,,,4,,,,"We are having kafka cluster of 2 nodes. (Using Kafka 0.8.0 version)
Replication Factor: 2
Number of partitions: 2

Actual Behaviour:
-------------------------
Out of two nodes, if leader node goes down then data lost happens.

Steps to Reproduce:
------------------------------
1. Create a 2 node kafka cluster with replication factor 2
2. Start the Kafka cluster
3. Create a topic lets say ""test-trunk111""
4. Restart any one node.
5. Check topic status using kafka-list-topic tool.
topic isr status is:

topic: test-trunk111    partition: 0    leader: 0    replicas: 1,0    isr: 0,1
topic: test-trunk111    partition: 1    leader: 0    replicas: 0,1    isr: 0,1

If there is only one broker node in isr list then wait for some time and again check isr status of topic. There should be 2 brokers in isr list.
6. Start producing the data.
7. Kill leader node (borker-0 in our case) meanwhile of data producing.
8. After all data is produced start consumer.
9. Observe the behaviour. There is data loss.

After leader goes down, topic isr status is:

topic: test-trunk111    partition: 0    leader: 1    replicas: 1,0    isr: 1
topic: test-trunk111    partition: 1    leader: 1    replicas: 0,1    isr: 1

We have tried below things to avoid data loss:
----------------------------------------------------------------

1. Configured ""request.required.acks=-1"" in producer configuration because as mentioned in documentation http://kafka.apache.org/documentation.html#producerconfigs, setting this value to -1 provides guarantee that no messages will be lost.
2. Increased the ""message.send.max.retries"" from 3 to 10 in producer configuration.

3. Set ""controlled.shutdown.enable"" to true in broker configuration.

4. Tested with Kafka-0.8.1 after applying patch KAFKA-1188.patch available on https://issues.apache.org/jira/browse/KAFKA-1188 

Nothing work out from above things in case of leader node is killed using ""kill -9 <pid>"".

Expected Behaviour:
----------------------------
No data should be lost.
",Centos 6.3,felixgv,guozhang,hanish.bansal.agarwal,hubez,junrao,nehanarkhede,smeder,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,365327,,,Thu Sep 04 22:01:57 UTC 2014,,,,,,,,,,"0|i1qywn:",365629,,,,,,,,,,,,,,,,,,,,"28/Dec/13 16:01;junrao;Do you see the following in the controller log? This indicates an unclean leader election and could cause data loss.

""No broker in ISR is alive for ... There's potential data loss."";;;","28/Dec/13 16:46;hanish.bansal.agarwal;If i don't perform 5th step (i.e. If there is only one broker node in isr list then wait for some time and again check isr status of topic. There should be 2 brokers in isr list.) listed properly then i am able to see logs like:
{quote}
[2013-12-23 10:25:07,648] DEBUG [OfflinePartitionLeaderSelector]: No broker in ISR is alive for [test-trunk111,1]. Pick the leader from the alive assigned replicas: 1 (kafka.controller.OfflinePartitionLeaderSelector)
[2013-12-23 10:25:07,648] WARN [OfflinePartitionLeaderSelector]: No broker in ISR is alive for [test-trunk111,1]. Elect leader 1 from live brokers 1. There's potential data loss. (kafka.controller.OfflinePartitionLeaderSelector)
[2013-12-23 10:25:07,649] INFO [OfflinePartitionLeaderSelector]: Selected new leader and ISR {""leader"":1,""leader_epoch"":1,""isr"":[1]} for offline partition [test-trunk111,1] (kafka.controller.OfflinePartitionLeaderSelector)
{quote}

In this case where only one broker is in isr list i experienced 50-60 % data loss where is the case where both 2 brokers are in isr list i experienced only 2-3 % data loss.
;;;","02/Jan/14 19:25;guozhang;After thinking about this I am still leaning towards this being caused by KAFKA-1188, and probably the previous patch does not fully resolved the issue.

[~hanish.bansal.agarwal] Could you retry this procedure after we have fully tested the patch and commit it to trunk?;;;","02/Jan/14 19:31;guozhang;Hanish, one thing I was wondering is do you always see data loss with step 5 performed or do you see data loss from time to time? And also what is your producer's throughput?;;;","06/Jan/14 10:30;hanish.bansal.agarwal;Once 1188 is fixed We can retest that it is reproducible or not.

Yes, we always see data loss if broker is killed using kill -9.

Producer's throughput is around 800 records/sec. ;;;","27/Mar/14 12:16;nehanarkhede;[~hanish.bansal.agarwal] Would you mind repeating your test since KAFKA-1188 is checked in.;;;","10/May/14 05:45;hanish.bansal.agarwal;Neha, i was quite busy and couldn't get time to test this on 0.8.1. Now i tested this bug on 0.8.1.1 and its working fine. 

Thanks.;;;","04/Sep/14 22:01;guozhang;[~hanish.bansal.agarwal] I am closing this ticket now. Please feel free to re-open it if you observed this issue again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable DumpLogSegments tool to deserialize messages,KAFKA-1192,12686328,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,omkreddy,guozhang,guozhang,24/Dec/13 22:01,24/Jul/14 00:10,14/Jul/23 05:39,24/Jul/14 00:10,,,,0.8.2.0,,,,,,,tools,,,0,newbie,,,"Currently the DumpLogSegments tool reads the message payloads as strings by default, which will not display the messages correctly if the messages are deserialized with another class. By enablding deserialization with a customized class we can use this tool to debug more issues where I need to read the message content.",,guozhang,junrao,omkreddy,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/14 15:17;omkreddy;KAFKA-1192_2014-07-21_20:44:08.patch;https://issues.apache.org/jira/secure/attachment/12656874/KAFKA-1192_2014-07-21_20%3A44%3A08.patch","23/Jul/14 14:56;omkreddy;KAFKA-1192_2014-07-23_20:23:08.patch;https://issues.apache.org/jira/secure/attachment/12657362/KAFKA-1192_2014-07-23_20%3A23%3A08.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,365308,,,Thu Jul 24 00:10:49 UTC 2014,,,,,,,,,,"0|i1qysn:",365611,,,,,,,,,,,,,,,,,,,,"18/Jul/14 04:58;omkreddy;Some points

1. Users need to provide customized Decoder class. This is used for deserialization of messages. If no Decoder is provided, default StringDecoder will be used. Customized Decoder jar should be available in kafka/libs directory.


2. Initially I thought of using kafka.serializer.Decoder trait for Decoder classes.
    But this class is parameterized with type. 
    How to instantiate this class inside DumpLogSegments without knowing type a priori? User will pass only implementation class name.


3. Can we create new Decoder trait, which takes byte array and returns Object.?
   User can implement this trait and decode their object. DumpLogSegments will
   print the toString() of the Object.
   
  {code}
   kafka.tools
    trait Decoder {
       def fromBytes(bytes: Array[Byte]]): Object
    }

    class StringDecoder() extends Decoder {
        def fromBytes(bytes: Array[Byte]): Object = {  
             new String(bytes, ""UTF-8"")
       }
     }
  
     
    class MyDecoder() extends Decoder {
        def fromBytes(bytes: Array[Byte]): Object = {  
             //
             //
             //
       }
     }
   {code}


4.  Topics can have different types of messages. So it the responsibility of the decoder class to decode the different types of messages of same topic.;;;","19/Jul/14 08:17;omkreddy;Created reviewboard https://reviews.apache.org/r/23705/diff/
 against branch origin/trunk;;;","19/Jul/14 08:29;omkreddy;Updated reviewboard https://reviews.apache.org/r/23705/diff/
 against branch origin/trunk;;;","19/Jul/14 08:31;omkreddy;Introduced new command-line parameter ""decoder-class"" to pass customized Decoder class.  This class should implement kafka.serializer.Decoder trait.;;;","21/Jul/14 15:17;omkreddy;Updated reviewboard https://reviews.apache.org/r/23705/diff/
 against branch origin/trunk;;;","21/Jul/14 15:24;omkreddy;Introduced new command-line parameter ""key-decoder-class"" to pass customized Decoder class for keys. This class should implement kafka.serializer.Decoder trait.;;;","21/Jul/14 17:28;guozhang;Assign back to Guozhang for review.;;;","23/Jul/14 14:56;omkreddy;Updated reviewboard https://reviews.apache.org/r/23705/diff/
 against branch origin/trunk;;;","24/Jul/14 00:10;junrao;Thanks for the patch. +1. Committed to trunk with the following minor change.

* change the property name ""decoder-class"" to ""value-decoder-class"";;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-server-stop.sh doesn't stop broker,KAFKA-1189,12685597,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,martinkl,bbaugher,bbaugher,19/Dec/13 14:24,26/Jan/16 14:16,14/Jul/23 05:39,06/Mar/14 17:48,0.8.0,,,0.8.2.0,,,,,,,tools,,,0,newbie,,,"Just before the 0.8.0 release this commit[1] changed the signal in the kafka-server-stop.sh script from SIGTERM to SIGINT. This doesn't seem to stop the broker. Changing this back to SIGTERM (or -15) fixes the issues.

[1] - https://github.com/apache/kafka/commit/51de7c55d2b3107b79953f401fc8c9530bd0eea0","RHEL 6.4 64bit, Java 6u35",bbaugher,ijuma,linwukang,martinkl,nehanarkhede,noslowerdna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 15:26;martinkl;KAFKA-1189.patch;https://issues.apache.org/jira/secure/attachment/12633158/KAFKA-1189.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,364672,,,Tue Jan 26 14:16:50 UTC 2016,,,,,,,,,,"0|i1quuv:",364972,,,,,,,,,,,,,,,,,,,,"06/Mar/14 15:26;martinkl;Created reviewboard https://reviews.apache.org/r/18846/
 against branch trunk;;;","06/Mar/14 15:30;martinkl;This happens if the broker is started with {{./bin/kafka-server-start.sh -daemon config/server.properties}} — it seems that {{nohup}} swallows the SIGINT signal. Changing the shutdown script to SIGTERM fixes the problem.;;;","06/Mar/14 17:48;nehanarkhede;Committed to trunk
;;;","26/Jan/16 06:57;linwukang;It seem that, this script cannot work in CentOS 6.4.
Command ""ps ax | grep -i 'kafka\.Kafka'""  return NOTHING so that kafka will not be killed.
-----------------------------------------------------------------------------------------------
[root@node63 bin]# lsb_release -a
LSB Version:	:base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
Distributor ID:	CentOS
Description:	CentOS release 6.4 (Final)
Release:	6.4
Codename:	Final
[root@node63 bin]# jps
8194 HRegionServer
3250 JournalNode
3612 NameNode
1915 QuorumPeerMain
10833 Master
6573 HMaster
4132 DFSZKFailoverController
13891 Worker
18524 Kafka
8073 ZooKeeperMain
4282 DataNode
4790 ResourceManager
4911 NodeManager
18903 Jps
[root@node63 bin]# ps ax | grep -i 'kafka\.Kafka'
[root@node63 bin]# ;;;","26/Jan/16 14:16;ijuma;[~linwukang], please file a new issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stale LeaderAndIsr request could be handled by the broker on Controller failover,KAFKA-1188,12685440,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,18/Dec/13 19:31,13/Feb/14 22:47,14/Jul/23 05:39,13/Feb/14 22:47,,,,,,,,,,,,,,1,,,,"... which will cause the broker to truncate log as responding in makeFollower, and hence lose data. 

One procedure to produce this issue:

3 brokers, 3 partitions, replication factor = 3.

1. Broker 1 is the original controller.
2. Broker 3 lost registration, 1 send LeaderAndIsr to 1 for isr shrinking  (leader of partition 1), but not to 2 since it is not the leader.
3. Broker 1 dies, new controller 2 sends all LeaderAndIsr to 2 and 3 (resumed), and 2 handles the makeFollower, truncates data.",,guozhang,hubez,mazhar.shaikh.in,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1211,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/13 03:01;guozhang;KAFKA-1188.patch;https://issues.apache.org/jira/secure/attachment/12619475/KAFKA-1188.patch","24/Dec/13 19:08;guozhang;KAFKA-1188_2013-12-24_11:07:12.patch;https://issues.apache.org/jira/secure/attachment/12620380/KAFKA-1188_2013-12-24_11%3A07%3A12.patch","02/Jan/14 18:44;guozhang;KAFKA-1188_2014-01-02_10:43:54.patch;https://issues.apache.org/jira/secure/attachment/12621127/KAFKA-1188_2014-01-02_10%3A43%3A54.patch","16/Jan/14 00:43;guozhang;KAFKA-1188_2014-01-15_16:43:30.patch;https://issues.apache.org/jira/secure/attachment/12623272/KAFKA-1188_2014-01-15_16%3A43%3A30.patch","16/Jan/14 19:07;guozhang;KAFKA-1188_2014-01-16_11:06:56.patch;https://issues.apache.org/jira/secure/attachment/12623444/KAFKA-1188_2014-01-16_11%3A06%3A56.patch","16/Jan/14 23:02;guozhang;KAFKA-1188_2014-01-16_15:02:22.patch;https://issues.apache.org/jira/secure/attachment/12623522/KAFKA-1188_2014-01-16_15%3A02%3A22.patch","21/Jan/14 22:01;guozhang;KAFKA-1188_2014-01-21_14:01:22.patch;https://issues.apache.org/jira/secure/attachment/12624196/KAFKA-1188_2014-01-21_14%3A01%3A22.patch","12/Feb/14 01:48;guozhang;KAFKA-1188_2014-02-11_17:48:06.patch;https://issues.apache.org/jira/secure/attachment/12628407/KAFKA-1188_2014-02-11_17%3A48%3A06.patch","12/Feb/14 01:50;guozhang;KAFKA-1188_2014-02-11_17:50:06.patch;https://issues.apache.org/jira/secure/attachment/12628408/KAFKA-1188_2014-02-11_17%3A50%3A06.patch",,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,364517,,,Thu Feb 13 22:47:42 UTC 2014,,,,,,,,,,"0|i1qtwn:",364817,,,,,,,,,,,,,,,,,,,,"19/Dec/13 03:01;guozhang;Created reviewboard https://reviews.apache.org/r/16360/
 against branch origin/trunk;;;","24/Dec/13 19:08;guozhang;Updated reviewboard https://reviews.apache.org/r/16360/
 against branch origin/trunk;;;","02/Jan/14 18:44;guozhang;Updated reviewboard https://reviews.apache.org/r/16360/
 against branch origin/trunk;;;","16/Jan/14 00:43;guozhang;Updated reviewboard https://reviews.apache.org/r/16360/
 against branch origin/trunk;;;","16/Jan/14 19:07;guozhang;Updated reviewboard https://reviews.apache.org/r/16360/
 against branch origin/trunk;;;","16/Jan/14 23:02;guozhang;Updated reviewboard https://reviews.apache.org/r/16360/
 against branch origin/trunk;;;","21/Jan/14 22:01;guozhang;Updated reviewboard https://reviews.apache.org/r/16360/
 against branch origin/trunk;;;","12/Feb/14 01:48;guozhang;Updated reviewboard  against branch origin/trunk;;;","12/Feb/14 01:50;guozhang;Updated reviewboard  against branch origin/trunk;;;","13/Feb/14 22:47;nehanarkhede;Fixed the minor suggestions and checked in.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller should retry connecting to brokers to send state change requests,KAFKA-1187,12685272,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,17/Dec/13 23:14,03/Jan/14 21:06,14/Jul/23 05:39,03/Jan/14 21:06,0.8.1,,,,,,,,,,controller,,,0,,,,"When brokers startup, controller tries connecting to the brokers. If the broker is temporarily in a gc pause or there is a network issue, this connection might fail. However, the controller will try sending state change requests that will fail since the connection was not successful",,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/13 00:29;nehanarkhede;KAFKA-1187.patch;https://issues.apache.org/jira/secure/attachment/12619203/KAFKA-1187.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,364349,,,Fri Jan 03 21:06:02 UTC 2014,,,,,,,,,,"0|i1qsvb:",364649,,,,,,,,,,,,,,,,,,,,"18/Dec/13 00:29;nehanarkhede;Created reviewboard https://reviews.apache.org/r/16335/
 against branch trunk;;;","03/Jan/14 21:06;nehanarkhede;Thanks for the reviews, committed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WhiteList topic filter gets a NullPointerException on complex Regex,KAFKA-1180,12684311,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,joestein,jbrosenberg@gmail.com,jbrosenberg@gmail.com,12/Dec/13 18:00,20/Jul/14 01:00,14/Jul/23 05:39,20/Jul/14 01:00,0.8.0,,,0.8.2.0,,,,,,,consumer,,,0,,,,"We are needing to create a stream selector that essentially combines the logic of the BlackList and WhiteList classes (which is not easily exposed in the high-level consumer api).  That is, we want to select a topic that contains a certain prefix, as long as it doesn't also contain a secondary string.

This should be easy to do with ordinary java Regex's, but we're running into some issues, trying to do this with the WhiteList class only.

We have a pattern that uses negative lookahead, like this:

""test-(?!bad\\b)[\\w]+""

So this should select a topic like: ""test-good"", but exclude a topic like ""test-bad"", and also exclude a topic without the ""test"" prefix, like ""foo-bar"".

Instead, what we see is a NullPointerException in the call to createMessageStreamsByFilter (after having previously sent a message to ""test-good"" followed by a message to ""test-bad""):

21700 [ConsumerFetcherThread-group1_square-1a7ac0.local-1386869343370-dc19c7dc-0-1946108683] ERROR kafka.consumer.ConsumerFetcherThread  - [ConsumerFetcherThread-group1_square-1a7ac0.local-1386869343370-dc19c7dc-0-1946108683], Error due to 
kafka.common.KafkaException: error processing data for partition [test-bad,0] offset 0
	at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1$$anonfun$apply$mcV$sp$2.apply(AbstractFetcherThread.scala:137)
	at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1$$anonfun$apply$mcV$sp$2.apply(AbstractFetcherThread.scala:109)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
	at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1.apply$mcV$sp(AbstractFetcherThread.scala:109)
	at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1.apply(AbstractFetcherThread.scala:109)
	at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1.apply(AbstractFetcherThread.scala:109)
	at kafka.utils.Utils$.inLock(Utils.scala:565)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:108)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:86)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)
Caused by: java.lang.NullPointerException
	at kafka.consumer.PartitionTopicInfo.enqueue(PartitionTopicInfo.scala:60)
	at kafka.consumer.ConsumerFetcherThread.processPartitionData(ConsumerFetcherThread.scala:49)
	at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1$$anonfun$apply$mcV$sp$2.apply(AbstractFetcherThread.scala:128)
	... 9 more",,charmalloc,guozhang,jbrosenberg@gmail.com,joestein,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/14 01:29;charmalloc;KAFKA-1180.patch;https://issues.apache.org/jira/secure/attachment/12621898/KAFKA-1180.patch","20/Dec/13 18:08;charmalloc;KAFKA-1180.patch;https://issues.apache.org/jira/secure/attachment/12619873/KAFKA-1180.patch","22/Dec/13 06:25;charmalloc;KAFKA-1180_2013-12-22_01:24:57.patch;https://issues.apache.org/jira/secure/attachment/12620067/KAFKA-1180_2013-12-22_01%3A24%3A57.patch","10/Feb/14 01:22;charmalloc;KAFKA-1180_2014-02-09_20:21:49.patch;https://issues.apache.org/jira/secure/attachment/12627907/KAFKA-1180_2014-02-09_20%3A21%3A49.patch","13/Feb/14 19:50;charmalloc;KAFKA-1180_2014-02-13_14:50:40.patch;https://issues.apache.org/jira/secure/attachment/12628831/KAFKA-1180_2014-02-13_14%3A50%3A40.patch","13/Feb/14 20:13;charmalloc;KAFKA-1180_2014-02-13_15:13:17.patch;https://issues.apache.org/jira/secure/attachment/12628836/KAFKA-1180_2014-02-13_15%3A13%3A17.patch","13/Feb/14 20:22;charmalloc;KAFKA-1180_2014-02-13_15:21:51.patch;https://issues.apache.org/jira/secure/attachment/12628839/KAFKA-1180_2014-02-13_15%3A21%3A51.patch","13/Feb/14 20:24;charmalloc;KAFKA-1180_2014-02-13_15:23:28.patch;https://issues.apache.org/jira/secure/attachment/12628840/KAFKA-1180_2014-02-13_15%3A23%3A28.patch","14/Jul/14 01:13;jbrosenberg@gmail.com;apply-patch-1180-to-0.8.1.patch;https://issues.apache.org/jira/secure/attachment/12655482/apply-patch-1180-to-0.8.1.patch",,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,363383,,,Sun Jul 20 01:00:07 UTC 2014,,,,,,,,,,"0|i1qmz3:",363689,,,,,,,,,,,,,,,,,,,,"12/Dec/13 23:42;jbrosenberg@gmail.com;Here's some code which reproduces the issue.  Assume zkConnect points to a running zk cluster (and there's also a running kafka instance using the same zkConnect), and also that kafka is running  on localhost, and using using 'metadataport':

    List<KeyedMessage<Integer, String>> data =  ImmutableList.of(
            new KeyedMessage<Integer, String>(""test-topic"", ""test-message1""),
            new KeyedMessage<Integer, String>(""test-bad"", ""test-message2"")),
    String regex =  ""test-(?!bad\\b)[\\w]+"",

    Properties pProps = new Properties();
    pProps.put(""metadata.broker.list"", ""localhost:"" + metadataport);
    pProps.put(""serializer.class"", ""kafka.serializer.StringEncoder"");
    ProducerConfig pConfig = new ProducerConfig(pProps);
    Producer<Integer, String> producer = new Producer<Integer, String>(pConfig);
    for (KeyedMessage<Integer, String> data : toSend) {
      System.out.println(""write"");
      producer.send(data);
    }
    producer.close();

    Properties cProps = new Properties();
    cProps.put(""zookeeper.connect"", zkConnect);
    cProps.put(""group.id"", ""group1"");
    cProps.put(""auto.offset.reset"", OffsetRequest.SmallestTimeString());
    ConsumerConfig consumerConfig = new ConsumerConfig(cProps);
    ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(consumerConfig);

    List<KafkaStream<byte[], byte[]>> streams =
        consumerConnector.createMessageStreamsByFilter(new Whitelist(regex), 1);
    System.out.println(""create streams"");

;;;","17/Dec/13 14:43;joestein;That code is all in TopicFilter, can you writeup a StreamSelector that extends TopicFilter(rawRegex) or another TopicFilter?  

case class Whitelist(rawRegex: String) extends TopicFilter(rawRegex) {
  override def requiresTopicEventWatcher = !regex.matches(""""""[\p{Alnum}-|]+"""""")

  override def isTopicAllowed(topic: String) = {
    val allowed = topic.matches(regex)

    debug(""%s %s"".format(
      topic, if (allowed) ""allowed"" else ""filtered""))

    allowed
  }


}

case class Blacklist(rawRegex: String) extends TopicFilter(rawRegex) {
  override def requiresTopicEventWatcher = true

  override def isTopicAllowed(topic: String) = {
    val allowed = !topic.matches(regex)

    debug(""%s %s"".format(
      topic, if (allowed) ""allowed"" else ""filtered""))

    allowed
  }

if can patch I can help it get upstream, might have a chance to get some to-do this in the next week or two ;;;","18/Dec/13 13:14;jbrosenberg@gmail.com;Joe, I'm not sure I understand your suggestion?  We are calling the WhiteList constructor from java.  Unfortunately, it does not look like there is an easy way to sub-class Whitelist (or even TopicFilter) from java.

Anyway, the bottom line, is that that regex I used in the example above causes an NPE, can you reproduce that?  I think that's the main issue in the bug.  Going beyond that, it would be great if WhiteList allowed arbitrary Regex's, that can you negative lookahead, etc., to allow complex patterns.  Or alternatively, a separate filter class, which is more general purpose.

There is some code I'm uncertain about in that WhiteList class (I am really not clear on what the distinction between 'regex' and 'rawRegex' is, and the business around 'requiresTopicEventWatcher'?).;;;","20/Dec/13 18:08;charmalloc;Created reviewboard https://reviews.apache.org/r/16425/
;;;","22/Dec/13 06:23;joestein;reproduced 

[2013-12-22 01:07:15,984] ERROR [ConsumerFetcherThread-console-consumer-55777_Joes-MacBook-Air.local-1387692435196-f52625ba-0-0], Error due to  (kafka.consumer.ConsumerFetcherThread)
kafka.common.KafkaException: error processing data for partition [test-bad,0] offset 0
	at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1$$anonfun$apply$mcV$sp$2.apply(AbstractFetcherThread.scala:137)
	at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1$$anonfun$apply$mcV$sp$2.apply(AbstractFetcherThread.scala:109)
	at scala.collection.immutable.Map$Map4.foreach(Map.scala:180)
	at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1.apply$mcV$sp(AbstractFetcherThread.scala:109)
	at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1.apply(AbstractFetcherThread.scala:109)
	at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1.apply(AbstractFetcherThread.scala:109)
	at kafka.utils.Utils$.inLock(Utils.scala:565)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:108)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:86)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)
Caused by: java.lang.NullPointerException
	at kafka.consumer.PartitionTopicInfo.enqueue(PartitionTopicInfo.scala:60)
	at kafka.consumer.ConsumerFetcherThread.processPartitionData(ConsumerFetcherThread.scala:49)
	at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1$$anonfun$apply$mcV$sp$2.apply(AbstractFetcherThread.scala:128)
	... 9 more
;;;","22/Dec/13 06:25;charmalloc;Updated reviewboard https://reviews.apache.org/r/16425/
;;;","22/Dec/13 12:00;jbrosenberg@gmail.com;So, is this patch applicable to the 0.8 branch?  I assume the fix will be committed, only to trunk / 0.8.1?;;;","22/Dec/13 14:26;joestein;Hi Jason, this patch was developed for you to try out against 0.8 branch, yes.

I would +1 on a 0.8.0.1 release with this fix (and anything else spring up) as it would also resolve the 2.8.0 zero length maven issue too (as that is just build related so two JIRA tickets).  I understand if folks didn't want to switch scala versions in production also along with making other changes in case something went wrong and what was the cause.   2.8.0 should be supported in maven but I still think (need to throw a documentation patch up for this) we should document sbt and maven using scala 2.10 (conflating JIRAs some here, sorry).

I haven't tried the patch yet on 0.8.1 as I wanted to 100% make sure first this resolved your issue and that you didn't hit another issue after this was fixed.

0.8.1 might not be too far away also and with release time and testing maybe 0.8.1 comes out within the same time frame the community would be looking for 0.8.0.1.  I haven't tried the Log Compaction stuff in 0.8.1 yet but it looks really awesome https://cwiki.apache.org/confluence/display/KAFKA/Log+Compaction will do that when I try this patch on 0.8.1 once you confirm it works for you as expected.;;;","02/Jan/14 18:05;jbrosenberg@gmail.com;[~joestein] I've applied this patch against the 0.8 branch, and can confirm that it seems to solve the issue we've been having.  Please go ahead and add it to the 0.8.1 branch.  I'm not sure it makes sense at this point to add a 0.8.0.1 release, but if you did, I would use that (for now, I'll run with my own patched 0.8.0 version).

Thanks!;;;","08/Jan/14 01:28;joestein;The KAFKA-1180_2013-12-22_01:24:57.patch is for the 0.8 branch;;;","08/Jan/14 01:29;charmalloc;Created reviewboard https://reviews.apache.org/r/16718/
 against branch origin/trunk;;;","08/Jan/14 01:31;joestein;Trunk patch = https://issues.apache.org/jira/secure/attachment/12621898/KAFKA-1180.patch;;;","02/Feb/14 20:05;jbrosenberg@gmail.com;Any updates on getting this merged into trunk, in time for 0.8.1?;;;","07/Feb/14 15:00;joestein;I have the changes done requested in the review, I need to dig them out of my branch and upload them for follow-up.  Should be able to-do that before Monday.;;;","10/Feb/14 01:22;charmalloc;Updated reviewboard https://reviews.apache.org/r/16718/
 against branch origin/0.8.1;;;","13/Feb/14 19:50;charmalloc;Updated reviewboard https://reviews.apache.org/r/16718/
 against branch origin/0.8.1;;;","13/Feb/14 19:51;joestein;The last patch uploaded just now is rebased on 0.8.1 and added blacklist test case that was missing too;;;","13/Feb/14 20:13;charmalloc;Updated reviewboard https://reviews.apache.org/r/16718/
 against branch origin/0.8.1;;;","13/Feb/14 20:22;charmalloc;Updated reviewboard https://reviews.apache.org/r/16718/
 against branch origin/0.8.1;;;","13/Feb/14 20:24;charmalloc;Updated reviewboard https://reviews.apache.org/r/16718/
 against branch origin/0.8.1;;;","13/Feb/14 20:26;joestein;version 3,4,5 were some issue with my local branches.  cleaned that up and fixed in version 6 (latest) which is reviewable now

added test cases and comments reviewed by Joel & Neha and missing Blacklist test reviewed by Guozhang;;;","20/Feb/14 14:13;jbrosenberg@gmail.com;is it too late to reconsider for 0.8.1?
will there at least be a patch applicable for 0.8.1?;;;","11/Apr/14 05:51;jbrosenberg@gmail.com;[~joestein] What's the status of this?  Will it be available only in 0.8.2?  If so, will there be a patch we can apply to 0.8.1 and 0.8.1.1?  Will the patch previously supplied for 0.8.0 work with 0.8.1.*?

We are currently using 0.8.0 with your patch.  We'd like to upgrade to 0.8.1, but would need to have the patch available before doing so!

Thanks,

Jason;;;","29/May/14 19:10;jbrosenberg@gmail.com;I see this is marked now for 0.8.2.  Is this confirmed?  What's the timeline for 0.8.2?

Thanks,

Jason;;;","14/Jul/14 01:13;jbrosenberg@gmail.com;I've uploaded a patch that refactors the previous patch, to work for the 0.8.1 branch (and presumably trunk).  I've tested and verified this patch and it's working for 0.8.1.1.;;;","14/Jul/14 16:53;guozhang;LGTM, +1. Patch applied with some line number offsets.;;;","15/Jul/14 02:59;jbrosenberg@gmail.com;anything more to be done to get this across the finish line?;;;","16/Jul/14 16:57;junrao;Joel Koshy, Joe Stein,

Do you want to review and commit this patch to trunk?;;;","16/Jul/14 17:17;charmalloc;Yes I can do it, sorry I am swamped to-do it at the moment.  Maybe tomorrow/Friday.  I am going to commit the patch that I did that Joel reviewed in rb to trunk when I do.  ;;;","17/Jul/14 17:09;nehanarkhede;[~charmalloc] Is there any more work to be done before we commit this? [~jjkoshy], If you think the patch looks good and if Joe doesn't have time, would you mind committing it?;;;","18/Jul/14 17:11;joestein;[~nehanarkhede] I was going to add to the WhiteList & BlackList test with the specific regex raised which [~jjkoshy] suggested in his comment for +1 the patch and then commit. I do not think committing without that is the worse thing (since the other test case does check for every scenario of escaping) but it does make the change overall more complete and concise which I think is what Joel was getting at.;;;","20/Jul/14 01:00;joestein;committed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
createMessageStreams() in javaapi.ZookeeperConsumerConnector does not throw,KAFKA-1179,12684090,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriharsha,vrischmann,vrischmann,11/Dec/13 19:06,22/Jul/14 14:39,14/Jul/23 05:39,16/May/14 00:36,0.8.0,,,0.8.2.0,,,,,,,consumer,,,0,newbie,usability,,"In kafka.consumer.javaapi.ZookeeperConsumerConnector.scala, the createMessageStreams() directly calls underlying.consume() (line 80)
In kafka.consumer.ZookeeperConsumerConnector.scala, the createMessageStreams() throws an exception if it has been called more than once (line 133). 

The javaapi should throw if it is called more than once, just like the scala api.",,nehanarkhede,noslowerdna,sriharsha,vrischmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/14 02:28;sriharsha;KAFKA-1179-v1.patch;https://issues.apache.org/jira/secure/attachment/12643676/KAFKA-1179-v1.patch","08/May/14 22:06;sriharsha;KAFKA-1179.patch;https://issues.apache.org/jira/secure/attachment/12644021/KAFKA-1179.patch","14/May/14 21:20;sriharsha;KAFKA-1179_2014-05-14_14:19:11.patch;https://issues.apache.org/jira/secure/attachment/12644893/KAFKA-1179_2014-05-14_14%3A19%3A11.patch","14/May/14 21:40;sriharsha;KAFKA-1179_2014-05-14_14:40:11.patch;https://issues.apache.org/jira/secure/attachment/12644897/KAFKA-1179_2014-05-14_14%3A40%3A11.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,363162,,,Fri May 16 00:36:31 UTC 2014,,,,,,,,,,"0|i1qlm7:",363468,,,,,,,,,,,,,,,,,,,,"28/Mar/14 15:37;noslowerdna;Using the Java API, calling createMessageStreams(...) multiple times using the same consumer connector instance appears to cause the consumer to hang in registerConsumerInZK(...), since a fresh timestamp value is generated for the consumer subscription. At a minimum, it should be clearly documented that this is not valid usage of the API.

The following messages are continually logged.
{code}
kafka.utils.ZkUtils$ - I wrote this conflicted ephemeral node [{""version"":1,""subscription"":{""topic"":1},""pattern"":""static"",""timestamp"":""1396019252998""}] at /consumers/test/ids/test_MAC-AO6517-1396019241689-631e0040 a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry
2014-03-28 10:08:39,086 [main] INFO  kafka.utils.ZkUtils$ - conflict in /consumers/test/ids/test_MAC-AO6517-1396019241689-631e0040 data: {""version"":1,""subscription"":{""topic"":1},""pattern"":""static"",""timestamp"":""1396019252998""} stored data: {""version"":1,""subscription"":{""topic"":1},""pattern"":""static"",""timestamp"":""1396019241812""}
{code}

We're using version 0.8.1.;;;","28/Mar/14 15:48;nehanarkhede;[~noslowerdna] It makes sense to throw an exception and clearly document that API. ;;;","08/May/14 22:06;sriharsha;Created reviewboard https://reviews.apache.org/r/21240/diff/
 against branch origin/trunk;;;","14/May/14 21:20;sriharsha;Updated reviewboard  against branch origin/trunk;;;","14/May/14 21:40;sriharsha;Updated reviewboard https://reviews.apache.org/r/21240/diff/
 against branch origin/trunk;;;","16/May/14 00:36;nehanarkhede;Thanks for the patches, pushed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replica fetcher thread dies while becoming follower,KAFKA-1178,12683963,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,11/Dec/13 04:16,11/Dec/13 19:28,14/Jul/23 05:39,11/Dec/13 19:28,0.8.1,,,,,,,,,,replication,,,0,,,,"Replica fetcher thread dies due to 

2013/12/10 17:48:24.845 [ReplicaFetcherThread] [ReplicaFetcherThread--3-500], Error due to 
kafka.common.KafkaException: error processing data for partition [pts-test_foo,1] offset 340
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1$$anonfun$apply$mcV$sp$2.apply(AbstractFetcherThread.scala:139)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1$$anonfun$apply$mcV$sp$2.apply(AbstractFetcherThread.scala:111)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:125)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1.apply$mcV$sp(AbstractFetcherThread.scala:111)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1.apply(AbstractFetcherThread.scala:111)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1.apply(AbstractFetcherThread.scala:111)
        at kafka.utils.Utils$.inLock(Utils.scala:538)
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:110)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:88)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)
Caused by: java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:185)
        at scala.None$.get(Option.scala:183)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:45)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$1$$anonfun$apply$mcV$sp$2.apply(AbstractFetcherThread.scala:130)
        ... 11 more
",,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/13 06:30;nehanarkhede;KAFKA-1178.patch;https://issues.apache.org/jira/secure/attachment/12618193/KAFKA-1178.patch","11/Dec/13 18:30;nehanarkhede;KAFKA-1178_2013-12-11_10:30:19.patch;https://issues.apache.org/jira/secure/attachment/12618274/KAFKA-1178_2013-12-11_10%3A30%3A19.patch","11/Dec/13 19:09;nehanarkhede;KAFKA-1178_2013-12-11_11:09:09.patch;https://issues.apache.org/jira/secure/attachment/12618276/KAFKA-1178_2013-12-11_11%3A09%3A09.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,363035,,,Wed Dec 11 19:28:45 UTC 2013,,,,,,,,,,"0|i1qktz:",363341,,,,,,,,,,,,,,,,,,,,"11/Dec/13 06:30;nehanarkhede;Created reviewboard https://reviews.apache.org/r/16175/
 against branch trunk;;;","11/Dec/13 18:30;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/16175/
 against branch trunk;;;","11/Dec/13 19:09;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/16175/
 against branch trunk;;;","11/Dec/13 19:28;nehanarkhede;Thanks for the reviews, committed the patch to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DeleteTopics gives Successful message even if the specified Topic is not present,KAFKA-1177,12683779,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,siddhesh_toraskar,siddhesh_toraskar,10/Dec/13 06:21,22/Aug/14 00:54,14/Jul/23 05:39,22/Aug/14 00:54,0.8.0,,,,,,,,,,core,,,0,newbie,,,"I am implementating basic functionalities of kafka. I have created some java classes for creating topics , a producer to send messages,a ConsumerGroup to consume those messages and at last,a class for deleting topic. My other functionalities are working normal but my class for deleting topic is not working as required. The problems are:
1. It is displaying 'deletion successful' for the topics not present.
2. If a topic is deleted and then another topic with same name is created, the messages from previous deleted topics are consumed.(This happens till I dont restart the server).
 So, is it required that everytime I delete a topic, I have to restart my kafka server?.Or, there are some other solutions?",Centos 6.3 with jdk 1.6 and apache kafka 0.8,gwenshap,junrao,siddhesh_toraskar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-330,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,362851,,,Fri Aug 22 00:54:21 UTC 2014,,,,,,,,,,"0|i1qjpj:",363157,,,,,,,,,,,,,,,,,,,,"10/Dec/13 16:06;junrao;Deleting topics is not supported yet. We have a jira (KAFKA-330) to track that.;;;","11/Dec/13 08:13;siddhesh_toraskar;Hi Jun,
    I think i didnt do homework before creating this issue. I have gone through kafka-330 , kafka-784& other issues and yes, there are already similar ones with the DeleteTopic. I will have to study on this again.
    Thanks,
    Siddhesh;;;","11/Dec/13 08:14;siddhesh_toraskar;Forgot to mention another thing, Long deleting operations are blocking some other operations in kafka as like electing new leader.;;;","22/Aug/14 00:54;gwenshap;Following KAFKA-1443 and KAFKA-1576, this should be resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ISR can be inconsistent during partition reassignment for low throughput partitions,KAFKA-1170,12683212,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,06/Dec/13 22:18,13/Jan/14 05:06,14/Jul/23 05:39,13/Jan/14 05:06,0.8.1,,,0.8.1,,,,,,,controller,,,0,,,,"During reassignment of partitions, if the reassigned replicas already has the current leader, then the ISR of the partition can have the old replicas. This is particularly true for partitions that don't have further incoming data, since the leader does not remove replicas that have stopped sending fetch requests from the ISR if their log end offsets match (no more data comes in).",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/13 22:49;nehanarkhede;KAFKA-1170.patch;https://issues.apache.org/jira/secure/attachment/12617495/KAFKA-1170.patch","07/Dec/13 00:22;nehanarkhede;KAFKA-1170_2013-12-06_16:22:00.patch;https://issues.apache.org/jira/secure/attachment/12617513/KAFKA-1170_2013-12-06_16%3A22%3A00.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,362464,,,Mon Jan 13 05:06:13 UTC 2014,,,,,,,,,,"0|i1qha7:",362758,,,,,,,,,,,,,,,,,,,,"06/Dec/13 22:49;nehanarkhede;Created reviewboard https://reviews.apache.org/r/16095/
 against branch trunk;;;","07/Dec/13 00:22;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/16095/
 against branch trunk;;;","13/Jan/14 05:06;junrao;already committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
missing synchronization in access to leaderCache in KafkaApis,KAFKA-1169,12682933,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,05/Dec/13 16:04,05/Dec/13 17:37,14/Jul/23 05:39,05/Dec/13 17:37,,,,0.8.1,,,,,,,core,,,0,,,,,,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/13 16:18;junrao;KAFKA-1169.patch;https://issues.apache.org/jira/secure/attachment/12617181/KAFKA-1169.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,362190,,,Thu Dec 05 17:37:50 UTC 2013,,,,,,,,,,"0|i1qflj:",362485,,,,,,,,,,,,,,,,,,,,"05/Dec/13 16:18;junrao;Created reviewboard  against branch origin/trunk;;;","05/Dec/13 16:22;junrao;Got the following error when creating the RB. Not sure what happened.

python kafka-patch-review.py -b origin/trunk -j KAFKA-1169 -s ""missing synchronization in access to leaderCache in KafkaApis""
Configuring reviewboard url to https://reviews.apache.org
Updating your remote branches to pull the latest changes
Traceback (most recent call last):
  File ""/usr/local/bin/post-review"", line 8, in <module>
    load_entry_point('RBTools==0.5.2', 'console_scripts', 'post-review')()
  File ""build/bdist.macosx-10.8-intel/egg/rbtools/postreview.py"", line 1372, in main
  File ""build/bdist.macosx-10.8-intel/egg/rbtools/postreview.py"", line 983, in tempt_fate
  File ""build/bdist.macosx-10.8-intel/egg/rbtools/postreview.py"", line 623, in publish
  File ""build/bdist.macosx-10.8-intel/egg/rbtools/postreview.py"", line 829, in api_put
  File ""build/bdist.macosx-10.8-intel/egg/rbtools/postreview.py"", line 662, in process_error
rbtools.api.errors.APIError: HTTP 502
Creating diff against origin/trunk and uploading patch to JIRA KAFKA-1169
Created a new reviewboard  
;;;","05/Dec/13 16:24;junrao;RB is created though:

https://reviews.apache.org/r/16040/;;;","05/Dec/13 17:11;nehanarkhede;That's probably because Apache reviewboard was down for sometime in the morning.;;;","05/Dec/13 17:37;junrao;Thanks for the review. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OfflinePartitionCount in JMX can be incorrect during controlled shutdown,KAFKA-1168,12682792,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,04/Dec/13 23:34,09/Dec/13 19:02,14/Jul/23 05:39,09/Dec/13 19:02,,,,0.8.1,,,,,,,,,,0,,,,,,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/13 23:42;junrao;KAFKA-1168.patch;https://issues.apache.org/jira/secure/attachment/12617077/KAFKA-1168.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,362049,,,Wed Dec 04 23:42:44 UTC 2013,,,,,,,,,,"0|i1qeq7:",362344,,,,,,,,,,,,,,,,,,,,"04/Dec/13 23:42;junrao;Created reviewboard https://reviews.apache.org/r/16022/
 against branch origin/trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka should depend on snappy 1.0.5 (instead of 1.0.4.1),KAFKA-1164,12682596,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jbrosenberg@gmail.com,jbrosenberg@gmail.com,jbrosenberg@gmail.com,04/Dec/13 02:23,04/Feb/14 05:25,14/Jul/23 05:39,04/Feb/14 05:17,0.8.0,,,0.8.1,,,,,,,,,,0,release,,,"There is a bug in snappy 1.0.4.1, that makes it not work when using java 7, on MacOSX.  This issue is fixed in snappy 1.0.5.  We've confirmed this locally.

https://github.com/ptaoussanis/carmine/issues/5

So, the kafka distribution should update the KafkaProject.scala file to call out version 1.0.5 instead of 1.0.4.1.  I believe this file is used when generating the pom.xml file for kafka.",,jbrosenberg@gmail.com,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/14 20:04;jbrosenberg@gmail.com;kafka-1164.patch;https://issues.apache.org/jira/secure/attachment/12626553/kafka-1164.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,361853,,,Tue Feb 04 05:17:28 UTC 2014,,,,,,,,,,"0|i1qdin:",362148,,,,,,,,,,,,,,,,,,,,"02/Feb/14 19:14;jbrosenberg@gmail.com;Can this be reconsidered for 0.8.1?  Seems very strange not to include this (kafka currently is broken on the macintosh, without it).;;;","02/Feb/14 19:29;nehanarkhede;Sure, would you like to submit a patch?;;;","02/Feb/14 20:03;jbrosenberg@gmail.com;Here's a patch, which updates the pom dependency for snappy to 1.0.5.  I tested this, by doing './sbt make-pom', and then inspecting the file:

{code}
core/target/scala-2.8.0/kafka_2.8.0-0.8.1.pom
{code}

I actually am not sure how the actual maven artifact pom.xml is generated, but I am guessing this should indicate that it will do the right thing?

I updated 4 files that mentioned snappy 1.0.4.1 (and updated to 1.0.5):

{code}
bin/windows/kafka-run-class.bat
core/build.sbt
project/build/KafkaProject.scala
system_test/migration_tool_testsuite/0.7/bin/kafka-run-class.sh
{code}

Admittedly, I was not able to test each of these use cases, such as the windows kafka-run-class.bat method, or the migration_tool_testsuite.;;;","02/Feb/14 20:04;jbrosenberg@gmail.com;Here's the actual patch file (not sure why the jira 'submit patch' interface didn't offer me the chance to upload a patch :));;;","04/Feb/14 05:17;nehanarkhede;+1. Thanks for the patch;;;","04/Feb/14 05:17;nehanarkhede;Committed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up Per-topic Configuration from Kafka properties,KAFKA-1157,12682269,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,03/Dec/13 00:16,03/Dec/13 05:55,14/Jul/23 05:39,03/Dec/13 05:55,,,,0.8.1,,,,,,,,,,0,,,,"After KAFKA-554, per-topic configurations could be removed from kafka properties.",,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/13 00:22;guozhang;KAFKA-1157.patch;https://issues.apache.org/jira/secure/attachment/12616653/KAFKA-1157.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,361526,,,Tue Dec 03 05:55:17 UTC 2013,,,,,,,,,,"0|i1qbjb:",361824,,,,,,,,,,,,,,,,,,,,"03/Dec/13 00:22;guozhang;Created reviewboard https://reviews.apache.org/r/15950/
 against branch origin/trunk;;;","03/Dec/13 05:55;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve reassignment tool to output the existing assignment to facilitate rollbacks,KAFKA-1156,12682233,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,02/Dec/13 21:58,13/Jan/14 05:07,14/Jul/23 05:39,13/Jan/14 05:07,0.8.1,,,0.8.1,,,,,,,tools,,,0,,,,It is useful for the partition reassignment tool to output the current partition assignment as part of the dry run. This will make rollbacks easier if the reassignment does not work out.,,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/13 14:35;nehanarkhede;KAFKA-1156.patch;https://issues.apache.org/jira/secure/attachment/12616785/KAFKA-1156.patch","03/Dec/13 18:21;nehanarkhede;KAFKA-1156_2013-12-03_10:20:57.patch;https://issues.apache.org/jira/secure/attachment/12616824/KAFKA-1156_2013-12-03_10%3A20%3A57.patch","04/Dec/13 00:20;nehanarkhede;KAFKA-1156_2013-12-03_16:20:10.patch;https://issues.apache.org/jira/secure/attachment/12616889/KAFKA-1156_2013-12-03_16%3A20%3A10.patch","04/Dec/13 00:22;nehanarkhede;KAFKA-1156_2013-12-03_16:22:21.patch;https://issues.apache.org/jira/secure/attachment/12616890/KAFKA-1156_2013-12-03_16%3A22%3A21.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,361490,,,Mon Jan 13 05:07:43 UTC 2014,,,,,,,,,,"0|i1qbb3:",361788,,,,,,,,,,,,,,,,,,,,"03/Dec/13 14:35;nehanarkhede;Created reviewboard https://reviews.apache.org/r/15964/
 against branch trunk;;;","03/Dec/13 18:21;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/15964/
 against branch trunk;;;","04/Dec/13 00:20;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/15964/
 against branch trunk;;;","04/Dec/13 00:22;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/15964/
 against branch trunk;;;","13/Jan/14 05:07;junrao;already committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
replicas may not have consistent data after becoming follower,KAFKA-1154,12682043,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,01/Dec/13 23:29,02/Dec/13 21:58,14/Jul/23 05:39,02/Dec/13 21:58,0.8.1,,,0.8.1,,,,,,,core,,,0,,,,"This is an issued introduced in KAFKA-1001. The issue is that in ReplicaManager.makeFollowers(), we truncate the log before marking the replica as the follower. New messages from the producer can still be added to the log after the log is truncated, but before the replica is marked as the follower. Those newly produced messages can actually be committed, which implies those truncated messages are also committed. However, the new leader is not guaranteed to have those truncated messages.",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/13 23:33;junrao;KAFKA-1154.patch;https://issues.apache.org/jira/secure/attachment/12616491/KAFKA-1154.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,361300,,,Mon Dec 02 21:58:10 UTC 2013,,,,,,,,,,"0|i1qa53:",361599,,,,,,,,,,,,,,,,,,,,"01/Dec/13 23:32;junrao;The fix is to switch the ordering in ReplicaManager.makeFollowers(). We should first mark the replica as the follower before truncating the log. This will ensure that after the log is truncated, no new messages from the produce requests will be added to the log.;;;","01/Dec/13 23:33;junrao;Created reviewboard https://reviews.apache.org/r/15938/
 against branch origin/trunk;;;","01/Dec/13 23:38;junrao;Attach a patch. Also made a minor change in KafkaApis to only expose data up to the high watermark to the debugger consumer. Data after the high watermark can be truncated and therefore could change after the consumer has read it. This fix will prevent the ReplicaVerificationTool to read stale data.;;;","02/Dec/13 16:48;nehanarkhede;We used to do this, looks like this was a regression introduced in KAFKA-1001. 

1. ReplicaManager
Do we still need this ""TODO: the above may need to be fixed later"" ?

2. We had added the ability for a special consumer to read the replica log for troubleshooting. This patch takes that convenience away. We should probably look for another way to prevent the replica verification tool from giving false negatives. Can it use a different consumer id?
;;;","02/Dec/13 21:58;junrao;Thanks for the review. Committed to trunk after addressing the minor review comments.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
typos in documentation,KAFKA-1153,12681917,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,charmalloc,joestein,joestein,29/Nov/13 16:22,02/Dec/13 14:25,14/Jul/23 05:39,30/Nov/13 01:24,,,,,,,,,,,,,,0,,,,"Dan Hoffman hoffmandan@gmail.com via kafka.apache.org 
9:45 AM (1 hour ago)

to users 
*'Not that partitioning means Kafka only provides a total order over
messages within a partition. This combined with the ability to partition
data by key is sufficient for the vast majority of applications. However,
if you require a total order over messages this can be achieved with a
topic that has only one partition, though this will mean only one consumer
process.'*

The first word should say *NOTE*, right?  Otherwise, I don't understand the
meaning.

...

Marc Labbe via kafka.apache.org 
12:57 PM (12 minutes ago)

to users 
while we're at it... I noticed the following typos in
section 4.1 Motivation (
http://kafka.apache.org/documentation.html#majordesignelements)

""we knew"" instead of ""we new""
----
Finally in cases where the stream is fed into other data systems for
serving we new the system would have to be able to guarantee
fault-tolerance in the presence of machine failures.
----

""led us"" instead of ""led use""
----
Supporting these uses led use to a design with a number of unique elements,
more akin to a database log then a traditional messaging system. We will
outline some elements of the design in the following sections.",,joestein,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/13 18:13;joestein;KAFKA-1153.patch;https://issues.apache.org/jira/secure/attachment/12616411/KAFKA-1153.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,361178,,,Sat Nov 30 01:24:20 UTC 2013,,,,,,,,,,"0|i1q9dz:",361477,,,,,,,,,,,,,,,,,,,,"30/Nov/13 01:24;junrao;Thanks for the patch. +1 and committed to the website.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReplicaManager's handling of the leaderAndIsrRequest should gracefully handle leader == -1,KAFKA-1152,12681700,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,swapnilghike,swapnilghike,28/Nov/13 05:15,04/Dec/13 00:51,14/Jul/23 05:39,29/Nov/13 17:29,0.8.0,,,0.8.1,,,,,,,,,,0,,,,"If a partition is created with replication factor 1, then the controller can set the partition's leader to -1 in leaderAndIsrRequest when the only replica of the partition is being bounced. 

The handling of this request with a leader == -1 throws an exception on the ReplicaManager which prevents the addition of fetchers for the remaining partitions in the leaderAndIsrRequest.

After the replica is bounced, the replica first receives a leaderAndIsrRequest with leader == -1, then it receives another leaderAndIsrRequest with the correct leader (which is the replica itself) due to OfflinePartition to OnlinePartition state change. 

In handling the first request, ReplicaManager should ignore the partition for which the request has leader == -1, and continue addition of fetchers for the remaining partitions. The next leaderAndIsrRequest will take care of setting the correct leader for that partition.",,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/13 06:31;swapnilghike;KAFKA-1152.patch;https://issues.apache.org/jira/secure/attachment/12616200/KAFKA-1152.patch","28/Nov/13 18:19;swapnilghike;KAFKA-1152_2013-11-28_10:19:05.patch;https://issues.apache.org/jira/secure/attachment/12616296/KAFKA-1152_2013-11-28_10%3A19%3A05.patch","29/Nov/13 06:41;swapnilghike;KAFKA-1152_2013-11-28_22:40:55.patch;https://issues.apache.org/jira/secure/attachment/12616342/KAFKA-1152_2013-11-28_22%3A40%3A55.patch","03/Dec/13 19:29;swapnilghike;incremental.patch;https://issues.apache.org/jira/secure/attachment/12616839/incremental.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,360964,,,Wed Dec 04 00:51:27 UTC 2013,,,,,,,,,,"0|i1q82n:",361263,,,,,,,,,,,,,,,,,,,,"28/Nov/13 06:31;swapnilghike;Created reviewboard https://reviews.apache.org/r/15901/
 against branch origin/trunk;;;","28/Nov/13 18:19;swapnilghike;Updated reviewboard https://reviews.apache.org/r/15901/
 against branch origin/trunk;;;","29/Nov/13 06:22;swapnilghike;Created reviewboard https://reviews.apache.org/r/15915/
 against branch origin/trunk;;;","29/Nov/13 06:41;swapnilghike;Updated reviewboard https://reviews.apache.org/r/15901/
 against branch origin/trunk;;;","29/Nov/13 17:29;junrao;Thanks for the patch. +1 and committed to trunk.;;;","03/Dec/13 19:29;swapnilghike;Attached a patch to fix logging statement.;;;","04/Dec/13 00:51;junrao;Thanks for the incremental patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Hadoop consumer API doc is not referencing the contrib consumer,KAFKA-1151,12681493,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,joestein,joestein,27/Nov/13 03:21,05/Dec/13 17:51,14/Jul/23 05:39,05/Dec/13 17:51,,,,0.8.1,,,,,,,,,,0,,,,"http://kafka.apache.org/documentation.html#kafkahadoopconsumerapi

it is pointing to https://github.com/linkedin/camus/tree/camus-kafka-0.8/

if we are still supporting the contrib/hadoop-consumer then we should point to the read me (maybe this link instead https://github.com/apache/kafka/tree/0.8/contrib/hadoop-consumer)

thoughts?",,joestein,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/13 14:33;joestein;KAFKA-1151.patch;https://issues.apache.org/jira/secure/attachment/12616553/KAFKA-1151.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,360758,,,Thu Dec 05 17:41:04 UTC 2013,,,,,,,,,,"0|i1q6t3:",361057,,,,,,,,,,,,,,,,,,,,"02/Dec/13 14:33;joestein;I gave a stab at what I think would preserve the spirit of the apache contrib code and the progress that Camus has also brought.  Not sure if the Hadoop consumer can benefit from some of the Camus up stream changes (without having to take on and require Avro) but probably a discussion for the list or another JIRA but figure I start to touch/ask about that here (or a sub project or something... dunno).;;;","05/Dec/13 17:41;junrao;Thanks for the patch. +1. Just remember adding the full stop back at the end of the first sentence when checking in.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Please delete old releases from mirroring system,KAFKA-1149,12681346,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,sebb,sebb,26/Nov/13 19:34,27/Mar/19 20:26,14/Jul/23 05:39,27/Mar/19 20:26,,,,,,,,,,,,,,0,,,,"To reduce the load on the ASF mirrors, projects are required to delete old releases [1]

Please can you remove all non-current releases?

Thanks!

[Note that older releases are always available from the ASF archive server]

[1] http://www.apache.org/dev/release.html#when-to-archive
",http://www.apache.org/dist/kafka/old_releases/,guozhang,joestein,mjsax,sebb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-6223,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,360611,,,Wed Mar 27 20:26:40 UTC 2019,,,,,,,,,,"0|i1q5wf:",360910,,,,,,,,,,,,,,,,,,,,"07/Dec/13 07:42;joestein;this was taken care of;;;","26/Mar/19 18:22;sebb;There are now 9 releases on the mirrors.

Please respect the 3rd party mirrors and remove all non-current releases.

If necessary, the download page can continue to point to old releases on the archive server.;;;","26/Mar/19 18:27;mjsax;Thanks [~sebb@apache.org] – just talked to [~guozhang] about this like 30 minutes ago.

Also cf https://github.com/apache/kafka-site/pull/194;;;","27/Mar/19 20:26;guozhang;I've removed all old releases from https://www.apache.org/dist/kafka/.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delayed fetch/producer requests should be satisfied on a leader change,KAFKA-1148,12681342,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jjkoshy,jjkoshy,26/Nov/13 19:29,11/Mar/16 19:22,14/Jul/23 05:39,11/Mar/16 19:22,,,,0.10.0.0,,,,,,,,,,0,,,,"Somewhat related to KAFKA-1016.

This would be an issue only if max.wait is set to a very high value. When a leader change occurs we should remove the delayed request from the purgatory - either satisfy with error/expire - whichever makes more sense.",,githubbot,jjkoshy,peoplebike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,360607,,,Fri Mar 11 19:22:51 UTC 2016,,,,,,,,,,"0|i1q5vj:",360906,,,,,,,,,,,,,,,,,,,,"05/Dec/15 08:32;peoplebike;Recently, we have lost a set of messages. After checking the log, we think it's very likely caused by this issue.

We have a broker A, when it was the leader, it appended a MessageSet to local log, and waiting for acks, soon it became a follower, and it truncated local log, deleted that message set. But, soon after that, it became leader for that partition again, shrunk ISR and increased its HW, so the previous DelayedProduce be satisfied and the producer client received a successful produce response. But, actually, the messages was lost.

I think Kafka should check leader epoch before sending produce response, to make sure the leader was not changed since it append the produce data to log;;;","06/Dec/15 12:05;githubbot;GitHub user iBuddha opened a pull request:

    https://github.com/apache/kafka/pull/633

    KAFKA-1148 check leader epoch for DelayedProduce

    KAFKA-1148: check leader epoch for DelayedProduce

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/iBuddha/kafka KAFKA-1148

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/633.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #633
    
----
commit 873b555906a773e19bdc3fc54fe9b3f5c3f8a6dd
Author: xhuang <peoplebike@gmail.com>
Date:   2015-12-06T12:02:49Z

    KAFKA-1148 check leader epoch for DelayedProduce

----
;;;","07/Dec/15 12:52;githubbot;Github user iBuddha closed the pull request at:

    https://github.com/apache/kafka/pull/633
;;;","11/Mar/16 19:22;jjkoshy;Issue resolved by pull request 1018
[https://github.com/apache/kafka/pull/1018];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer socket timeout should be greater than fetch max wait,KAFKA-1147,12681339,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,jjkoshy,jjkoshy,26/Nov/13 19:18,17/May/16 14:13,14/Jul/23 05:39,14/Sep/14 16:33,0.8.0,0.8.1,,0.8.2.0,,,,,,,,,,0,,,,"From the mailing list:
The consumer-config documentation states that ""The actual timeout set
will be max.fetch.wait + socket.timeout.ms."" - however, that change
seems to have been lost in the code a while ago - we should either fix the doc or re-introduce the addition.
",,guozhang,jjkoshy,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/13 22:34;guozhang;KAFKA-1147.patch;https://issues.apache.org/jira/secure/attachment/12617489/KAFKA-1147.patch","08/Dec/13 02:22;guozhang;KAFKA-1147_2013-12-07_18:22:18.patch;https://issues.apache.org/jira/secure/attachment/12617600/KAFKA-1147_2013-12-07_18%3A22%3A18.patch","09/Dec/13 17:14;guozhang;KAFKA-1147_2013-12-09_09:14:24.patch;https://issues.apache.org/jira/secure/attachment/12617860/KAFKA-1147_2013-12-09_09%3A14%3A24.patch","10/Dec/13 22:31;guozhang;KAFKA-1147_2013-12-10_14:31:46.patch;https://issues.apache.org/jira/secure/attachment/12618124/KAFKA-1147_2013-12-10_14%3A31%3A46.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,360604,,,Sun Sep 14 16:33:33 UTC 2014,,,,,,,,,,"0|i1q5uv:",360903,,nehanarkhede,,,,,,,,,,,,,,,,,,"06/Dec/13 22:34;guozhang;Created reviewboard https://reviews.apache.org/r/16092/
 against branch origin/trunk;;;","08/Dec/13 02:22;guozhang;Updated reviewboard https://reviews.apache.org/r/16092/
 against branch origin/trunk;;;","08/Dec/13 02:23;guozhang;Proposed to do the first option as Joel suggested. Someone needs to change the doc then.

Guozhang;;;","09/Dec/13 17:14;guozhang;Updated reviewboard https://reviews.apache.org/r/16092/
 against branch origin/trunk;;;","10/Dec/13 22:31;guozhang;Updated reviewboard https://reviews.apache.org/r/16092/
 against branch origin/trunk;;;","17/Jan/14 18:23;guozhang;[~junrao] Could you give another review at this?;;;","04/Sep/14 22:04;guozhang;This is quite an easy fix and could be reviewed / checked in by any committer I think.;;;","14/Sep/14 16:33;nehanarkhede;Fix checked in for the consumer and broker. Reverted the changes to the old producer.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
toString() on KafkaStream gets stuck indefinitely,KAFKA-1146,12681328,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,amalakar,amalakar,amalakar,26/Nov/13 19:03,19/Jun/14 05:09,14/Jul/23 05:39,02/May/14 14:46,0.8.0,,,0.8.2.0,,,,,,,consumer,,,1,newbie,,,"There is no toString implementation for KafkaStream, so if a user tries to print the stream it falls back to default toString implementation which tries to iterate over the collection and gets stuck indefinitely as it awaits messages. KafkaStream could instead override the toString and return a verbose description of the stream with topic name etc.

println(""Current stream: "" + stream) // This call never returns",,amalakar,donnchadh,jjkoshy,junrao,omnomnom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/May/14 06:31;amalakar;KAFKA-1146.patch;https://issues.apache.org/jira/secure/attachment/12643003/KAFKA-1146.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,360593,,,Fri May 02 14:46:45 UTC 2014,,,,,,,,,,"0|i1q5sf:",360892,,,,,,,,,,,,,,,,,,,,"26/Nov/13 19:57;jjkoshy;Yes this is due to KafkaStream being a scala iterable - toString on
an iterable (per the scala-doc): ""returns a string representation of
this collection. By default this string consists of the stringPrefix
of this immutable iterable collection, followed by all elements
separated by commas and enclosed in parentheses.""
We can just override it simce others have run into this as well.;;;","26/Nov/13 20:21;amalakar;[~jjkoshy] Yes overriding would definitely be beneficial. I can submit a patch for this. Any suggestion on what I could put in the toString method?;;;","27/Nov/13 20:33;jjkoshy;We could just print something like ""%s kafka stream"".format(clientId) - there's very little information that the KafkaStream class exposes. However, this is more to protect against iterating over the full stream on a toString call. toString on KafkaStream does not really make sense - so even a message to that effect should be fine.;;;","02/May/14 14:46;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Patch review tool should take diff with origin from last divergent point,KAFKA-1142,12680795,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,jjkoshy,jjkoshy,22/Nov/13 18:15,12/Dec/13 00:21,14/Jul/23 05:39,12/Dec/13 00:21,,,,,,,,,,,,,,0,,,,,,guozhang,jjkoshy,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/13 00:05;guozhang;KAFKA-1142.patch;https://issues.apache.org/jira/secure/attachment/12617936/KAFKA-1142.patch","09/Dec/13 23:56;guozhang;KAFKA-1142.patch;https://issues.apache.org/jira/secure/attachment/12617929/KAFKA-1142.patch","09/Dec/13 23:55;guozhang;KAFKA-1142.patch;https://issues.apache.org/jira/secure/attachment/12617928/KAFKA-1142.patch","22/Nov/13 18:20;jjkoshy;KAFKA-1142.patch;https://issues.apache.org/jira/secure/attachment/12615366/KAFKA-1142.patch","10/Dec/13 00:05;guozhang;KAFKA-1142_2013-12-09_16:05:14.patch;https://issues.apache.org/jira/secure/attachment/12617934/KAFKA-1142_2013-12-09_16%3A05%3A14.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,360060,,,Thu Dec 12 00:21:05 UTC 2013,,,,,,,,,,"0|i1q2in:",360359,,,,,,,,,,,,,,,,,,,,"22/Nov/13 18:20;jjkoshy;Created reviewboard  against branch origin/trunk;;;","22/Nov/13 18:21;jjkoshy;I got: rbtools.api.errors.APIError: HTTP 502
(from my laptop)
Can someone else try the above patch?;;;","22/Nov/13 18:28;jjkoshy;Actually, it did post a diff:
https://reviews.apache.org/r/15793/diff/
Not sure why it did not succeed posting the link to jira.

I used this jira to test:
- Checked out a much older git hash
- Applied the above patch
- The patch review tool now only shows the diff from that older git hash (i.e., the divergent point)

I think it is good to do this on 0.8 as well - just to avoid patches that accidentally delete old commits.
;;;","09/Dec/13 18:37;guozhang;Instead of doing a git diff with the common ancestor, shall we just force a rebase on the patch before doing the diff?;;;","09/Dec/13 19:14;nehanarkhede;[~guozhang] Makes sense. We need to test if there is a way to check for that and throw the appropriate error to the user. Would you like to take a stab at a patch?;;;","09/Dec/13 23:55;guozhang;Created reviewboard https://reviews.apache.org/r/16140/
 against branch origin/trunk;;;","09/Dec/13 23:56;guozhang;Created reviewboard https://reviews.apache.org/r/16141/
 against branch origin/trunk;;;","10/Dec/13 00:05;guozhang;Updated reviewboard  against branch origin/trunk;;;","10/Dec/13 00:05;guozhang;Created reviewboard https://reviews.apache.org/r/16143/
 against branch origin/trunk;;;","10/Dec/13 00:10;guozhang;Latest RB is 16143.

After looking into it I realize that forcing a rebase would be a bit tricky, so instead I add a check on the branch hash to see if it has changed since the current working copy has generated. If yes then throw an error message.

Test passed by

1. Checkout an old version, make some changes.
2. Commit, and call kafka-path-review, it outputs:

ERROR: Your current working branch is from an older version of origin/trunk. Please rebase first by using git pull --rebase
;;;","12/Dec/13 00:21;nehanarkhede;Committed the patch to trunk. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make changes to downloads for the archive old releases to new old_releases folder,KAFKA-1141,12680680,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,joestein,joestein,joestein,22/Nov/13 04:49,04/Dec/13 02:36,14/Jul/23 05:39,26/Nov/13 17:34,0.7.1,0.7.2,0.8.0,0.8.1,,,,,,,,,,0,,,,"I copied the files from dist incubator to where they belong now in dist, gotta modifiy the downloads page too

need to-do this before RC4

http://archive.apache.org/dist/kafka/ waiting on the artifacts to mirror",,joestein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,359945,,,Tue Nov 26 17:34:02 UTC 2013,,,,,,,,,,"0|i1q1t3:",360244,,,,,,,,,,,,,,,,,,,,"26/Nov/13 17:34;joestein;all set, downloads page is live with the updates

Sent email to Henk that he can clobber the old /x1/www/www.apache.org/dist/incubator/kafka now ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move the decoding logic from ConsumerIterator.makeNext to next,KAFKA-1140,12680664,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,22/Nov/13 02:01,28/Nov/13 05:56,14/Jul/23 05:39,28/Nov/13 05:56,,,,0.8.1,,,,,,,,,,0,,,,"Usually people will write code around consumer like

while(iter.hasNext()) {
try {
  msg = iter.next()
  // do something
}
catch{
}
}

----

However, the iter.hasNext() call itself can throw exceptions due to decoding failures. It would be better to move the decoding to the next function call.",,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/13 22:37;guozhang;KAFKA-1140.patch;https://issues.apache.org/jira/secure/attachment/12615399/KAFKA-1140.patch","25/Nov/13 20:53;guozhang;KAFKA-1140_2013-11-25_12:53:17.patch;https://issues.apache.org/jira/secure/attachment/12615664/KAFKA-1140_2013-11-25_12%3A53%3A17.patch","25/Nov/13 20:55;guozhang;KAFKA-1140_2013-11-25_12:55:34.patch;https://issues.apache.org/jira/secure/attachment/12615665/KAFKA-1140_2013-11-25_12%3A55%3A34.patch","26/Nov/13 22:41;guozhang;KAFKA-1140_2013-11-26_14:41:00.patch;https://issues.apache.org/jira/secure/attachment/12615932/KAFKA-1140_2013-11-26_14%3A41%3A00.patch","27/Nov/13 02:30;guozhang;KAFKA-1140_2013-11-26_18:29:53.patch;https://issues.apache.org/jira/secure/attachment/12615980/KAFKA-1140_2013-11-26_18%3A29%3A53.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,359929,,,Thu Nov 28 05:56:48 UTC 2013,,,,,,,,,,"0|i1q1pj:",360228,,,,,,,,,,,,,,,,,,,,"22/Nov/13 22:37;guozhang;Created reviewboard https://reviews.apache.org/r/15805/
 against branch origin/trunk;;;","25/Nov/13 20:53;guozhang;Updated reviewboard https://reviews.apache.org/r/15805/
 against branch origin/trunk;;;","25/Nov/13 20:55;guozhang;Updated reviewboard https://reviews.apache.org/r/15805/
 against branch origin/trunk;;;","26/Nov/13 22:41;guozhang;Updated reviewboard https://reviews.apache.org/r/15805/
 against branch origin/trunk;;;","27/Nov/13 02:30;guozhang;Updated reviewboard https://reviews.apache.org/r/15805/
 against branch origin/trunk;;;","28/Nov/13 05:56;junrao;Thanks for the patch. +1 and committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Topic data change handling callback should not call syncedRebalance directly,KAFKA-1139,12680568,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,guozhang,guozhang,21/Nov/13 18:54,19/Dec/13 20:55,14/Jul/23 05:39,19/Dec/13 20:55,0.8.1,,,,,,,,,,,,,0,,,,".. but should just set the flag as consumer change handling callback, so that a sequence of data changes on topics will not trigger unnecessarily many rebalances.",,guozhang,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1186,,,,,,,,,,,,,,,"18/Dec/13 21:33;nehanarkhede;KAFKA-1139.patch;https://issues.apache.org/jira/secure/attachment/12619410/KAFKA-1139.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,359833,,,Thu Dec 19 20:55:21 UTC 2013,,,,,,,,,,"0|i1q14n:",360132,,,,,,,,,,,,,,,,,,,,"18/Dec/13 21:33;nehanarkhede;Created reviewboard https://reviews.apache.org/r/16356/
 against branch trunk;;;","19/Dec/13 20:55;nehanarkhede;Thanks for the review.. committed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remote producer uses the hostname defined in broker,KAFKA-1138,12680447,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,vacuus,vacuus,21/Nov/13 06:36,31/Aug/17 18:29,14/Jul/23 05:39,31/Aug/17 18:29,0.8.0,,,,,,,,,,producer ,,,0,,,,"When the producer API in the node which is not the broker sends message to a broker, only TopicMetadataRequest is sent, but ProducerRequest is not by observing the log of ""kafka-request.log""
According to my analysis, when the producer api sends ProducerRequest, it seems to use the hostname defined in the broker. So, if the hostname is not the one registered in DNS, the producer cannot send the ProducerRequest. 


I am attaching the log:

[2013-11-21 15:28:49,464] ERROR Failed to collate messages by topic, partition due to: fetching topic metadata for topics [Set(test)] from broker [ArrayBuffer(id:0,host:111.111.111.111,port:9092)] failed (kafka.producer.async.DefaultEventHandler)
[2013-11-21 15:28:49,465] INFO Back off for 100 ms before retrying send. Remaining retries = 1 (kafka.producer.async.DefaultEventHandler)
[2013-11-21 15:28:49,566] INFO Fetching metadata from broker id:0,host:111.111.111.111,port:9092 with correlation id 6 for 1 topic(s) Set(test) (kafka.client.ClientUtils$)
[2013-11-21 15:28:49,819] ERROR Producer connection to 111.111.111.111:9092 unsuccessful (kafka.producer.SyncProducer)
java.net.ConnectException: 연결이 거부됨
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:465)
	at sun.nio.ch.Net.connect(Net.java:457)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:639)
	at kafka.network.BlockingChannel.connect(BlockingChannel.scala:57)
	at kafka.producer.SyncProducer.connect(SyncProducer.scala:146)
	at kafka.producer.SyncProducer.getOrMakeConnection(SyncProducer.scala:161)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:68)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:112)
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:53)
	at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82)
	at kafka.producer.async.DefaultEventHandler$$anonfun$handle$2.apply$mcV$sp(DefaultEventHandler.scala:79)
	at kafka.utils.Utils$.swallow(Utils.scala:186)
	at kafka.utils.Logging$class.swallowError(Logging.scala:105)
	at kafka.utils.Utils$.swallowError(Utils.scala:45)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:79)
	at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:104)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:87)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:67)
	at scala.collection.immutable.Stream.foreach(Stream.scala:254)
	at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:66)
	at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)
[2013-11-21 15:28:49,821] WARN Fetching topic metadata with correlation id 6 for topics [Set(test)] from broker [id:0,host:111.111.111.111,port:9092] failed (kafka.client.ClientUtils$)
java.net.ConnectException: 연결이 거부됨
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:465)
	at sun.nio.ch.Net.connect(Net.java:457)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:639)
	at kafka.network.BlockingChannel.connect(BlockingChannel.scala:57)
	at kafka.producer.SyncProducer.connect(SyncProducer.scala:146)
	at kafka.producer.SyncProducer.getOrMakeConnection(SyncProducer.scala:161)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:68)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:112)
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:53)
	at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82)
	at kafka.producer.async.DefaultEventHandler$$anonfun$handle$2.apply$mcV$sp(DefaultEventHandler.scala:79)
	at kafka.utils.Utils$.swallow(Utils.scala:186)
	at kafka.utils.Logging$class.swallowError(Logging.scala:105)
	at kafka.utils.Utils$.swallowError(Utils.scala:45)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:79)
	at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:104)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:87)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:67)
	at scala.collection.immutable.Stream.foreach(Stream.scala:254)
	at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:66)
	at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)
[2013-11-21 15:28:49,822] ERROR fetching topic metadata for topics [Set(test)] from broker [ArrayBuffer(id:0,host:111.111.111.111,port:9092)] failed (kafka.utils.Utils$)
kafka.common.KafkaException: fetching topic metadata for topics [Set(test)] from broker [ArrayBuffer(id:0,host:111.111.111.111,port:9092)] failed
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:67)
	at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82)
	at kafka.producer.async.DefaultEventHandler$$anonfun$handle$2.apply$mcV$sp(DefaultEventHandler.scala:79)
	at kafka.utils.Utils$.swallow(Utils.scala:186)
	at kafka.utils.Logging$class.swallowError(Logging.scala:105)
	at kafka.utils.Utils$.swallowError(Utils.scala:45)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:79)
	at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:104)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:87)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:67)
	at scala.collection.immutable.Stream.foreach(Stream.scala:254)
	at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:66)
	at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)
Caused by: java.net.ConnectException: 연결이 거부됨
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:465)
	at sun.nio.ch.Net.connect(Net.java:457)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:639)
	at kafka.network.BlockingChannel.connect(BlockingChannel.scala:57)
	at kafka.producer.SyncProducer.connect(SyncProducer.scala:146)
	at kafka.producer.SyncProducer.getOrMakeConnection(SyncProducer.scala:161)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:68)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:112)
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:53)
	... 12 more
[2013-11-21 15:28:49,825] INFO Fetching metadata from broker id:0,host:111.111.111.111,port:9092 with correlation id 7 for 1 topic(s) Set(test) (kafka.client.ClientUtils$)
[2013-11-21 15:28:50,021] ERROR Producer connection to 111.111.111.111:9092 unsuccessful (kafka.producer.SyncProducer)
java.net.ConnectException: 연결이 거부됨
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:465)
	at sun.nio.ch.Net.connect(Net.java:457)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:639)
	at kafka.network.BlockingChannel.connect(BlockingChannel.scala:57)
	at kafka.producer.SyncProducer.connect(SyncProducer.scala:146)
	at kafka.producer.SyncProducer.getOrMakeConnection(SyncProducer.scala:161)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:68)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:112)
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:53)
	at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82)
	at kafka.producer.BrokerPartitionInfo.getBrokerPartitionInfo(BrokerPartitionInfo.scala:49)
	at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$getPartitionListForTopic(DefaultEventHandler.scala:187)
	at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:151)
	at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:150)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
	at kafka.producer.async.DefaultEventHandler.partitionAndCollate(DefaultEventHandler.scala:150)
	at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:96)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:73)
	at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:104)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:87)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:67)
	at scala.collection.immutable.Stream.foreach(Stream.scala:254)
	at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:66)
	at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)
[2013-11-21 15:28:50,024] WARN Fetching topic metadata with correlation id 7 for topics [Set(test)] from broker [id:0,host:111.111.111.111,port:9092] failed (kafka.client.ClientUtils$)
java.net.ConnectException: 연결이 거부됨
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:465)
	at sun.nio.ch.Net.connect(Net.java:457)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:639)
	at kafka.network.BlockingChannel.connect(BlockingChannel.scala:57)
	at kafka.producer.SyncProducer.connect(SyncProducer.scala:146)
	at kafka.producer.SyncProducer.getOrMakeConnection(SyncProducer.scala:161)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:68)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:112)
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:53)
	at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82)
	at kafka.producer.BrokerPartitionInfo.getBrokerPartitionInfo(BrokerPartitionInfo.scala:49)
	at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$getPartitionListForTopic(DefaultEventHandler.scala:187)
	at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:151)
	at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:150)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
	at kafka.producer.async.DefaultEventHandler.partitionAndCollate(DefaultEventHandler.scala:150)
	at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:96)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:73)
	at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:104)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:87)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:67)
	at scala.collection.immutable.Stream.foreach(Stream.scala:254)
	at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:66)
	at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)
[2013-11-21 15:28:50,025] ERROR Failed to collate messages by topic, partition due to: fetching topic metadata for topics [Set(test)] from broker [ArrayBuffer(id:0,host:111.111.111.111,port:9092)] failed (kafka.producer.async.DefaultEventHandler)
[2013-11-21 15:28:50,026] INFO Back off for 100 ms before retrying send. Remaining retries = 0 (kafka.producer.async.DefaultEventHandler)
[2013-11-21 15:28:50,127] INFO Fetching metadata from broker id:0,host:111.111.111.111,port:9092 with correlation id 8 for 1 topic(s) Set(test) (kafka.client.ClientUtils$)
[2013-11-21 15:28:50,324] ERROR Producer connection to 111.111.111.111:9092 unsuccessful (kafka.producer.SyncProducer)
java.net.ConnectException: 연결이 거부됨
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:465)
	at sun.nio.ch.Net.connect(Net.java:457)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:639)
	at kafka.network.BlockingChannel.connect(BlockingChannel.scala:57)
	at kafka.producer.SyncProducer.connect(SyncProducer.scala:146)
	at kafka.producer.SyncProducer.getOrMakeConnection(SyncProducer.scala:161)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:68)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:112)
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:53)
	at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82)
	at kafka.producer.async.DefaultEventHandler$$anonfun$handle$2.apply$mcV$sp(DefaultEventHandler.scala:79)
	at kafka.utils.Utils$.swallow(Utils.scala:186)
	at kafka.utils.Logging$class.swallowError(Logging.scala:105)
	at kafka.utils.Utils$.swallowError(Utils.scala:45)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:79)
	at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:104)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:87)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:67)
	at scala.collection.immutable.Stream.foreach(Stream.scala:254)
	at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:66)
	at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)
[2013-11-21 15:28:50,326] WARN Fetching topic metadata with correlation id 8 for topics [Set(test)] from broker [id:0,host:111.111.111.111,port:9092] failed (kafka.client.ClientUtils$)
java.net.ConnectException: 연결이 거부됨
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:465)
	at sun.nio.ch.Net.connect(Net.java:457)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:639)
	at kafka.network.BlockingChannel.connect(BlockingChannel.scala:57)
	at kafka.producer.SyncProducer.connect(SyncProducer.scala:146)
	at kafka.producer.SyncProducer.getOrMakeConnection(SyncProducer.scala:161)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:68)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:112)
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:53)
	at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82)
	at kafka.producer.async.DefaultEventHandler$$anonfun$handle$2.apply$mcV$sp(DefaultEventHandler.scala:79)
	at kafka.utils.Utils$.swallow(Utils.scala:186)
	at kafka.utils.Logging$class.swallowError(Logging.scala:105)
	at kafka.utils.Utils$.swallowError(Utils.scala:45)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:79)
	at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:104)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:87)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:67)
	at scala.collection.immutable.Stream.foreach(Stream.scala:254)
	at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:66)
	at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)
[2013-11-21 15:28:50,328] ERROR fetching topic metadata for topics [Set(test)] from broker [ArrayBuffer(id:0,host:111.111.111.111,port:9092)] failed (kafka.utils.Utils$)
kafka.common.KafkaException: fetching topic metadata for topics [Set(test)] from broker [ArrayBuffer(id:0,host:111.111.111.111,port:9092)] failed
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:67)
	at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82)
	at kafka.producer.async.DefaultEventHandler$$anonfun$handle$2.apply$mcV$sp(DefaultEventHandler.scala:79)
	at kafka.utils.Utils$.swallow(Utils.scala:186)
	at kafka.utils.Logging$class.swallowError(Logging.scala:105)
	at kafka.utils.Utils$.swallowError(Utils.scala:45)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:79)
	at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:104)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:87)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:67)
	at scala.collection.immutable.Stream.foreach(Stream.scala:254)
	at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:66)
	at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)
Caused by: java.net.ConnectException: 연결이 거부됨
	at sun.nio.ch.Net.connect0(Native Method)
	at sun.nio.ch.Net.connect(Net.java:465)
	at sun.nio.ch.Net.connect(Net.java:457)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:639)
	at kafka.network.BlockingChannel.connect(BlockingChannel.scala:57)
	at kafka.producer.SyncProducer.connect(SyncProducer.scala:146)
	at kafka.producer.SyncProducer.getOrMakeConnection(SyncProducer.scala:161)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:68)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:112)
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:53)
	... 12 more
[2013-11-21 15:28:50,332] ERROR Failed to send requests for topics test with correlation ids in [0,8] (kafka.producer.async.DefaultEventHandler)
[2013-11-21 15:28:50,333] ERROR Error in handling batch of 1 events (kafka.producer.async.ProducerSendThread)
kafka.common.FailedToSendMessageException: Failed to send messages after 3 tries.
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:90)
	at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:104)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:87)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:67)
	at scala.collection.immutable.Stream.foreach(Stream.scala:254)
	at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:66)
	at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)
       
",,junrao,vacuus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,359712,,,Thu Nov 21 16:34:09 UTC 2013,,,,,,,,,,"0|i1q0dr:",360011,,,,,,,,,,,,,,,,,,,,"21/Nov/13 16:34;junrao;Do you think https://issues.apache.org/jira/browse/KAFKA-1092 addresses this issue?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Code cleanup - use Json.encode() to write json data to zk,KAFKA-1135,12679869,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,swapnilghike,swapnilghike,19/Nov/13 03:13,26/Nov/13 02:35,14/Jul/23 05:39,20/Nov/13 18:05,,,,0.8.1,,,,,,,,,,0,,,,,,davidlao,jjkoshy,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/13 03:14;swapnilghike;KAFKA-1135.patch;https://issues.apache.org/jira/secure/attachment/12614540/KAFKA-1135.patch","19/Nov/13 03:18;swapnilghike;KAFKA-1135_2013-11-18_19:17:54.patch;https://issues.apache.org/jira/secure/attachment/12614542/KAFKA-1135_2013-11-18_19%3A17%3A54.patch","19/Nov/13 03:21;swapnilghike;KAFKA-1135_2013-11-18_19:20:58.patch;https://issues.apache.org/jira/secure/attachment/12614543/KAFKA-1135_2013-11-18_19%3A20%3A58.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,359227,,,Tue Nov 26 02:35:11 UTC 2013,,,,,,,,,,"0|i1pxen:",359526,,,,,,,,,,,,,,,,,,,,"19/Nov/13 03:15;swapnilghike;Created reviewboard https://reviews.apache.org/r/15665/
 against branch origin/trunk;;;","19/Nov/13 03:18;swapnilghike;Updated reviewboard https://reviews.apache.org/r/15665/
 against branch origin/trunk;;;","19/Nov/13 03:21;swapnilghike;Updated reviewboard https://reviews.apache.org/r/15665/
 against branch origin/trunk;;;","20/Nov/13 18:05;junrao;Thanks for the patch. +1 and committed to trunk.;;;","25/Nov/13 22:57;davidlao;Hi. This patch seem to have undone all the KAFKA-1112 changes. Can you verify?;;;","25/Nov/13 23:06;swapnilghike;Thanks for catching this David! Jun, it seems that the diff in the reviewboard and what got attached to this JIRA is different. Can you please revert commit 9b0776d157afd9eacddb84a99f2420fa9c0d505b, download the diff from the reviewboard and commit it?;;;","25/Nov/13 23:09;swapnilghike;[~jjkoshy], does the above issue look similar to KAFKA-1142? ;;;","25/Nov/13 23:34;junrao;Thanks for pointing this out. Recommitted KAFKA-1112.;;;","26/Nov/13 02:35;jjkoshy;[~swapnilghike] I'm pretty sure the extra diff comes due to the issue in KAFKA-1142, but I don't know why that change does not show up in the RB itself.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
onControllerFailover function should be synchronized with other functions,KAFKA-1134,12679585,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,16/Nov/13 20:14,12/Dec/13 00:35,14/Jul/23 05:39,12/Dec/13 00:35,0.8.0,0.8.1,,,,,,,,,,,,1,,,,"Otherwise race conditions could happen. For example, handleNewSession will close all sockets with brokers while the handleStateChange in onControllerFailover tries to send requests to them.",,guozhang,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/13 00:58;guozhang;KAFKA-1134.patch;https://issues.apache.org/jira/secure/attachment/12616662/KAFKA-1134.patch","05/Dec/13 19:13;guozhang;KAFKA-1134_2013-12-05_11:13:33.patch;https://issues.apache.org/jira/secure/attachment/12617215/KAFKA-1134_2013-12-05_11%3A13%3A33.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,358945,,,Thu Dec 12 00:35:08 UTC 2013,,,,,,,,,,"0|i1pvkv:",359235,,,,,,,,,,,,,,,,,,,,"03/Dec/13 00:58;guozhang;Created reviewboard https://reviews.apache.org/r/15953/
 against branch origin/trunk;;;","03/Dec/13 17:20;nehanarkhede;[~guozhang] Do you happen to have a stack trace of the problem you have described?;;;","05/Dec/13 19:13;guozhang;Updated reviewboard https://reviews.apache.org/r/15953/
 against branch origin/trunk;;;","05/Dec/13 19:23;guozhang;After checking the stack trace again, now I think the problem is that

1) In KafkaController.handleNewSession

controllerContext.controllerLock synchronized {
        Utils.unregisterMBean(KafkaController.MBeanName)
        partitionStateMachine.shutdown()
        replicaStateMachine.shutdown()
        if(controllerContext.controllerChannelManager != null) {
          controllerContext.controllerChannelManager.shutdown()
          controllerContext.controllerChannelManager = null
        }
        controllerElector.elect
      }

elect function is called directly after controllerChannelManager.shutdown and is lock covered by controllerContext.controllerLock, however from the logs. elect is not immediately called since addpartition listener gets triggered due to ZK expiration (known issue similar as KAFKA-1143) and which are covered by the same lock:

2013/11/14 00:00:24.596 [RequestSendThread] [Controller-583-to-broker-587-send-thread], Stopped 
2013/11/14 00:00:24.596 [RequestSendThread] [Controller-583-to-broker-587-send-thread], Shutdown completed
2013/11/14 00:00:24.596 [RequestSendThread] [Controller-583-to-broker-579-send-thread], Shutting down
2013/11/14 00:00:24.596 [RequestSendThread] [Controller-583-to-broker-579-send-thread], Stopped 
2013/11/14 00:00:24.596 [RequestSendThread] [Controller-583-to-broker-579-send-thread], Shutdown completed
2013/11/14 00:00:24.603 [ReplicaStateMachine$BrokerChangeListener] [BrokerChangeListener on Controller 583]: Broker change listener fired for path /brokers/ids with children 583,575,585,587,579,589
2013/11/14 00:00:24.605 [ReplicaStateMachine$BrokerChangeListener] [BrokerChangeListener on Controller 583]: Broker change listener fired for path /brokers/ids with children 583,575,585,587,579,589
2013/11/14 00:00:24.614 [PartitionStateMachine$AddPartitionsListener] [AddPartitionsListener on 583]: Add Partition triggered { ""partitions"":{ ""0"":[ 577, 589 ], ""1"":[ 579, 575 ], ""2"":[ 581, 577 ], ""3"":[ 583, 579 ] }, ""version"":1 } for path /brokers/topics/databus2-relay-log_event
2013/11/14 00:00:24.616 [PartitionStateMachine$AddPartitionsListener] [AddPartitionsListener on 583]: New partitions to be added [Map()]
2013/11/14 00:00:24.616 [KafkaController] [Controller 583]: New partition creation callback for 
2013/11/14 00:00:24.618 [PartitionStateMachine$AddPartitionsListener] [AddPartitionsListener on 583]: Add Partition triggered { ""partitions"":{ ""0"":[ 577, 589 ], ""1"":[ 579, 575 ], ""2"":[ 581, 577 ], ""3"":[ 583, 579 ] }, ""version"":1 } for path /brokers/topics/databus2-relay-log_event

----------------

Without other logging info I cannot deduce any further, so I propose in this jira we just improve the logging info for better debugging if this issue comes up in the future.;;;","12/Dec/13 00:35;nehanarkhede;Thanks for the patch. Committed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LICENSE and NOTICE files need to get into  META-INF when jars are built before they're signed for publishing to maven,KAFKA-1133,12679095,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,charmalloc,joestein,joestein,13/Nov/13 19:56,02/Dec/13 17:25,14/Jul/23 05:39,25/Nov/13 02:35,,,,0.8.0,0.8.1,,,,,,,,,0,,,,This needs to happen in our Build.scala the sbt package docs http://www.scala-sbt.org/release/docs/Howto/package.html probably a straight forward line of code or ten or whatever to-do this maybe,,charmalloc,joestein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Nov/13 00:00;charmalloc;KAFKA-1133.patch;https://issues.apache.org/jira/secure/attachment/12615037/KAFKA-1133.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,358460,,,Thu Nov 21 00:00:22 UTC 2013,,,,,,,,,,"0|i1psl3:",358750,,,,,,,,,,,,,,,,,,,,"14/Nov/13 07:35;joestein;the archive function might be what we need to hook into http://www.scala-sbt.org/release/sxr/sbt/IO.scala.html#sbt.IO.archive.manifest;;;","20/Nov/13 23:55;charmalloc;Created reviewboard ;;;","21/Nov/13 00:00;charmalloc;Created reviewboard https://reviews.apache.org/r/15744/
 against branch trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Github is still showing 0.7 as the default branch,KAFKA-1128,12678232,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,joestein,joestein,08/Nov/13 14:46,07/Dec/13 08:02,14/Jul/23 05:39,07/Dec/13 08:02,,,,0.8.1,,,,,,,,,,0,,,,"https://github.com/apache/kafka

I think this is because we don't have a master branch.  My thoughts are we should branch trunk creating a master branch and then everything would be ok... we then start using master as we used to use trunk.

I think this is important to promote the adoption of 0.8.0 otherwise folks that find Kafka on github are still going to start with 0.7 and we should always be whatever is latest and live (0.9 moving forward and so on) from the main repo.

thoughts?",,joestein,omnomnom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,357607,,,Sat Dec 07 08:02:04 UTC 2013,,,,,,,,,,"0|i1pnbz:",357897,,,,,,,,,,,,,,,,,,,,"08/Nov/13 15:32;omnomnom;Actually you don't need to create any branches, just head to https://github.com/apache/kafka/settings and adjust settings appropriately: http://take.ms/Qqv8u2;;;","08/Nov/13 15:36;joestein;Yes, but I was trying to avoid bugging the INFRA folks as I am not sure how that would get changed if we could do it ourselves but contacting Apache INFRA is an option too, sure.;;;","08/Nov/13 15:37;joestein;I don't have access to this in github but if someone else does and can make the change to trunk and save then awesome!!!;;;","07/Dec/13 07:43;joestein;opened an INFRA ticket https://issues.apache.org/jira/browse/INFRA-7079;;;","07/Dec/13 08:02;joestein;all set now https://github.com/apache/kafka;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the DISCLAIMER it is left over from incubation,KAFKA-1126,12678120,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,joestein,joestein,08/Nov/13 00:20,08/Nov/13 01:39,14/Jul/23 05:39,08/Nov/13 01:39,,,,0.8.0,,,,,,,,,,0,,,,,,joestein,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/13 00:32;joestein;KAFKA-1122.patch;https://issues.apache.org/jira/secure/attachment/12612747/KAFKA-1122.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,357495,,,Fri Nov 08 01:14:02 UTC 2013,,,,,,,,,,"0|i1pmn3:",357785,,,,,,,,,,,,,,,,,,,,"08/Nov/13 01:14;nehanarkhede;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sending to a new topic (with auto.create.topics.enable) returns ERROR,KAFKA-1124,12677835,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jbrosenberg@gmail.com,jbrosenberg@gmail.com,06/Nov/13 18:32,20/Jan/15 10:51,14/Jul/23 05:39,20/Jan/15 10:51,0.8.0,0.8.1,,0.8.2.0,,,,,,,,,,2,usability,,,"I had thought this was reported issue, but can't seem to find a previous report for it.

If auto.create.topics.enable is true, a producer still gets an ERROR logged on the first attempt to send a message to a new topic, e.g.:

2013-11-06 03:00:08,638 ERROR [Thread-1] async.DefaultEventHandler - Failed to collate messages by topic, partition due to: Failed to fetch topic metadata for topic: mynewtopic
2013-11-06 03:00:08,638  INFO [Thread-1] async.DefaultEventHandler - Back off for 100 ms before retrying send. Remaining retries = 3

This usually clears itself up immediately on retry (after 100 ms), as handled by the the kafka.producer.async.DefaultEventHandler (with retries enabled).

However, this is logged to the client as an ERROR, and looks scary, when in fact it should have been a normal operation (since we have auto.create.topics.enable=true).

There should be a better interaction here between the producer client and the server.

Perhaps the server can create the topic in flight before returning the metadata request.

Or, if it needs to be asynchronous, it could return a code which indicates something like: ""The topic doesn't exist yet, it is being created, try again shortly"".....and have the client automatically retry (even if retries not enabled, since it's not an ERROR condition, really).

The ERROR log level is a problem since apps often have alert systems set up to notify when any ERROR happens, etc.....
",,anujmehta,chelseaz,guozhang,hanish.bansal.agarwal,hasrobin,jbrosenberg@gmail.com,jeffwidman,jjkoshy,jpotter,omkreddy,wangbo23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,357210,,,Tue Jan 20 10:51:39 UTC 2015,,,,,,,,,,"0|i1pkvr:",357500,,,,,,,,,,,,,,,,,,,,"14/Nov/13 05:26;anujmehta;Hi

I tried reproducing this issue but I am getting a WARN message instead of an ERROR

[2013-11-13 18:24:59,310] WARN Error while fetching metadata [{TopicMetadata for topic newTopic -> 
No partition metadata for topic newTopic due to kafka.common.LeaderNotAvailableException}] for topic [newTopic]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo);;;","14/Nov/13 14:47;jbrosenberg@gmail.com;[~anujmehta] are you looking in the logs for the producer, or the server?  The ERROR I see is in the producer.  It does go on to complain aobut LeaderNotAvailable, etc., but it also gives me the ERROR message.

I'm using 0.8, sha: cd3b79699341afb8d52c51d9ac7317d93c32eeb6  (dated Oct 16).  Which version are you using?

Jason;;;","19/Nov/13 12:00;anujmehta;Hi Jason Rosenberg 

Just checked again. Yes there is an ERROR message in producer logs. Let me check if I can fix this;;;","14/Jan/14 09:59;jbrosenberg@gmail.com;status?;;;","27/Mar/14 18:13;hasrobin;I'm running 0.8.1. and only run into this issue with auto created topic when replication-factor is greater than 1.  When topics are manually created, no errors.

[root@h-kafka01-1b.use01.ho.priv kafka]# ./bin/kafka-console-producer.sh --broker-list h-kafka01:9092,h-kafka02:9092,h-kafka03:9092 --topic RobinTest4
Test Message One, testing auto topic creation
[2014-03-26 23:25:24,253] WARN Error while fetching metadata [{TopicMetadata for topic RobinTest4 -> 
No partition metadata for topic RobinTest4 due to kafka.common.LeaderNotAvailableException}] for topic [RobinTest4]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)
[2014-03-26 23:25:24,289] WARN Error while fetching metadata [{TopicMetadata for topic RobinTest4 -> 
No partition metadata for topic RobinTest4 due to kafka.common.LeaderNotAvailableException}] for topic [RobinTest4]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)
[2014-03-26 23:25:24,290] ERROR Failed to collate messages by topic, partition due to: Failed to fetch topic metadata for topic: RobinTest4 (kafka.producer.async.DefaultEventHandler)
[2014-03-26 23:25:24,406] WARN Error while fetching metadata [{TopicMetadata for topic RobinTest4 -> 
No partition metadata for topic RobinTest4 due to kafka.common.LeaderNotAvailableException}] for topic [RobinTest4]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)
[2014-03-26 23:25:24,434] WARN Error while fetching metadata [{TopicMetadata for topic RobinTest4 -> 
No partition metadata for topic RobinTest4 due to kafka.common.LeaderNotAvailableException}] for topic [RobinTest4]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)
[2014-03-26 23:25:24,434] ERROR Failed to collate messages by topic, partition due to: Failed to fetch topic metadata for topic: RobinTest4 (kafka.producer.async.DefaultEventHandler)
[2014-03-26 23:25:24,545] WARN Error while fetching metadata [{TopicMetadata for topic RobinTest4 -> 
No partition metadata for topic RobinTest4 due to kafka.common.LeaderNotAvailableException}] for topic [RobinTest4]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)
[2014-03-26 23:25:24,561] WARN Error while fetching metadata [{TopicMetadata for topic RobinTest4 -> 
No partition metadata for topic RobinTest4 due to kafka.common.LeaderNotAvailableException}] for topic [RobinTest4]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)
[2014-03-26 23:25:24,561] ERROR Failed to collate messages by topic, partition due to: Failed to fetch topic metadata for topic: RobinTest4 (kafka.producer.async.DefaultEventHandler)
[2014-03-26 23:25:24,669] WARN Error while fetching metadata [{TopicMetadata for topic RobinTest4 -> 
No partition metadata for topic RobinTest4 due to kafka.common.LeaderNotAvailableException}] for topic [RobinTest4]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)
[2014-03-26 23:25:24,683] WARN Error while fetching metadata [{TopicMetadata for topic RobinTest4 -> 
No partition metadata for topic RobinTest4 due to kafka.common.LeaderNotAvailableException}] for topic [RobinTest4]: class kafka.common.LeaderNotAvailableException  (kafka.producer.BrokerPartitionInfo)
[2014-03-26 23:25:24,683] ERROR Failed to collate messages by topic, partition due to: Failed to fetch topic metadata for topic: RobinTest4 (kafka.producer.async.DefaultEventHandler)
[2014-03-26 23:25:24,817] ERROR Failed to send requests for topics RobinTest4 with correlation ids in [0,8] (kafka.producer.async.DefaultEventHandler)
[2014-03-26 23:25:24,819] ERROR Error in handling batch of 1 events (kafka.producer.async.ProducerSendThread)
kafka.common.FailedToSendMessageException: Failed to send messages after 3 tries.
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:90)
	at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:104)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:87)
	at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:67)
	at scala.collection.immutable.Stream.foreach(Stream.scala:526)
	at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:66)
	at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)

;;;","10/Apr/14 07:06;hanish.bansal.agarwal;Could it be handled by just changing the message level from Error to INFO and presenting some informative message?

If that so this issue can be released in 0.8.1.1 .;;;","10/Apr/14 15:04;jbrosenberg@gmail.com;Well, not sure if that will do the right thing when there really is an ERROR when a producer gets an error sending to a broker.  However, I think would would make sense, is to only log ERROR if we've run out of retry attempts and have truly given up on sending the message.  So, if it's a failure that will be retried, it could be logged as WARN or even INFO.  But I'm guessing that change is not specific to the initial auto-topic-creation case, it affects logging for all cases.  But yeah, I think that sounds good to me.;;;","10/Apr/14 15:18;guozhang;Hi Jason, in the new producer the retry would not log as an error, and only when all retries have exhausted it will report an error. While at the same time both retry rate and error rate will be reported in DMX. Would this resolve your problem?;;;","10/Apr/14 19:12;jbrosenberg@gmail.com;Yes, I think it sounds like it would......when is the 'new producer' going to be available?;;;","10/Apr/14 19:56;guozhang;The new producer is already available now in the clients package in trunk.;;;","10/Apr/14 21:32;jbrosenberg@gmail.com;when will it be part of the 'released' version of kafka?  Or it already there?  We're currently using 0.8.0 still....;;;","10/Apr/14 21:44;guozhang;It will be in the 0.9 release, whose timeline has not been finalized yet. But we are shooting for this fall.;;;","11/Apr/14 00:54;jjkoshy;We should have an 0.8.2 release before that which has the new producer.;;;","20/Jan/15 10:51;omkreddy;This got fixed in new producer.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker IPv6 addresses parsed incorrectly,KAFKA-1123,12677819,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,kszafran,ottomata,ottomata,06/Nov/13 17:57,08/Apr/15 21:26,14/Jul/23 05:39,18/Sep/14 22:55,0.8.2.0,,,0.8.2.0,,,,,,,producer ,,,0,newbie,,,"It seems that broker addresses are parsed incorrectly when IPv6 addresses are supplied.  IPv6 addresses have colons in them, and Kafka seems to be interpreting the first : as the address:port separator.

I have only tried this with the console-producer --broker-list option, so I don't know if this affects anything deeper than the CLI.
",,junrao,kszafran,mingtao.zhang,nehanarkhede,ottomata,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/14 11:46;kszafran;KAFKA-1123.patch;https://issues.apache.org/jira/secure/attachment/12662273/KAFKA-1123.patch","16/Sep/14 22:04;kszafran;KAFKA-1123_v2.patch;https://issues.apache.org/jira/secure/attachment/12669237/KAFKA-1123_v2.patch","17/Sep/14 18:01;kszafran;KAFKA-1123_v3.patch;https://issues.apache.org/jira/secure/attachment/12669459/KAFKA-1123_v3.patch","18/Sep/14 22:11;kszafran;KAFKA-1123_v4.patch;https://issues.apache.org/jira/secure/attachment/12669828/KAFKA-1123_v4.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,357194,,,Wed Apr 08 21:26:45 UTC 2015,,,,,,,,,,"0|i1pks7:",357484,,junrao,,,,,,,,,,,,,,,,,,"25/Jul/14 16:44;junrao;We probably should add the IPV6 support asap. It seems that for a given string of ""host:port"", we can just look up for the last : to separate the host and port. This should cover both IPV4 and IPV6. We will need to do that for both the old and the new producer.;;;","16/Aug/14 11:46;kszafran;I have added methods to split the host and port, and to put them back together (surrounding IPv6 addresses with '[', ']').;;;","18/Aug/14 18:01;junrao;Thanks for the patch. Some comments.

1. Utils.parseHostPort(): Should we just move the logic into org.apache.kafka.common.utils.ClientUtils and reuse it in parseAndValidateAddresses()? That way, we have only one way of parsing the hostAndPort.

2. Do we need to wrap the ipv6 address with ""[]""? It seems that InetSocketAddress can take that format. However, I am not sure if this is the standard representation. When specifying ipv6-based broker list, do we expect the user to wrap the address part with ""[]""?

;;;","18/Aug/14 18:52;kszafran;-1. I agree that parseAndValidateAddress() could reuse ClientUtils (though it implies dealing with Tuple2 in Java).- (""core"" module is not visible from ""clients"")
2. InetSocketAddress can take the address without the square braces, but my intention with addressString() was to propagate this notation throughout the system. And as for the standard it would be RFC 2732 (http://www.ietf.org/rfc/rfc2732.txt).;;;","20/Aug/14 22:59;junrao;1. kafka-client doesn't depend on core. However, core already depends on kafka-client. So, we can put the parsing code in org.apache.kafka.common.utils.ClientUtils. Then, both kafka-client and core can use the common code.

2. Yes, that makes sense.;;;","15/Sep/14 03:59;nehanarkhede;Bump [~kszafran]. Are you going to get a chance to update your patch?;;;","15/Sep/14 20:31;kszafran;Sorry, I've been busy lately. I'll move the parsing util to ClientUtils this week, and post an updated patch.;;;","17/Sep/14 17:12;junrao;Thanks for the patch. Looks good overall. Some minor comments below.

1. ClientUitls: There is still one place that references ""bootstrap url"". We can change it to use the BOOTSTRAP constant.

2. Broker: redundant import of kafka.util.Utils.

3. DataGenerator, TopicMetadata: unused import kafka.util.Utils

4. UtilsTest.testParseHostPort seems can be testParsePort?

5. ClientUtilsTest: Do we need to test ""[2001:db8:85a3:8d3:1319:8a2e:370:7348]:1234"" twice?

6. Could you rebase?

;;;","17/Sep/14 18:01;kszafran;I have updated the patch according to your remarks.;;;","18/Sep/14 21:20;junrao;Thanks for the latest patch. In Broker, you used the simple string interpolator like the following. So far, we haven't adopted this syntax. Perhaps we can leave out those changes for now for consistency?

s""Broker id $id does not exist"";;;","18/Sep/14 22:11;kszafran;I have now removed the string interpolation syntax.;;;","18/Sep/14 22:55;junrao;Thanks for patch v4. +1 and committed to trunk.;;;","08/Apr/15 18:41;mingtao.zhang;Curious anyone tested in Java, i.e. new Producer<>(config). I am on 0.8.2.1, it's doesn't work with metadata.broker.list=[FEDC:BA98:7654:3210:FEDC:BA98:7654:3210]:9092;;;","08/Apr/15 21:26;junrao;The new producer needs to use bootstrap.servers, instead of metadata.broker.list.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DumpLogSegments tool should print absolute file name to report inconsistencies,KAFKA-1121,12677657,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,swapnilghike,swapnilghike,05/Nov/13 21:58,06/Nov/13 02:15,14/Jul/23 05:39,06/Nov/13 02:15,0.8.0,,,0.8.1,,,,,,,,,,0,,,,"Normally, the user would know where the index file lies. But in case of a script that continuously checks the index files for consistency, it will help to have the absolute file path printed in the output.",,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/13 22:08;swapnilghike;KAFKA-1121.patch;https://issues.apache.org/jira/secure/attachment/12612261/KAFKA-1121.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,357032,,,Tue Nov 05 22:08:28 UTC 2013,,,,,,,,,,"0|i1pjs7:",357322,,,,,,,,,,,,,,,,,,,,"05/Nov/13 22:08;swapnilghike;Created reviewboard https://reviews.apache.org/r/15248/
 against branch origin/trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka 0.8.1 overwrites previous per topic config changes,KAFKA-1119,12677431,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,04/Nov/13 19:06,08/Nov/13 18:03,14/Jul/23 05:39,08/Nov/13 01:46,0.8.1,,,,,,,,,,config,,,0,,,,kafka-topics --alter --config overwrites the previous per topic configs. There is no way to override more than one per topic config for the same topic,,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/13 18:10;nehanarkhede;KAFKA-1119.patch;https://issues.apache.org/jira/secure/attachment/12612409/KAFKA-1119.patch","06/Nov/13 18:13;nehanarkhede;KAFKA-1119_2013-11-06_10:13:13.patch;https://issues.apache.org/jira/secure/attachment/12612410/KAFKA-1119_2013-11-06_10%3A13%3A13.patch","07/Nov/13 17:50;nehanarkhede;KAFKA-1119_2013-11-07_09:50:20.patch;https://issues.apache.org/jira/secure/attachment/12612644/KAFKA-1119_2013-11-07_09%3A50%3A20.patch","07/Nov/13 18:17;nehanarkhede;KAFKA-1119_2013-11-07_10:17:14.patch;https://issues.apache.org/jira/secure/attachment/12612651/KAFKA-1119_2013-11-07_10%3A17%3A14.patch","08/Nov/13 01:07;nehanarkhede;KAFKA-1119_2013-11-07_17:07:18.patch;https://issues.apache.org/jira/secure/attachment/12612752/KAFKA-1119_2013-11-07_17%3A07%3A18.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,356806,,,Fri Nov 08 01:46:01 UTC 2013,,,,,,,,,,"0|i1pief:",357096,,,,,,,,,,,,,,,,,,,,"05/Nov/13 02:33;nehanarkhede;Turns out the right way to use the existing tool to specify more than one override is -

./bin/kafka-topics.sh --zookeeper localhost:2181 --topic topicconfigtest --alter --config retention.ms=180000 --config segment.ms=180000

;;;","05/Nov/13 05:25;jkreps;Hmm, but it looks like the current behavior is to just blindly set whatever properties are given on the command line as the FULL set of properties. So if I say
./bin/kafka-topics.sh --alter --config retention.ms=180000
I might think I am just changing the retention.ms but in reality I am unsetting any previous per-topic configs (whatever they may be) and setting retention.ms.

Currently the code looks like this:
      val configs = parseTopicConfigs(opts)
      AdminUtils.changeTopicConfig(zkClient, topic, configs)

I think it might be more intuitive if we interpreted the args as a diff to apply to the current settings. So the code would be soemthing like:
      val configs = AdminUtils.fetchTopicConfig(zkClient, topic)
      configs.addAll(parseTopicConfigs(opts))
      AdminUtils.changeTopicConfig(zkClient, topic, configs)

This does have a downside, though. How do you remove an override? Let's say there is a default of X, setting X actually sets a topic override of X which is different from having no setting for the config (because if you change the default that topic would be unaffected). This is not hugely important but some options for handling this would be
  1. No value means delete the config: --config x=
  2. Add a new option for unconfiguring: --config x=12 and --unconfig x
;;;","05/Nov/13 17:03;nehanarkhede;Yes, removing overrides is an issue today. I like option #1 where no value means delete the per-topic override.;;;","06/Nov/13 18:10;nehanarkhede;Created reviewboard https://reviews.apache.org/r/15274/
 against branch trunk;;;","06/Nov/13 18:13;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/15274/
 against branch trunk;;;","07/Nov/13 17:50;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/15274/
 against branch trunk;;;","07/Nov/13 18:18;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/15274/
 against branch trunk;;;","08/Nov/13 01:07;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/15274/
 against branch trunk;;;","08/Nov/13 01:46;nehanarkhede;Thanks for the reviews! Pushed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log.cleanup.interval.mins property should be renamed,KAFKA-1113,12677104,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,vkolodrevskiy,vkolodrevskiy,01/Nov/13 16:45,01/Nov/13 17:13,14/Jul/23 05:39,01/Nov/13 17:09,0.8.1,,,,,,,,,,config,,,0,,,,"In trunk log.cleanup.interval.mins should be renamed to log.retention.check.interval.ms in config/server.properties

Also, documentation should be updated.",,nehanarkhede,vkolodrevskiy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,356480,,,Fri Nov 01 17:13:47 UTC 2013,,,,,,,,,,"0|i1pgdr:",356768,,,,,,,,,,,,,,,,,,,,"01/Nov/13 17:13;nehanarkhede;Filed KAFKA-1114 for adding 0.8.1 documentation;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
broker can not start itself after kafka is killed with -9,KAFKA-1112,12677081,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,jkreps,KaneK,KaneK,01/Nov/13 15:03,14/Jan/15 05:22,14/Jul/23 05:39,19/Nov/13 02:33,0.8.0,0.8.1,,0.8.1,,,,,,,log,,,3,,,,"When I kill kafka with -9, broker cannot start itself because of corrupted index logs. I think kafka should try to delete/rebuild indexes itself without manual intervention. ",,alexismidon,davidlao,dgoya,dmaverick,guozhang,jkreps,junrao,KaneK,kzadorozhny,mazhar.shaikh.in,nehanarkhede,noslowerdna,qdutj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-757,,,,,,,,,,,,,"13/Nov/13 05:04;jkreps;KAFKA-1112-v1.patch;https://issues.apache.org/jira/secure/attachment/12613527/KAFKA-1112-v1.patch","14/Nov/13 20:57;jkreps;KAFKA-1112-v2.patch;https://issues.apache.org/jira/secure/attachment/12613931/KAFKA-1112-v2.patch","15/Nov/13 18:04;jkreps;KAFKA-1112-v3.patch;https://issues.apache.org/jira/secure/attachment/12614094/KAFKA-1112-v3.patch","18/Nov/13 22:10;junrao;KAFKA-1112-v4.patch;https://issues.apache.org/jira/secure/attachment/12614489/KAFKA-1112-v4.patch","05/Nov/13 18:57;guozhang;KAFKA-1112.out;https://issues.apache.org/jira/secure/attachment/12612219/KAFKA-1112.out",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,356457,,,Wed Jan 14 05:22:10 UTC 2015,,,,,,,,,,"0|i1pg8n:",356745,,,,,,,,,,,,,,,,,,,,"01/Nov/13 17:22;dmaverick;We've also faced with such behavior. Moreover, it doesn't fail right after startup, it start listening for requests(at least open the port) before checking  the index and syncing internal state. Thus it kind of difficult to figure out from startup script whether Kafka actually started without adding some ugly sleeps.;;;","05/Nov/13 18:43;nehanarkhede;Stack trace -

[2013-11-01 17:46:02,685] INFO Loading log 'foo-4' (kafka.log.LogManager)
[2013-11-01 17:46:04,898] FATAL Fatal error during KafkaServerStable startup. Prepare to shutdown (kafka.server.KafkaServerStartable)
java.lang.IllegalArgumentException: requirement failed: Corrupt index found, index file (/mnt/u001/temp/kafka-logs/foo-4/00000000000000000000.index) has non-zero size but the last offset is 0 and the base offset is 0
at scala.Predef$.require(Predef.scala:145)
at kafka.log.Log$$anonfun$loadSegments$5.apply(Log.scala:161)
at kafka.log.Log$$anonfun$loadSegments$5.apply(Log.scala:160)
at scala.collection.Iterator$class.foreach(Iterator.scala:631)
at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:474)
at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
at scala.collection.JavaConversions$JCollectionWrapper.foreach(JavaCo;;;","05/Nov/13 18:57;guozhang;Attached another stack trace with more debugging turned on. The root cause is that when we load the index file, the initial size is set to the limit of the file, and hence the position is pointing to the last entry. In most cases the last entry will be 0, and the recoveryLog process will skip since it shows the latest offset is smaller than the checkpoint. But when doing the sanity check it finds the number of entries is non-zero (actually the max number of entries) while the last offset is equal to the base offset since reading the last entry gives you 0 relative offset.;;;","08/Nov/13 13:21;dmaverick;Could be related to this issue KAFKA-757;;;","08/Nov/13 19:53;guozhang;Yes. We have realized this bug before and tried to fix it, but it seems we still missed some cases.;;;","13/Nov/13 05:04;jkreps;The way the check was supposed to work was this: if the last offset in the file is the recoveryPoint-1 then skip the recovery (because the whole file is flushed). The way this was implemented was by using the last entry in the index to find the final message.

Overall I feel this is a bit of a hack, but we wanted to separate out the ""fsync is async"" feature from a full incremental recovery implementation that only recovers unflushed data.

The immediate problem was that we broke the short circuit by adding code to try to handle a corner case: what if log is truncated after to a flush and hence the end of the log is < recovery point. This was just totally broken and we were short circuiting out of the check in virtually all cases including corrupt index.

This issue wasn't caught because there was a bug in the log corruption unit test that gave a false pass on all index corruptions. :-(

The fix is the following:
1. Fix the logical bug
2. Add LogSegment.needsRecovery() which is a more paranoid version of what we were doing before that attempts to be safe regardless of any index or log corruption that may have occurred. Having this method here is a little hacky but probably okay until we get a full incremental recovery impl.
3. Fix the unit test that covers this.
;;;","13/Nov/13 17:25;junrao;Thanks for the patch. A few comments.

1. I am a bit concerned of depending on a potentially corrupted index to look for recoveryPoint - 1 in LogSegment.needsRecovery(). If the index points to an arbitrary position in FileMessageSet, the offset value that FileMessageSet.searchFor() finds  is garbage. If that value happens to be larger than targetOffset, we will assume that we find targetOffset, but in fact we haven't.

2. LogTest.testCorruptLog(): Is the println statement needed?

3. Could you rebase?;;;","13/Nov/13 17:56;nehanarkhede;Thanks for the patch. Few comments -

1. In the most common case of needsRecovery, the position of the last entry will be zero. In this case, we will search the entire log segment up until the recovery point. This will slow down server startup but probably only when we really need recovery.
2. LogSegment: We have to be carefully => We have to be careful
3. Log: If sanityCheck throws an exception, can we automatically invoke index rebuild instead of bailing out?
4. Could you rebase?
5. Could you give the patch review tool a spin? The setup is minimal and we can save time for this and future reviews - https://cwiki.apache.org/confluence/display/KAFKA/Kafka+patch+review+tool#Kafkapatchreviewtool-1.Setup
Usage: 
python kafka-patch-review.py -j KAFKA-1112 -b trunk;;;","13/Nov/13 17:58;jkreps;Actually I am attempting to cover every possible case here, so the only case that should go through is the one where the offset of the final message is recoveryPoint-1 exactly. Notice that the unit test actually runs through 50 cases of random garbage appended to the index so assuming that test is write I think this does work.;;;","13/Nov/13 18:23;guozhang;Regarding Jun's comment #1, I am more concerned about using searchFor function on a FileMessageSet that might be corrupted. From the code it seems if FileMessageSet is corrupted the searchFor function may actually not return due to variable position not monotonically increasing?;;;","14/Nov/13 05:54;davidlao;I hitting this exact issue. For what it's worth the content of the corrupt index file is consist of 00's for the entire file.;;;","14/Nov/13 07:14;davidlao;Jay , can you provide a patch for the 0.8 branch as well? Thanks.;;;","14/Nov/13 20:57;jkreps;Added a new patch that addresses issues raised.

Jun
1. I don't think this is true. The check is for exact match.
2. Removed.
3. Done

Neha
1. I think I am handling this--in the case of zero we don't do a full scan.
2. Ack, fixed.
3. Well the sanity check is POST recovery. So if we have a corrupt index after recovery we have a bug, I don't think we should automatically try to recovery from this (that would be another recovery).
4. done
5. Yeah, haven't had time yet.;;;","15/Nov/13 05:14;junrao;The following is my confusion.

The patch relies on a potentially corrupted index to find the right starting position in the segment file. What if the starting position given by the last index entry is corrupted? Then, the position could point to the middle of a message in the segment file. Then, the offset value we read from the segment file could be anything. If that value happens to match recoverPoint - 1, we could think no recovery is needed, but the segment file is actually corrupted. 

Similarly, even if the index file is not corrupted, the segment file could still be corrupted before recoverPoint - 1 (since the unit of flushing is a page). It's also possible that we read a corrupted piece of data as the offset that happens to match recoverPoint - 1, and therefore incorrectly think that recovery is not needed.;;;","15/Nov/13 17:14;jkreps;David, this should not be happening in 0.8. If it is I suspect it is a different problem that causes the same bad outcome. Are you seeing this on 0.8? If so how reproducable is it?;;;","15/Nov/13 17:20;jkreps;Jun, this is true.

However, if you think about it recovery of the log has the same problem. We read a message and then compare it to its CRC. The CRC is a 32 bit number. The crc could certainly match the message by chance.

In this case we compare to a 64 bit number so this should be less likely. But in reality there are many rare events here: (1) we hard crash, (2) hard crash leads to corruption, (3) corruption of index points to a location that exactly matches the recovery offset.

In general I think peoples concern with this approach is that it is just kind of hacky. I agree with this complaint and am sort of disappointed with this set of changes overall.

I will post a slightly more paranoid version of the check, and then let's discuss that.;;;","15/Nov/13 18:04;jkreps;Okay here is a maximally paranoid patch.;;;","15/Nov/13 18:07;guozhang;How about we resort back to the clean shutdown file for recovery checking, and if recovery is needed, we can use the recovery point to optimize recovery overhead.;;;","15/Nov/13 20:43;jkreps;Yeah I would not be opposed to that as an alternative. Both are really a hack.

I guess the questions is what should the end state be?;;;","18/Nov/13 17:13;junrao;Thinking about this a bit more. The end state is that we want to only recover the portion of the log segment from the recovery point, instead of recovering the whole log segment. The dilemma is that we are not sure what portion of the index is valid. Scanning from the beginning of the log segment defeats the purpose of incremental recovery. One possible solution is to checkpoint an index recovery point, in addition to the recovery offset per log. The index recovery point is the # of valid index entries in the segment to which the recovery offset belongs. This way, on startup, we will be sure that the data in the last valid index entry is not corrupted and we can use it to quickly locate the recovery offset in the log file.

;;;","18/Nov/13 18:36;guozhang;Did some research on network about ""fsync"", and it seems fsync can be reliable even with disk's block-write behavior since it is sequential, which means even file system crashed during fsync we will not expect random behavior.

;;;","18/Nov/13 18:57;nehanarkhede;[~junrao] This approach seems reasonable unless I'm missing any caveats in Log. [~jkreps] what do you think?;;;","18/Nov/13 20:02;jkreps;Yeah at a high-level there are a couple of things we could do:
1. Non-incremental 
    a. Harden the current approach (what the attached patches do)
    b. Use the clean shutdown file 
2. Implement incremental recovery (what Jun is proposing)

All of these are good. 1a is implemented, but is arguably gross. I am open to 1b or 2 or a short-term/long-term thing.

For 2 I think the details to figure out would be 
1. OffsetCheckpoint is shared so adding the position to that file will impact other use cases how will that be handled?
2. I suspect that if we want to move to positions we should do something like (file, log_position, index_position) rather than a mixture of logical and physical.
3. We need to ensure that log compaction is thought through. This could cause the physical position to change. That could be fine but we need to reason through it.
4. We need to ensure that we handle truncation which implies that a position X could be stable, then deleted, then rewritten differently without flush. This may be fine we just have to think it through.;;;","18/Nov/13 22:10;junrao;Ok, so it seems that the end state is not that simple and may need some more thoughts. I took patch v3 , removed the recovery part in LogSegment and replaced it with the simpler approach using the clean shutdown file.;;;","18/Nov/13 22:53;nehanarkhede;Thanks for the patch, Jun! Overall, looks good (+1). Few minor comments that you can address on checkin -

1. Log
- okay we need to actually recovery this log => okay we need to actually recover this log

2. OffsetIndex
- In sanityCheck(), in one error message, we print the index file's absolute path and in another, we print only the name. Can we standardize on one? It is better to print the entire path since we can have more than one data directories.
;;;","18/Nov/13 23:07;jkreps;+1 lgtm.;;;","19/Nov/13 02:33;junrao;Thanks for the reviews. Committed to trunk after addressing Neha's comments.;;;","25/Dec/13 19:25;dgoya;Commenting here as requested.

After migrating a cluster from 0.8.0 to 0.8.1 (trunk/87efda7) I had a few brokers that wouldn't come up.

This is the exception I ran into, I was able to fix it by deleting the /data/kafka/logs/Events2-124/ directory.  That directory contained a non zero size index file and a zero size log file.  I had a bunch of these directories scattered around the cluster.  I suspect they were there from partition reassignment failures which happened when the cluster was at 0.8.0.

[2013-12-18 02:40:37,163] FATAL Fatal error during KafkaServerStable startup. Prepare to shutdown (kafka.server.KafkaServerStartable)
java.lang.IllegalArgumentException: requirement failed: Corrupt index found, index file (/data/kafka/logs/Events2-124/00000000000000000000.index) has non-zero size but the last offset is 0 and the base offset is 0
	at scala.Predef$.require(Predef.scala:145)
	at kafka.log.Log$$anonfun$loadSegments$5.apply(Log.scala:160)
	at kafka.log.Log$$anonfun$loadSegments$5.apply(Log.scala:159)
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)
	at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:474)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
	at scala.collection.JavaConversions$JCollectionWrapper.foreach(JavaConversions.scala:495)
	at kafka.log.Log.loadSegments(Log.scala:159)
	at kafka.log.Log.<init>(Log.scala:64)
	at kafka.log.LogManager$$anonfun$loadLogs$1$$anonfun$apply$3.apply(LogManager.scala:120)
	at kafka.log.LogManager$$anonfun$loadLogs$1$$anonfun$apply$3.apply(LogManager.scala:115)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
	at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
	at kafka.log.LogManager$$anonfun$loadLogs$1.apply(LogManager.scala:115)
	at kafka.log.LogManager$$anonfun$loadLogs$1.apply(LogManager.scala:107)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32)
	at kafka.log.LogManager.loadLogs(LogManager.scala:107)
	at kafka.log.LogManager.<init>(LogManager.scala:59);;;","12/Jul/14 00:09;alexismidon;Hello,

I suffered from the same error using Kafka 0.8.1. Should I reopen this issue or create a new one?

{code}
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,696 INFO main kafka.server.KafkaServer.info - [Kafka Server 847605514], starting
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,698 INFO main kafka.server.KafkaServer.info - [Kafka Server 847605514], Connecting to zookeeper on zk-main0.XXX:2181,zk-main1.XXX:2181,zk-main2.XXXX:2181/production/kafka/main
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,708 INFO ZkClient-EventThread-14-zk-main0.XXX.com:2181,zk-main1.XXX.com:2181,zk-main2.XXX.com:2181,zk-main3.XXX.com:2181,zk-main4.XXX.com:2181/production/kafka/main org.I0Itec.zkclient.ZkEventThread.run - Starting ZkClient event thread.
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,714 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:zookeeper.version=3.3.3-1203054, built on 11/17/2011 05:47 GMT
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,714 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:host.name=i-6b948138.inst.aws.airbnb.com
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,714 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:java.version=1.7.0_55
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,715 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:java.vendor=Oracle Corporation
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,715 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:java.home=/usr/lib/jvm/jre-7-oracle-x64/jre
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,715 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:java.class.path=libs/snappy-java-1.0.5.jar:libs/scala-library-2.10.1.jar:libs/slf4j-api-1.7.2.jar:libs/jopt-simple-3.2.jar:libs/metrics-annotation-2.2.0.jar:libs/log4j-1.2.15.jar:libs/kafka_2.10-0.8.1.jar:libs/zkclient-0.3.jar:libs/zookeeper-3.3.4.jar:libs/metrics-core-2.2.0.jar
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,715 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,716 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:java.io.tmpdir=/tmp
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,716 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:java.compiler=<NA>
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,716 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:os.name=Linux
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,716 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:os.arch=amd64
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,717 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:os.version=3.2.0-61-virtual
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,717 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:user.name=kafka
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,717 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:user.home=/srv/kafka
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,717 INFO main org.apache.zookeeper.ZooKeeper.logEnv - Client environment:user.dir=/srv/kafka/kafka_2.10-0.8.1
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,718 INFO main org.apache.zookeeper.ZooKeeper.<init> - Initiating client connection, connectString=zk-main0.XXX.com:2181,zk-main1.XXX.com:2181,zk-main2.XXX.com:2181,zk-main3.XXX.com:2181,zk-main4.XXX.com:2181/production/kafka/main sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@4758af63
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,733 INFO main-SendThread() org.apache.zookeeper.ClientCnxn.startConnect - Opening socket connection to server zk-main1.XXX.com/10.12.135.61:2181
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,738 INFO main-SendThread(zk-main1.XXX.com:2181) org.apache.zookeeper.ClientCnxn.primeConnection - Socket connection established to zk-main1.XXX.com/10.12.135.61:2181, initiating session
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,745 INFO main-SendThread(zk-main1.XXX.com:2181) org.apache.zookeeper.ClientCnxn.readConnectResult - Session establishment complete on server zk-main1.XXX.com/10.12.135.61:2181, sessionid = 0x646838f07761601, negotiated timeout = 6000
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,747 INFO main-EventThread org.I0Itec.zkclient.ZkClient.processStateChanged - zookeeper state changed (SyncConnected)
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,961 INFO main kafka.log.LogManager.info - Found clean shutdown file. Skipping recovery for all logs in data directory '/mnt/kafka_logs'
2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,962 INFO main kafka.log.LogManager.info - Loading log 'flog-30'
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg 2014-07-11 - 00:53:18,349 FATAL main kafka.server.KafkaServerStartable.fatal - Fatal error during KafkaServerStable startup. Prepare to shutdown
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg java.lang.IllegalArgumentException: - requirement failed: Corrupt index found, index file (/mnt/kafka_logs/flog-30/00000000000121158146.index) has non-zero size but the last offset is 121158146 and the base offset is 121158146
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.Predef$.require(Predef.scala:233)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.OffsetIndex.sanityCheck(OffsetIndex.scala:352)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.Log$$anonfun$loadSegments$5.apply(Log.scala:159)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.Log$$anonfun$loadSegments$5.apply(Log.scala:158)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.Iterator$class.foreach(Iterator.scala:727)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.Log.loadSegments(Log.scala:158)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.Log.<init>(Log.scala:64)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.LogManager$$anonfun$loadLogs$1$$anonfun$apply$4.apply(LogManager.scala:118)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.LogManager$$anonfun$loadLogs$1$$anonfun$apply$4.apply(LogManager.scala:113)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:105)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.LogManager$$anonfun$loadLogs$1.apply(LogManager.scala:113)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.LogManager$$anonfun$loadLogs$1.apply(LogManager.scala:105)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.LogManager.loadLogs(LogManager.scala:105)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.log.LogManager.<init>(LogManager.scala:57)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.server.KafkaServer.createLogManager(KafkaServer.scala:275)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.server.KafkaServer.startup(KafkaServer.scala:72)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:34)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.Kafka$.main(Kafka.scala:46)
2014-07-11T00:53:18+00:00 i-6b948138 local3.emerg  -    at kafka.Kafka.main(Kafka.scala)
2014-07-11T00:53:18+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:18,351 INFO main kafka.server.KafkaServer.info - [Kafka Server 847605514], shutting down
2014-07-11T00:53:18+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:18,353 INFO ZkClient-EventThread-14-zk-main0.XXX.com:2181,zk-main1.XXX.com:2181,zk-main2.XXX.com:2181,zk-main3.XXX.com:2181,zk-main4.XXX.com:2181/production/kafka/main org.I0Itec.zkclient.ZkEventThread.run - Terminate ZkClient event thread.
{code};;;","16/Jul/14 15:22;junrao;This seems to happen on a clean restart (the following log entry indicates there is no log recovery). So, this could be a different issue. Could you open a separate jira?

2014-07-11T00:53:17+00:00 i-6b948138 local3.info 2014-07-11 - 00:53:17,961 INFO main kafka.log.LogManager.info - Found clean shutdown file. Skipping recovery for all logs in data directory '/mnt/kafka_logs';;;","21/Jul/14 17:49;alexismidon;I created https://issues.apache.org/jira/browse/KAFKA-1554.
thanks;;;","14/Jan/15 05:22;qdutj;I also met similar issues in Kafka-2.9.2-0.8.1 ( after an uncleanShutDown --- kill -9. ). 
I am afraid that I cannot provide more logs since it was months ago: 
******************************************************************************************************************************
FATAL Fatal error during KafkaServerStable startup. Prepare to shutdown (kafka.server.KafkaServerStartable)
java.lang.IllegalArgumentException: requirement failed: Corrupt index found, index file (/home/storm/kafka_2.9.2-0.8.1/logs/tracker-1/00000000000006911940.index) has non-zero size but the last offset is 6911940 and the base offset is 6911940
******************************************************************************************************************************
Here is my analysis:
Once unclean shutdown, Kafka needs to rebuild index files for her logsegments when restarted.
If InvalidOffsetException is thrown when appending entries to an index file,    recoverLog()  function will handle it by truncating the log segment to baseOffset (==> lastIndexEntry == baseOffset ). 
However, since the index files are written by mmap, OS will flush the update to disks in time. It means the index file already has some entries now.
I am a beginner for scala, just guessing the logSegment with InvalidOffsetException thrown has been passed by the iterator, so it was not deleted in fact? (I mean all log segments after it will be deleted in recoverLog() in log.scala. )
This log segment missed to delete could not pass sanityCheck(), since it has index file with non-zero size but lastIndexEntry == baseOffset ( it was truncated to baseOffset when handling the InvalidOffsetException. ).
Quite sorry if any mistake above.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Need to fix GC log configuration code, not able to override KAFKA_GC_LOG_OPTS",KAFKA-1109,12676674,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,omkreddy,vkolodrevskiy,vkolodrevskiy,30/Oct/13 17:01,26/Jan/15 02:43,14/Jul/23 05:39,26/Jan/15 02:43,0.8.0,0.8.1,,0.9.0.0,,,,,,,,,,0,,,,"kafka-run-class.sh contains GC log code:

# GC options
GC_FILE_SUFFIX='-gc.log'
GC_LOG_FILE_NAME=''
if [ ""$1"" = ""daemon"" ] && [ -z ""$KAFKA_GC_LOG_OPTS""] ; then
  shift
  GC_LOG_FILE_NAME=$1$GC_FILE_SUFFIX
  shift
  KAFKA_GC_LOG_OPTS=""-Xloggc:$LOG_DIR/$GC_LOG_FILE_NAME -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps ""
fi

So when in my scripts I start kafka and want to override KAFKA_GC_LOG_OPTS by exporting new values I get:

Exception in thread ""main"" java.lang.NoClassDefFoundError: daemon
Caused by: java.lang.ClassNotFoundException: daemon

That's because shift is not done when KAFKA_GC_LOG_OPTS is set and ""daemon"" is passed as main class.

I suggest to replace it with this code:

# GC options
GC_FILE_SUFFIX='-gc.log'
GC_LOG_FILE_NAME=''
if [ ""$1"" = ""daemon"" ] && [ -z ""$KAFKA_GC_LOG_OPTS"" ] ; then
  shift
  GC_LOG_FILE_NAME=$1$GC_FILE_SUFFIX
  shift
  KAFKA_GC_LOG_OPTS=""-Xloggc:$LOG_DIR/$GC_LOG_FILE_NAME -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps ""
else
    if [ ""$1"" = ""daemon"" ] && [ ""$KAFKA_GC_LOG_OPTS"" != """" ] ; then
      shift 2
    fi
fi",*nix,guozhang,junrao,nehanarkhede,omkreddy,vkolodrevskiy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/15 12:08;omkreddy;KAFKA-1109.patch;https://issues.apache.org/jira/secure/attachment/12693290/KAFKA-1109.patch","06/Nov/13 16:01;vkolodrevskiy;KAFKA_1109_fix.patch;https://issues.apache.org/jira/secure/attachment/12612392/KAFKA_1109_fix.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,356106,,,Mon Jan 26 02:43:51 UTC 2015,,,,,,,,,,"0|i1pe33:",356394,,,,,,,,,,,,,,,,,,,,"03/Nov/13 23:58;junrao;Thanks for pointing this out. Could you submit the proposed changes as a patch?;;;","06/Nov/13 16:01;vkolodrevskiy;Attached patch for fix;;;","06/Nov/13 22:02;guozhang;Actually, why the gc file name should be coupled with the ""daemon"" option?;;;","11/Nov/13 05:13;junrao;Does the patch in KAFKA-1127 work for you?;;;","20/Jan/15 12:08;omkreddy;Created reviewboard https://reviews.apache.org/r/30073/diff/
 against branch origin/trunk;;;","26/Jan/15 02:43;nehanarkhede;Thanks for the patch. Pushed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when controlled shutdown attempt fails, the reason is not always logged",KAFKA-1108,12676488,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ewencp,jbrosenberg@gmail.com,jbrosenberg@gmail.com,29/Oct/13 20:37,17/Oct/14 22:07,14/Jul/23 05:39,17/Oct/14 22:07,,,,0.8.2.0,,,,,,,,,,0,newbie,,,"In KafkaServer.controlledShutdown(), it initiates a controlled shutdown, and then if there's a failure, it will retry the controlledShutdown.

Looking at the code, there are 2 ways a retry could fail, one with an error response from the controller, and this messaging code:

{code}
info(""Remaining partitions to move: %s"".format(shutdownResponse.partitionsRemaining.mkString("","")))
info(""Error code from controller: %d"".format(shutdownResponse.errorCode))
{code}

Alternatively, there could be an IOException, with this code executed:

{code}
            catch {
              case ioe: java.io.IOException =>
                channel.disconnect()
                channel = null
                // ignore and try again
            }
{code}

And then finally, in either case:

{code}
          if (!shutdownSuceeded) {
            Thread.sleep(config.controlledShutdownRetryBackoffMs)
            warn(""Retrying controlled shutdown after the previous attempt failed..."")
          }
{code}

It would be nice if the nature of the IOException were logged in either case (I'd be happy with an ioe.getMessage() instead of a full stack trace, as kafka in general tends to be too willing to dump IOException stack traces!).

I suspect, in my case, the actual IOException is a socket timeout (as the time between initial ""Starting controlled shutdown...."" and the first ""Retrying..."" message is usually about 35 seconds (the socket timeout + the controlled shutdown retry backoff).  So, it would seem that really, the issue in this case is that controlled shutdown is taking too long.  It would seem sensible instead to have the controller report back to the server (before the socket timeout) that more time is needed, etc.",,donnchadh,ewencp,guozhang,jbrosenberg@gmail.com,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/14 18:55;ewencp;KAFKA-1108.patch;https://issues.apache.org/jira/secure/attachment/12675079/KAFKA-1108.patch","16/Oct/14 20:53;ewencp;KAFKA-1108_2014-10-16_13:53:11.patch;https://issues.apache.org/jira/secure/attachment/12675345/KAFKA-1108_2014-10-16_13%3A53%3A11.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,355920,,,Fri Oct 17 22:07:28 UTC 2014,,,,,,,,,,"0|i1pcxr:",356208,,,,,,,,,,,,,,,,,,,,"04/Sep/14 22:04;guozhang;Moving to 0.9 for now.;;;","15/Oct/14 18:55;ewencp;Created reviewboard https://reviews.apache.org/r/26770/diff/
 against branch origin/trunk;;;","16/Oct/14 20:53;ewencp;Updated reviewboard https://reviews.apache.org/r/26770/diff/
 against branch origin/trunk;;;","17/Oct/14 22:07;nehanarkhede;Thanks for the patch. Pushed to trunk and 0.8.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker unnecessarily recovers all logs when upgrading from 0.8 to 0.8.1,KAFKA-1107,12676454,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,29/Oct/13 18:16,01/Nov/13 04:20,14/Jul/23 05:39,01/Nov/13 04:20,0.8.1,,,,,,,,,,log,,,0,,,,"While upgrading from 0.8 to 0.8.1, broker unnecessarily recovers all logs",,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/13 17:25;nehanarkhede;KAFKA-1107.patch;https://issues.apache.org/jira/secure/attachment/12611430/KAFKA-1107.patch","31/Oct/13 17:54;nehanarkhede;KAFKA-1107_2013-10-31_10:54:25.patch;https://issues.apache.org/jira/secure/attachment/12611439/KAFKA-1107_2013-10-31_10%3A54%3A25.patch","31/Oct/13 22:18;nehanarkhede;KAFKA-1107_2013-10-31_15:18:05.patch;https://issues.apache.org/jira/secure/attachment/12611499/KAFKA-1107_2013-10-31_15%3A18%3A05.patch","31/Oct/13 22:22;nehanarkhede;KAFKA-1107_2013-10-31_15:22:03.patch;https://issues.apache.org/jira/secure/attachment/12611501/KAFKA-1107_2013-10-31_15%3A22%3A03.patch","31/Oct/13 22:28;nehanarkhede;KAFKA-1107_2013-10-31_15:28:09.patch;https://issues.apache.org/jira/secure/attachment/12611503/KAFKA-1107_2013-10-31_15%3A28%3A09.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,355886,,,Fri Nov 01 04:20:12 UTC 2013,,,,,,,,,,"0|i1pcq7:",356174,,,,,,,,,,,,,,,,,,,,"31/Oct/13 17:26;nehanarkhede;Created reviewboard https://reviews.apache.org/r/15137/
;;;","31/Oct/13 17:54;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/15137/
;;;","31/Oct/13 22:18;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/15137/
;;;","31/Oct/13 22:22;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/15137/
;;;","31/Oct/13 22:28;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/15137/
;;;","01/Nov/13 04:20;nehanarkhede;Thanks for the review, pushed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer uses two zkclients,KAFKA-1103,12675633,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,jjkoshy,jjkoshy,25/Oct/13 01:00,22/Nov/13 18:14,14/Jul/23 05:39,22/Nov/13 17:16,,,,0.8.1,,,,,,,,,,0,,,,".. which is very confusing when debugging consumer logs. I don't remember any good reason for this, and we should get rid of the one instantiated in ZookeeperTopicEventWatcher if possible.",,guozhang,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/13 23:30;guozhang;KAFKA-1103.patch;https://issues.apache.org/jira/secure/attachment/12614511/KAFKA-1103.patch","20/Nov/13 20:59;guozhang;KAFKA-1103_2013-11-20_12:59:09.patch;https://issues.apache.org/jira/secure/attachment/12614988/KAFKA-1103_2013-11-20_12%3A59%3A09.patch","21/Nov/13 19:22;guozhang;KAFKA-1103_2013-11-21_11:22:04.patch;https://issues.apache.org/jira/secure/attachment/12615168/KAFKA-1103_2013-11-21_11%3A22%3A04.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,355210,,,Fri Nov 22 18:14:21 UTC 2013,,,,,,,,,,"0|i1p8k7:",355498,,,,,,,,,,,,,,,,,,,,"18/Nov/13 22:33;guozhang;Since this is closely related to KAFKA-1004, will merge the two here.;;;","18/Nov/13 23:30;guozhang;Created reviewboard https://reviews.apache.org/r/15659/
 against branch origin/trunk;;;","20/Nov/13 20:59;guozhang;Updated reviewboard https://reviews.apache.org/r/15659/
 against branch origin/trunk;;;","20/Nov/13 23:21;guozhang;[~jjkoshy], can you give another round of review for this one? ;;;","21/Nov/13 16:38;junrao;The latest patch reverted some of the old commits. Could you rebase your branch and submit a new patch?;;;","21/Nov/13 19:22;guozhang;Updated reviewboard https://reviews.apache.org/r/15659/
 against branch origin/trunk;;;","22/Nov/13 05:23;junrao;Do you really intend to delete the following in the patch? 

system_test/migration_tool_testsuite/0.7/config/test-log4j.properties:deleted;;;","22/Nov/13 05:38;guozhang;Wired. I already did a rebase and git diff did not show that.. Let me re-try.;;;","22/Nov/13 17:16;junrao;Thanks for the patch. +1. Committed to trunk after removing the unnecessary change to log4j property file.;;;","22/Nov/13 18:14;jjkoshy;I think the issue is that the patch-review tool compares both the feature branch and the origin branch at the current HEAD. We should just have it take the diff from the last rebase point. I'll file a jira and provide a patch for that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
metrics shouldn't have generation/timestamp specific names,KAFKA-1100,12675195,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jbrosenberg@gmail.com,jbrosenberg@gmail.com,23/Oct/13 06:14,26/Jun/15 10:03,14/Jul/23 05:39,26/Jun/15 10:03,,,,0.8.2.1,,,,,,,,,,1,,,,"I've noticed that there are several metrics that seem useful for monitoring overtime, but which contain generational timestamps in the metric name.

We are using yammer metrics libraries to send metrics data in a background thread every 10 seconds (to kafka actually), and then they eventually end up in a metrics database (graphite, opentsdb).  The metrics then get graphed via UI, and we can see metrics going way back, etc.

Unfortunately, many of the metrics coming from kafka seem to have metric names that change any time the server or consumer is restarted, which makes it hard to easily create graphs over long periods of time (spanning app restarts).

For example:

names like: 
kafka.consumer.FetchRequestAndResponseMetrics....square-1371718712833-e9bb4d10-0-508818741-AllBrokersFetchRequestRateAndTimeMs

or: 
kafka.consumer.ZookeeperConsumerConnector...topicName.....square-1373476779391-78aa2e83-0-FetchQueueSize

In our staging environment, we have our servers on regular auto-deploy cycles (they restart every few hours).  So just not longitudinally usable to have metric names constantly changing like this.

Is there something that can easily be done?  Is it really necessary to have so much cryptic info in the metric name?",,adenysenko,grayaii,jbrosenberg@gmail.com,jjkoshy,junrao,omkreddy,otis,swapnilghike,vladimir.tretyakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1481,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,354815,,,Fri Jun 26 10:03:23 UTC 2015,,,,,,,,,,"0|i1p64n:",355104,,,,,,,,,,,,,,,,,,,,"23/Oct/13 06:25;swapnilghike;Hi Jason, at LinkedIn, we use wildcards/regexes to create graphs from such mbeans. Would you be able to do something similar?;;;","23/Oct/13 06:35;jbrosenberg@gmail.com;Hi Swapnil,

Unfortunately, we aren't using mbeans, but using the yammer MetricsRegistry and a reporter class based on com.yammer.metrics.reporting.AbstractPollingReporter.

This creates a background thread that wakes up every 10 seconds and transmits all the metrics in the registry (which is essentially all the yammer metric mbeans) and sends them as messages over kafka.  We then have time series db's which store these.   Unfortunately, the ts db's are not sophisticated enough to allow wild-card querying....

Is there a fundamental reason for having those cryptic metric names?;;;","23/Oct/13 17:43;jjkoshy;That's a good point - we don't need it to be that way. The metric names that you referred to are derived from the consumer's registration in zookeeper. There are a couple of cleanup tasks we need to do for mbeans especially wrt consumers:
- The names need not include timestamps. The reason we have timestamps and a hash in there is if you were to bring up two consumers under the same group on the same host at nearly the same time their registration would collide in zookeeper. Realistically this is something that only happens in system tests so it should be fine to drop the timestamp and hash for metrics registration.
- Metrics are not de-registered on a rebalance/shutdown. I think there is already a jira for the shutdown case, but I'm compiling a list of other shortcomings and will file an umbrella jira to cover most of these issues.
- I think the deregistration issues affect replica fetchers as well (need to check). i.e., if a broker transitions from a follower 
to leader for a partition the follower metrics for that partition need to be de-registered.;;;","24/Oct/13 01:52;swapnilghike;That makes sense Joel, we could also use the clientId to differentiate between two consumerConnectors that start up on the same host with the same group.;;;","24/Oct/13 02:32;otis;This sounds like something that would be good to have in 0.8 final, though it may be too late for that?;;;","24/Oct/13 17:07;junrao;Yes, I think this is too late for 0.8 final.;;;","24/Dec/13 07:07;jbrosenberg@gmail.com;can we think about this for 0.8.1?;;;","14/Jan/14 09:58;jbrosenberg@gmail.com;status?;;;","16/Jan/14 17:36;otis;I'm interested, too, so we can add Kafka 0.8 metrics support to SPM for Kafka.

Could this go in 0.8.2 by any chance?
;;;","29/May/14 19:00;otis;[~junrao] Any chance we could set Fix Version to 0.8.2 for this one?;;;","02/Jun/14 15:06;junrao;We have started developing the new consumer (for 0.9). Perhaps we should just fix the metric issue in the new consumer.;;;","02/Jun/14 15:15;jbrosenberg@gmail.com;what's the timeline for 0.9?;;;","02/Jun/14 15:25;junrao;That's probably 3-4 months away.;;;","02/Jun/14 20:31;otis;Uhuh, long time :(
There are other similar issues with metrics/beans, like http://search-hadoop.com/m/4TaT4lonIW&subj=How+to+parse+some+of+JMX+Bean+s+names which looks almost like a blocker for anyone trying to implement a Kafka monitoring tool that everyone could use (i.e. even those who have servers, topics, etc. with dashes in their names).
;;;","05/Nov/14 01:43;otis;I didn't check Kafka MBeans after the latest KAFKA-1481 patch, but I think they no longer contain generation/timestamps in them, right [~vladimir.tretyakov]?  If that's true, then maybe KAFKA-1481 will fix this issue, too?;;;","05/Nov/14 14:35;vladimir.tretyakov;Right [~otis], names will be:

{code}
kafka.consumer:type=FetchRequestAndResponseMetrics,name=FetchRequestRateAndTimeMs,clientId=af_servers,allBrokers=true

kafka.consumer:type=FetchRequestAndResponseMetrics,name=FetchRequestRateAndTimeMs,clientId=af_servers,brokerHost=wawanawna,brokerPort=9092

kafka.consumer:type=ZookeeperConsumerConnector,name=FetchQueueSize,clientId=af_servers,topic=spm_topic,threadId=0
{code};;;","26/Jun/15 10:03;omkreddy;This got fixed in KAFKA-1481. Hence closing the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit test failure in 0.8.1 related to LogCleaner,KAFKA-1098,12675058,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,nehanarkhede,nehanarkhede,22/Oct/13 16:43,25/Oct/13 04:35,14/Jul/23 05:39,25/Oct/13 04:35,0.8.1,,,,,,,,,,log,,,0,,,,"Floor = 0, To = -1
[2013-10-22 09:39:25,001] ERROR Error in cleaner thread 0: (kafka.log.LogCleaner:103)
java.lang.IllegalArgumentException: inconsistent range
	at java.util.concurrent.ConcurrentSkipListMap$SubMap.<init>(ConcurrentSkipListMap.java:2506)
	at java.util.concurrent.ConcurrentSkipListMap.subMap(ConcurrentSkipListMap.java:1984)
	at kafka.log.Log.logSegments(Log.scala:605)
	at kafka.log.LogToClean.<init>(LogCleaner.scala:596)
	at kafka.log.LogCleaner$$anonfun$5.apply(LogCleaner.scala:137)
	at kafka.log.LogCleaner$$anonfun$5.apply(LogCleaner.scala:137)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
	at scala.collection.immutable.List.foreach(List.scala:45)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
	at scala.collection.immutable.List.map(List.scala:45)
	at kafka.log.LogCleaner.kafka$log$LogCleaner$$grabFilthiestLog(LogCleaner.scala:137)
	at kafka.log.LogCleaner$CleanerThread.cleanOrSleep(LogCleaner.scala:203)
	at kafka.log.LogCleaner$CleanerThread.run(LogCleaner.scala:189)
",,jjkoshy,jkreps,junrao,jvanremoortere,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/13 04:52;jkreps;KAFKA-1098-v2.patch;https://issues.apache.org/jira/secure/attachment/12609802/KAFKA-1098-v2.patch","25/Oct/13 03:44;jkreps;KAFKA-1098-v3.patch;https://issues.apache.org/jira/secure/attachment/12610260/KAFKA-1098-v3.patch","22/Oct/13 18:46;jvanremoortere;kafka_1098-v1.patch;https://issues.apache.org/jira/secure/attachment/12609702/kafka_1098-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,354680,,,Fri Oct 25 04:29:09 UTC 2013,,,,,,,,,,"0|i1p5an:",354969,,,,,,,,,,,,,,,,,,,,"22/Oct/13 18:46;jvanremoortere;logSegments can currently be called with to = -1. This can trigger submap to be called with invalid arguments (i.e. from > to). We catch this case and return an empty iterable of logSegments.;;;","23/Oct/13 00:53;jjkoshy;Thanks for the patch - this is interesting/weird.

The real issue seems to be that since the map is defined as {{[Long, LogSegment]}} the null return of floorKey is getting converted to a 0 Long value and failing the eq null check.

i.e., the actual from value is also -1. floorKey should return null (and it does) but it is implicitly converted to 0, so it does not enter the {{floor eq null}} block but we want it to.

I'm wondering if we should just switch the map to use java.lang.Long instead of scala Long to avoid these implicit conversions.

{code}
scala> val m = new ConcurrentSkipListMap[Long, Any]
m: java.util.concurrent.ConcurrentSkipListMap[Long,Any] = {}

scala> m.floorKey(0)
res10: Long = 0

scala> val m = new ConcurrentSkipListMap[java.lang.Long, Any]
m: java.util.concurrent.ConcurrentSkipListMap[java.lang.Long,Any] = {}

scala> m.floorKey(0)
res11: java.lang.Long = null
{code};;;","23/Oct/13 01:03;jvanremoortere;I think the behavior is dependent on the version of Scala used. When I first wrote the patch for Kafka-1042 I was using 2.9.2. This error seems to arise when using 2.8.0.
Your suggestion makes sense, I'm not sure what the plan is for supporting older versions of Scala.;;;","23/Oct/13 04:01;nehanarkhede;+1 on [~jjkoshy]'s suggestion. [~jvanremoortere] Would you like to take a stab at that?;;;","23/Oct/13 04:20;jkreps;Joel, I think you're right. I can grab this.;;;","23/Oct/13 04:52;jkreps;Here is a patch that seems to fix the issue. ;;;","23/Oct/13 04:53;jkreps;Sorry about the churn btw I was only compiling for 2.9 which obviously isn't enough.;;;","23/Oct/13 15:30;junrao;Thanks for the patch. It doesn't compile with scala 2.10.1 though since asIterable no longer exists there. I guess we have to import all JavaConversions._ to get around the cross compilation issue.
[error] /Users/jrao/Intellij/kafka_git/core/src/main/scala/kafka/log/Log.scala:589: value asIterable is not a member of object scala.collection.JavaConversions
[error]     import JavaConversions.asIterable
[error]            ^

Also, in the following places, should we use eq to test null for the returned entry from the skipListMap?

In read(),
    if(startOffset > next || entry == null)

In roll(),
      segments.lastEntry() match {
        case null => 
        case entry => entry.getValue.index.trimToValidSize()
      }
;;;","23/Oct/13 16:36;jvanremoortere;Is the list of Scala versions in the README file (2.8.0, 2.8.2, 2.9.1, 2.9.2 or 2.10.1) a full and  accurate representation of the versions we intend to support? I can be more thorough in running the test suite under each one, but am not sure which versions I need to test.;;;","25/Oct/13 00:59;nehanarkhede;[~jvanremoortere] That list of Scala versions is correct.;;;","25/Oct/13 03:44;jkreps;Ack, fixed 2.10 issue. Frustrating. Do we have a single sbt command that can compile for all?

Jun--the other two null comparisons are for entries not offsets so == should be valid there.;;;","25/Oct/13 04:29;junrao;+1 for v3. 

To compile in all versions of scala, do the following. It's included in README.md
 ./sbt +package;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition while reassigning low throughput partition leads to incorrect ISR information in zookeeper ,KAFKA-1097,12674908,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,21/Oct/13 22:01,01/Nov/13 17:28,14/Jul/23 05:39,01/Nov/13 17:28,0.8.0,,,0.8.1,,,,,,,controller,,,0,,,,"While moving partitions, the controller moves the old replicas through the following state changes -

ONLINE -> OFFLINE -> NON_EXISTENT

During the offline state change, the controller removes the old replica and writes the updated ISR to zookeeper and notifies the leader. Note that it doesn't notify the old replicas to stop fetching from the leader (to be fixed in KAFKA-1032). During the non-existent state change, the controller does not write the updated ISR or replica list to zookeeper. Right after the non-existent state change, the controller writes the new replica list to zookeeper, but does not update the ISR. So an old replica can send a fetch request after the offline state change, essentially letting the leader add it back to the ISR. The problem is that if there is no new data coming in for the partition and the old replica is fully caught up, the leader cannot remove it from the ISR. That lets a non existent replica live in the ISR at least until new data comes in to the partition",,charmalloc,guozhang,jjkoshy,junrao,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/13 04:34;nehanarkhede;KAFKA-1097.patch;https://issues.apache.org/jira/secure/attachment/12609797/KAFKA-1097.patch","29/Oct/13 17:49;nehanarkhede;KAFKA-1097_2013-10-29_10:49:45.patch;https://issues.apache.org/jira/secure/attachment/12610879/KAFKA-1097_2013-10-29_10%3A49%3A45.patch","31/Oct/13 04:46;nehanarkhede;KAFKA-1097_2013-10-30_21:46:00.patch;https://issues.apache.org/jira/secure/attachment/12611249/KAFKA-1097_2013-10-30_21%3A46%3A00.patch","31/Oct/13 17:37;nehanarkhede;KAFKA-1097_2013-10-31_10:37:29.patch;https://issues.apache.org/jira/secure/attachment/12611433/KAFKA-1097_2013-10-31_10%3A37%3A29.patch","01/Nov/13 16:55;nehanarkhede;KAFKA-1097_2013-11-01_09:55:33.patch;https://issues.apache.org/jira/secure/attachment/12611629/KAFKA-1097_2013-11-01_09%3A55%3A33.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,354530,,,Fri Nov 01 17:28:34 UTC 2013,,,,,,,,,,"0|i1p4dj:",354820,,,,,,,,,,,,,,,,,,,,"22/Oct/13 18:42;nehanarkhede;This is a timing issue between when the controller executes related state changes and when those state changes actually get executed on the broker. In this case, the right time to remove the replica from ISR is actually on receiving the StopReplicaResponse from the broker. This ensures that the broker will not send any more fetch requests to the leader and add itself back to the ISR. This is the right fix, but is a non-trivial change to the controller and is dependent on KAFKA-1099. 

A short-term fix which will alleviate the issue, but not completely fix it is to send the stop replica request to the broker on the OfflineReplica state change, then shrink the ISR on the controller. In addition, send the stop replica (with delete) request on the NonExistentReplica state change and shrink the ISR again. There is a small chance that the broker hasn't acted on the 2 stop replica requests and still adds itself back to the ISR, after the controller shrinks the ISR during the NonExistentReplica state change. But this is unlikely. 

So the options for this bug fix are -

1. Take the short term fix for 0.8 and leave the larger change for trunk
2. Don't even take the short term fix and just fix it properly on trunk;;;","22/Oct/13 18:56;sriramsub;From the description, it does not seem like it causes bad things to happen. The non existent replica would be in the ISR till new data comes to the partition. Is there any bad things that can happen in this state?;;;","22/Oct/13 20:49;nehanarkhede;[~sriramsub] Agree that it is sort of corner case. A potential risk is that while a partition is in this state (with no new data coming in) and the leader dies or needs to be bounced, there is a risk that the controller might try to elect a non existent replica as the leader. Since we don't retry leader elections based on response from the broker, this might be a potential issue.

On the other hand, there is a workaround. The admin can patch the ISR manually in zookeeper and run the preferred replica election which makes the controller send the right ISR and assigned replica list to the leader, which will also fix the leader's internal data structures that hold the ISR.;;;","22/Oct/13 21:32;guozhang;Hi [~nehanarkhede], as you have mentioned leader should be able to kick the stale replica out of ISR even if there is no more data coming, since it can also kick a replica based on time instead of data lagging. Why this did not happen?;;;","22/Oct/13 21:56;nehanarkhede;[~guozhang] The ISR shrink logic marks replicas as possibly stuck only if the replicas are lagging behind the leader's highwatermark. You can argue that the leader can keep track of fully caught up replicas that have stopped fetching and kick the replicas out of ISR based on the replica's long poll timeout, but that is pretty complicated. So if the partition is not getting any more data and the old replica is fully caught up with the leader, there is nothing much the leader can do to shrink the ISR.;;;","22/Oct/13 22:03;jjkoshy;[~guozhang] Neha's comment wrt not getting any more data and the leader not shrinking the ISR is when the old replica is already fully caught up. The time-based shrinking happens only if the replica was at a smaller log-end-offset.

WRT this issue given that the old replica re-enters the ISR at the end of the new ISR, the likelihood of this being a issue (i.e., the old replica being elected as a leader) is relatively low (can you confirm?) That said, my preference would be the long-term fix on trunk instead of a partially correct fix on 0.8. However, the fact that we have an spurious/incorrect entry in the ISR list would skew the under-replicated partition count wouldn't it? In which case that would be a blocker issue.
;;;","22/Oct/13 22:07;jjkoshy;nm.. i think the previous comment addressed the concern already. the spurious entry would exist only as long as there is no data coming in. or else the leader would in fact kick it out of the ISR.;;;","23/Oct/13 03:06;charmalloc;So, quick question.  If you have 3 brokers with replication 3 then this issue would not occur, right? If not then I am not understanding the problem :(;;;","23/Oct/13 03:51;nehanarkhede;[~joestein] This can happen only during partition reassignment.;;;","23/Oct/13 04:34;nehanarkhede;Created reviewboard https://reviews.apache.org/r/14865/
;;;","23/Oct/13 04:38;nehanarkhede;I think the changes are tricky and non trivial, so I suggest we take this on trunk, since we are close to the 0.8-final release and this error is recoverable through admin tools;;;","24/Oct/13 16:32;junrao;Posted the comment on the RB. The patch is a bit tricky. I agree that we should patch it only in trunk so that we will can test it out more thoroughly.;;;","24/Oct/13 19:56;nehanarkhede;Thinking about it more, my original proposed long term fix (mentioned in my 1st comment) is much less hackier and precise compared to the one in the rb. ;;;","29/Oct/13 17:49;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/14865/
 against branch trunk;;;","31/Oct/13 04:46;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/14865/
 against branch trunk;;;","31/Oct/13 17:37;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/14865/
 against branch trunk;;;","01/Nov/13 16:55;nehanarkhede;Updated reviewboard https://reviews.apache.org/r/14865/
 against branch trunk;;;","01/Nov/13 17:28;nehanarkhede;Thanks a lot for the reviews, pushed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
An old controller coming out of long GC could update its epoch to the latest controller's epoch,KAFKA-1096,12674867,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriharsha,swapnilghike,swapnilghike,21/Oct/13 18:44,26/Jun/14 23:16,14/Jul/23 05:39,26/Jun/14 23:16,0.8.0,,,0.8.2.0,,,,,,,controller,,,0,,,,"If a controller GCs for too long, we could have two controllers in the cluster. The controller epoch is supposed to minimize the damage in such a situation, as the brokers will reject the requests sent by the controller with an older epoch.

When the old controller is still in long GC, a new controller could be elected. This will fire ControllerEpochListener on the old controller. When it comes out of GC, its ControllerEpochListener will update its own epoch to the new controller's epoch. So both controllers are now able to send out requests with the same controller epoch until the old controller's handleNewSession() can execute in the controller lock. 

ControllerEpochListener does not seem necessary, so we can probably delete it.",,mazhar.shaikh.in,nehanarkhede,sriharsha,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/14 04:52;sriharsha;KAFKA-1096.patch;https://issues.apache.org/jira/secure/attachment/12650000/KAFKA-1096.patch","20/Jun/14 22:27;sriharsha;KAFKA-1096_2014-06-20_15:27:44.patch;https://issues.apache.org/jira/secure/attachment/12651754/KAFKA-1096_2014-06-20_15%3A27%3A44.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,354489,,,Thu Jun 26 23:16:38 UTC 2014,,,,,,,,,,"0|i1p44f:",354779,,,,,,,,,,,,,,,,,,,,"01/Feb/14 19:53;nehanarkhede;This is relatively rare, moving to 0.8.2;;;","12/Jun/14 04:52;sriharsha;Created reviewboard https://reviews.apache.org/r/22496/diff/
 against branch origin/trunk;;;","20/Jun/14 22:27;sriharsha;Updated reviewboard https://reviews.apache.org/r/22496/diff/
 against branch origin/trunk;;;","25/Jun/14 19:55;sriharsha;[~nehanarkhede] When you get a chance can you please take a look at the last patch.;;;","26/Jun/14 23:16;nehanarkhede;Thanks for the updated patch! Pushed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka does not compile with sbt,KAFKA-1095,12674824,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,mailq,mailq,21/Oct/13 16:21,20/Mar/14 21:45,14/Jul/23 05:39,20/Mar/14 21:45,0.10.1.0,,,,,,,,,,packaging,,,0,,,,"Expected behaviour:
After `git pull`, `./sbt update` and `./sbt package` the current snapshot version should compile without errors.

Current behaviour:
It fails with different error messages. This is one possible error log: https://gist.github.com/mumrah/7086356
With `./sbt ""++2.10.1 package""` the errors are different but still results in a failed compile. Other scala versions fail, too.

The responsible commit which leads to the error messages is unknown to me.","Linux 64bit, OpenJDK 1.7",guozhang,jkreps,jobswang,junrao,jvanremoortere,mailq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,354446,,,Thu Mar 20 21:45:02 UTC 2014,,,,,,,,,,"0|i1p3uv:",354736,,,,,,,,,,,,,,,,,,,,"21/Oct/13 16:28;mailq;with `./sbt ""++2.9.2 package""` produces many warnings but finally compiles. But other versions must be repaired.;;;","21/Oct/13 16:35;guozhang;This is due one current jira KAFKA-1042, with the last comment.;;;","21/Oct/13 18:09;jvanremoortere;I added a patch (v3) to KAFKA-1042 to fix this error.;;;","29/Jan/14 14:02;jobswang;Does compiling kafka with sbt support scala 2.9.3? ;;;","29/Jan/14 15:22;junrao;Not sure, could you try adding 2.9.3 to the sbt build file?;;;","05/Feb/14 10:02;jobswang;i add 2.9.3 to sbt build file, but report errors!;;;","20/Mar/14 21:45;jkreps;We moved off sbt so presumably this is no longer valid.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configure reviewboard url in kafka-patch-review tool,KAFKA-1094,12674664,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,swapnilghike,swapnilghike,19/Oct/13 22:47,19/Oct/13 23:38,14/Jul/23 05:39,19/Oct/13 23:38,0.8.0,,,,,,,,,,,,,0,,,,"If someone forgets to configure review board, then the tool uploads a patch without creating an RB.",,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/13 22:47;swapnilghike;KAFKA-1094.patch;https://issues.apache.org/jira/secure/attachment/12609283/KAFKA-1094.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,354286,,,Sat Oct 19 23:38:49 UTC 2013,,,,,,,,,,"0|i1p2vj:",354576,,,,,,,,,,,,,,,,,,,,"19/Oct/13 22:47;swapnilghike;Created reviewboard https://reviews.apache.org/r/14773/
;;;","19/Oct/13 22:48;swapnilghike;Same patch will work for trunk.;;;","19/Oct/13 23:38;nehanarkhede;Thanks for the patch, committed to 0.8 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
full topic list can be read from metadata cache in the broker instead of ZK,KAFKA-1091,12674155,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,junrao,junrao,junrao,16/Oct/13 17:18,17/Oct/13 16:37,14/Jul/23 05:39,17/Oct/13 16:37,,,,0.8.1,,,,,,,,,,0,,,,,,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/13 17:20;junrao;KAFKA-1091.patch;https://issues.apache.org/jira/secure/attachment/12608751/KAFKA-1091.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,353777,,,Thu Oct 17 16:37:34 UTC 2013,,,,,,,,,,"0|i1ozsn:",354069,,,,,,,,,,,,,,,,,,,,"16/Oct/13 17:20;junrao;Created reviewboard https://reviews.apache.org/r/14676/
;;;","17/Oct/13 16:37;junrao;Thanks for  the review. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testPipelinedRequestOrdering has transient failures,KAFKA-1090,12674151,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,junrao,junrao,junrao,16/Oct/13 16:56,17/Oct/13 17:27,14/Jul/23 05:39,17/Oct/13 17:27,,,,0.8.1,,,,,,,core,,,0,,,,"The issue is that after the 1st response is added to the response queue, the socket key may or may not be readable depending on how quickly the response is sent through socket.",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/13 17:00;junrao;KAFKA-1090.patch;https://issues.apache.org/jira/secure/attachment/12608744/KAFKA-1090.patch","17/Oct/13 16:33;junrao;KAFKA-1090_2013-10-17_09:33:17.patch;https://issues.apache.org/jira/secure/attachment/12608959/KAFKA-1090_2013-10-17_09%3A33%3A17.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,353773,,,Thu Oct 17 17:27:44 UTC 2013,,,,,,,,,,"0|i1ozrz:",354065,,,,,,,,,,,,,,,,,,,,"16/Oct/13 17:00;junrao;Created reviewboard https://reviews.apache.org/r/14675/
;;;","17/Oct/13 16:33;junrao;Updated reviewboard https://reviews.apache.org/r/14675/
;;;","17/Oct/13 17:27;junrao;Thanks for the review. Committed to trunk after changing the test case to a better name.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migration tool system tests soft-fail on 0.8 and trunk,KAFKA-1089,12674073,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jjkoshy,jjkoshy,jjkoshy,16/Oct/13 07:07,16/Oct/13 17:40,14/Jul/23 05:39,16/Oct/13 17:40,,,,,,,,,,,,,,0,,,,Due to a logging issue (similar to KAFKA-1076),,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/13 07:15;jjkoshy;KAFKA-1089-v1.patch;https://issues.apache.org/jira/secure/attachment/12608668/KAFKA-1089-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,353696,,,Wed Oct 16 17:40:03 UTC 2013,,,,,,,,,,"0|i1ozav:",353988,,,,,,,,,,,,,,,,,,,,"16/Oct/13 15:07;junrao;Thanks for the patch. +1.;;;","16/Oct/13 17:07;jjkoshy;Thanks for the review - committed to 0.8. I'll double-commit to trunk as well.;;;","16/Oct/13 17:40;jjkoshy;Committed to both 0.8 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Empty topic list causes consumer to fetch metadata of all topics,KAFKA-1087,12673798,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,swapnilghike,swapnilghike,15/Oct/13 00:32,15/Oct/13 16:31,14/Jul/23 05:39,15/Oct/13 16:31,0.8.0,,,,,,,,,,,,,0,,,,"The ClientUtils fetches metadata for all topics if the topic set is empty. 

If the topic list of a consumer is empty, the following happens if a rebalance is triggered:
- The fetcher is restarted, fetcher.startConnections() starts a LeaderFinderThread
- LeaderFinderThread waits on a condition
- fetcher.startConnections() signals the aforementioned condition
- LeaderFinderThread obtains metadata for all topics since the topic list is empty.",,guozhang,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/13 00:33;swapnilghike;KAFKA-1087.patch;https://issues.apache.org/jira/secure/attachment/12608394/KAFKA-1087.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,353421,,,Tue Oct 15 16:31:08 UTC 2013,,,,,,,,,,"0|i1oxlr:",353713,,,,,,,,,,,,,,,,,,,,"15/Oct/13 00:33;swapnilghike;Unit tests pass.;;;","15/Oct/13 02:17;guozhang;Great find. +1

PS: Shall we actually enforce topic list as required parameters also besides this fix?;;;","15/Oct/13 02:25;nehanarkhede;Good catch, +1 !;;;","15/Oct/13 03:22;nehanarkhede;Checked into 0.8, could you please provide a patch for trunk as well?;;;","15/Oct/13 07:41;swapnilghike;Same patch should apply fine to trunk.;;;","15/Oct/13 16:31;nehanarkhede;Thanks for the patch. Checked into 0.8 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
run-class.sh and other shell scripts are copy pasted all over the place,KAFKA-1085,12673505,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,granthenke,jkreps,jkreps,11/Oct/13 22:28,26/Jan/16 04:23,14/Jul/23 05:39,26/Jan/16 04:23,,,,,,,,,,,,,,0,,,,"This is kind of embaressing.

jkreps-mn:kafka jkreps$ find . -name ""*.sh""
./bin/kafka-add-partitions.sh
./bin/kafka-console-consumer.sh
./bin/kafka-console-producer.sh
./bin/kafka-consumer-perf-test.sh
./bin/kafka-create-topic.sh
./bin/kafka-list-topic.sh
./bin/kafka-preferred-replica-election.sh
./bin/kafka-producer-perf-test.sh
./bin/kafka-reassign-partitions.sh
./bin/kafka-replay-log-producer.sh
./bin/kafka-run-class.sh
./bin/kafka-server-start.sh
./bin/kafka-server-stop.sh
./bin/kafka-simple-consumer-perf-test.sh
./bin/kafka-simple-consumer-shell.sh
./bin/run-rat.sh
./bin/zookeeper-server-start.sh
./bin/zookeeper-server-stop.sh
./bin/zookeeper-shell.sh
./contrib/hadoop-consumer/copy-jars.sh
./contrib/hadoop-consumer/hadoop-setup.sh
./contrib/hadoop-consumer/run-class.sh
./examples/bin/java-producer-consumer-demo.sh
./examples/bin/java-simple-consumer-demo.sh
./system_test/broker_failure/bin/kafka-run-class.sh
./system_test/broker_failure/bin/run-test.sh
./system_test/common/util.sh
./system_test/migration_tool_testsuite/0.7/bin/kafka-run-class.sh
./system_test/migration_tool_testsuite/0.7/bin/zookeeper-server-start.sh
./system_test/mirror_maker/bin/run-test.sh
./system_test/producer_perf/bin/run-compression-test.sh
./system_test/producer_perf/bin/run-test.sh
./system_test/run_sanity.sh",,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,353128,,,Tue Jan 26 04:23:11 UTC 2016,,,,,,,,,,"0|i1ovrj:",353415,,,,,,,,,,,,,,,,,,,,"26/Jan/16 04:23;granthenke;Looks like this was resolved through contrib and systest removal.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-run-class.sh is broken,KAFKA-1081,12673281,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,fsaintjacques,fsaintjacques,10/Oct/13 18:11,15/Oct/13 21:08,14/Jul/23 05:39,15/Oct/13 21:08,0.8.0,,,,,,,,,,,,,0,,,,"Please apply this patch, this is why log4j exists. Rerunning at non-deterministic command twice to catch error message is extremely dangerous.

diff --git a/bin/kafka-run-class.sh b/bin/kafka-run-class.sh
index eb6ff1b..2f2d8b5 100755
--- a/bin/kafka-run-class.sh
+++ b/bin/kafka-run-class.sh
@@ -102,19 +102,3 @@ if [ ""$1"" = ""daemon"" ] && [ -z ""$KAFKA_GC_LOG_OPTS""] ; then
 fi

 $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS ""$@""
-
-exitval=$?
-
-if [ $exitval -eq ""1"" ] ; then
-       $JAVA $KAFKA_HEAP_OPTS $KAFKA_JVM_PERFORMANCE_OPTS $KAFKA_GC_LOG_OPTS $KAFKA_JMX_OPTS $KAFKA_LOG4J_OPTS -cp $CLASSPATH $KAFKA_OPTS ""$@"" >& exception.txt
-       exception=`cat exception.txt`
-       noBuildMessage='Please build the project using sbt. Documentation is available at http://kafka.apache.org/'
-       pattern=""(Could not find or load main class)|(java\.lang\.NoClassDefFoundError)""
-       match=`echo $exception | grep -E ""$pattern""`
-       if [[ -n ""$match"" ]]; then
-               echo $noBuildMessage
-       fi
-       rm exception.txt
-fi
-
-",,charmalloc,fanatoly,fsaintjacques,jkreps,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/13 22:29;jkreps;KAFKA-1081-v1.patch;https://issues.apache.org/jira/secure/attachment/12608092/KAFKA-1081-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,352904,,,Tue Oct 15 21:08:23 UTC 2013,,,,,,,,,,"0|i1oudr:",353191,,,,,,,,,,,,,,,,,,,,"10/Oct/13 18:28;charmalloc;please can you put together reproducible steps for the issue, thanks!;;;","10/Oct/13 18:35;fsaintjacques;Look for the 3 last lines, not the usage error.

$ cd /
$ /opt/kafka/bin/kafka-list-topic.sh
Missing required argument ""[zookeeper]""
Option                                  Description
------                                  -----------
--topic <topic>                         REQUIRED: The topic to be listed.
                                          Defaults to all existing topics.
                                          (default: )
--unavailable-partitions                if set, only show partitions whose
                                          leader is not available
--under-replicated-partitions           if set, only show under replicated
                                          partitions
--zookeeper <urls>                      REQUIRED: The connection string for
                                          the zookeeper connection in the form
                                          host:port. Multiple URLS can be
                                          given to allow fail-over.
/opt/kafka/bin/kafka-run-class.sh: line 72: exception.txt: Permission denied
cat: exception.txt: No such file or directory
rm: cannot remove 'exception.txt': No such file or directory;;;","10/Oct/13 18:47;charmalloc;I don't see removing that function as a solution, we could re work it so that it communicates the error that you need to build the project.

i don't have permission issue writing exception.txt but could see another way of fixing the function to not use a file and keep it all in vars if you wanted to rework your patch or chmod a+rw in your folder maybe not sure how you are running things but you should build first (see README)

Joes-MacBook-Air:kafka joestein$ bin/kafka-list-topic.sh 
Missing required argument ""[zookeeper]""
Option                                  Description                            
------                                  -----------                            
--topic <topic>                         REQUIRED: The topic to be listed.      
                                          Defaults to all existing topics.     
                                          (default: )                          
--unavailable-partitions                if set, only show partitions whose     
                                          leader is not available              
--under-replicated-partitions           if set, only show under replicated     
                                          partitions                           
--zookeeper <urls>                      REQUIRED: The connection string for    
                                          the zookeeper connection in the form 
                                          host:port. Multiple URLS can be      
                                          given to allow fail-over.            
Joes-MacBook-Air:kafka joestein$ 
;;;","10/Oct/13 19:14;fsaintjacques;Look, this is an ugly hack. The real problem here is not the directory permission or using a temp file but RERUNNING the first java command. 

I'm not even trying to build it, I'm trying to correctly package kafka on a production server. Whenever I run any command (bin/*) that doesn't return properly, it borks if kafka-run-class.sh is called within a directory where the user don't have write permission.

I understand that these lines are there to help new users who checkout the project and forget to build before running any command, but we're talking of deploying quality code in production.;;;","10/Oct/13 19:28;charmalloc;We have binary release http://kafka.apache.org/downloads.html since 0.8.0-beta1 and moving forward, you could use that since it is already packaged.  I would go so far as to even bootstrap the sbt launcher but that is a lot more changes than just a consistent error when not compiling source before run.;;;","10/Oct/13 19:32;fsaintjacques;I know you guys have a binary release, this is what I'm deploying. My point is, this specific snippet should never go in production release.;;;","11/Oct/13 18:48;jkreps;Yeah I agree, rerunning the command to get the error is really weird. Given that we have a binary release do we need to retain this behavior? I.e. if you download the source release and try to run without building then it won't work but this shouldn't be as confusing as it was when we only had a source release and didn't really label the download as such.;;;","11/Oct/13 20:17;fanatoly;There is another reason why removing this behavior would be beneficial.

Currently, the run-class script spawns a subprocess. This makes running it under supervisor more difficult than it needs to be. This likely applies to other process monitoring systems. I maintain an internal patch on top of kafka that prepends the java invocation in run-class with exec and removes the error reporting. In production, the error is not very useful.;;;","11/Oct/13 22:29;jkreps;Here is a proposed patch that uses exec for all the main commands and removes the re-execution for errors.;;;","12/Oct/13 03:50;charmalloc;+1 on KAFKA-1081-v1.patch;;;","13/Oct/13 21:27;junrao;Thanks for the patch. +1. Could you patch for trunk as well?;;;","15/Oct/13 21:08;jkreps;Checked in on both 0.8 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update System Test to handle controller data returned by ZK,KAFKA-1078,12672839,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jfung,jfung,jfung,08/Oct/13 16:11,08/Oct/13 16:27,14/Jul/23 05:39,08/Oct/13 16:23,,,,0.8.0,,,,,,,,,,0,,,,"There is a change in the result returned by ZK when querying for controller:

2013-10-07 09:39:15,599 - DEBUG - executing command [ssh localhost ""JAVA_HOME=/export/apps/jdk/JDK-1_6_0_27 /home/user/kafka/bin/kafka-run-class.sh org.apache.zookeeper.ZooKeeperMain -server localhost:2188 get /controller 2> /dev/null | tail -1""] (kafka_system_test_utils)

1. Previously it returned : 1
2. It is currently returning : { ""brokerid"":1, ""timestamp"":""1381163914835"", ""version"":1 }

The impact is that System Test doesn't have the correct entity ID for bouncing controller. There are a few test cases that are impacted by this change when they are bouncing the controller.",,jfung,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/13 16:18;jfung;kafka-1078.patch;https://issues.apache.org/jira/secure/attachment/12607376/kafka-1078.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,352462,,,Tue Oct 08 16:27:04 UTC 2013,,,,,,,,,,"0|i1oro7:",352749,,,,,,,,,,,,,,,,,,,,"08/Oct/13 16:18;jfung;Updated kafka_system_test_utils.get_controller_attributes to read the json data of controller.;;;","08/Oct/13 16:23;junrao;Thanks for the patch. Committed to 0.8.;;;","08/Oct/13 16:27;jfung;1. Before the patch (no controller entity id found and system test throws exception)

2013-10-04 01:04:38,070 - INFO - ======================================================
2013-10-04 01:04:38,070 - INFO - Found controller with entity id: 
2013-10-04 01:04:38,070 - INFO - ======================================================

2013-10-04 01:04:38,070 - INFO - ======================================================
2013-10-04 01:04:38,070 - INFO - stopping controller : { ""brokerid"":1, ""timestamp"":""1380873836165"", ""version"":1 }
2013-10-04 01:04:38,070 - INFO - ======================================================

2013-10-04 01:04:38,071 - INFO - ======================================================
2013-10-04 01:04:38,071 - INFO - Exception while running test ''
2013-10-04 01:04:38,071 - INFO - ======================================================
Traceback (most recent call last):
  File ""/home/user/kafka/system_test/replication_testsuite/replica_basic_test.py"", line 301, in runTest
    kafka_system_test_utils.stop_remote_entity(self.systemTestEnv, controllerDict[""entity_id""], self.testcaseEnv.entityBrokerParentPidDict[controllerDict[""entity_id""]])
KeyError: ''


2. After the patch (controller entity id found)

2013-10-07 16:30:48,122 - INFO - ======================================================
2013-10-07 16:30:48,122 - INFO - Found controller with entity id: 1
2013-10-07 16:30:48,122 - INFO - ======================================================

2013-10-07 16:30:48,123 - INFO - ======================================================
2013-10-07 16:30:48,123 - INFO - stopping controller : 1
2013-10-07 16:30:48,123 - INFO - ======================================================
2013-10-07 16:30:48,123 - DEBUG - executing command [ssh localhost 'pid=26805; prev_pid=""""; echo $pid; while [[ ""x$pid"" != ""x"" ]]; do prev_pid=$pid;   for child in $(ps -o pid,ppid a
x | awk ""{ if ( \$2 == $pid ) { print \$1 }}"");     do echo $child; pid=$child;   done;   if [ $prev_pid == $pid ]; then     break;   fi; done' 2> /dev/null (system_test_utils)
2013-10-07 16:30:48,477 - DEBUG - terminating (SIGTERM) process id: 26805 in host: localhost (kafka_system_test_utils)
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system tests in 0.8 are broken due to wrong log4j config,KAFKA-1076,12672483,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,05/Oct/13 18:07,06/Oct/13 16:11,14/Jul/23 05:39,06/Oct/13 16:11,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,,,jkreps,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/13 18:09;junrao;KAFKA-1076.patch;https://issues.apache.org/jira/secure/attachment/12607007/KAFKA-1076.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,352110,,,Sun Oct 06 16:11:50 UTC 2013,,,,,,,,,,"0|i1opif:",352398,,,,,,,,,,,,,,,,,,,,"05/Oct/13 18:09;junrao;Created reviewboard https://reviews.apache.org/r/14511/
;;;","06/Oct/13 00:03;jkreps;+1;;;","06/Oct/13 16:11;junrao;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer will not rebalance upon topic partition change,KAFKA-1075,12672421,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,04/Oct/13 22:18,07/Oct/13 21:24,14/Jul/23 05:39,07/Oct/13 21:24,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"Due to the watcher and zk data structure mismatch, consumer will not rebalance upon topic partition change. Details:

ZK data structure for topic partitions:

/brokers/topics/[topic-name]/partitions/[partition-id]/state

When new partitions are added, it will change the data in /brokers/topics/[topic-name], however the ZK client is watching on child change of /brokers/topics/[topic-name], which will always be a single child 'partitions'.

Proposal: add a data change listener, which will trigger rebalance upon data changes on /brokers/topics/[topic-name].",,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/13 22:23;guozhang;KAFKA-1075.patch;https://issues.apache.org/jira/secure/attachment/12606912/KAFKA-1075.patch","07/Oct/13 17:25;guozhang;KAFKA-1075_2013-10-07_10:25:01.patch;https://issues.apache.org/jira/secure/attachment/12607185/KAFKA-1075_2013-10-07_10%3A25%3A01.patch","07/Oct/13 17:58;guozhang;KAFKA-1075_2013-10-07_10:58:40.patch;https://issues.apache.org/jira/secure/attachment/12607195/KAFKA-1075_2013-10-07_10%3A58%3A40.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,352048,,,Mon Oct 07 21:24:14 UTC 2013,,,,,,,,,,"0|i1op4n:",352336,,,,,,,,,,,,,,,,,,,,"04/Oct/13 22:23;guozhang;Created reviewboard https://reviews.apache.org/r/14497/
;;;","07/Oct/13 17:25;guozhang;Updated reviewboard https://reviews.apache.org/r/14497/
;;;","07/Oct/13 17:58;guozhang;Updated reviewboard https://reviews.apache.org/r/14497/
;;;","07/Oct/13 21:24;junrao;Thanks for the patch. +1 and committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reassign partitions should delete the old replicas from disk,KAFKA-1074,12672415,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,04/Oct/13 21:37,08/Jan/14 06:38,14/Jul/23 05:39,08/Jan/14 06:38,0.8.0,,,0.8.1,,,,,,,core,,,0,,,,"Currently, after reassigning replicas to other brokers, the old replicas are not removed from disk and have to be deleted manually.",,junrao,pkwarren,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-330,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/13 16:29;junrao;KAFKA-1074.patch;https://issues.apache.org/jira/secure/attachment/12614642/KAFKA-1074.patch","04/Dec/13 18:13;junrao;KAFKA-1074_2013-12-04_10:14:13.patch;https://issues.apache.org/jira/secure/attachment/12617033/KAFKA-1074_2013-12-04_10%3A14%3A13.patch","16/Dec/13 17:42;junrao;KAFKA-1074_2013-12-16_09:43:53.patch;https://issues.apache.org/jira/secure/attachment/12618940/KAFKA-1074_2013-12-16_09%3A43%3A53.patch","02/Jan/14 16:33;junrao;KAFKA-1074_2014-01-02_08:36:37.patch;https://issues.apache.org/jira/secure/attachment/12621097/KAFKA-1074_2014-01-02_08%3A36%3A37.patch","03/Jan/14 20:27;junrao;KAFKA-1074_2014-01-03_12:30:47.patch;https://issues.apache.org/jira/secure/attachment/12621382/KAFKA-1074_2014-01-03_12%3A30%3A47.patch","04/Jan/14 16:37;junrao;KAFKA-1074_2014-01-04_08:40:55.patch;https://issues.apache.org/jira/secure/attachment/12621471/KAFKA-1074_2014-01-04_08%3A40%3A55.patch","07/Jan/14 18:52;junrao;KAFKA-1074_2014-01-07_10:55:08.patch;https://issues.apache.org/jira/secure/attachment/12621829/KAFKA-1074_2014-01-07_10%3A55%3A08.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,352042,,,Wed Jan 08 06:38:26 UTC 2014,,,,,,,,,,"0|i1op3b:",352330,,,,,,,,,,,,,,,,,,,,"19/Nov/13 16:28;junrao;Removing the old replica log from the disk itself is simple. What we need to make sure is that all potential outstanding operations on the deleted log are handled properly. In particular, we don't want any potential IOException due to the missing log causes the broker to halt.

1. All read operations are ok since we already handle unexpected exceptions in KafkaApi.

2. Writing to the log by the producer request, the replica fetcher or the log flusher: We need to make sure that after the log is deleted, no more writes/flushes will be attempted on the log. This can be achieved by:
2.1 For producer requests, the delete partition operation will synchronize on the leaderAndIsrUpdate lock.
2.2 For replica fetcher, this is already handled since the fetcher is removed before the log is deleted.
2.3 For log flusher, the flush and the delete will now synchronize on a delete lock.;;;","19/Nov/13 16:29;junrao;Created reviewboard https://reviews.apache.org/r/15674/
 against branch origin/trunk;;;","04/Dec/13 18:13;junrao;Updated reviewboard https://reviews.apache.org/r/15674/
 against branch origin/trunk;;;","04/Dec/13 18:15;junrao;Summary of the new patch:
LogCleaner:
*added the logic to cancel an in-progress cleaning task
*made the cleaner thread a shutdownable, but not interruptible thread. Interrupting the cleaner may cause the file channel to be closed, which will fail other operations like log flushing during shutdown.

LogManager:
* added the logic to wait until an in-progress flushing process complete
* to delete a log do
  (1) take the log to be deleted from log lists so that log cleaner and log flusher won't see it in the future
  (2) cancel any in-progress cleaning task
  (3) wait until any in-progress flushing process to complete
  (4) at this moment, we know the log won't be touched by the cleaner or the flusher any more and we can delete the whole directory synchronously.

Todos:
* If the overall logic looks good, we can get rid of OptimisticLockFailureException in LogCleaner too by canceling any cleaner task before truncating the log. This can be done in a follow up jira.;;;","16/Dec/13 17:42;junrao;Updated reviewboard https://reviews.apache.org/r/15674/
 against branch origin/trunk;;;","16/Dec/13 17:42;junrao;Summary of changes since the previous patch.

LogCleaner:
* Added the logic to pause and resume the cleaning of a log. Canceling the cleaning is implemented as pausing, followed by resuming.

LogManager:
* Don't halt when background flush hits an IOException since the same IOException will be hit during log append, which will halt the broker. This removes the need to synchronize btw the flushing and the deleting of the log.
* Removed OptimisticLockFailureException in LogCleaner. When a log needs to be truncated, first pause log cleaning, then truncate the log, and finally resume log cleaning.;;;","02/Jan/14 16:33;junrao;Updated reviewboard https://reviews.apache.org/r/15674/
 against branch origin/trunk;;;","02/Jan/14 16:33;junrao;Summary of changes since the previous patch.
LogCleaner:
* moved all variables and locks associated with state management into a separate class LogCleanerStates.
* removed the semaphore for unit test and used a simpler approach to just keep retrying every 10ms. Will file a separate jira for improving the unit test.
Log:
* limited the scope of a few methods to log.
LogManager:
* forced checkpointing recovery points in truncateFullyAndStartAt() as well.;;;","03/Jan/14 20:27;junrao;Updated reviewboard https://reviews.apache.org/r/15674/
 against branch origin/trunk;;;","04/Jan/14 16:37;junrao;Updated reviewboard https://reviews.apache.org/r/15674/
 against branch origin/trunk;;;","07/Jan/14 18:52;junrao;Updated reviewboard https://reviews.apache.org/r/15674/
 against branch origin/trunk;;;","08/Jan/14 06:38;junrao;Thanks for the review. Committed to trunk.

File a followup jira KAFKA-1201 to address unit test issue for LogCleanTest.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckReassignmentStatus is broken,KAFKA-1073,12672413,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,04/Oct/13 21:27,07/Oct/13 16:22,14/Jul/23 05:39,07/Oct/13 16:22,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"CheckReassignmentStatus is supposed to take the output from ReassignPartitionsCommand as the input. However, the output from ReassignPartitionsCommand is not a valid json. It's also not clear how to prepare the input to CheckReassignmentStatus.",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/13 21:33;junrao;KAFKA-1073.patch;https://issues.apache.org/jira/secure/attachment/12606900/KAFKA-1073.patch","05/Oct/13 18:02;junrao;KAFKA-1073_2013-10-05_11:02:45.patch;https://issues.apache.org/jira/secure/attachment/12607006/KAFKA-1073_2013-10-05_11%3A02%3A45.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,352040,,,Mon Oct 07 16:22:30 UTC 2013,,,,,,,,,,"0|i1op2v:",352328,,,,,,,,,,,,,,,,,,,,"04/Oct/13 21:33;junrao;Created reviewboard https://reviews.apache.org/r/14496/
;;;","05/Oct/13 18:02;junrao;Updated reviewboard https://reviews.apache.org/r/14496/
;;;","07/Oct/13 16:22;junrao;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow mulitple topics selected with a TopicFilter to be balanced among consumers,KAFKA-1072,12672155,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jbrosenberg@gmail.com,jbrosenberg@gmail.com,03/Oct/13 15:10,17/May/16 14:12,14/Jul/23 05:39,07/Feb/15 23:15,0.8.0,,,0.8.2.0,,,,,,,,,,1,,,,"Currently, there is no parallelism used when consuming a set of topics selected by a white list topic filter, if those topics all have a partition count of 1.  Currently, all topics that match the filter get assigned to the same thread on the same consumer, even though there may be plenty of different topics (and therefore many partitions to be consumed from).

There are often good reasons to use a partition count of only 1 (e.g. to preserve message ordering).  For arbitrary scalability, over a large number of topics, this would be a great benefit to be able to consume topics balanced over a set of available consumers.",,jbrosenberg@gmail.com,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,351781,,,Sat Feb 07 23:15:12 UTC 2015,,,,,,,,,,"0|i1onhr:",352069,,,,,,,,,,,,,,,,,,,,"07/Feb/15 23:15;jkreps;I think this is solved with the partition.assignment.strategy option in 0.8.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The random partition selected in DefaultEventHandler is not random across producer instances,KAFKA-1071,12672044,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,junrao,junrao,02/Oct/13 22:17,04/Oct/13 00:47,14/Jul/23 05:39,04/Oct/13 00:47,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"In DefaultEventHandler, partitionCounter is initialized to 0. If multiple producers start at about the same time, they likely will always select the same partition.",,guozhang,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/13 22:40;guozhang;KAFKA-1071.v1.patch;https://issues.apache.org/jira/secure/attachment/12606471/KAFKA-1071.v1.patch","03/Oct/13 21:52;guozhang;KAFKA-1071.v2.patch;https://issues.apache.org/jira/secure/attachment/12606653/KAFKA-1071.v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,351670,,,Fri Oct 04 00:47:58 UTC 2013,,,,,,,,,,"0|i1omtb:",351958,,,,,,,,,,,,,,,,,,,,"02/Oct/13 22:40;guozhang;Use random int as init value.;;;","03/Oct/13 04:32;junrao;Thanks for the patch. Would it be better to just get a random int in getPartition()? In the current approach, if the multiple producer instances somehow get into the lock-step mode, they will never get out of that mode, if partitions are always available.;;;","03/Oct/13 21:52;guozhang;Good point. Attached v2 for that, also tested that Random.nextInt will not return the same numbers even being called at roughly the same time.;;;","03/Oct/13 22:02;nehanarkhede;It might work better if we seed the random with the current timestamp - http://docs.oracle.com/javase/6/docs/api/java/util/Random.html#Random(long)

Not sure how good Scala's Random implementation is, I tend to trust Java more on that.;;;","03/Oct/13 22:13;guozhang;I tested the Random implementation a bit and the results look good. One thing of using timestamp is that producer clients may start at roughly the same time, and I do not know if there is a correlation between numeric difference of seed values and the numeric difference between the generated values.;;;","03/Oct/13 23:03;nehanarkhede;Though it is unlikely that the nanosecond timestamp across producer instances would match, I don't feel strongly about the change.

+1;;;","04/Oct/13 00:47;junrao;Thanks for patch v2. +1. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Auto-assign node id,KAFKA-1070,12671970,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriharsha,jkreps,jkreps,02/Oct/13 15:59,18/Dec/15 16:59,14/Jul/23 05:39,12/Jan/15 23:46,,,,0.9.0.0,,,,,,,,,,3,usability,,,"It would be nice to have Kafka brokers auto-assign node ids rather than having that be a configuration. Having a configuration is irritating because (1) you have to generate a custom config for each broker and (2) even though it is in configuration, changing the node id can cause all kinds of bad things to happen.",,amuraru,ancoron,astubbs,clarkhaskins,CpuID,hakman,jkreps,joestein,nehanarkhede,otis,paetling,sriharsha,wangbo23,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-3012,,,,,,,,,,,,,"19/Jul/14 00:24;sriharsha;KAFKA-1070.patch;https://issues.apache.org/jira/secure/attachment/12656678/KAFKA-1070.patch","19/Jul/14 23:06;sriharsha;KAFKA-1070_2014-07-19_16:06:13.patch;https://issues.apache.org/jira/secure/attachment/12656763/KAFKA-1070_2014-07-19_16%3A06%3A13.patch","22/Jul/14 18:34;sriharsha;KAFKA-1070_2014-07-22_11:34:18.patch;https://issues.apache.org/jira/secure/attachment/12657155/KAFKA-1070_2014-07-22_11%3A34%3A18.patch","25/Jul/14 03:58;sriharsha;KAFKA-1070_2014-07-24_20:58:17.patch;https://issues.apache.org/jira/secure/attachment/12657765/KAFKA-1070_2014-07-24_20%3A58%3A17.patch","25/Jul/14 04:05;sriharsha;KAFKA-1070_2014-07-24_21:05:33.patch;https://issues.apache.org/jira/secure/attachment/12657767/KAFKA-1070_2014-07-24_21%3A05%3A33.patch","21/Aug/14 17:26;sriharsha;KAFKA-1070_2014-08-21_10:26:20.patch;https://issues.apache.org/jira/secure/attachment/12663433/KAFKA-1070_2014-08-21_10%3A26%3A20.patch","20/Nov/14 18:50;sriharsha;KAFKA-1070_2014-11-20_10:50:04.patch;https://issues.apache.org/jira/secure/attachment/12682709/KAFKA-1070_2014-11-20_10%3A50%3A04.patch","26/Nov/14 04:29;sriharsha;KAFKA-1070_2014-11-25_20:29:37.patch;https://issues.apache.org/jira/secure/attachment/12683762/KAFKA-1070_2014-11-25_20%3A29%3A37.patch","02/Jan/15 01:39;sriharsha;KAFKA-1070_2015-01-01_17:39:30.patch;https://issues.apache.org/jira/secure/attachment/12689762/KAFKA-1070_2015-01-01_17%3A39%3A30.patch","12/Jan/15 18:47;sriharsha;KAFKA-1070_2015-01-12_10:46:54.patch;https://issues.apache.org/jira/secure/attachment/12691693/KAFKA-1070_2015-01-12_10%3A46%3A54.patch","13/Jan/15 02:30;sriharsha;KAFKA-1070_2015-01-12_18:30:17.patch;https://issues.apache.org/jira/secure/attachment/12691827/KAFKA-1070_2015-01-12_18%3A30%3A17.patch",,,,,,,,,,,,,,,,,11.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,351596,,,Tue Sep 08 14:57:59 UTC 2015,,,,,,,,,,"0|i1omd3:",351885,,nehanarkhede,,,,,,,,,,,,,,,,,,"02/Oct/13 16:06;jkreps;I think the right way to do this is to have a sequence in zookeeper that atomically increments and use this for id generation. On startup a node that has no id can generate one for itself and store it.

One tricky bit is that this node id needs to be stored with the data, but we actually partition data up over multiple disks now and hope to be able to survive the destruction of any of them. Which disk should we store the node id on? I would recommend we store it on all of them--if it is missing on some we will add it there, if the id is inconsistent between disks we will error out (this should never happen).

I would recommend adding a properties file named ""meta"" in every data directory containing the ""id=x"" value, we can extend this later with more perminant values. For example, I think it would be nice to add a data format version to help with in-place data upgrades.

On startup the broker would check this value for consistency across directories. If it is not present in any directory it would auto-generate a node id and persist that for future use.

For compatibility we would retain the current id configuration value--if it is present we will use it and ensure the id sequence is larger than this value.;;;","07/May/14 17:06;ancoron;For me, a solution that generates a unique ID by itself would also be fine.

E.g. as a workaround, I currently generate an integer based on the MAC address from the interface that clients are configured to see. However, this is not reliably unique, because a MAC address is 48-bit and the broker-ID is only a signed 32-bit integer, a long would be better fit here.

Going further on standalone generation of unique IDs, within applications I am used to rely on something that does provide me some guarantees. A UUID of Type 1 (MAC/timestamp-based) is exactly that. However, here the problem is even worse, because a UUID is a 128-bit number.

My question here would be (and I am not yet very much into the Kafka code base - so please excuse if this is a very dumb question): what is the brokerId actually being used for? In other words: which requirements does it have to fulfill and what functionality relies on it being unique inside a cluster?

Another question is: why does it have to be a non-negative integer?;;;","17/Jul/14 22:12;sriharsha;[~jkreps] [~nehanarkhede]
I am working on this JIRA. Following the proposed design in the comments. I see there might be a issue  where
one of the broker configs have broker.id defined as 0 and another one doesn't have broker.id. If the second broker started first we fetch a global sequence id from zookeeper that starts with 0 and the next one has already have a defined config we will use that in this case we have two brokers with same id. Should we consider this case in which a kafka cluster's broker might have a inconsistent config interms of broker.id.
Instead of using zookeeper for generating a global sequence number why shouldn't we be using UUID and make brokerId Long type. 
Thanks.;;;","17/Jul/14 22:42;jkreps;Hey [~harsha_ch], that is a great point.

I'd like to avoid changing the type of the id to a long or UUID so as to not have to bump up the protocol format for the metadata request which hands these out to clients (we would need a way to handle compatibility with older clients that don't expect the longer types).

I think we can get around the problem you point out by just defaulting the node id sequence to 1000. This could theoretically conflict but most people number from 0 or 1 and we can discuss this in the release notes. Our plan will be to release with support for both configured node ids and assigned node ids for compatibility. After a couple of releases we will remove the config.

So the behavior would be this:
If there is a node id in the config we will validate it against the node id in the data directory
If it matches that good, we'll use that.
If it doesn't match that is bad, we'll crash with an error.
If there is a node id in the data directory but none in the config, we'll use whatever is in the data directory.
If there is no node id in the data directory yet but there is  one in the config we'll write that to the data directory and use it.
If there is neither a node id in the data directory nor in the config we'll allocate a node id and write it to the data directory.
;;;","19/Jul/14 00:24;sriharsha;Created reviewboard https://reviews.apache.org/r/23702/diff/
 against branch origin/trunk;;;","19/Jul/14 00:25;sriharsha;Thanks [~jkreps];;;","19/Jul/14 23:06;sriharsha;Updated reviewboard https://reviews.apache.org/r/23702/diff/
 against branch origin/trunk;;;","22/Jul/14 18:34;sriharsha;Updated reviewboard https://reviews.apache.org/r/23702/diff/
 against branch origin/trunk;;;","24/Jul/14 19:42;clarkhaskins;We currently have node IDs in the 0-10000 range. I think some better logic for identifying used node IDs is necessary rather than just starting from 1000. ;;;","24/Jul/14 21:14;jonbringhurst;[~clarkhaskins], [~sriharsha], having ReservedBrokerIdMaxValue as a configurable option (perhaps with a default of 1000) would probably solve clark's use case.

-Also, if I'm reading this right, it will ignore the broker ID if it's set in the config but out of the reserved range (set to -1). It will then create a generated broker ID to overwrite the one in the config. It might be nice to have the broker error out on startup if a broker.id is set in the config, but is out of range. A new broker ID should only be generated if an id isn't specified in the config AND the meta.properties doesn't exist (otherwise the config file wouldn't match the actual broker ID).- Edit: nevermind, my Scala reading skills aren't up to par. This isn't the case here.;;;","25/Jul/14 03:58;sriharsha;Updated reviewboard https://reviews.apache.org/r/23702/diff/
 against branch origin/trunk;;;","25/Jul/14 04:05;sriharsha;Updated reviewboard https://reviews.apache.org/r/23702/diff/
 against branch origin/trunk;;;","05/Aug/14 05:04;joestein;This sounds like a very cool and VERY useful feature. Excited to use it myself often.

I know of a few (>10) different clusters that not only use varying sized numbers for their broker.id but do so in what is a seemingly random (but not really when you think about it) way.

so in a cluster there may be broker.id that is 1721632 and another 172164875 and another 172162240 . Making your brokers by replacing ""."" in chef/puppet/salt/ansemble/etc type scripts and sometimes folks get more fancy just doing 2288, 2388, 17, 177 (where just the last two octets get used and ""."" is replaced). 

I am not saying I agree with this approach and I actively advocate away from doing this but in some/many scenarios it is the best/only way to automate their deploys for how things are setup.  It is also what seems to make sense when folks are building their automation scripts since they have no other option without doing more than they should be expected to-do (and the IP replace ""."" is so obvious to them, and it is).

So, for folks in these cases they would just pick the upper bound to be, lets say 17216255256 and then it would auto assign from there?

Is there some better way to go about this where you might have a start increment and and some exclusion list? or a way to see broker.id already had that and not use it?  I think a lot of folks would like to get having broker id be more continious and be easier to communicate but the desire to automate everything will outweigh that.  We could give them some sanity back with brokers 1,2,3,4,5,6,7,8,9,10,11,12 for a 12 node cluster.

not crucial and you may have already accounted for the scenario I brought up, but wanted to bring it up as a real use case for how people automate things.

it might be better for folks to manually migrate their scripts but not sure they would do it and if they did would have to decommission brokers which in a prod environment could take a few weeks/months.  If we let them start at 1 and exclude what they have then they can do it one at a time.  After taking down the first broker and bring it back up (empty) it is broker.id=1, and so on (and if they have a 5 they don't have to take it down), etc.

For new clusters this is a slam dunk and wouldn't want to hold up the feature for existing users that have already decided a work around as I don't know what the intent of this was or not. Some folks might not change knowing broker.id=17216520 sometimes is nice you just login to that box but talking about broker 17216520 over and over again is a pita.

;;;","05/Aug/14 14:37;sriharsha;[~charmalloc] Thanks for the comments. Currently the approach allows users to specify MaxBrokerId and kafka starts generating a sequence from MaxBrokerId + 1. If user specifies brokerId , kafka uses that instead of generating one from zookeeper.  In the current implementation we expect user configured broker.id to be less than MaxBrokerId.
The tricky case here is if the user have a cluster where some of the brokers have broker.id present  in their config and others don't. In this case we use MaxBrokerId (defaults to 1000) and start generating a sequenceId from there.
The issue with brokerId exception list is we do not know from which point to generate a sequenceId. 
In your example "" in a cluster there may be broker.id that is 1721632 and another 172164875 and another 172162240 ""
and user added another broker to the cluster without broker.id in config. If the user specifies that MaxReserveBrokerId is 172164875 than we can assign +1 to that new broker. If we take the approach of looking at brokerIds that exists in the cluster and generate a new one that doesn't exist or taken by a broker already(exclusion list as you suggested) than the problem would be how do we know what are all the broker.ids existed in the cluster one way to look at is zookeeper but a broker registers at zk after it starts. 
For ex: if a broker1 starts with 1721632 and broker2 starts with no broker.id than we generate 1721633 from zk and there is no guarantee that another broker has this same id  in their config. Of course we can have a exclusion list of brokerids in a config file but that feels cumbersome for the user to list out all the brokerids.
;;;","05/Aug/14 15:10;joestein;[~sriharsha] I understand.  What I am struggling with is this for new users or existing users?  For existing users what I have seen them do (at least a dozen times now) is to avoid the conflict of broker.id in the properties file (like you are saying) is to take the IP address, strip out the ""."" and use that (since IP is unique that is what they go with).  They do this in their automation scripts.  I have to imagine it is larger than I have seen as they all did this independently of each other as an intuitive way to make unique broker.id.;;;","05/Aug/14 23:37;sriharsha;[~jkreps] [~junrao] [~nehanarkhede] Can you please review the last patch I sent. Thanks.;;;","21/Aug/14 17:26;sriharsha;Updated reviewboard https://reviews.apache.org/r/23702/diff/
 against branch origin/trunk;;;","21/Aug/14 17:27;sriharsha;[~jkreps] [~junrao] [~nehanarkhede] Can you please review the latest patch. Thanks.;;;","22/Aug/14 23:18;nehanarkhede;In an attempt to keep up with reviews, assigning to [~jkreps] since he has most context on the patch. Feel free to reassign :);;;","08/Sep/14 13:32;sriharsha;[~nehanarkhede] [~junrao] [~jkreps] [~guozhang] Hi All, I am trying to get this patch reviewed. Please let me know if there is anything that need to be fixed with this patch. Thank you.;;;","03/Nov/14 23:21;otis;[~harsha_ch] - it looks like a number of people would really like to use IPs for broker.id.  There is a lot of interest in having that. Please see this thread: http://search-hadoop.com/m/4TaT4dTPKi1
Do you think this is something you could add to this patch, maybe as another broker ID assignment scheme/policy?
;;;","03/Nov/14 23:33;sriharsha;[~otis@apache.org] this patch pending the 0.8.2 release. Since the we have 0.8.2 branched out I'll upmerge this patch and add a configurable option to server.properties where if user prefers IP based broker Id we will generate that or use the seq id that I currently have as a default.
[~jkreps] [~nehanarkhede] [~junrao] please let me know if the above approach looks good to you. Thanks.;;;","04/Nov/14 04:02;otis;bq.  this patch pending the 0.8.2 release

Note this issue doesn't have Fix Version set to 0.8.2.  Maybe that's what you want? +1 from me -- it looks like a number of people would like to see this issue resolved!;;;","04/Nov/14 04:21;sriharsha;[~otis] Sorry I meant pending review after 0.8.2 release. Not planned for 0.8.2 release. I'll update the patch with ip as brokerid option.;;;","04/Nov/14 23:17;nehanarkhede;Sorry for the late review. Expect to get to this shortly.;;;","10/Nov/14 23:01;nehanarkhede;[~sriharsha] Would you mind rebasing your patch? Can help you review it in the next few days.;;;","10/Nov/14 23:04;sriharsha;[~nehanarkhede] will implement the requested ip based broker id as an option and send an updated patch. Try to get it done this week.;;;","20/Nov/14 18:50;sriharsha;Updated reviewboard https://reviews.apache.org/r/23702/diff/
 against branch origin/trunk;;;","20/Nov/14 18:57;sriharsha;[~nehanarkhede] [~charmalloc] [~otis]
Please take a look at the latest patch. I added broker.id.policy as a configurable option with ""ip"" and ""sequence"" .
I've few questions regarding ""ip"" based broker.id.
currently the patch expects /etc/hosts to be properly configured with an ip associated with the hostname.
1) should we ignore loopback ip
    i) if we don't consider loopback ip than on a single node machine with /etc/hosts is not being configured properly it will thrown an error
   ii) if we consider this than there is a same broker id being generate on multiple hosts or multiple brokers on a single host ( this could be the case in non loopback ips too)
2) broker.id is Int and there is a  possibility of  throwing  a NumberFormatException if the ip doesn't fit in Int.MaxValue
3) If a host contains multiple network interfaces which ip should we pick up. Current implementation takes whatever configured for the hostname in /etc/hosts which seems ok to me . 
Please let me know your thoughts on the above. Thanks.;;;","23/Nov/14 20:36;nehanarkhede;[~sriharsha] Reviewed your latest patch. Posted review comments on the rb.;;;","26/Nov/14 04:29;sriharsha;Updated reviewboard https://reviews.apache.org/r/23702/diff/
 against branch origin/trunk;;;","26/Nov/14 04:30;sriharsha;[~nehanarkhede] Thanks for the review. Updated the patch please take a look .;;;","16/Dec/14 18:17;sriharsha;[~nehanarkhede] Can you please take a look at the patch and also reply to your comments. Thanks.;;;","17/Dec/14 05:22;nehanarkhede;Thanks for the updated patch [~sriharsha]. Almost there. Left a few more cleanup comments.;;;","02/Jan/15 01:39;sriharsha;Updated reviewboard https://reviews.apache.org/r/23702/diff/
 against branch origin/trunk;;;","02/Jan/15 01:43;sriharsha;Thanks for the review [~nehanarkhede]. I updated the patch , please take a look when you get a chance . Thanks.;;;","09/Jan/15 23:40;nehanarkhede;[~sriharsha] There are still questions around the broker metadata file serialization. I've reviewed the latest patch and left my comments on the rb.;;;","09/Jan/15 23:49;sriharsha;Thanks [~nehanarkhede] . I'll send an updated patch.;;;","12/Jan/15 18:47;sriharsha;Updated reviewboard https://reviews.apache.org/r/23702/diff/
 against branch origin/trunk;;;","12/Jan/15 18:53;sriharsha;[~nehanarkhede] updated patch addresses 2 of your comments and I've replied to your other comments. Please check and let me know. Thank for the review.;;;","12/Jan/15 23:46;nehanarkhede;[~sriharsha] Thanks for your efforts in getting this patch in shape. Pushed to trunk;;;","13/Jan/15 02:30;sriharsha;Updated reviewboard https://reviews.apache.org/r/23702/diff/
 against branch origin/trunk;;;","18/Aug/15 21:37;paetling;Hello Kafka Geniuses,
     I have a question regarding this PR.  Some context: 
    We have started playing around with kafka 0.8.2.1 to build a data pipeline at the company I work at.  Initially, to get autoscaling with AWS working, we were mapping the IP of our boxes to our broker id.  For a while everything was good.  Today I realized (please correct me if I am wrong) that kafka assigns the replicas to a topic at topic creation time. These replicas are not modified later unless you specifically rebalance the cluster(this is different than ISR which can go from 0 servers to the set of replicas).  This leads to an interesting question on how to cycle in new boxes.  The easiest way seems to be to copy all data from one box to another, kill the old box and start the new box with the same broker.id.  This is not really easy when you do a direct mapping of IP -> broker.id.  
     So now we come to this Jira ticket.  I was wondering if you could enumerate for me how this auto-assign node id would deal with the cycling of a box.  If a bring down a box that was auto-assigned a broker.id of X and bring back up a new box, what will happen.  Will that new box have broker.id X as well?  What if I bring down two boxes with broker.id X and broker.id Y, what is the broker.id of the new box i spin up. 

Thanks for the help,
Alex
     ;;;","18/Aug/15 22:02;sriharsha;[~paetling] Here auto-assign node id is one-time op. Currently users have to declare a broker.id in server.properties and this is unique per broker in a cluster.
Whats this JIRA addressed is to allow users to not to declare broker.id instead kafka when it startsup acquire a sequnce id from zookeeper writes it into meta.properties to use as broker.id
once this is generated and written meta.properties it won't generate a new id. So the problem you are trying solve is not addressed by this JIRA it just allows users not to worry about declaring a broker.id in server.properties.;;;","19/Aug/15 13:55;paetling;Thank you for the help!   I appreciate it.   ;;;","04/Sep/15 13:08;amuraru;[~nehanarkhede] For some reason this patch was not applied correctly in trunk branch, I see an {{.orig}} file in the patch here:
https://github.com/apache/kafka/commit/b1b80860a01cc378cfada3549a3480f0773c3ff8#diff-d0716898c8daed9887ef83500ee0e16e;;;","04/Sep/15 14:38;sriharsha;[~amuraru] Patch is already merged into trunk.;;;","05/Sep/15 11:40;amuraru;Saw it, that's why I raised the flag here, the patch in trunk seems broken with the .orig file added. A new jira issue might be needed to fix it;;;","08/Sep/15 14:43;sriharsha;[~amuraru] I am not sure where you are seeing .orig file in the trunk. Can you paste the link to that file from here https://github.com/apache/kafka;;;","08/Sep/15 14:57;amuraru;[~harsha_ch] I see KAFKA-1973 already fixed that, nw :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MBean kafka.cluster.Partition report wrong UnderReplicated status,KAFKA-1069,12671600,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,vpernin,vpernin,01/Oct/13 15:16,08/Oct/13 16:13,14/Jul/23 05:39,08/Oct/13 16:13,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"Happens on kafka-0.8.0-beta1.
The MBean kafka.cluster.Partition shows an UnderReplicated status which is wrong.

Let's take a simple example :
- one topic named topictest
- replication factor = 2
- 3 nodes

Output of kafka-list-topic.sh command :
topic: topictest        partition: 0    leader: 2       replicas: 3,2   isr: 2,3
topic: topictest        partition: 1    leader: 1       replicas: 1,3   isr: 1,3
topic: topictest        partition: 2    leader: 1       replicas: 2,1   isr: 1,2
topic: topictest        partition: 3    leader: 1       replicas: 3,1   isr: 1,3
topic: topictest        partition: 4    leader: 1       replicas: 1,2   isr: 1,2
topic: topictest        partition: 5    leader: 2       replicas: 2,3   isr: 2,3
topic: topictest        partition: 6    leader: 2       replicas: 3,2   isr: 2,3
topic: topictest        partition: 7    leader: 1       replicas: 1,3   isr: 1,3

So everything is ok, each partition have one follower and the ISR size is 2.

Node 1 which is leader of partition 1 :
MBean ""kafka.cluster"":type=""Partition"",name=""acorreler-1-UnderReplicated"" = 0
Node 3 which if a follower of partition 1 :
""kafka.cluster"":type=""Partition"",name=""acorreler-1-UnderReplicated"" = 1

On each node, the MBean ""kafka.server"":type=""ReplicaManager"",name=""UnderReplicatedPartitions"" reports 0 which is correct.

So it seems that the followers of a partition believe that their partitions replicates are under replicated.",,junrao,nehanarkhede,vpernin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/13 15:25;junrao;KAFKA-1069.patch;https://issues.apache.org/jira/secure/attachment/12607368/KAFKA-1069.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,351310,,,Tue Oct 08 16:13:00 UTC 2013,,,,,,,,,,"0|i1okm7:",351602,,,,,,,,,,,,,,,,,,,,"08/Oct/13 15:25;junrao;Created reviewboard https://reviews.apache.org/r/14534/
;;;","08/Oct/13 15:48;nehanarkhede;Thanks for the patch, +1;;;","08/Oct/13 16:13;junrao;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OfflinePartitionCount metrics may be incorrect after the controller failover,KAFKA-1068,12671290,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,29/Sep/13 23:19,01/Oct/13 20:36,14/Jul/23 05:39,01/Oct/13 20:36,0.8.0,,,0.8.0,,,,,,,,,,0,,,,,,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/13 23:25;junrao;KAFKA-1068.patch;https://issues.apache.org/jira/secure/attachment/12605835/KAFKA-1068.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,350996,,,Tue Oct 01 20:36:43 UTC 2013,,,,,,,,,,"0|i1oio7:",351287,,,,,,,,,,,,,,,,,,,,"29/Sep/13 23:25;junrao;Created reviewboard https://reviews.apache.org/r/14399/
;;;","30/Sep/13 16:05;nehanarkhede;Gave a Ship It on the rb. Thanks for trying out the patch review tool.;;;","01/Oct/13 20:36;nehanarkhede;Checked in the patch to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the default partitioner should be randomizing messages and a new partition for the meta refresh requirements created,KAFKA-1067,12670963,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,charmalloc,charmalloc,27/Sep/13 16:12,07/Oct/16 17:02,14/Jul/23 05:39,11/Jul/14 17:20,,,,0.8.2.0,,,,,,,,,,1,,,,Details behind this http://mail-archives.apache.org/mod_mbox/kafka-dev/201310.mbox/%3CCAFbh0Q0aVh%2Bvqxfy7H-%2BMnRFBt6BnyoZk1LWBoMspwSmTqUKMg%40mail.gmail.com%3E,,amuraru,asafm,charmalloc,joestein,junrao,rberdeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1183,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,350792,,,Thu Jul 07 07:19:50 UTC 2016,,,,,,,,,,"0|i1ohf3:",351083,,,,,,,,,,,,,,,,,,,,"13/Dec/13 19:49;amuraru;A more readable version of the email thread discussing this issue: http://markmail.org/thread/wbr2jni7ndimeoml
Any news on updating the FAQ with the conclusion in that thread?;;;","13/Dec/13 20:55;joestein;I just updated the FAQ https://cwiki.apache.org/confluence/display/KAFKA/FAQ#FAQ-Iamnotsettingthepartitionkeyandhavemorethanonepartitionbutonlyoneconsumerisgettingthemessagesfrommyproducer%3F  
 ;;;","14/Dec/13 04:18;junrao;Thanks, Joe. I reworded the FAQ a bit.;;;","07/Jul/16 07:19;asafm;After hitting this issue in production and searching for a long time until finding this FAQ I'd recommended documenting it where you first search (at least me) - in the code. Have the javadoc of the default partitioner document this weird behavior.
Next would be to document it in the old consumer docs in the site.
I only got to this FAQ through someone who knew about this issue, in the local Java forum (Java.IL).
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad pom.xml definition,KAFKA-1064,12670214,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,ben.manes,ben.manes,24/Sep/13 06:23,10/Oct/13 17:56,14/Jul/23 05:39,10/Oct/13 17:56,0.8.0,,,,,,,,,,,,,0,,,,"The generated pom.xml includes *two* dependency sections. This results in Gradle being unable to resolve transitive dependencies as it inspects the first dependency section, not the second which includes the actual dependencies. Thus, using Kafka in Gradle requires specifying all the dependencies by hand or rewriting the pom.xml and deploying to an internal repository.

http://search.maven.org/remotecontent?filepath=org/apache/kafka/kafka_2.9.1/0.8.0-beta1/kafka_2.9.1-0.8.0-beta1.pom",,ben.manes,charmalloc,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,350043,,,Thu Oct 10 17:56:04 UTC 2013,,,,,,,,,,"0|i1octr:",350337,,,,,,,,,,,,,,,,,,,,"24/Sep/13 13:21;nehanarkhede;I believe the file to be patched is project/Build.scala. Would you like to take a stab at a patch?;;;","24/Sep/13 18:58;ben.manes;My naive change would be to remove the ivyXML declaration as unnecessary. I'm not very familiar with sbt and why ivy metadata is defined, so that may not be the proper fix.;;;","10/Oct/13 17:56;charmalloc;See KAFKA-1018;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reading topic metadata from zookeeper leads to incompatible ordering of partition list,KAFKA-1062,12669601,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,smeder,smeder,20/Sep/13 04:41,20/Sep/13 16:57,14/Jul/23 05:39,20/Sep/13 16:57,0.8.0,,,,,,,,,,consumer,,,0,,,,"The latest consumer changes to read data from Zookeeper during rebalance have made the consumer rebalance code incompatible with older versions (making rolling upgrades without downtime hard). The problem relates to how partitions are ordered. The old code seems to have returned the partitions sorted:

... rebalancing the following partitions: ArrayBuffer(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19) for topic produce-indexable-views with consumers: ...

the new code instead uses:

... rebalancing the following partitions: List(0, 5, 10, 14, 1, 6, 9, 13, 2, 17, 12, 7, 3, 18, 16, 11, 8, 19, 4, 15) for topic produce-indexable-views with consumers: ...

This causes new consumers and old consumers to claim the same partitions. I realize that this may not be a big deal (although painful for us since it disagrees with our deployment automation) since the code wasn't officially released, but it seems simple enough to sort the partitions if you'd take such a patch.
",,guozhang,nehanarkhede,smeder,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/13 04:45;smeder;sorted.patch;https://issues.apache.org/jira/secure/attachment/12604196/sorted.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,349533,,,Fri Sep 20 16:57:54 UTC 2013,,,,,,,,,,"0|i1o9pb:",349831,,,,,,,,,,,,,,,,,,,,"20/Sep/13 05:14;guozhang;+1. Thanks for the patch.

Guozhang;;;","20/Sep/13 16:57;nehanarkhede;+1;;;","20/Sep/13 16:57;nehanarkhede;Checked into 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Break-down sendTime into responseQueueTime and the real sendTime,KAFKA-1060,12669555,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,19/Sep/13 21:11,30/Oct/13 23:23,14/Jul/23 05:39,30/Oct/13 23:23,,,,0.8.1,,,,,,,,,,0,,,,"Currently the responseSendTime in updateRequestMetrics actually contains two portions, the responseQueueTime and the real SendTime. We would like to distinguish these two cases.

This is related to KAFKA-1043",,guozhang,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/13 01:43;guozhang;KAFKA-1060.patch;https://issues.apache.org/jira/secure/attachment/12609975/KAFKA-1060.patch","27/Oct/13 01:45;guozhang;KAFKA-1060_2013-10-26_18:45:01.patch;https://issues.apache.org/jira/secure/attachment/12610462/KAFKA-1060_2013-10-26_18%3A45%3A01.patch","28/Oct/13 17:52;guozhang;KAFKA-1060_2013-10-28_10:52:29.patch;https://issues.apache.org/jira/secure/attachment/12610603/KAFKA-1060_2013-10-28_10%3A52%3A29.patch","29/Oct/13 18:12;guozhang;KAFKA-1060_2013-10-29_11:12:45.patch;https://issues.apache.org/jira/secure/attachment/12610890/KAFKA-1060_2013-10-29_11%3A12%3A45.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,349487,,,Wed Oct 30 23:23:20 UTC 2013,,,,,,,,,,"0|i1o9f3:",349785,,,,,,,,,,,,,,,,,,,,"24/Oct/13 01:43;guozhang;Created reviewboard https://reviews.apache.org/r/14898/
;;;","27/Oct/13 01:45;guozhang;Updated reviewboard https://reviews.apache.org/r/14898/
;;;","28/Oct/13 16:28;nehanarkhede;Looks good to me. [~jkreps], this touches the request channel and adds a metric. Would you like to review it?;;;","28/Oct/13 17:52;guozhang;Updated reviewboard https://reviews.apache.org/r/14898/
;;;","29/Oct/13 18:12;guozhang;Updated reviewboard https://reviews.apache.org/r/14898/
;;;","30/Oct/13 23:23;nehanarkhede;Pushed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trim whitespaces from user specified configs,KAFKA-1057,12669037,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,omkreddy,nehanarkhede,nehanarkhede,17/Sep/13 15:59,03/Sep/15 00:04,14/Jul/23 05:39,04/Oct/14 23:35,,,,0.9.0.0,,,,,,,config,,,2,newbie,,,Whitespaces in configs are a common problem that leads to config errors. It will be nice if Kafka can trim the whitespaces from configs automatically,,gmaas,guozhang,nehanarkhede,omkreddy,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/14 12:20;omkreddy;KAFKA-1057.patch;https://issues.apache.org/jira/secure/attachment/12672931/KAFKA-1057.patch","04/Oct/14 14:48;omkreddy;KAFKA-1057_2014-10-04_20:15:32.patch;https://issues.apache.org/jira/secure/attachment/12672938/KAFKA-1057_2014-10-04_20%3A15%3A32.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,348969,,,Sat Oct 04 23:35:04 UTC 2014,,,,,,,,,,"0|i1o67z:",349267,,,,,,,,,,,,,,,,,,,,"08/May/14 00:15;sriharsha;[~nehanarkhede] can you please give me a bit of background on this bug. 
I tried adding white spaces to the kafka configs they are read properly
for ex:
broker.id =  1   
 port =  9092  
etc..
Let me know if I am looking at the wrong place.;;;","25/Aug/14 13:16;gmaas;Here's an example that has costed me a good day of work:
{code}
14/08/25 12:40:06 WARN kafka.consumer.ConsumerFetcherManager$LeaderFinderThread: [abc.private-1408970387063-6283475b-leader-finder-thread], Failed to find leader for Set([dev,0], [dev,1])
kafka.common.KafkaException: fetching topic metadata for topics [Set(dev)] from broker [ArrayBuffer(id:0,host:172.17.0.91 ,port:9092)] failed
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:67)
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:88)
	at kafka.consumer.ConsumerFetcherManager$LeaderFinderThread.doWork(ConsumerFetcherManager.scala:66)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)
Caused by: java.nio.channels.UnresolvedAddressException
	at sun.nio.ch.Net.checkAddress(Net.java:127)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:644)
	at kafka.network.BlockingChannel.connect(BlockingChannel.scala:57)
	at kafka.producer.SyncProducer.connect(SyncProducer.scala:141)
	at kafka.producer.SyncProducer.getOrMakeConnection(SyncProducer.scala:156)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:68)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:112)
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:53)
{code}

This exception is thrown b/c the configured ip address contains a space:
{code}
advertised.host.name=172.17.0.91  
{code}

(BTW, this ip address is configured by a script using 'hostname -I' which adds a space after the address)

+1 for this bug;;;","04/Sep/14 22:03;guozhang;Moving out of 0.8.2 for now.;;;","14/Sep/14 16:19;nehanarkhede;[~sriharsha] The example used by [~maasg] above is reproducible, though I may not get a chance to take a stab at this anytime soon.;;;","04/Oct/14 12:20;omkreddy;Created reviewboard https://reviews.apache.org/r/26341/diff/
 against branch origin/trunk;;;","04/Oct/14 14:48;omkreddy;Updated reviewboard https://reviews.apache.org/r/26341/diff/
 against branch origin/trunk;;;","04/Oct/14 23:35;nehanarkhede;Thanks for the patch, pushed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BrokerTopicStats is updated before checking for MessageSizeTooLarge,KAFKA-1055,12668919,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,17/Sep/13 00:33,25/Jan/14 02:00,14/Jul/23 05:39,25/Jan/14 02:00,,,,0.8.1,,,,,,,,,,0,,,,"In Log.append function, BrokerTopicStats is updated before the checking for MessageSizeTooLarge, hence even if messages are refused at the server due to this exception, their counts would still be aggregated. Need to move the BrokerTopicStats update after the checking.",,guozhang,jjkoshy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/14 18:28;guozhang;KAFKA-1055.patch;https://issues.apache.org/jira/secure/attachment/12623701/KAFKA-1055.patch","21/Jan/14 21:29;guozhang;KAFKA-1055_2014-01-21_13:29:04.patch;https://issues.apache.org/jira/secure/attachment/12624191/KAFKA-1055_2014-01-21_13%3A29%3A04.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,348851,,,Sat Jan 25 02:00:22 UTC 2014,,,,,,,,,,"0|i1o5hz:",349149,,,,,,,,,,,,,,,,,,,,"17/Jan/14 18:28;guozhang;Created reviewboard https://reviews.apache.org/r/17055/
 against branch origin/trunk;;;","21/Jan/14 21:29;guozhang;Updated reviewboard https://reviews.apache.org/r/17055/
 against branch origin/trunk;;;","25/Jan/14 02:00;jjkoshy;Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
integrate add-partitions command into topicCommand,KAFKA-1052,12668082,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriramsub,junrao,junrao,11/Sep/13 17:07,10/Oct/13 03:46,14/Jul/23 05:39,10/Oct/13 03:46,0.8.1,,,0.8.1,,,,,,,core,,,0,,,,"After merging from 0.8 (kafka-1051), we dragged in a new admin command add-partitions. This needs to be integrated with the general topicCommand.",,junrao,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/13 06:48;sriramsub;KAFKA-1052.patch;https://issues.apache.org/jira/secure/attachment/12607525/KAFKA-1052.patch","09/Oct/13 17:55;sriramsub;KAFKA-1052_2013-10-09_10:55:05.patch;https://issues.apache.org/jira/secure/attachment/12607607/KAFKA-1052_2013-10-09_10%3A55%3A05.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,348016,,,Thu Oct 10 03:46:10 UTC 2013,,,,,,,,,,"0|i1o0cn:",348312,,,,,,,,,,,,,,,,,,,,"09/Oct/13 06:48;sriramsub;Created reviewboard https://reviews.apache.org/r/14554/
;;;","09/Oct/13 17:55;sriramsub;Updated reviewboard ;;;","10/Oct/13 03:46;junrao;Thanks for the patch. +1. Committed to trunk after removing an unused package import.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Type in Documentation for Kafka - server to serve,KAFKA-1048,12667414,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,,jimplush,jimplush,06/Sep/13 23:40,26/Mar/14 23:57,14/Jul/23 05:39,26/Mar/14 23:57,,,,,,,,,,,,,,0,,,,"on the documentation page found here: 
http://kafka.apache.org/documentation.html

this line:
""replicas"" is the list of nodes that are supposed to server the log for this partition regardless of whether they are currently alive.

to this lein (server to serve)
""replicas"" is the list of nodes that are supposed to serve the log for this partition regardless of whether they are currently alive.",,jimplush,jstanier,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,347351,,,Wed Mar 26 21:24:10 UTC 2014,,,,,,,,,,"0|i1nw9j:",347650,,,,,,,,,,,,,,,,,,,,"26/Mar/14 21:24;jstanier;Like KAFKA-1047, this has also been fixed in the documentation so the ticket can be closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Typo in Documentation for Kafka, change ""on"" to ""one""",KAFKA-1047,12667391,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,,jimplush,jimplush,06/Sep/13 20:43,26/Mar/14 23:57,14/Jul/23 05:39,26/Mar/14 23:57,,,,,,,,,,,website,,,0,,,,"Under the ""consumers"" section located at
http://kafka.apache.org/documentation.html

it reads:
""Messaging systems often work around this by having a notion of ""exclusive consumer"" that allows only on process to consume from a queue, but of course this means that there is no parallelism in processing.""

should read: (on vs one)
""that allows only one""",,jimplush,jstanier,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,347328,,,Wed Mar 26 21:22:41 UTC 2014,,,,,,,,,,"0|i1nw4f:",347627,,,,,,,,,,,,,,,,,,,,"26/Mar/14 21:22;jstanier;It looks like this has been fixed in the documentation, so this ticket can be closed now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
producer zk.connect config,KAFKA-1045,12667241,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,shijinkui,shijinkui,06/Sep/13 01:34,26/Jan/15 02:36,14/Jul/23 05:39,20/Jan/15 12:15,,,,0.8.2.0,,,,,,,,,,0,,,,"java.lang.IllegalArgumentException: requirement failed: Missing required property 'metadata.broker.list'


        props.put(""zk.connect"", KafkaConfig.getZooAddress());

when i config zk, why the above tip appear?",,nehanarkhede,omkreddy,shijinkui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,347178,,,Tue Jan 20 12:15:52 UTC 2015,,,,,,,,,,"0|i1nv7b:",347477,,,,,,,,,,,,,,,,,,,,"06/Sep/13 01:47;nehanarkhede;Zookeeper is not required for the Kafka producer anymore, starting Kafka 08. We now require you to plug in a list of brokers. The configuration options are described here - http://kafka.apache.org/08/configuration.html;;;","20/Jan/15 12:15;omkreddy;Zookeeper configs are not required for new and old producer implementations. Hence closing the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
change log4j to slf4j ,KAFKA-1044,12667240,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,viktorsomogyi,shijinkui,shijinkui,06/Sep/13 01:32,22/Nov/17 15:35,14/Jul/23 05:39,22/Nov/17 15:35,0.8.0,,,1.1.0,,,,,,,log,,,7,newbie,,,"can u chanage the log4j to slf4j, in my project, i use logback, it's conflict with log4j.",,ewencp,githubbot,grussell,ijuma,jbarksdale,jkreps,michaelmoss,rehevkor5,rkellogg,shijinkui,stevenschlansker,viktorsomogyi,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,347177,,,Wed Nov 22 15:13:08 UTC 2017,,,,,,,,,,"0|i1nv73:",347476,,,,,,,,,,,,,,,,,,,,"05/Dec/14 23:33;jbarksdale;More specifically, at least swapping to slf4j for the producer and consumer related stuff will allow users to use logging frameworks other than log4j, since it currently does not compile without that as a dependency.  ;;;","08/Dec/14 22:56;jbarksdale;As as workaround, you can exclude log4j from kafka, and just use the org.slf4j:log4j-over-slf4j instead.  That feeds in most log4j commands to slf4j, and then logback or another slf4j binding can be used.  ;;;","07/Feb/15 23:16;jkreps;This would be nice to do. If anyone wants to take it on...;;;","21/Jul/16 20:44;rehevkor5;In addition, can we change the dependency on slf4j-log4j12 from ""compile"" scope to ""runtime"" scope and mark it ""optional""?;;;","24/Jan/17 05:27;ewencp;To clarify for anyone coming back to this, I believe wrt clients this should only apply to the core project, the new client libs should already be using just the slf4j API except in tests.;;;","07/Jun/17 00:27;viktorsomogyi;Hi,

I'm new to Kafka and was searching for a fitting task.
If this is still relevant then I'd like to pick it up. Could someone please add me as contributor so I could assign it to me?;;;","07/Jun/17 04:03;ewencp;[~viktorsomogyi] I've added you as a contributor. I assigned this JIRA to you, but since you are a contributor now you should be able to assign any other JIRAs to yourself as well.;;;","09/Jun/17 18:47;viktorsomogyi;[~ewencp], thank you, started working on it.
I found two incompatibilities and I'm asking your advice in the resolution.
 * Fatal logging level: slf4j doesn't support it
 * kafka.utils.Log4jController: there are some log4j specific features used here that can't be replaced with a generic solution

I have two solutions for these:
 # we could simply use the log4j-over-slf4j package. This will redirect all Fatal logs to Error level and also provides some of the functionalities required by Log4jController (but not all).
    * Pros: a few lines of changes only, quite straightforward
    * Cons: loss of features. However Fatal can't be worked around so we have to accept that, but we also lose Log4jController.getLoggers as there is no way of collecting the current loggers by simply relying on slf4j. Also other methods' behaviour will change (no way to detect existing loggers in slf4j as far as I'm aware of it)
# we could separate off Log4jController into a separate module and have the users to put it on the classpath explicitly if they use log4j for logging. From Logging class we can instantiate it by reflection.
    * Pros: except Fatal logging level we keep all the features
    * Cons: more complicated and breaking, also requires documentation so users will be aware of this

I'll try to create PRs of both solutions (and they are rather work in progress, I just want to show approximate solutions) just in case if that is fine.;;;","03/Jul/17 07:46;githubbot;GitHub user viktorsomogyi opened a pull request:

    https://github.com/apache/kafka/pull/3477

    KAFKA-1044: eliminating log4j from core

    The aim of this PR to move kafka core/main away from log4j and introduce using slf4j only.
    To accomplish this task I:
    - refactored Log4jController into its own module as it is very tightly coupled with log4j (and removing these dependencies would have been impossible without feature loss).
    - as log4j supports FATAL level but slf4j doesn't, I introduced a FATAL marker similarly to the log4j-slf4j bridge

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/viktorsomogyi/kafka KAFKA-1044

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/3477.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #3477
    
----
commit 9abe1a37c2ca4f9ea95fdee394b7135ad779d971
Author: Viktor Somogyi <viktor.somogyi@cloudera.com>
Date:   2017-06-08T22:22:36Z

    KAFKA-1044: eliminating log4j from core

----
;;;","10/Jul/17 10:23;viktorsomogyi;[~ewencp] could you please look at my change when you have time?
An important question has been asked on the review: should we use scala logging?;;;","08/Sep/17 13:24;grussell;Since [log4j is EOL | https://blogs.apache.org/foundation/entry/apache_logging_services_project_announces] it would be beneficial to the community if a solution to this issue (dependency elimination) could get some priority.;;;","08/Sep/17 13:31;viktorsomogyi;Hi [~grussell], currently I need to resolve some merge conflits but let me do that and try to get it reviewed with someone :).;;;","08/Sep/17 13:50;ijuma;I do think we should use scala-logging since it's the most efficient. It doesn't create closures, there is no syntactic penalty and it doesn't generate garbage if the log message won't be printed.;;;","22/Nov/17 15:13;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/3477
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Number of file handles increases indefinitely in producer if broker host is unresolvable,KAFKA-1041,12666893,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,erajasekar,erajasekar,erajasekar,04/Sep/13 14:43,04/Feb/15 16:22,14/Jul/23 05:39,28/Feb/14 16:03,0.8.0,,,0.8.2.0,,,,,,,producer ,,,2,features,newbie,,"We found a issue that if broker host is un resolvable, the number of file handle keep increasing for every message we produce and eventually it uses up all available files handles in operating system. If broker itself is not running and broker host name is resolvable, open file handles count stays flat.

lsof output shows number of these open file handles continue to grow for every message we produce.

 java      19631    relango   81u     sock                0,6      0t0  196966526 can't identify protocol

I can easily reproduce this with console producer,  If I run console producer with right hostname and if broker is not running, the console producer will exit after three tries. But If I run console producer with unresolvable broker, it throws below exception and continues to wait for user input, every time I enter new message, it opens socket and file handle count keeps increasing.. 

Here is Exception in producer

ERROR fetching topic metadata for topics [Set(test-1378245487417)] from broker [ArrayBuffer(id:0,host:localhost1,port:6667)] failed (kafka.utils.Utils$)
kafka.common.KafkaException: fetching topic metadata for topics [Set(test-1378245487417)] from broker [ArrayBuffer(id:0,host:localhost1,port:6667)] failed
        at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:51)
        at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82)
        at kafka.producer.async.DefaultEventHandler$$anonfun$handle$2.apply$mcV$sp(DefaultEventHandler.scala:79)
        at kafka.utils.Utils$.swallow(Utils.scala:186)
        at kafka.utils.Logging$class.swallowError(Logging.scala:105)
        at kafka.utils.Utils$.swallowError(Utils.scala:45)
        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:79)
        at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:104)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:87)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:67)
        at scala.collection.immutable.Stream.foreach(Stream.scala:526)
        at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:66)
        at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)
Caused by: java.nio.channels.UnresolvedAddressException
        at sun.nio.ch.Net.checkAddress(Net.java:30)
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:487)
        at kafka.network.BlockingChannel.connect(BlockingChannel.scala:59)
        at kafka.producer.SyncProducer.connect(SyncProducer.scala:151)
        at kafka.producer.SyncProducer.getOrMakeConnection(SyncProducer.scala:166)
        at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:73)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:117)
        at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:37)
        ... 12 more

",*unix*,akhanzode,erajasekar,fullung,junrao,kdombeck,pasthelod,soumen.sarkar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SAMZA-101,,,,,,"28/Feb/14 15:21;akhanzode;KAFKA-1041-patch.diff;https://issues.apache.org/jira/secure/attachment/12631759/KAFKA-1041-patch.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,346830,,,Fri Feb 28 18:09:05 UTC 2014,,,,,,,,,,"0|i1nt27:",347130,,,,,,,,,,,,,,,,,,,,"13/Sep/13 15:18;junrao;Moving to 0.8.1 since it's not very critical;;;","27/Feb/14 15:28;akhanzode;Is this good enough for this?;;;","27/Feb/14 17:03;junrao;Thanks for the patch. Some comments.

1. We probably should catch all throwables.
2. We can probably just call disconnect() when we hit an exception.;;;","28/Feb/14 15:21;akhanzode;Here is the updated patch for 0.8
Thanks for looking into it.;;;","28/Feb/14 16:03;junrao;Thanks for patch v2. +1 and committed to trunk.;;;","28/Feb/14 17:58;akhanzode;Is 0.8.1 released? Can I get this applied on a released stable branch?;;;","28/Feb/14 18:09;junrao;0.8.1 is being voted now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fetch response should use empty messageset instead of null when handling errors,KAFKA-1038,12666641,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,03/Sep/13 04:26,13/Sep/13 23:30,14/Jul/23 05:39,13/Sep/13 23:30,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"Saw the following exception:

Exception when handling request (kafka.server.KafkaRequestHandler)
java.lang.NullPointerException
        at
kafka.api.FetchResponsePartitionData.<init>(FetchResponse.scala:46)
        at kafka.api.FetchRequest$$anonfun$2.apply(FetchRequest.scala:158)
        at kafka.api.FetchRequest$$anonfun$2.apply(FetchRequest.scala:156)
        at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
        at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
        at
scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:178)
        at
scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:347)
        at
scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:347)
        at
scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
        at scala.collection.immutable.HashMap.map(HashMap.scala:38)
        at kafka.api.FetchRequest.handleError(FetchRequest.scala:156)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:78)
        at
kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:42)
        at java.lang.Thread.run(Thread.java:662)
",,guozhang,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/13 04:27;junrao;kafka-1038.patch;https://issues.apache.org/jira/secure/attachment/12601110/kafka-1038.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,346579,,,Fri Sep 13 23:30:02 UTC 2013,,,,,,,,,,"0|i1nriv:",346880,,,,,,,,,,,,,,,,,,,,"03/Sep/13 04:27;junrao;Attach a patch.;;;","03/Sep/13 05:33;guozhang;What about the ack=0 case? Would the response be sent back to the client with client not expect it coming?;;;","03/Sep/13 15:09;junrao;This patch only fixes the response for the fetch request. So it's unrelated to ack=0, since it is only used in the produce request. You did bring up a good point on ack=0. Will comment on kafka-955.;;;","13/Sep/13 17:01;nehanarkhede;+1;;;","13/Sep/13 23:30;junrao;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
switching to gzip compression no error message for missing snappy jar on classpath,KAFKA-1037,12666353,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,charmalloc,charmalloc,30/Aug/13 16:55,08/Sep/17 09:25,14/Jul/23 05:39,08/Sep/17 09:25,,,,,,,,,,,,,,2,noob,,,seems to be swallowed by not setting the log4j.properties but shows up when this and setting to DEBUG ,,AndrewStein,charmalloc,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1039,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,346292,,,Fri Sep 08 09:25:27 UTC 2017,,,,,,,,,,"0|i1nprb:",346593,,,,,,,,,,,,,,,,,,,,"02/Mar/16 23:10;AndrewStein;This is a severe issue AFAIK. It took three of us three days to track down why our kafka messages were disappearing. At a minimum, if one cannot revert easily to ""gzip"" or ""none"" for compression, this should be logged as an ERROR.;;;","08/Sep/17 09:25;omkreddy;This has been fixed in newer versions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to rename replication offset checkpoint in windows,KAFKA-1036,12666189,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,tnachen,tnachen,29/Aug/13 21:02,14/Dec/18 16:35,14/Jul/23 05:39,15/Oct/13 22:59,0.8.1,,,0.8.1,,,,,,,,,,0,windows,,,"Although there was a fix for checkpoint file renaming in windows that tries to delete the existing checkpoint file if renamed failed, I'm still seeing renaming errors on windows even though the destination file doesn't exist.

A bit investigation shows that it wasn't able to rename the file since the kafka jvm still holds a fie lock on the tmp file and wasn't able to rename it. 

Attaching a patch that calls a explict writer.close so it can release the lock and can able to rename it.",windows,davidlao,jantxu,jkreps,junrao,nehanarkhede,tnachen,trjianjianjiao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-7575,,,,,,,,,,,,,,,,,,,,,"08/Nov/13 11:20;jantxu;filelock.patch.diff;https://issues.apache.org/jira/secure/attachment/12612813/filelock.patch.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,346128,,,Thu Dec 05 17:46:37 UTC 2013,,,,,,,,,,"0|i1norb:",346429,,,,,,,,,,,,,,,,,,,,"05/Sep/13 14:55;junrao;Hi, Tim,

Do you still want to provide a patch? I saw a patch attached and then deleted.

Thanks,;;;","09/Sep/13 17:18;tnachen;Hi Jun, I just realized I don't have clearance to provide a patch yet. It will be much easier if you can help fix this since it's just a one line fix.;;;","13/Sep/13 15:29;junrao;Hmm, not sure how to patch this since we close the writer before renaming the file.;;;","20/Sep/13 22:28;jkreps;I'm a little confused. I don't see any file locking happening in our code. The lock I see is just an in-memory lock and should prevent the file from being deleted.

So perhaps the problem you are describing is that we don't close the file until after the file move? This is legit in unix but perhaps not in windows.;;;","07/Oct/13 23:51;junrao;This seems to be only affecting trunk. So, moving to 0.8.1.;;;","15/Oct/13 22:59;jkreps;Checked in fix on trunk. Note that I don't have access to a windows box so I can't actually validate the fix if anyone who does have access gave this a spin that would be great.;;;","25/Oct/13 10:51;jantxu;Hi Jay, 

I just stumbled upon this issue, since I am on Windows. I just checked out the trunk since I had this problem with the current beta due to this issue. But I still face the same issue:
[2013-10-25 12:43:43,422] FATAL [Replica Manager on Broker 0]: Error writing to
highwatermark file:  (kafka.server.ReplicaManager)
java.io.IOException: File rename from D:\Databases\Kafka\kafka-logs\replication-
offset-checkpoint.tmp to D:\Databases\Kafka\kafka-logs\replication-offset-checkpoint failed.

I also tried to use nio move functionality to see if that solved the problem, but fails for the same reason

Thanks a lot and regards

Jan
;;;","07/Nov/13 19:23;davidlao;Hi Jay, Can you provide a patch for 0.8 as well?  I'm running into similar issue on Windows.;;;","07/Nov/13 19:39;tnachen;Hi Jay,

The code isn't doing any locking, but looks like in Windows if you don't close the writer there is still a pending file lock on the file itself in Windows looking via the file monitor.

That's why I needed to add a extra writer.close after the rename fails.

Tim;;;","08/Nov/13 11:19;jantxu;Hi all,

I think the problem is that the second check for renameTo == true fails although the rename was executed properly. When I remove the second check, it works without problems and the file gets renamed properly (see the attached patch). I guess the root cause of this problem is the platform dependency of the old File API:
Many aspects of the behavior of this method are inherently platform-dependent: The rename operation might not be able to move a file from one filesystem to another, it might not be atomic, and it might not succeed if a file with the destination abstract pathname already exists

Maybe it would be a solution to use NIO instead?

Best regards

Jan;;;","08/Nov/13 11:20;jantxu;The second check for renaming fails on windows, although the renaming worked. ;;;","02/Dec/13 17:16;nehanarkhede;[~jantxu] Are you sure this is required? If we always delete the destination file and then execute renameTo, it should work in all cases, no? [~sriramsub] What do you think?;;;","05/Dec/13 17:46;junrao;We are relying on file renaming being an atomic operation. So, if supported, we should still do rename, instead of deletion followed by creation. The issue with the latter is that if the broker crashes btw the two operations, the broker is left with no checkpoint file.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add message-send-max-retries and retry-backoff-ms options to console producer,KAFKA-1035,12666154,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,erajasekar,erajasekar,erajasekar,29/Aug/13 16:10,05/Sep/13 14:47,14/Jul/23 05:39,05/Sep/13 14:46,0.7,0.8.0,,0.8.0,,,,,,,producer ,,,0,patch,,,It's possible for console producer to give up too soon if it can't find a leader of a topic. Increasing message-send-max-retries would resolve this but.  Console producer doesn't provide options to set message-send-max-retries and retry-backoff-ms. ,,erajasekar,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Aug/13 20:54;erajasekar;console_producer_add_options-v2.patch;https://issues.apache.org/jira/secure/attachment/12600855/console_producer_add_options-v2.patch","29/Aug/13 16:12;erajasekar;console_producer_add_options.patch;https://issues.apache.org/jira/secure/attachment/12600604/console_producer_add_options.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,346093,,,Thu Sep 05 14:46:49 UTC 2013,,,,,,,,,,"0|i1nojj:",346394,,,,,,,,,,,,,,,,,,,,"29/Aug/13 16:12;erajasekar;Patch for fixing this.;;;","29/Aug/13 16:13;erajasekar;See attachment for patch https://issues.apache.org/jira/secure/attachment/12600604/console_producer_add_options.patch;;;","30/Aug/13 00:42;guozhang;Thanks for the patch. One minor comment:

Could you modify the description of message-send-max-retries:

""The leader may be unavailable transiently, which can fail the sending of a message. This property specifies the number of retries when such failures occur""?

1. Brokers can fail receiving the message for multiple reasons, and being unavailable transiently is just one of them.
2. This properties specifies the number of retires before the producer give up and drop this message.;;;","30/Aug/13 17:20;erajasekar;Sure. do you want me to change description as 
""Brokers can fail receiving the message for multiple reasons, and being unavailable transiently is just one of them.This properties specifies the number of retires before the producer give up and drop this message."";;;","30/Aug/13 17:56;guozhang;That is fine.;;;","30/Aug/13 20:56;erajasekar;Attached patch with modified description. Please review.;;;","31/Aug/13 16:52;guozhang;Looks good to me. +1;;;","31/Aug/13 17:44;erajasekar;Thanks, So could you apply patch and commit changes to repo.

Raja.;;;","05/Sep/13 01:03;guozhang;Sorry but I am not one of the committers.. Can someone ([~junrao],[~nehanarkhede],[~jjkoshy]) review/commit it?;;;","05/Sep/13 14:46;junrao;Thanks for the patch. +1. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Messages sent to the old leader will be lost on broker GC resulted failure,KAFKA-1032,12666051,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,29/Aug/13 01:53,17/May/16 14:11,14/Jul/23 05:39,30/Apr/14 16:58,0.8.0,,,0.8.1,,,,,,,,,,0,,,,"As pointed out by Swapnil, today when a broker in on long GC, it will marked by the controller as failed and trigger the onBrokerFailure function to migrate leadership to other brokers. However, since the Controller does not notify the broker with stopReplica request even after a new leader has been elected for its partitions. The new leader will hence stop fetching from the old leader while the old leader is not aware that he is no longer the leader. And since the old leader is not really dead producers will not refresh their metadata immediately and will continue sending messages to the old leader. The old leader will only know it is no longer the leader when it gets notified by controller in the onBrokerStartup function, and message sent starting from the time the new leader is elected to the timestamp the old leader realize it is no longer the leader will be lost.",,guozhang,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/13 23:32;guozhang;KAFKA-1032.v1.patch;https://issues.apache.org/jira/secure/attachment/12600687/KAFKA-1032.v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,345990,,,Wed Apr 30 16:58:38 UTC 2014,,,,,,,,,,"0|i1nnwn:",346291,,,,,,,,,,,,,,,,,,,,"29/Aug/13 01:56;guozhang;Proposed approach:

1. Add addStopReplicaRequestForBrokers with deletion = false to handling replica state change to offline. Now this is only triggered by onBrokerFailure and stopOldReplicasOfReassignedPartition.

2. In shutdownBroker of KafkaController, remove the direct call

brokerRequestBatch.addStopReplicaRequestForBrokers(Seq(id), topicAndPartition.topic, topicAndPartition.partition, deletePartition = false);;;","29/Aug/13 03:39;swapnilghike;The problem is that the leader that GC-ed did not receive become-follower request from controller soon enough, so it kept acting like a leader post GC for some time and appended new messages. These messages were lost when the affected broker became a follower.

The other approach to fix this could involve changing OfflinePartitionLeaderSelector to send LeaderAndIsrRequest to dead brokers, this will ensure that the old leader (if still alive) will stop acting like a leader much sooner. ;;;","29/Aug/13 04:03;nehanarkhede;It seems more natural to send a stop replica request to a failed broker.;;;","29/Aug/13 23:31;guozhang;Note that we also need to delay removal of dead brokers after onBrokerFailure, and inside this function we need to wait until all messages sent by the sender thread.;;;","01/Feb/14 20:31;nehanarkhede;[~junrao], [~guozhang] Do we want this in 0.8.1?;;;","03/Feb/14 21:25;guozhang;I think this can wait until 0.9;;;","30/Apr/14 14:43;junrao;It seems that the main thing that this jira is trying to fix---send StopReplicaRequest to dead broker--- is already checked in. Do we still need this jira?;;;","30/Apr/14 16:58;guozhang;Confirmed that the StopReplicaRequest is sent to the dead broker, hence closing this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Little modification to the stop script to be able to kill the proper process,KAFKA-1031,12665859,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,vpernin,vpernin,vpernin,28/Aug/13 07:21,28/Aug/13 16:49,14/Jul/23 05:39,28/Aug/13 16:49,,,,0.8.0,,,,,,,,,,0,,,,"Escape the . in the kafka.Kafka chain
Also add a grep java to get the real java process and exclude the kafka-run-class.sh process",,junrao,vpernin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/13 07:23;vpernin;0001-Escape-the-.-in-the-kafka.Kafka-chain.patch;https://issues.apache.org/jira/secure/attachment/12600344/0001-Escape-the-.-in-the-kafka.Kafka-chain.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,345798,,,Wed Aug 28 16:49:41 UTC 2013,,,,,,,,,,"0|i1nmpz:",346099,,,,,,,,,,,,,,,,,,,,"28/Aug/13 16:49;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Addition of partitions requires bouncing all the consumers of that topic,KAFKA-1030,12665845,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,guozhang,swapnilghike,swapnilghike,28/Aug/13 04:58,17/Sep/13 21:24,14/Jul/23 05:39,17/Sep/13 21:24,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"Consumer may not notice new partitions because the propagation of the metadata to servers can be delayed. 

Options:
1. As Jun suggested on KAFKA-956, the easiest fix would be to read the new partition data from zookeeper instead of a kafka server.
2. Run a fetch metadata loop in consumer, and set auto.offset.reset to smallest once the consumer has started.

1 sounds easier to do. If 1 causes long delays in reading all partitions at the start of every rebalance, 2 may be worth considering.
 
The same issue affects MirrorMaker when new topics are created, MirrorMaker may not notice all partitions of the new topics until the next rebalance.",,guozhang,nehanarkhede,richardatcloudera,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/13 18:00;guozhang;KAFKA-1030-v1.patch;https://issues.apache.org/jira/secure/attachment/12603632/KAFKA-1030-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,345784,,,Tue Sep 17 21:24:46 UTC 2013,,,,,,,,,,"0|i1nmmv:",346085,,,,,,,,,,,,,,,,,,,,"28/Aug/13 05:02;swapnilghike;As Jun suggested on KAFKA-956, the easiest fix would be to read the new partition data from zookeeper instead of a kafka server.;;;","28/Aug/13 17:45;nehanarkhede;One alternative is to change the topic metadata request handling on the controller broker and let the consumer re-issue a metadata request to the controller broker when the partition change listener fires. If the broker is the controller, it should serve the metadata request by reading from the controller cache directly. If not, it can rely on leaderCache that is updated via the UpdateMetadata request. Upside of this approach is that it won't kill performance and will solve the problem. Downside is that it might make the metadata request handling on the controller broker somewhat slower since it invovlves locking on the controller lock.;;;","28/Aug/13 18:27;swapnilghike;Hmm, this will mean that the consumer client will cease to be controller agnostic. Is that a good idea? Plus if there is a controller failover at the same time as a consumer trying to fetch metadata, the broker the consumer was talking to for fetching metadata may have stale metadata. So, we may need to implement a controller failover watcher on consumer to trigger fetching metadata. Thoughts?;;;","28/Aug/13 20:05;nehanarkhede;Longer term, the right fix will be to move rebalancing to the controller and let it co-ordinate state changes for the consumer. Until then, it will be some sort of a work around to get the latest state changes. To your point, if controller failover is happening, the rebalance attempt will fail. This is no different from leadership changes. I don't see why we need controller failover watch. This controller metadata is only required when partitions change, so it is on-demand.;;;","29/Aug/13 01:06;guozhang;I think the first option might work just okay, since the consumer do not actually needs the partition leader id, etc. All it needs is the map of topic -> list of partition ids. This can be done by just reading one ZK path per topic: /brokers/topics/[topic]. Of course this will put more pressure on MirrorMaker though, but we should really not do full rebalance for added partition or topic anyways..;;;","17/Sep/13 18:00;guozhang;Updated reviewboard https://reviews.apache.org/r/14041/
;;;","17/Sep/13 20:15;guozhang;Here are the performance testing results:

Setup: 1) 5 instances of mirror maker consuming from around 3800 topic/partitions, 2) 1 instance of console consumer consuming from around 300 topic/partitions.

1). Bouncing mirror makers:

ZK-located-in-same-DC: 4 minutes and 20 seconds with the fix

ZK-located-in-same-DC: 3 minutes 50 secs without the fix

ZK-located-in-other-DC: 8 minutes 2 seconds with the fix

ZK-located-in-other-DC: 7 minutes 6 seconds without the fix

2). Bouncing console consumer 

ZK-located-in-same-DC: 15 seconds with the fix

ZK-located-in-same-DC: 15 seconds without the fix

---------------

Given the results, I think it worth pushing this approach (read-from-ZK) in 0.8 and we can later pursue the other approach Joel proposed in the reviewboard in trunk.
;;;","17/Sep/13 20:18;swapnilghike;+1 that Guozhang, thanks for running the tests.;;;","17/Sep/13 21:24;nehanarkhede;Thanks for the updated patch and the performance comparison analysis. I agree that the ideal change might prove to be too large for 0.8 and will require non-trivial amount of time stabilizing it since it is fairly tricky. We can just do it properly on trunk and live with this minor performance hit for consumer rebalancing on 0.8.
;;;","17/Sep/13 21:24;nehanarkhede;
Checked in the latest patch to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Zookeeper leader election stuck in ephemeral node retry loop,KAFKA-1029,12665679,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,smeder,smeder,smeder,27/Aug/13 09:39,21/Aug/14 16:58,14/Jul/23 05:39,27/Aug/13 17:20,0.8.0,,,0.8.0,,,,,,,controller,,,0,,,,"We're seeing the following log statements (over and over):

[2013-08-27 07:21:49,538] INFO conflict in /controller data: { ""brokerid"":3, ""timestamp"":""1377587945206"", ""version"":1 } stored data: { ""brokerid"":2, ""timestamp"":""1377587460904"", ""version"":1 } (kafka.utils.ZkUtils$)

[2013-08-27 07:21:49,559] INFO I wrote this conflicted ephemeral node [{ ""brokerid"":3, ""timestamp"":""1377587945206"", ""version"":1 }] at /controller a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)

where the broker is essentially stuck in the loop that is trying to deal with left-over ephemeral nodes. The code looks a bit racy to me. In particular:

ZookeeperLeaderElector:

  def elect: Boolean = {
    controllerContext.zkClient.subscribeDataChanges(electionPath, leaderChangeListener)
    val timestamp = SystemTime.milliseconds.toString
    val electString = ...

    try {
      createEphemeralPathExpectConflictHandleZKBug(controllerContext.zkClient, electionPath, electString, leaderId,
        (controllerString : String, leaderId : Any) => KafkaController.parseControllerId(controllerString) == leaderId.asInstanceOf[Int],
        controllerContext.zkSessionTimeout)

leaderChangeListener is registered before the create call (by the way, it looks like a new registration will be added every elect call - shouldn't it register in startup()?) so can update leaderId to the current leader before the call to create. If that happens then we will continuously get node exists exceptions and the checker function will always return true, i.e. we will never get out of the while(true) loop.

I think the right fix here is to pass brokerId instead of leaderId when calling create, i.e.

createEphemeralPathExpectConflictHandleZKBug(controllerContext.zkClient, electionPath, electString, brokerId,
        (controllerString : String, leaderId : Any) => KafkaController.parseControllerId(controllerString) == leaderId.asInstanceOf[Int],
        controllerContext.zkSessionTimeout)

The loop dealing with the ephemeral node bug is now only triggered for the broker that owned the node previously, although I am still not 100% sure if that is sufficient.


",,guozhang,jbrosenberg@gmail.com,junrao,miguno,nehanarkhede,smeder,tysonnorris,vincentye38,Yiyang Li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/13 16:51;smeder;0002-KAFKA-1029-Use-brokerId-instead-of-leaderId-when-tri.patch;https://issues.apache.org/jira/secure/attachment/12600201/0002-KAFKA-1029-Use-brokerId-instead-of-leaderId-when-tri.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,345619,,,Thu Aug 21 16:58:58 UTC 2014,,,,,,,,,,"0|i1nlm7:",345920,,,,,,,,,,,,,,,,,,,,"27/Aug/13 16:36;guozhang;Thanks for the catch Sam. Yes the comparison value should be brokerId instead of leaderId. This is a bug in the patch of KAFKA-992.

Also I agree that we only need to register leaderChangeListener once, which we can do in startup, before the elect statement. Could you also make this change?

Thanks,
Guozhang;;;","27/Aug/13 16:40;nehanarkhede;Good catch. The election and leader change update happens on the same lock. It is possible that the old controller becomes the new controller, and the leaderId field is not updated since the elect() API has acquired the controllerLock. This makes broker 3 retry the controller election on behalf of broker 2 and it may never stop until the next controller election.

+1 on this patch;;;","27/Aug/13 16:52;smeder;Updated patch to also include moving the listener registration to startup();;;","27/Aug/13 17:06;guozhang;Thanks for v2. +1 on this one.;;;","27/Aug/13 17:20;nehanarkhede;Thanks for patch v2. Committed it to 0.8;;;","22/Mar/14 05:28;jbrosenberg@gmail.com;I'm seeing this happening now (we are running 0.8).  Should this have been fixed?

It appears to only happen every once in a while, but when it does, it seems persist and repeat indefinitely.  Sometimes restarting the consumer app can fix the problem, but not always.;;;","22/Mar/14 21:00;guozhang;This is a little wired. The patch should have been in 0.8 already. Could you upload the related logs here?;;;","24/Mar/14 06:22;jbrosenberg@gmail.com;Perhaps this should be re-opened a separate ticket?

The issue seems to have started when we had a network outage.  Several high-level consumers could not communicate at all with zookeeper (or kafka) for several minutes.  When the network was restarted, these continual ""I wrote this conlicted ephemeral node...."" log messages have been running steadily, e.g.:

2014-03-19 00:13:14,165  INFO [ZkClient-EventThread-51-myzkserver] utils.ZkUtils$ - conflict in /consumers/myapp/ids/myapp_myhost-1394905418548-e159fc25 data: { ""pattern"":""white_list"", ""subscription"":{ ""(^\\Qmy.event\\E(\\.[\\w-]+)*$)"" : 1 }, ""timestamp"":""1395187970147"", ""version"":1 } stored data: { ""pattern"":""white_list"", ""subscription"":{ ""(^\\Qmy.event\\E(\\.[\\w-]+)*$)"" : 1 }, ""timestamp"":""1395187967170"", ""version"":1 }
2014-03-19 00:13:14,166  INFO [ZkClient-EventThread-51-myzkserver] utils.ZkUtils$ - I wrote this conflicted ephemeral node [{ ""pattern"":""white_list"", ""subscription"":{ ""(^\\Qmy.event\\E(\\.[\\w-]+)*$)"" : 1 }, ""timestamp"":""1395187970147"", ""version"":1 }] at /consumers/myapp/ids/myapp_awa60.sjc1b.square-1394905418548-e159fc25 a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry

These are happening continuously.  We have tried doing a rolling restart of our consumer apps, and even a rolling restart of zk.  We are using zk 3.3.6, in this case, but will try upgrading to 3.4.5 shortly.

It seems that these ephemeral node errors always happen right after a consumer rebalance occurs.;;;","24/Mar/14 15:40;jbrosenberg@gmail.com;I'm starting to suspect my issue is actually due to network latency introduced between the consumers and the zk servers, which is causing zk connection timeouts and flapping.  Thus, I think the issue is probably not related to the original bug in this issue per se (it is having trouble with zk timeouts, possibly resulting in consumer clients not seeing/updating a consistent view of these ephemeral nodes?  Does that make sense?

That being said, it still doesn't seem to make sense that an ephemeral node would not ever get removed eventually.  Also, perhaps the strange folksy error log messaging in this case is assuming rather specific scenarios that don't always apply, which could be more simplified here?;;;","24/Mar/14 16:29;nehanarkhede;[~jbrosenberg@gmail.com] It is a little difficult to say what's going on. It would help if you can tell us how to reproduce this? If you suspect this is due to network outage, try doing a test where you disable the network interface and see if you can reproduce it. That will help us troubleshoot this.;;;","25/Mar/14 03:09;smeder;We've seen this as well (once or twice in ~4 months). The exact conditions under which it occurs have been pretty hard to pin-point.;;;","31/Mar/14 15:51;jbrosenberg@gmail.com;Just to wrap this up, it turns out the app that was seeing this behavior had a but which was causing unnecessary memory pressure and continual garbage collection.  This ended up slowing the app down, such that zk timeouts were continual, and the consumer connector decided continually to keep re-balancing.  This caused the flapping of ephemeral nodes, and other problems, it would seem (the last part here is conjecture).

So, I think the app was the problem, not the high-level kafka consumer code.  However, perhaps there is something that can be done to prevent or throttle continual re-balancing in this case!  It was confusing to look at the logs and see almost exclusively warnings/exceptions coming from kafka, and not the app itself.;;;","29/Apr/14 12:58;miguno;Interestingly I ran into a very similar issue while doing basic validation of Kafka 0.8.1.1.  Note that unlike Jason Rosenberg's case I was only using code/tools/scripts that are shipped with Kafka (e.g. console producer and console consumer).

Here is an example log message, which was repeated indefinitely:

{code}
[2014-04-29 09:48:27,207] INFO conflict in /controller data: {""version"":1,""brokerid"":0,""timestamp"":""1398764901156""} stored data: {""version"":1,""brokerid"":0,""timestamp"":""1398764894941""} (kafka.utils.ZkUtils$)
[2014-04-29 09:48:27,218] INFO I wrote this conflicted ephemeral node [{""version"":1,""brokerid"":0,""timestamp"":""1398764901156""}] at /controller a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
{code}

*How to reproduce (unsolved)*

Unfortunately I cannot consistently reproduce the issue and, to be honest, I am still not sure what actually triggers the bug.  As such I can only summarize what I did before and around the time when this bug was triggered, which I could observe through errors in log files and through errors being displayed after running certain commands.  So yes, it's a bit like shooting in the dark.

Here's an overview of my test setup:

- I deployed Kafka 0.8.1.1 to one machine {{kafka1}} and ZooKeeper 3.4.5 to a second machine {{zookeeper1}}.
- I followed the Kafka 0.8.1 quick start guide to create a topic ""test"" with 1 partition and a replication factor of 1.
- I sent test messages to the topic ""test"" via the console producer.
- I read test messages from the topic ""test"" via the console consumer.
- Apart from producing and consuming a handful of test messages I also ran some supposedly read-only admin commands such as ""describing"" the topic, and running the consumer offset checker tool.
- At ""some"" point, Kafka was caught in an indefinite loop complaining about ""conflict in {{/controller}} data"".


The following paragraphs list in more detail what I did before the error popped up.

Producer:

{code}
$ sudo su - kafka
$ cd /opt/kafka

# This command returned no topics at this point = worked as expected
$ bin/kafka-topics.sh --list --zookeeper zookeeper1

# I created a topic, worked as expected
$ bin/kafka-topics.sh --create --zookeeper zookeeper1 --replication-factor 1 --partitions 1 --topic test

# I requested details of topic ""test"", worked as expected
$ bin/kafka-topics.sh --zookeeper zookeeper1 --describe --topic test
Topic:test	PartitionCount:1	ReplicationFactor:1	Configs:
	Topic: test	Partition: 0	Leader: 0	Replicas: 0	Isr: 0

# I started a console producer and manually send a handful of test messages, worked as expected (see consumer below)
$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
{code}

Consumer:

{code}
$ sudo su - kafka
$ cd /opt/kafka

# I started a console consumer, worked as expected (i.e. could read messages sent by producer, see above)
$ bin/kafka-console-consumer.sh --zookeeper zookeeper1 --topic test --from-beginning
{code}

Up to that point, everything worked.  But then the Kafka broker went the way of the dodo.  As I said I can't pinpoint the cause, and re-running the same commands on a fresh Kafka/ZooKeeper deployment (fresh VMs etc.) didn't consistently trigger the issue like I hoped.

Here's what I did after the commands above, and at some point I eventually did observe the original error described in this JIRA ticket.  Again, at the moment I cannot tell what actually triggered the bug.

Producer:

{code}
# Test-driving the consumer offset checker
$ bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zkconnect zookeeper1
# At this point consumer ""foo"" was not expected to exist.
$ bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zkconnect zookeeper1 --broker-info --group foo

#
# I then re-started the console producer (see below), now configured to use the group id ""foo"".
#
{code}

Consumer:

{code}
# I re-started the console consumer, now configured to use the group id ""foo"".
$ bin/kafka-console-consumer.sh --zookeeper zookeeper1 --topic test --from-beginning --group foo
{code}

At this point ""describing"" the topic gave the following info, indicating that there was a problem (e.g. no leader for partition, no ISR available):

{code}
$ bin/kafka-topics.sh --zookeeper zookeeper1 --describe --topic test
Topic:test	PartitionCount:1	ReplicationFactor:1	Configs:
	Topic: test	Partition: 0	Leader: -1	Replicas: 0	Isr:
{code}

Log files such as {{state-change.log}} showed these error messages:

{code}


[2014-04-29 07:44:47,673] TRACE Controller 0 epoch 1 changed partition [test,0] state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger)
[2014-04-29 07:44:47,679] TRACE Controller 0 epoch 1 changed state of replica 0 for partition [test,0] from NonExistentReplica to NewReplica (state.change.logger)
[2014-04-29 07:44:47,703] TRACE Controller 0 epoch 1 changed partition [test,0] from NewPartition to OnlinePartition with leader 0 (state.change.logger)
[2014-04-29 07:44:47,708] TRACE Controller 0 epoch 1 sending become-leader LeaderAndIsr request (Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:1) with correlationId 7 to broker 0 for partition [test,0] (state.change.logger)
[2014-04-29 07:44:47,716] TRACE Controller 0 epoch 1 sending UpdateMetadata request (Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:1) with correlationId 7 to broker 0 for partition [test,0] (state.change.logger)
[2014-04-29 07:44:47,721] TRACE Controller 0 epoch 1 changed state of replica 0 for partition [test,0] from NewReplica to OnlineReplica (state.change.logger)
[2014-04-29 07:44:47,729] TRACE Broker 0 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:0) correlation id 7 from controller 0 epoch 1 for partition [test,0] (state.change.logger)
[2014-04-29 07:44:47,733] TRACE Broker 0 handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition [test,0] (state.change.logger)
[2014-04-29 07:44:47,741] TRACE Broker 0 stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition [test,0] (state.change.logger)
[2014-04-29 07:44:47,783] TRACE Broker 0 completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition [test,0] (state.change.logger)
[2014-04-29 07:44:47,796] TRACE Controller 0 epoch 1 received response correlationId 7 for a request sent to broker id:0,host:kafka1,port:9092 (state.change.logger)
[2014-04-29 07:44:47,816] TRACE Broker 0 cached leader info (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:0) for partition [test,0] in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 7 (state.change.logger)
[2014-04-29 07:44:47,818] TRACE Controller 0 epoch 1 received response correlationId 7 for a request sent to broker id:0,host:kafka1,port:9092 (state.change.logger)
[2014-04-29 08:48:06,559] TRACE Controller 0 epoch 1 changed partition [test,0] state from OnlinePartition to OfflinePartition (state.change.logger)
[2014-04-29 08:48:06,561] TRACE Controller 0 epoch 1 started leader election for partition [test,0] (state.change.logger)
[2014-04-29 08:48:06,574] ERROR Controller 0 epoch 1 initiated state change for partition [test,0] from OfflinePartition to OnlinePartition failed (state.change.logger)
kafka.common.NoReplicaOnlineException: No replica for partition [test,0] is alive. Live brokers are: [Set()], Assigned replicas are: [List(0)]
        at kafka.controller.OfflinePartitionLeaderSelector.selectLeader(PartitionLeaderSelector.scala:61)
        at kafka.controller.PartitionStateMachine.electLeaderForPartition(PartitionStateMachine.scala:336)
        at kafka.controller.PartitionStateMachine.kafka$controller$PartitionStateMachine$$handleStateChange(PartitionStateMachine.scala:185)
        at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:99)
        at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:96)
        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
[2014-04-29 09:33:16,161] INFO conflict in /controller data: {""version"":1,""brokerid"":0,""timestamp"":""1398761286651""} stored data: {""version"":1,""brokerid"":0,""timestamp"":""1398761286608""} (kafka.utils.ZkUtils$)
[2014-04-29 09:33:16,163] INFO I wrote this conflicted ephemeral node [{""version"":1,""brokerid"":0,""timestamp"":""1398761286651""}] at /controller a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
{code}

I also tried a few admin commands to see what would happen:

{code}
$ bin/kafka-preferred-replica-election.sh --zookeeper zookeeper1 --path-to-json-file /tmp/test.json
Failed to start preferred replica election
kafka.common.AdminCommandFailedException: Admin command failed
	at kafka.admin.PreferredReplicaLeaderElectionCommand.moveLeaderToPreferredReplica(PreferredReplicaLeaderElectionCommand.scala:115)
	at kafka.admin.PreferredReplicaLeaderElectionCommand$.main(PreferredReplicaLeaderElectionCommand.scala:60)
	at kafka.admin.PreferredReplicaLeaderElectionCommand.main(PreferredReplicaLeaderElectionCommand.scala)
Caused by: kafka.admin.AdminOperationException: Preferred replica leader election currently in progress for Set(). Aborting operation
	at kafka.admin.PreferredReplicaLeaderElectionCommand$.writePreferredReplicaElectionData(PreferredReplicaLeaderElectionCommand.scala:101)
	at kafka.admin.PreferredReplicaLeaderElectionCommand.moveLeaderToPreferredReplica(PreferredReplicaLeaderElectionCommand.scala:113)
	... 2 more
{code}

*Workaround*

One workaround I found was to delete the {{/controller}} znode in ZooKeeper, but this seems like swinging a big hammer.  Here's the console history of my ZooKeeper session.  The data associated with {{/controller}} was from a ""buggy"" test run, i.e. a test run where I did observe the indefinite loop error.

{code}
[zk: zookeeper1:2181(CONNECTED) 7] ls /
[zookeeper, admin, consumers, config, controller, brokers, controller_epoch]
[zk: zookeeper1:2181(CONNECTED) 21] ls /controller
[]
[zk: zookeeper1:2181(CONNECTED) 22] get /controller
{""version"":1,""brokerid"":0,""timestamp"":""1398764894941""}
cZxid = 0x2f2
ctime = Tue Apr 29 09:48:21 UTC 2014
mZxid = 0x2f2
mtime = Tue Apr 29 09:48:21 UTC 2014
pZxid = 0x2f2
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x145ac679efa0023
dataLength = 54
numChildren = 0
[zk: zookeeper1:2181(CONNECTED) 23] rmr /controller
{code}


*Addendum 1*

In Jason Rosenberg's case Kafka complained about a {{/consumer/*}} znode:

{code}
conflict in /consumers/myapp/ids/myapp_myhost-1394905418548-e159fc25 data: [...] stored data: [...]
2014-03-19 00:13:14,166 INFO [ZkClient-EventThread-51-myzkserver] utils.ZkUtils$ - I wrote this conflicted ephemeral node [...] at /consumers/myapp/ids/myapp_awa60.sjc1b.square-1394905418548-e159fc25 a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry
{code}

In my case Kafka complained about {{/controller}}:

{code}
[2014-04-29 08:48:06,574] ERROR Controller 0 epoch 1 initiated state change for partition [test,0] from OfflinePartition to OnlinePartition failed (state.change.logger)
kafka.common.NoReplicaOnlineException: No replica for partition [test,0] is alive. Live brokers are: [Set()], Assigned replicas are: [List(0)]
[...]
[2014-04-29 09:33:16,161] INFO conflict in /controller data: {""version"":1,""brokerid"":0,""timestamp"":""1398761286651""} stored data: {""version"":1,""brokerid"":0,""timestamp"":""1398761286608""} (kafka.utils.ZkUtils$)
[2014-04-29 09:33:16,163] INFO I wrote this conflicted ephemeral node [{""version"":1,""brokerid"":0,""timestamp"":""1398761286651""}] at /controller a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
{code}

From what I understand my error message is rather similar to the original error report, which a) also mentions a conflict for {{/controller}}, and b) where the issue seems to be different timestamp values.

*Addendum 2*

FWIW I noticed another similarity between my two ""buggy"" test runs.  In both cases the bug ""triggered"" 64 minutes after the last normal/successful state change.  Not sure what to make out of this observation. :-)  Unfortunately I do not have the full log files from those test runs, otherwise I would have provided them here.  I will make sure to capture all log files now that I'm on alert.

Buggy run #1, going from 07:44:47 to 08:48:06.
{code}
[2014-04-29 07:44:47,783] TRACE Broker 0 completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition [test,0] (state.change.logger)
[2014-04-29 07:44:47,796] TRACE Controller 0 epoch 1 received response correlationId 7 for a request sent to broker id:0,host:kafka1,port:9092 (state.change.logger)
[2014-04-29 07:44:47,816] TRACE Broker 0 cached leader info (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:0) for partition [test,0] in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 7 (state.change.logger)
[2014-04-29 07:44:47,818] TRACE Controller 0 epoch 1 received response correlationId 7 for a request sent to broker id:0,host:kafka1,port:9092 (state.change.logger)
[2014-04-29 08:48:06,559] TRACE Controller 0 epoch 1 changed partition [test,0] state from OnlinePartition to OfflinePartition (state.change.logger)
[2014-04-29 08:48:06,561] TRACE Controller 0 epoch 1 started leader election for partition [test,0] (state.change.logger)
[2014-04-29 08:48:06,574] ERROR Controller 0 epoch 1 initiated state change for partition [test,0] from OfflinePartition to OnlinePartition failed (state.change.logger)
kafka.common.NoReplicaOnlineException: No replica for partition [test,0] is alive. Live brokers are: [Set()], Assigned replicas are: [List(0)]
{code}

Buggy run #2, going from 10:08:31 to 11:12:31.
{code}
[2014-04-29 10:03:16,837] TRACE Broker 0 cached leader info (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:0) for partition [test,0] in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 7 (state.change.logger)
[2014-04-29 10:03:16,838] TRACE Controller 0 epoch 1 received response correlationId 7 for a request sent to broker id:0,host:kafka1,port:9092 (state.change.logger)
[2014-04-29 10:08:31,393] TRACE Controller 0 epoch 1 started leader election for partition [test,0] (state.change.logger)
[2014-04-29 10:08:31,407] TRACE Controller 0 epoch 1 changed partition [test,0] from OnlinePartition to OnlinePartition with leader 0 (state.change.logger)
[2014-04-29 11:12:31,318] TRACE Controller 0 epoch 1 changed partition [test,0] state from OnlinePartition to OfflinePartition (state.change.logger)
[2014-04-29 11:12:31,318] TRACE Controller 0 epoch 1 started leader election for partition [test,0] (state.change.logger)
[2014-04-29 11:12:31,333] ERROR Controller 0 epoch 1 initiated state change for partition [test,0] from OfflinePartition to OnlinePartition failed (state.change.logger)
kafka.common.NoReplicaOnlineException: No replica for partition [test,0] is alive. Live brokers are: [Set()], Assigned replicas are: [List(0)]
{code};;;","29/Apr/14 15:11;junrao;Interesting, did you see any ZK session expiration in the broker log (search for Expired)?;;;","29/Apr/14 18:34;jbrosenberg@gmail.com;I'll add that we still see this from time to time (we're still on 0.8.0), and it's usually after an abnormal event, such as a failed meta-data request for a large group of topics, that times out, etc.  But once this happens, it's very difficult to make it go away, other than restarting the consumer.;;;","30/Apr/14 15:26;miguno;I could finally reproduce the bug again, though still not consistently yet.

And I think I have an idea what the root cause might be:  I was test-driving Kafka in several VMs locally on a Mac MBP laptop, and the energy saver settings (or sth similar) might be responsible for triggering the problem when the laptop was idle.  (In the test run below I intentionally let the laptop idle for 2-3 hours after creating a ""test"" topic and sending/consuming a few test massages.)  The timestamp of when the problem was triggered (11:01) correlates with me unlocking the laptop (user screen).  What's strange is that the laptop is configured not to go to sleep when idle, the hard disk is not asked to sleep either (and even if you did enable that setting in Mac OS X it does not have an effect on SSD's, which is the only disk installed in the laptop I was using), etc.  -- only the display is allowed to turn off after 10 minutes.  So even though the laptop is configured to be ""always on"" there apparently is something that throws off Kafka/ZooKeeper.  Also, as I said in my earlier comment I still cannot reproduce the issue consistently, i.e. sometimes Kafka/ZK work correctly after idling/unlocking;  still I think the root cause has ultimately to do with idling on Mac OS X.

Lastly, I don't know what the expected failure handling of Kafka/ZK is in such a scenario.  From what I can read from the logs below my setup seems to have simulated what could happen during a network partitioning (Kafka broker could not talk to ZooKeeper for a long time, hence ZK session expired, then Kafka could talk again to ZK, but couldn't fully recover).

FWIW I still list further log messages and details just in case that information may be useful in the future.


@Jun Rao:  Yes, I did see ZK expiration in {{controller.log}}:

{code}
INFO [SessionExpirationListener on 0], ZK expired; shut down all controller components and try to re-elect (kafka.controller.KafkaController$SessionExpirationListener)
{code}

Below are some further details.  Things turn bad at 11:01.

*kafka1 (Kafka broker)*

{{server.log}}

{code}
[2014-04-30 07:18:56,481] INFO Completed load of log test-0 with log end offset 0 (kafka.log.Log)
[2014-04-30 07:18:56,485] INFO Created log for partition [test,0] in /app/kafka/log with properties {segment.index.bytes -> 10485760, file.delete.delay.ms -> 60000, segment.bytes -> 1073741824, flush.ms -> 9223372036854775807, delete.retention.ms -> 86400000, index.interval.bytes -> 4096, retention.bytes -> -1, cleanup.policy -> delete, segment.ms -> 172800000, max.message.bytes -> 1000012, flush.messages -> 9223372036854775807, min.cleanable.dirty.ratio -> 0.5, retention.ms -> 172800000}. (kafka.log.LogManager)
[2014-04-30 07:18:56,486] WARN Partition [test,0] on broker 0: No checkpointed highwatermark is found for partition [test,0] (kafka.cluster.Partition)
[2014-04-30 07:19:32,637] INFO Closing socket connection to /127.0.1.1. (kafka.network.Processor)
[2014-04-30 07:19:37,328] INFO Closing socket connection to /127.0.0.1. (kafka.network.Processor)
[2014-04-30 07:19:56,356] INFO Closing socket connection to /127.0.1.1. (kafka.network.Processor)
[2014-04-30 07:20:52,090] ERROR Closing socket for /127.0.1.1 because of error (kafka.network.Processor)
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcher.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:251)
        at sun.nio.ch.IOUtil.read(IOUtil.java:224)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)
        at kafka.utils.Utils$.read(Utils.scala:375)
        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)
        at kafka.network.Processor.read(SocketServer.scala:347)
        at kafka.network.Processor.run(SocketServer.scala:245)
        at java.lang.Thread.run(Thread.java:701)
[2014-04-30 07:21:02,805] INFO Closing socket connection to /127.0.1.1. (kafka.network.Processor)
[2014-04-30 07:21:05,803] INFO Closing socket connection to /127.0.0.1. (kafka.network.Processor)
[2014-04-30 11:01:57,138] INFO Closing socket connection to /127.0.1.1. (kafka.network.Processor)
[2014-04-30 11:01:59,561] INFO Closing socket connection to /127.0.1.1. (kafka.network.Processor)
[2014-04-30 11:01:59,648] INFO 0 successfully elected as leader (kafka.server.ZookeeperLeaderElector)
[2014-04-30 11:01:59,692] INFO conflict in /controller data: {""version"":1,""brokerid"":0,""timestamp"":""1398855719690""} stored data: {""version"":1,""brokerid"":0,""timestamp"":""1398855719646""} (kafka.utils.ZkUtils$)
[2014-04-30 11:01:59,699] INFO I wrote this conflicted ephemeral node [{""version"":1,""brokerid"":0,""timestamp"":""1398855719690""}] at /controller a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
[2014-04-30 11:02:05,704] INFO conflict in /controller data: {""version"":1,""brokerid"":0,""timestamp"":""1398855719690""} stored data: {""version"":1,""brokerid"":0,""timestamp"":""1398855719646""} (kafka.utils.ZkUtils$)
[2014-04-30 11:02:05,711] INFO I wrote this conflicted ephemeral node [{""version"":1,""brokerid"":0,""timestamp"":""1398855719690""}] at /controller a while back in a different session, hence I will backoff for this node to be deleted by Zookeeper and retry (kafka.utils.ZkUtils$)
{code}


{{state-change.log}}

{code}
[2014-04-30 07:18:56,433] TRACE Controller 0 epoch 1 changed state of replica 0 for partition [test,0] from NewReplica to OnlineReplica (state.change.logger)
[2014-04-30 07:18:56,443] TRACE Broker 0 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:0) correlation id 7 from controller 0 epoch 1 for partition [test,0] (state.change.logger)
[2014-04-30 07:18:56,448] TRACE Broker 0 handling LeaderAndIsr request correlationId 7 from controller 0 epoch 1 starting the become-leader transition for partition [test,0] (state.change.logger)
[2014-04-30 07:18:56,455] TRACE Broker 0 stopped fetchers as part of become-leader request from controller 0 epoch 1 with correlation id 7 for partition [test,0] (state.change.logger)
[2014-04-30 07:18:56,495] TRACE Broker 0 completed LeaderAndIsr request correlationId 7 from controller 0 epoch 1 for the become-leader transition for partition [test,0] (state.change.logger)
[2014-04-30 07:18:56,506] TRACE Controller 0 epoch 1 received response correlationId 7 for a request sent to broker id:0,host:kafka1,port:9092 (state.change.logger)
[2014-04-30 07:18:56,525] TRACE Broker 0 cached leader info (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:1),ReplicationFactor:1),AllReplicas:0) for partition [test,0] in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 7 (state.change.logger)
[2014-04-30 07:18:56,526] TRACE Controller 0 epoch 1 received response correlationId 7 for a request sent to broker id:0,host:kafka1,port:9092 (state.change.logger)
[2014-04-30 11:01:59,564] TRACE Controller 0 epoch 1 changed partition [test,0] state from OnlinePartition to OfflinePartition (state.change.logger)
[2014-04-30 11:01:59,569] TRACE Controller 0 epoch 1 started leader election for partition [test,0] (state.change.logger)
[2014-04-30 11:01:59,589] ERROR Controller 0 epoch 1 initiated state change for partition [test,0] from OfflinePartition to OnlinePartition failed (state.change.logger)
kafka.common.NoReplicaOnlineException: No replica for partition [test,0] is alive. Live brokers are: [Set()], Assigned replicas are: [List(0)]
        at kafka.controller.OfflinePartitionLeaderSelector.selectLeader(PartitionLeaderSelector.scala:61)
        at kafka.controller.PartitionStateMachine.electLeaderForPartition(PartitionStateMachine.scala:336)
        at kafka.controller.PartitionStateMachine.kafka$controller$PartitionStateMachine$$handleStateChange(PartitionStateMachine.scala:185)
        at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:99)
        at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:96)
        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
        at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:96)
        at kafka.controller.KafkaController.onBrokerFailure(KafkaController.scala:433)
        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ReplicaStateMachine.scala:344)
        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1$$anonfun$apply$mcV$sp$1.apply(ReplicaStateMachine.scala:330)
        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1$$anonfun$apply$mcV$sp$1.apply(ReplicaStateMachine.scala:330)
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1.apply$mcV$sp(ReplicaStateMachine.scala:329)
        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1.apply(ReplicaStateMachine.scala:328)
        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1.apply(ReplicaStateMachine.scala:328)
        at kafka.utils.Utils$.inLock(Utils.scala:538)
        at kafka.controller.ReplicaStateMachine$BrokerChangeListener.handleChildChange(ReplicaStateMachine.scala:327)
        at org.I0Itec.zkclient.ZkClient$7.run(ZkClient.java:568)
        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[2014-04-30 11:01:59,625] TRACE Controller 0 epoch 1 changed state of replica 0 for partition [test,0] from OnlineReplica to OfflineReplica (state.change.logger)
[2014-04-30 11:01:59,676] TRACE Controller 0 epoch 2 started leader election for partition [test,0] (state.change.logger)
[2014-04-30 11:01:59,686] ERROR Controller 0 epoch 2 initiated state change for partition [test,0] from OfflinePartition to OnlinePartition failed (state.change.logger)
kafka.common.NoReplicaOnlineException: No replica for partition [test,0] is alive. Live brokers are: [Set()], Assigned replicas are: [List(0)]
        at kafka.controller.OfflinePartitionLeaderSelector.selectLeader(PartitionLeaderSelector.scala:61)
{code}


{{controller.log}}

{code}
[2014-04-30 07:18:56,354] DEBUG [TopicChangeListener on Controller 0]: Topic change listener fired for path /brokers/topics with children test (kafka.controller.PartitionStateMachine$TopicChangeListener)
[2014-04-30 07:18:56,373] INFO [TopicChangeListener on Controller 0]: New topics: [Set(test)], deleted topics: [Set()], new partition replica assignment [Map([test,0] -> List(0))] (kafka.controller.PartitionStateMachine$TopicChangeListener)
[2014-04-30 07:18:56,373] INFO [Controller 0]: New topic creation callback for [test,0] (kafka.controller.KafkaController)
[2014-04-30 07:18:56,376] INFO [Controller 0]: New partition creation callback for [test,0] (kafka.controller.KafkaController)
[2014-04-30 07:18:56,376] INFO [Partition state machine on Controller 0]: Invoking state change to NewPartition for partitions [test,0] (kafka.controller.PartitionStateMachine)
[2014-04-30 07:18:56,391] INFO [Replica state machine on controller 0]: Invoking state change to NewReplica for replicas [Topic=test,Partition=0,Replica=0] (kafka.controller.ReplicaStateMachine)
[2014-04-30 07:18:56,394] INFO [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions [test,0] (kafka.controller.PartitionStateMachine)
[2014-04-30 07:18:56,395] DEBUG [Partition state machine on Controller 0]: Live assigned replicas for partition [test,0] are: [List(0)] (kafka.controller.PartitionStateMachine)
[2014-04-30 07:18:56,398] DEBUG [Partition state machine on Controller 0]: Initializing leader and isr for partition [test,0] to (Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:1) (kafka.controller.PartitionStateMachine)
[2014-04-30 07:18:56,431] INFO [Replica state machine on controller 0]: Invoking state change to OnlineReplica for replicas [Topic=test,Partition=0,Replica=0] (kafka.controller.ReplicaStateMachine)
[2014-04-30 11:01:59,560] INFO [BrokerChangeListener on Controller 0]: Broker change listener fired for path /brokers/ids with children  (kafka.controller.ReplicaStateMachine$BrokerChangeListener)
[2014-04-30 11:01:59,560] INFO [BrokerChangeListener on Controller 0]: Newly added brokers: , deleted brokers: 0, all live brokers:  (kafka.controller.ReplicaStateMachine$BrokerChangeListener)
[2014-04-30 11:01:59,561] INFO [Controller-0-to-broker-0-send-thread], Shutting down (kafka.controller.RequestSendThread)
[2014-04-30 11:01:59,562] INFO [Controller-0-to-broker-0-send-thread], Stopped  (kafka.controller.RequestSendThread)
[2014-04-30 11:01:59,562] INFO [Controller-0-to-broker-0-send-thread], Shutdown completed (kafka.controller.RequestSendThread)
[2014-04-30 11:01:59,563] INFO [Controller 0]: Broker failure callback for 0 (kafka.controller.KafkaController)
[2014-04-30 11:01:59,564] INFO [Controller 0]: Removed ArrayBuffer() from list of shutting down brokers. (kafka.controller.KafkaController)
[2014-04-30 11:01:59,564] INFO [Partition state machine on Controller 0]: Invoking state change to OfflinePartition for partitions [test,0] (kafka.controller.PartitionStateMachine)
[2014-04-30 11:01:59,588] DEBUG [OfflinePartitionLeaderSelector]: No broker in ISR is alive for [test,0]. Pick the leader from the alive assigned replicas:  (kafka.controller.OfflinePartitionLeaderSelector)
[2014-04-30 11:01:59,595] INFO [Replica state machine on controller 0]: Invoking state change to OfflineReplica for replicas [Topic=test,Partition=0,Replica=0] (kafka.controller.ReplicaStateMachine)
[2014-04-30 11:01:59,597] DEBUG [Controller 0]: Removing replica 0 from ISR 0 for partition [test,0]. (kafka.controller.KafkaController)
[2014-04-30 11:01:59,625] INFO [Controller 0]: New leader and ISR for partition [test,0] is {""leader"":-1,""leader_epoch"":1,""isr"":[]} (kafka.controller.KafkaController)
[2014-04-30 11:01:59,628] DEBUG The stop replica request (delete = true) sent to broker 0 is  (kafka.controller.ControllerBrokerRequestBatch)
[2014-04-30 11:01:59,629] DEBUG The stop replica request (delete = false) sent to broker 0 is [Topic=test,Partition=0,Replica=0] (kafka.controller.ControllerBrokerRequestBatch)
[2014-04-30 11:01:59,635] WARN [Channel manager on controller 0]: Not sending request Name: StopReplicaRequest; Version: 0; CorrelationId: 11; ClientId: ; DeletePartitions: false; ControllerId: 0; ControllerEpoch: 1; Partitions: [test,0] to broker 0, since it is offline. (kafka.controller.ControllerChannelManager)
[2014-04-30 11:01:59,644] INFO [Controller 0]: Controller shutdown complete (kafka.controller.KafkaController)
[2014-04-30 11:01:59,649] INFO [Controller 0]: Broker 0 starting become controller state transition (kafka.controller.KafkaController)
[2014-04-30 11:01:59,651] INFO [Controller 0]: Controller 0 incremented epoch to 2 (kafka.controller.KafkaController)
[2014-04-30 11:01:59,672] INFO [Controller 0]: Partitions undergoing preferred replica election:  (kafka.controller.KafkaController)
[2014-04-30 11:01:59,672] INFO [Controller 0]: Partitions that completed preferred replica election:  (kafka.controller.KafkaController)
[2014-04-30 11:01:59,672] INFO [Controller 0]: Resuming preferred replica election for partitions:  (kafka.controller.KafkaController)
[2014-04-30 11:01:59,673] INFO [Controller 0]: Partitions being reassigned: Map() (kafka.controller.KafkaController)
[2014-04-30 11:01:59,673] INFO [Controller 0]: Partitions already reassigned: List() (kafka.controller.KafkaController)
[2014-04-30 11:01:59,673] INFO [Controller 0]: Resuming reassignment of partitions: Map() (kafka.controller.KafkaController)
[2014-04-30 11:01:59,675] INFO [Controller 0]: List of topics to be deleted:  (kafka.controller.KafkaController)
[2014-04-30 11:01:59,675] INFO [Controller 0]: List of topics ineligible for deletion: test (kafka.controller.KafkaController)
[2014-04-30 11:01:59,675] INFO [Controller 0]: Currently active brokers in the cluster: Set() (kafka.controller.KafkaController)
[2014-04-30 11:01:59,675] INFO [Controller 0]: Currently shutting brokers in the cluster: Set() (kafka.controller.KafkaController)
[2014-04-30 11:01:59,675] INFO [Controller 0]: Current list of topics in the cluster: Set(test) (kafka.controller.KafkaController)
[2014-04-30 11:01:59,676] INFO [Replica state machine on controller 0]: Started replica state machine with initial state -> Map([Topic=test,Partition=0,Replica=0] -> ReplicaDeletionIneligible) (kafka.controller.ReplicaStateMachine)
[2014-04-30 11:01:59,685] DEBUG [OfflinePartitionLeaderSelector]: No broker in ISR is alive for [test,0]. Pick the leader from the alive assigned replicas:  (kafka.controller.OfflinePartitionLeaderSelector)
[2014-04-30 11:01:59,686] INFO [Partition state machine on Controller 0]: Started partition state machine with initial state -> Map([test,0] -> OfflinePartition) (kafka.controller.PartitionStateMachine)
[2014-04-30 11:01:59,687] INFO [Controller 0]: Broker 0 is ready to serve as the new controller with epoch 2 (kafka.controller.KafkaController)
[2014-04-30 11:01:59,688] INFO [Controller 0]: Starting preferred replica leader election for partitions  (kafka.controller.KafkaController)
[2014-04-30 11:01:59,688] INFO [Partition state machine on Controller 0]: Invoking state change to OnlinePartition for partitions  (kafka.controller.PartitionStateMachine)
[2014-04-30 11:01:59,690] INFO [SessionExpirationListener on 0], ZK expired; shut down all controller components and try to re-elect (kafka.controller.KafkaController$SessionExpirationListener)
[2014-04-30 11:01:59,690] INFO [Controller 0]: Controller shutdown complete (kafka.controller.KafkaController)
{code}


*zookeeper1 (ZooKeeper server)*

{{zookeeper.log}}

{code}
2014-04-30 07:32:37,460 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.0.0.21:43468
2014-04-30 07:32:37,462 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@793] - Connection request from old client /10.0.0.21:43468; will be dropped if server is in r-o mode
2014-04-30 07:32:37,462 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@832] - Client attempting to renew session 0x145b15f015d0000 at /10.0.0.21:43468
2014-04-30 07:32:37,463 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@595] - Established session 0x145b15f015d0000 with negotiated timeout 6000 for client /10.0.0.21:43468
2014-04-30 11:01:57,832 [myid:] - INFO  [SessionTracker:ZooKeeperServer@325] - Expiring session 0x145b15f015d0000, timeout of 6000ms exceeded
2014-04-30 11:01:57,833 [myid:] - INFO  [SessionTracker:ZooKeeperServer@325] - Expiring session 0x145b15f015d0009, timeout of 6000ms exceeded
2014-04-30 11:01:57,837 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@476] - Processed session termination for sessionid: 0x145b15f015d0000
2014-04-30 11:01:57,837 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@476] - Processed session termination for sessionid: 0x145b15f015d0009
2014-04-30 11:01:57,842 [myid:] - INFO  [SyncThread:0:NIOServerCnxn@1001] - Closed socket connection for client /10.0.0.21:43468 which had sessionid 0x145b15f015d0000
2014-04-30 11:01:57,845 [myid:] - INFO  [SyncThread:0:NIOServerCnxn@1001] - Closed socket connection for client /10.0.0.21:43467 which had sessionid 0x145b15f015d0009
2014-04-30 11:01:59,001 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.0.0.21:43469
2014-04-30 11:01:59,002 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@793] - Connection request from old client /10.0.0.21:43469; will be dropped if server is in r-o mode
2014-04-30 11:01:59,002 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@832] - Client attempting to renew session 0x145b15f015d0009 at /10.0.0.21:43469
2014-04-30 11:01:59,003 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@588] - Invalid session 0x145b15f015d0009 for client /10.0.0.21:43469, probably expired
2014-04-30 11:01:59,004 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1001] - Closed socket connection for client /10.0.0.21:43469 which had sessionid 0x145b15f015d0009
2014-04-30 11:01:59,005 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.0.0.21:43470
2014-04-30 11:01:59,008 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@793] - Connection request from old client /10.0.0.21:43470; will be dropped if server is in r-o mode
2014-04-30 11:01:59,008 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@839] - Client attempting to establish new session at /10.0.0.21:43470
2014-04-30 11:01:59,012 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@595] - Established session 0x145b15f015d000b with negotiated timeout 6000 for client /10.0.0.21:43470
2014-04-30 11:01:59,545 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.0.0.21:43471
2014-04-30 11:01:59,545 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@793] - Connection request from old client /10.0.0.21:43471; will be dropped if server is in r-o mode
2014-04-30 11:01:59,545 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@832] - Client attempting to renew session 0x145b15f015d0000 at /10.0.0.21:43471
2014-04-30 11:01:59,546 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@588] - Invalid session 0x145b15f015d0000 for client /10.0.0.21:43471, probably expired
2014-04-30 11:01:59,546 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1001] - Closed socket connection for client /10.0.0.21:43471 which had sessionid 0x145b15f015d0000
2014-04-30 11:01:59,553 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.0.0.21:43472
2014-04-30 11:01:59,555 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@793] - Connection request from old client /10.0.0.21:43472; will be dropped if server is in r-o mode
2014-04-30 11:01:59,556 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@839] - Client attempting to establish new session at /10.0.0.21:43472
2014-04-30 11:01:59,557 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@595] - Established session 0x145b15f015d000c with negotiated timeout 6000 for client /10.0.0.21:43472
2014-04-30 11:01:59,687 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@627] - Got user-level KeeperException when processing sessionid:0x145b15f015d000c type:delete cxid:0x19 zxid:0x5c txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
2014-04-30 11:01:59,689 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@627] - Got user-level KeeperException when processing sessionid:0x145b15f015d000c type:create cxid:0x1a zxid:0x5d txntype:-1 reqpath:n/a Error Path:/controller Error:KeeperErrorCode = NodeExists for /controller
2014-04-30 11:02:05,700 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@627] - Got user-level KeeperException when processing sessionid:0x145b15f015d000c type:create cxid:0x1d zxid:0x5e txntype:-1 reqpath:n/a Error Path:/controller Error:KeeperErrorCode = NodeExists for /controller
{code}


*State of topic after issue did trigger*

{code}
$ date;bin/kafka-topics.sh --zookeeper zookeeper1 --describe --topic test
Wed Apr 30 11:02:25 UTC 2014
Topic:test	PartitionCount:1	ReplicationFactor:1	Configs:
	Topic: test	Partition: 0	Leader: -1	Replicas: 0	Isr:
{code}


*Notes*

Kafka kinda recovered at some point.

What indicated a recovery:

- {{state-change.log}} reverted back to normal messages (partition wen tfrom OfflinePartition to OnlinePartition with leader 0; leader -1 was replaced with leader 0; etc.)
- {{kafka-topics.sh --descripe --topic test}} showed normal operations, too, i.e. one partition with one replica with one leader and with one ISR.

What speaks against a full recovery:

- {{server.log}} was still showing an indefinite loop of messages {{I wrote this conflicted ephemeral node [{""version"":1,""brokerid"":0,""timestamp"":""1398860644679""}] at /controller a while back in a different session}}.;;;","24/Jul/14 06:58;vincentye38;I think the problem is because registerSessionExpirationListener() is called before controllerElector.startup in kafkaController.startup().
If the session expired before the election happens in controllerElector.startup, SessionExpirationListener calls election to create a ephemeral node. Then the election triggered by controllerElector.startup will run into the infinite loop dealing with the ephemeral node bug.;;;","25/Jul/14 05:19;junrao;I think the proposed fix in KAFKA-1451 will fix this issue too.;;;","21/Aug/14 15:49;Yiyang Li;Hello, I am totally new to Kafka, and we are .Net developers who is testing the kafka by the kafka-net library. The Nuget package is still under alpha version, leaving lots of functionality not implemented. 

We recently got this problem when we embed a producer in a service, where according to the library, it will do a ResponseTimeoutCheck every 30000 ms. However, one of the client throws an error (the others are fine)

ERROR Closing socket for /10.207.x.x because of error (kafka.network.Processor)
java.io.IOException: An existing connection was forcibly closed by the remote host
        at sun.nio.ch.SocketDispatcher.write0(Native Method)
        at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:51)
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
        at sun.nio.ch.IOUtil.write(IOUtil.java:65)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:466)
        at kafka.api.FetchResponseSend.writeTo(FetchResponse.scala:217)
        at kafka.network.Processor.write(SocketServer.scala:375)
        at kafka.network.Processor.run(SocketServer.scala:247)
        at java.lang.Thread.run(Thread.java:745)
log4j:ERROR Failed to rename [/cygdrive/c/kafka/bin/../logs/server.log] to [/cygdrive/c/kafka/bin/../logs/server.log.2014-08-20-17].

under the kafka-server-start.sh

I have change the log4j properties to 

log4j.appender.kafkaAppender.DatePattern='.'yyyy-MM
log4j.appender.kafkaAppender.File=${kafka.logs.dir}/server.log

Other backgound:
Kafka binaries: Scala 2.9.2 - kafka_2.9.2-0.8.1.1
Brokers: 3 brokers in 3 different machines, using the same port under each IP  (borker id = 0, 1, 2)
Zookeeper: 2181 port at one of the brokers

I understand that it might be hard to reproduce as it might be the problem in incomplete .Net client

Details of ResponseTimeoutCheck:

https://github.com/YiyangLi/kafka-net/blob/master/src/kafka-net/KafkaConnection.cs (Line 200)

Thanks. 
;;;","21/Aug/14 16:32;guozhang;Hello Yiyang,

Could you send an email to the users mailing list about your issue since it seems not relevant to this jira?;;;","21/Aug/14 16:36;Yiyang Li;could you elaborate the users mailing list? 

It's the same issue as the following:

http://grokbase.com/t/kafka/users/141nmnah7e/kafka-server-occure-java-nio-bufferunderflowexception;;;","21/Aug/14 16:58;guozhang;You can send an email to users@kafka.apache.org.

Here is the summary of the mailing lists:

http://kafka.apache.org/contact.html;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamically Adjust Batch Size Upon Receiving MessageSizeTooLargeException,KAFKA-1026,12665638,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,27/Aug/13 02:04,11/Jul/14 17:44,14/Jul/23 05:39,11/Jul/14 17:44,,,,0.8.2.0,,,,,,,,,,0,newbie++,,,"Among the exceptions that can possibly received in Producer.send(), MessageSizeTooLargeException is currently not recoverable since the producer does not change the batch size but still retries on sending. It is better to have a dynamic batch size adjustment mechanism based on MessageSizeTooLargeException.

This is related to KAFKA-998",,guozhang,jkreps,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,345578,,,Fri Jul 11 17:44:16 UTC 2014,,,,,,,,,,"0|i1nld3:",345879,,,,,,,,,,,,,,,,,,,,"27/Aug/13 18:05;swapnilghike;As David Demaagd had mentioned earlier, it will be useful to have a bytes/numMessages/time based decision in sending the messages.;;;","28/Aug/13 17:01;jkreps;This is a good idea, but I recommend we put this off until the client re-write.;;;","11/Jul/14 17:26;jkreps;I believe this is fixed in the new producer, no? We will allocate up to the maximum memory size for messages larger than the batch size.;;;","11/Jul/14 17:31;guozhang;There is a rare case when we can still hit the MessageSizeTooLargeException, which is when compression is turned on, we can have overflow in the underlying byte buffer due to inaccurate compression rate estimate and reallocate with larger size. But this case would be very rare I think.

In general, I think most of the JIRAs related to the old producer can be closed after 0.8.2 is released with the new producer officially replacing the old producer.;;;","11/Jul/14 17:44;jkreps;Cool, closing with fix as 0.8.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove getAllReplicasOnBroker from KafkaController,KAFKA-1020,12665134,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,22/Aug/13 22:39,15/Jan/14 16:36,14/Jul/23 05:39,15/Jan/14 16:36,,,,0.8.1,,,,,,,,,,0,,,,"Today KafkaController call getAllReplicasOnBroker on broker failure and new broker start up to get all the replicas that broker is holding (or suppose to hold). This function actually issue a read on each topic's partition znodes. With large number of topic/partitions this could seriously increase the latency of handling broker failure and new broker startup.

On the other hand, ControllerContext maintains a partitionReplicaAssignment cache, which is designed to keep the most updated partition replica assignment according to ZK. So instead of reading from ZK, we could just read from the local cache, given that partitionReplicaAssignment is guaranteed to be up-to-date.",,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/13 22:00;guozhang;KAFKA-1020.v1.patch;https://issues.apache.org/jira/secure/attachment/12600270/KAFKA-1020.v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,345075,,,Wed Jan 15 16:36:37 UTC 2014,,,,,,,,,,"0|i1ni9r:",345376,,,,,,,,,,,,,,,,,,,,"27/Aug/13 00:48;guozhang;Proposed Approach:

1. Currently the partitionReplicaAssignment cache has two point of inconsistency with the ZK data: 1) in handling replica state change to NonExistentReplica we remove the replica in partitionReplicaAssignment before we update ZK; 2) in handling replica state change from New to Online we add the replica to partitionReplicaAssignment before we update ZK. Since partitionReplicaAssignment will be updated later in ZK, we can safely remove these two updates.

2. Then we can safely change the getAllReplicasOnBroker to reading from partitionReplicaAssignment instead of from ZK.

3. Some minor issues such as comments will also be fixed in this JIRA. ;;;","15/Jan/14 16:36;junrao;Fixed as part of kafka-1202.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tidy up the POM from what feedback has come from the 0.8 beta and publishing to maven,KAFKA-1018,12664627,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,charmalloc,charmalloc,20/Aug/13 15:35,11/Oct/13 05:00,14/Jul/23 05:39,10/Oct/13 17:53,,,,0.8.0,,,,,,,,,,1,,,,"from Chris Riccomini 

1. Maven central can't resolve it properly (POM is different from Apache release). Have to use Apache release repo directly to get things to work.
2. Exclusions must be manually applied even though they exist in Kafka's POM already. I think Maven can handle this automatically, if the POM is done right.
3. Weird parent block in Kafka POMs that points to org.apache.
4. Would be nice to publish kafka-test jars as well.
5. Would be nice to have SNAPSHOT releases off of trunk using a Hudson job.
Our hypothesis regarding the first issue is that it was caused by duplicate publishing during testing, and it should go away in the future.
Regarding number 2, I have to explicitly exclude the following when depending on Kafka:
exclude module: 'jms'
exclude module: 'jmxtools'
exclude module: 'jmxri'
I believe these just need to be excluded from the appropriate jars in the actual SBT build file, to fix this issue. I see JMS is excluded from ZK, but it's probably being pulled in from somewhere else, anyway.
Regarding number 3, it is indeed listed as something to do on the Apache publication page (http://www.apache.org/dev/publishing-maven-artifacts.html). I can't find an example of anyone using it, but it doesn't seem to be doing any harm.
Also, regarding your intransitive() call, that is disabling ALL dependencies not just the exclusions, I believe. I think that the ""proper"" way to do that would be to do what I've done: exclude(""jms"", ""jmxtools"", ""jmxri""). Regardless, fixing number 2, above, should mean that intransitive()/exclude() are not required.",,charmalloc,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/13 07:10;charmalloc;KAFKA-1018.patch;https://issues.apache.org/jira/secure/attachment/12607762/KAFKA-1018.patch","10/Oct/13 07:10;charmalloc;KAFKA-1018.patch;https://issues.apache.org/jira/secure/attachment/12607761/KAFKA-1018.patch","10/Oct/13 07:09;charmalloc;KAFKA-1018.patch;https://issues.apache.org/jira/secure/attachment/12607760/KAFKA-1018.patch","10/Oct/13 07:09;charmalloc;KAFKA-1018.patch;https://issues.apache.org/jira/secure/attachment/12607759/KAFKA-1018.patch","10/Oct/13 07:09;charmalloc;KAFKA-1018.patch;https://issues.apache.org/jira/secure/attachment/12607758/KAFKA-1018.patch","10/Oct/13 07:08;charmalloc;KAFKA-1018.patch;https://issues.apache.org/jira/secure/attachment/12607757/KAFKA-1018.patch","10/Oct/13 07:08;charmalloc;KAFKA-1018.patch;https://issues.apache.org/jira/secure/attachment/12607756/KAFKA-1018.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,344570,,,Fri Oct 11 05:00:14 UTC 2013,,,,,,,,,,"0|i1nf5b:",344870,,,,,,,,,,,,,,,,,,,,"19/Sep/13 23:19;nehanarkhede;[~joestein] This is marked for the 0.8 final release. Do you think you could help look into this?;;;","10/Oct/13 04:56;charmalloc;1) maven central is immutable, resolved in this coming release
2) I think your right, yup.  This was fixed on branch after found in maven central.
3) just going by the docs
4) I have a fix for this I am testing right now
5) yup, agreed.  I can create one patch for trunk and one for the 0.8 branch, leave trunk on SNAPSHOT and then another JIRA for hooking it up to hudson I gotta look into that

The changes I am testing are in a repo here http://ec2-54-224-196-57.compute-1.amazonaws.com:8081/nexus/content/repositories/releases with the pom of http://ec2-54-224-196-57.compute-1.amazonaws.com:8081/nexus/content/repositories/releases/org/apache/kafka/kafka_2.10/0.8.0/kafka_2.10-0.8.0.pom

I am running into some issues I am bumping into while testing, working that out now

;;;","10/Oct/13 07:10;charmalloc;Created reviewboard https://reviews.apache.org/r/14571/
;;;","11/Oct/13 04:54;junrao;Could you double commit this to trunk? Thanks,;;;","11/Oct/13 05:00;charmalloc;yup, done;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
High number of open file handles in 0.8 producer,KAFKA-1017,12664461,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,swapnilghike,swapnilghike,19/Aug/13 21:49,22/Aug/13 17:04,14/Jul/23 05:39,22/Aug/13 16:48,0.8.0,,,0.8.0,,,,,,,producer ,,,0,,,,"Reported by Jun Rao:

For over-partitioned topics, each broker could be the leader for at least 1 partition. In the producer, we randomly select a partition to send the data. Pretty soon, each producer will establish a connection to each of the n brokers. Effectively, we increased the # of socket connections by a factor of n, compared to 0.7.

The increased number of socket connections increases the number of open file handles, this could come pretty  close to the OS limit.",,guozhang,hsaputra,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/13 01:24;swapnilghike;kafka-1017.patch;https://issues.apache.org/jira/secure/attachment/12598885/kafka-1017.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,344462,,,Thu Aug 22 17:04:28 UTC 2013,,,,,,,,,,"0|i1nehr:",344762,,,,,,,,,,,,,,,,,,,,"20/Aug/13 01:24;swapnilghike;This patch fixes two issues:

1. Reduce the number of open file handles on the producer: I think we can clear the topic-partition send-cache on the EventHandler once in every refresh metadata interval. This will make the producer produce to the same partition for say, 15 mins, for a given topic. That should be ok. The topic-partition send-cache will be refreshed when the metadata is refreshed.

2. There is a race condition in Producer.send() and Producer.close(). This can lead to reopening of a closed ProducerPool and thereby socket leaks.;;;","21/Aug/13 16:34;nehanarkhede;Thanks for fixing this issue, Swapnil. The patch looks good to me. +1;;;","22/Aug/13 16:48;junrao;Good patch. +1. Committed to 0.8.;;;","22/Aug/13 17:04;guozhang;Good catch, thanks Swapnil!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decompression and re-compression on MirrorMaker could result in messages being dropped in the pipeline,KAFKA-1011,12663922,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,15/Aug/13 21:10,24/May/16 20:57,14/Jul/23 05:39,24/May/16 20:57,,,,,,,,,,,,,,0,,,,"The way MirrorMaker works today is that its consumers could use deep iterator to decompress messages received from the source brokers and its producers could re-compress the messages while sending them to the target brokers. Since MirrorMakers use a centralized data channel for its consumers to pipe messages to its producers, and since producers would compress messages with the same topic within a batch as a single produce request, this could result in messages accepted at the front end of the pipeline being dropped at the target brokers of the MirrorMaker due to MesageSizeTooLargeException if it happens that one batch of messages contain too many messages of the same topic in MirrorMaker's producer. If we can use shallow iterator at the MirrorMaker's consumer side to directly pipe compressed messages this issue can be fixed. 

Also as Swapnil pointed out, currently if the MirrorMaker lags and there are large messages in the MirrorMaker queue (large after decompression), it can run into an OutOfMemoryException. Shallow iteration will be very helpful in avoiding this exception.

The proposed solution of this issue is also related to KAFKA-527.",,chienle,guozhang,gwenshap,junrao,me.venkatr,nehanarkhede,vanyatka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/13 17:51;guozhang;KAFKA-1011.v1.patch;https://issues.apache.org/jira/secure/attachment/12599666/KAFKA-1011.v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,343923,,,Tue May 24 20:57:13 UTC 2016,,,,,,,,,,"0|i1nb6n:",344225,,,,,,,,,,,,,,,,,,,,"22/Aug/13 16:23;guozhang;Proposed Approach:

1. Since the compression function ByteBufferMessageSet.create will only be called over a set of messages either with the same key hash value or with key null, we can write the key to the compressed wrapper message according to their destination partition id (currently it is always written as null).

2. Add a isShallow parameter to consumerIterator and KafkaStream, and passing the parameter from KafkaStream to consumerIterator; in consumerIterator, if isShallow is true call currentDataChunk.messages.shallowIterator otherwise call currentDataChunk.messages.iterator

3. In MirrorMaker, set shallowIterator to true.

4. Also in MirrorMaker, set CompressionCodec to NoCompression to avoid second compression of compressed message.

5. Ordering in MirrorMaker will be automatically preserved since MirrorMaker producer's event handler would use the message key to decide the outgoing partition, hence compressed messages with the same key would go to the same partition.;;;","23/Aug/13 17:51;guozhang;Passed Unit Test and System Test MirrorMaker 5001.

One drawback is that for if the number of partitions for certain topic in the source cluster is less than the number of partitions in the target cluster, then some partitions would not get any data.;;;","03/Sep/13 15:36;junrao;Thanks for the patch. Not sure if it works correctly though. The issue is that in MirrorMaker, if we pass in the compressed bytes to Producer, we need to let the producer mark the message as compressed (in the attribute in the message header). We can only do that by enabling compression in the producer. However, we can't do that since that will compress the compressed bytes again.

So, we will have to either change the Producer api to give us enough hook to package the message correctly. Alternatively, we could send data using SyncProducer by packaging the message in exactly the way that we want. However, some of the logic in Producer will have to be duplicated.;;;","05/Sep/13 01:09;guozhang;Thank Jun, you are right. The message metadata is thrown away when the producer of the MirrorMaker sends it to the target partition, and the new metadata will be added at the producer side. And after thinking about this I believe the viable solution would be a producer API change, sending MessageAndMetadata instead of KeyedMessage so that the metadata will not be dropped if there is any.

That said, I agree that this would not be an 0.8 fix but rather goes to trunk. And it may also need to align with the client-redesign stuff:

https://cwiki.apache.org/confluence/display/KAFKA/Client+Rewrite

;;;","15/Sep/13 17:09;nehanarkhede;I think using the SyncProducer in the MirrorMaker might solve this issue until the client redesign is complete. The only extra work that is involved in making this work is batching. This will resolve the unintuitive behavior that we see today where a message is accepted at the source but gets dropped at one or more of the intermediate pipelines. I think we can attempt this is in trunk instead of waiting for the client rewrite since the work is not significantly large and the client rewrite release is many months away.;;;","16/Sep/13 17:31;guozhang;Besides batching, two other critical functions that was missing from SyncProducer are partition-leader-discovery and retry-on-failure. I think without those the MirrorMaker SyncProducer will not work.;;;","16/Sep/13 21:45;guozhang;Currently the main issue is the MessageAndMetadata returned by ConsumerIterator does not maintain the compression information, and hence we would not know if the message contained is compressed or not. So in order for this to work we have to add the compression info into MessageAndMetadata also. With the SyncProducer approach the changes on MM would be:

1) Add compression info into MessageAndMetadata;
2) Use SyncProducer instead of Producer in MM;
3) Add the batching and retry mechanism around SyncProducer;
4) Use a specific SyncProducer to send MetadataRequest, and maintain the cached metadata structure;
5) Reconstruct the Message object from the MessageAndMetadata object, and then construct the MessageSetByteBuffer object, and then the ProduceRequest, and call the SyncProducer;

3), 4), 5) would be implemented as a different class which will mimic the Producer's behavior but would use a different interface that takes MessageAndMetadata instead of KeyedMessage.;;;","24/May/16 20:57;gwenshap;MirrorMaker was rewritten, compression was re-written, lets just assume it was fixed :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Concurrency issue in getCluster() causes rebalance failure and dead consumer,KAFKA-1010,12663814,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,smeder,smeder,smeder,15/Aug/13 08:39,16/Aug/13 17:09,14/Jul/23 05:39,16/Aug/13 17:09,0.8.0,,,0.8.0,,,,,,,consumer,,,0,,,,"We're seeing the following stack trace on the consumer when brokers are (forcefully) removed from the cluster:

Thu Aug 15 05:10:06 GMT 2013 Exception in thread ""main"" org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids/4
at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47)
at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685)
at org.I0Itec.zkclient.ZkClient.readData(ZkClient.java:766)
at org.I0Itec.zkclient.ZkClient.readData(ZkClient.java:761)
at kafka.utils.ZkUtils$.readData(ZkUtils.scala:407)
at kafka.utils.ZkUtils$$anonfun$getCluster$1.apply(ZkUtils.scala:453)
at kafka.utils.ZkUtils$$anonfun$getCluster$1.apply(ZkUtils.scala:452)
at scala.collection.Iterator$class.foreach(Iterator.scala:631)
at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:549)
at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
at scala.collection.JavaConversions$JListWrapper.foreach(JavaConversions.scala:596)
at kafka.utils.ZkUtils$.getCluster(ZkUtils.scala:452)
at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener$$anonfun$syncedRebalance$1.apply$mcVI$sp(ZookeeperConsumerConnector.scala:394)
at scala.collection.immutable.Range$ByOne$class.foreach$mVc$sp(Range.scala:282)
at scala.collection.immutable.Range$$anon$2.foreach$mVc$sp(Range.scala:265)
at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.syncedRebalance(ZookeeperConsumerConnector.scala:391)
at kafka.consumer.ZookeeperConsumerConnector.kafka$consumer$ZookeeperConsumerConnector$$reinitializeConsumer(ZookeeperConsumerConnector.scala:722)
at kafka.consumer.ZookeeperConsumerConnector.consume(ZookeeperConsumerConnector.scala:206)
at kafka.javaapi.consumer.ZookeeperConsumerConnector.createMessageStreams(ZookeeperConsumerConnector.scala:77)
at kafka.javaapi.consumer.ZookeeperConsumerConnector.createMessageStreams(ZookeeperConsumerConnector.scala:89)

I'm pretty sure this is due to the following logic in getCluster():

    val nodes = getChildrenParentMayNotExist(zkClient, BrokerIdsPath)
    for (node <- nodes) {
      val brokerZKString = readData(zkClient, BrokerIdsPath + ""/"" + node)._1
      cluster.add(Broker.createBroker(node.toInt, brokerZKString))
    }

which is obviously not safe since the nodes retrieved in the first call may have disappeared by the time we iterate to get the values.

getCluster() seems to only be used in ZookeeperConsumerConnector.syncedRebalance and in ImportZkOffsets.updateZkOffsets (which doesn't actually look like it is using the values), so the simplest solution may be to just move the getCluster() call into the try block in syncedRebalance and kill the usage in the other call.",,guozhang,junrao,smeder,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/13 16:30;smeder;get_cluster_0_8_git.patch;https://issues.apache.org/jira/secure/attachment/12598488/get_cluster_0_8_git.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,343815,,,Fri Aug 16 17:09:43 UTC 2013,,,,,,,,,,"0|i1nain:",344117,,,,,,,,,,,,,,,,,,,,"15/Aug/13 09:08;smeder;Simply move getCluster() call intro retry loop and eliminate second call. Also would be happy to provide a retry inside of getCluster based patch.;;;","15/Aug/13 16:28;guozhang;This looks good to me. If the broker ephemeral node disappears after the first Zk Call, another rebalance should be triggered so it is safe to fail-fast this trial.

+1;;;","16/Aug/13 16:15;junrao;Thanks for the patch. It doesn't seem to apply to 0.8 though. Could you rebase?

 git apply ~/Downloads/get_cluster_0_8.patch 
error: patch failed: core/src/main/scala/kafka/tools/ImportZkOffsets.scala:96
error: core/src/main/scala/kafka/tools/ImportZkOffsets.scala: patch does not apply
;;;","16/Aug/13 16:24;smeder;weird, patch -p1 -i get_cluster_0_8.patch worked, but git apply didn't. Let me see if I can get something that git apply likes.;;;","16/Aug/13 16:31;smeder;Git formatted patch is now attached.;;;","16/Aug/13 17:09;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DumpLogSegments tool should return error on non-existing files,KAFKA-1009,12663687,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,guozhang,junrao,junrao,14/Aug/13 16:41,19/Aug/13 16:46,14/Jul/23 05:39,19/Aug/13 16:46,0.8.0,,,0.8.1,,,,,,,log,,,0,,,,"If we run the tool on an non-existing file, we get the following

bin/kafka-run-class.sh kafka.tools.DumpLogSegments --files 00.log
Dumping 00.log
Starting offset: 0

The tool should return an error message instead.",,guozhang,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/13 00:29;guozhang;KAFKA-1009.v1.patch;https://issues.apache.org/jira/secure/attachment/12598114/KAFKA-1009.v1.patch","19/Aug/13 16:04;guozhang;KAFKA-1009.v2.patch;https://issues.apache.org/jira/secure/attachment/12598777/KAFKA-1009.v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,343688,,,Mon Aug 19 16:46:13 UTC 2013,,,,,,,,,,"0|i1n9qv:",343992,,,,,,,,,,,,,,,,,,,,"15/Aug/13 00:11;guozhang;This is due to the fact that in dumpLog, it create a FileMessageSet(file) for the filename, which use ""rw"" in creating the RandomAccessFile. When under ""rw"" mode if the file does not exist it will create one.

Proposed fix: instead of call FileMessageSet(file), call a new function FileMessageSet(file, mutable), and set mutable to false to specify it is read-only.

;;;","18/Aug/13 15:56;nehanarkhede;Thanks for the patch, Guozhang. Two comments -

1. We should default mutable=true in FileMessageSet
2. I think we should take this patch on trunk. Could you provide another patch for trunk since this one fails?;;;","19/Aug/13 16:04;guozhang;Thanks for the comments Neha.

1. We do set default mutable to true, what I said is that we add another function that do accept the mutable parameter and pass it instead of always using the default.

2. Rebased on trunk for v2.;;;","19/Aug/13 16:44;nehanarkhede;1. I see what you are saying. 

The patch looks good. Thanks for fixing the bug. +1;;;","19/Aug/13 16:46;nehanarkhede;Committed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unmap before resizing,KAFKA-1008,12663349,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,lizziew,lizziew,13/Aug/13 04:28,15/Oct/13 22:58,14/Jul/23 05:39,15/Oct/13 22:58,0.8.0,,,0.8.0,,,,,,,core,log,,2,patch,,,"While I was studying how MappedByteBuffer works, I saw a sharing runtime exception on Windows. I applied what I learned to generate a patch which uses an internal open JDK API to solve this problem.

Following Jay's advice, I made a helper method called tryUnmap(). 
","Windows, Linux, Mac OS",davidlao,dennyglee,guozhang,jkreps,junrao,lizziew,nehanarkhede,sriramsub,tnachen,trjianjianjiao,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,KAFKA-1065,,,,,,,,,,,,,,,,,,,"29/Aug/13 00:23;jkreps;KAFKA-0.8-1008-v7.patch;https://issues.apache.org/jira/secure/attachment/12600516/KAFKA-0.8-1008-v7.patch","20/Sep/13 22:05;jkreps;KAFKA-0.8-1008-v8.patch;https://issues.apache.org/jira/secure/attachment/12604322/KAFKA-0.8-1008-v8.patch","21/Aug/13 20:21;jkreps;KAFKA-1008-v6.patch;https://issues.apache.org/jira/secure/attachment/12599263/KAFKA-1008-v6.patch","10/Oct/13 00:02;jkreps;KAFKA-1008-v9-trunk.patch;https://issues.apache.org/jira/secure/attachment/12607697/KAFKA-1008-v9-trunk.patch","23/Aug/13 08:13;lizziew;KAFKA-trunk-1008-v7.patch;https://issues.apache.org/jira/secure/attachment/12599592/KAFKA-trunk-1008-v7.patch","17/Aug/13 05:38;lizziew;unmap-v5.patch;https://issues.apache.org/jira/secure/attachment/12598572/unmap-v5.patch",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,343350,,,Tue Oct 15 22:58:46 UTC 2013,,,,,,,,,,"0|i1n7nr:",343654,,,,,,,,,,,,,,,,,,,,"13/Aug/13 04:57;lizziew;Patch file;;;","13/Aug/13 05:18;guozhang;Will this patch restrict to Sun JVM-only environments?;;;","13/Aug/13 05:26;lizziew;For non Sun JVM environments, tryUnmap is no-op for now; we can add support for other JVM environments later on.;;;","13/Aug/13 15:36;jkreps;Hey Elizabeth, thanks for the patch!

Three follow-up items for us to take this:
1. If we refer to sun.nio.ch.DirectBuffer that introduces a build-time dependency on Sun java. I think that is probably okay. But what is the behavior of tryUnmap on a non-sun jvm at runtime (or if sun ever changes their implementation)? My suspicion is that we would get a ClassNotFoundException for sun.nio.ch.DirectBuffer right? I think we would be better off wrapping everything inside tryUnmap inside a try/catch and trace logging if there is an exception.
2. It looks like you added back in a call to flush which we removed as part of another patch. Probably accidental, right?
3. I would like to understand the security issue on the Sun ticket for unmap here: http://bugs.sun.com/view_bug.do?bug_id=4724038 If we don't understand what the concern is then there is a possibility we are introducing a security problem (though I don't think so...).;;;","14/Aug/13 01:02;lizziew;Thanks for the feedback!

1 - Currently tryUnmap does a type checking. If the super class of ""m"" is changed in the future, the type check will be false, and there will be no casting errors like ClassNotFoundException at runtime. 

2 - I removed the flush(). I probably used an older copy a couple of weeks ago.

3 - Reading through the bug report, I'm not sure if the cases matter in terms of Kafka - I think all the threads in a Kafka process should be trusted and the race condition between the unmap/remap shouldn't happen if coded properly. I noticed that in the most recent version, the resize method is synchronized, which should prevent multiple threads trying to resize/unmap the files. ;;;","14/Aug/13 04:02;jkreps;Hey Elizabeth, thanks for the patch.

Two issues.

The first is that I think that instanceof check will actually throw an exception on a non-sun jvm. See the experiment below and see what you think.

The second issue is actually a more subtle thing. Currently the synchronization is really just for changes to the buffer, the read-access are lock-free (which is good). My concern is what happens if we force clean a buffer while a read is occurring? Not sure if this can happen or not, but I think we need to somehow be sure it can't.

Here was my test:
$ cat /tmp/code/Test.java 
import test.MyTest;

public class Test {
    public static void main(String[] args) {
	Object o = new Object();
	    if(o instanceof MyTest)
		System.out.println(""Hello"");
    }
}

$ cat /tmp/code/test/MyTest.java 
package test;

public class MyTest {
  
}

I do 
javac /tmp/code/test/MyTest.java
javac -cp /tmp/code /tmp/code/Test.java
rm /tmp/code/test/MyTest.class
$ java -cp /tmp/code Test
Exception in thread ""main"" java.lang.NoClassDefFoundError: test/MyTest
	at Test.main(Test.java:6)
Caused by: java.lang.ClassNotFoundException: test.MyTest
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	... 1 more

We also can't really be too sure what kind of exceptions clean() might throw.

;;;","14/Aug/13 17:38;lizziew;Thanks, I added the try/catch to handle the exceptions.

I found the following coding pattern 
val idx = mmap.duplicate
in this file. It seems like it always makes a copy of the buffer for ""read"". 

;;;","15/Aug/13 02:38;jkreps;But my understanding is that these copies are just copies of pointer object (i.e. a position, limit, mark, etc) so if you unmap the underlying mmap while reads are occurring something bad will happen.

Another way to say this is what happens in the following code execution:
MappedByteBuffer orig = // map file
MappedByteBuffer copy = orig.duplicate()
(orig.asInstanceOf[sun.nio.ch.DirectBuffer]).cleaner().clean()
copy.get()

My suspicion is that something terrible will happen, but I could be wrong.;;;","15/Aug/13 06:19;lizziew;That's right, the duplicate is a shallow copy of the buffer. Do you have a case where the buffer is being used while resizing? I did a quick grep, and it looks like resize is only being used during logfile loading or truncation.;;;","15/Aug/13 15:00;jkreps;Yeah, the problem is reads continue while the log is being rolled.

Here are a bunch of possible solutions I can think of
1. Lock reads
2. Delay the truncate until the larger mmap is collected

For (2) the problem is that it actually interferes with the recoverability of the log. If we have old log segments with a bunch of unfilled bytes in their index segment we need to recover those segments. But currently we only recover from the last flush point.

So I think we need to lock read access. This isn't the end of the world but I would prefer to do this only on windows. My suggestion on implementation would be
1. Change from synchronized in OffsetIndex to Lock
2. Add an object called Os in kafka.utils that is something like

object Os {
  private val osName = System.getProperty(""os.name"").toLowerCase
  val isWindows = osName.startsWith(""windows"")
}

We can expand this later if we need more specific OS detection capabilities or other OS-specific functionality.

Then elsewhere we can just do
  if(Os.isWindows) lock.lock()
and the corresponding unlock.

Does that seem like it would work?


;;;","15/Aug/13 15:01;jkreps;Here is a list of os.name values (at least according to this random page):
http://www.javaneverdie.com/java/java-os-name-property-values/;;;","16/Aug/13 05:26;lizziew;Sounds like a good idea to have lock reads. Do you want to file a separate JIRA to address the lock read issue? ;;;","16/Aug/13 17:02;jkreps;I think it makes sense to do it as part of this patch since unmapping is the reason we need it and without it we will core dump the process under concurrency (so I don't think we can take the resizing change without the locking change). Sound reasonable?

If we have the Os.isWindows check I think we can make BOTH the locking and the forced mmap clenaing all be inside the isWindows check. Thisshould fix things on windows and not functionally change perf at all on linux (which is good).;;;","17/Aug/13 05:39;lizziew;Thanks Jay!
Please review the patch to see if it reflects your suggestion. I'm still learning Scala, so please provide any feedback!;;;","21/Aug/13 18:53;tnachen;I wonder if this patch can go in soon? It's a major blocker for anyone that wants to use Kafka on windows;;;","21/Aug/13 20:21;jkreps;Hey Elizabeth, this basically looks good. A couple of minor things.

One is I'm not sure we are covering everything that needs to be locked. The next is that we are using synchronized with the lock instance which doesn't actually call lock (as in java that just acquires the monitor associated with the lock argument--sucky right?).

I took a stab at reworking it. To make things not get too crazy I added a helper method inLock which makes the try/finally locking pattern a little more readable (hopefully).

Would you be willing to take a detailed look at this and let me know if you think this works.

The next thing we need to do is actually get this tested on Windows. I'm not sure if there is anyone who has access to a Windows machine and could reproduce the old problem who could verify that this patch fixes it?;;;","21/Aug/13 21:28;lizziew;The code change looks good. Using a higher order function makes the code look a lot cleaner! 

;;;","23/Aug/13 01:57;davidlao;Hi Jay,
The patch does not seem to apply cleanly on the 0.8 branch (see below). Can you look into generating a new patch for 0.8? 

git apply --check KAFKA-1008-v6.patch
error: patch failed: core/src/main/scala/kafka/log/OffsetIndex.scala:52
error: core/src/main/scala/kafka/log/OffsetIndex.scala: patch does not apply
error: patch failed: core/src/main/scala/kafka/utils/Utils.scala:21
error: core/src/main/scala/kafka/utils/Utils.scala: patch does not apply
error: patch failed: core/src/test/scala/unit/kafka/utils/UtilsTest.scala:18
error: core/src/test/scala/unit/kafka/utils/UtilsTest.scala: patch does not apply
;;;","23/Aug/13 08:17;lizziew;I generated a patch against the trunk to fix some of the issues in Jay's patch. Please check if it works! ;;;","26/Aug/13 21:16;davidlao;Hi Jay,
The master branch seems to be broken on Windows. Can you look into this? or produce a patch for 0.8? 

[2013-08-26 14:09:41,761] FATAL [Replica Manager on Broker 3]: Error writing to highwatermark file:  (kafka.server.ReplicaManager)
java.io.IOException: File rename from c:\Apps\logs\broker-3\replication-offset-checkpoint.tmp to c:\Apps\logs\broker-3\replication-offset-checkpoint failed.
        at kafka.server.OffsetCheckpoint.liftedTree1$1(OffsetCheckpoint.scala:61)
        at kafka.server.OffsetCheckpoint.write(OffsetCheckpoint.scala:39)
        at kafka.server.ReplicaManager$$anonfun$checkpointHighWatermarks$2.apply(ReplicaManager.scala:312)
        at kafka.server.ReplicaManager$$anonfun$checkpointHighWatermarks$2.apply(ReplicaManager.scala:309)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:119)
        at kafka.server.ReplicaManager.checkpointHighWatermarks(ReplicaManager.scala:309)
        at kafka.server.ReplicaManager$$anonfun$startHighWaterMarksCheckPointThread$1.apply$mcV$sp(ReplicaManager.scala:92)
        at kafka.utils.KafkaScheduler$$anon$1.run(KafkaScheduler.scala:100)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
;;;","29/Aug/13 00:23;jkreps;Attached a version rebased to 0.8.;;;","29/Aug/13 05:16;sriramsub;Looks good.

1. OffsetIndex.scala

1.1 Why do you need to re-calculate this.maxEntries = this.mmap.limit / 8 after remapping in resize instead of leaving it how it was previously (recalculated during the method call of maxEntries)?
1.2 maybeLock should be something that exist outside OffsetIndex. Seems like OS specific methods should reside together.
1.3 readLastOffset now locks. This is a behavior change on linux. This seems to only do a read so does it need to be ""maybeLock""?

2. Should we use the autolock for the cases below

TestUtils.scala

method - waitUntilLeaderIsElectedOrChanged

HighWaterCheckpoint.scala

method - write and read  

3. Is autolock a better name than inlock?;;;","30/Aug/13 01:30;davidlao;Thanks Jay. The 0.8 patch seems to be working on Windows. Please check it in.;;;","12/Sep/13 16:20;junrao;Thanks for the patch. Reviewed patch v7 for 0.8. Looks good overall. Just a couple of minor comments.

70. OffsetIndex: Should forceUnmap() be private?

71. Just a question. Does anyone know if the OS property shows up as windows on cygwin?

Sriram,

For 1.1, the purpose is probably to save the division calculation, which is a bit expensive.;;;","19/Sep/13 23:22;nehanarkhede;ping [~lizziew], [~jkreps]. Could you address Jun's review comments and see if we can resolve this JIRA? This is marked for the 0.8 final release;;;","20/Sep/13 22:17;jkreps;Sriram:
1.1 This is because this.mmap can be null so you have to either acquire the lock. To avoid this I just make the max size a separate variable.
1.2 It's actually only really useful in this class because the fact that we want to lock only on windows is very specific to the logic of this class.
1.3 Yeah this is technically not necessary since this method is only used during initialization but I try never to have the correctness of methods depend on when they are used.

2. Added a lock for the leader election test cases. I am skipping the other usage because that file was heavily refactored in trunk and that lock removed (I think).

3. I intend 
      inLock\(x\) {
        foo
      }
to be read as ""in lock x do foo"". So I like it since it is declarative.;;;","20/Sep/13 22:18;jkreps;Jun, added private on that method. Not sure about cygwin.;;;","20/Sep/13 22:24;sriramsub;+1;;;","23/Sep/13 01:55;lizziew;Sorry - I am not as active on this at the moment. I am back at Exeter to start my junior year. I just did a quick test on my friend's computer:
System.out.println(System.getProperty(""os.name""));   
prints out ""Windows 7"" for both Windows cmd shell and cygwin/bash. 

This makes sense since cygwin is mainly a shell. ;;;","23/Sep/13 15:18;junrao;Thanks for patch v8. Just one more comment.

80. OffsetIndex: The patch synchronizes in readLastOffset(), is that necessary?;;;","23/Sep/13 16:18;jkreps;Sriram had the same comment. It is possible to reason that the existing ways the method is used don't need synchronization but I don't think the method is thread safe since it depends on both size and mmap both of which can change (so e.g. mmap could be null and a truncate call could theoretically interleave with this call). I don't think it is very safe to have methods whose correctness depends on the existing call pattern. The synchronization doesn't hurt, in any case since this is not in any critical read or write path.;;;","23/Sep/13 16:32;junrao;Sorry, I missed that comment. +1 on v8.;;;","10/Oct/13 00:02;jkreps;Attached KAFKA-1008-v9-trunk.patch which ports the final 0.8 patch to trunk and also adds a fix for the windows compatibility issue in KAFKA-1036 in OffsetCheckpoint.scala.;;;","10/Oct/13 04:12;junrao;Thanks for the patch for trunk. +1.;;;","15/Oct/13 22:58;jkreps;Checked in on 0.8 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document new tools before 0.8 release,KAFKA-1007,12662874,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,sriramsub,sriramsub,09/Aug/13 15:35,29/Aug/13 17:00,14/Jul/23 05:39,29/Aug/13 17:00,,,,,,,,,,,,,,0,,,,"We need to document the following tools before 0.8 release
1. Add partition tool
2. ReassignPartition tool",,guozhang,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,342876,,,Thu Aug 29 17:00:10 UTC 2013,,,,,,,,,,"0|i1n4qf:",343180,,,,,,,,,,,,,,,,,,,,"17/Aug/13 20:03;guozhang;Have added the AddPartitionCommand Tool section. Waiting KAFKA-990 for ReassignPartition Tool.;;;","28/Aug/13 23:56;guozhang;Doc for ReassignPartitionsCommand is also added.;;;","28/Aug/13 23:56;guozhang;https://cwiki.apache.org/confluence/display/KAFKA/Administrative+tools;;;","29/Aug/13 17:00;nehanarkhede;Thanks for updating the documentation diligently Guozhang!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka.perf.ConsumerPerformance not shutting down consumer,KAFKA-1005,12662770,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,pmackles,pmackles,pmackles,09/Aug/13 02:34,05/Apr/15 15:45,14/Jul/23 05:39,05/Apr/15 02:38,,,,,,,,,,,tools,,,0,,,,"I have been using the consumer-perf and producer-perf scripts to try out different failure scenarios with 0.8. In one such test I had consumer-perf reading from a topic that was no longer being written to. While consumer-perf finished normally, I noticed that ConsumerOffsetChecker reported lags >0 for several partitions on my topic. I believe this is due to kafka.perf.ConsumerPerformance not calling shutdown on the consumer after all of the threads have completed. After adding the shutdown call, I was able to verify that lag=0 for all partitions on my test topic after consumer-perf finished normally.

This is pretty minor but since I am guessing the perf tools are used pretty heavily by newbies like myself, might as well make them right.

Patch attached.",,jkreps,pmackles,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/13 02:36;pmackles;ConsumerPerformance.scala.patch;https://issues.apache.org/jira/secure/attachment/12597015/ConsumerPerformance.scala.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,342772,,,Sun Apr 05 02:38:36 UTC 2015,,,,,,,,,,"0|i1n43b:",343076,,,,,,,,,,,,,,,,,,,,"05/Apr/15 02:38;jkreps;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle topic event for trivial whitelist topic filters,KAFKA-1004,12662714,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,guozhang,guozhang,08/Aug/13 21:43,26/Nov/13 03:54,14/Jul/23 05:39,26/Nov/13 03:54,,,,0.7,0.8.1,,,,,,,,,0,,,,"Toay consumer's TopicEventWatcher is not subscribed with trivial whitelist topic names. Hence if the topic is not registered on ZK when the consumer is started, it will not trigger the rebalance of consumers later when it is created and hence not be consumed even if it is in the whilelist. A proposed fix would be always subscribe TopicEventWatcher for all whitelist consumers.",,guozhang,jjkoshy,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/13 01:26;guozhang;KAFKA-1004.v1.patch;https://issues.apache.org/jira/secure/attachment/12598127/KAFKA-1004.v1.patch","16/Aug/13 16:35;guozhang;KAFKA-1004.v2.patch;https://issues.apache.org/jira/secure/attachment/12598489/KAFKA-1004.v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,342716,,,Tue Nov 26 03:54:52 UTC 2013,,,,,,,,,,"0|i1n3qv:",343020,,,,,,,,,,,,,,,,,,,,"16/Aug/13 16:22;junrao;Thanks for the patch. That looks good. However, shouldn't we just remove requiresTopicEventWatcher from TopicFilter completely?;;;","16/Aug/13 16:35;guozhang;Good point. v2 applied.

Also removed part of the topic watcher registration comment, since I think for now we would register the consumer no matter if its allowed topics becomes available.;;;","16/Aug/13 16:58;junrao;Looks good to me. Joel, do you want to take another look?;;;","26/Nov/13 03:54;jjkoshy;This was fixed in KAFKA-1103
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsumerFetcherManager should pass clientId as metricsPrefix to AbstractFetcherManager,KAFKA-1003,12662262,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,swapnilghike,swapnilghike,07/Aug/13 01:43,16/Sep/13 21:30,14/Jul/23 05:39,07/Aug/13 17:20,,,,0.8.0,,,,,,,,,,0,,,,For consistency. We use clientId in the metric names elsewhere on clients.,,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/13 01:45;swapnilghike;kafka-1003.patch;https://issues.apache.org/jira/secure/attachment/12596488/kafka-1003.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,342266,,,Mon Sep 16 21:14:29 UTC 2013,,,,,,,,,,"0|i1n0zr:",342571,,,,,,,,,,,,,,,,,,,,"07/Aug/13 17:20;junrao;Thanks for the patch. Committed to 0.8.;;;","16/Sep/13 00:26;swapnilghike;Created reviewboard ;;;","16/Sep/13 00:27;swapnilghike;Created reviewboard ;;;","16/Sep/13 00:27;swapnilghike;Created reviewboard ;;;","16/Sep/13 00:36;swapnilghike;Created reviewboard ;;;","16/Sep/13 01:06;swapnilghike;Created reviewboard ;;;","16/Sep/13 17:01;swapnilghike;Created reviewboard ;;;","16/Sep/13 17:01;swapnilghike;Created reviewboard ;;;","16/Sep/13 17:02;swapnilghike;Created reviewboard ;;;","16/Sep/13 21:13;swapnilghike;Updated reviewboard ;;;","16/Sep/13 21:14;swapnilghike;Created reviewboard ;;;","16/Sep/13 21:14;swapnilghike;Created reviewboard ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delete aliveLeaders field from LeaderAndIsrRequest,KAFKA-1002,12662030,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,swapnilghike,swapnilghike,06/Aug/13 04:51,14/Sep/14 16:43,14/Jul/23 05:39,14/Sep/14 16:43,0.8.1,,,0.8.2.0,,,,,,,,,,0,,,,"After KAFKA-999 is committed, we don't need aliveLeaders in LeaderAndIsrRequest.",,guozhang,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,342034,,,Sun Sep 14 16:43:46 UTC 2014,,,,,,,,,,"0|i1mzk7:",342339,,,,,,,,,,,,,,,,,,,,"04/Sep/14 22:19;guozhang;I think we still need aliveLeaders today, so this may not be an issue any more. [~junrao] [~swapnilghike] Could you comment on this one?;;;","14/Sep/14 16:43;nehanarkhede;[~guozhang] We have renamed it appropriately and adjusted it's usage correctly now. So we can close this JIRA.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controlled shutdown never succeeds until the broker is killed,KAFKA-999,12661600,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,swapnilghike,nehanarkhede,nehanarkhede,04/Aug/13 15:35,07/Aug/13 03:42,14/Jul/23 05:39,07/Aug/13 03:42,0.8.0,,,,,,,,,,controller,,,0,,,,"A race condition in the way leader and isr request is handled by the broker and controlled shutdown can lead to a situation where controlled shutdown can never succeed and the only way to bounce the broker is to kill it.

The root cause is that broker uses a smart to avoid fetching from a leader that is not alive according to the controller. This leads to the broker aborting a become follower request. And in cases where replication factor is 2, the leader can never be transferred to a follower since it keeps rejecting the become follower request and stays out of the ISR. This causes controlled shutdown to fail forever

One sequence of events that led to this bug is as follows -

- Broker 2 is leader and controller
- Broker 2 is bounced (uncontrolled shutdown)
- Controller fails over
- Controlled shutdown is invoked on broker 1
- Controller starts leader election for partitions that broker 2 led
- Controller sends become follower request with leader as broker 1 to broker 2. At the same time, it does not include broker 1 in alive broker list sent as part of leader and isr request
- Broker 2 rejects leaderAndIsr request since leader is not in the list of alive brokers
- Broker 1 fails to transfer leadership to broker 2 since broker 2 is not in ISR
- Controlled shutdown can never succeed on broker 1

Since controlled shutdown is a config option, if there are bugs in controlled shutdown, there is no option but to kill the broker",,nehanarkhede,sriramsub,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/13 21:35;swapnilghike;kafka-999-v1.patch;https://issues.apache.org/jira/secure/attachment/12596431/kafka-999-v1.patch","06/Aug/13 23:25;swapnilghike;kafka-999-v2.patch;https://issues.apache.org/jira/secure/attachment/12596451/kafka-999-v2.patch","07/Aug/13 00:02;swapnilghike;kafka-999-v3.patch;https://issues.apache.org/jira/secure/attachment/12596460/kafka-999-v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,341789,,,Wed Aug 07 03:42:38 UTC 2013,,,,,,,,,,"0|i1my1z:",342095,,,,,,,,,,,,,,,,,,,,"05/Aug/13 19:45;nehanarkhede;I think the fix is to remove the smart in the broker's handling of a become follower request. Even if the leader is not alive, it should depend on the controller to send it another LeaderAndIsrRequest to connect to some other leader or to become a leader itself.;;;","05/Aug/13 19:48;sriramsub;+1 on the proposed fix. Also this could happen only if the retry attempt is infinite.;;;","06/Aug/13 01:16;swapnilghike;I think the right fix would be to undo any changes made to controllerContext during shutdownBroker() if the shutdown attempt failed on the controller. If the shutdown attempt failed, the controller's state should be equivalent to the state it would be in if the shutdown attempt was never made.

In this case, it would mean removing broker 1 which is being shut down from controllerContext.shuttingDownBrokerIds set if a shutdown attempt failed. Thus, when the controller is sending out leaderAndIsr requests after broker 2 comes up, it will include broker 1 in the alive brokers.

As far as dumbing down the broker is concerned to make it accept whatever controller says, I don't have a strong feeling. As Neha described, it will also resolve this ticket. However, doing only that and not fixing KafkaController.shutdownBroker() will hide an underlying bug - the controller had sent prematurely optimized information to broker 2 (the broker 1 was not dead, but the controller thought it will soon be dead, so why include it in alive leaders anyways).

I also think it's ok for the broker to be a bit intelligent. As such, become-follower operation starts a thread that fetches from the leader. This fetching is a peer-to-peer action, so I don't see anything wrong with a broker being able to decide whether it wants to fetch or not.

What do you think?
;;;","06/Aug/13 03:02;nehanarkhede;Sriram, Ack you are right. Forgot to mention that caveat in the bug description.

Swapnil,

The problem with undoing changes to controllerContext and removing the broker if the controlled shutdown request failed, is that there is a risk of mistakenly moving leaders to that broker. The reason is because even if the controlled shutdown attempt failed, it goes ahead with an uncontrolled shutdown. In that case, there is no value to letting it become a leader for more partitions since the point of controlled shutdown is to move existing leaders off.

Split brain is always a problem that is both dangerous and causes unnecessary failures (e.g. consumer rebalancing). The purpose of a controller in Kafka is to be the only brain in the cluster and make state change decisions based on a single global view of the system. In this case, even if we keep the extra check on the broker, the leader can fail immediately after the if statement succeeds. So that check doesn't help anyways (Jun had pointed that out to me in the past, but I wasn't convinced back then :) ). Also, even if the leader fails, the broker should depend on the controller to do the right thing and send another leader and isr request with a new leader. That way any changes to the controller logic (new features like controlled shutdown) will not cause unexpected side effects such as this bug.;;;","06/Aug/13 04:35;swapnilghike;I see your points. After thinking a bit about the controlled shutdown --> uncontrolled shutdown scenario, your comments make sense.

However controller passing live brokers minus shutting down brokers as live leaders in the LeaderAndIsrRequest sounds like a premature optimization and a bug. The check on the broker to not check live leaders will allow the broker to circumvent it.

But, the good news is that after we make the fix you proposed for broker's handling of LeaderAndIsrRequest, LeaderAndIsrRequest does not need to contain aliveLeaders. :) So deleting that field automatically solves my concern. ;;;","06/Aug/13 04:43;nehanarkhede;>> However controller passing live brokers minus shutting down brokers as live leaders in the LeaderAndIsrRequest sounds like a premature optimization and a bug

Well, the field is not required only after this JIRA is fixed, but there is no bug in the field. Like I explained above, live leaders cannot include the broker being shut down, so the controller passes accurate information to the broker. The bug is in the way the broker uses that field.

Deleting the field in the future is a good idea. However, it is a change in wire protocol, so I suggest we file that JIRA and handle that part on trunk.;;;","06/Aug/13 04:55;swapnilghike;Perhaps the word live misled me. Thanks for the clarifications, filed KAFKA-1002 to delete the field.;;;","06/Aug/13 06:46;swapnilghike;Since we need leader broker's host:port to create ReplicaFetcherThread, the easiest fix for this ticket's purpose seems to be to pass all leaders through LeaderAndIsrRequest:

val leaders = liveOrShuttingDownBrokers.filter(b => leaderIds.contains(b.id))
val leaderAndIsrRequest = new LeaderAndIsrRequest(partitionStateInfos, leaders, controllerId, controllerEpoch, correlationId, clientId)

Any suggestions on avoiding a wire protocol change?;;;","06/Aug/13 21:34;swapnilghike;Small-scale fix for 0.8. ;;;","06/Aug/13 23:15;nehanarkhede;Thanks for the patch Swapnil. Overall, well thought through. Few minor comments -

1. ControllerChannelManager

leaderIds is not used anymore

2. LeaderAndIsrRequest

The field actually means all the brokers in the cluster. So can we rename it from leaders to allBrokers?
Same for Partition.scala and ReplicaManager.scala;;;","06/Aug/13 23:25;swapnilghike;Thanks for pointing that out. Actually in ControllerChannelManager, we should rather pass liveOrShuttingDownBroker.filter(b => leaderIds.contains(b.id)) as the leaders to LeaderAndIsrRequest.

Attached patch v2.;;;","06/Aug/13 23:28;swapnilghike;Renamed to leaders at one another place.;;;","07/Aug/13 03:41;nehanarkhede;+1 on v3. ;;;","07/Aug/13 03:42;nehanarkhede;Committed v3 to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
High level consumer doesn't throw an exception when the message it is trying to fetch exceeds the configured fetch size,KAFKA-994,12661130,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,smeder,smeder,smeder,01/Aug/13 13:59,03/Aug/13 16:49,14/Jul/23 05:39,02/Aug/13 04:35,0.8.0,,,0.8.0,,,,,,,consumer,,,0,,,,"The high level consumer code is supposed to throw an exception when it encounters a message that exceeds its configured max message size. The relevant code form ConsumerIterator.scala is:

      // if we just updated the current chunk and it is empty that means the fetch size is too small!
      if(currentDataChunk.messages.validBytes == 0)
        throw new MessageSizeTooLargeException(""Found a message larger than the maximum fetch size of this consumer on topic "" +
                                               ""%s partition %d at fetch offset %d. Increase the fetch size, or decrease the maximum message size the broker will allow.""
                                               .format(currentDataChunk.topicInfo.topic, currentDataChunk.topicInfo.partitionId, currentDataChunk.fetchOffset))
    }

The problem is that KAFKA-846 changed PartitionTopicInfo.enqueue:

   def enqueue(messages: ByteBufferMessageSet) {
-    val size = messages.sizeInBytes
+    val size = messages.validBytes
     if(size > 0) {

i.e. chunks that contain messages that are too big (validBytes = 0) will never even be enqueued, so won't ever hit the too-large message check in ConsumerIterator... 

I've attached a patch that passes our tests...",,jkreps,junrao,smeder,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/13 15:17;smeder;messageSize.patch;https://issues.apache.org/jira/secure/attachment/12595428/messageSize.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,341319,,,Fri Aug 02 04:35:55 UTC 2013,,,,,,,,,,"0|i1mv87:",341637,,,,,,,,,,,,,,,,,,,,"01/Aug/13 15:33;junrao;Thanks for filing this jira. Looks like a real issue. Just changing validBytes to sizeInBytes may not be enough. In enqueue(), currently we expect the chunk to have at least one message in order to get the next fetch offset. Of course, if we hit a large message, that expectation won't be true. So, we have to change the logic a bit such that if there is not a single message in the chunk, we don't move the fetch offset, but still insert the chunk to the queue (so that the consumer thread can see it).

Thanks,

Jun;;;","01/Aug/13 15:37;smeder;Yea, I realized that after the initial write-up, the attached patch actually does what you describe.;;;","01/Aug/13 15:38;jkreps;Ack, nice catch.;;;","02/Aug/13 04:35;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Offset Management API is either broken or mis-documented,KAFKA-993,12660911,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,eapache,eapache,31/Jul/13 14:43,23/Aug/13 17:36,14/Jul/23 05:39,21/Aug/13 13:27,0.8.0,0.8.1,,,,,,,,,network,,,0,,,,"I am in the process of building a set of Go client bindings for the new 0.8 protocol (https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol). Everything works but the Offset Commit/Fetch APIs. Fetch never returns any data, and trying to Commit results in the broker forcibly disconnecting my client. I have double-checked the bytes on the wire using Wireshark, and my client is obeying the protocol spec.

After some digging, I found KAFKA-852 which seems related, but I have tried my client against the 0.8 beta, 0.8 branch, and even trunk with the same results.

When I try and commit, the stack-trace that the broker produces is:
[2013-07-31 10:34:14,423] ERROR Closing socket for /192.168.12.71 because of error (kafka.network.Processor)
java.nio.BufferUnderflowException
	at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:127)
	at java.nio.ByteBuffer.get(ByteBuffer.java:675)
	at kafka.api.ApiUtils$.readShortString(ApiUtils.scala:38)
	at kafka.api.UpdateMetadataRequest$$anonfun$readFrom$1.apply(UpdateMetadataRequest.scala:42)
	at kafka.api.UpdateMetadataRequest$$anonfun$readFrom$1.apply(UpdateMetadataRequest.scala:41)
	at scala.collection.immutable.Range$ByOne$class.foreach(Range.scala:282)
	at scala.collection.immutable.Range$$anon$2.foreach(Range.scala:265)
	at kafka.api.UpdateMetadataRequest$.readFrom(UpdateMetadataRequest.scala:41)
	at kafka.api.RequestKeys$$anonfun$7.apply(RequestKeys.scala:42)
	at kafka.api.RequestKeys$$anonfun$7.apply(RequestKeys.scala:42)
	at kafka.network.RequestChannel$Request.<init>(RequestChannel.scala:49)
	at kafka.network.Processor.read(SocketServer.scala:345)
	at kafka.network.Processor.run(SocketServer.scala:245)
	at java.lang.Thread.run(Thread.java:680)

Is this a bug, or is the protocol spec wrong? Also, since I can't seem to find a straight answer anywhere else: is offset fetch/commit expected to be in 0.8, 0.8.1, or some later release?

Thanks,
Evan",,eapache,edenhill,zhangzs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-657,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,341100,,,Fri Aug 23 17:36:40 UTC 2013,,,,,,,,,,"0|i1mtvj:",341418,,,,,,,,,,,,,,,,,,,,"20/Aug/13 19:20;eapache;The library is now available at https://github.com/Shopify/sarama

In case somebody wants to add it to the wiki page of client libraries...;;;","20/Aug/13 21:39;lanzaa;I believe this is more of a documentation bug. In the push to get Kafka 0.8 out I think it was decided to postpone the release of the server side Offset Commit API.

As such, the OffsetCommit APIs are only available post 0.8.;;;","21/Aug/13 13:27;eapache;I have updated the protocol wiki to note that these APIs are not available in 0.8. Thanks for the clarification.;;;","23/Aug/13 17:36;lanzaa;Thank you for the help!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Double Check on Broker Registration to Avoid False NodeExist Exception,KAFKA-992,12660847,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,nehanarkhede,guozhang,31/Jul/13 02:39,27/Nov/14 00:11,14/Jul/23 05:39,05/Aug/13 17:06,,,,,,,,,,,,,,0,,,,"The current behavior of zookeeper for ephemeral nodes is that session expiration and ephemeral node deletion is not an atomic operation. 

The side-effect of the above zookeeper behavior in Kafka, for certain corner cases, is that ephemeral nodes can be lost even if the session is not expired. The sequence of events that can lead to lossy ephemeral nodes is as follows -

1. The session expires on the client, it assumes the ephemeral nodes are deleted, so it establishes a new session with zookeeper and tries to re-create the ephemeral nodes. 
2. However, when it tries to re-create the ephemeral node,zookeeper throws back a NodeExists error code. Now this is legitimate during a session disconnect event (since zkclient automatically retries the
operation and raises a NodeExists error). Also by design, Kafka server doesn't have multiple zookeeper clients create the same ephemeral node, so Kafka server assumes the NodeExists is normal. 
3. However, after a few seconds zookeeper deletes that ephemeral node. So from the client's perspective, even though the client has a new valid session, its ephemeral node is gone.

This behavior is triggered due to very long fsync operations on the zookeeper leader. When the leader wakes up from such a long fsync operation, it has several sessions to expire. And the time between the session expiration and the ephemeral node deletion is magnified. Between these 2 operations, a zookeeper client can issue a ephemeral node creation operation, that could've appeared to have succeeded, but the leader later deletes the ephemeral node leading to permanent ephemeral node loss from the client's perspective. 

Thread from zookeeper mailing list: http://zookeeper.markmail.org/search/?q=Zookeeper+3.3.4#query:Zookeeper%203.3.4%20date%3A201307%20+page:1+mid:zma242a2qgp6gxvx+state:results",,guozhang,jjkoshy,jkreps,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/13 03:42;guozhang;KAFKA-992.v1.patch;https://issues.apache.org/jira/secure/attachment/12595336/KAFKA-992.v1.patch","13/Aug/13 18:44;guozhang;KAFKA-992.v10.patch;https://issues.apache.org/jira/secure/attachment/12597785/KAFKA-992.v10.patch","14/Aug/13 18:24;guozhang;KAFKA-992.v11.patch;https://issues.apache.org/jira/secure/attachment/12598021/KAFKA-992.v11.patch","15/Aug/13 22:30;guozhang;KAFKA-992.v12.patch;https://issues.apache.org/jira/secure/attachment/12598311/KAFKA-992.v12.patch","16/Aug/13 16:25;guozhang;KAFKA-992.v13.patch;https://issues.apache.org/jira/secure/attachment/12598487/KAFKA-992.v13.patch","16/Aug/13 23:28;guozhang;KAFKA-992.v14.patch;https://issues.apache.org/jira/secure/attachment/12598549/KAFKA-992.v14.patch","01/Aug/13 17:40;guozhang;KAFKA-992.v2.patch;https://issues.apache.org/jira/secure/attachment/12595452/KAFKA-992.v2.patch","02/Aug/13 23:25;guozhang;KAFKA-992.v3.patch;https://issues.apache.org/jira/secure/attachment/12595697/KAFKA-992.v3.patch","05/Aug/13 16:08;guozhang;KAFKA-992.v4.patch;https://issues.apache.org/jira/secure/attachment/12596136/KAFKA-992.v4.patch","06/Aug/13 23:41;guozhang;KAFKA-992.v5.patch;https://issues.apache.org/jira/secure/attachment/12596455/KAFKA-992.v5.patch","08/Aug/13 17:56;guozhang;KAFKA-992.v6.patch;https://issues.apache.org/jira/secure/attachment/12596897/KAFKA-992.v6.patch","09/Aug/13 00:17;guozhang;KAFKA-992.v7.patch;https://issues.apache.org/jira/secure/attachment/12596993/KAFKA-992.v7.patch","09/Aug/13 22:08;guozhang;KAFKA-992.v8.patch;https://issues.apache.org/jira/secure/attachment/12597186/KAFKA-992.v8.patch","12/Aug/13 21:43;guozhang;KAFKA-992.v9.patch;https://issues.apache.org/jira/secure/attachment/12597569/KAFKA-992.v9.patch",,,,,,,,,,,,,,14.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,341036,,,Thu Nov 27 00:11:07 UTC 2014,,,,,,,,,,"0|i1mthb:",341354,,,,,,,,,,,,,,,,,,,,"31/Jul/13 02:42;guozhang;We can differentiate this edge case from a temporal connection loss by adding a timestamp into the broker ZK string so that the conflict will be reflected. Then we can check if the host:port are the same. If this is the case, then we can treat this ephemeral node as written by the broker itself but from a previous session, hence backoff for it to be deleted on session timeout and retry creating the ephemeral node. This will make the temporal connection loss a false positive case, but it should be fine since this case happens rarely.
;;;","31/Jul/13 02:45;guozhang;In ZkUtils.registerBrokerInZk:

1. Add the current timestamp into the Json string.

2. Upon ZkNodeExistsException, read the broker id path:

2.1. If hit a ZkNoNodeException, then retry immediately.

2.2. Compare the read broker hostname and port, if they are the same, backoff and retry since it will be deleted eventually; otherwise throw the RuntimeException.

;;;","01/Aug/13 17:40;guozhang;Unit tests passed;;;","02/Aug/13 07:09;swapnilghike;- I am not completely clear on why timestamp is required to be stored in zookeeper along with other broker info. If I am not wrong, znode stores the session that created it in the field ephemeralOwner. There should be a way to get its value when we read broker znode info.
- Perhaps we should have fixed number of retries. If zookeeper cannot delete the znode after session expiration after sufficient amount of time, we would probably like to know that we are dealing with a buggy zookeeper setup.

Then this should suffice:

catch ZkNodeExistsException =>
for (numRetries) {
 if (broker.host == host && broker.port == port && sessionId == lastSessionId) {
   Thread.sleep(..)
 } else {
   throw new RuntimeException(...)
 }
};;;","02/Aug/13 22:49;nehanarkhede;Thanks for patch v2, Guozhang. Few review suggestions -

1. How about keeping the unix timestamp as is. All we have to make sure is that it is the equal to what was written. I'm not sure there is an advantage to converting it to some date format. 
2. Typo => ephermeral
3. The following log statement is not completely correct -

                info(""I wrote this conflicted ephermeral node a while back in a different session, ""
                  + ""hence I will backoff for this node to be deleted by Zookeeper after session timeout and retry"")

The reason is because there are 3 cases when the broker might get NodeExists and the ephemeral node will have the same host and port -
3.1 It ran into one of the recoverable zookeeper errors while creating the ephemeral nodes, in which case ZkClient retried the operation under the covers, and it got a NodeExists error on the 2nd retry. In this case, the timestamp will be useful as it will match what was written and we do not need to retry.
3.2 It hit the zookeeper non-atomic session expiration problem. In this case, the timestamp will not match and we just have to retry.
3.3 The server was killed and restarted within the session timeout. In this case, it is useful to back off for session timeout and retry ephemeral node creation. 

It will be useful from a logging perspective if we can distinguish between these 3.1 & 3.2/3 cases and retry accordingly. Another way to look at this is to not store the timestamp and just retry on any NodeExists as that has to go through at some point, but we will not get meaningful logging which is not ideal.

4. Regarding the backoff time, I think it is better to backoff for the session timeout

5. Regarding the case where the broker host and port do not match -

                throw new RuntimeException(""A broker is already registered on the path "" + brokerIdPath
                  + "". This probably indicates that you either have configured a brokerid that is already in use, or ""
                  + ""else you have shutdown this broker and restarted it faster than the zookeeper ""
                  + ""timeout so it appears to be re-registering."")

The else part of this statement is incorrect since if you shutdown and restarted the same broker, the broker host and port should in fact match. We should fix the exception message to reflect that another
 broker [host, port] is registered under that id.
;;;","02/Aug/13 22:54;nehanarkhede;Swapnil,

- You are right in observing that zookeeper stores the session id as part of the znode. However, when a session is established, we don't have access to the session id through ZkClient. So even though session id comparison is the best way to fix the bug, we can't do that.
- There are a lot of things that will go wrong if zookeeper is not able to create or expire ephemeral nodes. In such cases, Kafka server will backoff and retry registering, the controller will trigger leader elections repeatedly. So we will know this by through the LeaderElectionRate and UnderReplicatedPartitionCount metrics. ;;;","02/Aug/13 22:59;guozhang;Swapnil, we also considered this option. The problem is that zkClient does not expose such kind of information. Hence we came out with the timestamp approach.;;;","02/Aug/13 23:25;guozhang;Thanks for the comments Neha.

1,2,5: Done

3. 3.1 and 3.2/3 are distincted since for 3.1 the createEphemeralPathExpectConflict would not throw the ZkNodeExistsException.

4. It is not possible to get the session timeout from zkClient, so I use the default value (6000ms).;;;","03/Aug/13 00:57;swapnilghike;Makes sense. Just one comment, you can use the session timeout from KafkaConfig, it will give you the value that is being used at runtime.
;;;","03/Aug/13 01:07;swapnilghike;Also the while loop should be fixed, the first sleep will lead to return. ;;;","03/Aug/13 16:50;jkreps;This is good, this can go on trunk, right?;;;","04/Aug/13 15:39;nehanarkhede;Thanks for patch v3. Few more review comments -

6. We should get session timeout from KafkaConfig, instead of hardcoding it.
7. It seems like the return should actually be moved inside the try block. That is the only time we don't want to retry since the operation is successful
8. You are right about createEphemeralPathExpectConflict. It already handles 3.1 (in my comments above)

This bug is very serious that can halt correct operation of a 0.8 cluster. In a typical production deployment of Kafka where there are many consumers writing offsets to the same zookeeper cluster that the 08 cluster is connected to, there is a higher risk of hitting this bug. On the other hand, you can always increase the session timeout enough to get around this. However, in that case, if a broker crashes or has to be killed, it takes as long as session timeout for the consumers to recover. We have hit this bug in production at LinkedIn several times and have also had to kill 08 brokers due to bugs in controlled shutdown (KAFKA-999). 

I understand that we want to stop taking patches on 08. We are still on 08 beta in open source. Until trunk is ready to be released, companies that have Kafka 08-beta running in production can run into blocker bugs (KAFKA-992, KAFKA-999). What release pattern can we follow here ? Does it make sense to only take critical fixes on 0.8 and leave other changes to trunk. That allows critical bug fixes to go to production before 0.8.1 is ready for release.




;;;","05/Aug/13 15:25;junrao;It seems that it's going to take some time before this issue is fixed in ZK. So, I suggest that we patch this in 0.8.;;;","05/Aug/13 16:08;guozhang;Thanks for the comments Neha.

6,7: Done.

Going to deploy for scenario-recreation testing.;;;","05/Aug/13 16:15;swapnilghike;+1;;;","05/Aug/13 17:06;nehanarkhede;+1 on v4;;;","05/Aug/13 17:06;nehanarkhede;Committed to 0.8;;;","06/Aug/13 16:33;junrao;Thinking about this more. The same ZK issue can affect the controller and the consumer re-registration too. Should those be handled too?;;;","06/Aug/13 19:59;jjkoshy;Delayed review - looks good to me, although I still don't see a benefit in                                                                                                                                  
storing the timestamp. i.e., the approach to retry on nodeexists if the host                                                                                                                                
and port are the same would remain the same. i.e., it seems more for                                                                                                                                        
informative purposes. Let me know if I'm missing something.                                                                                                                                                 
                                                                                                                                                                                                            
@Jun, you have a point about the controller. It seems it may not be a                                                                                                                                       
problem there since controller re-election will happen only after the data                                                                                                                                  
is actually deleted. For consumers it may not be an issue either given that                                                                                                                                 
the consumer id string includes a random uuid.                                                                                                                                                              ;;;","06/Aug/13 21:18;jjkoshy;ok nm the comment about timestamp. I had forgotten that nodeexists wouldn't be thrown if the data is the same.;;;","06/Aug/13 21:44;jjkoshy;and nm for my comments about controller/consumers as well. For consumers, we
don't regenerate the consumer id string.

For controller, what can end up happening is:
- controller session expires and becomes the controller again (with the
  stale ephemeral node)
- another broker (whose session may not have expired) receives a watch when the
  stale ephemeral node is actually deleted
- so we can end up with two controllers in this scenario.

;;;","06/Aug/13 21:58;nehanarkhede;We just found a way to reliably reproduce the zookeeper bug and verify that KAFKA-992 fix works. Now, we can fix the controller and consumer the same way.;;;","06/Aug/13 22:02;guozhang;The zookeeper bug can be reproduced as follows:

1. Checkout a clean 0.8 branch, revert back the KAFKA-992 fix:

2. Build and create a server connecting to a Zookeeper instance (make sure maxClientCnxns = 0 in ZK config so that one IP address can create as many connections as wanted)

3. Load the Zookeeper with dummy sessions, each creates and maintains a thousand ephemeral nodes.

4. Write a script that pause and resume the Zookeeper process continuously, for example:

---
while true
do
        kill -STOP $1
        sleep 8
        kill -CONT $1
        sleep 60
done
---

5.  Then when the Zookeeper process resumes, it will mark all sessions as timeout, but since the ephemeral nodes to delete are too many, the server's registration node may not be deleted yet when the servers tries to re-register itself and the server think himself as registered successfully.

6. And later Zookeeper will delete the server's registration node without the server's awareness.

7. If we re-apply KAFKA-992's patch, and redo the same testing setup. Under similar conditions the server will wait and retry.

-----

Since we can now re-produce the bug and verify the fix, the same fix will be applied to Controller and Consumer.;;;","07/Aug/13 16:25;nehanarkhede;Thanks for the follow up patch. The changes to consumer look good. I have a few concerns about the changes to controller -

1. ZookeeperLeaderElector

1.1 This change is backwards incompatible. Unfortunately, when we versioned the zookeeper data, we left out the controller path. So we have to handle both the previous format and the new format in the code until the old format can be phased out. This will be hacky but we cannot accept this patch without handling this correctly, since that would require downtime at relase
1.2 We have moved to using json for zookeeper data. It will be good if we can follow that while making this change to the controller path
1.3 The while loop has a lot of return statements. How about refactoring it to have while(!writeSucceeded) {} and keep the return amILeader at the very end ?;;;","08/Aug/13 17:56;guozhang;1.1-3. Done.;;;","08/Aug/13 22:43;nehanarkhede;Thanks for the follow up patch Guozhang. Overall, looks correct. Few minor suggestions -

9. ZkUtils

9.1. Could you add more details in the log message when the json parsing of the controller path fails? Since we know we are changing the format, something along the lines of ""Json parsing of the controller path failed. Probably this controller is still using the old format [%s] of storing the broker id in the zookeeper path""
9.2 We don't need to convert the controller variable to string since it is already a string
9.3 Improve the error message when both json parsing and the toInt conversion fails. ""Failed to parse the leader leaderinfo "" doesn't say that we failed to parse the controller's
 leader election path.

10. ZookeeperLeaderElector
10.1 Remove unused import BrokerNotAvailableException
10.2 In elect() API, should'nt we use readDataMaybeNull instead of readData? That covers the case if the ephemeral node disappears before you get a chance to read it.
10.3 Since the changes to elect() are new, I suggest we convert the debug to info or warn statements. This elect() is rarely called, this will not pollute the log.
10.4 One suggestion to reduce code and make it somewhat cleaner - If we change electFinished to electionNotDone, we need to change it only in one place - where we don't need to retry. Currently we have to change electFinished multiple times at different places
;;;","09/Aug/13 00:17;guozhang;Thanks for the comment Neha.

9.1: Done. 
9.2,3 Done.

10.1,2,3: Done
10.4: Done. Great point!;;;","09/Aug/13 15:30;junrao;Thanks for the patch. It doesn't seem to apply for me. Do you need to rebase? Just one quick comment. It seems there is common code in the re-registering logic of broker, controller and consumer. Instead of duplicating the code, could we create a common util to share the code?;;;","09/Aug/13 17:04;guozhang;Thanks for the comments Jun. I think the re-registering logic is slightly different for broker, controller and consumer:

Broker: need to check hostname + port
Controller: only need to check brokerId
Consumer: need not check anything since the consumer info like hostname and port is encoded in the ZkPath.

So I think it is hard to unify consumer's logic with broker and controller's logic; it is possible to unify the broker and controller's logic though, by passing the list of json fields that we need to check. But I am not sure it worth the effort.;;;","09/Aug/13 17:08;nehanarkhede;I agree with Guozhang that the logic to ensure we get around the de-registration issue is very nuanced to the specific path and semantics of that path. 

+1 on the latest patch.;;;","09/Aug/13 22:08;guozhang;Incremental patch uploaded. Fixed a small issue that Json.parse will not throw exception but instead returns None. System test passed.;;;","09/Aug/13 22:52;nehanarkhede;+1 on v8. Good catch!;;;","12/Aug/13 16:09;junrao;Thanks for patch v8. I think the code can still be made cleaner.

80. If you look at the code in ZkUtils.registerBrokerInZk(), ZookeeperConsumerConnecter.registerConsumerInZK() and ZookeeperLeaderEelection.elect(), they all have the logic for handling the ZK bug. They only differ slightly because the way that they check whether the registration is from the same client is different. I was thinking that we can write a new util function called sth like createEphemeralPathExpectConflictHandleZKBug(). This function will take a function that checks if the value in a ZK path is from the caller. The function will then keep trying to create the path until either it detects a value is put in by a different caller or the creation succeeds. We will get several benefits if you do that: (1) there is a centralized place to handle the ZK bug and therefore we avoid code duplication; (2) this separates the logic of handling the ZK bug from the rest of the logic in the caller, which will make the latter easier to understand; (3) it makes it easier to remove the logic in the future when the ZK bug is fixed.

81. In ZookeeperLeaderEelection.elect(), we also have the logic to handle different formats of the value of the controller path. It seems that can probably be simplified a bit too. Basically, if we read the old format (in the new code), we can treat it as if someone else already did the registration.

82. There is code duplication in ZkUtils.getController() and ZookeeperLeaderElection.LeaderChangeListener.handleDataChange(). Could we share the logic in a separate util?;;;","12/Aug/13 19:00;nehanarkhede;+1 on 80. That's a great suggestion, Jun!;;;","12/Aug/13 21:43;guozhang;Thanks for the comments.

80. Added createEphemeralPathExpectConflictHandleZKBug, which takes the checker function, but also a caller info reference for comparison.

81. We need to read the leader Id in this case, since it is needed in shutting down.

82. Added a new Controller object, which takes the string and parse the int value. This function can remove its old-new version handling later.;;;","13/Aug/13 18:44;guozhang;Add the Controller.scala file to kafka.cluster;;;","14/Aug/13 16:45;junrao;v10 doesn't seem to apply to current 0.8. ;;;","14/Aug/13 18:24;guozhang;Forgot to pull before rebase, the new version apply to latest 0.8 now.;;;","15/Aug/13 16:56;nehanarkhede;Overall, v11 is a good refactor. Few minor formatting comments -
1. Broker
I think getZkString can be removed. This is a nice to have clean up item, not introduced in your patch

2. ZkUtils
2.1 Can we break the long log line that says ""A broker is already registered...""
2.2 Typo in the comments above createEphemeralPathExpectConflictHandleZKBug() => NodeExistEception

3. KafkaController
Typo => zkSessionTimout

4. Controller
Can we add back the following statement in warn. It was helpful for me to know this while testing a cluster upgrade with this patch -

            warn(""Failed to parse the controller info as json. "" +
              ""Probably this controller is still using the old format [%s] of storing the broker id in the zookeeper path"".format(controller))
;;;","15/Aug/13 17:18;junrao;Thanks for patch v11. Much better, but can still be improved. Some more comments:

110. ZkUtils.createEphemeralPathExpectConflictHandleZKBug: 
110.1 Could we list the ZK jira related to this bug? If that doesn't exist, create a new one. That way, we can track when the bug is fixed.
110.2 Typos in the comment: ata and NodeExistEception
110.3 Is it better to change caller to expectedCallerData
110.4 Could you explain what checker() does in the comment?
110.5 It would be useful to log the ZK path in addition to the value.

111. ZkUtils.registerBrokerInZk: Is it better to rename selfBroker to expectedBroker?

112. KafkaController: typo zkSessionTimout

113. ZookeeperLeaderElector.elect(): I am bit confused what resign() should do. It seems that it needs to either reset leaderId or throw an exception to the caller.

114. Controller: I think it's better to rename it to ControllerUtils. Also, remove unused imports.



;;;","15/Aug/13 22:28;guozhang;Thanks for the comments, Neha, Jun. And sorry for these typos..

Neha:

1. Done.
2. Done.
3. Done.
4. Done.

Jun:

110.1 Done.
110.2. Done.
110.3. Done.
110.4. Done.
110.5. Done.
111. Done.
112. Done.
113. As by the meaning of ""resign"", which indicates a valid leader actively resign its role as the leader, deleting its election path is the correct way of resigning. The question here is that upon receiving a non-ZkNodeExistsException should we really call resign or not. I am proposing not and instead just logging the error and setting leaderId = -1.
114. Removed unused imports. As for renaming, my expectation is that Controller object might be extend just as Broker object in Broker.scala , which will be used to create a Controller class instance when more fields are added to Controller besides just the ID. So I would propose keep its name and its location in kafka.cluster for now.;;;","16/Aug/13 15:01;guozhang;As few more thoughts about 113: currently leaderId is only read by amILeader, which is only called by the end of elect to determine if election succeeds or not. At controller shutdown, it will try to read the controller id from ZK again instead of directly use leaderId. Hence if we do not want to make this exception a fatal one and shutdown the whole broker setting leaderId to -1 and logging the error is OK. 

This conclusion is based on the assumption that if all the brokers failed on election and no one becomes the leader then it is supposed to be a fatal error and should be detected by monitoring.;;;","16/Aug/13 15:51;junrao;Thanks for patch v12. A few more comments.

114. If that's the intention, we should put the logic in object KafkaController, which already exists. Also, the comment above the class is incorrect.

120. ZookeeperLeaderElector.resign() is no longer being used and can be removed.

121. ZookeeperLeaderElector.elect(): Just to be consistent with case e2, shouldn't we just log an error and set leaderId to -1 in the following case too?
            case None => throw new KafkaException(""Controller doesn't exist"")


;;;","16/Aug/13 16:25;guozhang;Thanks for the comments Jun.

114. Agreed, deleted the Controller.scala and moved logic to KafkaController object.

120. I thought ZookeeperLeaderElector.resign() is a public function that can be called by the parent process of the Elector. Currently ZookeeperLeaderElector is dependent on KafkaController (it takes controllerContext as its parameters), but I think it would be refactored in the future as an independent election module?

121. In this case what really happens is that another broker has elected as the leader but somehow gets ""resigned"". This will trigger another election round. So instead of log an error we would better log it as warn and set leaderId to -1?;;;","16/Aug/13 16:57;junrao;Thanks for patch v13. Looks good. My only suggestion is to set leaderId to -1 in ZookeeperLeaderElector.resign() if we want to keep it.;;;","16/Aug/13 23:28;guozhang;Thanks for the comments, added to resign, rebased on 0.8.;;;","17/Aug/13 04:48;junrao;Thanks for patch v14. Committed to 0.8.;;;","27/Nov/14 00:11;guozhang;We have seen some scenarios which are not fully resolved by this patch: under certain cases the ephemeral node are not deleted ever after the session has expired (there is a ticket ZOOKEEPER-1208 for this and it is marked to be fixed in 3.3.4, but we are still seeing this issue with a newer version).

For this corner case one thing we can do (or more precisely hack around) is to force-delete the ZK path when the written timestamp and the current timestamp's difference is larger than the ZK session timeout value already.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce the queue size in hadoop producer,KAFKA-991,12660812,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,swapnilghike,swapnilghike,30/Jul/13 23:11,02/Aug/13 00:12,14/Jul/23 05:39,02/Aug/13 00:12,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,,,"Currently the queue.size in hadoop producer is 10MB. This means that the KafkaRecordWriter will hit the send button on kafka producer after the size of uncompressed queued messages becomes greater than 10MB. (The other condition on which the messages are sent is if their number exceeds SHORT.MAX_VALUE).

Considering that the server accepts a (compressed) batch of messages of sizeupto 1 million bytes minus the log overhead, we should probably reduce the queue size in hadoop producer. We should do two things:

1. change max message size on the broker to 1 million + log overhead, because that will make the client message size easy to remember. Right now the maximum number of bytes that can be accepted from a client in a batch of messages is an awkward 999988. (I don't have a stronger reason). We have set fetch size on the consumer to 1MB, this gives us a lot of room even if the log overhead increased with further versions.

2. Set the default number of bytes on hadoop producer to 1 million bytes. Anyone who wants higher throughput can override this config using kafka.output.queue.size",,jjkoshy,jkreps,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Aug/13 23:49;swapnilghike;kafka-991-followup-3.patch;https://issues.apache.org/jira/secure/attachment/12595517/kafka-991-followup-3.patch","01/Aug/13 22:54;swapnilghike;kafka-991-followup-v2.patch;https://issues.apache.org/jira/secure/attachment/12595506/kafka-991-followup-v2.patch","01/Aug/13 22:33;swapnilghike;kafka-991-followup.patch;https://issues.apache.org/jira/secure/attachment/12595504/kafka-991-followup.patch","31/Jul/13 04:53;swapnilghike;kafka-991-v1.patch;https://issues.apache.org/jira/secure/attachment/12595126/kafka-991-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,341001,,,Fri Aug 02 00:12:44 UTC 2013,,,,,,,,,,"0|i1mt9j:",341319,,,,,,,,,,,,,,,,,,,,"01/Aug/13 21:03;jkreps;+1;;;","01/Aug/13 21:59;swapnilghike;Also reviewed by Jun, Neha. The reason why we use sync producer in hadoop producer is because the caller may want to get the exceptions from kafka producer and kill their job.;;;","01/Aug/13 21:59;jjkoshy;+1

Committed to 0.8

Minor comment:
- queue size is unintuitive. sounds like number of messages, but it is bytes
- The totalSize > queueSize check should ideally be done before adding it to msgList.;;;","01/Aug/13 22:32;swapnilghike;Thanks Joel. Would you mind committing the follow up patch as well, so that we can complete the hadoop producer queue size/queue bytes related change.;;;","01/Aug/13 22:54;swapnilghike;Fixed README.;;;","01/Aug/13 23:42;jjkoshy;Thanks for the follow-up patch. totalBytes is set to zero in sendMsgList so the next batch totalBytes will less (incorrect) by valBytes.;;;","01/Aug/13 23:49;swapnilghike;Thanks, attached a new patch to address that.;;;","02/Aug/13 00:12;jjkoshy;+1

Committed to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ReassignPartitionCommand and improve usability,KAFKA-990,12660544,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriramsub,sriramsub,sriramsub,30/Jul/13 02:01,28/Aug/13 05:24,14/Jul/23 05:39,28/Aug/13 05:24,,,,,,,,,,,,,,0,,,,"1. The tool does not register for IsrChangeListener on controller failover.
2. There is a race condition where the previous listener can fire on controller failover and the replicas can be in ISR. Even after re-registering the ISR listener after failover, it will never be triggered.
3. The input the tool is a static list which is very hard to use. To improve this, as a first step the tool needs to take a list of topics and list of brokers to do the assignment to and then generate the reassignment plan.",,diederik,guozhang,jjkoshy,junrao,nehanarkhede,sriramsub,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/13 17:20;sriramsub;KAFKA-990-v1-rebased.patch;https://issues.apache.org/jira/secure/attachment/12596883/KAFKA-990-v1-rebased.patch","06/Aug/13 20:31;sriramsub;KAFKA-990-v1.patch;https://issues.apache.org/jira/secure/attachment/12596414/KAFKA-990-v1.patch","12/Aug/13 21:56;sriramsub;KAFKA-990-v2.patch;https://issues.apache.org/jira/secure/attachment/12597574/KAFKA-990-v2.patch","27/Aug/13 20:13;sriramsub;KAFKA-990-v3.patch;https://issues.apache.org/jira/secure/attachment/12600230/KAFKA-990-v3.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,340735,,,Wed Aug 28 05:24:02 UTC 2013,,,,,,,,,,"0|i1mrmf:",341053,,,,,,,,,,,,,,,,,,,,"07/Aug/13 23:40;nehanarkhede;Thanks for patch v1. Could you please fix the compilation errors though?;;;","08/Aug/13 18:15;nehanarkhede;Thanks for the rebased patch, Sriram. Overall, the changes look great. +1. One minor suggestion -

ReassignPartitionsCommand

For determining the replication factor for replica assignment, can we just use the first or last partition in the map instead of relying on a partition id 0? That way if we change the assumption that partition id should always start from 0, this will not break. -
topicInfo._2.head._2.size instead of topicInfo._2.get(TopicAndPartition(topicInfo._1, 0)).get.size

If you are ok with this suggestion, I can make it on checkin.

Also, it seems that #2 in the description above was not really a problem. This is because onPartitionReassignment checks areReplicasInIsr and hence restarts the reassignment correctly. This is however not true if we hit #1, which is a real issue.;;;","08/Aug/13 18:53;jjkoshy;- Topics to move json file format seems unnecessarily complicated. Why not just a JSON array?
- Use CommandLineUtils.checkRequiredArgs
- May be helpful to also print out the existing partition assignment and the final assignment.
- ""dryrun"" to ""dry-run"" which I think is the spelling unix tools like patch tend to use.
- line 88: use head instead of assuming 0 exists (start partition id could be != 0)

I did not finish going through all the changes in controller, but thought I would put in my comments so far :)
;;;","09/Aug/13 18:36;jjkoshy;Can you elaborate on the change to shutdownBroker in KafkaController? I
think we need to include shutting down brokers because the previous shutdown
attempt may have been incomplete due to no other brokers in ISR for some
partition which would have prevented leader movement. Subsequent attempts
would now be rejected.

Good catches on the controller failover. Agree with Neha that #2 is not a
problem for replicas that are in ISR, however, we do need to re-register the
ISR change listener for those replicas that are in ISR.

Finally, we should probably open a separate jira to implement a feature to
cancel an ongoing reassignment given that it is a long-running operation.
The dry-run option reduces the need for this but nevertheless I think it's a
good feature to support in the future.

;;;","10/Aug/13 01:17;jjkoshy;Looks like I might have looked at the wrong patch. I'll review this again this weekend.;;;","12/Aug/13 16:36;jjkoshy;The rebased patch looks good - the shutdown changes I was referring to were in v1.

+1 on the rebased patch - we can fix the minor comments either on check-in or in a separate jira.;;;","12/Aug/13 16:40;guozhang;Looks good to me overall. One question though: in shutdownBroker, should we check liveOrShuttingDownBrokerIds or liveBrokerIds? I saw going back and forth, and hence a little confused which criteria should be applied here?;;;","12/Aug/13 16:51;jjkoshy;It should be liveOrShuttingDownBrokerIds. This is required because a controlled shutdown attempt may
fail - if there are no other brokers in ISR for a partition led by the broker being shutdown. In this case we
would want to proceed with a retry (if there are retries left).;;;","12/Aug/13 17:06;junrao;Thanks for the patch. Looks good overall. Some comments.

1. KafkaController:
1.1 onPartitionReassignment(): The comment above the function needs to be updated. For example, we no longer register the ISR listener here.
1.2 initiateReassignPartitionForTopic(): We fail the reassignment if not all brokers in RAR are alive. I am wondering if this is necessary. A broker can go down when the reassignment process is in progress and we still need to handle this case.
1.3 Should watchIsrChangesForReassignedPartition() be private?

2. ReassignPartitionsCommand
2.1 It seems that we don't allow the options ""topics-to-move-json-file"" and ""manual-assignment-json-file"" to co-exist. Could we add an explicit check and output an appropriate message?
2.2 If ""broker-list"" is not specified, it seems that we should default to the current list of live brokers.
2.3 Could we somehow make dryRun the default behavior? In other words, the user has to add another option to disable dry run.

3. Since the reassignment process requires fetching old data and may pollute the pagecache, do you see any performance impact to produce/fetch request latency when the reassignment is in progress?


;;;","12/Aug/13 17:37;nehanarkhede;1.2 If a broker goes down after the reassignment has started, it will not enter the ISR. So the reassignment will not complete. If the replica fails after the 2nd stage of reassignment is complete, it will get handled through the normal logic of handling failed replicas since the reassignment is complete at that point. I'm not sure there is any value in starting an expensive process like reassignment of partitions when the target replicas are not even alive.
3. This is the same problem we have if we replace the broker machine and bring up another broker with the same id. I think the problem is off somehow throttling replica fetches. This is a good idea. Can we file a separate JIRA for this?;;;","12/Aug/13 18:23;swapnilghike;The rebased patch also failed for me on 0.8 HEAD

$patch -p1 --dry-run < ~/Downloads/KAFKA-990-v1-rebased.patch 
patching file core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala
patching file core/src/main/scala/kafka/utils/ZkUtils.scala
Hunk #1 succeeded at 620 (offset 42 lines).
patching file core/src/main/scala/kafka/admin/ReassignPartitionsCommand.scala
Hunk #1 FAILED at 29.
Hunk #2 FAILED at 81.
;;;","12/Aug/13 18:41;swapnilghike;v1 works with git apply.;;;","12/Aug/13 22:01;sriramsub;Neha - fixed what you suggested

Jun - 

1. KafkaController:
1.1 done 
1.2 we can fail for now. we can revisit this.
1.3 done

2. ReassignPartitionsCommand
2.1 I did not do that as it makes the code ugly and it does not cause any harm. Let me know if you are strong about this.
2.2 i think it is safer to be explicit instead of using the live brokers for the move and causing perf issues
2.3 done
3. polluting the page cache is debatable. We could do the log appends on the follower by-passing the cache but when the follower becomes the leader, it could cause lot of IO. Another option is to throttle the rate at which the appends happen on the follower that reduces the sudden influx of messages at the follower and fetch requests at the leader. Both of these are outside the scope of this JIRA.;;;","12/Aug/13 22:03;swapnilghike;Comments on v1:

I think a much clearer name for this tool would be ReassignReplicasCommand.

Admin tool:
11. I think the tool should also have a verify/validate option that you can run after the replica reassignment has been completed. As of now, the reassignment of a certain partition can fail and the admin won't know without looking at the controller log.
12. It would be good to make dry-run the default behaviour.
13. topics could be a json array, but I don't have a strong opinion one way or the other.
14. we should explicitly check that the options for the manual replica assignment and the assignment using brokerList should not be allowed at the same time. We should also check that the brokerList option is always provided with the topicsToMoveJsonFile option.
15. The ""failed"" messages could probably go to System.err.
16. We should probably not use the partition Id 0 in calling assignReplicasToBrokers, can we instead use { val topicAndPartition = groupedByTopic.get(topicInfo._1).get.get(0); val replicationFactor= topicInfo._2.get(topicAndPartition).get.size} ?
17. dryrun --> dry-run? I just remember seeing the latter more often across other tools.

On the controller:
21. I think a clearer name for initiateReassignPartitionForTopic would be initiateReassignReplicasForTopicPartition; and similar renames if any.
22. We should fix the comment in ReassignPartitionsIsrChangeListener
23. We should update controllerContext.partitionReplicaAssignment only once in KafkaController.updateAssignedReplicasForPartition(). There is an extra over-write as of now.
24. We should batch the requests in KafkaController.StopOldReplicasOfReassignedPartition() 
25. We should call startNewReplicasForReassignedPartition directly in initiateReassignPartitionForTopic instead of calling onPartitionReassignment. As of now, every time areReplicasInIsr returns fals, the controller will call startNewReplicasForReassignedPartition and then log a StateChangeFailedException, because the replicas were already in the new state. This exception will be logged in every call of onPartitionReassignment except for the first call.
26. We should remove the false condition from onPartitionReassignment
27. Currently, for each partition that is reassigned, controller deletes the /admin/reassign_partitions zk path, and populates it with a new list with the reassigned partition removed from the original list. This is probably an overkill, and we can delete the zk path completely once the reassignment of all partitions has completed successfully or in error. Even if there was a controller failover when the reassignment was in progress, the new controller should be able to decide which partitions have already been reassigned and which have not been in initiateReassignPartitionForTopic.;;;","19/Aug/13 15:56;junrao;Thanks for patch v2. A few more comments.

2.1 I think it's better to guard this in the command line. The issue is that if a user provided both options, it's not clear which one takes precedence.
2.2 In that case, we should make sure that brokerList is a mandatory field (like zkConnect).

30. KafkaController.initializeAndMaybeTriggerPartitionReassignment(): The following comment is weird.
    // need to call method

31. Related to Swapnil's comment in #11, currently, the tool finishes after the ZK path is created. It would be useful to add an option to check the state of partition reassignment so that we know either all assignments have completed or the set of partitions that are remaining.
;;;","19/Aug/13 16:06;sriramsub;2.1 will do so.
2.2 We cannot make it mandatory. It is not required when explicit list is specified. In the case when only topics are specified we do make it mandatory.

31. There is already a tool for that. It is called CheckReassignmentStatus.;;;","19/Aug/13 16:33;guozhang;Just one more comment: as for Swapnil's 16, currently partition id 0 is also used for AddPartitionCommand. Shall we change that also?;;;","27/Aug/13 20:19;sriramsub;- made the dry run the default
- added some more input validations for the tool
- some renaming to the controller methods

Swapnil - 

23. We seem to be updating only once. Let me know if that is not the case.
24. It is hard to batch with the way we have the code now. The handleStateChange works per topic partition
25. That would cause us to explicitly invoke startNewReplicasForReassignedPartition in each case outside onPartitionReassignment which is hard to maintain. 
26. Same comment as above
27. This is a lot more than what we want to do in 0.8. The issue is if we do not update, we need to add more checks to ensure it is already done or has failed. We can try to optimize that in trunk.;;;","27/Aug/13 20:19;sriramsub;Guozhang - We have not merged addpartition to trunk yet. We plan to do once we merge the 0.8 code to trunk.;;;","28/Aug/13 05:23;nehanarkhede;+1 on v3. Also, it will be good to file a separate JIRA for Swapnil's suggestion #27;;;","28/Aug/13 05:24;nehanarkhede;Thanks for the patches, committed v3 to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition shutting down high-level consumer results in spinning background thread,KAFKA-989,12660099,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,phargett,phargett,phargett,26/Jul/13 16:50,06/Aug/13 14:13,14/Jul/23 05:39,06/Aug/13 02:04,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"Running an application that uses the Kafka client under load, can often hit this issue within a few hours.

High-level consumers come and go over this application's lifecycle, but there are a variety of defenses that ensure each high-level consumer lasts several seconds before being shutdown.  Nevertheless, some race is causing this background thread to continue long after the ZKClient it is using has been disconnected.  Since the thread was spawned by a consumer that has already been shutdown, the application has no way to find this thread and stop it.

Reported on the users-kafka mailing list 6/25 as ""0.8 throwing exception 'Failed to find leader' and high-level consumer fails to make progress"". 

The only remedy is to shutdown the application and restart it.  Externally detecting that this state has occurred is not pleasant: need to grep log for repeated occurrences of the same exception.

Stack trace:

Failed to find leader for Set([topic6,0]): java.lang.NullPointerException
	at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:416)
	at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:413)
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)
	at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:413)
	at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:409)
	at kafka.utils.ZkUtils$.getChildrenParentMayNotExist(ZkUtils.scala:438)
	at kafka.utils.ZkUtils$.getAllBrokersInCluster(ZkUtils.scala:75)
	at kafka.consumer.ConsumerFetcherManager$LeaderFinderThread.doWork(ConsumerFetcherManager.scala:63)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)",Ubuntu Linux x64,junrao,phargett,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/13 14:27;phargett;KAFKA-989-failed-to-find-leader-patch2.patch;https://issues.apache.org/jira/secure/attachment/12596121/KAFKA-989-failed-to-find-leader-patch2.patch","05/Aug/13 19:13;phargett;KAFKA-989-failed-to-find-leader-patch3.patch;https://issues.apache.org/jira/secure/attachment/12596189/KAFKA-989-failed-to-find-leader-patch3.patch","02/Aug/13 17:32;phargett;KAFKA-989-failed-to-find-leader.patch;https://issues.apache.org/jira/secure/attachment/12595635/KAFKA-989-failed-to-find-leader.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,340291,,,Tue Aug 06 14:13:48 UTC 2013,,,,,,,,,,"0|i1mowf:",340609,,,,,,,,,,,,,,,,,,,,"02/Aug/13 17:31;phargett;This patch may minimize the issue, as there does seem to be a race between startConnections / stopConnections in ConsumerFetcherManager *and* the doWork method of the inner LeaderFinderThread class.

It seems that a thread could be started (from startConnections) but shutdown could happen (from stopConnections) even before the leader thread actually even started to do work.;;;","02/Aug/13 17:32;phargett;Here's the patch: KAFKA-989-failed-to-find-leader.patch.;;;","02/Aug/13 19:57;phargett;Not good enough.  Deadlocks because ShutdownableThread.shutdown grabs another lock.;;;","05/Aug/13 14:26;phargett;When in doubt about how to fix a locking issue...add another lock. ;)

While the real race here involves startConnections / stopConnections in ConsumerFetcherManager, the real trigger for such races appears to be the lack of protection in the shutdown and rebalance operations on ZookeeperConsumerConnector.  There is nothing to prevent a rebalance while a shutdown is in progress, and it would appear that could trigger the race in ConsumerFetcherManager.

The patch I'm attaching (see KAFKA-989-failed-to-find-leader-patch2.patch) adds a shutdown lock grabbed first in both shutdown() and in the run method of the ZKRebalancerListener.  This should prevent a rebalance from happening on a consumer that has already shutdown.  This prevents the fetcher or the zkclient from being in intermediate states, and thus should prevent the race.;;;","05/Aug/13 16:33;junrao;Thanks for the patch. I think it addresses one particular issue: When the consumer connector is shut down, there could still be an outstanding rebalance that uses zkclient which is already set to null. I am not sure if it addresses the problem that you hit though. Some comments:

1. Since syncedRebalance() is called in multiple places, so the shutdown lock should be checked inside syncedRebalance(). Since there is already a rebalanceLock in syncedRebalance(), perhaps shutdown() can just synchronize on that.

;;;","05/Aug/13 16:48;phargett;Thanks for the feedback! Working on a patch that incorporates your suggestions now.

FWIW, I think this will indirectly address my original situation.  I think the reason the original stack trace has occurred is because a rebalance has started a fetcher with a LeaderFinderThread while the consumer itself is in the process of being shutdown.  The LeaderFinderThread is left with a stale ZkClient, and has no other way to know that it's should shutdown.  Since the fetcher has already set its reference to null its reference to the LeaderFinderThread, the running state of the thread will never be changed to shut the thread off.

I'm stuck trying to find an acceptable solution in LeaderFinderThread; this change in ZooKeeperConsumerConnector may have to do until then.

Will post new patch once I have it coded and tested.;;;","05/Aug/13 17:18;junrao;Hmm, in the shutdown logic of consumer connector, we set zkclient to null the last. So, all fetchers and the leader finder thread should have been stopped when zkclient is null.;;;","05/Aug/13 17:27;phargett;Yes, but my working hypothesis is that because there are at least 2 sets of races (in consumer connector syncedRebalance/shutdown, then in ConsumerFetcherManager startConnections/stopConnections), it is actually possible to have a LeaderFinderThread still running that has not been shutdown, even though its consumer has--because a stopConnections call completed before a startConnections call finished.  So there's a started leader finder thread, but its ZkClient has been closed.

The key, I think, is that there is no guarantee that while the consumer connector is shutting down a rebalance event won't actually startup another leader finder thread (by starting fetchers again).

I believe the race in ConsumerFetcherManager is not likely to happen, if the race in ZookeeperConsumerConnector is fixed instead. Thus I avoid fixing the harder race by fixing an easier one that may be its only trigger (at present). :);;;","05/Aug/13 19:13;phargett;Changed to reuse the existing rebalanceLock in shutdown() rather than add in yet another lock.  Modified both shutdown and syncedRebalance accordingly.;;;","05/Aug/13 19:15;phargett;I also think I've convinced myself that while the race in ConsumerFetcherManager is not ideal, the real resource to protect is not the leader finder thread but the shared ZkClient instance--which is managed by ZookeeperConsumerConnector, where these fixes are made.

By reducing the races in the consumer connector, then we're less likely to mismanage the ZkClient.;;;","06/Aug/13 02:04;junrao;Thanks for patch v3. Committed to 0.8.;;;","06/Aug/13 14:13;phargett;Thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make ReassignReplica tool more usable,KAFKA-988,12659708,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriramsub,sriramsub,sriramsub,25/Jul/13 05:27,29/Aug/13 02:50,14/Jul/23 05:39,29/Aug/13 02:50,,,,,,,,,,,,,,0,,,,"As part of this first iteration, we will have two options - 

The manual option takes a list of topic - partition - replicas list and reassigns them. 

The automatic option takes a list of topics and broker list to move the topics to. The tool assigns the replicas for the topic partitions to these brokers using the default assignment strategy.

A dry run will be provided to see the assignment before actually doing the assignment.",,guozhang,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,339900,,,Thu Aug 29 01:29:47 UTC 2013,,,,,,,,,,"0|i1mmhj:",340218,,,,,,,,,,,,,,,,,,,,"29/Aug/13 01:29;guozhang;Is this already resolved?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid checkpointing offsets in Kafka consumer that have not changed since the last commit,KAFKA-987,12659224,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,swapnilghike,swapnilghike,22/Jul/13 23:08,26/Jul/13 21:19,14/Jul/23 05:39,23/Jul/13 18:21,0.8.0,,,0.8.0,,,,,,,,,,0,improvement,,,We need to fix the Kafka zookeeper consumer to avoid checkpointing offsets that have not changed since the last offset commit. This will help reduce the write load on zookeeper.,,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/13 17:41;swapnilghike;kafka-987-v2.patch;https://issues.apache.org/jira/secure/attachment/12593742/kafka-987-v2.patch","22/Jul/13 23:56;swapnilghike;kafka-987.patch;https://issues.apache.org/jira/secure/attachment/12593616/kafka-987.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,339417,,,Fri Jul 26 21:19:01 UTC 2013,,,,,,,,,,"0|i1mjiv:",339737,,,,,,,,,,,,,,,,,,,,"22/Jul/13 23:56;swapnilghike;The only case wherein the code does not have a bug and yet the consumer could end up consuming from an offset < the previously committed offset is when there is an unclean leader election. Hence, this patch will commit new offsets as long as they are different from the previously committed offsets.;;;","23/Jul/13 16:47;nehanarkhede;Thanks for the patch, Swapnil. Good thinking about not limiting the check to offset < committed offset. Just one question about your patch -

 In commitOffsets, should the the map update move to inside the try block to ensure that the map is updated only if the zk write succeeds ?
;;;","23/Jul/13 17:41;swapnilghike;Yes, thank you. Attached v2.;;;","23/Jul/13 18:18;nehanarkhede;+1 on v2.;;;","23/Jul/13 18:20;nehanarkhede;committed v2 to 0.8;;;","24/Jul/13 04:55;junrao;It seems that we don't need to update the offset map in addPartitionTopicInfo(). In fact, currently, if there is no new messages coming in, we won't checkpoint the first offset.;;;","24/Jul/13 06:48;swapnilghike;I think that any call to createMessageStreams will trigger a rebalance, that will fill up the topicregistry and the checkpointing of offsets will start regardless of whether new messages are being consumed or not. Hence, we should probably update the cached checkpointedOffsets map in addPartitionTopicInfo().

May be I have missed something?;;;","24/Jul/13 16:55;nehanarkhede;Jun,

I think what you are suggesting makes sense on startup before the consumer has consumed any messages. However, since the offset map is a cache for what's in zookeeper, the safest way is to keep it in sync with the zookeeper data. Before the consumer can pull any data, it has to rebalance and while rebalancing we read the offsets from zk anyways. So I think it is correct to update the offset cache in addPartitionTopicInfo()
;;;","25/Jul/13 15:52;junrao;1. The issue on startup is the following. If a consumer starts up from the end of the log and there is no new message coming in, no offset will be checkpointed to ZK. This will affect tools like ConsumerOffsetChecker.

2. During rebalance, a consumer may pick up offsets committed by other consumer instances. If we don't update the offset cache in addPartitionTopicInfo(), we will do an extra unnecessary offset update to ZK.

It seems to me that the impact for #1 is bigger than the slight performance impact in #2. Another way to do that is to always force the very first offset (per partition) write to ZK. However, I am not sure if it's worth the complexity.;;;","26/Jul/13 15:34;nehanarkhede;I'm trying to see if I understand what you are saying here. 

1. The basic logic is that as long as the consumer rebalances before starting consumption, the offset cache will be updated. This is true for the zookeeper consumer behavior today. Now, it really doesn't matter much where the consumer starts consuming from. If it hasn't read any messages, there is no need to update offsets in zookeeper. If it reads messages, the offsets will be different from what's in the cache, so they will get checkpointed.

2. I don't think this is worth doing since it only reduces one zookeeper write.;;;","26/Jul/13 21:19;swapnilghike;I discussed this yesterday with Jun. If there is no offset already present in zookeeper, we set the offset value to -1 in the offset cache in addPartitionInfo(). Later, even if no message is consumed, the real offset will be checkpointed. Jun said that he was ok with this patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Increasing log retention quickly overflows scala Int,KAFKA-985,12659210,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jkreps,ffejes,ffejes,22/Jul/13 22:17,03/Aug/13 17:10,14/Jul/23 05:39,03/Aug/13 17:10,0.10.1.0,,,0.8.1,,,,,,,log,,,0,,,,"After increasing log.retention.hours from the default of 168 to 744 (31 days) I noticed that logs were being deleted at every cleanup interval.

scala> val retentionMs: Long = 60 * 60 * 1000 * 31 * 24
retentionMs: Long = -1616567296

This only appears to affect trunk.  I'll attach the minor patch I'm using locally.  Thanks.",,ffejes,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/13 22:19;ffejes;KAFKA-985.patch;https://issues.apache.org/jira/secure/attachment/12593605/KAFKA-985.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,339403,,,Sat Aug 03 17:10:45 UTC 2013,,,,,,,,,,"0|i1mjfr:",339723,,,,,,,,,,,,,,,,,,,,"03/Aug/13 17:10;jkreps;Ack, nice catch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose cleanshutdown method in MirrorMaker at the object level,KAFKA-983,12659198,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,swapnilghike,swapnilghike,22/Jul/13 21:08,25/Jul/13 01:06,14/Jul/23 05:39,25/Jul/13 01:06,,,,,,,,,,,,,,0,bugs,,,"Making clean shutdown in MirrorMaker public at the object level will be useful. Currently if MirrorMaker is run in a container process, the only way to stop it seems to be triggering the shutdown hook (System.exit(0)) which may have unwarranted side effects on the other threads running in that container process. ",,guozhang,jjkoshy,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/13 00:57;swapnilghike;KAFKA-983-rebased.patch;https://issues.apache.org/jira/secure/attachment/12594076/KAFKA-983-rebased.patch","22/Jul/13 21:09;swapnilghike;KAFKA-983.patch;https://issues.apache.org/jira/secure/attachment/12593592/KAFKA-983.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,339391,,,Thu Jul 25 01:06:58 UTC 2013,,,,,,,,,,"0|i1mjd3:",339711,,,,,,,,,,,,,,,,,,,,"22/Jul/13 21:20;guozhang;+1. Looks good to me.;;;","25/Jul/13 00:54;jjkoshy;+1 - can you rebase? Also, may be better to have an if null check in the shutdown statements.;;;","25/Jul/13 00:57;swapnilghike;rebased.;;;","25/Jul/13 01:06;jjkoshy;Thanks for the patch. Committed to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to pull Kafka binaries with Ivy,KAFKA-981,12659097,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,charmalloc,mumrah,mumrah,22/Jul/13 14:20,10/Oct/13 17:54,14/Jul/23 05:39,10/Oct/13 17:54,0.8.0,,,,,,,,,,packaging,,,0,,,,"I am trying to pull the published Kafka binary with a simple Ivy file.

<dependency org=""org.apache.kafka"" name=""kafka_2.9.2"" rev=""0.8.0-beta1"" conf=""default->default""/>

I get the following exception: [ivy:resolve] problem occurred while resolving dependency: org.apache.kafka#kafka_2.9.2;0.8.0-beta1 {default=[default]} with main: java.lang.IllegalArgumentException: null name not allowed
",,charmalloc,dorzey,jkreps,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/13 14:21;mumrah;ant.log;https://issues.apache.org/jira/secure/attachment/12593516/ant.log","22/Jul/13 14:21;mumrah;ivy.xml;https://issues.apache.org/jira/secure/attachment/12593517/ivy.xml",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,339290,,,Thu Oct 10 17:54:22 UTC 2013,,,,,,,,,,"0|i1miqn:",339610,,,,,,,,,,,,,,,,,,,,"22/Jul/13 14:21;mumrah;stdout log from Ant showing full exception;;;","22/Jul/13 14:21;mumrah;Ivy descriptor file I used;;;","22/Jul/13 14:22;mumrah;Also should note this is Ivy 2.3.0;;;","22/Jul/13 14:24;mumrah;For a full example, try cloning this repository and running ""ant ivy""

https://github.com/mumrah/trihug-kafka-demo

I encountered this error when setting up this demo;;;","22/Jul/13 16:54;jkreps;Joe any idea what this might be?;;;","23/Jul/13 01:08;mumrah;Looking closer at the pom.xml, I see two <dependencies> sections. One of which looks like a copy/paste error from ivy.xml, the other looks like normal Maven pom dependencies. See https://gist.github.com/mumrah/6059092#file-pom-xml-L25 for what I'm talking about.;;;","23/Jul/13 01:55;charmalloc;The first dependency section you see is only from maven central http://repo1.maven.org/maven2/org/apache/kafka/kafka_2.9.2/0.8.0-beta1/kafka_2.9.2-0.8.0-beta1.pom its not there in https://repository.apache.org/content/repositories/releases/org/apache/kafka/kafka_2.9.2/0.8.0-beta1/kafka_2.9.2-0.8.0-beta1.pom which I believe is due to me posting the release more than once with making pom changes that did not bump the version in order to get things to work and through https://issues.apache.org/jira/browse/KAFKA-974

I don't know if that is what is causing the error but looking at the source here https://svn.apache.org/repos/asf/ant/ivy/core/trunk/src/java/org/apache/ivy/core/module/id/ModuleId.java that error is because the name of the module is coming in as null, which is odd.

Can you try to override the repository to use https://repository.apache.org/content/repositories/releases instead to see if it works as expected it has resolved other issues folks have brought up.  ;;;","23/Jul/13 02:18;mumrah;[~charmalloc], ah yes - that pesky ""released versions are immutable"" thing. Changing Ivy to pull from Apache's repo did the trick.

BTW, I debugged and stepped through Ivy during that error and it was failing on the bad part of the XML (the ivy <dependency/> elements). So it was definitely caused by the invalid pom.xml.

Now I'm getting failure to resolve ""com.sun.jdmk"" nonsense, but I can deal with that.

Should there be a beta2 release to fix this issue in Maven central?;;;","24/Jul/13 13:29;mumrah;Bump. We really should get the invalid pom out of Maven Central, it's affecting a lot of people. I'm not 100% sure, but I think once a release has been signed/published it is immutable. We probably need to delete beta1 and create beta2

Thoughts?;;;","03/Oct/13 14:50;dorzey;Just started investigating Kafka and hit the invalid Maven Central pom as well. I've used the Apache repo in my Gradle build and it works fine.

When will this get fixed in Maven Central? With the next release?

Thanks!;;;","03/Oct/13 15:02;charmalloc;Next release, yes;;;","10/Oct/13 17:54;charmalloc;See KAFKA-1018;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Crash during log recovery can cause full recovery to never run,KAFKA-980,12658612,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,blakesmith,blakesmith,18/Jul/13 19:20,07/Sep/17 17:59,14/Jul/23 05:39,07/Sep/17 17:59,0.7.1,,,,,,,,,,,,,0,,,,"After an unclean shutdown of the Kafka server, if the broker throws an unhandled exception during log recovery, the broker can get in a state where recovery never runs on a log file.

We saw this problem manifest in production and is summarized on the mailing list here: http://mail-archives.apache.org/mod_mbox/kafka-users/201307.mbox/%3CCAKSpikjgp2sW2ycuf86JrjtAPxWBp92OOEmigVed=u=JFoPvTA@mail.gmail.com%3E

Because recovery state is not tracked explicitly, our kafka broker started writing data even when the log files were not fully recovered. It feels to me like a separate state flag for recovery should also be tracked in cases where recovery does not fully run. What do you guys think?

Steps to reproduce:

1. Shutdown the kafka broker
2. Create a directory named 'bogus' under the kafka log directory (won't parse since it has no partition number)
3. Remove .kafka_cleanshutdown from the log directory to force a recovery
4. Start the kafka broker, observe:
    - Recovery will run on partition segments until it reaches the bogus directory
    - Exception will be thrown during log loading from the bogus directory
    - Kafka will initiate a clean shutdown after the exception is thrown
5. Once the Kafka server is cleanly shutdown, start it again, observe:
    - Recovery will not try to run, since kafka was shutdown cleanly
    - Some partition log files have never been recovered
6. Remove the bogus log directory
7. Start Kafka broker, observe:
    - Recovery will not run
    - Kafka will start cleanly and begin accepting writes again, even though recovery has never run and logs might be in a corrupt state
",,blakesmith,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,338806,,,Thu Jul 18 23:33:06 UTC 2013,,,,,,,,,,"0|i1mfr3:",339126,,,,,,,,,,,,,,,,,,,,"18/Jul/13 20:10;jkreps;Yeah this is a bug. You put down that it effects 0.8, where you able to reproduce it on 0.8?;;;","18/Jul/13 23:33;blakesmith;My mistake, I was not able to repro on 0.8 - it looks like the CleanShutdown logic was moved into the LogManager itself, which will be null if startup failed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add jitter for time based rolling,KAFKA-979,12658419,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,ewencp,sriramsub,sriramsub,17/Jul/13 23:58,15/Oct/14 04:23,14/Jul/23 05:39,15/Oct/14 04:19,,,,,,,,,,,,,,0,newbie,,,"Currently, for low volume topics time based rolling happens at the same time. This causes a lot of IO on a typical cluster and creates back pressure. We need to add a jitter to prevent them from happening at the same time.",,ewencp,guozhang,jjkoshy,jkreps,nehanarkhede,sriharsha,sriramsub,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/14 23:16;ewencp;KAFKA-979.patch;https://issues.apache.org/jira/secure/attachment/12674636/KAFKA-979.patch","14/Oct/14 22:33;ewencp;KAFKA-979_2014-10-14_15:33:31.patch;https://issues.apache.org/jira/secure/attachment/12674866/KAFKA-979_2014-10-14_15%3A33%3A31.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,338613,,,Wed Oct 15 04:23:29 UTC 2014,,,,,,,,,,"0|i1mek7:",338933,,,,,,,,,,,,,,,,,,,,"18/Jul/13 04:23;swapnilghike;Hey Sriram, can you explain what we are trying to achieve here? I am not sure if I understood the meaning of ""jitter"" completely.;;;","18/Jul/13 16:53;guozhang;I think KAFKA-615 would resolve this issue eventually, but I agree that adding a jitter (I am guessing it means adding a random shift for time based rolling policy) is a quick solution for now.;;;","20/Mar/14 22:00;jkreps;KAFKA-615 isn't quite suffient. We still do fsync on log roll it just is done asynchronously. So if you have 1000 partitions and they all roll due to time-based roll you still get a kind of I/O storm.;;;","13/Oct/14 23:16;ewencp;Created reviewboard https://reviews.apache.org/r/26663/diff/
 against branch origin/trunk;;;","14/Oct/14 22:33;ewencp;Updated reviewboard https://reviews.apache.org/r/26663/diff/
 against branch origin/trunk;;;","15/Oct/14 04:19;nehanarkhede;Thanks for the patch. Pushed to trunk and 0.8.2;;;","15/Oct/14 04:23;jjkoshy;Just had one more comment in the RB on how to use the jitter config. Can you take a quick look at that?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"kafka pom file has 2 entries for zookeeper (one with exclusion, one without)",KAFKA-978,12658175,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jbrosenberg,jbrosenberg,17/Jul/13 04:06,10/Oct/13 17:55,14/Jul/23 05:39,10/Oct/13 17:55,,,,0.8.0,,,,,,,,,,0,,,,"I've noticed, in the 0.8-beta the pom file, there are 2 dependencies listed for zookeeper, one of which has an exclusion clause, the other does not.   I assume this is not intended:

<dependency>
<groupId>org.apache.zookeeper</groupId>
<artifactId>zookeeper</artifactId>
<version>3.3.4</version>
<exclusions>
<exclusion>
<groupId>log4j</groupId>
<artifactId>log4j</artifactId>
</exclusion>
<exclusion>
<groupId>jline</groupId>
<artifactId>jline</artifactId>
</exclusion>
</exclusions>
</dependency>

and then

<dependency>
<groupId>org.apache.zookeeper</groupId>
<artifactId>zookeeper</artifactId>
<version>3.3.4</version>
</dependency>",,charmalloc,jbrosenberg,jbrosenberg@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,338369,,,Thu Oct 10 17:55:15 UTC 2013,,,,,,,,,,"0|i1md27:",338689,,,,,,,,,,,,,,,,,,,,"10/Oct/13 17:55;charmalloc;See KAFKA-1018;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement generation/term per leader to reconcile messages correctly,KAFKA-977,12658108,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriramsub,sriramsub,sriramsub,16/Jul/13 20:32,16/May/17 16:08,14/Jul/23 05:39,16/May/17 13:01,,,,0.11.0.0,,,,,,,,,,0,,,,"During unclean leader election, the log messages can diverge and when the followers come back up Kafka does not reconcile correctly. To implement it correctly, we need to add a term/generation to each message and use that to reconcile.",,ijuma,junrao,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,338302,,,Tue May 16 16:08:27 UTC 2017,,,,,,,,,,"0|i1mcnb:",338622,,,,,,,,,,,,,,,,,,,,"22/Sep/14 07:44;sriramsub;I would like to bring this issue to discussion again. Kafka is used a lot more now for use cases other than just moving data from point A to point B. For example, consider the case where Kafka acts as the log and materialized views are created by consuming these logs. In such scenarios, it is important that the logs are consistent and do not diverge even under unclean leader elections (Replaying these replicas should create the same view). Having a generation/term is essential for log replication and it would be great for Kafka to have the same guarantees as other log replication protocols.  I would be happy to give more detailed examples for this but would want to know if we think this is an issue to address soon.;;;","28/Sep/14 15:40;junrao;Sriram,

There has been some discussion related to this in KAFKA-1211 as well. Yes, by using the leader generations per partition, we can (1) make sure replicas are consistent after unclean leader election; (2) make sure there is no data loss in the corner case discussed in KAFKA-1211 (i.e., another leader failure happens just after the follower has truncated the log, but before it has re-replicated existing committed data from the leader). The change potentially requires wire protocol and on-disk format change though. So, we need to think through how to do that in a backward compatible way.;;;","16/May/17 13:01;ijuma;Marking this as fixed since it seems to be the same as KIP-101/KAFKA-1211.;;;","16/May/17 15:59;junrao;Just to clarify. KAFKA-1211 added leader epoch in message set to prevent data losses. However, it didn't address the log divergency issue due to unclean leader election. The complexity is mostly on compacted topics. When a log is compacted, it's possible for all messages in a given leader epoch to be deleted. Therefore, it's a bit tricky to fully reconcile the log when an unclean leader election happens. This is less an issue since unclean leader election will be turned off by default from 0.11.0.;;;","16/May/17 16:08;ijuma;[~junrao], should we reopen this then? I believe there was at least one more JIRA for the log divergence issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
can't use public release maven repo because of failure of downloaded dependency,KAFKA-974,12657567,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,charmalloc,charmalloc,13/Jul/13 05:56,28/Nov/13 09:38,14/Jul/23 05:39,20/Aug/13 14:02,,,,0.8.0,,,,,,,,,,2,,,,"trying to use the 0.8.0-beta1 release from public maven


name := ""Stub""

version := ""1.0.0.0""

scalaVersion := ""2.9.2""

mainClass := Some(""Stub"")

libraryDependencies ++= Seq(
	""org.apache.kafka"" % ""kafka_2.9.2"" % ""0.8.0-beta1""
)

results in

Joes-MacBook-Air:stub joestein$ ./sbt compile
[info] Set current project to default-63d5f2 (in build file:/opt/medialets/SymanticManager/scala/stub/)
[info] Updating {file:/opt/medialets/SymanticManager/scala/stub/}default-63d5f2...
[warn] 	[NOT FOUND  ] javax.jms#jms;1.1!jms.jar (50ms)
[warn] ==== public: tried
[warn]   http://repo1.maven.org/maven2/javax/jms/jms/1.1/jms-1.1.jar
[warn] 	[NOT FOUND  ] com.sun.jdmk#jmxtools;1.2.1!jmxtools.jar (12ms)
[warn] ==== public: tried
[warn]   http://repo1.maven.org/maven2/com/sun/jdmk/jmxtools/1.2.1/jmxtools-1.2.1.jar
[warn] 	[NOT FOUND  ] com.sun.jmx#jmxri;1.2.1!jmxri.jar (71ms)
[warn] ==== public: tried
[warn]   http://repo1.maven.org/maven2/com/sun/jmx/jmxri/1.2.1/jmxri-1.2.1.jar
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn] 	::              FAILED DOWNLOADS            ::
[warn] 	:: ^ see resolution messages for details  ^ ::
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn] 	:: javax.jms#jms;1.1!jms.jar
[warn] 	:: com.sun.jdmk#jmxtools;1.2.1!jmxtools.jar
[warn] 	:: com.sun.jmx#jmxri;1.2.1!jmxri.jar
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[info] 
[warn] :: problems summary ::
[warn] :::: WARNINGS
[warn] 		[NOT FOUND  ] javax.jms#jms;1.1!jms.jar (50ms)
[warn] 	==== public: tried
[warn] 	  http://repo1.maven.org/maven2/javax/jms/jms/1.1/jms-1.1.jar
[warn] 		[NOT FOUND  ] com.sun.jdmk#jmxtools;1.2.1!jmxtools.jar (12ms)
[warn] 	==== public: tried
[warn] 	  http://repo1.maven.org/maven2/com/sun/jdmk/jmxtools/1.2.1/jmxtools-1.2.1.jar
[warn] 		[NOT FOUND  ] com.sun.jmx#jmxri;1.2.1!jmxri.jar (71ms)
[warn] 	==== public: tried
[warn] 	  http://repo1.maven.org/maven2/com/sun/jmx/jmxri/1.2.1/jmxri-1.2.1.jar
[warn] 		::::::::::::::::::::::::::::::::::::::::::::::
[warn] 		::              FAILED DOWNLOADS            ::
[warn] 		:: ^ see resolution messages for details  ^ ::
[warn] 		::::::::::::::::::::::::::::::::::::::::::::::
[warn] 		:: javax.jms#jms;1.1!jms.jar
[warn] 		:: com.sun.jdmk#jmxtools;1.2.1!jmxtools.jar
[warn] 		:: com.sun.jmx#jmxri;1.2.1!jmxri.jar
[warn] 		::::::::::::::::::::::::::::::::::::::::::::::
[info] 
[info] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS
[error] {file:/opt/medialets/SymanticManager/scala/stub/}default-63d5f2/*:update: sbt.ResolveException: download failed: javax.jms#jms;1.1!jms.jar
[error] download failed: com.sun.jdmk#jmxtools;1.2.1!jmxtools.jar
[error] download failed: com.sun.jmx#jmxri;1.2.1!jmxri.jar
[error] Total time: 3 s, completed Jul 13, 2013 1:55:36 AM
",,charmalloc,craigwblake,criccomini,jkreps,miguno,otis,scruffy323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/13 06:14;charmalloc;KAFKA-974.patch;https://issues.apache.org/jira/secure/attachment/12592127/KAFKA-974.patch","15/Jul/13 01:19;charmalloc;KAFKA-974.v2.patch;https://issues.apache.org/jira/secure/attachment/12592225/KAFKA-974.v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,337788,,,Thu Nov 28 09:38:14 UTC 2013,,,,,,,,,,"0|i1m9hj:",338110,,,,,,,,,,,,,,,,,,,,"13/Jul/13 06:38;charmalloc;first patch that didn't work;;;","13/Jul/13 09:01;charmalloc;tried this, same error

diff --git a/project/Build.scala b/project/Build.scala
index bad93db..c758178 100644
--- a/project/Build.scala
+++ b/project/Build.scala
@@ -39,7 +39,21 @@ object KafkaBuild extends Build {
     <url>http://www.apache.org/licenses/LICENSE-2.0.txt</url>
     <distribution>repo</distribution>
   </license>
-</licenses>,
+</licenses>
+<dependencies>
+  <exclusion>
+    <groupId>com.sun.jmx</groupId>
+    <artifactId>jmxri</artifactId>
+  </exclusion>
+  <exclusion>
+    <groupId>com.sun.jdmk</groupId>
+    <artifactId>jmxtools</artifactId>
+  </exclusion>
+  <exclusion>
+    <groupId>javax.jms</groupId>
+    <artifactId>jms</artifactId>
+  </exclusion>
+</dependencies>,
     scalacOptions ++= Seq(""-deprecation"", ""-unchecked"", ""-g:none""),
     crossScalaVersions := Seq(""2.8.0"",""2.8.2"", ""2.9.1"", ""2.9.2""),
     scalaVersion := ""2.8.0"",
;;;","15/Jul/13 00:44;charmalloc;tried this with no luck, same error

diff --git a/project/Build.scala b/project/Build.scala
index bad93db..219d7e0 100644
--- a/project/Build.scala
+++ b/project/Build.scala
@@ -52,7 +52,11 @@ object KafkaBuild extends Build {
     javacOptions ++= Seq(""-Xlint:unchecked"", ""-source"", ""1.5""),
     parallelExecution in Test := false, // Prevent tests from overrunning each other
     libraryDependencies ++= Seq(
-      ""log4j""                 % ""log4j""        % ""1.2.15"",
+      ""log4j""                 % ""log4j""        % ""1.2.15"" excludeAll(
+    ExclusionRule(organization = ""com.sun.jdmk""),
+    ExclusionRule(organization = ""com.sun.jmx""),
+    ExclusionRule(organization = ""javax.jms"")
+    ),
       ""net.sf.jopt-simple""    % ""jopt-simple""  % ""3.2"",
       ""org.slf4j""             % ""slf4j-simple"" % ""1.6.4""
     ),
;;;","15/Jul/13 01:08;charmalloc;well, as far as SBT goes http://www.scala-sbt.org/release/docs/Detailed-Topics/Library-Management#exclude-transitive-dependencies it looks like exclude is the right command to use not excludeAll 

diff --git a/project/Build.scala b/project/Build.scala
index bad93db..b3858f3 100644
--- a/project/Build.scala
+++ b/project/Build.scala
@@ -52,7 +52,7 @@ object KafkaBuild extends Build {
     javacOptions ++= Seq(""-Xlint:unchecked"", ""-source"", ""1.5""),
     parallelExecution in Test := false, // Prevent tests from overrunning each other
     libraryDependencies ++= Seq(
-      ""log4j""                 % ""log4j""        % ""1.2.15"",
+      ""log4j""                 % ""log4j""        % ""1.2.15"" exclude(""javax.jms"", ""jms""),
       ""net.sf.jopt-simple""    % ""jopt-simple""  % ""3.2"",
       ""org.slf4j""             % ""slf4j-simple"" % ""1.6.4""
     ),


however, still same error though 

[error] {file:/opt/medialets/SymanticManager/scala/stub/}default-63d5f2/*:update: sbt.ResolveException: download failed: javax.jms#jms;1.1!jms.jar

the pom does look better with the exclusion in the place I expected 

<dependency>
<groupId>log4j</groupId>
<artifactId>log4j</artifactId>
<version>1.2.15</version>
<exclusions>
<exclusion>
<groupId>javax.jms</groupId>
<artifactId>jms</artifactId>
</exclusion>
</exclusions>
</dependency>

still not working;;;","15/Jul/13 01:19;charmalloc;That last fix did work and attached patch

to use public maven

""org.apache.kafka"" % ""kafka_2.9.2"" % ""0.8.0-beta1"" intransitive()

the key in this last patch was to include intransitive() so that SBT knows to not fetch the dependencies we specified in the pom http://www.scala-sbt.org/release/docs/Detailed-Topics/Library-Management#disable-transitivity;;;","15/Jul/13 16:33;jkreps;w00t!;;;","15/Jul/13 18:55;criccomini;Hey Joe,

This looks really good. Per our discussion on the mailing list, I'm updating with some issues I've found:

1. Maven central can't resolve it properly (POM is different from Apache release). Have to use Apache release repo directly to get things to work.
2. Exclusions must be manually applied even though they exist in Kafka's POM already. I think Maven can handle this automatically, if the POM is done right.
3. Weird parent block in Kafka POMs that points to org.apache.
4. Would be nice to publish kafka-test jars as well.
5. Would be nice to have SNAPSHOT releases off of trunk using a Hudson job.

Our hypothesis regarding the first issue is that it was caused by duplicate publishing during testing, and it should go away in the future.

Regarding number 2, I have to explicitly exclude the following when depending on Kafka:

      exclude module: 'jms'
      exclude module: 'jmxtools'
      exclude module: 'jmxri'

I believe these just need to be excluded from the appropriate jars in the actual SBT build file, to fix this issue. I see JMS is excluded from ZK, but it's probably being pulled in from somewhere else, anyway.

Regarding number 3, it is indeed listed as something to do on the Apache publication page (http://www.apache.org/dev/publishing-maven-artifacts.html). I can't find an example of anyone using it, but it doesn't seem to be doing any harm.

Also, regarding your intransitive() call, that is disabling ALL dependencies not just the exclusions, I believe. I think that the ""proper"" way to do that would be to do what I've done: exclude(""jms"", ""jmxtools"", ""jmxri""). Regardless, fixing number 2, above, should mean that intransitive()/exclude() are not required.;;;","20/Aug/13 14:02;charmalloc;resolving because we need another release to take the patch and publish it since it is working in the apache repository fine;;;","28/Nov/13 09:38;miguno;FWIW here is an sbt snippet that resolves this problem.  Chris' last comment was pointing me in the right direction but I still had to figure out the correct sbt syntax -- literally using exclude(""jms"", ""jmxtools"", ""jmxri"") did not work.

{code}
libraryDependencies ++= Seq(
  ""org.apache.kafka"" % ""kafka_2.10"" % ""0.8.0""
    exclude(""javax.jms"", ""jms"")
    exclude(""com.sun.jdmk"", ""jmxtools"")
    exclude(""com.sun.jmx"", ""jmxri""),
  // Alternatively, this apparently also works but it will exclude ALL deps of the excluded organizations:
  //""org.apache.kafka"" % ""kafka_2.10"" % ""0.8.0"" excludeAll(
  //  ExclusionRule(organization = ""com.sun.jdmk""),
  //  ExclusionRule(organization = ""com.sun.jmx""),
  //  ExclusionRule(organization = ""javax.jms"")
  //),
  ...other dependencies...
)
{code}

Versions:
- sbt 0.13.0
- Scala 2.10.3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetadataRequest returns stale list of brokers,KAFKA-972,12657261,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,singhashish,vccarvalho,vccarvalho,11/Jul/13 13:34,14/Jul/15 18:22,14/Jul/23 05:39,14/Jul/15 00:17,0.8.0,,,0.9.0.0,,,,,,,core,,,1,,,,"When we issue an metadatarequest towards the cluster, the list of brokers is stale. I mean, even when a broker is down, it's returned back to the client. The following are examples of two invocations one with both brokers online and the second with a broker down:

{
    ""brokers"": [
        {
            ""nodeId"": 0,
            ""host"": ""10.139.245.106"",
            ""port"": 9092,
            ""byteLength"": 24
        },
        {
            ""nodeId"": 1,
            ""host"": ""localhost"",
            ""port"": 9093,
            ""byteLength"": 19
        }
    ],
    ""topicMetadata"": [
        {
            ""topicErrorCode"": 0,
            ""topicName"": ""foozbar"",
            ""partitions"": [
                {
                    ""replicas"": [
                        0
                    ],
                    ""isr"": [
                        0
                    ],
                    ""partitionErrorCode"": 0,
                    ""partitionId"": 0,
                    ""leader"": 0,
                    ""byteLength"": 26
                },
                {
                    ""replicas"": [
                        1
                    ],
                    ""isr"": [
                        1
                    ],
                    ""partitionErrorCode"": 0,
                    ""partitionId"": 1,
                    ""leader"": 1,
                    ""byteLength"": 26
                },
                {
                    ""replicas"": [
                        0
                    ],
                    ""isr"": [
                        0
                    ],
                    ""partitionErrorCode"": 0,
                    ""partitionId"": 2,
                    ""leader"": 0,
                    ""byteLength"": 26
                },
                {
                    ""replicas"": [
                        1
                    ],
                    ""isr"": [
                        1
                    ],
                    ""partitionErrorCode"": 0,
                    ""partitionId"": 3,
                    ""leader"": 1,
                    ""byteLength"": 26
                },
                {
                    ""replicas"": [
                        0
                    ],
                    ""isr"": [
                        0
                    ],
                    ""partitionErrorCode"": 0,
                    ""partitionId"": 4,
                    ""leader"": 0,
                    ""byteLength"": 26
                }
            ],
            ""byteLength"": 145
        }
    ],
    ""responseSize"": 200,
    ""correlationId"": -1000
}


{
    ""brokers"": [
        {
            ""nodeId"": 0,
            ""host"": ""10.139.245.106"",
            ""port"": 9092,
            ""byteLength"": 24
        },
        {
            ""nodeId"": 1,
            ""host"": ""localhost"",
            ""port"": 9093,
            ""byteLength"": 19
        }
    ],
    ""topicMetadata"": [
        {
            ""topicErrorCode"": 0,
            ""topicName"": ""foozbar"",
            ""partitions"": [
                {
                    ""replicas"": [
                        0
                    ],
                    ""isr"": [],
                    ""partitionErrorCode"": 5,
                    ""partitionId"": 0,
                    ""leader"": -1,
                    ""byteLength"": 22
                },
                {
                    ""replicas"": [
                        1
                    ],
                    ""isr"": [
                        1
                    ],
                    ""partitionErrorCode"": 0,
                    ""partitionId"": 1,
                    ""leader"": 1,
                    ""byteLength"": 26
                },
                {
                    ""replicas"": [
                        0
                    ],
                    ""isr"": [],
                    ""partitionErrorCode"": 5,
                    ""partitionId"": 2,
                    ""leader"": -1,
                    ""byteLength"": 22
                },
                {
                    ""replicas"": [
                        1
                    ],
                    ""isr"": [
                        1
                    ],
                    ""partitionErrorCode"": 0,
                    ""partitionId"": 3,
                    ""leader"": 1,
                    ""byteLength"": 26
                },
                {
                    ""replicas"": [
                        0
                    ],
                    ""isr"": [],
                    ""partitionErrorCode"": 5,
                    ""partitionId"": 4,
                    ""leader"": -1,
                    ""byteLength"": 22
                }
            ],
            ""byteLength"": 133
        }
    ],
    ""responseSize"": 188,
    ""correlationId"": -1000
}
",,junrao,nehanarkhede,singhashish,sslavic,vccarvalho,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1367,,,,,,,,,,,,,"24/Jun/15 15:28;granthenke;BrokerMetadataTest.scala;https://issues.apache.org/jira/secure/attachment/12741634/BrokerMetadataTest.scala","30/Jun/15 00:46;singhashish;KAFKA-972.patch;https://issues.apache.org/jira/secure/attachment/12742664/KAFKA-972.patch","01/Jul/15 01:42;singhashish;KAFKA-972_2015-06-30_18:42:13.patch;https://issues.apache.org/jira/secure/attachment/12742993/KAFKA-972_2015-06-30_18%3A42%3A13.patch","01/Jul/15 08:37;singhashish;KAFKA-972_2015-07-01_01:36:56.patch;https://issues.apache.org/jira/secure/attachment/12743020/KAFKA-972_2015-07-01_01%3A36%3A56.patch","01/Jul/15 08:43;singhashish;KAFKA-972_2015-07-01_01:42:42.patch;https://issues.apache.org/jira/secure/attachment/12743021/KAFKA-972_2015-07-01_01%3A42%3A42.patch","01/Jul/15 15:06;singhashish;KAFKA-972_2015-07-01_08:06:03.patch;https://issues.apache.org/jira/secure/attachment/12743100/KAFKA-972_2015-07-01_08%3A06%3A03.patch","07/Jul/15 06:07;singhashish;KAFKA-972_2015-07-06_23:07:34.patch;https://issues.apache.org/jira/secure/attachment/12743895/KAFKA-972_2015-07-06_23%3A07%3A34.patch","07/Jul/15 17:42;singhashish;KAFKA-972_2015-07-07_10:42:41.patch;https://issues.apache.org/jira/secure/attachment/12744001/KAFKA-972_2015-07-07_10%3A42%3A41.patch","08/Jul/15 06:24;singhashish;KAFKA-972_2015-07-07_23:24:13.patch;https://issues.apache.org/jira/secure/attachment/12744146/KAFKA-972_2015-07-07_23%3A24%3A13.patch",,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,337483,,,Tue Jul 14 18:22:20 UTC 2015,,,,,,,,,,"0|i1m7lz:",337806,,,,,,,,,,,,,,,,,,,,"13/Sep/13 15:58;junrao;Could you describe how to reproduce this?;;;","13/Sep/13 17:00;nehanarkhede;Is this repetitive or the metadata starts returning consistent data after some time ? Since the metadata is communicated to the brokers by the controller, it is possible that there is a time window after an event has happened and before all the brokers have learned of the event.;;;","24/Jun/15 15:28;granthenke;Sample Failing Tests:
- testBrokerMetadataOnClusterWithNoTopics
- testBrokerMetadataOnBrokerShutdown
- testBrokerMetadataOnBrokerAddition;;;","25/Jun/15 03:10;singhashish;Hey Guys,

I spent some time reproducing the issue and finding the root cause. Turns out KAFKA-1367 is not the issue here. Below is the problem and my suggested solution.

Problem:
Alive brokers list not being propagated to brokers by coordinator. When a broker is started, it writes to ZK brokers path. Coordinator watches that path and notices the new broker. On noticing a new broker, the coordinator sends the UpdateMetadataRequest to only the new broker that just started up. The other brokers in cluster never gets to know that there are new brokers in the cluster.

Effect of KAFKA-1367: After KAFKA-1367 goes in it correct alive brokers information will be propagated to all live brokers after ISR changes at any broker. However, if there are no topics/ partitions KAFKA-1367 will not help and this issue will still be there.

Solution:
Instead of sending the UpdateMetadataRequest only to new broker, send it to all live brokers in the cluster.

[~junrao], [~nehanarkhede], [~granthenke], [~gwenshap], [~charmalloc], [~jjkoshy] please provide your thoughts. I have a patch ready which I will post if you guys think this is indeed the correct approach. I have verified that above approach fixes the issue.;;;","25/Jun/15 19:50;granthenke;This solutions sounds reasonable to me.;;;","30/Jun/15 00:46;singhashish;Created reviewboard https://reviews.apache.org/r/36030/
 against branch trunk;;;","01/Jul/15 01:42;singhashish;Updated reviewboard https://reviews.apache.org/r/36030/
 against branch trunk;;;","01/Jul/15 08:37;singhashish;Updated reviewboard https://reviews.apache.org/r/36030/
 against branch trunk;;;","01/Jul/15 08:43;singhashish;Updated reviewboard https://reviews.apache.org/r/36030/
 against branch trunk;;;","01/Jul/15 15:06;singhashish;Updated reviewboard https://reviews.apache.org/r/36030/
 against branch trunk;;;","07/Jul/15 06:07;singhashish;Updated reviewboard https://reviews.apache.org/r/36030/
 against branch trunk;;;","07/Jul/15 17:42;singhashish;Updated reviewboard https://reviews.apache.org/r/36030/
 against branch trunk;;;","08/Jul/15 06:24;singhashish;Updated reviewboard https://reviews.apache.org/r/36030/
 against branch trunk;;;","10/Jul/15 22:08;singhashish;[~junrao] could you take a look, thanks.;;;","14/Jul/15 00:17;junrao;Thanks for the latest patch. +1 and committed to trunk.;;;","14/Jul/15 18:22;singhashish;Thanks [~junrao]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
./sbt +package rebuilds the Hadoop consumer jar N times with the same output file,KAFKA-970,12657183,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jkreps,jkreps,11/Jul/13 00:23,20/Mar/14 22:02,14/Jul/23 05:39,20/Mar/14 22:02,0.8.0,,,,,,,,,,,,,0,,,,Running ./sbt +package now builds all jars for all scala versions. Unfortunately for the Hadoop producer and consumer since it uses the same file name every time this just means it is overwriting the same file over and over and the final file is whatever the last scala version is that is built. This should be a trivial fix.,,jkreps,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,337406,,,Wed Sep 11 17:29:07 UTC 2013,,,,,,,,,,"0|i1m74v:",337729,,,,,,,,,,,,,,,,,,,,"11/Sep/13 17:29;junrao;This affects the following jars.

./contrib/hadoop-consumer/target/hadoop-consumer-0.8.0-beta1.jar
./contrib/hadoop-producer/target/hadoop-producer-0.8.0-beta1.jar
./examples/target/kafka-java-examples-0.8.0-beta1.jar
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to prevent failure of rebalance when there are no brokers available when consumer comes up,KAFKA-969,12657154,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriramsub,sriramsub,sriramsub,10/Jul/13 22:00,11/Jul/13 16:43,14/Jul/23 05:39,11/Jul/13 05:49,,,,0.8.0,,,,,,,,,,0,,,,"There are some rare instances when a consumer would be up before bringing up the Kafka brokers. This would usually happen in a test scenario. In such conditions, during rebalance instead of failing the rebalance we just log the error and subscribe to broker changes. When the broker comes back up, we trigger the rebalance.",,guozhang,jjkoshy,junrao,smeder,sriramsub,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/13 22:01;sriramsub;emptybrokeronrebalance-1.patch;https://issues.apache.org/jira/secure/attachment/12591741/emptybrokeronrebalance-1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,337377,,,Thu Jul 11 16:43:52 UTC 2013,,,,,,,,,,"0|i1m6yf:",337700,,,,,,,,,,,,,,,,,,,,"11/Jul/13 01:09;jjkoshy;This seems reasonable, but I'm not fully convinced about it. E.g., a test
framework should ensure external dependencies are up before attempting to
make service calls to those dependencies. That said, it is perhaps also
reasonable from a consumer's perspective to expect that returned streams be
empty at first, and whenever brokers and topics show up, then events should
just show up.

I'm +1 on this patch except for the if-else formatting issue.  Also, I think
this patch alone would be insufficient to meet the above.  There are two
other issues:

- We should register a watcher under the topics path (currently done only if
  a wildcard is specified)
- KAFKA-956 is also related. I need to give that one some thought.


;;;","11/Jul/13 04:54;sriramsub;The other issues you mention are separate from this. You should file JIRAs for those. ;;;","11/Jul/13 05:40;guozhang;I think KAFKA-956 is orthogonal to this JIRA. It is about broker metadata not able to propagate to the brokers yet.;;;","11/Jul/13 05:49;junrao;Thanks for the patch. Committed to 0.8 with the following minor change.

* change ERROR logging to WARN.;;;","11/Jul/13 05:53;jjkoshy;As I already said, I'm +1 on this patch for what it intends to address. Those two issues I mentioned are orthogonal. By ""above"" in my comment I was referring to the possible expectation from consumers: "".. from a consumer's perspective to expect that returned streams be empty at first, and whenever brokers and topics show up, then events should just show up."" - not the ""failed to rebalance issue"".
;;;","11/Jul/13 06:34;swapnilghike;I agree with Joel. Registering a consumers' subscription in zookeeper and being open to brokers and topics showing up after the consumer has started is reasonable. It's similar to a consumer waiting to consume data from the tail of a topic when no new data is coming in and partition count could change.;;;","11/Jul/13 16:28;guozhang;I think this is related to how we would enforce the order of starting servers/clients. In the quickstart Wiki page

http://kafka.apache.org/08/quickstart.html

We suggest to 1) start servers, 2) start producers and start sending messages, and then 3) start consumers. This is the normal case for production, whereas for dev testing this order would be different, such as 1) start servers with no topics created, 2) start producers and consumers, 3) producer start sending messages; or start 1) and 2) at roughly the same time; or even start 2) before start 1). When such order happens, these issues will arise.;;;","11/Jul/13 16:43;smeder;I feel that Kafka servers and clients should really not have to care about their respective start-up order and should be able to recover from any order. Otherwise certain recovery scenarios become cumbersome and it introduces a lot of extra complexity in the deployment system.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typographical Errors in Output,KAFKA-968,12657135,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,nehanarkhede,rsealfon,rsealfon,10/Jul/13 19:23,14/Jul/13 21:24,14/Jul/23 05:39,14/Jul/13 21:24,0.8.0,,,0.8.0,,,,,,,core,replication,,0,partition,typo,,"The word ""partition"" is referred to as ""partion"" in system_test/replication_testsuite/testcase_0106/testcase_0106_properties.json line 2 and core/src/main/scala/kafka/server/AbstractFetcherManager.scala line 49.  This typo may interfere with text-based searching of output.",Kafka was run on GNU/Linux x86_64 but this is relevant to all environments,rsealfon,,,,,,,,,,,,,,,,,,,,,,,,,,,,300,300,,0%,300,300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,337358,,,Sun Jul 14 21:24:20 UTC 2013,,,,,,,,,,"0|i1m6u7:",337681,,,,,,,,,,,,,,,,,,,,"14/Jul/13 21:24;rsealfon;I checked the repository and this seems to have been fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default hadoop-producer configs to request.required.acks = 1,KAFKA-964,12656646,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,08/Jul/13 18:40,08/Jul/13 20:07,14/Jul/23 05:39,08/Jul/13 20:07,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,,,,,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/13 19:49;swapnilghike;kafka-964-v1.patch;https://issues.apache.org/jira/secure/attachment/12591261/kafka-964-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,336869,,,Mon Jul 08 20:07:54 UTC 2013,,,,,,,,,,"0|i1m3tz:",337192,,,,,,,,,,,,,,,,,,,,"08/Jul/13 19:49;swapnilghike;Set request.required.acks = 1 in hadoop-producer.

Maintained the override of compression codec to gzip to not change the behavior.

Maintained the override of producer type to sync, since async does not make sense in case of hadoop-producer. So it would be good to explicitly set the producer type to sync.

Removed the override of send.buffer.bytes as there was no particular reason to not use kafka default.;;;","08/Jul/13 20:07;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
when publishing to maven repository central missing signature on everything,KAFKA-963,12656516,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,charmalloc,charmalloc,07/Jul/13 12:48,07/Jul/13 13:20,14/Jul/23 05:39,07/Jul/13 13:20,,,,0.8.0,,,,,,,,,,0,,,,"I got the publishing to work but when closing the staged release I get 

 -Missing Signature: '/org/apache/kafka/kafka_2.9.2/0.8.0-beta1/kafka_2.9.2-0.8.0-beta1.jar.asc' does not exist for 'kafka_2.9.2-0.8.0-beta1.jar'.
    -Missing Signature: '/org/apache/kafka/kafka_2.9.2/0.8.0-beta1/kafka_2.9.2-0.8.0-beta1-javadoc.jar.asc' does not exist for 'kafka_2.9.2-0.8.0-beta1-javadoc.jar'.
    -Missing Signature: '/org/apache/kafka/contrib_2.8.2/0.8.0-beta1/contrib_2.8.2-0.8.0-beta1.pom.asc' does not exist for 'contrib_2.8.2-0.8.0-beta1.pom'.
    -Missing Signature: '/org/apache/kafka/contrib_2.8.2/0.8.0-beta1/contrib_2.8.2-0.8.0-beta1.jar.asc' does not exist for 'contrib_2.8.2-0.8.0-beta1.jar'.
    -Missing Signature: '/org/apache/kafka/contrib_2.8.2/0.8.0-beta1/contrib_2.8.2-0.8.0-beta1-sources.jar.asc' does not exist for 'contrib_2.8.2-0.8.0-beta1-sources.jar'.
    -Missing Signature: '/org/apache/kafka/contrib_2.8.2/0.8.0-beta1/contrib_2.8.2-0.8.0-beta1-javadoc.jar.asc' does not exist for 'contrib_2.8.2-0.8.0-beta1-javadoc.jar'.
    -Missing Signature: '/org/apache/kafka/kafka-perf_2.8.2/0.8.0-beta1/kafka-perf_2.8.2-0.8.0-beta1-javadoc.jar.asc' does not exist for 'kafka-perf_2.8.2-0.8.0-beta1-javadoc.jar'.
    -Missing Signature: '/org/apache/kafka/kafka-perf_2.8.2/0.8.0-beta1/kafka-perf_2.8.2-0.8.0-beta1.jar.asc' does not exist for 'kafka-perf_2.8.2-0.8.0-beta1.jar'.
    -Missing Signature: '/org/apache/kafka/kafka-perf_2.8.2/0.8.0-beta1/kafka-perf_2.8.2-0.8.0-beta1.pom.asc' does not exist for 'kafka-perf_2.8.2-0.8.0-beta1.pom'.
    -Missing Signature: '/org/apache/kafka/kafka-perf_2.8.2/0.8.0-beta1/kafka-perf_2.8.2-0.8.0-beta1-sources.jar.asc' does not exist for 'kafka-perf_2.8.2-0.8.0-beta1-sources.jar'.
    -Missing Signature: '/org/apache/kafka/contrib_2.8.0/0.8.0-beta1/contrib_2.8.0-0.8.0-beta1-javadoc.jar.asc' does not exist for 'contrib_2.8.0-0.8.0-beta1-javadoc.jar'.
    -Missing Signature: '/org/apache/kafka/contrib_2.8.0/0.8.0-beta1/contrib_2.8.0-0.8.0-beta1.jar.asc' does not exist for 'contrib_2.8.0-0.8.0-beta1.jar'.
    -Missing Signature: '/org/apache/kafka/contrib_2.8.0/0.8.0-beta1/contrib_2.8.0-0.8.0-beta1-sources.jar.asc' does not exist for 'contrib_2.8.0-0.8.0-beta1-sources.jar'.
    -Missing Signature: '/org/apache/kafka/contrib_2.8.0/0.8.0-beta1/contrib_2.8.0-0.8.0-beta1.pom.asc' does not exist for 'contrib_2.8.0-0.8.0-beta1.pom'.
    -Missing Signature: '/org/apache/kafka/kafka_2.8.2/0.8.0-beta1/kafka_2.8.2-0.8.0-beta1-sources.jar.asc' does not exist for 'kafka_2.8.2-0.8.0-beta1-sources.jar'.
    -Missing Signature: '/org/apache/kafka/kafka_2.8.2/0.8.0-beta1/kafka_2.8.2-0.8.0-beta1-javadoc.jar.asc' does not exist for 'kafka_2.8.2-0.8.0-beta1-javadoc.jar'.
    -Missing Signature: '/org/apache/kafka/kafka_2.8.2/0.8.0-beta1/kafka_2.8.2-0.8.0-beta1.pom.asc' does not exist for 'kafka_2.8.2-0.8.0-beta1.pom'.
    -Missing Signature: '/org/apache/kafka/kafka_2.8.2/0.8.0-beta1/kafka_2.8.2-0.8.0-beta1.jar.asc' does not exist for 'kafka_2.8.2-0.8.0-beta1.jar'.
    -Missing Signature: '/org/apache/kafka/kafka-perf_2.8.0/0.8.0-beta1/kafka-perf_2.8.0-0.8.0-beta1-javadoc.jar.asc' does not exist for 'kafka-perf_2.8.0-0.8.0-beta1-javadoc.jar'.
    -Missing Signature: '/org/apache/kafka/kafka-perf_2.8.0/0.8.0-beta1/kafka-perf_2.8.0-0.8.0-beta1.pom.asc' does not exist for 'kafka-perf_2.8.0-0.8.0-beta1.pom'.
    -Missing Signature: '/org/apache/kafka/kafka-perf_2.8.0/0.8.0-beta1/kafka-perf_2.8.0-0.8.0-beta1-sources.jar.asc' does not exist for 'kafka-perf_2.8.0-0.8.0-beta1-sources.jar'.
    -Missing Signature: '/org/apache/kafka/kafka-perf_2.8.0/0.8.0-beta1/kafka-perf_2.8.0-0.8.0-beta1.jar.asc' does not exist for 'kafka-perf_2.8.0-0.8.0-beta1.jar'.
    -Missing Signature: '/org/apache/kafka/contrib_2.9.1/0.8.0-beta1/contrib_2.9.1-0.8.0-beta1.pom.asc' does not exist for 'contrib_2.9.1-0.8.0-beta1.pom'.
    -Missing Signature: '/org/apache/kafka/contrib_2.9.1/0.8.0-beta1/contrib_2.9.1-0.8.0-beta1-sources.jar.asc' does not exist for 'contrib_2.9.1-0.8.0-beta1-sources.jar'.
    -Missing Signature: '/org/apache/kafka/contrib_2.9.1/0.8.0-beta1/contrib_2.9.1-0.8.0-beta1-javadoc.jar.asc' does not exist for 'contrib_2.9.1-0.8.0-beta1-javadoc.jar'.
    -Missing Signature: '/org/apache/kafka/contrib_2.9.1/0.8.0-beta1/contrib_2.9.1-0.8.0-beta1.jar.asc' does not exist for 'contrib_2.9.1-0.8.0-beta1.jar'.
    -Missing Signature: '/org/apache/kafka/kafka-perf_2.9.1/0.8.0-beta1/kafka-perf_2.9.1-0.8.0-beta1.pom.asc' does not exist for 'kafka-perf_2.9.1-0.8.0-beta1.pom'.
    -Missing Signature: '/org/apache/kafka/kafka-perf_2.9.1/0.8.0-beta1/kafka-perf_2.9.1-0.8.0-beta1.jar.asc' does not exist for 'kafka-perf_2.9.1-0.8.0-beta1.jar'.
    -Missing Signature: '/org/apache/kafka/kafka-perf_2.9.1/0.8.0-beta1/kafka-perf_2.9.1-0.8.0-beta1-sources.jar.asc' does not exist for 'kafka-perf_2.9.1-0.8.0-beta1-sources.jar'.
    -Missing Signature: '/org/apache/kafka/kafka-perf_2.9.1/0.8.0-beta1/kafka-perf_2.9.1-0.8.0-beta1-javadoc.jar.asc' does not exist for 'kafka-perf_2.9.1-0.8.0-beta1-javadoc.jar'.
    -Missing Signature: '/org/apache/kafka/kafka_2.9.1/0.8.0-beta1/kafka_2.9.1-0.8.0-beta1-javadoc.jar.asc' does not exist for 'kafka_2.9.1-0.8.0-beta1-javadoc.jar'.
    -Missing Signature: '/org/apache/kafka/kafka_2.9.1/0.8.0-beta1/kafka_2.9.1-0.8.0-beta1.pom.asc' does not exist for 'kafka_2.9.1-0.8.0-beta1.pom'.
    -Missing Signature: '/org/apache/kafka/kafka_2.9.1/0.8.0-beta1/kafka_2.9.1-0.8.0-beta1-sources.jar.asc' does not exist for 'kafka_2.9.1-0.8.0-beta1-sources.jar'.
    -Missing Signature: '/org/apache/kafka/kafka_2.9.1/0.8.0-beta1/kafka_2.9.1-0.8.0-beta1.jar.asc' does not exist for 'kafka_2.9.1-0.8.0-beta1.jar'.
    -Missing Signature: '/org/apache/kafka/contrib_2.9.2/0.8.0-beta1/contrib_2.9.2-0.8.0-beta1-sources.jar.asc' does not exist for 'contrib_2.9.2-0.8.0-beta1-sources.jar'.
    -Missing Signature: '/org/apache/kafka/contrib_2.9.2/0.8.0-beta1/contrib_2.9.2-0.8.0-beta1-javadoc.jar.asc' does not exist for 'contrib_2.9.2-0.8.0-beta1-javadoc.jar'.
    -Missing Signature: '/org/apache/kafka/contrib_2.9.2/0.8.0-beta1/contrib_2.9.2-0.8.0-beta1.jar.asc' does not exist for 'contrib_2.9.2-0.8.0-beta1.jar'.
    -Missing Signature: '/org/apache/kafka/contrib_2.9.2/0.8.0-beta1/contrib_2.9.2-0.8.0-beta1.pom.asc' does not exist for 'contrib_2.9.2-0.8.0-beta1.pom'.
 
OK	

looking at http://www.scala-sbt.org/0.12.3/docs/Community/Using-Sonatype.html we need the sbt signing plugin like

This file specifies the plugins for your project. If you intend to sign the artefacts, you'll need to include @jsuereth's xsbt-gpg-plugin:

resolvers += Resolver.url(""sbt-plugin-releases"", /* no new line */
  new URL(""http://scalasbt.artifactoryonline.com/scalasbt/sbt-plugin-releases"")) /* no new line */
  (Resolver.ivyStylePatterns)

addSbtPlugin(""com.jsuereth"" % ""xsbt-gpg-plugin"" % ""0.5"")
 
 ",,charmalloc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/13 13:04;charmalloc;KAFKA-963.patch;https://issues.apache.org/jira/secure/attachment/12591123/KAFKA-963.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,336739,,,Sun Jul 07 13:04:46 UTC 2013,,,,,,,,,,"0|i1m313:",337062,,,,,,,,,,,,,,,,,,,,"07/Jul/13 13:01;charmalloc;ho hum

[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn] 	::          UNRESOLVED DEPENDENCIES         ::
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn] 	:: com.jsuereth#xsbt-gpg-plugin;0.5: not found
[warn] 	::::::::::::::::::::::::::::::::::::::::::::::
[warn] 
[warn] 	Note: Some unresolved dependencies have extra attributes.  Check that these dependencies exist with the requested attributes.
[warn] 		com.jsuereth:xsbt-gpg-plugin:0.5 (sbtVersion=0.12, scalaVersion=2.9.2)
[warn] 
sbt.ResolveException: unresolved dependency: com.jsuereth#xsbt-gpg-plugin;0.5: not found
	at sbt.IvyActions$.sbt$IvyActions$$resolve(IvyActions.scala:214)
	at sbt.IvyActions$$anonfun$update$1.apply(IvyActions.scala:122)
	at sbt.IvyActions$$anonfun$update$1.apply(IvyActions.scala:121)
;;;","07/Jul/13 13:04;charmalloc;this worked

[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1-javadoc.jar
[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1.jar
[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1.pom.asc
[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1.pom
[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1-sources.jar.asc
[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1-javadoc.jar.asc
[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1-sources.jar
[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1.jar.asc
[info] 	published kafka-perf_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka-perf_2.8.0/0.8.0-beta1/kafka-perf_2.8.0-0.8.0-beta1-javadoc.jar
[info] 	published kafka-perf_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka-perf_2.8.0/0.8.0-beta1/kafka-perf_2.8.0-0.8.0-beta1-sources.jar.asc
[info] 	published kafka-perf_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka-perf_2.8.0/0.8.0-beta1/kafka-perf_2.8.0-0.8.0-beta1.jar.asc
[info] 	published kafka-perf_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka-perf_2.8.0/0.8.0-beta1/kafka-perf_2.8.0-0.8.0-beta1.pom.asc
[info] 	published kafka-perf_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka-perf_2.8.0/0.8.0-beta1/kafka-perf_2.8.0-0.8.0-beta1.pom
[info] 	published kafka-perf_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka-perf_2.8.0/0.8.0-beta1/kafka-perf_2.8.0-0.8.0-beta1.jar
[info] 	published kafka-perf_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka-perf_2.8.0/0.8.0-beta1/kafka-perf_2.8.0-0.8.0-beta1-sources.jar
[info] 	published kafka-perf_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka-perf_2.8.0/0.8.0-beta1/kafka-perf_2.8.0-0.8.0-beta1-javadoc.jar.asc
[info] 	published contrib_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/contrib_2.8.0/0.8.0-beta1/contrib_2.8.0-0.8.0-beta1.jar.asc
[info] 	published contrib_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/contrib_2.8.0/0.8.0-beta1/contrib_2.8.0-0.8.0-beta1-sources.jar.asc
[info] 	published contrib_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/contrib_2.8.0/0.8.0-beta1/contrib_2.8.0-0.8.0-beta1-sources.jar
[info] 	published contrib_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/contrib_2.8.0/0.8.0-beta1/contrib_2.8.0-0.8.0-beta1.jar
[info] 	published contrib_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/contrib_2.8.0/0.8.0-beta1/contrib_2.8.0-0.8.0-beta1-javadoc.jar.asc
[info] 	published contrib_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/contrib_2.8.0/0.8.0-beta1/contrib_2.8.0-0.8.0-beta1.pom.asc
[info] 	published contrib_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/contrib_2.8.0/0.8.0-beta1/contrib_2.8.0-0.8.0-beta1.pom
[info] 	published contrib_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/contrib_2.8.0/0.8.0-beta1/contrib_2.8.0-0.8.0-beta1-javadoc.jar
[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1-javadoc.jar
[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1.jar
[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1.pom.asc
[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1.pom
[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1-sources.jar.asc
[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1-javadoc.jar.asc
[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1-sources.jar
[info] 	published kafka_2.8.0 to https://repository.apache.org/service/local/staging/deploy/maven2/org/apache/kafka/kafka_2.8.0/0.8.0-beta1/kafka_2.8.0-0.8.0-beta1.jar.asc
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultEventHandler can send more produce requests than necesary,KAFKA-959,12656061,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,junrao,junrao,03/Jul/13 16:17,25/Jul/13 17:08,14/Jul/23 05:39,25/Jul/13 17:08,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"In DefaultEventHandler, for a batch of messages, it picks a random partition per message (when there is no key specified). This means that it can send up to P produce requests where P is the number of partitions in a topic. A better way is probably to pick a single random partition for the whole batch of messages. This will reduce the number of produce requests.",,guozhang,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/13 23:40;guozhang;KAFKA-959.v1.patch;https://issues.apache.org/jira/secure/attachment/12594060/KAFKA-959.v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,336336,,,Thu Jul 25 17:08:00 UTC 2013,,,,,,,,,,"0|i1m0jr:",336660,,,,,,,,,,,,,,,,,,,,"09/Jul/13 00:54;guozhang;Proposed code change:

1. Add a Map<Topic, PartitionIndex> to DefaultEventHandller.

2. At the start of handle(events: Seq[KeyedMessage[K,V]]) function, clear the map.

3. In getPartition function, if key == null, check if there is already an entry in the map with the given topic:

3.1. If not, choose one partition from the availablePartitions, and add that partition to the map, return this partition index.

3.2. If yes, check if its leaderBrokerIdOpt is defined; if yes, return this partition index directly, otherwise throw an exception.;;;","24/Jul/13 23:40;guozhang;Passed unit tests;;;","25/Jul/13 17:08;junrao;Thanks for the patch. +1 Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Please compile list of key metrics on the broker and client side and put it on a wiki,KAFKA-958,12656030,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jjkoshy,vkeylis,vkeylis,03/Jul/13 14:47,24/Aug/17 14:57,14/Jul/23 05:39,24/Aug/17 14:57,0.8.0,,,,,,,,,,website,,,0,,,,Please compile list of important metrics that need to be monitored by companies  to insure healthy operation of the kafka service,,grayaii,omkreddy,otis,vkeylis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,336305,,,Thu Aug 24 14:57:35 UTC 2017,,,,,,,,,,"0|i1m0cv:",336629,,,,,,,,,,,,,,,,,,,,"24/Aug/17 14:57;omkreddy;Key metrics are listed on monitoring section of  Kafka documentation page;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MirrorMaker needs to preserve the key in the source cluster,KAFKA-957,12655965,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,junrao,junrao,03/Jul/13 04:51,24/Jul/13 18:44,14/Jul/23 05:39,24/Jul/13 18:44,0.8.0,,,,,,,,,,core,,,0,,,,"Currently, MirrorMaker only propagates the message to the target cluster, but not the associated key.",,guozhang,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/13 02:22;guozhang;KAFKA-957.v1.patch;https://issues.apache.org/jira/secure/attachment/12591954/KAFKA-957.v1.patch","17/Jul/13 00:41;guozhang;KAFKA-957.v2.patch;https://issues.apache.org/jira/secure/attachment/12592690/KAFKA-957.v2.patch","17/Jul/13 00:23;guozhang;KAFKA-957.v2.patch;https://issues.apache.org/jira/secure/attachment/12592686/KAFKA-957.v2.patch","17/Jul/13 02:01;guozhang;KAFKA-957.v3.patch;https://issues.apache.org/jira/secure/attachment/12592705/KAFKA-957.v3.patch","24/Jul/13 17:21;guozhang;KAFKA-957.v4.patch;https://issues.apache.org/jira/secure/attachment/12593981/KAFKA-957.v4.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,336240,,,Wed Jul 24 18:44:20 UTC 2013,,,,,,,,,,"0|i1lzyn:",336564,,,,,,,,,,,,,,,,,,,,"09/Jul/13 00:14;guozhang;The proposed design is the following:

1. Keep the ProducerDataChannel for non-keyed messages.

2. For each message in the consumers' stream, check if its key is null:

2.1. If yes, put it directly to the ProducerDataChannel.

2.2. If not, select the producer based on the key's hash value, and call the send function of that producer directly (would possibly block).

Patch will be available once the system tests are passed.;;;","12/Jul/13 02:22;guozhang;4. Add a ByteArrayPartitioner since Array[Byte].hashCode will result in different values for objects even with the same content. Enforce MirrorMaker to use this ByteArray Partitioner

Passed MirrorMaker testcase 5001;;;","17/Jul/13 00:23;guozhang;5. Use java.util.Arrays.hashCode to determine which producer to send a keyed message

6. Check if the partitioner class is specified instead of always trying to override the partitioner.

Passed testcase 5007 referring to KAFKA-976.;;;","17/Jul/13 01:06;jjkoshy;Thanks for incorporating 5 and 6. Couple additional comments:
- For the two match statements you have it is probably sufficient and
  clearer to just use if (key == null) .... and if (props.contains(..))
- I'm not so sure if the trace is required but it could be useful. Would
  prefer the following format: ""Sending message with key <key>"" - no need to
  show the payload. Also, may want to use java.util.Arrays.toString on the
  byte array.
- Per Jay's offline comments, hashCode in general is a bit unsafe to ""rely"".
  For e.g., it could be a non-uniform distribution or the underlying
  function could change. That said, your usage is safe. Still, it should be
  straightforward to do a custom hash function that we can rely on for
  consistency.
;;;","17/Jul/13 02:01;guozhang;Thanks for the comment.

For the first comment, since I needed to make a var for creating the config object if I use if () instead of match, I did not incorporate that. For the second match I have changed it to ""if"".;;;","24/Jul/13 17:21;guozhang;7. Use Utils.abs to make sure the result of the hashCode function would be always non-negative.;;;","24/Jul/13 18:44;jjkoshy;+1;;;","24/Jul/13 18:44;jjkoshy;Committed to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
High-level consumer fails to check topic metadata response for errors,KAFKA-956,12654682,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,smeder,smeder,25/Jun/13 08:50,20/Sep/13 04:25,14/Jul/23 05:39,20/Sep/13 04:24,0.8.0,,,0.8.0,,,,,,,consumer,,,0,,,,"In our environment we noticed that consumers would sometimes hang when started too close to starting the Kafka server. I tracked this down and it seems to be related to some code in rebalance (ZookeeperConsumerConnector.scala). In particular the following code seems problematic:

      val topicsMetadata = ClientUtils.fetchTopicMetadata(myTopicThreadIdsMap.keySet,
                                                          brokers,
                                                          config.clientId,
                                                          config.socketTimeoutMs,
                                                          correlationId.getAndIncrement).topicsMetadata
      val partitionsPerTopicMap = new mutable.HashMap[String, Seq[Int]]
      topicsMetadata.foreach(m => {
        val topic = m.topic
        val partitions = m.partitionsMetadata.map(m1 => m1.partitionId)
        partitionsPerTopicMap.put(topic, partitions)
      })

The response is never checked for error, so may not actually contain any partition info! Rebalance goes its merry way, but doesn't know about any partitions so never assigns them...
",,craigwblake,junrao,nehanarkhede,smeder,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/13 08:58;smeder;consumer_metadata_fetch.patch;https://issues.apache.org/jira/secure/attachment/12589573/consumer_metadata_fetch.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,334959,,,Fri Sep 20 04:24:27 UTC 2013,,,,,,,,,,"0|i1ls2f:",335283,,,,,,,,,,,,,,,,,,,,"25/Jun/13 08:58;smeder;Patch to throw an exception if no partition info is available.;;;","26/Jun/13 05:13;junrao;Thanks for the patch. A couple of comments:

1. When there is an error in metadata response, we shouldn't throw an exception. A consumer may be consuming multiple topics and we should let topics with no error in metadata response proceeds. So, instead, we could add the topic and an empty sequence to partitionsPerTopicMap. The rest of the logic in rebalance seems to be able to handle the case when there is no partition in a topic.

2. In the following statement, we should probably log that the string is for a metadata response.
debug(m.string);;;","26/Jun/13 06:16;smeder;Regarding 1. - how does the consumer recover from having no partition information for a topic? That seems to me the crux of the problem, which is why I was throwing an exception.;;;","26/Jun/13 15:30;junrao;We register a ZK watcher on the subscribed topics. If a topic is created subsequently, a ZK watcher will be fired and will we call the rebalance logic to pick up the new partitions.;;;","26/Jun/13 15:35;smeder;Right, but the topic already exists at the time of failure, so nothing is going to trigger. The error I am seeing in this case indicates that no leaders exist across any of the partitions.;;;","26/Jun/13 18:43;smeder;Basically the scenario is (roughly):

1. Start consumer(s)
2. Start Kafka server with pre-existing topic

Consumer notices server is up, but gets error response on on topic metadata with error code mapping to exception LeaderNotAvailable exception (this is the error on the topic metadata response, not individual partition metadata items).;;;","27/Jun/13 05:17;junrao;Got it. The weird part is that if a topic exists, why would the metadata request return an empty list of partitions (with the LeaderNotAvailableCode)? One case that I can think of is that the metadata may not have been propagated to all brokers yet. Since metadata propagation is quick in general, if we retry rebalance (which will happen with your patch), chances are that we will pick up the updated metadata. However, there is an issue. If a topic doesn't exist (and auto topic creation is turned off), throwing a KafkaException when there is no partition will prevent the consumer from starting. Instead, we should let rebalance proceed. When the topic is created in the future,  a ZK watcher will be fired and a rebalance will be triggered. I am not sure what's the best way to deal with both cases. Need to think about this a bit more.;;;","27/Jun/13 06:01;smeder;Seems like we need to allow for rebalancing only a subset of topics and keep trying on the rest?;;;","01/Jul/13 04:31;junrao;One possible solution is to let the consumer read the partition data from ZK directly. This way, if a consumer finds out that a topic doesn't exist, a ZK watcher is guaranteed to be triggered when the topic is created later. The only problem is that if there are many topics, reading them one at a time from ZK can be slow. ZK 3.4.x has the multi api support and we do plan to upgrade to that version. Perhaps we can revisit this issue at that point?;;;","01/Jul/13 13:15;smeder;I guess we can carry my patch locally for now (we don't use more than one topic per consumer right now), but it doesn't seem great to have a corner case that can basically deadlock the consumer. I did look into whether it would be possible to scope rebalancing to just a subset of the topics for the consumer, but it looks like that would require quite a bit of detangling of the listener from the zookeeper consumer class. Not something I would put in 0.8 right now, but is that something your intern would tackle as part of the offset work? Seems worthwhile...

;;;","01/Jul/13 15:51;junrao;This problem seems to only occur if a consumer is started at the same time when the topic is created. Do you have lots of dynamically created topics? A workaround could be just creating those topics before the consumers are started.;;;","01/Jul/13 16:45;smeder;I don't think it has anything to do with topic creation, for us the problem occurs with existing topics as well. It occurs when the consumer is started too close to starting the Kafka server...;;;","28/Aug/13 05:21;nehanarkhede;The root cause of this issue is the fact that a broker, that is not really ready to serve requests, ends up serving requests and misleading clients. When a broker starts up, it expects to receive an UpdateMetadata request from the controller. Until this happens, the broker should explicitly return a BrokerNotReady error code. Upon receiving this error code, the client should try to send the request to another broker. In the specific example of rebalance, the consumer will get BrokerNotReady error code and will try fetching metadata from all the brokers at least once before giving up. A similar problem exists on the producer side. If you rolling bounce a Kafka cluster when several thousands of producer clients are connected to the cluster, and auto creation of topics is turned on, it creates a storm of topic metadata requests turning into create topic requests to the brokers. The brokers spend a lot of time trying to create topics since they don't yet know that the topic exists.

You could argue that a broker that is not ready should not accept connections and probably not even start the socket server until it is ready to serve requests. But currently since the broker uses the same socket server to communicate with the controller, this is not an easy fix to put in 0.8;;;","13/Sep/13 16:12;junrao;This can probably be fixed as part of kafka-1030.;;;","19/Sep/13 23:21;nehanarkhede;[~smeder] KAFKA-1030 is now checked in and the problem reported in this JIRA should be fixed. Can you confirm that and close this JIRA?;;;","20/Sep/13 04:24;smeder;I can confirm that this issue was fixed by KAFKA-1030;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After a leader change, messages sent with ack=0 are lost",KAFKA-955,12654664,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,guozhang,jbrosenberg,jbrosenberg,25/Jun/13 05:24,03/Nov/16 07:12,14/Jul/23 05:39,28/Aug/13 17:17,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"If the leader changes for a partition, and a producer is sending messages with ack=0, then messages will be lost, since the producer has no active way of knowing that the leader has changed, until it's next metadata refresh update.

The broker receiving the message, which is no longer the leader, logs a message like this:

Produce request with correlation id 7136261 from client  on partition [mytopic,0] failed due to Leader not local for partition [mytopic,0] on broker 508818741

This is exacerbated by the controlled shutdown mechanism, which forces an immediate leader change.

A possible solution to this would be for a broker which receives a message, for a topic that it is no longer the leader for (and if the ack level is 0), then the broker could just silently forward the message over to the current leader.

",,Bill Zhang,diederik,guozhang,jbrosenberg,jbrosenberg@gmail.com,jkreps,junrao,mazhar.shaikh.in,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/13 22:17;guozhang;KAFKA-955-followup.v1.patch;https://issues.apache.org/jira/secure/attachment/12603129/KAFKA-955-followup.v1.patch","26/Jul/13 00:55;guozhang;KAFKA-955.v1.patch;https://issues.apache.org/jira/secure/attachment/12594303/KAFKA-955.v1.patch","25/Jul/13 23:51;guozhang;KAFKA-955.v1.patch;https://issues.apache.org/jira/secure/attachment/12594296/KAFKA-955.v1.patch","31/Jul/13 17:23;guozhang;KAFKA-955.v2.patch;https://issues.apache.org/jira/secure/attachment/12595217/KAFKA-955.v2.patch","02/Aug/13 22:35;guozhang;KAFKA-955.v3.patch;https://issues.apache.org/jira/secure/attachment/12595686/KAFKA-955.v3.patch","22/Aug/13 17:58;guozhang;KAFKA-955.v4.patch;https://issues.apache.org/jira/secure/attachment/12599462/KAFKA-955.v4.patch","23/Aug/13 16:59;guozhang;KAFKA-955.v5.patch;https://issues.apache.org/jira/secure/attachment/12599655/KAFKA-955.v5.patch","23/Aug/13 21:50;guozhang;KAFKA-955.v6.patch;https://issues.apache.org/jira/secure/attachment/12599718/KAFKA-955.v6.patch","26/Aug/13 23:28;guozhang;KAFKA-955.v7.patch;https://issues.apache.org/jira/secure/attachment/12600049/KAFKA-955.v7.patch",,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,334941,,,Thu Nov 03 07:12:34 UTC 2016,,,,,,,,,,"0|i1lryf:",335265,,,,,,,,,,,,,,,,,,,,"29/Jun/13 13:14;jbrosenberg;Here's a related scenario, that I see after a rolling restart of my brokers (using ack=0).

looking back at my logs, I'm wondering if a producer will reuse the same socket to send data to the same broker, for multiple topics (I'm guessing yes).  In which case, it looks like I'm seeing this scenario:

1. producer1 is happily sending messages for topicX and topicY to serverA (serverA is the leader for both topics, only 1 partition for each topic for simplicity).
2. serverA is restarted, and in the process, serverB becomes the new leader for both topicX and topicY.
3. producer1 decides to send a new message to topicX to serverA.
3a. this results in an exception (""Connection reset by peer"").  producer1's connection to serverA is invalidated.
3b. producer1 makes a new metadata request for topicX, and learns that serverB is now the leader for topicX.
3c. producer1 resends the message to topicX, on serverB.
4. producer1 decides to send a new message to topicY to serverA.
4a. producer1 notes that it's socket to serverA is invalid, so it creates a new connection to serverA.
4b. producer1 successfully sends it's message to serverA (without realizing that serverA is no longer the leader for topicY).
4c. serverA logs to it's console:  
2013-06-23 08:28:46,770  WARN [kafka-request-handler-2] server.KafkaApis - [KafkaApi-508818741] Produce request with correlation id 7136261 from client  on partition [mytopic,0] failed due to Leader not local for partition [mytopic,0] on broker 508818741
5. producer1 continues to send messages for topicY to serverA, and serverA continues to log the same messages.
6. 10 minutes later, producer1 decides to update it's metadata for topicY, and learns that serverB is now the leader for topidY.
7. the warning messages finally stop in the console for serverA.

I am pretty sure this scenario, or one very close to it, is what I'm seeing in my logs, after doing a rolling restart, with controlled shutdown.

I think this scenario makes the issue more severe than just a problem with controlled restart and ack=0.

;;;","01/Jul/13 04:20;junrao;It seems there are various ways that can cause this to happen. (a) In the above scenario, after the leaders fail over, topicX causes new sockets to be established. Then topicY uses the newly established socket without realizing that the leader for topic Y has changed. (b) When we fetch the metadata for a topic, we fetch the metadata for all partitions. Let's say that we never get to send any data to a particular partition. The socket for this partition is not established since  SyncProducer make socket connections lazily on first send. Then the leader for the partition changes. Finally, the producer sends a message to that partition. Now a socket is established to the wrong leader without the producer realizing it.

In general, if we hit any error for produce requests with ack=0, currently the producer won't notice it. For example, if the broker hits a MessageTooLargeException or if the broker hits any other unexpected exceptions. In those cases, forwarding the requests will not help. Also, forwarding requests will complicate the logic in the broker since we have to figure out the broker's host and port, and potentially cache the socket connection to other brokers.

An alternative solution is to simply close the socket connection when we hit any error for produce requests with ack=0. This way, the producer will realize the error on next send.
;;;","25/Jul/13 23:50;guozhang;Following the close-socket approach, I propose the following change:

1. Add a closeSocket: Boolean field in Response class.

2. In KafkaApi.handleProducerRequest, if requireAck == 0 check if numPartitionsInError != 0. If yes set closeSocket to true in the returning response.

3. SocketServer.Processor.processNewResponses, if curr.responseSend == null, check if closeSocket == true. If yes, log the closing socket info and close the key.

;;;","26/Jul/13 00:55;guozhang;Add one case for ack=0 in testSendWithDeadBroker, passed.;;;","31/Jul/13 15:13;junrao;Thanks for the patch. Some comments:

1. SocketServer: We should call updateRequestMetrics even when we close the socket. Otherwise, total time will be broken for that request.

2. ProducerTest: Let's add a new unit test instead of piggybacking on the existing one. What we can do is to create a sync producer and send a produce request with ack=0 that will introduce an error (e.g., a message larger than max size). After that, we can verified that the underlying socket is closed.

3. KafkaApi: In the debug logging, why not log the whole producer request? ;;;","31/Jul/13 17:23;guozhang;Thanks for the comments Jun.

1,2,3. Done.
;;;","02/Aug/13 17:20;junrao;Thanks for patch v2. Some more comments.

20. testSendWithAckZeroDeadBroker(): I am not sure if the unit test does what you want. First of all, setup() will always start brokers for each test unless you explicitly shut them down. So, in this test, the brokers are not dead. Second, the test doesn't really test that the socket is closed after error. I suggest that we add a new test in SyncProducerTest. We send a request with ack=0 with a large message. After that, we can try to send a new request again and we should hit a socket I/O exception. We may have to wait for some time between the two requests.;;;","02/Aug/13 18:13;guozhang;Sorry for the name misleading, I did not shut down the broker but instead send a large message to it to trigger the MessageSizeTooLargeException. The name of the test should be testSendTooLargeMessageWithAckZero.

I will use SyncProducer instead of Producer in this test, and send a normal message to the broker after this message, and expecting it to fail due to socket IO exception.;;;","02/Aug/13 22:35;guozhang;Add the testMessageSizeTooLargeWithAckZero to syncProducerTest, which:

1. First send a large message that will cause the MessageSizeTooLarge exception, and hence close the socket. But this message will be silently dropped and lost.

2. Then send another large message, but just to make sure its size exceeds the buffer size so the socket buffer will be flushed immediately; this send should fail since the socket has been closed.;;;","21/Aug/13 16:57;edenhill;
If the producer is sending messages through the same broker for other topic+partitions that did not have a leader change they will also be affected by the close of the socket, resulting in lost messages.

It would be better if the broker would notify all connected clients of broker changes (leader change, broker add/delete, topic add/delete)
by sending an unsolicited MetadataResponse message (with corrid 0) (or by some other mean).

This would propogate topology changes in a faster and less intrusive way.;;;","21/Aug/13 20:13;guozhang;Hello Magnus,

1. Under Ack=0, the producer does not expect any responses for produce request, and it does not listen to any possible connections from the producer either. So actively sending MetadataResponse would not work: producers are only expecting MetadataResponse when they send MetadataRequest.

2. When we close the socket, producer would be notified and try to refresh their Metadata and retry. Since by default each produce request will be retried multiple times before it is got dropped, the current approach would not cause lost messages.;;;","21/Aug/13 20:33;edenhill;Hi Guozhang,

I understand that you might not want to introduce a new message semantic at this point of the 0.8 beta, but it wont get easier after the release.

My proposal is a change of the protocol definition to allow unsolicited metadata response messages to be sent from the broker, this would of course require changes in most clients, but a very small one for those that are not interested in keeping their leader cache up to date.

Consider a producer forwarding >100kmsgs/s for a number of topics to a broker that suddenly drops the connection because one of those topics changed leader, the producer message queue will quickly build up and might start dropping messages (for topics that didnt loose their leader) due to local queue thresholds or very slowly recover if the current rate of messages is close to the maximum thruput.


In my mind closing the socket because one top+par changed leader is a very intrusive way to signal an event for sub-set of the communication, and it should instead be fixed properly with an unsoliticed metadata response message.

The unsolicited metadata response message is useful for other scenarios aswell, new brokers and topics being added, for instance.

My two cents on the topic, thank you.;;;","22/Aug/13 17:32;junrao;Magnus, thanks for your comment. What you suggested is interesting and could be a more effective way of communicating between the producer and the broker. It does require that the producer be able to receive requests initiated at the broker. We do plan to make the producer side processing selector based for efficiency reason. However, this will be a post 0.8 item. We could consider your suggestion then. Regarding your concern about dropped messages, my take is the following. If a client chooses not to receive an ack, it probably means that losing a few batch of messages is not that important. If a client does care about data loss, it can choose ack with 1 or -1. The throughout will be less. However, there are other ways to improve the throughput (e.g., using a larger batch size and/or more instances of producers).

Guozhang, patch v3 looks good to me overall. A few more comments:

30. SyncProducerTest.testMessagesizeTooLargeWithAckZero(): You hardcoded the sleep to 500ms. Could you change it to the waitUntil style wait such that the test can finish early if the conditions have been met?

31. KafkaApi.handleProducerRequest(): The logging should probably be at debug level since this doesn't indicate an error at the broker. It's really an error for the client.



;;;","22/Aug/13 17:58;guozhang;Thanks for the comments Jun.

30. Done.
31. After a second thought I realized that we do not need to sleep since the second message size is large enough to cause the socket buffer to flush immediately, and by then the socket close should have been triggered by the server. This has been verified in the unit test.

Made some minor changes on comments and rebased on 0.8;;;","23/Aug/13 16:15;nehanarkhede;Thanks for the patches, Guozhang. I reviewed patch v4 and here are some comments -

KafkaApis and SocketServer
1.1 One way to allow the socket server to close the channel is to just mark the request's key cancelled in the Response object. This way when the socket server is handling the response, it will throw a CancelledKeyException and we close the key in this case. One advantage of this approach is we can avoid introducing the close socket flag, just to handle this case. To make sure the request metrics are always updated, we can move curr.request.updateRequestMetrics to the first statement in the (curr.responseSend == null) block.
 
1.2 I think the below warn message can be improved -
Sending the close socket signal due to error handling produce request [%s] with Ack=0

Let's include the client id, correlation id and list of topics and partitions that this request had. This is probably more useful than printing the entire produce request as is, since that attempts to print things like ByteBufferMessageSet and is unreadable.;;;","23/Aug/13 16:59;guozhang;Thanks for the comments Neha.

1.1. Great point. The only concern is that now KafkaApis need to know that requestKey is actually java.nio.channels.SelectionKey. But I think this is fine.

1.2. Done.;;;","23/Aug/13 21:50;guozhang;After talking around with people I now proposed an approach similar to v4 but generalized with a responseCode instead of just a close socket flag. And on SocketServer the processor would act based on the code instead of checking if the responseSend is null or not.

Also change aliveBrokers in KafkaApis from var to val since it is not overwritten in lifetime.;;;","24/Aug/13 15:23;nehanarkhede;I like the way the responseCode is generalized. Patch v6 looks good, few minor comments before checkin -

1. Remove unused variable allBrokers from KafkaApis
2. This comment needs to be changed according to the new response code logic - 
// a null response send object indicates

Maybe we should wait for review from [~jkreps] since he has most context on the socket server.;;;","26/Aug/13 18:32;jkreps;Great fix. A few minor comments, mostly stylistic.

RequestChannel.scala:
1. This usage exposes a bit much:
   requestChannel.sendResponse(new RequestChannel.Response(request.processor, request, null, RequestChannel.CloseSocket))
I think it might be nicer to have this instead:
   requestChannel.close(request.processor, request)
and
   requestChannel.noResponse(req.processor, request)
Implementation would be the same, it just would just be a little more clear for the user and the response codes can be private.

Likewise in the response object I should be able to 

2. These are a little confusing:
val SendResponse: Short = 0
val NoResponse: Short = 1
val CloseSocket: Short = 9
Why is it 0, 1, and 9?

What is the relationship between these and ErrorMapping? It should be clear from reading.

Is there a reason we can't use a case class
  case class ResponseAction
  case object SendAction extends ResponseAction
  case object NoOpAction extends ResponseAction
  case object CloseConnectionAction extends ResponseAction

Then to use it
 
response.action match {
  case SendAction => do send
  case NoOpAction => read more
  case CloseConnectionAction => something
}

This seems clearer to me and I don't think it is significantly more expensive.

Can we also standardize the usage so that we no longer have the user EITHER give null or NoResponse? It should be one or the other.

3. This logging ""Cancelling the request key to notify socket server close the connection due to error handling produce request "" is not informative to the user. What does it mean to cancel a key? What broke? What should they do? I also think this should be info unless we want the server admin to take some action (I don't think so, right? This is a normal occurance).

SocketServer.scala
4. The comment ""a null response send object"" is retained but we are no longer using null to indicate this we are using RequestChannel.NoResponse. I think this comment is actually a little verbose given that we now have a nicely named response action.

ProducerTest.scala:
5. org.scalatest.TestFailedException: Is there a reason you are giving the full path here instead of importing it

Question on testing, what is the message loss rate with acks=0 under moderate load if we do something like a controlled shutdown with other replicas available?;;;","26/Aug/13 23:28;guozhang;Thanks for the comments, Neha, Jay.

Neha:

1. Done.
2. Incorporated with Jay's comments.

Jay:

1. Done.
2. Done. I ended up using trait and objects, and let requestChannel.close and requestChannel.noOperation (I changed the name from noResponse here since it matches the noOpAction better) create new responses themselves.
3. Done.
4. Done.
5. Done.

Regarding your question, the message loss is depending on the producer throughput and queue size. Since the first message will always be silently dropped, and once the producer noticed the socket closure, it will stop producing and refresh new metadata, and if during this time the producer queue is full then it will drop more messages. So the answer would be the range between [1, produce-throughput / time-taken-to-refresh-metadata - queue-size].;;;","28/Aug/13 17:06;jkreps;+1 Gorgeous.;;;","28/Aug/13 17:14;nehanarkhede;This is great. +1. One improvement on logging -

        info((""Send the close connection response due to error handling produce request "" +
          ""[clientId = %s, correlationId = %s, topicAndPartition = %s] with Ack=0"")
          .format(produceRequest.clientId, produceRequest.correlationId, produceRequest.topicPartitionMessageSizeMap.mkString(""["","","",""]"")))

Here we only want to print the topic and partition, so it seems that we should be printing the keys of the map, not the entire map ?
produceRequest.topicPartitionMessageSizeMap.keySet.mkString("","")

I can make this change on checkin.;;;","28/Aug/13 17:17;nehanarkhede;Committed patch v7 to 0.8 after making the logging fix described above;;;","03/Sep/13 15:14;junrao;Thanks for patch v7. A couple of more comments.

70. There is a long standing bug in ProducerRequest.handleError(). If ack=0, we shouldn't send a response when the broker hits an unexpected error. We should either close the socket connection or send no response. Not sure which one is better.

71. A minor issue. The following comment in RequestChannel is a bit confusing. It sounds like that it needs to read more data from network to complete this request, but it is not.
  /** No operation to take for the request, need to read more over the network */
  def noOperation(processor: Int, request: RequestChannel.Request) {
;;;","13/Sep/13 22:17;guozhang;Created reviewboard https://reviews.apache.org/r/14140/
;;;","13/Sep/13 23:18;junrao;Thanks for the followup patch. +1 and committed to 0.8.;;;","25/Aug/16 14:37;mazhar.shaikh.in;Hi All,

affected version & fixed version is same, just want to know if this fix is available in ""0.8.2""

I'm facing similar issue in ""0.8.2"".

Thanks.;;;","03/Nov/16 07:12;Bill Zhang;I am using Flume with Kafka Channel & facing below issues. 

Kafka Version: kafka_2.9.1-0.8.2.0
Flume Version: apache-flume-1.6.0

It seems was resolved from below :
Step 1: copy zookeeper Jar file to Flume classpath
Step 2: a1.channels.c1.kafka.producer.type = async

Note:
i didn't change default value of request.required.acks. It seems works, it is still in testing...


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Issue 1:
02 Nov 2016 22:20:06,201 WARN  [ConsumerFetcherThread-flume_tbox-topic_SATL2036-1478087994030-54387da2-0-2] (kafka.utils.Logging$class.warn:83)  - Reconnect due to socket error: null
02 Nov 2016 22:20:06,203 INFO  [ConsumerFetcherThread-flume_tbox-topic_SATL2036-1478087994030-54387da2-0-2] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherThread-flume_tbox-topic_SATL2036-1478087994030-54387da2-0-2], Stopped 
02 Nov 2016 22:20:06,203 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherThread-flume_tbox-topic_SATL2036-1478087994030-54387da2-0-2], Shutdown completed
02 Nov 2016 22:20:06,203 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherThread-flume_tbox-topic_SATL2036-1478087994030-54387da2-0-1], Shutting down
02 Nov 2016 22:20:06,204 WARN  [ConsumerFetcherThread-flume_tbox-topic_SATL2036-1478087994030-54387da2-0-1] (kafka.utils.Logging$class.warn:83)  - Reconnect due to socket error: null
02 Nov 2016 22:20:06,204 INFO  [ConsumerFetcherThread-flume_tbox-topic_SATL2036-1478087994030-54387da2-0-1] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherThread-flume_tbox-topic_SATL2036-1478087994030-54387da2-0-1], Stopped 
02 Nov 2016 22:20:06,204 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherThread-flume_tbox-topic_SATL2036-1478087994030-54387da2-0-1], Shutdown completed
02 Nov 2016 22:20:06,205 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherManager-1478087994042] All connections stopped
02 Nov 2016 22:20:06,207 INFO  [ZkClient-EventThread-58-SATL2036:2181,SATL2037:2181,SATL2038:2181/kafka] (org.I0Itec.zkclient.ZkEventThread.run:82)  - Terminate ZkClient event thread.
02 Nov 2016 22:20:06,212 WARN  [PollableSourceRunner-KafkaSource-r1] (kafka.utils.Logging$class.warn:89)  - Failed to send producer request with correlation id 34198503 to broker 1 with data for partitions [channel-tbox-parsed-topic,3]
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:202)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:511)
	at java.nio.channels.SocketChannel.write(SocketChannel.java:502)
	at kafka.network.BoundedByteBufferSend.writeTo(BoundedByteBufferSend.scala:56)
	at kafka.network.Send$class.writeCompletely(Transmission.scala:75)
	at kafka.network.BoundedByteBufferSend.writeCompletely(BoundedByteBufferSend.scala:26)
	at kafka.network.BlockingChannel.send(BlockingChannel.scala:92)
	at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:72)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:71)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SyncProducer.scala:102)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:102)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:102)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply$mcV$sp(SyncProducer.scala:101)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply(SyncProducer.scala:101)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply(SyncProducer.scala:101)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:100)
	at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$send(DefaultEventHandler.scala:255)
	at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$2.apply(DefaultEventHandler.scala:106)
	at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$2.apply(DefaultEventHandler.scala:100)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:100)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:72)
	at kafka.producer.Producer.send(Producer.scala:76)
	at kafka.javaapi.producer.Producer.send(Producer.scala:42)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doCommit(KafkaChannel.java:357)
	at org.apache.flume.channel.BasicTransactionSemantics.commit(BasicTransactionSemantics.java:151)
	at org.apache.flume.channel.ChannelProcessor.processEventBatch(ChannelProcessor.java:192)
	at org.apache.flume.source.kafka.KafkaSource.process(KafkaSource.java:130)
	at org.apache.flume.source.PollableSourceRunner$PollingRunner.run(PollableSourceRunner.java:139)
	at java.lang.Thread.run(Thread.java:745)
02 Nov 2016 22:20:06,214 INFO  [PollableSourceRunner-KafkaSource-r1] (kafka.utils.Logging$class.info:68)  - Back off for 100 ms before retrying send. Remaining retries = 3
02 Nov 2016 22:20:06,214 WARN  [PollableSourceRunner-KafkaSource-r1] (org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doCommit:363)  - Sending events to Kafka failed
java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:76)
	at kafka.producer.Producer.send(Producer.scala:76)
	at kafka.javaapi.producer.Producer.send(Producer.scala:42)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doCommit(KafkaChannel.java:357)
	at org.apache.flume.channel.BasicTransactionSemantics.commit(BasicTransactionSemantics.java:151)
	at org.apache.flume.channel.ChannelProcessor.processEventBatch(ChannelProcessor.java:192)
	at org.apache.flume.source.kafka.KafkaSource.process(KafkaSource.java:130)
	at org.apache.flume.source.PollableSourceRunner$PollingRunner.run(PollableSourceRunner.java:139)
	at java.lang.Thread.run(Thread.java:745)
02 Nov 2016 22:20:06,215 ERROR [PollableSourceRunner-KafkaSource-r1] (org.apache.flume.source.kafka.KafkaSource.process:153)  - KafkaSource EXCEPTION, {}
org.apache.flume.ChannelException: Unable to put batch on required channel: org.apache.flume.channel.kafka.KafkaChannel{name: c1}
	at org.apache.flume.channel.ChannelProcessor.processEventBatch(ChannelProcessor.java:200)
	at org.apache.flume.source.kafka.KafkaSource.process(KafkaSource.java:130)
	at org.apache.flume.source.PollableSourceRunner$PollingRunner.run(PollableSourceRunner.java:139)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.flume.ChannelException: Commit failed as send to Kafka failed
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doCommit(KafkaChannel.java:364)
	at org.apache.flume.channel.BasicTransactionSemantics.commit(BasicTransactionSemantics.java:151)
	at org.apache.flume.channel.ChannelProcessor.processEventBatch(ChannelProcessor.java:192)
	... 3 more
Caused by: java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:76)
	at kafka.producer.Producer.send(Producer.scala:76)
	at kafka.javaapi.producer.Producer.send(Producer.scala:42)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doCommit(KafkaChannel.java:357)
	... 5 more
02 Nov 2016 22:20:06,233 INFO  [agent-shutdown-hook] (org.apache.zookeeper.ZooKeeper.close:684)  - Session: 0x2581dc726ab01ad closed
02 Nov 2016 22:20:06,233 INFO  [lifecycleSupervisor-1-1-EventThread] (org.apache.zookeeper.ClientCnxn$EventThread.run:512)  - EventThread shut down
02 Nov 2016 22:20:06,239 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [flume_tbox-topic_SATL2036-1478087994030-54387da2], ZKConsumerConnector shut down completed
02 Nov 2016 22:20:06,239 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:150)  - Component type: SOURCE, name: r1 stopped
02 Nov 2016 22:20:06,239 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:156)  - Shutdown Metric for type: SOURCE, name: r1. source.start.time == 1478087994119
02 Nov 2016 22:20:06,239 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:162)  - Shutdown Metric for type: SOURCE, name: r1. source.stop.time == 1478096406239
02 Nov 2016 22:20:06,239 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: SOURCE, name: r1. source.kafka.commit.time == 555
02 Nov 2016 22:20:06,239 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: SOURCE, name: r1. source.kafka.event.get.time == 1409513
02 Nov 2016 22:20:06,239 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: SOURCE, name: r1. src.append-batch.accepted == 0
02 Nov 2016 22:20:06,240 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: SOURCE, name: r1. src.append-batch.received == 0
02 Nov 2016 22:20:06,240 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: SOURCE, name: r1. src.append.accepted == 0
02 Nov 2016 22:20:06,240 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: SOURCE, name: r1. src.append.received == 0
02 Nov 2016 22:20:06,240 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: SOURCE, name: r1. src.events.accepted == 343
02 Nov 2016 22:20:06,240 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: SOURCE, name: r1. src.events.received == 343
02 Nov 2016 22:20:06,240 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: SOURCE, name: r1. src.open-connection.count == 0
02 Nov 2016 22:20:06,240 INFO  [agent-shutdown-hook] (org.apache.flume.source.kafka.KafkaSource.stop:237)  - Kafka Source r1 stopped. Metrics: SOURCE:r1{src.events.accepted=343, src.open-connection.count=0, src.append.received=0, source.kafka.event.get.time=1409513, src.append-batch.received=0, src.append-batch.accepted=0, src.append.accepted=0, src.events.received=343, source.kafka.commit.time=555}
02 Nov 2016 22:20:06,240 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [flume-channel-tbox-topic_SATL2036-1478087992871-1a60fb21], ZKConsumerConnector shutting down
02 Nov 2016 22:20:06,241 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherManager-1478087993018] Stopping leader finder thread
02 Nov 2016 22:20:06,241 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [flume-channel-tbox-topic_SATL2036-1478087992871-1a60fb21-leader-finder-thread], Shutting down
02 Nov 2016 22:20:06,241 INFO  [flume-channel-tbox-topic_SATL2036-1478087992871-1a60fb21-leader-finder-thread] (kafka.utils.Logging$class.info:68)  - [flume-channel-tbox-topic_SATL2036-1478087992871-1a60fb21-leader-finder-thread], Stopped 
02 Nov 2016 22:20:06,241 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [flume-channel-tbox-topic_SATL2036-1478087992871-1a60fb21-leader-finder-thread], Shutdown completed
02 Nov 2016 22:20:06,241 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherManager-1478087993018] Stopping all fetchers
02 Nov 2016 22:20:06,241 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherThread-flume-channel-tbox-topic_SATL2036-1478087992871-1a60fb21-0-1], Shutting down
02 Nov 2016 22:20:06,242 WARN  [ConsumerFetcherThread-flume-channel-tbox-topic_SATL2036-1478087992871-1a60fb21-0-1] (kafka.utils.Logging$class.warn:83)  - Reconnect due to socket error: null
02 Nov 2016 22:20:06,242 INFO  [ConsumerFetcherThread-flume-channel-tbox-topic_SATL2036-1478087992871-1a60fb21-0-1] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherThread-flume-channel-tbox-topic_SATL2036-1478087992871-1a60fb21-0-1], Stopped 
02 Nov 2016 22:20:06,242 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherThread-flume-channel-tbox-topic_SATL2036-1478087992871-1a60fb21-0-1], Shutdown completed
02 Nov 2016 22:20:06,242 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherManager-1478087993018] All connections stopped
02 Nov 2016 22:20:06,243 INFO  [ZkClient-EventThread-42-SATL2036:2181,SATL2037:2181,SATL2038:2181/kafka] (org.I0Itec.zkclient.ZkEventThread.run:82)  - Terminate ZkClient event thread.
02 Nov 2016 22:20:06,244 INFO  [agent-shutdown-hook] (org.apache.zookeeper.ZooKeeper.close:684)  - Session: 0x356eb2d4b833fab closed
02 Nov 2016 22:20:06,244 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [flume-channel-tbox-topic_SATL2036-1478087992871-1a60fb21], ZKConsumerConnector shut down completed
02 Nov 2016 22:20:06,244 INFO  [SinkRunner-PollingRunner-FailoverSinkProcessor-EventThread] (org.apache.zookeeper.ClientCnxn$EventThread.run:512)  - EventThread shut down
02 Nov 2016 22:20:06,246 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - Shutting down producer
02 Nov 2016 22:20:06,247 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - Closing all sync producers
02 Nov 2016 22:20:06,256 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - Disconnecting from 10.25.20.36:9092
02 Nov 2016 22:20:06,256 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:150)  - Component type: CHANNEL, name: c1 stopped
02 Nov 2016 22:20:06,256 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:156)  - Shutdown Metric for type: CHANNEL, name: c1. channel.start.time == 1478087992836
02 Nov 2016 22:20:06,256 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:162)  - Shutdown Metric for type: CHANNEL, name: c1. channel.stop.time == 1478096406256
02 Nov 2016 22:20:06,257 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: CHANNEL, name: c1. channel.capacity == 0
02 Nov 2016 22:20:06,257 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: CHANNEL, name: c1. channel.current.size == 0
02 Nov 2016 22:20:06,257 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: CHANNEL, name: c1. channel.event.put.attempt == 0
02 Nov 2016 22:20:06,257 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: CHANNEL, name: c1. channel.event.put.success == 343
02 Nov 2016 22:20:06,257 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: CHANNEL, name: c1. channel.event.take.attempt == 0
02 Nov 2016 22:20:06,257 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: CHANNEL, name: c1. channel.event.take.success == 342
02 Nov 2016 22:20:06,257 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: CHANNEL, name: c1. channel.kafka.commit.time == 201
02 Nov 2016 22:20:06,258 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: CHANNEL, name: c1. channel.kafka.event.get.time == 531
02 Nov 2016 22:20:06,258 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: CHANNEL, name: c1. channel.kafka.event.send.time == 1789
02 Nov 2016 22:20:06,258 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:178)  - Shutdown Metric for type: CHANNEL, name: c1. channel.rollback.count == 0
02 Nov 2016 22:20:06,258 INFO  [agent-shutdown-hook] (org.apache.flume.channel.kafka.KafkaChannel.stop:123)  - Kafka channel c1 stopped. Metrics: CHANNEL:c1{channel.event.put.attempt=0, channel.event.put.success=343, channel.kafka.event.get.time=531, channel.current.size=0, channel.event.take.attempt=0, channel.event.take.success=342, channel.kafka.event.send.time=1789, channel.capacity=0, channel.kafka.commit.time=201, channel.rollback.count=0}
02 Nov 2016 22:20:06,264 WARN  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake:332)  - Error while getting events from Kafka
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2088)
	at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
	at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:65)
	at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:33)
	at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:66)
	at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:58)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:306)
	at org.apache.flume.channel.BasicTransactionSemantics.take(BasicTransactionSemantics.java:113)
	at org.apache.flume.channel.BasicChannelSemantics.take(BasicChannelSemantics.java:95)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:97)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
02 Nov 2016 22:20:06,274 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.kafka.KafkaSink.process:139)  - Failed to publish events
org.apache.flume.ChannelException: Error while getting events from Kafka
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:333)
	at org.apache.flume.channel.BasicTransactionSemantics.take(BasicTransactionSemantics.java:113)
	at org.apache.flume.channel.BasicChannelSemantics.take(BasicChannelSemantics.java:95)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:97)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2088)
	at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
	at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:65)
	at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:33)
	at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:66)
	at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:58)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:306)
	... 6 more
02 Nov 2016 22:20:06,275 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:160)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: Failed to publish events
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:150)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:68)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.flume.ChannelException: Error while getting events from Kafka
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:333)
	at org.apache.flume.channel.BasicTransactionSemantics.take(BasicTransactionSemantics.java:113)
	at org.apache.flume.channel.BasicChannelSemantics.take(BasicChannelSemantics.java:95)
	at org.apache.flume.sink.kafka.KafkaSink.process(KafkaSink.java:97)
	... 3 more
Caused by: java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2014)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2088)
	at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:467)
	at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:65)
	at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:33)
	at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:66)
	at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:58)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:306)
	... 6 more
02 Nov 2016 22:20:06,794 INFO  [flume_tbox-topic_SATL2036-1478087994030-54387da2_watcher_executor] (kafka.utils.Logging$class.info:68)  - [flume_tbox-topic_SATL2036-1478087994030-54387da2], stopping watcher executor thread for consumer flume_tbox-topic_SATL2036-1478087994030-54387da2
02 Nov 2016 22:20:06,816 INFO  [flume-channel-tbox-topic_SATL2036-1478087992871-1a60fb21_watcher_executor] (kafka.utils.Logging$class.info:68)  - [flume-channel-tbox-topic_SATL2036-1478087992871-1a60fb21], stopping watcher executor thread for consumer flume-channel-tbox-topic_SATL2036-1478087992871-1a60fb21
02 Nov 2016 22:20:06,887 WARN  [SinkRunner-PollingRunner-FailoverSinkProcessor] (org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake:332)  - Error while getting events from Kafka
java.util.NoSuchElementException
	at kafka.utils.IteratorTemplate.next(IteratorTemplate.scala:39)
	at kafka.consumer.ConsumerIterator.next(ConsumerIterator.scala:46)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:310)
	at org.apache.flume.channel.BasicTransactionSemantics.take(BasicTransactionSemantics.java:113)
	at org.apache.flume.channel.BasicChannelSemantics.take(BasicChannelSemantics.java:95)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:374)
	at org.apache.flume.sink.FailoverSinkProcessor.process(FailoverSinkProcessor.java:182)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
02 Nov 2016 22:20:06,888 ERROR [SinkRunner-PollingRunner-FailoverSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:459)  - process failed
org.apache.flume.ChannelException: Error while getting events from Kafka
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:333)
	at org.apache.flume.channel.BasicTransactionSemantics.take(BasicTransactionSemantics.java:113)
	at org.apache.flume.channel.BasicChannelSemantics.take(BasicChannelSemantics.java:95)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:374)
	at org.apache.flume.sink.FailoverSinkProcessor.process(FailoverSinkProcessor.java:182)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException
	at kafka.utils.IteratorTemplate.next(IteratorTemplate.scala:39)
	at kafka.consumer.ConsumerIterator.next(ConsumerIterator.scala:46)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:310)
	... 6 more
02 Nov 2016 22:20:06,889 WARN  [SinkRunner-PollingRunner-FailoverSinkProcessor] (org.apache.flume.sink.FailoverSinkProcessor.process:185)  - Sink k1 failed and has been sent to failover list
org.apache.flume.EventDeliveryException: org.apache.flume.ChannelException: Error while getting events from Kafka
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:463)
	at org.apache.flume.sink.FailoverSinkProcessor.process(FailoverSinkProcessor.java:182)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.flume.ChannelException: Error while getting events from Kafka
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:333)
	at org.apache.flume.channel.BasicTransactionSemantics.take(BasicTransactionSemantics.java:113)
	at org.apache.flume.channel.BasicChannelSemantics.take(BasicChannelSemantics.java:95)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:374)
	... 3 more
Caused by: java.util.NoSuchElementException
	at kafka.utils.IteratorTemplate.next(IteratorTemplate.scala:39)
	at kafka.consumer.ConsumerIterator.next(ConsumerIterator.scala:46)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:310)
	... 6 more
02 Nov 2016 22:20:06,896 WARN  [SinkRunner-PollingRunner-FailoverSinkProcessor] (org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake:332)  - Error while getting events from Kafka
java.util.NoSuchElementException
	at kafka.utils.IteratorTemplate.next(IteratorTemplate.scala:39)
	at kafka.consumer.ConsumerIterator.next(ConsumerIterator.scala:46)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:310)
	at org.apache.flume.channel.BasicTransactionSemantics.take(BasicTransactionSemantics.java:113)
	at org.apache.flume.channel.BasicChannelSemantics.take(BasicChannelSemantics.java:95)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:374)
	at org.apache.flume.sink.FailoverSinkProcessor.process(FailoverSinkProcessor.java:182)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
02 Nov 2016 22:20:06,897 ERROR [SinkRunner-PollingRunner-FailoverSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:459)  - process failed
org.apache.flume.ChannelException: Error while getting events from Kafka
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:333)
	at org.apache.flume.channel.BasicTransactionSemantics.take(BasicTransactionSemantics.java:113)
	at org.apache.flume.channel.BasicChannelSemantics.take(BasicChannelSemantics.java:95)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:374)
	at org.apache.flume.sink.FailoverSinkProcessor.process(FailoverSinkProcessor.java:182)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException
	at kafka.utils.IteratorTemplate.next(IteratorTemplate.scala:39)
	at kafka.consumer.ConsumerIterator.next(ConsumerIterator.scala:46)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:310)
	... 6 more
02 Nov 2016 22:20:06,898 WARN  [SinkRunner-PollingRunner-FailoverSinkProcessor] (org.apache.flume.sink.FailoverSinkProcessor.process:185)  - Sink k2 failed and has been sent to failover list
org.apache.flume.EventDeliveryException: org.apache.flume.ChannelException: Error while getting events from Kafka
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:463)
	at org.apache.flume.sink.FailoverSinkProcessor.process(FailoverSinkProcessor.java:182)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.flume.ChannelException: Error while getting events from Kafka
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:333)
	at org.apache.flume.channel.BasicTransactionSemantics.take(BasicTransactionSemantics.java:113)
	at org.apache.flume.channel.BasicChannelSemantics.take(BasicChannelSemantics.java:95)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:374)
	... 3 more
Caused by: java.util.NoSuchElementException
	at kafka.utils.IteratorTemplate.next(IteratorTemplate.scala:39)
	at kafka.consumer.ConsumerIterator.next(ConsumerIterator.scala:46)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:310)
	... 6 more
02 Nov 2016 22:20:06,898 WARN  [SinkRunner-PollingRunner-FailoverSinkProcessor] (org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake:332)  - Error while getting events from Kafka
java.util.NoSuchElementException
	at kafka.utils.IteratorTemplate.next(IteratorTemplate.scala:39)
	at kafka.consumer.ConsumerIterator.next(ConsumerIterator.scala:46)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:310)
	at org.apache.flume.channel.BasicTransactionSemantics.take(BasicTransactionSemantics.java:113)
	at org.apache.flume.channel.BasicChannelSemantics.take(BasicChannelSemantics.java:95)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:374)
	at org.apache.flume.sink.FailoverSinkProcessor.process(FailoverSinkProcessor.java:182)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
02 Nov 2016 22:20:06,898 ERROR [SinkRunner-PollingRunner-FailoverSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:459)  - process failed
org.apache.flume.ChannelException: Error while getting events from Kafka
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:333)
	at org.apache.flume.channel.BasicTransactionSemantics.take(BasicTransactionSemantics.java:113)
	at org.apache.flume.channel.BasicChannelSemantics.take(BasicChannelSemantics.java:95)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:374)
	at org.apache.flume.sink.FailoverSinkProcessor.process(FailoverSinkProcessor.java:182)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.util.NoSuchElementException
	at kafka.utils.IteratorTemplate.next(IteratorTemplate.scala:39)
	at kafka.consumer.ConsumerIterator.next(ConsumerIterator.scala:46)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:310)
	... 6 more
02 Nov 2016 22:20:06,899 WARN  [SinkRunner-PollingRunner-FailoverSinkProcessor] (org.apache.flume.sink.FailoverSinkProcessor.process:185)  - Sink k3 failed and has been sent to failover list
org.apache.flume.EventDeliveryException: org.apache.flume.ChannelException: Error while getting events from Kafka
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:463)
	at org.apache.flume.sink.FailoverSinkProcessor.process(FailoverSinkProcessor.java:182)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.flume.ChannelException: Error while getting events from Kafka
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:333)
	at org.apache.flume.channel.BasicTransactionSemantics.take(BasicTransactionSemantics.java:113)
	at org.apache.flume.channel.BasicChannelSemantics.take(BasicChannelSemantics.java:95)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:374)
	... 3 more
Caused by: java.util.NoSuchElementException
	at kafka.utils.IteratorTemplate.next(IteratorTemplate.scala:39)
	at kafka.consumer.ConsumerIterator.next(ConsumerIterator.scala:46)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doTake(KafkaChannel.java:310)
	... 6 more
02 Nov 2016 22:20:06,899 ERROR [SinkRunner-PollingRunner-FailoverSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:160)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: All sinks failed to process, nothing left to failover to
	at org.apache.flume.sink.FailoverSinkProcessor.process(FailoverSinkProcessor.java:191)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:147)
	at java.lang.Thread.run(Thread.java:745)
02 Nov 2016 22:20:07,216 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [flume_tbox_parsed_SATL2036-1478088061094-7badc6c0], ZKConsumerConnector shutting down
02 Nov 2016 22:20:07,217 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherManager-1478088061100] Stopping leader finder thread
02 Nov 2016 22:20:07,217 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [flume_tbox_parsed_SATL2036-1478088061094-7badc6c0-leader-finder-thread], Shutting down
02 Nov 2016 22:20:07,218 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [flume_tbox_parsed_SATL2036-1478088061094-7badc6c0-leader-finder-thread], Shutdown completed
02 Nov 2016 22:20:07,218 INFO  [flume_tbox_parsed_SATL2036-1478088061094-7badc6c0-leader-finder-thread] (kafka.utils.Logging$class.info:68)  - [flume_tbox_parsed_SATL2036-1478088061094-7badc6c0-leader-finder-thread], Stopped 
02 Nov 2016 22:20:07,218 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherManager-1478088061100] Stopping all fetchers
02 Nov 2016 22:20:07,218 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherThread-flume_tbox_parsed_SATL2036-1478088061094-7badc6c0-0-2], Shutting down
02 Nov 2016 22:20:07,219 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherThread-flume_tbox_parsed_SATL2036-1478088061094-7badc6c0-0-2], Shutdown completed
02 Nov 2016 22:20:07,219 INFO  [ConsumerFetcherThread-flume_tbox_parsed_SATL2036-1478088061094-7badc6c0-0-2] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherThread-flume_tbox_parsed_SATL2036-1478088061094-7badc6c0-0-2], Stopped 
02 Nov 2016 22:20:07,220 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherThread-flume_tbox_parsed_SATL2036-1478088061094-7badc6c0-0-1], Shutting down
02 Nov 2016 22:20:07,220 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherThread-flume_tbox_parsed_SATL2036-1478088061094-7badc6c0-0-1], Shutdown completed
02 Nov 2016 22:20:07,220 INFO  [ConsumerFetcherThread-flume_tbox_parsed_SATL2036-1478088061094-7badc6c0-0-1] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherThread-flume_tbox_parsed_SATL2036-1478088061094-7badc6c0-0-1], Stopped 
02 Nov 2016 22:20:07,221 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [ConsumerFetcherManager-1478088061100] All connections stopped
02 Nov 2016 22:20:07,223 INFO  [ZkClient-EventThread-48-SATL2036:2181,SATL2037:2181,SATL2038:2181/kafka] (org.I0Itec.zkclient.ZkEventThread.run:82)  - Terminate ZkClient event thread.
02 Nov 2016 22:20:07,226 INFO  [flume_tbox_parsed_SATL2036-1478088061094-7badc6c0_watcher_executor] (kafka.utils.Logging$class.info:68)  - [flume_tbox_parsed_SATL2036-1478088061094-7badc6c0], stopping watcher executor thread for consumer flume_tbox_parsed_SATL2036-1478088061094-7badc6c0
02 Nov 2016 22:20:07,226 INFO  [agent-shutdown-hook] (org.apache.zookeeper.ZooKeeper.close:684)  - Session: 0x156eb2d4a70421e closed
02 Nov 2016 22:20:07,226 INFO  [lifecycleSupervisor-1-0-EventThread] (org.apache.zookeeper.ClientCnxn$EventThread.run:512)  - EventThread shut down
02 Nov 2016 22:20:07,227 INFO  [agent-shutdown-hook] (kafka.utils.Logging$class.info:68)  - [flume_tbox_parsed_SATL2036-1478088061094-7badc6c0], ZKConsumerConnector shut down completed



Issue 2:
03 Nov 2016 13:31:29,287 INFO  [PollableSourceRunner-KafkaSource-r1] (kafka.utils.Logging$class.info:68)  - Back off for 100 ms before retrying send. Remaining retries = 1
03 Nov 2016 13:31:29,388 INFO  [PollableSourceRunner-KafkaSource-r1] (kafka.utils.Logging$class.info:68)  - Fetching metadata from broker id:1,host:SATL2037,port:9092 with correlation id 2307612 for 1 topic(s) Set(channel-tbox-topic)
03 Nov 2016 13:31:29,388 INFO  [PollableSourceRunner-KafkaSource-r1] (kafka.utils.Logging$class.info:68)  - Connected to SATL2037:9092 for producing
03 Nov 2016 13:31:29,389 INFO  [PollableSourceRunner-KafkaSource-r1] (kafka.utils.Logging$class.info:68)  - Disconnecting from SATL2037:9092
03 Nov 2016 13:31:29,443 INFO  [PollableSourceRunner-KafkaSource-r1] (kafka.utils.Logging$class.info:68)  - Connected to SATL2037:9092 for producing
03 Nov 2016 13:31:29,549 INFO  [PollableSourceRunner-KafkaSource-r1] (kafka.utils.Logging$class.info:68)  - Disconnecting from SATL2037:9092
03 Nov 2016 13:31:29,550 WARN  [PollableSourceRunner-KafkaSource-r1] (kafka.utils.Logging$class.warn:89)  - Failed to send producer request with correlation id 2308613 to broker 2 with data for partitions [channel-tbox-topic,4]
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.writev0(Native Method)
	at sun.nio.ch.SocketDispatcher.writev(SocketDispatcher.java:51)
	at sun.nio.ch.IOUtil.write(IOUtil.java:148)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:504)
	at java.nio.channels.SocketChannel.write(SocketChannel.java:502)
	at kafka.network.BoundedByteBufferSend.writeTo(BoundedByteBufferSend.scala:56)
	at kafka.network.Send$class.writeCompletely(Transmission.scala:75)
	at kafka.network.BoundedByteBufferSend.writeCompletely(BoundedByteBufferSend.scala:26)
	at kafka.network.BlockingChannel.send(BlockingChannel.scala:92)
	at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:72)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:71)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SyncProducer.scala:102)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:102)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:102)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply$mcV$sp(SyncProducer.scala:101)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply(SyncProducer.scala:101)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply(SyncProducer.scala:101)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:100)
	at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$send(DefaultEventHandler.scala:255)
	at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$2.apply(DefaultEventHandler.scala:106)
	at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$2.apply(DefaultEventHandler.scala:100)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:100)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:72)
	at kafka.producer.Producer.send(Producer.scala:76)
	at kafka.javaapi.producer.Producer.send(Producer.scala:42)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doCommit(KafkaChannel.java:357)
	at org.apache.flume.channel.BasicTransactionSemantics.commit(BasicTransactionSemantics.java:151)
	at org.apache.flume.channel.ChannelProcessor.processEventBatch(ChannelProcessor.java:192)
	at org.apache.flume.source.kafka.KafkaSource.process(KafkaSource.java:130)
	at org.apache.flume.source.PollableSourceRunner$PollingRunner.run(PollableSourceRunner.java:139)
	at java.lang.Thread.run(Thread.java:745)
03 Nov 2016 13:31:29,550 INFO  [PollableSourceRunner-KafkaSource-r1] (kafka.utils.Logging$class.info:68)  - Back off for 100 ms before retrying send. Remaining retries = 0
03 Nov 2016 13:31:29,651 INFO  [PollableSourceRunner-KafkaSource-r1] (kafka.utils.Logging$class.info:68)  - Fetching metadata from broker id:0,host:SATL2036,port:9092 with correlation id 2308614 for 1 topic(s) Set(channel-tbox-topic)
03 Nov 2016 13:31:29,651 INFO  [PollableSourceRunner-KafkaSource-r1] (kafka.utils.Logging$class.info:68)  - Connected to SATL2036:9092 for producing
03 Nov 2016 13:31:29,652 INFO  [PollableSourceRunner-KafkaSource-r1] (kafka.utils.Logging$class.info:68)  - Disconnecting from SATL2036:9092
03 Nov 2016 13:31:29,653 ERROR [PollableSourceRunner-KafkaSource-r1] (kafka.utils.Logging$class.error:97)  - Failed to send requests for topics channel-tbox-topic with correlation ids in [2304607,2308614]
03 Nov 2016 13:31:29,653 WARN  [PollableSourceRunner-KafkaSource-r1] (org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doCommit:363)  - Sending events to Kafka failed
kafka.common.FailedToSendMessageException: Failed to send messages after 3 tries.
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:90)
	at kafka.producer.Producer.send(Producer.scala:76)
	at kafka.javaapi.producer.Producer.send(Producer.scala:42)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doCommit(KafkaChannel.java:357)
	at org.apache.flume.channel.BasicTransactionSemantics.commit(BasicTransactionSemantics.java:151)
	at org.apache.flume.channel.ChannelProcessor.processEventBatch(ChannelProcessor.java:192)
	at org.apache.flume.source.kafka.KafkaSource.process(KafkaSource.java:130)
	at org.apache.flume.source.PollableSourceRunner$PollingRunner.run(PollableSourceRunner.java:139)
	at java.lang.Thread.run(Thread.java:745)
03 Nov 2016 13:31:29,653 ERROR [PollableSourceRunner-KafkaSource-r1] (org.apache.flume.source.kafka.KafkaSource.process:153)  - KafkaSource EXCEPTION, {}
org.apache.flume.ChannelException: Unable to put batch on required channel: org.apache.flume.channel.kafka.KafkaChannel{name: c1}
	at org.apache.flume.channel.ChannelProcessor.processEventBatch(ChannelProcessor.java:200)
	at org.apache.flume.source.kafka.KafkaSource.process(KafkaSource.java:130)
	at org.apache.flume.source.PollableSourceRunner$PollingRunner.run(PollableSourceRunner.java:139)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.flume.ChannelException: Commit failed as send to Kafka failed
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doCommit(KafkaChannel.java:364)
	at org.apache.flume.channel.BasicTransactionSemantics.commit(BasicTransactionSemantics.java:151)
	at org.apache.flume.channel.ChannelProcessor.processEventBatch(ChannelProcessor.java:192)
	... 3 more
Caused by: kafka.common.FailedToSendMessageException: Failed to send messages after 3 tries.
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:90)
	at kafka.producer.Producer.send(Producer.scala:76)
	at kafka.javaapi.producer.Producer.send(Producer.scala:42)
	at org.apache.flume.channel.kafka.KafkaChannel$KafkaTransaction.doCommit(KafkaChannel.java:357)
	... 5 more





;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tidy up README file for better general availability,KAFKA-954,12654470,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,charmalloc,charmalloc,24/Jun/13 11:28,08/Oct/13 20:05,14/Jul/23 05:39,08/Oct/13 20:04,0.8.0,,,0.8.0,,,,,,,,,,0,0.8.0-beta1,,,e.g. how to start server after building and all would be good too,,charmalloc,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/13 16:50;junrao;KAFKA-954.patch;https://issues.apache.org/jira/secure/attachment/12607380/KAFKA-954.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,334747,,,Tue Oct 08 20:04:53 UTC 2013,,,,,,,,,,"0|i1lqrz:",335073,,,,,,,,,,,,,,,,,,,,"08/Oct/13 16:50;junrao;Created reviewboard https://reviews.apache.org/r/14538/
;;;","08/Oct/13 20:04;junrao;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove release-zip from README we are not releasing with it,KAFKA-953,12654469,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,charmalloc,charmalloc,24/Jun/13 11:27,10/Sep/13 16:44,14/Jul/23 05:39,10/Sep/13 16:44,0.8.0,,,0.8.0,,,,,,,,,,0,0.8.0-beta1,,,,,charmalloc,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,334746,,,Tue Sep 10 16:44:13 UTC 2013,,,,,,,,,,"0|i1lqrr:",335072,,,,,,,,,,,,,,,,,,,,"10/Sep/13 16:44;junrao;We actually do support the release-zip target, as well as release-tar.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leader election rate may be reported on a non-controller,KAFKA-951,12654411,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,23/Jun/13 22:47,24/Jun/13 05:04,14/Jul/23 05:39,24/Jun/13 05:04,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"If a broker was a controller but is not a current controller, it will still update leader election mbean even though it doesn't really do any leader election.",,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/13 22:52;junrao;kafka-951.patch;https://issues.apache.org/jira/secure/attachment/12589348/kafka-951.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,334688,,,Mon Jun 24 05:04:30 UTC 2013,,,,,,,,,,"0|i1lqev:",335014,,,,,,,,,,,,,,,,,,,,"23/Jun/13 22:52;junrao;Attach a patch.;;;","23/Jun/13 22:55;swapnilghike;+1, thanks.;;;","24/Jun/13 05:04;junrao;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bytesSinceLastIndexEntry needs to be reset after log segment is truncated,KAFKA-950,12654080,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,21/Jun/13 04:48,24/Jun/13 05:04,14/Jul/23 05:39,24/Jun/13 05:04,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"bytesSinceLastIndexEntry needs to be reset after log segment is truncated. Otherwise, it's possible to add an index entry that points to the first message in a log segment.",,junrao,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/13 04:49;junrao;kafka-950.patch;https://issues.apache.org/jira/secure/attachment/12588998/kafka-950.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,334357,,,Mon Jun 24 05:04:08 UTC 2013,,,,,,,,,,"0|i1loef:",334683,,,,,,,,,,,,,,,,,,,,"21/Jun/13 04:50;junrao;Attach a patch.;;;","21/Jun/13 05:00;sriramsub;+1;;;","24/Jun/13 05:04;junrao;Thanks for the review. Committed for 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
isr-expiration-thread may block LeaderAndIsr request for a relatively long period ,KAFKA-947,12653748,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,19/Jun/13 16:42,20/Jun/13 20:23,14/Jul/23 05:39,20/Jun/13 20:23,0.8.1,,,0.8.0,,,,,,,core,,,0,,,,"If there are lots of partitions whose isr needs to be shrank, isr-expiration-thread will hold a long lock on leaderPartitionsLock, which will delay LeaderAndIsr requests.",,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/13 16:45;junrao;kafka-947.patch;https://issues.apache.org/jira/secure/attachment/12588635/kafka-947.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,334025,,,Thu Jun 20 20:23:41 UTC 2013,,,,,,,,,,"0|i1lmcn:",334351,,,,,,,,,,,,,,,,,,,,"19/Jun/13 16:46;junrao;Attach a patch.;;;","20/Jun/13 00:55;jjkoshy;+1 thanks for the patch!;;;","20/Jun/13 20:23;junrao;Thanks for the patch. Committed to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Hadoop Consumer fails when verifying message checksum,KAFKA-946,12653547,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,smeder,smeder,smeder,18/Jun/13 18:28,24/Sep/13 15:32,14/Jul/23 05:39,24/Sep/13 15:31,0.8.0,,,0.8.0,,,,,,,contrib,,,0,,,,"The code tries to verify the checksum, but fails because the data available isn't the same. In KafkaETLContext:

    protected boolean get(KafkaETLKey key, BytesWritable value) throws IOException {
	if (_messageIt != null && _messageIt.hasNext()) {
            MessageAndOffset messageAndOffset = _messageIt.next();

            ByteBuffer buf = messageAndOffset.message().payload();
            int origSize = buf.remaining();
            byte[] bytes = new byte[origSize];
          buf.get(bytes, buf.position(), origSize);
            value.set(bytes, 0, origSize);

            key.set(_index, _offset, messageAndOffset.message().checksum());

            _offset = messageAndOffset.nextOffset();  //increase offset                                                                                                                                  
            _count ++;  //increase count                                                                                                                                                                 

            return true;
        }
        else return false;
    }

Note that the message payload is used and the message checksum is included in the key. The in SimpleKafkaETLMapper:

    @Override
    public void map(KafkaETLKey key, BytesWritable val,
            OutputCollector<LongWritable, Text> collector,
            Reporter reporter) throws IOException {


	byte[] bytes = KafkaETLUtils.getBytes(val);

        //check the checksum of message                                                                                                                                                                  
        Message message = new Message(bytes);
        long checksum = key.getChecksum();
	if (checksum != message.checksum())
            throw new IOException (""Invalid message checksum ""
                                            + message.checksum() + "". Expected "" + key + ""."");

the Message object is initialized with the payload bytes and a new checksum is calculated. The problem is that the original message checksum also contains the key so checksum verification fails...
",,jkreps,junrao,smeder,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/13 04:42;smeder;hadoop_consumer_1.patch;https://issues.apache.org/jira/secure/attachment/12604734/hadoop_consumer_1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,333824,,,Tue Sep 24 15:31:54 UTC 2013,,,,,,,,,,"0|i1ll47:",334151,,,,,,,,,,,,,,,,,,,,"18/Jun/13 18:33;smeder;Attached a patch that simply passes the full message buffer instead of just the payload...;;;","03/Jul/13 00:10;jkreps;Ack, well that's broken.

Richard--is this a reasonable thing to do?
Jun--any concern with this going in 0.8?

A meta question is how to handle the poor state of maintenance of this Hadoop code. Started a discussion on that on the mailing list...;;;","11/Jul/13 23:41;jkreps;From Richard:

Sorry, It took a while to remember the context.
In a Kafka Message, the checksum is created on the whole message: header and payload included.

The contrib code passes only the Message payload to the mapper, and not the whole buffer. I believe the reason for this is that we wanted to pass just the message data (not any of the kafka special bits) for the mapper handle. The Message that is created in the SimpleKafkaETLMapper is then creating using the incorrect payload bytes. It can be argued that this is desirable. For instance, Mappers can decode the byte buffer directly into Avro without stripping away the header or dealing with kafka Messages at all.

Also, changing the KafakETLContext code could be affect a lot of users. This is definitely not a backwards compatible change. It can also be argued that the BytesWriteable only contains the payload code, and that checksum-ing of the message should've occurred well before the Mapper gets the message.

However, I think that Sam's fix still has merit. It would be good for the KafkaETLContext to pass the Message buffer instead of the payload and the RecordReader could strip away the kafka bits before giving the payload to the Mapper. Perhaps put in a config switch to either get just payload or the whole kafka message buffer?

Additional thoughts:
I assume there are plenty of users of this code. If there's anyone who uses the KafkaETLContext directly, they'll find the patch's changes to break their stuff. However, for those who are using KafkaETLContext through the KafkaETLRecordReader (as they should), then there is a way to make it backwards compatible.

The checksumming and payload stripping code could go into the RecordReader rather than the KafkaETLContext.

If the scope of these changes are too big, I'd just fix the SimpleKafkaETLMapper to not parse the payload bytes.;;;","23/Sep/13 16:41;junrao;Sorry for not looking at this earlier. I think that we can include the fix in 0.8 since it's simple enough. It doesn't apply to 0.8 though. Could you rebase?;;;","24/Sep/13 04:43;smeder;Rebased patch attached.;;;","24/Sep/13 15:31;junrao;Thanks for the rebased patch. +1 and committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the pom output from publish and publish-local is not accurate,KAFKA-944,12653347,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,charmalloc,charmalloc,charmalloc,17/Jun/13 23:30,20/Jun/13 03:00,14/Jul/23 05:39,20/Jun/13 03:00,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"the groupId in the publish-local and publish output is wrong

its

<groupId>org.apache</groupId>

and should be

<groupId>org.apache.kafka</groupId> 

per https://issues.apache.org/jira/browse/INFRA-6399

also, I think we should be adding this http://www.apache.org/dev/publishing-maven-artifacts.html#adjust-maven maybe some other things going through it over again",,charmalloc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/13 23:37;charmalloc;KAFKA-944.patch;https://issues.apache.org/jira/secure/attachment/12588722/KAFKA-944.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,333625,,,Thu Jun 20 03:00:06 UTC 2013,,,,,,,,,,"0|i1ljw7:",333953,,,,,,,,,,,,,,,,,,,,"17/Jun/13 23:41;charmalloc;Some sbt docs in publishing http://www.scala-sbt.org/0.12.3/docs/Detailed-Topics/Publishing.html;;;","19/Jun/13 23:37;charmalloc;so far in my research and testing these are the changes required

i tried to stage the artifacts to apache central but getting an auth error (even when navigating outside of SBT through a browser) so I will open a INFRA ticket for that part;;;","20/Jun/13 03:00;charmalloc;before publish local

[warn] 		::::::::::::::::::::::::::::::::::::::::::::::
[warn] 		::          UNRESOLVED DEPENDENCIES         ::
[warn] 		::::::::::::::::::::::::::::::::::::::::::::::
[warn] 		:: org.apache.kafka#kafka_2.9.1;0.8.0-beta1: not found
[warn] 		::::::::::::::::::::::::::::::::::::::::::::::

and after publish local :)

Joes-MacBook-Air:stub joestein$ ./sbt compile
[info] Set current project to default-63d5f2 (in build file:/opt/medialets/SymanticManager/scala/stub/)
[info] Updating {file:/opt/medialets/SymanticManager/scala/stub/}default-63d5f2...
[info] Done updating.
[success] Total time: 4 s, completed Jun 19, 2013 10:59:14 PM;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the version of the jar should be 0.8.0-beta1 not 0.8.0-SNAPSHOT,KAFKA-942,12653198,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,charmalloc,charmalloc,charmalloc,17/Jun/13 15:14,17/Jun/13 16:21,14/Jul/23 05:39,17/Jun/13 16:21,0.8.0,,,0.8.0,,,,,,,,,,0,,,,,,charmalloc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/13 15:47;charmalloc;KAFKA-941.patch;https://issues.apache.org/jira/secure/attachment/12588156/KAFKA-941.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,333521,,,2013-06-17 15:14:40.0,,,,,,,,,,"0|i1lj93:",333849,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Apache 2.0 license to missing code source files,KAFKA-941,12652712,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,charmalloc,charmalloc,charmalloc,13/Jun/13 17:56,13/Jun/13 20:14,14/Jul/23 05:39,13/Jun/13 20:14,,,,0.8.0,,,,,,,,,,0,,,,"Unapproved licenses:

  ./core/src/main/scala/kafka/admin/CheckReassignmentStatus.scala
  ./core/src/main/scala/kafka/api/ApiUtils.scala
  ./core/src/main/scala/kafka/api/UpdateMetadataRequest.scala
  ./core/src/main/scala/kafka/client/ClientUtils.scala
  ./core/src/main/scala/kafka/common/BrokerNotExistException.scala
  ./core/src/main/scala/kafka/consumer/ConsumerTopicStat.scala
  ./core/src/main/scala/kafka/consumer/package.html
  ./core/src/main/scala/kafka/log/LogSegment.scala
  ./core/src/main/scala/kafka/log/package.html
  ./core/src/main/scala/kafka/message/package.html
  ./core/src/main/scala/kafka/network/package.html
  ./core/src/main/scala/kafka/producer/async/AsyncProducerStats.scala
  ./core/src/main/scala/kafka/server/package.html
  ./core/src/main/scala/kafka/utils/CommandLineUtils.scala
  ./core/src/main/scala/kafka/utils/FileLock.scala
  ./core/src/main/scala/kafka/utils/Json.scala
  ./core/src/main/scala/kafka/utils/Topic.scala
  ./core/src/main/scala/kafka/utils/package.html
  ./core/src/test/scala/unit/kafka/log/LogSegmentTest.scala
",,charmalloc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/13 18:14;charmalloc;KAFKA-941.txt;https://issues.apache.org/jira/secure/attachment/12587674/KAFKA-941.txt","13/Jun/13 18:02;charmalloc;rat.kafka_0.8.0-beta1-candidate1.txt;https://issues.apache.org/jira/secure/attachment/12587671/rat.kafka_0.8.0-beta1-candidate1.txt",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,333036,,,Thu Jun 13 18:02:41 UTC 2013,,,,,,,,,,"0|i1lg9b:",333364,,,,,,,,,,,,,,,,,,,,"13/Jun/13 18:02;charmalloc;rat report on 0.8.0-beta1-candidate1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala match error in javaapi.Implicits,KAFKA-940,12652533,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jjkoshy,jjkoshy,jjkoshy,13/Jun/13 00:19,13/Jun/13 03:43,14/Jul/23 05:39,13/Jun/13 03:43,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"This would affect javaapi users who (correctly) test for null on API calls (e.g., if (partitionMetadata.leader == null))

Right now, we actually get a match error:
scala.MatchError: null
	at kafka.javaapi.Implicits$.optionToJavaRef(Implicits.scala:38)
	at kafka.javaapi.Implicits$.optionToJavaRef(Implicits.scala:40)
	at kafka.javaapi.PartitionMetadata.leader(TopicMetadata.scala:51)
<truncated>
",,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/13 00:21;jjkoshy;KAFKA-940-v1.patch;https://issues.apache.org/jira/secure/attachment/12587531/KAFKA-940-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,332857,,,Thu Jun 13 03:43:55 UTC 2013,,,,,,,,,,"0|i1lf5j:",333185,,,,,,,,,,,,,,,,,,,,"13/Jun/13 00:21;jjkoshy;Simple fix.;;;","13/Jun/13 03:43;junrao;Thanks for the patch. +1. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
./sbt publish-local fails due to invalid javac flags passed to javadoc,KAFKA-939,12652489,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,frankgrimes97,frankgrimes97,12/Jun/13 20:28,27/Sep/13 03:02,14/Jul/23 05:39,27/Sep/13 03:02,0.8.0,,,0.8.0,,,,,,,,,,0,,,,Fixed by applying suggestion found here: https://groups.google.com/forum/?fromgroups#!topic/simple-build-tool/I75AODwFlH0,,charmalloc,frankgrimes97,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,332813,,,Fri Sep 27 03:02:10 UTC 2013,,,,,,,,,,"0|i1levz:",333142,,,,,,,,,,,,,,,,,,,,"12/Jun/13 20:29;frankgrimes97;diff --git a/project/Build.scala b/project/Build.scala
index f177215..ecc74d4 100644
--- a/project/Build.scala
+++ b/project/Build.scala
@@ -34,7 +34,8 @@ object KafkaBuild extends Build {
     buildNumber := System.getProperty(""build.number"", """"),
     version <<= (buildNumber, version)  { (build, version)  => if (build == """") version else version + ""+"" + build},
     releaseName <<= (name, version, scalaVersion) {(name, version, scalaVersion) => name + ""_"" + scalaVersion + ""-"" + v
-    javacOptions ++= Seq(""-Xlint:unchecked"", ""-source"", ""1.5""),
+    javacOptions in compile ++= Seq(""-Xlint:unchecked"", ""-source"", ""1.5""),
+    javacOptions in doc ++= Seq(""-source"", ""1.5""),
     parallelExecution in Test := false, // Prevent tests from overrunning each other
     libraryDependencies ++= Seq(
       ""log4j""                 % ""log4j""        % ""1.2.15"",
@@ -57,7 +58,7 @@ object KafkaBuild extends Build {
   )
 
   val hadoopSettings = Seq(
-    javacOptions ++= Seq(""-Xlint:deprecation""),
+    javacOptions in compile ++= Seq(""-Xlint:deprecation""),
     libraryDependencies ++= Seq(
       ""org.apache.avro""      % ""avro""               % ""1.4.0"",
       ""org.apache.pig""       % ""pig""                % ""0.8.0"",;;;","27/Sep/13 02:43;charmalloc;reproduced

./sbt ""++2.9.2"" clean package publish-local

[error] (hadoop-consumer/compile:doc) javadoc returned nonzero exit code
[error] (hadoop-producer/compile:doc) javadoc returned nonzero exit code
[error] (java-examples/compile:doc) javadoc returned nonzero exit code
[error] Total time: 408 s, completed Sep 26, 2013 9:12:34 PM;;;","27/Sep/13 03:00;charmalloc;+1, patches works great, committing to branch;;;","27/Sep/13 03:02;charmalloc;committed to branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
High CPU usage when more or less idle,KAFKA-938,12652282,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,smeder,smeder,11/Jun/13 21:39,12/Jun/13 18:43,14/Jul/23 05:39,12/Jun/13 05:44,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"We've noticed Kafka using a lot of CPU in a pretty much idle environment and tracked it down to it's DelayedItem implementation. In particular, the time conversion for how much longer to wait:

  def getDelay(unit: TimeUnit): Long = {
    val elapsedMs = (SystemTime.milliseconds - createdMs)
    unit.convert(max(delayMs - elapsedMs, 0), unit)
  }

does not actually convert, so Kafka ends up treating a ms value like nanoseconds, e.g. waking up every 100 ns or so. The above code should really be:

  def getDelay(unit: TimeUnit): Long = {
    val elapsedMs = (SystemTime.milliseconds - createdMs)
    unit.convert(max(delayMs - elapsedMs, 0), TimeUnit.MILLISECONDS)
  }

I'll attach a patch.

",,jjkoshy,junrao,nidaley,smeder,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/13 21:41;smeder;timeunit.patch;https://issues.apache.org/jira/secure/attachment/12587325/timeunit.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,332606,,,Wed Jun 12 18:43:57 UTC 2013,,,,,,,,,,"0|i1ldlz:",332935,,,,,,,,,,,,,,,,,,,,"12/Jun/13 05:44;junrao;Wow, thanks for finding this and providing a patch. +1 and committed to 0.8.;;;","12/Jun/13 18:28;jjkoshy;Excellent catch Sam!

One comment: I think the DelayedItem class was intended to support arbitrary (non-millisecond) timunits but that was buggy in two ways:
(i)  The getDelay's 'unit' parameter shadowed the DelayedItem's 'unit' member
(ii) The delayMs val assumes that the delay is always in ms (which prevents DelayedItem from supporting arbitrary time units).
Also, I think we must have missed the bit of the DelayQueue documentation that says getDelay is called with TimeUnit.NANOSECONDS

I think we can tweak this a bit to make it support arbitrary timeunits - otherwise, the ""unit"" parameter of DelayedItem is of no use. I can attach a patch to make this clearer.;;;","12/Jun/13 18:43;jjkoshy;Actually, small correction: I overlooked that the delayMs val converts the given delay from its source unit to milliseconds. So the only caveat is that precision will be lost if the desired timeunit is nanos - which we don't really need so I don't think we need any further changes here. Thanks again!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsumerFetcherThread can deadlock,KAFKA-937,12651933,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,09/Jun/13 15:25,04/Sep/13 03:52,14/Jul/23 05:39,13/Jun/13 03:51,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"We have the following access pattern that can introduce a deadlock.

AbstractFetcherThread.processPartitionsWithError() ->
ConsumerFetcherThread.processPartitionsWithError() -> 
ConsumerFetcherManager.addPartitionsWithError() wait for lock ->
LeaderFinderThread holding lock while calling AbstractFetcherManager.shutdownIdleFetcherThreads() ->
AbstractFetcherManager calling fetcher.shutdown, which needs to wait until AbstractFetcherThread.processPartitionsWithError() completes.",,aozeritsky,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/13 15:45;junrao;kafka-937.patch;https://issues.apache.org/jira/secure/attachment/12586957/kafka-937.patch","25/Jun/13 04:26;junrao;kafka-937_ConsumerOffsetChecker.patch;https://issues.apache.org/jira/secure/attachment/12589544/kafka-937_ConsumerOffsetChecker.patch","24/Jun/13 03:44;junrao;kafka-937_delta.patch;https://issues.apache.org/jira/secure/attachment/12589364/kafka-937_delta.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,332257,,,Wed Sep 04 03:52:14 UTC 2013,,,,,,,,,,"0|i1lbgv:",332586,,,,,,,,,,,,,,,,,,,,"09/Jun/13 15:45;junrao;Attach a patch. The fix is to make sure that the fetcher thread never gets blocked, no matter what other threads like the LeaderFindThread does. Specifically, LeaderFinderThread no longer holds lock when calling addFetcher() or shudownIdleFetcherThreads(). This way ConsumerFetcherManager.addPartitionsWithError() never gets blocked, which in turn means that the ConsumerFetcherThread never gets blocked and can complete the shutdown if required.

Double-checked other paths and don't see any other potential deadlocks.

Also fixed another potential socket leak through SimpleConsumer. When we shutdown a fetcher, we first interrupt the fetcher thread and close the SimpleConsumer. However, after that, it is possible for the fetcher thread to make another fetch request on SimpleConsumer. This will establish the socket connection again. Add a fix in SimpleConsumer so that after it is closed, no new socket connections will be established and the fetch call will get a ClosedChannelException instead.;;;","13/Jun/13 00:12;jjkoshy;+1 on the patch.

Additionally, can you make this small (unrelated change) -  make the console consumer's autoCommitIntervalOpt default to ConsumerConfig.AutoCommitInterval ?

I think it is worth documenting the typical path of getting into the above deadlock:
- Assume at least two fetchers F1, F2
- One or more partitions on F1 go into error and leader finder thread L is notified
- L unblocks and proceeds to handle partitions without leader. It holds the ConsumerFetcherManager's lock at this point.
- All partitions on F2 go into error.
- F2's handlePartitionsWithError removes partitions from its fetcher's partitionMap. (At this point, F2 is by definition an idle fetcher thread.)
- L tries to shutdown idle fetcher threads - i.e., tries to shutdown F2.
- However, F2 at this point is trying to addPartitionsWithError which needs to acquire the ConsumerFetcherManager's lock (which is currently held by L).

It is relatively rare in the sense that it can happen only if all partitions on the fetcher are in error. This could happen for example if all the leaders for those partitions move or become unavailable. Another instance where this may be seen in practice is mirroring: we ran into it when running the mirror maker with a very large number of producers and ran out of file handles. Running out of file handles could easily lead to exceptions on most/all fetches and result in an error state for all partitions.
;;;","13/Jun/13 03:51;junrao;Thanks for the review. Committed to 0.8.;;;","24/Jun/13 03:44;junrao;Just realize a rare corner case issue. addFetcher() may call ConsumerFetcherThread.handleOffsetOutOfRange() and can get an exception from SimpleConsumer.earliestOrLatestOffset(). In this case, we shouldn't kill the leaderFinderThread unless it is shut down. Attach a patch.;;;","24/Jun/13 14:12;aozeritsky;That patch breaks kafka.tools.ConsumerOffsetChecker:

[2013-06-24 18:11:17,638] INFO Reconnect due to socket error:  (kafka.consumer.SimpleConsumer)
java.nio.channels.ClosedChannelException
        at kafka.network.BlockingChannel.send(BlockingChannel.scala:89)
        at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:72)
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:71)
        at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:125)
        at kafka.tools.ConsumerOffsetChecker$.kafka$tools$ConsumerOffsetChecker$$processPartition(ConsumerOffsetChecker.scala:72)
        at kafka.tools.ConsumerOffsetChecker$$anonfun$kafka$tools$ConsumerOffsetChecker$$processTopic$1.apply$mcVI$sp(ConsumerOffsetChecker.scala:90)
        at kafka.tools.ConsumerOffsetChecker$$anonfun$kafka$tools$ConsumerOffsetChecker$$processTopic$1.apply(ConsumerOffsetChecker.scala:90)
        at kafka.tools.ConsumerOffsetChecker$$anonfun$kafka$tools$ConsumerOffsetChecker$$processTopic$1.apply(ConsumerOffsetChecker.scala:90)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at kafka.tools.ConsumerOffsetChecker$.kafka$tools$ConsumerOffsetChecker$$processTopic(ConsumerOffsetChecker.scala:89)
        at kafka.tools.ConsumerOffsetChecker$$anonfun$main$3.apply(ConsumerOffsetChecker.scala:154)
        at kafka.tools.ConsumerOffsetChecker$$anonfun$main$3.apply(ConsumerOffsetChecker.scala:154)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at kafka.tools.ConsumerOffsetChecker$.main(ConsumerOffsetChecker.scala:153)
        at kafka.tools.ConsumerOffsetChecker.main(ConsumerOffsetChecker.scala);;;","24/Jun/13 15:45;junrao;Alexey,

This issue seems to be unrelated to this patch. The exception is thrown in SimpleConsumer and this patch doesn't touch SimpleConsumer. Could you describe how you get to this issue and how reproducible it is?;;;","24/Jun/13 21:35;aozeritsky;kafka.tools.ConsumerOffsetChecker uses SimpleConsumer for OffsetRequest

To reproduce just make git pull and run
bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --group group --zkconnect zk-servers --topic topic

The problem is in the following diff:

diff --git a/core/src/main/scala/kafka/consumer/SimpleConsumer.scala b/core/src/main/scala/kafka/consumer/SimpleConsumer.scala
index bdeee91..1c28328 100644
--- a/core/src/main/scala/kafka/consumer/SimpleConsumer.scala
+++ b/core/src/main/scala/kafka/consumer/SimpleConsumer.scala
@@ -37,6 +37,7 @@ class SimpleConsumer(val host: String,
   private val blockingChannel = new BlockingChannel(host, port, bufferSize, BlockingChannel.UseDefaultBufferSize, soTimeout)
   val brokerInfo = ""host_%s-port_%s"".format(host, port)
   private val fetchRequestAndResponseStats = FetchRequestAndResponseStatsRegistry.getFetchRequestAndResponseStats(clientId)
+  private var isClosed = false

   private def connect(): BlockingChannel = {
     close
@@ -58,7 +59,8 @@ class SimpleConsumer(val host: String,

   def close() {
     lock synchronized {
-        disconnect()
+      disconnect()
+      isClosed = true
     }
   }

@@ -123,7 +125,7 @@ class SimpleConsumer(val host: String,
   def getOffsetsBefore(request: OffsetRequest) = OffsetResponse.readFrom(sendRequest(request).buffer)

   private def getOrMakeConnection() {
-    if(!blockingChannel.isConnected) {
+    if(!isClosed && !blockingChannel.isConnected) {
       connect()
     }
   }


SimpleConsumer stops working after close (ConsumerOffsetChecker.scala, line 77);;;","24/Jun/13 21:41;aozeritsky;This patch touches SimpleConsumer.

proof:

https://git-wip-us.apache.org/repos/asf?p=kafka.git;a=blobdiff;f=core/src/main/scala/kafka/consumer/SimpleConsumer.scala;h=1c283280873eef597018f2f0a5ddfec942356c18;hp=bdeee9174a32a02209d769c18a0337ade0356e99;hb=5bd33c1517bb2e7734166dc3e787ac90a4ef8f86;hpb=640026467cf705fbcf6fd6bcada058b18a95bff5;;;","25/Jun/13 04:26;junrao;Thanks for reporting this. It is actually a real issue. However, the problem is not because of the change in SimpleConsumer, but in how ConsumerOffsetChecker uses SimpleConsumer. It should only close a SimpleConsumer after it's no longer needed. Could you try the attached patch?;;;","25/Jun/13 07:43;aozeritsky;This patch works, thanks.;;;","26/Jun/13 05:49;junrao;Alexey,

Thanks for the review. Committed the ConsumerOffsetChecker patch to 0.8.;;;","03/Sep/13 17:28;jjkoshy;The delta patch slipped through the cracks. We hit that issue recently - a network glitch led to the leader-finder-thread hitting an exception while adding fetchers and the thread quit:

{code}
leader-finder-thread], Error due to 
java.net.ConnectException: Connection timed out
        at sun.nio.ch.Net.connect(Native Method)
        at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:507)
        at kafka.network.BlockingChannel.connect(BlockingChannel.scala:57)
        at kafka.consumer.SimpleConsumer.connect(SimpleConsumer.scala:44)
        at kafka.consumer.SimpleConsumer.getOrMakeConnection(SimpleConsumer.scala:129)
        at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:69)
        at kafka.consumer.SimpleConsumer.getOffsetsBefore(SimpleConsumer.scala:125)
        at kafka.consumer.SimpleConsumer.earliestOrLatestOffset(SimpleConsumer.scala:144)
        at kafka.consumer.ConsumerFetcherThread.handleOffsetOutOfRange(ConsumerFetcherThread.scala:60)
        at kafka.server.AbstractFetcherThread.addPartition(AbstractFetcherThread.scala:180)
        at kafka.server.AbstractFetcherManager.addFetcher(AbstractFetcherManager.scala:80)
        at kafka.consumer.ConsumerFetcherManager$LeaderFinderThread$$anonfun$doWork$7.apply(ConsumerFetcherManager.scala:95)
        at kafka.consumer.ConsumerFetcherManager$LeaderFinderThread$$anonfun$doWork$7.apply(ConsumerFetcherManager.scala:92)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
        at kafka.consumer.ConsumerFetcherManager$LeaderFinderThread.doWork(ConsumerFetcherManager.scala:92)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)
{code}



+1 on kafka-937_delta with one minor comment: change the log to indicate that will attempt to look up the leader again and add fetchers - right now it just says ""failed to add"".;;;","04/Sep/13 03:52;junrao;Thanks for the review. Committed the delta patch to 0.8, after fixing the logging.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka Metrics Memory Leak ,KAFKA-936,12651436,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,nehanarkhede,senthilchittibabu,senthilchittibabu,06/Jun/13 20:31,02/Dec/15 05:36,14/Jul/23 05:39,28/Mar/15 16:00,0.8.0,,,,,,,,,,consumer,,,0,,,,"I am using kafka_2.8.0-0.8.0-SNAPSHOT version. I am running into OutOfMemoryError in PermGen Space. I have set the -XX:MaxPermSize=512m, but I still get the same error. I used profiler to trace the memory leak, and found the following kafka classes to be the cause for the memory leak. Please let me know if you need any additional information to debug this issue. 

kafka.server.FetcherLagMetrics
kafka.consumer.FetchRequestAndResponseMetrics
kafka.consumer.FetchRequestAndResponseStats
kafka.metrics.KafkaTimer
kafka.utils.Pool
","centos linux, jdk 1.6, jboss",hechen.gao,jkreps,junrao,omkreddy,senthilchittibabu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,331762,,,Wed Dec 02 05:36:48 UTC 2015,,,,,,,,,,"0|i1l8fz:",332093,,,,,,,,,,,,,,,,,,,,"07/Jun/13 15:28;junrao;Is the OOME on the broker or the consumer? How many topics and partitions do you have? We maintain some stats per partition. So the more partitions you have, the more memory is needed.;;;","07/Jun/13 17:20;senthilchittibabu;The OOME is on the consumer. I have defined around 25 topics and 25 partition per topic. I can increase the memory but sooner or later it's going to crash with OOME, right?;;;","10/Jun/13 15:18;junrao;There is a fixed amount memory needed per topic/partition for metrics. So, if the # partitions consumed in a consumer doesn't change, the memory needed for metrics shouldn't grow. Since you have more than 600 partitions, could you try a larger heap size and see if the problem persists?;;;","10/Jun/13 16:49;senthilchittibabu;The application ramps up within few minutes to consume the message from the queue, I understand if the metrics memory grows during the ramp up, but shouldn't it remain flat after that. But I see the Metrics instance grow steadily. 

I did noticed the kafka_2.8.0-0.8.0-SNAPSHOT downgraded the metrics jars from 3.0 to 2.2.0. I did not have this issue with initial version of kafka 0.8 with metrics_core_3.0.0. Is there any reason this was done?

Anyway, I will increase the maxPermSize to 1g or 2g from 512m and run the test again. 

;;;","11/Jun/13 04:16;junrao;We downgraded metrics to 2.2.0 because that's the most stable released version (see the discussion in KAFKA-826). Let us know how your change goes. We have been running a consumer in (mirrormaker) with thousands of partitions for weeks and haven't seen this memory issue. ;;;","11/Jun/13 15:42;senthilchittibabu;The application did not crash yet after increasing the max PermGen to 2g. However the permGen memory is slowly increasing. I can clearly see the following object instance are created in thousands and ten thousands. And never gets garbage collected even after force garbage collection by using the profiler. I believe since these object size are small, you will not see the application crash for days/weeks if you set the max memory to high number. You will notice the leak if you watch the following objects in the profiler. Can you please run your app and watch the following object in the profiler?

kafka.server.FetcherLagMetrics 
kafka.consumer.FetchRequestAndResponseMetrics 
kafka.consumer.FetchRequestAndResponseStats 
kafka.metrics.KafkaTimer 
kafka.utils.Pool 

BTW, what is your consumer app memory settings?
;;;","11/Jun/13 16:57;senthilchittibabu;The application did not crash yet after increasing the max PermGen to 2g. However the permGen memory is slowly increasing. I can clearly see the following object instance are created in thousands and ten thousands. And never gets garbage collected even after force garbage collection by using the profiler. I believe since these object size are small, you will not see the application crash for days/weeks if you set the max memory to high number. You will notice the leak if you watch the following objects in the profiler. Can you please run your app and watch the following object in the profiler?

kafka.server.FetcherLagMetrics 
kafka.consumer.FetchRequestAndResponseMetrics 
kafka.consumer.FetchRequestAndResponseStats 
kafka.metrics.KafkaTimer 
kafka.utils.Pool 

BTW, what is your consumer app memory settings?
;;;","12/Jun/13 05:16;junrao;This is the heap setting in our MirrorMaker tool.
Xms3g -Xmx3g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 

BTW, do you see lots of rebalances in your consumer log?;;;","12/Jun/13 13:59;senthilchittibabu;Yes, I see lot of rebalances (can't rebalance after 4 retries) in the log.. And also I see the below the warning often. Not sure is there any relation with memory leak?

2013-06-11 23:59:59,861 WARN  [kafka.consumer.ConsumerFetcherManager$LeaderFinderThread] (UrlWorker-grp_hms.com-1370985065479-bb5e7de1-leader-finder-thread], Failed to find leader for Set([dev--raw-topic,10], [ption
 at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:416) [zkclient-0.2.jar:0.2]
 at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:413) [zkclient-0.2.jar:0.2]
 at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675) [zkclient-0.2.jar:0.2]
 at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:413) [zkclient-0.2.jar:0.2]
 at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:409) [zkclient-0.2.jar:0.2]
 at kafka.utils.ZkUtils$.getChildrenParentMayNotExist(ZkUtils.scala:438) [kafka_2.8.0-0.8.0-SNAPSHOT.jar:0.8.0
 at kafka.utils.ZkUtils$.getAllBrokersInCluster(ZkUtils.scala:75) [kafka_2.8.0-0.8.0-SNAPSHOT.jar:0.8.0-SNAPSH
 at kafka.consumer.ConsumerFetcherManager$LeaderFinderThread.doWork(ConsumerFetcherManager.scala:62) [kafka_2.
 at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51) [kafka_2.8.0-0.8.0-SNAPSHOT.jar:0.8.0-SNAP;;;","12/Jun/13 14:47;junrao;Normally, rebalances are rare. The common cause of too many rebalances is GC (see #6 in http://kafka.apache.org/faq.html for details). Not sure if the memory leak issue you reported is related to too many rebalances. Nevertheless, you should try to tune the GC to minimize rebalances.;;;","24/Jun/13 15:16;senthilchittibabu;Finally the memory leak issue resolved after providing ""consumer.id"" property during consumer creation. We found that kafka is not cleaning up all the metrics object during the shutdown process. We read messages based on the interval, so the application open/close consumer frequently. 

By providing the static ""consumer.id"" looks like kafka is reusing the metrics object, hence it stopped creating lots of metrics object. However now we see duplicate message consumption by the consumer within the same group as side effect. Basically we have 10 partition and 10 consumer thread, all reading the same message even though they all have same ""group.id"".

I believe cleaning up all the metrics object during shutdown process is the right fix for this memory leak. If you setup small testcase which open/close consumer frequently, you can see the memory leak immediately. All Metrics object like MetricName, EWMA, Histogram, etc will never gets garbage collected. 

I am not sure why kafka needs unique consumer id to read the message from the stream. Please advice.;;;","07/Feb/15 22:23;jkreps;[~junrao] Does this issue still exist?;;;","08/Feb/15 06:26;omkreddy;After KAFKA-1481,  I am observing one issue. We are not removing the newly added Version/AppInfo Mbean on old producer/consumer close call.  There is no other memory leak. Will submit a patch for this.;;;","02/Dec/15 05:36;hechen.gao;I am using kafka_2.10-0.8.1.1.jar in my project (which depend on metrics-core-2.2.0.jar), and i also ran into this issue recently.
Metrics related object such as ConcurrentSkipListMap<Double, Long> used up to 90.5% of the memory.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix shutdown tool to work with new shutdown api,KAFKA-935,12651345,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriramsub,sriramsub,sriramsub,06/Jun/13 14:41,06/Jun/13 16:30,14/Jul/23 05:39,06/Jun/13 16:30,,,,,,,,,,,,,,0,,,,"This seems to have been missed in the last patch ""Integrating controlled shutdown"".",,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/13 14:41;sriramsub;shutdowntool.patch;https://issues.apache.org/jira/secure/attachment/12586509/shutdowntool.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,331671,,,Thu Jun 06 16:30:54 UTC 2013,,,,,,,,,,"0|i1l7vr:",332002,,,,,,,,,,,,,,,,,,,,"06/Jun/13 16:29;nehanarkhede;+1;;;","06/Jun/13 16:30;nehanarkhede;Committed patch to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop example running DataGenerator causes kafka.message.Message cannot be cast to [B exception,KAFKA-933,12651149,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,andrewmilkowski,andrewmilkowski,05/Jun/13 16:33,22/Nov/13 05:37,14/Jul/23 05:39,22/Nov/13 05:37,0.8.0,,,0.8.1,,,,,,,contrib,,,0,hadoop,,,"Working of git master codebase

and following instructions at

https://github.com/apache/kafka/blob/trunk/contrib/hadoop-consumer/README

https://github.com/apache/kafka

when running

./run-class.sh kafka.etl.impl.DataGenerator test/test.properties

an exception is thrown

Exception in thread ""main"" java.lang.ClassCastException: kafka.message.Message cannot be cast to [B
	at kafka.serializer.DefaultEncoder.toBytes(Encoder.scala:34)
	at kafka.producer.async.DefaultEventHandler$$anonfun$serialize$1.apply(DefaultEventHandler.scala:129)
	at kafka.producer.async.DefaultEventHandler$$anonfun$serialize$1.apply(DefaultEventHandler.scala:124)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
	at scala.collection.Iterator$class.foreach(Iterator.scala:772)
	at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:573)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:73)
	at scala.collection.JavaConversions$JListWrapper.foreach(JavaConversions.scala:615)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
	at scala.collection.JavaConversions$JListWrapper.map(JavaConversions.scala:615)
	at kafka.producer.async.DefaultEventHandler.serialize(DefaultEventHandler.scala:124)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:54)
	at kafka.producer.Producer.send(Producer.scala:74)
	at kafka.javaapi.producer.Producer.send(Producer.scala:41)","[amilkowski@localhost ~]$ uname -a
Linux localhost.localdomain 3.9.4-200.fc18.x86_64 #1 SMP Fri May 24 20:10:49 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux
[amilkowski@localhost ~]$ 
",andrewmilkowski,drunkedcat,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,331475,,,Fri Nov 22 05:37:11 UTC 2013,,,,,,,,,,"0|i1l6of:",331807,,,,,,,,,,,,,,,,,,,,"05/Jun/13 16:40;andrewmilkowski;workaround was to patch DataGenerator to send the message string, instead the Message object 


public void run() throws Exception {

		List<KeyedMessage<Integer, String>> list = new ArrayList<KeyedMessage<Integer, String>>();
		for (int i = 0; i < 50; i++) {
			Long timestamp = RANDOM.nextLong();
			if (timestamp < 0) timestamp = -timestamp;
			String messageStr = timestamp.toString();
			log.info("" creating message: "" + messageStr);
			list.add(new KeyedMessage<Integer, String>(topic, null, messageStr));
		}

		log.info("" send "" + list.size() + "" "" + topic + "" count events to "" + uri);
		producer.send(list);
		producer.close();
		
		generateOffsets();
	};;;","22/Nov/13 03:43;drunkedcat;as 0.8.0-beta1, the following change will work:

                        byte[] bytes = timestamp.toString().getBytes(""UTF8"");
--                       Message message = new Message(bytes);
--                       list.add(new KeyedMessage<Integer, Message>(_topic, null, message));
++
++                       list.add(new KeyedMessage<Integer, byte[]>(_topic, null, bytes));
                }
;;;","22/Nov/13 05:37;junrao;Thanks for the patch. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate preferred replica election logic into kafka,KAFKA-930,12650765,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriramsub,sriramsub,sriramsub,03/Jun/13 23:09,25/Feb/14 08:47,14/Jul/23 05:39,23/Dec/13 18:09,,,,0.8.1,,,,,,,,,,1,,,,It seems useful to integrate the preferred replica election logic into kafka controller. A simple way to implement this would be to have a background thread that periodically finds the topic partitions that are not assigned to the preferred broker and initiate the move. We could come up with some heuristics to initiate the move only if the imbalance over a specific threshold in order to avoid rebalancing too aggressively. Making the software do this reduces operational cost.,,noslowerdna,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/13 01:19;sriramsub;KAFKA-930.patch;https://issues.apache.org/jira/secure/attachment/12614757/KAFKA-930.patch","20/Nov/13 01:37;sriramsub;KAFKA-930_2013-11-19_17:37:29.patch;https://issues.apache.org/jira/secure/attachment/12614763/KAFKA-930_2013-11-19_17%3A37%3A29.patch","20/Nov/13 01:39;sriramsub;KAFKA-930_2013-11-19_17:38:49.patch;https://issues.apache.org/jira/secure/attachment/12614764/KAFKA-930_2013-11-19_17%3A38%3A49.patch","21/Nov/13 17:42;sriramsub;KAFKA-930_2013-11-21_09:42:11.patch;https://issues.apache.org/jira/secure/attachment/12615155/KAFKA-930_2013-11-21_09%3A42%3A11.patch","10/Dec/13 06:52;sriramsub;KAFKA-930_2013-12-09_22:51:57.patch;https://issues.apache.org/jira/secure/attachment/12618002/KAFKA-930_2013-12-09_22%3A51%3A57.patch","20/Dec/13 19:13;sriramsub;KAFKA-930_2013-12-20_11:13:01.patch;https://issues.apache.org/jira/secure/attachment/12619892/KAFKA-930_2013-12-20_11%3A13%3A01.patch","20/Dec/13 19:23;sriramsub;KAFKA-930_2013-12-20_11:22:36.patch;https://issues.apache.org/jira/secure/attachment/12619894/KAFKA-930_2013-12-20_11%3A22%3A36.patch","27/Jan/14 21:28;sriramsub;KAFKA-930_2014-01-27_13:28:51.patch;https://issues.apache.org/jira/secure/attachment/12625445/KAFKA-930_2014-01-27_13%3A28%3A51.patch","24/Feb/14 09:59;sriramsub;KAFKA-930_2014-02-24_01:59:46.patch;https://issues.apache.org/jira/secure/attachment/12630639/KAFKA-930_2014-02-24_01%3A59%3A46.patch",,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,331092,,,Tue Feb 25 08:47:21 UTC 2014,,,,,,,,,,"0|i1l4bj:",331425,,,,,,,,,,,,,,,,,,,,"20/Nov/13 01:19;sriramsub;Created reviewboard https://reviews.apache.org/r/15711/
 against branch origin/trunk;;;","20/Nov/13 01:37;sriramsub;Updated reviewboard https://reviews.apache.org/r/15711/
 against branch origin/trunk;;;","20/Nov/13 01:39;sriramsub;Updated reviewboard https://reviews.apache.org/r/15711/
 against branch origin/trunk;;;","21/Nov/13 17:42;sriramsub;Updated reviewboard https://reviews.apache.org/r/15711/
 against branch origin/trunk;;;","10/Dec/13 06:52;sriramsub;Updated reviewboard https://reviews.apache.org/r/15711/
 against branch origin/trunk;;;","20/Dec/13 19:13;sriramsub;Updated reviewboard https://reviews.apache.org/r/15711/
 against branch origin/trunk;;;","20/Dec/13 19:23;sriramsub;Updated reviewboard https://reviews.apache.org/r/15711/
 against branch origin/trunk;;;","20/Dec/13 22:05;sriramsub;checked in to trunk;;;","27/Jan/14 21:29;sriramsub;Updated reviewboard https://reviews.apache.org/r/15711/
 against branch origin/trunk;;;","24/Feb/14 09:59;sriramsub;Updated reviewboard https://reviews.apache.org/r/15711/
 against branch origin/trunk;;;","25/Feb/14 08:47;sriramsub;final changes pushed to 0.8.1 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Download link in 0.7 quickstart broken,KAFKA-929,12650705,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,mumrah,mumrah,03/Jun/13 18:38,04/Jun/13 05:02,14/Jul/23 05:39,04/Jun/13 05:02,,,,,,,,,,,website,,,0,,,,"http://kafka.apache.org/07/quickstart.html

links to http://kafka.apache.org/07/downloads.html, instead of http://kafka.apache.org/downloads.html",,junrao,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,331032,,,Tue Jun 04 05:02:28 UTC 2013,,,,,,,,,,"0|i1l3yf:",331365,,,,,,,,,,,,,,,,,,,,"04/Jun/13 05:02;junrao;Thanks for identifying this. Fixed the website.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
new topics may not be processed after ZK session expiration in controller,KAFKA-928,12650431,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,junrao,junrao,01/Jun/13 01:43,03/Jun/13 21:09,14/Jul/23 05:39,03/Jun/13 21:07,0.8.0,,,,,,,,,,controller,,,0,,,,"When controller loses its ZK session, it calls partitionStateMachine.shutdown in SessionExpirationListener, which marks the partitionStateMachine as down. However, when the controller regains its controllership, it doesn't mark partitionStateMachine as up. In TopicChangeListener, we only process new topics if the partitionStateMachine is marked up.",,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/13 19:06;nehanarkhede;kafka-928-v2.patch;https://issues.apache.org/jira/secure/attachment/12585919/kafka-928-v2.patch","01/Jun/13 20:12;nehanarkhede;kafka-928.patch;https://issues.apache.org/jira/secure/attachment/12585737/kafka-928.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,330758,,,Mon Jun 03 21:09:51 UTC 2013,,,,,,,,,,"0|i1l29j:",331091,,,,,,,,,,,,,,,,,,,,"01/Jun/13 20:12;nehanarkhede;The bug is more serious. If the controller goes through a session expiration and gets re-elected, which is rare, it will stop responding to all new topic state changes. Not only that, it will also stop responding to broker failures or startups.

The root cause of the bug is in the startup() API of the state machines. Both hasStarted and hasShutdown() are required since the former prevents the state machines from acting on state changes before their internal data structures are ready. The latter prevents state machines from acting on state changes while they are being shutdown.;;;","02/Jun/13 03:39;swapnilghike;+1, thanks for fixing this.;;;","03/Jun/13 03:13;junrao;Thanks for the patch. It seems to me that hasStarted should be set to false on shutdown too. If that's the case, I don't see why we need both hasStarted and hasShutdown.;;;","03/Jun/13 19:06;nehanarkhede;I think you are right, we don't need both anymore. See the updated patch.;;;","03/Jun/13 20:45;junrao;Thanks for patch v2. +1.;;;","03/Jun/13 21:07;nehanarkhede;Thanks for the review, committed patch to 08;;;","03/Jun/13 21:09;swapnilghike;Was just about to comment, perhaps it would be good to rename hasStarted to isRunning like in KafkaController. +1 otherwise.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate controlled shutdown into kafka shutdown hook,KAFKA-927,12650258,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriramsub,sriramsub,sriramsub,31/May/13 00:50,04/Jun/13 17:40,14/Jul/23 05:39,03/Jun/13 23:09,,,,0.8.0,,,,,,,,,,0,,,,The controlled shutdown mechanism should be integrated into the software for better operational benefits. Also few optimizations can be done to reduce unnecessary rpc and zk calls. This patch has been tested on a prod like environment by doing rolling bounces continuously for a day. The average time of doing a rolling bounce with controlled shutdown for a cluster with 7 nodes without this patch is 340 seconds. With this patch it reduces to 220 seconds. Also it ensures correctness in scenarios where the controller shrinks the isr and the new leader could place the broker to be shutdown back into the isr.,,jjkoshy,junrao,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/13 17:50;sriramsub;KAFKA-927-v2-revised.patch;https://issues.apache.org/jira/secure/attachment/12585900/KAFKA-927-v2-revised.patch","01/Jun/13 00:23;sriramsub;KAFKA-927-v2.patch;https://issues.apache.org/jira/secure/attachment/12585687/KAFKA-927-v2.patch","03/Jun/13 22:08;sriramsub;KAFKA-927-v3-removeimports.patch;https://issues.apache.org/jira/secure/attachment/12585967/KAFKA-927-v3-removeimports.patch","03/Jun/13 20:49;sriramsub;KAFKA-927-v3.patch;https://issues.apache.org/jira/secure/attachment/12585936/KAFKA-927-v3.patch","03/Jun/13 22:44;sriramsub;KAFKA-927-v4.patch;https://issues.apache.org/jira/secure/attachment/12585977/KAFKA-927-v4.patch","31/May/13 00:52;sriramsub;KAFKA-927.patch;https://issues.apache.org/jira/secure/attachment/12585519/KAFKA-927.patch",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,330585,,,Tue Jun 04 17:40:32 UTC 2013,,,,,,,,,,"0|i1l17b:",330919,,,,,,,,,,,,,,,,,,,,"31/May/13 15:56;nehanarkhede;Sriram,

This patch doesn't compile with the following errors - 

[error] /home/nnarkhed/Projects/apache-kafka-git/core/src/main/scala/kafka/server/KafkaApis.scala:133: not found: type ControlledShutdownRequest
[error]     val controlledShutdownRequest = request.requestObj.asInstanceOf[ControlledShutdownRequest]
[error]                                                                     ^
[error] /home/nnarkhed/Projects/apache-kafka-git/core/src/main/scala/kafka/server/KafkaApis.scala:135: not found: type ControlledShutdownResponse
[error]     val controlledShutdownResponse = new ControlledShutdownResponse(controlledShutdownRequest.correlationId,
[error]                                          ^
[error] /home/nnarkhed/Projects/apache-kafka-git/core/src/main/scala/kafka/server/KafkaServer.scala:28: ControlledShutdownResponse is not a member of kafka.api
[error] import kafka.api.{ControlledShutdownResponse, ControlledShutdownRequest}
[error]        ^
[error] /home/nnarkhed/Projects/apache-kafka-git/core/src/main/scala/kafka/server/KafkaServer.scala:157: not found: type ControlledShutdownRequest
[error]             val request = new ControlledShutdownRequest(correlationId.getAndIncrement, config.brokerId)
[error]                               ^
[error] /home/nnarkhed/Projects/apache-kafka-git/core/src/main/scala/kafka/server/KafkaServer.scala:160: not found: value ControlledShutdownResponse
[error]             val shutdownResponse = ControlledShutdownResponse.readFrom(response.buffer)
[error]                                    ^
;;;","31/May/13 16:10;junrao;Thanks for the patch. Overall, a well thought-out patch. Some comments.

1. KafkaController.shutdownBroker: We should probably only do controlled shutdown if the controller is active.

2. KafkaApis: If the controller is not active, we should send an errorcode back to the ControlledShutdownRequest.

3. KafkaServer: I am not sure that we should call replicaManager.replicaFetcherManager.closeAllFetchers() at the beginning of the controlled shutdown. Once the fetchers are closed, the affected leaders have to wait for the timeout before committing new messages since they have to shrink the ISR. Instead, it's better if we let the fetcher to be closed through leaderAndIsr requests from the controller.

4. KafkaConfig: Could we consolidate controlledShutdownMaxRetries and controlledShutdownEnable to one config controlledShutDownWaitTime? If that value is <=0, no controlled shutdown is done. Otherwise, we will try controlled shutdown until that time has passed.

5. With the new logic added in ReplicaManager/Partition, I am not sure if the old controlled shutdown tool still works properly. Should we just remove the tool and the jmx hook?

6. There are new files not included in the patch.;;;","31/May/13 16:45;nehanarkhede;Thanks for the patch, very well thought out! Few comments -
1. KafkaServer
1.1 doControlledShutdown()
- Is there a reason why we cannot just invoke shutdown() on the ReplicaManager instead of hacking into the replica fetcher manager and shutting down the fetchers ?
- ""starting controlled shutdown"" -> ""Starting controlled shutdown"". Though it is not introduced in this patch, can we please change the same in the shutdown() API as well?
- Typo -> shutdownSuceeded
- This method is pretty big and slightly hard to read, for someone who is new to controlled shutdown. Can we move controller discovery/connection logic to a separate API named connectToController() ? -
        val controllerId = ZkUtils.getController(kafkaZookeeper.getZookeeperClient)
        ZkUtils.getBrokerInfo(kafkaZookeeper.getZookeeperClient, controllerId) match {
          case Some(broker) =>
            if (channel == null || prevController == null || !prevController.equals(broker)) {
              // if this is the first attempt or if the controller has changed, create a channel to the most recent
              // controller
              if (channel != null) {
                channel.disconnect()
              }
              channel = new BlockingChannel(broker.host, broker.port,
                BlockingChannel.UseDefaultBufferSize,
                BlockingChannel.UseDefaultBufferSize,
                config.controllerSocketTimeoutMs)
              channel.connect()
              prevController = broker
            }
          case None=>
            //ignore and try again
        }
- I also think it will be cleaner for the loop to look like, but it's upto you :) 
      while (!shutdownSucceeded && remainingRetries > 0) {
         val controller = connectToController(zkClient)
         val shutdownSucceeded = sendControllerShutdownRequest(controller)
         if(!shutdownSucceeded)
            Thread.sleep(...)
         remainingRetries -= 1
      }
- Can we add either a warn or an info message that the broker will retry controlled shutdown after n ms ?
        if (!shutdownSuceeded) {
          Thread.sleep(config.controlledShutdownRetryBackoffMs)
        }
- Can we rename doControlledShutdown() to just controlledShutdown(). This will follow the naming conventions in the rest of the code, since we don't name methods doSomething.
- Let's remove the zkClient unused variable

2. KafkaApis
- If the controller is not active, we should send the appropriate error code
  
3. KafkaController
- getPartitionsAssignedToBroker() does not need to read from zookeeper. The controller should already have the latest data available as the controllerLock is acquired at this point. 
- The following updates zookeeper which is not required since the leader would've done that long before the controller does it. This is because you shutdown the replica fetchers at the beginning of controlled shutdown. It will be much faster to just send a leader and isr request with the shrunk ISR to the existing leader, though I doubt that is required as well.
            else {
              // if the broker is a follower, updates the isr in ZK and notifies the current leader
              replicaStateMachine.handleStateChanges(Set(PartitionAndReplica(topicAndPartition.topic,
                topicAndPartition.partition, id)), OfflineReplica)
            }

4. We want to know that the broker is rejecting the become-follower request in the state change log when the following happens. So it is not enough to just surround the addFetcher call with this condition 
5. New files are not included in the patch
          if (!replicaManager.isShuttingDown.get()) {
            // start fetcher thread to current leader if we are not shutting down
            replicaFetcherManager.addFetcher(topic, partitionId, localReplica.logEndOffset, leaderBroker)
          }
;;;","01/Jun/13 00:22;sriramsub;Thank you for the review guys.

Jun - 

1. I did not add an explicit check since the state manager anyway throw. Added one explicitly.
3. I have thought different approaches to do it exactly right but all of them seems ugly. I will think a little bit more but it should not be a blocker since most of the active topic would realize the isr change soon due to the number of messages.
4. I don't like to overload config value with multiple meanings. The config name should do exactly what it is meant to in my opinion.
5. It works perfectly fine. Infact there is a unit test for the old tool that works fine. The functionality of the tool has not changed.
6. Added the missing files

Neha -
1.
1.1 Done
1.2 Done
1.3 Done
1.4 Refactoring the method is actually pretty tricky. There are actually multiple dependencies between the methods (previousController, channels). The individual functions (if we manage to refactor), dont really do a single operation to provide a good name to it. I have added comments to make things clear. 
1.6 Added a warn
1.7 renamed the method name

2. Done
3. If the leader did realize that the follower has fallen off the isr soon then there are no issues. However that does not seem to be the case. We use the number of messages or time period to decide when to remove the follower from the isr. The default time period used is 10sec. So it could take as long as 10 seconds to realize the isr change. However the controller may do it much sooner and hence we get the better of the two. Having said that, there is an ugly fix to make the leader realize the isr change immediately after the replica thread stops but I need more time to think about that.
4. updated the state change log
5. added the missing files

;;;","03/Jun/13 14:55;junrao;Thanks for patch v2. A few more comments:

20. KafkaController: If when shutdownBroker is called, the controller is no longer active, both state machines will throw an exception on state change calls. However, the issue is that we add the shutdown broker to controllerContext.shuttingDownBrokerIds and it's never reset. This may become a problem if this broker becomes a controller again. At the minimum, we need to reset controllerContext.shuttingDownBrokerIds in  onControllerFailover(). However, I am a bit confused why we never reset controllerContext.shuttingDownBrokerIds and the shutdown logic still works.

21. ControlledShutdownRequest.handleError(): We should probably set partitionsRemaining in ControlledShutdownResponse to empty instead of null, since the serialization of ControlledShutdownResponse doesn't handle partitionsRemaining being null.

22. testRollingBounce:
22.1 The test makes sure that the leader for topic1 is changed after broker 0 is shutdown. However, the leader for topic1 could be on broker 1 initially. In this case, the leader won't be changed after broker 0 is shutdown.
22.2 The default controlledShutdownRetryBackoffMs is 5secs, which is probably too long for the unit test. 

23. KafkaServer: We need to handle the errorCode in ControlledShutdownResponse since the controller may have moved after we send the ControlledShutdown request.

From the previous review:
3. I think a simple solution is to (1) not call replicaManager.replicaFetcherManager.closeAllFetchers() in KafkaServer during shutdown; (2) in KafkaController.shutdownBroker(), for each partition on the shutdown broker, we first send a stopReplicaRequest to it for that partition before going through the state machine logic. Since the state machine logic involves ZK reads/writes, it's very likely that the stopReplicaRequest will reach the broker before the subsequent LeaderAndIsr requests. So, in most cases, the leader should be able to shrink ISR quicker than the timeout, without churns in ISR.;;;","03/Jun/13 17:50;sriramsub;Realized my previous patch did not have my latest changes just the new files.

20. shuttingDownBrokerIds does get updated on broker failure
21 done
22.1 i had already fixed this. The new patch should have the change
23. This is also handled in the new patch

3. That sounds reasonable among all the hacky fixes. ;;;","03/Jun/13 18:07;nehanarkhede;Thanks for the revised v2 patch. Few more comments -

1. KafkaServer
1.1 startupComplete should either be a volatile variable to AtomicBoolean. Two different threads call startup() and controlledShutdown(), which modify startupComplete.
1.2 In controlledShutdown(), we need to handle error codes in ControlledShutdownResponse explicitly. It can happen that the error code is set and partitionsRemaining are 0, which will lead to errors.

2. Partition

From previous review #4, if the broker has to ignore the become follower request anyway, does it make sense to even process part of it and truncate log etc ?

3. From previous review #3, I meant that it is pointless to do the ZK write on the controller since right after the write, since the follower hasn't received the stop replica request and the leader hasn't received shrunk isr, the broker being shut down will get added back to ISR. You can verify that this happens from the logs. It also makes controlled shutdown very slow since typically in production we move ~1000 partitions from the broker and zk writes can take ~20ms which means several seconds wasted just doing the ZK writes. Instead, it is enough to let the leader shrink the isr by sending it the leader and isr request. On the other hand, we can argue that the OfflineReplica state change itself should be changed to avoid the ZK write. But that is a bigger change, so we should avoid that right now.;;;","03/Jun/13 20:49;sriramsub;1.1 Done
1.2 Done
2. We would need to do some of these to ensure the new leader is updated and the log itself is going to be truncated either on startup or shutdown. Hence did not feel a strong reason to make this path more optimized.

3. As we spoke offline, there seems to be edge case where not updating ZK could lead to bad things happening. So updating ZK before leaderisr request.;;;","03/Jun/13 21:57;junrao;Thanks for patch v3. A few more comments:

30. KafkaServer:
30.1 Could you combine isShuttingDown and startupComplete?
30.2 In controlledShutdown(), it's not clear if it's worth caching the socket channel. Technically, it's possible for a controller to come back on the broker with the same id, but with a different broker host/port. It's simpler to just always close the socket channel on each ControlledShutdownRequest and create a new channel on retry.

31. KafkaController:
31.1 remove unused import java.util.concurrent.{Semaphore
31.2 I think we still need to set shuttingDownBrokerIds to empty in onControllerFailover(). A controller may failover during a controlled shutdown and later regain the controllership. OnBrokerFailure() is only called if the controller is active. So shuttingDownBrokerIds may not be empty when the controllership switches back.;;;","03/Jun/13 22:02;nehanarkhede;+1 on v3 other than Jun's comments.;;;","03/Jun/13 22:07;sriramsub;30.1 Don't feel strong about this. I think it makes things less readable with not much savings
30.2 The new broker includes the host and port and hence it works.

31.1 Done
31.2 This is already there in the previous patch. It is in InitializeControllerContext;;;","03/Jun/13 22:44;sriramsub;From offline feedback
1. reset startupcomplete flag on shutdown for unit test
2. cleaned channel before shutting down;;;","03/Jun/13 23:09;junrao;Thanks for patch v4. +1 and committed to 0.8.;;;","04/Jun/13 17:40;jjkoshy;+1 - sorry I got to this late.

Small nit: the scaladoc for shutdown broker needs an edit which we will clean up later.
We probably don't need the adminTest's testShutdownBroker given that the rolling bounce test exercises the same logic.

Also, I think we can close KAFKA-817 - another approach with similar goals.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Specify console consumer properties via a single --property command line parameter,KAFKA-924,12649831,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriharsha,swapnilghike,swapnilghike,29/May/13 01:14,03/Jul/14 13:41,14/Jul/23 05:39,27/May/14 21:56,,,,0.8.2.0,,,,,,,,,,0,newbie,,,"Quoting Neha from KAFKA-917:

I think the right way to add access to all consumer properties is to specify it through a single --property command line parameter that takes in ""key1=value1,key2=value2,..."" list. That will make sure we don't have to keep changing console consumer as well add/remove config options on the consumer. Some configs make sense to be top level for console consumer though. Things like topic, from-beginning, groupid etc. Rest can be specified through the ""property"" parameter. ",,junrao,michalm,nehanarkhede,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1519,,,,,,,,,,"03/Jun/14 04:53;sriharsha;KAFKA-924.patch;https://issues.apache.org/jira/secure/attachment/12648074/KAFKA-924.patch","18/May/14 20:28;sriharsha;KAFKA-924.patch;https://issues.apache.org/jira/secure/attachment/12645481/KAFKA-924.patch","23/May/14 21:47;sriharsha;KAFKA-924_2014-05-23_14:47:30.patch;https://issues.apache.org/jira/secure/attachment/12646614/KAFKA-924_2014-05-23_14%3A47%3A30.patch","27/May/14 21:51;sriharsha;KAFKA-924_2014-05-27_14:51:31.patch;https://issues.apache.org/jira/secure/attachment/12646996/KAFKA-924_2014-05-27_14%3A51%3A31.patch","29/May/14 00:48;sriharsha;KAFKA-924_2014-05-28_17:47:53.patch;https://issues.apache.org/jira/secure/attachment/12647269/KAFKA-924_2014-05-28_17%3A47%3A53.patch","30/May/14 01:34;sriharsha;KAFKA-924_2014-05-29_18:34:13.patch;https://issues.apache.org/jira/secure/attachment/12647506/KAFKA-924_2014-05-29_18%3A34%3A13.patch","30/May/14 01:40;sriharsha;KAFKA-924_2014-05-29_18:40:10.patch;https://issues.apache.org/jira/secure/attachment/12647507/KAFKA-924_2014-05-29_18%3A40%3A10.patch","30/May/14 18:26;sriharsha;KAFKA-924_2014-05-30_11:25:57.patch;https://issues.apache.org/jira/secure/attachment/12647660/KAFKA-924_2014-05-30_11%3A25%3A57.patch",,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,330158,,,Tue Jun 03 14:36:19 UTC 2014,,,,,,,,,,"0|i1kykn:",330492,,,,,,,,,,,,,,,,,,,,"18/May/14 20:28;sriharsha;Created reviewboard https://reviews.apache.org/r/21616/diff/
 against branch origin/trunk;;;","18/May/14 20:34;sriharsha;I've removed all the consumer related properties as top level command line arguments and added --consumer-properties option. Please let me know if you would like to have group.id as top-level argument.
Thanks,
Harsha;;;","23/May/14 21:47;sriharsha;Updated reviewboard https://reviews.apache.org/r/21616/diff/
 against branch origin/trunk;;;","27/May/14 21:51;sriharsha;Updated reviewboard https://reviews.apache.org/r/21616/diff/
 against branch origin/trunk;;;","27/May/14 21:56;nehanarkhede;Thanks for the patches, pushed to trunk;;;","28/May/14 16:16;junrao;Thanks for the patch. This seems to have broken our system tests though. Our system tests rely on console consumer to verify the output, which now fails with the following.

bin/kafka-run-class.sh kafka.consumer.ConsoleConsumer --zookeeper localhost:2181 --topic test --consumer-timeout-ms 10000 --csv-reporter-enabled  --from-beginning 
'consumer-timeout-ms' is not a recognized option

Perhaps the easiest fix is to resurrect the --consumer-timeout-ms for backward compatibility. ;;;","28/May/14 16:26;sriharsha;Thanks [~junrao] . Will send a updated patch. Are those system tests part of kafka integration tests. I wanted to check if this patch breaks anything else. Otherwise I'll add consumer-timeout-ms  and send a update patch.;;;","28/May/14 16:44;junrao;All system tests are under system_test/. You can read the README file in that dir. If you run the following, it will run testcase_0001 locally. It fails currently because of the above issue.

python -u -B system_test_runner.py 2>&1 | tee system_test_output.log;;;","28/May/14 17:02;nehanarkhede;[~junrao] Thanks for catching that. However, is the right fix to patch the system tests?;;;","28/May/14 18:15;junrao;That's also possible. We will have to figure out how to pass in a config file to console consumer in system tests.;;;","29/May/14 00:48;sriharsha;Updated reviewboard https://reviews.apache.org/r/21616/diff/
 against branch origin/trunk;;;","29/May/14 00:50;sriharsha;[~junrao] [~nehanarkhede] This patch contains changes to system_test/utils/kafka_system_test_utils.py to create a consumer.properties file under /tmp and scp's to target host where the ConsoleConsumer going to run.
Thanks.;;;","30/May/14 01:34;sriharsha;Updated reviewboard https://reviews.apache.org/r/21616/diff/
 against branch origin/trunk;;;","30/May/14 01:40;sriharsha;Updated reviewboard https://reviews.apache.org/r/21616/diff/
 against branch origin/trunk;;;","30/May/14 18:26;sriharsha;Updated reviewboard https://reviews.apache.org/r/21616/diff/
 against branch origin/trunk;;;","30/May/14 18:33;sriharsha;[~nehanarkhede] [~junrao]
I made a mistake of uploading patch for different JIRA onto this one. I am not to able to see an option for deleting a diff revision. Can anyone let me know how should I fix this. 
Thanks;;;","02/Jun/14 15:01;junrao;Perhaps you could just submit a complete new RB.;;;","02/Jun/14 16:43;junrao;Also, have you tested some system tests locally with that patch?;;;","03/Jun/14 04:51;sriharsha;[~junrao] I ran the system tests as suggested.
Total failures count : 0

_test_case_name  :  testcase_1
_test_class_name  :  ReplicaBasicTest
arg : bounce_broker  :  true
arg : broker_type  :  leader
arg : message_producing_free_time_sec  :  15
arg : num_iteration  :  2
arg : num_messages_to_produce_per_producer_call  :  50
arg : num_partition  :  2
arg : replica_factor  :  3
arg : sleep_seconds_between_producer_calls  :  1
validation_status  :
     No. of messages from consumer on [test_1] at simple_consumer_test_1-0_r1.log  :  33125
     No. of messages from consumer on [test_1] at simple_consumer_test_1-0_r2.log  :  33125
     No. of messages from consumer on [test_1] at simple_consumer_test_1-0_r3.log  :  33125
     No. of messages from consumer on [test_1] at simple_consumer_test_1-1_r1.log  :  29375
     No. of messages from consumer on [test_1] at simple_consumer_test_1-1_r2.log  :  29375
     No. of messages from consumer on [test_1] at simple_consumer_test_1-1_r3.log  :  29375
     Unique messages from consumer on [test_1]  :  62500
     Unique messages from producer on [test_1]  :  62500
     Validate for data matched on topic [test_1]  :  PASSED
     Validate for data matched on topic [test_1] across replicas  :  PASSED
     Validate for merged log segment checksum in cluster [source]  :  PASSED
     Validate index log in cluster [source]  :  PASSED
     Validate leader election successful  :  PASSED;;;","03/Jun/14 04:53;sriharsha;Created reviewboard https://reviews.apache.org/r/22175/diff/
 against branch origin/trunk;;;","03/Jun/14 14:36;junrao;Thanks for the followup patch. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose max lag mbean for consumers and replica fetchers,KAFKA-921,12649792,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jjkoshy,jjkoshy,28/May/13 21:38,31/May/13 17:45,14/Jul/23 05:39,31/May/13 17:45,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"We have a ton of consumer mbeans with names that are derived from the consumer id, broker being fetched from, fetcher id, etc. This makes it difficult to do basic monitoring of consumer/replica fetcher lag - since the mbean to monitor can change. A more useful metric for monitoring purposes is the maximum lag across all fetchers.",,jjkoshy,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/May/13 21:42;jjkoshy;KAFKA-921-v1.patch;https://issues.apache.org/jira/secure/attachment/12585089/KAFKA-921-v1.patch","29/May/13 18:37;jjkoshy;KAFKA-921-v2.patch;https://issues.apache.org/jira/secure/attachment/12585273/KAFKA-921-v2.patch","30/May/13 23:56;jjkoshy;KAFKA-921-v3.patch;https://issues.apache.org/jira/secure/attachment/12585507/KAFKA-921-v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,330119,,,Fri May 31 17:45:01 UTC 2013,,,,,,,,,,"0|i1kybz:",330453,,,,,,,,,,,,,,,,,,,,"28/May/13 21:42;jjkoshy;This provides a max lag mbean for both consumer fetcher manager and replica fetcher manager; although I think it is more useful for monitoring consumers. For replica fetchers we need to closely monitor all replica fetchers anyway. i.e., the set of mbeans is static. I can reduce the scope to just consumers if others agree.;;;","29/May/13 04:58;junrao;Thanks for the patch. Could we add the max lag in one place in AbstractFetcherThread and AbstractFetcherManager? We can pass in the proper metrics name.;;;","29/May/13 18:37;jjkoshy;Yes - I think that would be better. Moved it to AbstractFetcherManager. So depending on whether you are looking at replica fetchers or consumer fetchers, the MaxLag mbean will show up in ReplicaFetcherManager or ConsumerFetcherManager respectively.;;;","30/May/13 23:56;jjkoshy;One caveat in this approach is that if a fetcher is wedged for any reason, then the reported lag is inaccurate since it depends on getting the high watermark from fetch responses. i.e., to check on the health of a consumer you would need to look at both the max lag and min fetch rate across all fetchers.
;;;","31/May/13 04:03;junrao;Thanks for patch v3. +1. Just one minor comment. 

1. ReplicaFetcherManager: It seems that we can just use ""Replica"", instead of ""Replica-"" + brokerConfig.brokerId as the metric prefix, since the metric is local to the broker.;;;","31/May/13 15:53;nehanarkhede;+1 on patch v3. ;;;","31/May/13 17:45;jjkoshy;Thanks for the reviews. Committed with the minor change - i.e., Replica instead of Replica-<id>;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
zkclient jar 0.2.0 is not compatible with 0.1.0,KAFKA-920,12649354,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,junrao,junrao,24/May/13 16:49,31/May/13 03:33,14/Jul/23 05:39,31/May/13 03:33,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"Just realized that zkclient 0.2.0 introduced a non-backward compatible api. In 0.1.0, it has
   public void writeData(java.lang.String path, java.lang.Object datat)
   public void writeData(java.lang.String path, java.lang.Object datat, int expectedVersion)

In 0.2.0, they are changed to

   public Stat writeData(java.lang.String path, java.lang.Object datat)
   public Stat writeData(java.lang.String path, java.lang.Object datat, int expectedVersion)

This means that If an application uses Kafka and also drags in another library (X) that depends on zkclient 0.1.0 (and uses ""void writeData())"", then when they upgrade to Kafka 0.8 consumer (which uses zkclient 0.2.0), their application can't just upgrade to zkclient 0.2.0 since library X's call to ""void writeData()"" will fail because of the signature change. Since zkclient 0.1.0 is widely used, this issue may affect many applications.

This non-backward compatible change was introduced by me since I didn't realize it's a signature change then. I am trying to see if zkclient can release a new version that's compatible. If that can't be done in time, we will have to downgrade zkclient to 0.1.0 and add the needed ZK functionality inside Kafka. This is not ideal, but can solve the issue quicker.
",,grayaii,junrao,nehanarkhede,phargett,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/13 16:55;junrao;kafka-920-downgrade-zkclient.patch;https://issues.apache.org/jira/secure/attachment/12584712/kafka-920-downgrade-zkclient.patch","24/May/13 21:48;junrao;kafka-920-downgrade-zkclient_v2.patch;https://issues.apache.org/jira/secure/attachment/12584765/kafka-920-downgrade-zkclient_v2.patch","30/May/13 15:44;junrao;kafka_920_zkclient_0.3.patch;https://issues.apache.org/jira/secure/attachment/12585425/kafka_920_zkclient_0.3.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,329681,,,Fri May 31 03:33:08 UTC 2013,,,,,,,,,,"0|i1kvmv:",330016,,,,,,,,,,,,,,,,,,,,"24/May/13 16:55;junrao;Attach a patch that downgrades zkclient to 0.1.0. The idea is to create a kafka.zookeeper.ZkClient that wraps zkclient and exposes the needed new api. All other changes are package renaming. Unit tests pass.;;;","24/May/13 17:36;nehanarkhede;I tried applying your patch, it fails at the new file - 

can't find file to patch at input line 392
Perhaps you used the wrong -p or --strip option?
The text leading up to this was:
--------------------------
|diff --git a/core/src/main/scala/kafka/zookeeper/ZkClient.java b/core/src/main/scala/kafka/zookeeper/ZkClient.java
|index 9a82120..0c6e305 100644
|--- a/core/src/main/scala/kafka/zookeeper/ZkClient.java
|+++ b/core/src/main/scala/kafka/zookeeper/ZkClient.java
--------------------------
;;;","24/May/13 21:48;junrao;Attach patch v2 (by using git diff HEAD). 

The zkclient folks actually just committed a change to make the api backward compatible with zkclient 0.1. So, we may be able to use zkclient 0.3 if it's released in time.;;;","30/May/13 15:44;junrao;Thanks to the zkclient guys, they just released an 0.3 jar that's backward compatible with the 0.1 jar. Attached is a patch that upgrades zkclient to 0.3. Need to run sbt update to pick up the new jar.;;;","30/May/13 22:45;nehanarkhede;+1, thanks for the patch!;;;","31/May/13 03:33;junrao;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disabling of auto commit is ignored during consumer group rebalancing,KAFKA-919,12649353,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,phargett,phargett,24/May/13 16:38,29/May/13 17:14,14/Jul/23 05:39,25/May/13 05:02,0.8.0,,,0.8.0,,,,,,,consumer,,,0,,,,"From the mailing list:

In one of our applications using Kafka, we are using the high-level consumer to pull messages from our topic.

Because we pull messages from topics in discrete units (e.g., an hour's worth of messages), we want to control explicitly when offsets are committed.

Even though ""auto.commit.enable"" is set to false, during consumer group rebalancing, offsets are committed anyway, regardless of the setting of this flag.

Is this a bug? Or just a known gap in offset management? I do see plenty of notes on the wiki suggesting future releases may enable applications using the high-level consumer to have more fine-grained control over offset management.

I also fully realize that different applications have different needs, and meeting all of them with a clean API can be challenging.

In the case of this application, the high-level consumer solves the problem of locating the correct in a cluster for a given topic, so there are advantages to using it, even if we are not using it to balance fetch load across multiple consumers. We ideally have only 1 consumer active per consumer group, and can tolerate some duplicate messages. But, the consumer groups make it easy for 1 consumer to recover at the correct starting point, should the prior consumer in the group have failed before doing a commit.",java 7/linux,davelatham,junrao,phargett,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/13 16:41;phargett;kafka-919.patch;https://issues.apache.org/jira/secure/attachment/12584709/kafka-919.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,329680,,,Sat May 25 05:02:23 UTC 2013,,,,,,,,,,"0|i1kvmn:",330015,,,,,,,,,,,,,,,,,,,,"24/May/13 16:40;phargett;Note from Jun on the users mailing list: ""it's a bug."";;;","24/May/13 16:41;phargett;Very simple patch; no change to any comments;;;","25/May/13 05:02;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock between fetcher shutdown and handling partitions with error,KAFKA-916,12648995,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jjkoshy,jjkoshy,22/May/13 23:23,28/May/13 16:56,14/Jul/23 05:39,28/May/13 16:55,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"Here is another consumer deadlock that we encountered. All consumers are
vulnerable to this during a rebalance if there happen to be partitions in
error.

On a rebalance, the fetcher manager closes all fetchers and this holds on to
the fetcher thread map's lock. (mapLock in AbstractFetcherManager). [t1]
While the fetcher manager is iterating over fetchers to stop them, a fetcher
that is yet to be stopped hits an error on a partition and proceeds to
handle partitions with error [t2]. This handling involves looking up the
fetcher for that partition and then removing it from the fetcher's set of
partitions to consume. This requires grabbing the same map lock in [t1],
hence the deadlock.

[t1]
2013-05-22_20:23:11.95767 ""main"" prio=10 tid=0x00007f1b24007800 nid=0x573b waiting on condition [0x00007f1b2bd38000]
2013-05-22_20:23:11.95767    java.lang.Thread.State: WAITING (parking)
2013-05-22_20:23:11.95767 	at sun.misc.Unsafe.park(Native Method)
2013-05-22_20:23:11.95767 	- parking to wait for  <0x00007f1a25780598> (a java.util.concurrent.CountDownLatch$Sync)
2013-05-22_20:23:11.95767 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
2013-05-22_20:23:11.95767 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
2013-05-22_20:23:11.95768 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)
2013-05-22_20:23:11.95768 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)
2013-05-22_20:23:11.95768 	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:207)
2013-05-22_20:23:11.95768 	at kafka.utils.ShutdownableThread.shutdown(ShutdownableThread.scala:36)
2013-05-22_20:23:11.95769 	at kafka.server.AbstractFetcherThread.shutdown(AbstractFetcherThread.scala:68)
2013-05-22_20:23:11.95769 	at kafka.server.AbstractFetcherManager$$anonfun$closeAllFetchers$1.apply(AbstractFetcherManager.scala:79)
2013-05-22_20:23:11.95769 	at kafka.server.AbstractFetcherManager$$anonfun$closeAllFetchers$1.apply(AbstractFetcherManager.scala:78)
2013-05-22_20:23:11.95769 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
2013-05-22_20:23:11.95769 	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
2013-05-22_20:23:11.95770 	at scala.collection.Iterator$class.foreach(Iterator.scala:631)
2013-05-22_20:23:11.95770 	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
2013-05-22_20:23:11.95770 	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
2013-05-22_20:23:11.95770 	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
2013-05-22_20:23:11.95771 	at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
2013-05-22_20:23:11.95771 	at kafka.server.AbstractFetcherManager.closeAllFetchers(AbstractFetcherManager.scala:78)
---> 2013-05-22_20:23:11.95771 	- locked <0x00007f1a2ae92510> (a java.lang.Object)
2013-05-22_20:23:11.95771 	at kafka.consumer.ConsumerFetcherManager.stopConnections(ConsumerFetcherManager.scala:156)
2013-05-22_20:23:11.95771 	at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$closeFetchersForQueues(ZookeeperConsumerConnector.scala:488)
2013-05-22_20:23:11.95772 	at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.closeFetchers(ZookeeperConsumerConnector.scala:525)
2013-05-22_20:23:11.95772 	at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$rebalance(ZookeeperConsumerConnector.scala:422)
2013-05-22_20:23:11.95772 	at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener$$anonfun$syncedRebalance$1.apply$mcVI$sp(ZookeeperConsumerConnector.scala:374)
2013-05-22_20:23:11.95772 	at scala.collection.immutable.Range$ByOne$class.foreach$mVc$sp(Range.scala:282)
2013-05-22_20:23:11.95773 	at scala.collection.immutable.Range$$anon$2.foreach$mVc$sp(Range.scala:265)
2013-05-22_20:23:11.95773 	at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.syncedRebalance(ZookeeperConsumerConnector.scala:369)
2013-05-22_20:23:11.95773 	- locked <0x00007f1a2a29b450> (a java.lang.Object)
2013-05-22_20:23:11.95773 	at kafka.consumer.ZookeeperConsumerConnector.kafka$consumer$ZookeeperConsumerConnector$$reinitializeConsumer(ZookeeperConsumerConnector.scala:680)
2013-05-22_20:23:11.95774 	at kafka.consumer.ZookeeperConsumerConnector$WildcardStreamsHandler.handleTopicEvent(ZookeeperConsumerConnector.scala:754)
2013-05-22_20:23:11.95774 	at kafka.consumer.ZookeeperTopicEventWatcher$ZkTopicEventListener.liftedTree1$1(ZookeeperTopicEventWatcher.scala:74)
2013-05-22_20:23:11.95774 	at kafka.consumer.ZookeeperTopicEventWatcher$ZkTopicEventListener.handleChildChange(ZookeeperTopicEventWatcher.scala:69)
2013-05-22_20:23:11.95774 	- locked <0x00007f1a2a69b1d8> (a java.lang.Object)
2013-05-22_20:23:11.95774 	at kafka.consumer.ZookeeperTopicEventWatcher.startWatchingTopicEvents(ZookeeperTopicEventWatcher.scala:46)
2013-05-22_20:23:11.95775 	at kafka.consumer.ZookeeperTopicEventWatcher.<init>(ZookeeperTopicEventWatcher.scala:33)
2013-05-22_20:23:11.95775 	at kafka.consumer.ZookeeperConsumerConnector$WildcardStreamsHandler.<init>(ZookeeperConsumerConnector.scala:721)
2013-05-22_20:23:11.95775 	at kafka.consumer.ZookeeperConsumerConnector.createMessageStreamsByFilter(ZookeeperConsumerConnector.scala:140)
2013-05-22_20:23:11.95776 	at kafka.tools.MirrorMaker$$anonfun$main$3.apply(MirrorMaker.scala:118)
2013-05-22_20:23:11.95776 	at kafka.tools.MirrorMaker$$anonfun$main$3.apply(MirrorMaker.scala:118)
2013-05-22_20:23:11.95776 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
2013-05-22_20:23:11.95776 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
2013-05-22_20:23:11.95776 	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
2013-05-22_20:23:11.95777 	at scala.collection.immutable.List.foreach(List.scala:45)
2013-05-22_20:23:11.95777 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
2013-05-22_20:23:11.95777 	at scala.collection.immutable.List.map(List.scala:45)
2013-05-22_20:23:11.95777 	at kafka.tools.MirrorMaker$.main(MirrorMaker.scala:118)
2013-05-22_20:23:11.95777 	at kafka.tools.MirrorMaker.main(MirrorMaker.scala)

[t2]

2013-05-22_20:23:11.87465 ""ConsumerFetcherThread-xxxx-1369238724254-cff180ff-0-505"" prio=10 tid=0x00007f196401a800 nid=0x717a waiting for monitor entry [0x00007f19bf0ef000]
2013-05-22_20:23:11.87466    java.lang.Thread.State: BLOCKED (on object monitor)
2013-05-22_20:23:11.87467 	at kafka.server.AbstractFetcherManager.removeFetcher(AbstractFetcherManager.scala:57)
---> 2013-05-22_20:23:11.87467 	- waiting to lock <0x00007f1a2ae92510> (a java.lang.Object)
2013-05-22_20:23:11.87468 	at kafka.consumer.ConsumerFetcherManager$$anonfun$addPartitionsWithError$2.apply(ConsumerFetcherManager.scala:170)
2013-05-22_20:23:11.95682 	at kafka.consumer.ConsumerFetcherManager$$anonfun$addPartitionsWithError$2.apply(ConsumerFetcherManager.scala:170)
2013-05-22_20:23:11.95683 	at scala.collection.mutable.HashSet.foreach(HashSet.scala:61)
2013-05-22_20:23:11.95684 	at kafka.consumer.ConsumerFetcherManager.addPartitionsWithError(ConsumerFetcherManager.scala:170)
2013-05-22_20:23:11.95684 	at kafka.consumer.ConsumerFetcherThread.handlePartitionsWithErrors(ConsumerFetcherThread.scala:69)
2013-05-22_20:23:11.95684 	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:168)
2013-05-22_20:23:11.95684 	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:88)
2013-05-22_20:23:11.95684 	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)
2013-05-22_20:23:11.95686 
2013-05-22_20:23:11.95686 ""main-EventThread"" daemon prio=10 tid=0x00007f1b2471d000 nid=0x605a waiting on condition [0x00007f19bedec000]
2013-05-22_20:23:11.95686    java.lang.Thread.State: WAITING (parking)
2013-05-22_20:23:11.95686 	at sun.misc.Unsafe.park(Native Method)
2013-05-22_20:23:11.95686 	- parking to wait for  <0x00007f1a2a4426f8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
2013-05-22_20:23:11.95687 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
2013-05-22_20:23:11.95687 	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
2013-05-22_20:23:11.95687 	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
2013-05-22_20:23:11.95687 	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:503)
",,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/13 00:09;jjkoshy;KAFKA-916-v1.patch;https://issues.apache.org/jira/secure/attachment/12584794/KAFKA-916-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,329324,,,Tue May 28 16:56:17 UTC 2013,,,,,,,,,,"0|i1ktfz:",329659,,,,,,,,,,,,,,,,,,,,"23/May/13 16:08;junrao;Thanks for finding this. The deadlock is introduced because AbstractFetcherManager.removeFetcher() can be called from AbstractFetcherThread and AbstractFetcherManager.closeAllFetchers can wait for AbstractFetcherThread to stop. This is only happening for the ConsumerFetcherThread. So, one possible fix is to remove the error partitions from the fetcher directly in ConsumerFetcherThread.handlePartitionsWithErrors(), instead of going through ConsumerFetcherManager. This will break the cycle.;;;","25/May/13 00:09;jjkoshy;Agreed - I think that should fix the issue.;;;","25/May/13 05:13;junrao;Thanks for the patch. +1.;;;","28/May/13 16:56;jjkoshy;Thanks for the review. Committed to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System Test - Mirror Maker testcase_5001 failed,KAFKA-915,12648966,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,jjkoshy,jfung,jfung,22/May/13 21:14,02/Aug/13 23:18,14/Jul/23 05:39,02/Aug/13 23:18,,,,,,,,,,,,,,0,kafka-0.8,replication-testing,,"This case passes if brokers are set to partition = 1, replicas = 1
It fails if brokers are set to partition = 5, replicas = 3 (consistently reproducible)

This test case is set up as shown below.

1. Start 2 ZK as a cluster in Source
2. Start 2 ZK as a cluster in Target
3. Start 3 brokers as a cluster in Source (partition = 1, replicas = 1)
4. Start 3 brokers as a cluster in Target (partition = 1, replicas = 1)
5. Start 1 MM
6. Start ProducerPerformance to send some data
7. After Producer is done, start ConsoleConsumer to consume data
8. Stop all processes and validate if there is any data loss.
9. No failure is introduced to any process in this test

Attached a tar file which contains the logs and system test output for both cases.",,jfung,jjkoshy,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/13 17:10;jfung;kafka-915-v1.patch;https://issues.apache.org/jira/secure/attachment/12591230/kafka-915-v1.patch","22/May/13 21:17;jfung;testcase_5001_debug_logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12584384/testcase_5001_debug_logs.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,329295,,,Fri Aug 02 23:13:16 UTC 2013,,,,,,,,,,"0|i1kt9z:",329632,,,,,,,,,,,,,,,,,,,,"22/May/13 23:07;jfung;This issue is not reproducible in the following commit (or before it) :

commit 988d4d8e65a14390abd748318a64e281e4a37c19
Author: Neha Narkhede <neha.narkhede@gmail.com>
Date:   Tue Apr 30 17:20:54 2013 -0700
;;;","29/May/13 16:45;nehanarkhede;Joel, could you take a quick look ?;;;","04/Jul/13 00:45;jjkoshy;This failure is due to the fact that the leaderAndIsr request has not yet made it to the brokers until after the mirror maker's rebalance completes. This is related to the issue reported in KAFKA-956. Previously (before we started caching metadata at the brokers) the partition information was retrieved directly from zk.

The fix for now would be to use the create topic admin before starting the mirror maker (or move the producer performance start up to well before the mirror maker startup).
;;;","08/Jul/13 17:10;jfung;Thanks Joel for the suggestion. It's working by calling the create topic admin before starting mirror maker. kafka-915-v1.patch is uploaded for the change.;;;","09/Jul/13 21:31;jfung;Hi Joel,

After apply kafka-915-v1.patch (which is to create topic manually before starting mirror maker), testcase_5001 passes. However, testcase_5003 & testcase_5005 are failing due to data loss.

Thanks,
John;;;","02/Aug/13 23:13;jjkoshy;+1 on the patch. I actually could not reproduce the other failures, so I'll check this in.

========================================================

_test_case_name  :  testcase_5001
_test_class_name  :  MirrorMakerTest
arg : bounce_leader  :  false
arg : bounce_mirror_maker  :  false
arg : message_producing_free_time_sec  :  15
arg : num_iteration  :  1
arg : num_messages_to_produce_per_producer_call  :  50
arg : num_partition  :  1
arg : replica_factor  :  3
arg : sleep_seconds_between_producer_calls  :  1
validation_status  :
     Unique messages from consumer on [test_1]  :  500
     Unique messages from producer on [test_1]  :  500
     Validate for data matched on topic [test_1]  :  PASSED
     Validate for merged log segment checksum in cluster [source]  :  PASSED
     Validate for merged log segment checksum in cluster [target]  :  PASSED

========================================================

_test_case_name  :  testcase_5002
_test_class_name  :  MirrorMakerTest
validation_status  :

========================================================

_test_case_name  :  testcase_5003
_test_class_name  :  MirrorMakerTest
arg : bounce_leader  :  false
arg : bounce_mirror_maker  :  true
arg : bounced_entity_downtime_sec  :  30
arg : message_producing_free_time_sec  :  15
arg : num_iteration  :  1
arg : num_messages_to_produce_per_producer_call  :  50
arg : num_partition  :  1
arg : replica_factor  :  3
arg : sleep_seconds_between_producer_calls  :  1
validation_status  :
     Unique messages from consumer on [test_1]  :  2200
     Unique messages from producer on [test_1]  :  2200
     Validate for data matched on topic [test_1]  :  PASSED
     Validate for merged log segment checksum in cluster [source]  :  PASSED
     Validate for merged log segment checksum in cluster [target]  :  PASSED

========================================================

_test_case_name  :  testcase_5004
_test_class_name  :  MirrorMakerTest
validation_status  :

========================================================

_test_case_name  :  testcase_5005
_test_class_name  :  MirrorMakerTest
arg : bounce_leader  :  false
arg : bounce_mirror_maker  :  true
arg : bounced_entity_downtime_sec  :  30
arg : message_producing_free_time_sec  :  15
arg : num_iteration  :  1
arg : num_messages_to_produce_per_producer_call  :  50
arg : num_partition  :  2
arg : replica_factor  :  3
arg : sleep_seconds_between_producer_calls  :  1
validation_status  :
     Unique messages from consumer on [test_1]  :  1400
     Unique messages from consumer on [test_2]  :  1400
     Unique messages from producer on [test_1]  :  1400
     Unique messages from producer on [test_2]  :  1400
     Validate for data matched on topic [test_1]  :  PASSED
     Validate for data matched on topic [test_2]  :  PASSED
     Validate for merged log segment checksum in cluster [source]  :  PASSED
     Validate for merged log segment checksum in cluster [target]  :  PASSED

========================================================
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock between initial rebalance and watcher-triggered rebalances,KAFKA-914,12648572,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jjkoshy,jjkoshy,21/May/13 07:38,13/Feb/16 01:20,14/Jul/23 05:39,22/May/13 23:27,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"Summary doesn't give the full picture and the fetcher-manager/fetcher-thread                                                                                                                                
code is very complex so it's a bit hard to articulate the following very                                                                                                                                    
clearly. I will try and describe the sequence that results in a deadlock                                                                                                                                    
when starting up a large number of consumers at around the same time:                                                                                                                                       
                                                                                                                                                                                                            
- When a consumer's createMessageStream method is called, it initiates an                                                                                                                                   
  initial inline rebalance.                                                                                                                                                                                 
- However, before the above initial rebalance actually begins, a ZK watch                                                                                                                                   
  may trigger (due to some other consumers starting up) and initiate a                                                                                                                                      
  rebalance. This happens successfully so fetchers start and start filling                                                                                                                                  
  up the chunk queues.                                                                                                                                                                                      
- Another watch triggers and initiates yet another rebalance. This rebalance                                                                                                                                
  attempt tries to close the fetchers. Before the fetchers are stopped, we                                                                                                                                  
  shutdown the leader-finder-thread to prevent new fetchers from being                                                                                                                                      
  started.                                                                                                                                                                                                  
- The shutdown is accomplished by interrupting the leader-finder-thread and                                                                                                                                 
  then awaiting its shutdown latch.                                                                                                                                                                         
- If the leader-finder-thread still has a partition without leader to                                                                                                                                       
  process and tries to add a fetcher for it, it will get an exception                                                                                                                                       
  (InterruptedException if acquiring the partitionMapLock or                                                                                                                                                
  ClosedByInterruptException if performing an offset request). If we get an                                                                                                                                 
  InterruptedException the thread's interrupted flag is cleared.                                                                                                                                            
- However, the leader-finder-thread may have multiple partitions without                                                                                                                                    
  leader that it is currently processing. So if the interrupted flag is                                                                                                                                     
  cleared and the leader-finder-thread tries to add a fetcher for a another                                                                                                                                 
  partition, it does not receive an InterruptedException when it tries to                                                                                                                                   
  acquire the partitionMapLock. It can end up blocking indefinitely at that                                                                                                                                 
  point.                                                                                                                                                                                                    
- The problem is that until now, the createMessageStream's initial inline                                                                                                                                   
  rebalance has not yet returned - it is blocked on the rebalance lock                                                                                                                                      
  waiting on the second watch-triggered rebalance to complete. i.e., the                                                                                                                                    
  consumer iterators have not been created yet and thus the fetcher queues                                                                                                                                  
  get filled up. [td1]                                                                                                                                                                                      
- As a result, processPartitionData (which holds on to the partitionMapLock)                                                                                                                                
  in one or more fetchers will be blocked trying to enqueue into a full                                                                                                                                     
  chunk queue.[td2]                                                                                                                                                                                         
- So the leader-finder-thread cannot finish adding fetchers for the                                                                                                                                         
  remaining partitions without leader and thus cannot shutdown.                                                                                                                                             
                                                                                                                                                                                                            
One way to fix would be to let the exception from the leader-finder-thread                                                                                                                                  
propagate outside if the leader-finder-thread is currently shutting down -                                                                                                                                  
and avoid the subsequent (unnecessary) attempt to add a fetcher and lock                                                                                                                                    
partitionMapLock. There may be more elegant fixes (such as rewriting the                                                                                                                                    
whole consumer manager logic) but obviously we want to avoid extensive                                                                                                                                      
changes at this point in 0.8.                                                                                                                                                                               
                                                                                                                                                                                                            
Relevant portions of the thread-dump are here:                                                                                                                                                              
                                                                                                                                                                                                            
[td1] createMessageStream's initial inline rebalance (blocked on the ongoing                                                                                                                                
watch-triggered rebalance)                                                                                                                                                                                  
                                                                                                                                                                                                            
2013-05-20_17:50:13.04848 ""main"" prio=10 tid=0x00007f5960008000 nid=0x3772 waiting for monitor entry [0x00007f59666c3000]                                                                                   
2013-05-20_17:50:13.04848    java.lang.Thread.State: BLOCKED (on object monitor)                                                                                                                            
2013-05-20_17:50:13.04848       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.syncedRebalance(ZookeeperConsumerConnector.scala:368)                                                     
2013-05-20_17:50:13.04849       - waiting to lock <0x00007f58637dfe10> (a java.lang.Object)                                                                                                                 
2013-05-20_17:50:13.04849       at kafka.consumer.ZookeeperConsumerConnector.kafka$consumer$ZookeeperConsumerConnector$$reinitializeConsumer(ZookeeperConsumerConnector.scala:678)                          
2013-05-20_17:50:13.04850       at kafka.consumer.ZookeeperConsumerConnector$WildcardStreamsHandler.<init>(ZookeeperConsumerConnector.scala:712)                                                            
2013-05-20_17:50:13.04850       at kafka.consumer.ZookeeperConsumerConnector.createMessageStreamsByFilter(ZookeeperConsumerConnector.scala:140)                                                             
2013-05-20_17:50:13.04850       at kafka.tools.MirrorMaker$$anonfun$4.apply(MirrorMaker.scala:118)                                                                                                          
2013-05-20_17:50:13.04850       at kafka.tools.MirrorMaker$$anonfun$4.apply(MirrorMaker.scala:118)                                                                                                          
2013-05-20_17:50:13.04850       at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)                                                                                         
2013-05-20_17:50:13.04851       at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)                                                                                         
2013-05-20_17:50:13.04851       at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)                                                                                           
2013-05-20_17:50:13.04851       at scala.collection.immutable.List.foreach(List.scala:45)                                                                                                                   
2013-05-20_17:50:13.04851       at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)                                                                                                    
2013-05-20_17:50:13.04852       at scala.collection.immutable.List.map(List.scala:45)                                                                                                                       
2013-05-20_17:50:13.04852       at kafka.tools.MirrorMaker$.main(MirrorMaker.scala:118)                                                                                                                     
2013-05-20_17:50:13.04852       at kafka.tools.MirrorMaker.main(MirrorMaker.scala)                                                                                                                          
                                                                                                                                                                                                            
[td2] A consumer fetcher thread blocked on full queue.                                                                                                                                                      
                                                                                                                                                                                                            
2013-05-20_17:50:13.04703 ""ConsumerFetcherThread-xxxx-1368836182178-2009023c-0-3248"" prio=10 tid=0x00007f57a4010800 nid=0x3920 waiting on condition [0x00                                                   
007f58316ae000]                                                                                                                                                                                             
2013-05-20_17:50:13.04703    java.lang.Thread.State: WAITING (parking)                                                                                                                                      
2013-05-20_17:50:13.04703       at sun.misc.Unsafe.park(Native Method)                                                                                                                                      
2013-05-20_17:50:13.04704       - parking to wait for  <0x00007f586381d6c0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)                                                       
2013-05-20_17:50:13.04704       at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)                                                                                                        
2013-05-20_17:50:13.04704       at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)                                                        
2013-05-20_17:50:13.04704       at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:306)                                                                                               
2013-05-20_17:50:13.04704       at kafka.consumer.PartitionTopicInfo.enqueue(PartitionTopicInfo.scala:60)                                                                                                   
2013-05-20_17:50:13.04705       at kafka.consumer.ConsumerFetcherThread.processPartitionData(ConsumerFetcherThread.scala:50)                                                                                
2013-05-20_17:50:13.04706       at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$4.apply(AbstractFetcherThread.scala:131)                                                                 
2013-05-20_17:50:13.04707       at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$4.apply(AbstractFetcherThread.scala:112)                                                                 
2013-05-20_17:50:13.04708       at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)                                                                                                               
2013-05-20_17:50:13.04709       at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:112)                                                                                  
2013-05-20_17:50:13.04709       at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:88)                                                                                                
2013-05-20_17:50:13.04709       at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)                                                                                                          
2                                                                                                                                                                                                           
                                                                                                                                                                                                            
[td3] Second watch-triggered rebalance                                                                                                                                                                      
                                                                                                                                                                                                            
2013-05-20_17:50:13.04725 ""xxxx-1368836182178-2009023c_watcher_executor"" prio=10 tid=0x00007f5960777800 nid=0x37af waiting on condition [0x00007f58318b00                                                   
00]                                                                                                                                                                                                         
2013-05-20_17:50:13.04725    java.lang.Thread.State: WAITING (parking)                                                                                                                                      
2013-05-20_17:50:13.04726       at sun.misc.Unsafe.park(Native Method)                                                                                                                                      
2013-05-20_17:50:13.04726       - parking to wait for  <0x00007f5863728de8> (a java.util.concurrent.CountDownLatch$Sync)                                                                                    
2013-05-20_17:50:13.04726       at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)                                                                                                        
2013-05-20_17:50:13.04727       at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)                                                         
2013-05-20_17:50:13.04727       at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)                                                  
2013-05-20_17:50:13.04728       at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)                                                   
2013-05-20_17:50:13.04728       at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:207)                                                                                                       
2013-05-20_17:50:13.04729       at kafka.utils.ShutdownableThread.shutdown(ShutdownableThread.scala:36)                                                                                                     
2013-05-20_17:50:13.04729       at kafka.consumer.ConsumerFetcherManager.stopConnections(ConsumerFetcherManager.scala:125)                                                                                  
2013-05-20_17:50:13.04730       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$closeFetchersForQueues(ZookeeperConsumerCo
nnector.scala:486)                                                                                                                                                                                          
2013-05-20_17:50:13.04730       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.closeFetchers(ZookeeperConsumerConnector.scala:523)                                                       
2013-05-20_17:50:13.04731       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$rebalance(ZookeeperConsumerConnector.scala
:420)                                                                                                                                                                                                       
2013-05-20_17:50:13.04731       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener$$anonfun$syncedRebalance$1.apply$mcVI$sp(ZookeeperConsumerConnector.scala:373)                            
2013-05-20_17:50:13.04732       at scala.collection.immutable.Range$ByOne$class.foreach$mVc$sp(Range.scala:282)                                                                                             
2013-05-20_17:50:13.04733       at scala.collection.immutable.Range$$anon$2.foreach$mVc$sp(Range.scala:265)                                                                                                 
2013-05-20_17:50:13.04733       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.syncedRebalance(ZookeeperConsumerConnector.scala:368)                                                     
2013-05-20_17:50:13.04733       - locked <0x00007f58637dfe10> (a java.lang.Object)                                                                                                                          
2013-05-20_17:50:13.04734       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener$$anon$1.run(ZookeeperConsumerConnector.scala:325)                                                         
                                                                                                                                                                                                            
[td4] leader-finder-thread still trying to process partitions without leader, blocked on the partitionMapLock held by processPartitionData in td2.                                                          
                                                                                                                                                                                                            
2013-05-20_17:50:13.04712 ""xxxx-1368836182178-2009023c-leader-finder-thread"" prio=10 tid=0x00007f57b0027800 nid=0x38d8 waiting on condition [0x00007f5831                                                   
7af000]                                                                                                                                                                                                     
2013-05-20_17:50:13.04712    java.lang.Thread.State: WAITING (parking)                                                                                                                                      
2013-05-20_17:50:13.04713       at sun.misc.Unsafe.park(Native Method)                                                                                                                                      
2013-05-20_17:50:13.04713       - parking to wait for  <0x00007f586375e3d8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)                                                                        
2013-05-20_17:50:13.04713       at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)                                                                                                        
2013-05-20_17:50:13.04714       at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)                                                         
2013-05-20_17:50:13.04714       at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireInterruptibly(AbstractQueuedSynchronizer.java:867)                                                        
2013-05-20_17:50:13.04717       at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1201)                                                         
2013-05-20_17:50:13.04718       at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:312)                                                                                       
2013-05-20_17:50:13.04718       at kafka.server.AbstractFetcherThread.addPartition(AbstractFetcherThread.scala:173)                                                                                         
2013-05-20_17:50:13.04719       at kafka.server.AbstractFetcherManager.addFetcher(AbstractFetcherManager.scala:48)                                                                                          
2013-05-20_17:50:13.04719       - locked <0x00007f586374b040> (a java.lang.Object)                                                                                                                          
2013-05-20_17:50:13.04719       at kafka.consumer.ConsumerFetcherManager$LeaderFinderThread$$anonfun$doWork$4.apply(ConsumerFetcherManager.scala:83)                                                        
2013-05-20_17:50:13.04720       at kafka.consumer.ConsumerFetcherManager$LeaderFinderThread$$anonfun$doWork$4.apply(ConsumerFetcherManager.scala:79)                                                        
2013-05-20_17:50:13.04721       at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)                                                                                              
2013-05-20_17:50:13.04721       at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)                                                                                              
2013-05-20_17:50:13.04721       at scala.collection.Iterator$class.foreach(Iterator.scala:631)                                                                                                              
2013-05-20_17:50:13.04722       at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)                                                                                                  
2013-05-20_17:50:13.04723       at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)                                                                                               
2013-05-20_17:50:13.04723       at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)                                                                                                          
2013-05-20_17:50:13.04723       at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)                                                                                                               
2013-05-20_17:50:13.04724       at kafka.consumer.ConsumerFetcherManager$LeaderFinderThread.doWork(ConsumerFetcherManager.scala:79)                                                                         
2013-05-20_17:50:13.04724       at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)                                                                                                          
",,jjkoshy,junrao,nehanarkhede,rekhajoshm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/13 21:48;jjkoshy;KAFKA-914-v1.patch;https://issues.apache.org/jira/secure/attachment/12584179/KAFKA-914-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,328927,,,Fri Feb 12 23:14:17 UTC 2016,,,,,,,,,,"0|i1kr1j:",329269,,,,,,,,,,,,,,,,,,,,"21/May/13 17:18;jjkoshy;One more point: [td3] above does not need to originate from a watcher-triggered rebalance. The initial rebalance can also run into the same deadlock. i.e., as long as one or more watcher-triggered rebalances succeed and start fetchers prior to the initial rebalance, we may end up in this wedged state. E.g., on another instance I saw [td3] but on the main thread:

2013-05-21_17:07:14.34308 ""main"" prio=10 tid=0x00007f5e34008000 nid=0x4e49 waiting on condition [0x00007f5e3b410000]
2013-05-21_17:07:14.34308    java.lang.Thread.State: WAITING (parking)
2013-05-21_17:07:14.34309       at sun.misc.Unsafe.park(Native Method)
2013-05-21_17:07:14.34309       - parking to wait for  <0x00007f5d36d99fa0> (a java.util.concurrent.CountDownLatch$Sync)
2013-05-21_17:07:14.34309       at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
2013-05-21_17:07:14.34310       at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
2013-05-21_17:07:14.34310       at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:969)
2013-05-21_17:07:14.34310       at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1281)
2013-05-21_17:07:14.34311       at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:207)
2013-05-21_17:07:14.34312       at kafka.utils.ShutdownableThread.shutdown(ShutdownableThread.scala:36)
2013-05-21_17:07:14.34313       at kafka.consumer.ConsumerFetcherManager.stopConnections(ConsumerFetcherManager.scala:125)
2013-05-21_17:07:14.34313       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$closeFetchersForQueues(ZookeeperConsumerCo
nnector.scala:486)
2013-05-21_17:07:14.34313       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.closeFetchers(ZookeeperConsumerConnector.scala:523)
2013-05-21_17:07:14.34314       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$rebalance(ZookeeperConsumerConnector.scala
:420)
2013-05-21_17:07:14.34314       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener$$anonfun$syncedRebalance$1.apply$mcVI$sp(ZookeeperConsumerConnector.scala:373)
2013-05-21_17:07:14.34315       at scala.collection.immutable.Range$ByOne$class.foreach$mVc$sp(Range.scala:282)
2013-05-21_17:07:14.34315       at scala.collection.immutable.Range$$anon$2.foreach$mVc$sp(Range.scala:265)
2013-05-21_17:07:14.34316       at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.syncedRebalance(ZookeeperConsumerConnector.scala:368)
2013-05-21_17:07:14.34316       - locked <0x00007f5d36d4b2e0> (a java.lang.Object)
2013-05-21_17:07:14.34317       at kafka.consumer.ZookeeperConsumerConnector.kafka$consumer$ZookeeperConsumerConnector$$reinitializeConsumer(ZookeeperConsumerConnector.scala:678)
2013-05-21_17:07:14.34317       at kafka.consumer.ZookeeperConsumerConnector$WildcardStreamsHandler.<init>(ZookeeperConsumerConnector.scala:712)
2013-05-21_17:07:14.34318       at kafka.consumer.ZookeeperConsumerConnector.createMessageStreamsByFilter(ZookeeperConsumerConnector.scala:140)
2013-05-21_17:07:14.34318       at kafka.tools.MirrorMaker$$anonfun$4.apply(MirrorMaker.scala:118)
2013-05-21_17:07:14.34318       at kafka.tools.MirrorMaker$$anonfun$4.apply(MirrorMaker.scala:118)
2013-05-21_17:07:14.34319       at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
2013-05-21_17:07:14.34319       at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
2013-05-21_17:07:14.34319       at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
2013-05-21_17:07:14.34320       at scala.collection.immutable.List.foreach(List.scala:45)
2013-05-21_17:07:14.34320       at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
2013-05-21_17:07:14.34320       at scala.collection.immutable.List.map(List.scala:45)
2013-05-21_17:07:14.34321       at kafka.tools.MirrorMaker$.main(MirrorMaker.scala:118)
2013-05-21_17:07:14.34322       at kafka.tools.MirrorMaker.main(MirrorMaker.scala)
;;;","21/May/13 21:48;jjkoshy;Patch with the mentioned fix.

1 - I added comments with some detail since the manager/fetcher/connector interaction is very tricky.
2 - Passing through throwables while shutting down. The isRunning check is probably unnecessary, but safer to keep.
3 - Made the following changes to the mirrormaker - I can put that in a separate jira as well.
  a - Currently if no streams are created, the mirrormaker doesn't quit. Setting streams to empty/nil fixes that issue.
  b - If a consumer-side exception (e.g., iterator timeout) gets thrown the mirror-maker does not exit. Addressed this by awaiting on the consumer threads at the end of the main method.

;;;","21/May/13 23:51;junrao;Thanks for the patch. Looks good. +1. One minor comment: The following statement in the catch clause in MirrorMaker is unnecessary.
        streams = Nil
;;;","22/May/13 19:59;nehanarkhede;Thanks for the patch! Good catch, +1;;;","22/May/13 23:27;jjkoshy;Thanks for the review. Committed after removing the unnecessary assignment in MirrorMaker.;;;","12/Feb/16 23:14;rekhajoshm;Hi,
Facing similar issue; raised in https://issues.apache.org/jira/browse/KAFKA-3238
Thanks
Rekha
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in controlled shutdown logic in controller leads to controller not sending out some state change request ,KAFKA-911,12648505,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,20/May/13 21:16,03/Jul/13 22:00,14/Jul/23 05:39,03/Jul/13 22:00,0.8.0,,,,,,,,,,controller,,,0,kafka-0.8,p1,,"The controlled shutdown logic in the controller first tries to move the leaders from the broker being shutdown. Then it tries to remove the broker from the isr list. During that operation, it does not synchronize on the controllerLock. This causes a race condition while dispatching data using the controller's channel manager.",,jjkoshy,junrao,nehanarkhede,sriramsub,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/13 17:18;nehanarkhede;kafka-911-v1.patch;https://issues.apache.org/jira/secure/attachment/12584342/kafka-911-v1.patch","28/May/13 17:14;nehanarkhede;kafka-911-v2.patch;https://issues.apache.org/jira/secure/attachment/12585046/kafka-911-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,328860,,,Wed Jul 03 21:59:58 UTC 2013,,,,,,,,,,"0|i1kqmn:",329202,,,,,,,,,,,,,,,,,,,,"22/May/13 17:18;nehanarkhede;The root cause of this bug was a race condition while using the ControllerBrokerRequestBatch that assumes synchronization at the caller. This patch synchronizes access to the ControllerBrokerRequestBatch while sending out the StopReplicaRequest. 

While working on this and testing the patch, I noticed some inefficiency with the controller shutdown API. When we move the leader for a partition one at a time, we also remove the broker being shutdown from the ISR. This involves at least one read and one write per partition to zookeeper, sometimes more. Besides being slow, this operation is not effective. This is because the broker being shutdown still has alive fetchers to the new leader before it receives and acts on the StopReplicaRequest. So the new leader adds it back to the ISR anyways. 

Then I thought I could move it to the end of the controlled shutdown API where we check if all partitions are successfully moved, then send StopReplicaRequest to the broker being shut down. Right after this, we could move the replica to Offline and remove it from ISR as part of that operation. Even if we can invoke this operation as a batch, it will still be slow since the zookeeper reads/writes will happen serially. Also, I realized this is not required as well for 2 reasons -

1. Since the shutting broker is sent the StopReplicaRequest, it will stop the fetcher and fall out of ISR. Until then, as long as the controller doesn't failover, it will not be elected as the leader, even if it is in the ISR, since it is one of the shuttingDownBrokers.

2. Even if the controller fails over, by the time the new controller has initialized and is ready to serve, the StopReplicaRequest would've ensured that the shutdown broker is no longer in the ISR. And until the controller fails over, there cannot be any leader election anyway.

I've tested this patch on a 7 node cluster that continuously gets rolling bounced and has ~100 producers sending production data to it with ~1000 consumers consuming data from it. ;;;","22/May/13 17:21;nehanarkhede;testShutdownBroker() testcase in AdminTest will fail with this patch since it assumes that the controlled shutdown logic will shrink the ISR proactively. I will fix the test if the changes in this patch of not shrinking the ISR are acceptable.;;;","23/May/13 15:16;junrao;If we just stop the replica to be shut down without sending a reduced ISR to the leader, it will take replicaLagTimeMaxMs (defaults to 10s) before the leader realize that the follower is gone. Before that, no new messages can be committed. The idea of letting the controller send a reduced ISR to the leader is to allow the leader to commit new messages sooner. Not very sure if the existing logic does this effectively though. It seems to me that it's better if we stop the shutdown replica one at a time after the leader is moved. Maybe Joel can comment?;;;","24/May/13 16:43;nehanarkhede;You are right that we can send the reduced ISR request to the leader, but that is independent of removing the shutting down broker from the ISR in zookeeper. I'm arguing that the zookeeper write is unnecessary. To handle the issue you described, we can send a leader and isr request just to the leader with the reduced isr.;;;","24/May/13 18:41;jjkoshy;I had to revisit the notes from KAFKA-340. I think this was touched upon. i.e., the fact that the current implementation's attempt to shrink ISR may be ineffective for partitions whose leadership has been moved from the current broker - https://issues.apache.org/jira/browse/KAFKA-340?focusedCommentId=13483478&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13483478

<quote>
> 3.4 What is the point of sending leader and isr request at the end of shutdownBroker, since the OfflineReplica state 
> change would've taken care of that anyway. It seems like you just need to send the stop replica request with the delete 
> partitions flag turned off, no ? 

I still need (as an optimization) to send the leader and isr request to the leaders of all partitions that are present 
on the shutting down broker so it can remove the shutting down broker from its inSyncReplicas cache 
(in Partition.scala) so it no longer waits for acks from the shutting down broker if a producer request's num-acks is 
set to -1. Otherwise, we have to wait for the leader to ""organically"" shrink the ISR. 

This also applies to partitions which are moved (i.e., partitions for which the shutting down broker was the leader): 
the ControlledShutdownLeaderSelector needs to send the updated leaderAndIsr request to the shutting down broker as well 
(to tell it that it is no longer the leader) at which point it will start up a replica fetcher and re-enter the ISR. 
So in fact, there is actually not much point in removing the ""current leader"" from the ISR in the 
ControlledShutdownLeaderSelector.selectLeader. 
</quote>

and 

https://issues.apache.org/jira/browse/KAFKA-340?focusedCommentId=13484727&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13484727
(I don't think I actually filed that jira though.)
;;;","28/May/13 17:14;nehanarkhede;I agree with Joel's suggestion. Removing the shutting down brokers from the ISR is better. This patch sends the LeaderAndIsrRequest with the reduced isr to the new leader for the partitions on the shutting down brokers. This ensures the leader will remove the shutting down broker from the isr in zookeeper. This also makes it unnecessary for the shrunk isr zookeeper write to happen during the controlled shutdown on the controller. ;;;","28/May/13 17:22;sriramsub;I suggest we wait for my patch. My patch changes quite a bit of this logic and it just adds to the merge problem.;;;","03/Jul/13 21:59;sriramsub;This has been fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Write duplicate messages during broker failure,KAFKA-908,12647977,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,nehanarkhede,tscurtu,tscurtu,16/May/13 15:43,31/Aug/17 18:34,14/Jul/23 05:39,31/Aug/17 18:34,0.8.0,,,,,,,,,,replication,,,0,,,,"Reproduction steps:
1. Start a multi-broker quorum (e.g. 3 brokers)
2. Create a multi-replica topic (e.g. 3 replicas)
3. Start an async performance producer with a fixed number of messages to produce
4. Force kill a partition leader broker using SIGKILL (no clean shutdown) - make sure you kill it during actual writes
5. Wait for the producer to stop
6. Read from the topic from the beginning - there will a small amount of duplicate messages

Reproduction rate: sometimes
",,chaitanyap,omkreddy,tscurtu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,328333,,,Thu Aug 31 18:34:04 UTC 2017,,,,,,,,,,"0|i1kndz:",328677,,,,,,,,,,,,,,,,,,,,"31/Aug/17 18:34;omkreddy;Closing inactive issue. Pl reopen if you think the issue still exists;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
controller needs to close socket channel to brokers on exception,KAFKA-907,12647856,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,15/May/13 23:03,22/May/13 23:31,14/Jul/23 05:39,22/May/13 23:24,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"When the controller sends a request to a broker (e.g., leaderAndIsrRequest), it may hit a exception (e.g. SocketTimeException). When this happens, the socket channel needs to be closed and recreated.",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/May/13 23:04;junrao;kafka-907.patch;https://issues.apache.org/jira/secure/attachment/12583396/kafka-907.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,328212,,,Wed May 22 23:24:17 UTC 2013,,,,,,,,,,"0|i1kmn3:",328556,,,,,,,,,,,,,,,,,,,,"15/May/13 23:04;junrao;Attach a patch.;;;","22/May/13 17:32;nehanarkhede;Thanks for the patch. The following if statement is unnecessary since that condition gets checked in the connect() API of BlockingChannel.

    if(!channel.isConnected) {
      channel.connect()
    }

Other than that, +1;;;","22/May/13 23:24;junrao;Thanks for the review. Addressed the comment and committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logs can have same offsets causing recovery failure,KAFKA-905,12647785,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriramsub,sriramsub,sriramsub,15/May/13 17:17,10/Jun/13 17:20,14/Jul/23 05:39,04/Jun/13 00:05,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"Consider the following scenario - 

L                       F
1  m1,m2        1 m1,m2
3  m3,m4        3 m3,m4
5  m5,m6        5 m5,m6

HW = 6           HW = 4

Follower goes down and comes back up. Truncates its log to HW

L                             F
1  m1,m2               1 m1,m2
3  m3,m4               3 m3,m4
5  m5,m6

HW = 6            HW = 4

Before follower catches up with the leader, leader goes down and follower becomes the leader. It then gets new messages

F                       L
1  m1,m2        1  m1,m2
3  m3,m4        3  m3,m4
5  m5,m6      10 m5-m10

HW=6              HW=4

follower fetches from offset 7. Since offset 7 is within the compressed message 10 in the leader, the whole message chunk is sent to the follower

F                        L      
1   m1,m2         1  m1,m2
3   m3,m4         3  m3,m4  
5   m5,m6       10  m5-m10
10 m5-m10

HW=4               HW=10

The follower logs now contain the same offsets. On recovery, re-indexing will fail due to repeated offsets.

Possible ways to fix this - 
1. The fetcher thread can do deep iteration instead of shallow iteration and drop the offsets that are less than the log end offset. This would however incur performance hit.
2. To optimize step 1, we could do the deep iteration till the logical offset of the fetched message set is greater than the log end offset of the follower log and then switch to shallow iteration.
3. On recovery we just truncate the active segment and refetch the data.

All the above 3 steps are hacky. The right fix is to ensure we never corrupt the logs. We can incur data loss but should not compromise consistency. For 0.8, the easiest and simplest fix would be 3. ",,jkreps,junrao,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/13 00:24;sriramsub;KAFKA-905-trunk.patch;https://issues.apache.org/jira/secure/attachment/12586003/KAFKA-905-trunk.patch","03/Jun/13 23:44;sriramsub;KAFKA-905-v2.patch;https://issues.apache.org/jira/secure/attachment/12585994/KAFKA-905-v2.patch","30/May/13 22:35;sriramsub;KAFKA-905.patch;https://issues.apache.org/jira/secure/attachment/12585495/KAFKA-905.patch","15/May/13 17:24;sriramsub;KAFKA-905.rtf;https://issues.apache.org/jira/secure/attachment/12583339/KAFKA-905.rtf",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,328141,,,Mon Jun 10 17:20:34 UTC 2013,,,,,,,,,,"0|i1km7b:",328485,,,,,,,,,,,,,,,,,,,,"15/May/13 17:24;sriramsub;Attached a file that has the right formatting. JIRA seems to swallow the spaces.;;;","30/May/13 22:35;sriramsub;We have decided to fix this using step 3 above. We just truncate the active segment to the start offset and re-fetch the data.;;;","31/May/13 15:12;junrao;Thanks for the patch. Looks good. Some minor comments:

1. Log: 
1.1 logSegments.get(logSegments.size - 1) is used twice when handling InvalidOffsetExcetpion. Could we just call it once and reuse the result?
1.2 The info logging should probably be warning. Also, it would be useful to log the dir name so that we know the topic/partition.

2. OffsetIndex: It would be useful to include the full file name in the message of the exception so that we know the topic/partition.

3. The patch doesn't compile since it's missing the new file InvalidOffsetExcetpion.;;;","31/May/13 17:33;nehanarkhede;Great catch, patch looks good except the new exception file is missing.;;;","31/May/13 18:56;jkreps;This patch looks good. One thing worth pointing out is that any corruption will result in deleting the corrupt file which will potentially make it hard to debug.

I am +1 if we have also a patch for trunk.;;;","03/Jun/13 23:44;sriramsub;- made the logging changes
- added the missing file;;;","04/Jun/13 00:05;nehanarkhede;Thanks for v2, committed it to 0.8;;;","04/Jun/13 00:24;sriramsub;changes for trunk;;;","10/Jun/13 17:07;sriramsub;Ping...Take a look at the patch for trunk.;;;","10/Jun/13 17:20;junrao;I have a pending merge from trunk (kafka-896). I'd like to get that in before this patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[0.8.0 - windows]  FATAL - [highwatermark-checkpoint-thread1] (Logging.scala:109) - Attempt to swap the new high watermark file with the old one failed,KAFKA-903,12647144,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,reefedjib,reefedjib,11/May/13 01:18,28/Sep/17 01:27,14/Jul/23 05:39,03/Jun/13 23:52,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"This FATAL shuts down both brokers on windows, 
{2013-05-10 18:23:57,636} DEBUG [local-vat] (Logging.scala:51) - Sending 1 
messages with no compression to [robert_v_2x0,0]
{2013-05-10 18:23:57,637} DEBUG [local-vat] (Logging.scala:51) - Producer 
sending messages with correlation id 178 for topics [robert_v_2x0,0] to 
broker 1 on 192.168.1.100:9093
{2013-05-10 18:23:57,689} FATAL [highwatermark-checkpoint-thread1] 
(Logging.scala:109) - Attempt to swap the new high watermark file with the 
old one failed
{2013-05-10 18:23:57,739}  INFO [Thread-4] (Logging.scala:67) - [Kafka 
Server 0], shutting down

Furthermore, attempts to restart them fail, with the following log:
{2013-05-10 19:14:52,156}  INFO [Thread-1] (Logging.scala:67) - [Kafka Server 0], started
{2013-05-10 19:14:52,157}  INFO [ZkClient-EventThread-32-localhost:2181] (Logging.scala:67) - New leader is 0
{2013-05-10 19:14:52,193} DEBUG [ZkClient-EventThread-32-localhost:2181] (ZkEventThread.java:79) - Delivering event #1 done
{2013-05-10 19:14:52,193} DEBUG [ZkClient-EventThread-32-localhost:2181] (ZkEventThread.java:69) - Delivering event #4 ZkEvent[Data of /controller_epoch changed sent to kafka.controller.ControllerEpochListener@5cb88f42]
{2013-05-10 19:14:52,210} DEBUG [SyncThread:0] (FinalRequestProcessor.java:78) - Processing request:: sessionid:0x13e9127882e0001 type:exists cxid:0x1d zxid:0xfffffffffffffffe txntype:unknown reqpath:/controller_epoch
{2013-05-10 19:14:52,210} DEBUG [SyncThread:0] (FinalRequestProcessor.java:160) - sessionid:0x13e9127882e0001 type:exists cxid:0x1d zxid:0xfffffffffffffffe txntype:unknown reqpath:/controller_epoch
{2013-05-10 19:14:52,213} DEBUG [Thread-1-SendThread(localhost:2181)] (ClientCnxn.java:838) - Reading reply sessionid:0x13e9127882e0001, packet:: clientPath:null serverPath:null finished:false header:: 29,3  replyHeader:: 29,37,0  request:: '/controller_epoch,T  response:: s{16,36,1368231712816,1368234889961,1,0,0,0,1,0,16} 
{2013-05-10 19:14:52,219}  INFO [Thread-5] (Logging.scala:67) - [Kafka Server 0], shutting down
","Windows 7 with SP 1; jdk 7_0_17; scala-library-2.8.2, probably copied on 4/30. kafka-0.8, built current on 4/30.

-rwx------+ 1 reefedjib None   41123 Mar 19  2009 commons-cli-1.2.jar
-rwx------+ 1 reefedjib None   58160 Jan 11 13:45 commons-codec-1.4.jar
-rwx------+ 1 reefedjib None  575389 Apr 18 13:41 commons-collections-3.2.1.jar
-rwx------+ 1 reefedjib None  143847 May 21  2009 commons-compress-1.0.jar
-rwx------+ 1 reefedjib None   52543 Jan 11 13:45 commons-exec-1.1.jar
-rwx------+ 1 reefedjib None   57779 Jan 11 13:45 commons-fileupload-1.2.1.jar
-rwx------+ 1 reefedjib None  109043 Jan 20  2008 commons-io-1.4.jar
-rwx------+ 1 reefedjib None  279193 Jan 11 13:45 commons-lang-2.5.jar
-rwx------+ 1 reefedjib None   60686 Jan 11 13:45 commons-logging-1.1.1.jar
-rwx------+ 1 reefedjib None 1891110 Apr 18 13:41 guava-13.0.1.jar
-rwx------+ 1 reefedjib None  206866 Apr  7 21:24 jackson-core-2.1.4.jar
-rwx------+ 1 reefedjib None  232245 Apr  7 21:24 jackson-core-asl-1.9.12.jar
-rwx------+ 1 reefedjib None   69314 Apr  7 21:24 jackson-dataformat-smile-2.1.4.jar
-rwx------+ 1 reefedjib None  780385 Apr  7 21:24 jackson-mapper-asl-1.9.12.jar
-rwx------+ 1 reefedjib None   47913 May  9 23:39 jopt-simple-3.0-rc2.jar
-rwx------+ 1 reefedjib None 2365575 Apr 30 13:06 kafka_2.8.0-0.8.0-SNAPSHOT.jar
-rwx------+ 1 reefedjib None  481535 Jan 11 13:46 log4j-1.2.16.jar
-rwx------+ 1 reefedjib None   20647 Apr 18 13:41 log4j-over-slf4j-1.6.6.jar
-rwx------+ 1 reefedjib None  251784 Apr 18 13:41 logback-classic-1.0.6.jar
-rwx------+ 1 reefedjib None  349706 Apr 18 13:41 logback-core-1.0.6.jar
-rwx------+ 1 reefedjib None   82123 Nov 26 13:11 metrics-core-2.2.0.jar
-rwx------+ 1 reefedjib None 1540457 Jul 12  2012 ojdbc14.jar
-rwx------+ 1 reefedjib None 6418368 Apr 30 08:23 scala-library-2.8.2.jar
-rwx------+ 1 reefedjib None 3114958 Apr  2 07:47 scalatest_2.10-1.9.1.jar
-rwx------+ 1 reefedjib None   25962 Apr 18 13:41 slf4j-api-1.6.5.jar
-rwx------+ 1 reefedjib None   62269 Nov 29 03:26 zkclient-0.2.jar
-rwx------+ 1 reefedjib None  601677 Apr 18 13:41 zookeeper-3.3.3.jar
",jkreps,junrao,nehanarkhede,reefedjib,seglo,sriramsub,tnachen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-876,,,,,,,,,,,,,,,,,,,"11/May/13 05:04;junrao;kafka-903.patch;https://issues.apache.org/jira/secure/attachment/12582769/kafka-903.patch","11/May/13 22:09;junrao;kafka-903_v2.patch;https://issues.apache.org/jira/secure/attachment/12582802/kafka-903_v2.patch","29/May/13 04:47;junrao;kafka-903_v3.patch;https://issues.apache.org/jira/secure/attachment/12585155/kafka-903_v3.patch","11/May/13 22:10;junrao;kafka_2.8.0-0.8.0-SNAPSHOT.jar;https://issues.apache.org/jira/secure/attachment/12582803/kafka_2.8.0-0.8.0-SNAPSHOT.jar",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,327501,,,Mon Jun 03 23:52:27 UTC 2013,,,,,,,,,,"0|i1ki93:",327845,,,,,,,,,,,,,,,,,,,,"11/May/13 05:04;junrao;Attach a patch. Rob, could you give it a try?;;;","11/May/13 05:28;reefedjib;I'd love too.  Could you build me a jar and email it too me, please?  :)
;;;","11/May/13 05:53;junrao;Attach the kafka jar with the patch.;;;","11/May/13 06:32;reefedjib;That jar is missing a lot - it only has a MANIFEST.  Should I be able to do anything with it?;;;","11/May/13 15:39;junrao;Attach the right jar this time.;;;","11/May/13 16:09;reefedjib;It still fails.  Where should I look in the jar to ensure the patch is in there?  Which class method?   Also, could you generate a jar for me with sources?

thanks a lot and happy saturday!


{2013-05-11 10:00:58,630} DEBUG [local-vat] (Logging.scala:51) - Producer sent messages with correlation id 232 for topics [robert_v_2x0,0] to broker 0 on 192.168.1.100:9092
{2013-05-11 10:00:58,637} DEBUG [local-vat] (Logging.scala:51) - Getting broker partition info for topic robert_v_2x0
{2013-05-11 10:00:58,638} DEBUG [local-vat] (Logging.scala:51) - Partition [robert_v_2x0,0] has leader 0
{2013-05-11 10:00:58,639} DEBUG [local-vat] (Logging.scala:51) - Broker partitions registered for topic: robert_v_2x0 are 0
{2013-05-11 10:00:58,639} DEBUG [local-vat] (Logging.scala:51) - Sending 1 messages with no compression to [robert_v_2x0,0]
{2013-05-11 10:00:58,640} DEBUG [local-vat] (Logging.scala:51) - Producer sending messages with correlation id 234 for topics [robert_v_2x0,0] to broker 0 on 192.168.1.100:9092
{2013-05-11 10:00:58,648} DEBUG [kafka-request-handler-1] (Logging.scala:51) - [Kafka Request Handler 1 on Broker 0], handles request Request(1,sun.nio.ch.SelectionKeyImpl@2b12cf49,null,1368288058646,/192.168.1.100:60622)
{2013-05-11 10:00:58,649} DEBUG [kafka-request-handler-1] (Logging.scala:51) - Adding index entry 115 => 5220900 to 00000000000000000000.index.
{2013-05-11 10:00:58,650} DEBUG [kafka-request-handler-1] (Logging.scala:51) - Partition [robert_v_2x0,0] on broker 0: Highwatermark for partition [robert_v_2x0,0] updated to 116
{2013-05-11 10:00:58,650} DEBUG [kafka-request-handler-1] (Logging.scala:51) - [KafkaApi-0] Produce to local log in 2 ms
{2013-05-11 10:00:58,656} DEBUG [local-vat] (Logging.scala:51) - Producer sent messages with correlation id 234 for topics [robert_v_2x0,0] to broker 0 on 192.168.1.100:9092
{2013-05-11 10:00:58,666} DEBUG [local-vat] (Logging.scala:51) - Getting broker partition info for topic robert_v_2x0
{2013-05-11 10:00:58,667} DEBUG [local-vat] (Logging.scala:51) - Partition [robert_v_2x0,0] has leader 0
{2013-05-11 10:00:58,667} DEBUG [local-vat] (Logging.scala:51) - Broker partitions registered for topic: robert_v_2x0 are 0
{2013-05-11 10:00:58,669} DEBUG [local-vat] (Logging.scala:51) - Sending 1 messages with no compression to [robert_v_2x0,0]
{2013-05-11 10:00:58,670} DEBUG [local-vat] (Logging.scala:51) - Producer sending messages with correlation id 236 for topics [robert_v_2x0,0] to broker 0 on 192.168.1.100:9092
{2013-05-11 10:00:58,716} DEBUG [kafka-request-handler-0] (Logging.scala:51) - [Kafka Request Handler 0 on Broker 0], handles request Request(1,sun.nio.ch.SelectionKeyImpl@2b12cf49,null,1368288058674,/192.168.1.100:60622)
{2013-05-11 10:00:58,717} DEBUG [kafka-request-handler-0] (Logging.scala:51) - Adding index entry 116 => 5232005 to 00000000000000000000.index.
{2013-05-11 10:00:58,718} DEBUG [kafka-request-handler-0] (Logging.scala:51) - Partition [robert_v_2x0,0] on broker 0: Highwatermark for partition [robert_v_2x0,0] updated to 117
{2013-05-11 10:00:58,718} DEBUG [kafka-request-handler-0] (Logging.scala:51) - [KafkaApi-0] Produce to local log in 2 ms
{2013-05-11 10:00:58,726} FATAL [highwatermark-checkpoint-thread1] (Logging.scala:109) - Attempt to swap the new high watermark file with the old one failed
;;;","11/May/13 19:56;junrao;Hmm, this may have to do with Windows not supporting file.renameTo() if the target file already exists (http://stackoverflow.com/questions/1000183/reliable-file-renameto-alternative-on-windows). We periodically checkpoint high watermarks to disk. The way we do this is to first write all values to a tmp file replication-offset-checkpoint.tmp and then rename the tmp file to replication-offset-checkpoint. This way, if there is any I/O error during checkpointing, we still have the old checkpoint file for use. Not sure what the best way to do this in Windows.

Could you try java 7 and see if it still has the same issue (the above link suggests that it's fixed in jdk 7)?;;;","11/May/13 20:39;reefedjib;I am on jdk 7.0.17,  That thread mentions apache.commons.io.FileUtils.moveFile().   Also, my suggestion to be done with the issue: I know there is a way to detect platform, just write a small strategy pattern for the general platform and for windows and plug her in.;;;","11/May/13 22:09;junrao;Attach patch v2. If a file can't be renamed, it deletes the target file first. The implication is that during a hard crash, if the high watermark file is missing, one has to manually rename it from the temporary high watermark file.;;;","11/May/13 22:10;junrao;Deleted the old jar and a attach a new jar built with patch v2.;;;","12/May/13 21:50;reefedjib;Jun, it looks like this works.  I have 23 MB in the log file, on broker0.  No replication and I guess I am only sending to partition 0.  So, please close this issue.

I am still having trouble with consumption, but given it is a weekend, I am busy with other stuff.  I will dig in to it later and report my progress.  

Thanks for all your help,
rob;;;","16/May/13 18:31;jkreps;I don't think this rename functionality is such a good idea. There are many ways a rename can fail: (1) permissions, (2) disk errors, (3) bad target name, (4) rename is to another volume, etc. Java doesn't differentiate these, so in any of these cases we would then try to delete the file. Not sure this is a good idea.

Another approach would be to add a Utils.IsWindows = System.getProperty(""os.name"").startsWith(""Windows"") and using this to fall back to the funky non-atomic behavior.;;;","16/May/13 20:59;sriramsub;There are two claims in this JIRA -

1. ""this may have to do with Windows not supporting file.renameTo() if the target file already exists""
2. renameTo is not atomic in windows

Claim 1 is wrong. MoveFileEx is the native api that helps you to rename an existing file. ""MOVEFILE_REPLACE_EXISTING"" is the flag you would use. This might be a bug in the java api or as the SO link indicates, does not work for non empty directories. (http://msdn.microsoft.com/en-us/library/windows/desktop/aa365240(v=vs.85).aspx)

Claim 2 is possible depending on the OS settings. The caller of MoveFileEx is supposed to handle the failure.;;;","28/May/13 22:30;tnachen;Do you know when this is going to be pushed to 0.8 branch?;;;","29/May/13 04:47;junrao;Attach patch v3. 

To address Jay's concern, instead of using a generic renameTo util, only falls back to the non-atomic renameTo in checkpointing the high watermark file. Since both files are in the same dir and we control the naming, those other causes you listed that can fail renameTo won't happen. I didn't do the os level checking since I am not sure it that works well for environments like cygwin. We could guard this under a broker config parameter, but I am not sure if it's worth it.

For Sriram's concern, this seems to be at least a problem for some versions of java on Windows since other projects like Hadoop (https://issues.apache.org/jira/browse/HADOOP-959) have also seen this before.
  ;;;","31/May/13 17:56;nehanarkhede;+1 on v3;;;","03/Jun/13 23:32;jkreps;+1;;;","03/Jun/13 23:52;junrao;Thanks for the review. Committed v3 to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Randomize backoff on the clients for metadata requests,KAFKA-902,12646929,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,granders,nehanarkhede,nehanarkhede,10/May/13 00:38,21/Feb/18 17:27,14/Jul/23 05:39,21/Feb/18 17:27,0.8.0,,,,,,,,,,clients,,,0,newbie,,,"If a Kafka broker dies and there are a large number of clients talking to the Kafka cluster, each of the clients can end up shooting metadata requests at around the same time. It is better to randomize the backoff on the clients so the metadata requests are more evenly spread out",,futtre,granders,jkreps,junrao,nehanarkhede,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-910,KAFKA-3878,,,,,,,,,,,,,,,,,,,,"04/Mar/15 01:09;granders;KAFKA-902.patch;https://issues.apache.org/jira/secure/attachment/12702330/KAFKA-902.patch","02/Mar/15 19:16;granders;KAFKA-902.patch;https://issues.apache.org/jira/secure/attachment/12701950/KAFKA-902.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,327286,,,Wed Feb 21 17:27:41 UTC 2018,,,,,,,,,,"0|i1kgxb:",327630,,,,,,,,,,,,,,,,,,,,"07/Feb/15 22:27;jkreps;This could be accomplished by just adding +/-5 to the various backoff parameters specified in the producer and consumer.

However we should still probably interpret 0 as a strict ""don't backoff"" instruction.;;;","02/Mar/15 19:16;granders;Created reviewboard https://reviews.apache.org/r/31633/diff/
 against branch origin/trunk;;;","03/Mar/15 03:46;granthenke;A few thoughts on the patch:
Should the ""jitter"" be added to 'reconnect.backoff.ms' too? 
Would there ever be a good reason to change the jitter value from 10? Should it be added to the CommonClientConfigs?



;;;","03/Mar/15 04:37;jkreps;This looks good to me. I'd second Grant's comments:
1. I agree we should probably make it configurable and mark the configuration low importance. This kind of configuration is hyper-annoying because no one will ever set it but it's probably the right thing to do.
2. We should definitely apply the same thing to the reconnect backoff as well as metadata max age (if everyone disconnects at time X they will all expire their metadata at X+metadata.max.age.ms so jittering that will help too).

Another thing is that this jitter is only additive, so if you configure a backoff of 10 ms, your observed backoff time will be 15 ms. Also 10 ms will be a bit large if you configure a 1 ms backoff and zero ends up being kind of magical. I don't think this is really too terrible and it is simple, so maybe we should just leave it.

Another possibility would be something like using a jitter that is a random int in +/- min(20, 0.2 * backoff_ms). ;;;","04/Mar/15 00:22;granders;Thanks for the feedback.

Is there any reason to keep 0 -> 0 behavior? In other words, is there any harm to making 0 less magical and perturbing it just like any other value?

Also, I like the idea of scaling the upper bound on jitter with backoff_ms, though it seems like we still want some amount of jitter even if backoff_ms is small (in your example, if backoff_ms < 5, then jitter is always 0)
In that case, we might want to make sure that the jitter can be non-zero with something like max(3, min(20, 0.2 * backoff_ms))

But then we end up with a couple more semi-arbitrary parameters - a scaling parameter and a hard minimum.;;;","04/Mar/15 01:09;granders;Created reviewboard https://reviews.apache.org/r/31715/diff/
 against branch origin/trunk;;;","12/Mar/15 21:53;junrao;Thanks for the patch. A few comments.

1. I feel that it's simpler to express jitter just as a percentage of the backoff. So the actual backoff will be backoff (1 + jitter-percent). This is simpler and it covers the case when backoff = 0 naturally, i.e., if backoff is 0, no jitter is added. It's true that if the backoff is small, no jitter may be added. However, if backoff is really small, adding a small jitter may not spread the retries enough anyway.

2. We probably need to add jitter as a configuration. Should we add a single jitter config for all backoffs or one per backoff? Probably a single jitter config is enough.

3. On the broker side, we already have a config for log rolling jittter. It would be good to make the jitter implementation consistent. So, if we pick a new jitter strategy for the clients, we should use the same in the broker as well. This may mean that we will change some of the broker configs. This change will be of low impact, but we probably should file a KIP to keep everyone informed.;;;","21/Feb/18 17:27;omkreddy;Resolving this Duplicate of KAFKA-3878. Pls reopen if you think otherwise.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka server can become unavailable if clients send several metadata requests,KAFKA-901,12646927,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,10/May/13 00:34,21/May/13 17:07,14/Jul/23 05:39,18/May/13 00:00,0.8.0,,,,,,,,,,replication,,,0,,,,"Currently, if a broker is bounced without controlled shutdown and there are several clients talking to the Kafka cluster, each of the clients realize the unavailability of leaders for some partitions. This leads to several metadata requests sent to the Kafka brokers. Since metadata requests are pretty slow, all the I/O threads quickly become busy serving the metadata requests. This leads to a full request queue, that stalls handling of finished responses since the same network thread handles requests as well as responses. In this situation, clients timeout on metadata requests and send more metadata requests. This quickly makes the Kafka cluster unavailable. ",,jjkoshy,junrao,nehanarkhede,reefedjib,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/13 17:29;nehanarkhede;kafka-901-followup.patch;https://issues.apache.org/jira/secure/attachment/12583861/kafka-901-followup.patch","21/May/13 16:18;nehanarkhede;kafka-901-followup2.patch;https://issues.apache.org/jira/secure/attachment/12584030/kafka-901-followup2.patch","16/May/13 16:22;nehanarkhede;kafka-901-v2.patch;https://issues.apache.org/jira/secure/attachment/12583495/kafka-901-v2.patch","17/May/13 02:13;nehanarkhede;kafka-901-v4.patch;https://issues.apache.org/jira/secure/attachment/12583603/kafka-901-v4.patch","17/May/13 19:43;nehanarkhede;kafka-901-v5.patch;https://issues.apache.org/jira/secure/attachment/12583681/kafka-901-v5.patch","14/May/13 23:58;nehanarkhede;kafka-901.patch;https://issues.apache.org/jira/secure/attachment/12583245/kafka-901.patch","10/May/13 00:58;nehanarkhede;metadata-request-improvement.patch;https://issues.apache.org/jira/secure/attachment/12582557/metadata-request-improvement.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,327284,,,Tue May 21 17:07:46 UTC 2013,,,,,,,,,,"0|i1kgwv:",327628,,,,,,,,,,,,,,,,,,,,"10/May/13 00:58;nehanarkhede;This patch changes the way a Kafka server handles metadata requests. Metadata requests are currently a bottleneck in the system since the server reads several paths from zookeeper to serve one metadata request. The latency is also proportional to the number of topics in a metadata request. A faster way to serve metadata requests is through the controller. The reason is the controller makes all state change decisions for the cluster, so it has a cache with the latest leadership information. The patch uses the following algorithm to serve metadata requests -

1. If broker is controller, read leadership information from cache and send the response
2. If broker is not controller, instantiate a sync producer to forward metadata request to the controller
3. If broker is not controller and a controller is unavailable, send ControllerNotAvailable error code back to the client

There are some things to note here -

1. How does a broker know who the current controller and its host/port ? If we read from zookeeper, then that makes metadata requests slow, although less slower than before. However, upon controller failover, the controller sends a LeaderAndIsr request to all* brokers. So when a broker receives any state change request from the controller, it stores the controller's host/port in cache from zookeeper. Since state change requests are rare, this is ok

Now there is a corner case we need to take care of. If a new broker is brought up and it doesn't have any partitions assigned to it, it won't receive any LeaderAndIsr request in the current code base. This patch takes care of this by changing controller to always send leader and isr request to newly restarted brokers, even if there are no partitions in the request.

2. What timeout should be used when a broker wants to forward a metadata request to the controller ? 
Since there isn't a way to know the timeout specified by the original metadata request, we can potentially set the socket timeout for the forwarded request to be Integer.MAX. This will ensure that the forwarded request will not prematurely time out. However, we need to ensure that the controller always sends a response back to the broker OR closes the socket. This will prevent the broker's I/O thread from waiting indefinitely for a response from the controller. This also requires the controller to not block when serving metadata from its cache OR it will block some other broker's I/O thread, which is bad.

3. On the controller, should it acquire a lock when reading its cache to serve a metadata request ?

This is not required and is potentially dangerous. It is not required since even if the controller's cache information is undergoing change and we send stale information to the client, it will get an error and retry. Eventually, the cache will be consistent and the accurate metadata will be sent to the client. This is also ok since changes to the controller's cache are relatively rare so chances of stale metadata are low.

4. Do we need to make the timeout for the forwarded metadata request configurable ?

Ideally no, except for unit tests. The reason is unit tests involve shutting down kafka brokers. However, when a broker is shut down in a unit test, it does not close the socket connections associated with that broker. The impact of that is if a controller broker is shut down, it will not respond to few forwarded metadata requests and it will not close the socket. That leads to some other broker indefinitely wait on receiving a response from the controller. This doesn't happen in non unit test environments, since if the broker is shutdown or fails, the processes releases all socket connections associated with it. So the other broker gets a broken pipe exception and doesn't end up waiting forever;;;","10/May/13 17:56;jjkoshy;Haven't looked at the patch yet, but went through the overview. An alternate approach that we may want to consider is to maintain a metadata cache at every broker. The cache can be kept consistent by having the controller send a (new) update-metadata request to all brokers whenever it sends out a leaderAndIsr request. A new request type would avoid needing to ""overload"" the leader and isr request.

This would help avoid the herd effect of multiple clients flooding the controller with metadata requests (although these requests should return quickly with your patch).
;;;","14/May/13 23:58;nehanarkhede;I like Joel's suggestion for 2 reasons -

1. Like he mentioned, if the controller pushes metadata updates to brokers, it will avoid the herd effect when multiple clients need to update metadata.
2. It is not a good idea to overload LeaderAndIsrRequest with UpdateMetadata since those 2 requests are fired under different circumstances. Part of this complication is also due to the fact that LeaderAndIsrRequest is also overloaded by start replica state change.

The latest patch includes Joel's suggestion. It includes the following changes -

1. A new controller state change request and response is defined - UpdateMetadataRequest and UpdateMetadataResponse. UpdateMetadataRequest has the partition state information like leader, isr, replicas for a list of partitions. In addition to this, it also has a list of live brokers and the broker id -> host:port mapping for all brokers in the cluster. The live brokers information is used when the broker handles metadata request to figure out if the leader is alive or not. UpdateMetadataResponse is similar to LeaderAndIsrResponse, in the sense that it has an error code per partition and a top level error code just like any other state change request

2. Every kafka broker maintains 3 data structures - leader cache, the list of alive brokers, the broker id-> host:port mapping for all brokers. These data structures are updated by the UpdateMetadataRequest and queried by the TopicMetadataRequest. So those accesses need to be synchronized

3. The controller fires the update metadata request -
3.1 When a new broker is started up. The newly restarted brokers are sent the partition state info for all partitions in the cluster.
3.2 When a broker fails, since leaders for some partitions would've changed
3.3 On a controller failover, since there could've been leader changes during the failover
3.4 On preferred replica election, since leaders for many partitions could've changed
3.5 On partition reassignment, since leader could've changed for the reassigned partitions
3.6 On controlled shutdown, since leaders move for the partitions hosted on the broker being shut down

4. Unit tests have changed to wait until the update metadata request has trickled to all servers. The best way I could think of is to make the KafkaApis object and the leaderCache accessible from KafkaServer.
;;;","15/May/13 16:22;junrao;Thanks for the patch. Overall, this is a good patch. It adds the new functionality in a less intrusive way to the controller. Let's spend a bit more time on the wire protocol change since it may not be trivial to change it later. Some comments:

1. KafkaController:
1.1 It seems that everytime that we try to send a LeaderAndIsrRequest, we follow that with an UpdateMetadataRequest. Would it be simpler to consolidate the logic in ControllerBrokerRequestBatch.sendRequestsToBrokers() such that for every LeaderAndIsrRequest that we send, we also send an UpdateMetadataRequest (with partitions in LeaderAndIsrReqeust) to all live brokers?
1.2 ControllerContext: Not sure why we need allBrokers. Could we just return liveBrokersUnderlying as all brokers?
1.3 The commet under sendUpdateMetadataRequest is not accurate since it doesn't always just send to new brokers.
1.4 removeReplicaFromIsr():
1.4.1 The logging in the following statement says ISR, but actually prints both leaderAndISr.
debug(""Removing replica %d from ISR %s for partition %s.""
1.4.2 Two new statements are added to change partitionLeadershipInfo. Is that fixing an existing issue not related to metadata?

2. KafkaApis: Not sure why we need to maintain both aliveBrokers and allBrokers. It seems just storing aliveBrokers (with the broker info) is enough for the purpose of answering metadata request.

3. ControllerBrokerRequestBatch: If we just need to send live brokers in the updateMetadata request, there is no need to maintain aliveBrokers and allBrokers here.

4. PartitionStateInfo:
4.1 Now that we are sending allReplicas, there is no need to explicitly send replicationFactor.
4.2 We encode ISR as strings, but all replicas as ints. We should make them consistent. It's better to encode ISR as ints too.

5. UpdateMetadataRequest: Not sure why we need ackTimeoutMs.

6. BrokerPartitionInfo: Are the changes in getBrokerPartitionInfo() necessary? If partitionMetadata is empty, the caller in DefaultEventHandler already throws NoBrokersForPartitionException.

7. ConsumerFetcherManager.LeaderFinderThread: The code change here is just for logging. Would it be simpler to just log the metadata response in debug mode? If we want to see the exception type associated with the error coder, we can fix the toString() method in metadata response.

8. AdminUtils:
8.1 Could you explain why the test testGetTopicMetadata() is deleted?
8.2 To ensure that the metadata is propagated to all brokers, could we add a utility function waitUntilMetadataPropagated() that takes in a list of brokers, a list of topics and a timeout? We can reuse this function in all relevant tests.

9. AsyncProducerTest.testInvalidPartition(): Not sure about the change. If we hit any exception (including UnknowTopicOrPartitionException) during partitionAndCollate(), the event handler will retry. 
;;;","15/May/13 22:55;swapnilghike;Just to be sure, the two attached patches are independent of each other, right?;;;","15/May/13 23:23;nehanarkhede;That's correct.;;;","16/May/13 16:22;nehanarkhede;Thanks for the great review!

1. KafkaController:
1.1 This is a good suggestion, however it wouldn't suffice in the following cases -
- new broker startup - Here we have to send the metadata for all partitions to the new brokers. The leader and isr request only sends the relevant partitions
- controller failover - Here we have to send metadata for all partitions to all brokers
- partition reassignment - Here we have to send another metadata request just to communicate the change in isr and other replicas.

For now, I've left the old calls to sendUpdateMetadataRequest commented out to show what has changed. I will remove those comments before check in. I still think that the send update metadata request handling can be optimized to make it reach the brokers sooner, but every optimization will come with a risk. So I suggest, we first focus on correctness and then optimize if it works on large deployments. 

1.2 ControllerContext: I thought that it is unintuitive to not send broker information and only send broker id for brokers that are offline. There was a bug filed for this where users complained it was unintuitive. However, this change will need more thought to do it correctly. So I might include another patch to fix it properly. This patch doesn't have this change

1.3 Fixed
1.4.1 Fixed
1.4.2 Correct, it is to fix updating the partition leadership info while shrinking isr since the leader can also change in those cases and we use partition leadership info while sending update metadata request, so it should always be kept current

2,3. Same concern as 1.2

4. PartitionStateInfo:
4.1 We still need to send the number of all replicas to be able to deserialize the replica list correctly, which is the replication factor.
4.2 Good point, changed that

5. Good observation. This was somehow leftover in all controller state change requests. Didn't make sense, so removed it from LeaderAndIsrRequest, StopReplicaRequest and UpdateMetadataRequest

6. It really didn't make sense to me that the producer throw NoBrokersForPartitionException when in reality it had failed to fetch metadata. This will help us read errors better

7. Good point, moved it to the toString() API of TopicMetadata

8. AdminUtils:
8.1 Because it is a duplicate of the test in TopicMetadataTest.
8.2 Good point, added that and changed all tests to use it

9. Ideally yes. But the old code was not retrying for any exception, including UnknownTopicOrPartitionException. I've changed DefaultEventHandler to retry no matter what exception it hits. So the test is changed to reflect that it shouldn't give up with UnknownTopicOrPartitionException but instead should retry sending the message and succeed.
~                                                                                                           ;;;","17/May/13 02:13;nehanarkhede;Changes in the latest patch include -

1. Removed the all brokers and just included alive brokers in the update metadata request. So the topic metadata will not include broker information for dead brokers.

2. My guess about there being a bug that with the update metadata request processing was right. The bug doesn't affect the correctness of update metadata, but it just delays the communication of new leaders to all brokers. The bug was that we were removing the broker going through controlled shutdown from the alive brokers list before it is really shutdown. So from a client's perspective, it takes much longer for a new leader to be available. Fixed it to include shutting down brokers in the list of alive brokers

3. Fixed another bug related to new topic creation. This bug caused the controller to not communicate the leaders of newly created topics to all brokers causing metadata requests to fail.

4. Tested this fix with 100s of migration tools sending data to ~400 topics to a 7 node cluster. There are ~500 consumers consuming data from this cluster. The test continuously bounces the brokers in a rolling restart fashion. The clients notice the new leaders within few 10s of ms in most cases.

5. Also the queue time for all requests is mostly < 10 ms since metadata requests are not a bottleneck in the system anymore. The latency of a metadata request for ~300 topics itself has dropped from 10s of seconds to 10s of ms.;;;","17/May/13 16:28;junrao;Thanks for patch v4. A few more comments:

40. KafkaController.ControllerContext: This is not introduced in this patch, but serveOrShuttingDownBrokerIds should just be liveBrokerIdUnderlying.

41.ControllerBrokerRequestBatch: Instead of maintaining aliveBrokers, could we just get it from controllerContext?

42. KafkaApis.handleTopicMetadataRequest:
42.1 We can rewrite the following statement
      val partitionMetadata = sortedPartitions.map { partitionReplicaMap => 
   to 
      val partitionMetadata = sortedPartitions.map { case(topicAndPartition, partitionState) =>
 Then, we don't have to redefine topicAndPartition and partitionState.
42.2 The val partitionReplicaAssignment seems unintuitive. Should we rename it to partitionStateInfo?
42.3 The outermost try/catch is unnecessary since it should be handled by the caller handle().
42.4 Not sure if we need to log the following error since it's either due to LeaderNotAvailable or ReplicaNotAvailable, both are expected.
  error(""Error while fetching topic metadata for topic %s due to %s "".format(topicMetadata.topic,

43. UpdateMetadataRequest: This class needs to define handleError(). This method is actually required to be defined in every request. So we should remove the empty body of handleError() in RequestOrResponse.

44. UpdateMetadataResponse: Do we really need the per partition level error code? It seems that a global error code is enough.

45. ConsumerFetcherManager: We should put the following statement under logger.isDebugEnabled().
          topicsMetadata.foreach(topicMetadata => debug(topicMetadata.toString()))

46. TopicMetadata.toString(): It only prints the leader. We need to print other fields in PartitionMetadata too.

47. BrokerPartitionInfo: If the metadata response has no error, it seems that we show throw an UnknownTopicOrPartitionException, instead of a KafkaException. Alternatively, should we not throw exception at all in this case since the caller already has to deal with the case when there is no metadata?

48. AsyncProducerTest.testInvalidPartition(): The message in the following statement is a bit missing leading. It's probably better to say sth like ""Should not thrown any exception"". Actually, instead of catching just UnknownTopicOrPartitionException, we should catch and fail any exception.
        case e: UnknownTopicOrPartitionException => fail(""Should fail with UnknownTopicOrPartitionException"");;;","17/May/13 17:11;nehanarkhede;I'm not sure I understood your review comment #47. Other review comments are addressed.;;;","17/May/13 19:43;nehanarkhede;All review comments are addressed. Also made another change to update metadata request handling. Basically, if an update metadata request from a stale controller epoch arrives at the broker, it should reject that request;;;","17/May/13 22:32;junrao;Thanks for patch v5. Looks good. +1. I have some minor follow up comments that we can address in a separate patch.;;;","17/May/13 22:43;junrao;Some minor comments:

50. KafkaApis.handleTopicMetadataRequest: partitionStateOpt is not necessary since inside sortedPartitions.map { }, it can't be undefined.

51. AsyncProducerTest.testInvalidPartition(): In the case statement inside catch, let's catch all exceptions and fail if it happens.
;;;","18/May/13 00:00;nehanarkhede;Thanks for your careful reviews. Checked in patch v5 and the follow up comments.;;;","20/May/13 17:29;nehanarkhede;Attaching a follow up patch that fixes 2 minor things -

1. Added update metadata request handling to the state change log. This makes is much easier to troubleshoot any issue with metadata cache refreshing
2. A minor bug in the controller channel manager that fixes it to read an UpdateMetadataResponse properly. ;;;","21/May/13 15:46;junrao;Thanks for the followup patch. Some comments:

60. KafkaApis: The following logging logs the whole request for each partition. This will probably pollute the log. Is it enough just to log the whole request once?
    if(stateChangeLogger.isTraceEnabled)
      updateMetadataRequest.partitionStateInfos.foreach(p => stateChangeLogger.trace((""Broker %d handling "" +
        ""UpdateMetadata request %s correlation id %d received from controller %d epoch %d for partition %s"")
        .format(brokerId, p._2, updateMetadataRequest.correlationId, updateMetadataRequest.controllerId,
        updateMetadataRequest.controllerEpoch, p._1)))
Is the following logging necessary? If we know a request, we already know what should be in the cache after processing the request.
        if(stateChangeLogger.isTraceEnabled)
          stateChangeLogger.trace((""Broker %d caching leader info %s for partition %s in response to UpdateMetadata request sent by controller %d"" +
            "" epoch %d with correlation id %d"").format(brokerId, partitionState._2, partitionState._1,
            updateMetadataRequest.controllerId, updateMetadataRequest.controllerEpoch, updateMetadataRequest.correlationId))
      }


;;;","21/May/13 16:18;nehanarkhede;I agree. Kept only the 2nd logging message about caching the leader info.

Also, both log messages logged only the partition leader info, not the whole request. 

Fixed another exception in ControllerChannelManager ;;;","21/May/13 16:53;junrao;Thanks for the second followup patch. +1.;;;","21/May/13 17:07;nehanarkhede;Thanks for the quick review, committed it;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClosedByInterruptException when high-level consumer shutdown normally,KAFKA-900,12646752,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,jbrosenberg,jbrosenberg,09/May/13 05:21,29/May/13 16:56,14/Jul/23 05:39,29/May/13 16:56,0.8.0,,,0.8.0,,,,,,,consumer,,,0,,,,"I'm porting some unit tests from 0.7.2 to 0.8.0. The test does the following, all embedded in the same java process: 

-- spins up a zk instance 
-- spins up a kafka server using a fresh log directory 
-- creates a producer and sends a message 
-- creates a high-level consumer and verifies that it can consume the message 
-- shuts down the consumer 
-- stops the kafka server 
-- stops zk 

The test seems to be working fine now, however, I consistently see the following exception, when the consumer connector is shutdown:

1699 [ConsumerFetcherThread-group1_square-1a7ac0.local-1368076598439-d66bb2eb-0-1946108683] WARN kafka.consumer.ConsumerFetcherThread  - [ConsumerFetcherThread-group1_square-1a7ac0.local-1368076598439-d66bb2eb-0-1946108683], Error in fetch Name: FetchRequest; Version: 0; CorrelationId: 1; ClientId: group1-ConsumerFetcherThread-group1_square-1a7ac0.local-1368076598439-d66bb2eb-0-1946108683; ReplicaId: -1; MaxWait: 100 ms; MinBytes: 1 bytes; RequestInfo: [test-topic,0] -> PartitionFetchInfo(1,1048576)
java.nio.channels.ClosedByInterruptException
	at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)
	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:543)
	at kafka.network.BlockingChannel.connect(BlockingChannel.scala:57)
	at kafka.consumer.SimpleConsumer.connect(SimpleConsumer.scala:47)
	at kafka.consumer.SimpleConsumer.reconnect(SimpleConsumer.scala:60)
	at kafka.consumer.SimpleConsumer.liftedTree1$1(SimpleConsumer.scala:81)
	at kafka.consumer.SimpleConsumer.kafka$consumer$SimpleConsumer$$sendRequest(SimpleConsumer.scala:73)
	at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SimpleConsumer.scala:112)
	at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:112)
	at kafka.consumer.SimpleConsumer$$anonfun$fetch$1$$anonfun$apply$mcV$sp$1.apply(SimpleConsumer.scala:112)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
	at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply$mcV$sp(SimpleConsumer.scala:111)
	at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:111)
	at kafka.consumer.SimpleConsumer$$anonfun$fetch$1.apply(SimpleConsumer.scala:111)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
	at kafka.consumer.SimpleConsumer.fetch(SimpleConsumer.scala:110)
	at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:96)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:88)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)
1721 [Thread-12] INFO com.squareup.kafka.server.KafkaServer  - Shutting down KafkaServer
2030 [main] INFO com.squareup.kafka.server.KafkaServer  - Shut down complete for KafkaServer
Disconnected from the target VM, address: '127.0.0.1:49243', transport: 'socket'


It would be great if instead, something meaningful was logged, like:

""Consumer connector has been shutdown""",,jbrosenberg,jbrosenberg@gmail.com,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/13 16:35;junrao;kafka-900.patch;https://issues.apache.org/jira/secure/attachment/12582491/kafka-900.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,327110,,,Wed May 29 16:56:58 UTC 2013,,,,,,,,,,"0|i1kfu7:",327454,,,,,,,,,,,,,,,,,,,,"09/May/13 16:35;junrao;Attach a patch. Jason, could you give it a try?;;;","29/May/13 16:39;nehanarkhede;+1, thanks for the patch!;;;","29/May/13 16:56;junrao;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeaderNotAvailableException the first time a new message for a partition is processed.,KAFKA-899,12646751,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,jbrosenberg,jbrosenberg,09/May/13 05:18,01/Oct/14 23:01,14/Jul/23 05:39,31/May/13 05:18,0.8.0,,,0.8.2.0,,,,,,,core,,,0,,,,"I'm porting some unit tests from 0.7.2 to 0.8.0.  The test does the following, all embedded in the same java process:

-- spins up a zk instance
-- spins up a kafka server using a fresh log directory
-- creates a producer and sends a message
-- creates a high-level consumer and verifies that it can consume the message
-- shuts down the consumer
-- stops the kafka server
-- stops zk

The test seems to be working fine now, however, I consistently see the following exceptions (which from poking around the mailing list seem to be expected?).  If these are expected, can we suppress the logging of these exceptions, since it clutters the output of tests, and presumably, clutters the logs of the running server/consumers, during clean startup and shutdown......

When I call producer.send(), I get:

1071 [main] WARN kafka.producer.BrokerPartitionInfo  - Error while fetching metadata 	partition 0	leader: none	replicas: 	isr: 	isUnderReplicated: false for topic partition [test-topic,0]: [class kafka.common.LeaderNotAvailableException]
1081 [main] WARN kafka.producer.async.DefaultEventHandler  - Failed to collate messages by topic,partition due to
kafka.common.LeaderNotAvailableException: No leader for any partition
	at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$getPartition(DefaultEventHandler.scala:212)
	at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:150)
	at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:148)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
	at kafka.producer.async.DefaultEventHandler.partitionAndCollate(DefaultEventHandler.scala:148)
	at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:94)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:72)
	at kafka.producer.Producer.send(Producer.scala:74)
	at kafka.javaapi.producer.Producer.send(Producer.scala:32)
	at com.squareup.kafka.server.KafkaServerTest.produceAndConsumeMessage(KafkaServerTest.java:98)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:69)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:48)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:292)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:77)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:195)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:63)
1133 [kafka-request-handler-1] WARN kafka.server.HighwaterMarkCheckpoint  - No highwatermark file is found. Returning 0 as the highwatermark for partition [test-topic,0]
	...
        ...

It would be great if instead of this exception, it would just log a meaningful message, like:

""No leader was available for partition X, one will now be created""

Jason",,Andras Hatvani,jbrosenberg,jbrosenberg@gmail.com,joshrosen,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/13 22:40;junrao;kafka-899.patch;https://issues.apache.org/jira/secure/attachment/12582535/kafka-899.patch","29/May/13 16:19;junrao;kafka-899_v2.patch;https://issues.apache.org/jira/secure/attachment/12585236/kafka-899_v2.patch","30/May/13 01:21;junrao;kafka-899_v3.patch;https://issues.apache.org/jira/secure/attachment/12585349/kafka-899_v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,327109,,,Wed Oct 01 23:01:39 UTC 2014,,,,,,,,,,"0|i1kftz:",327453,,,,,,,,,,,,,,,,,,,,"09/May/13 22:40;junrao;Attach a patch. Jason, could you give it a try?;;;","29/May/13 16:19;junrao;Attach patch v2 after the rebase.;;;","29/May/13 16:35;nehanarkhede;Thanks for the patch!

1. Another place where we can make it easier for the user to know the reason for the send failure -

            error(""Produce request with correlation id %d failed due to response %s. List of failed topic partitions is %s""
              .format(currentCorrelationId, response.toString, failedTopicPartitions.mkString("","")))

Here, we print the entire response. Rather we should only print the partition and corresponding error status for partitions with non-zero error code

2. When we do print the error status above, we should print the text name of the error instead of the integer error code. For this, we can override toString() in ProducerResponseStatus.;;;","30/May/13 01:21;junrao;Thanks for the review. Attach patch v3 that addresses the above issue. Changed the logging level from error to warning since the real error will be reported when all retries have failed.;;;","30/May/13 22:51;nehanarkhede;+1 on the latest patch;;;","31/May/13 05:18;junrao;Thanks for the review. Committed to 0.8 after fixing a bug and a typo.;;;","27/Sep/14 16:09;Andras Hatvani;This isn't fixed in 0.8.1.1 as the behavior is the same.

As a workaround increase retry.backoff.ms from the default 100 ms to 1000 ms.
In case this would be not enough for you, you can try to change the values of 
- message.send.max.retries from the default 5 to e.g. 10 and
- topic.metadata.refresh.interval.ms to 0.

This is the expected behavior, therefore an exception mustn't be thrown, rather it has to be communicated that the leader election is in progress. Furthermore, suggestions regarding changing the values variables I mentioned should be mandatory.;;;","28/Sep/14 22:08;junrao;This fix actually wasn't included in 0.8.1. Changed the fix version in the jira.;;;","29/Sep/14 14:12;Andras Hatvani;Jun, can I do any support regarding this issue (e.g. verify the implementation)?;;;","29/Sep/14 16:08;junrao;Andras,

It seems that your issue is a bit different from this jira. This jira is about removing the stacktrace in the producer log when the metadata is not available. Your issue seems to be that the metadata is not propagated as quickly as you expect. Normally, 100ms should be long enough for a new topic to be created and its metadata be propagated to all brokers. In your case, it seems that process takes more than 1 sec. Could you look at the controller and the state-change log to see where the delay is? For example, is the write to ZK slow or is the propagation of metadata from the controller to the broker slow?;;;","29/Sep/14 17:11;Andras Hatvani;Jun,

Although the reasons may be different, the objective is identical (see my last post in the thread ""LeaderNotAvailableException, although leader elected"" on the Kafka user mailing list): There shouldn't be any exception in case no leader can be communicated to the producer (whether it's because metadata propagation delay or non-completed leader election or any other valid non-erroneous cause), but rather a status message enabling the producer to be tuned.
This exception should really only cover exceptional cases. 

But you're right, my case will exactly be covered by KAFKA-1494. I'll provide further data in that issue.;;;","29/Sep/14 18:16;junrao;Andras,

If a message couldn't be sent (after all retries), we need to indicate this to the producer client. We currently do that by throwing an exception back to the caller. The caller can decide what to do. Are you suggesting sth else?;;;","29/Sep/14 20:29;Andras Hatvani;Jun,

Yes, I suggest a classification of the server's response so that the client can distinguish between technical failures (e.g. network unavailable) and functional state (e.g. leader election for partition in progress). For example, a topic's state could be: non-existent, being created, existent, leader election in progress, failed (and in this case the reason of the failure, like no disk-space). 
Furthermore, in case of topic auto-creation I'd separate and communicate the fact of creation from the message sending and handle the results and failures separately, too.
Returning a value instead of void would support both mechanisms. What do you think?;;;","01/Oct/14 23:01;junrao;We started doing that classification in the new java producer. For example, there are certain exceptions are of RetriableException. Transient failures like leader not available are in that category. Exceptions like MessageTooLarge are in a different category. Perhaps you can take a look at that in the new producer and see if that makes sense.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in ConsoleConsumer,KAFKA-897,12646533,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,nehanarkhede,lanzaa,lanzaa,08/May/13 00:28,03/Jun/13 18:01,14/Jul/23 05:39,30/May/13 04:27,0.8.0,,,0.8.1,,,,,,,consumer,,,0,,,,"The protocol document [1] mentions that keys and values in message sets can be null. However the ConsoleConsumer throws a NPE when a null is passed for the value.

java.lang.NullPointerException
        at kafka.utils.Utils$.readBytes(Utils.scala:141)
        at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:106)
        at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:33)
        at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:61)
        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:53)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:32)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
        at kafka.consumer.KafkaStream.foreach(KafkaStream.scala:25)
        at kafka.consumer.ConsoleConsumer$.main(ConsoleConsumer.scala:195)
        at kafka.consumer.ConsoleConsumer.main(ConsoleConsumer.scala)

[1] https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-Messagesets",,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/May/13 18:59;lanzaa;KAFKA-897-v2.patch;https://issues.apache.org/jira/secure/attachment/12582341/KAFKA-897-v2.patch","08/May/13 00:34;lanzaa;Kafka897-v1.patch;https://issues.apache.org/jira/secure/attachment/12582217/Kafka897-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,326891,,,Thu May 30 04:27:23 UTC 2013,,,,,,,,,,"0|i1ke7r:",327236,,,,,,,,,,,,,,,,,,,,"08/May/13 00:34;lanzaa;Patch for this issue is attached. Kafka897-v1.patch;;;","08/May/13 16:29;junrao;Thanks for the patch. Actually, I think the wiki is inaccurate. The following is the comment on Message in the code. It seems that we only allow key to be null, but not value (since it's length is always >=0).

/**
 * A message. The format of an N byte message is the following:
 *
 * 1. 4 byte CRC32 of the message
 * 2. 1 byte ""magic"" identifier to allow format changes, value is 2 currently
 * 3. 1 byte ""attributes"" identifier to allow annotations on the message independent of the version (e.g. compression enabled, type of codec used)
 * 4. 4 byte key length, containing length K
 * 5. K byte key
 * 6. (N - K - 10) byte payload
;;;","08/May/13 16:34;jkreps;Yup, I think we are confusing the protocol and the code. The protocol allows null values. The code didn't handle this until we added log compaction which is on trunk. In other words this was intentional sequencing so we wouldn't change the protocol again.;;;","08/May/13 17:26;junrao;Ok. So the confusing part is that the comment in the code is inaccurate. We actually explicitly store the size of the value in the binary representation. So we should change 6. in the above comment to the following:
 * 6. 4 byte payload length, containing length V
 * 7. V byte payload

Colin,

It seems that our code only allows null key/value in trunk. So, we can fix this in trunk. It seems that ConsumerIterator is already fixed in trunk. For the changes in DefaultMessageFormatter, could you verify if FilterOutputStream.write() supports null input? If so, we don't need to patch it either.;;;","08/May/13 18:07;lanzaa;I had been testing on 0.8 and did not notice KAFKA-739 (Handle null values in Message payload) had been commited. I will test the DefaultMessageFormatter with nulls on trunk. I think however that the write calls do not support null.;;;","08/May/13 18:59;lanzaa;Looks like FilterOutputStream does not handle nulls.

java.lang.NullPointerException
        at java.io.FilterOutputStream.write(FilterOutputStream.java:97)
        at kafka.consumer.DefaultMessageFormatter.writeTo(ConsoleConsumer.scala:288)
...
        at kafka.consumer.ConsoleConsumer$.main(ConsoleConsumer.scala:195)

Attached is patch v2, which only touches ConsoleConsumer and allows the output of null values as ""null"".;;;","23/May/13 21:55;nehanarkhede;+1. I think this looks good until we upgrade to trunk.
Jun, any thoughts ?;;;","30/May/13 04:27;junrao;Thanks for the patch. Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Protocol documentation is not clear about requiredAcks = 0.,KAFKA-895,12645846,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,junrao,fparadis2,fparadis2,02/May/13 18:12,03/May/13 03:58,14/Jul/23 05:39,03/May/13 03:58,0.8.0,,,0.8.0,,,,,,,producer ,,,0,documentation,,,"After reading the protocol guide (https://cwiki.apache.org/KAFKA/a-guide-to-the-kafka-protocol.html), I was under the impression that the Produce API was sending a response even when requiredAcks = 0 (immediately after receiving the request). However, after some tests, I realized that no response is sent in that case. The protocol documentation should specify clearly that behavior.",,fparadis2,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,326205,,,Fri May 03 03:58:30 UTC 2013,,,,,,,,,,"0|i1k9zj:",326550,,,,,,,,,,,,,,,,,,,,"03/May/13 03:58;junrao;Thanks for pointing this out. Updated the wiki.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to access kafka via Tomcat,KAFKA-893,12645684,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,arathim,arathim,01/May/13 19:18,21/May/14 16:34,14/Jul/23 05:39,21/May/14 16:34,,,,,,,,,,,tools,,,1,,,,"Iam using kafka_2.8.0-0.8-SNAPSHOT.jar and Apache Tomcat 7. Upon deploying, the Consumer example (provided at Kafka quickstart) in a Tomcat web app, I get the following error 

java.lang.VerifyError: class scala.Tuple2$mcLL$sp overrides final method _1.()Ljava/lang/Object;
	java.lang.ClassLoader.defineClass1(Native Method)
	java.lang.ClassLoader.defineClass(ClassLoader.java:791)
	java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	org.apache.catalina.loader.WebappClassLoader.findClassInternal(WebappClassLoader.java:2888)
	org.apache.catalina.loader.WebappClassLoader.findClass(WebappClassLoader.java:1172)
	org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1680)
	org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1558)
	kafka.consumer.ConsumerConfig.<init>(ConsumerConfig.scala:75)
	TestServlet.kafkaconsumer(TestServlet.java:44)
	TestServlet.doGet(TestServlet.java:20)
	javax.servlet.http.HttpServlet.service(HttpServlet.java:621)
	javax.servlet.http.HttpServlet.service(HttpServlet.java:728)

Please suggest what should be done to fix this error.

Thanks in advance.
Arathi
",Apache Tomcat 7,arathim,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,326045,,,Wed May 21 16:34:15 UTC 2014,,,,,,,,,,"0|i1k8zz:",326390,,,,,,,,,,,,,,,,,,,,"03/Jul/13 03:50;jkreps;I think this happens if you have the wrong version of scala on your classpath.;;;","21/May/14 16:34;arathim;Using Kafka dependency in my maven pom.xml
<dependency>
       <groupId>org.apache.kafka</groupId>
       <artifactId>kafka_2.10</artifactId>
       <version>0.8.0</version>
</dependency>
 fixed the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The list of brokers for fetching metadata should be shuffled ,KAFKA-890,12645359,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,30/Apr/13 01:58,30/Apr/13 19:42,14/Jul/23 05:39,30/Apr/13 19:42,0.8.0,,,,,,,,,,core,,,0,kafka-0.8,p1,,"The list of brokers in the metadata request is never shuffled. Which means that if some clients are not using a VIP for metadata requests, the first broker ends up servicing most metadata requests, leaving imbalanced load on the brokers. This issue is even more pronounced when there are several thousand clients talking to a cluster each using a broker list to fetch metadata.",,jjkoshy,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/13 02:01;nehanarkhede;kafka-890.patch;https://issues.apache.org/jira/secure/attachment/12581107/kafka-890.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,325721,,,Tue Apr 30 17:13:32 UTC 2013,,,,,,,,,,"0|i1k707:",326066,,,,,,,,,,,,,,,,,,,,"30/Apr/13 02:01;nehanarkhede;This patch shuffles the list of brokers used by both producer and consumer to fetch metadata. ;;;","30/Apr/13 17:13;jjkoshy;+1

It is worth noting that this is useful even in the presence of a VIP since the consumers don't currently use a VIP to to look up metadata.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get java.lang.NoSuchMethodError: com.yammer.metrics.core.TimerContext.stop()J when stopping kafka brokers,KAFKA-884,12644944,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,kkasravi,kkasravi,26/Apr/13 21:40,27/Apr/13 04:42,14/Jul/23 05:39,27/Apr/13 04:42,0.8.0,,,0.8.0,,,,,,,config,,,0,,,,"When stopping kafka brokers I get the following stack trace:

2013-04-26 14:30:16,394] INFO [Replica Manager on Broker 1]: Shutted down completely (kafka.server.ReplicaManager)
[2013-04-26 14:30:16,398] WARN com.yammer.metrics.core.TimerContext.stop()J (kafka.utils.Utils$)
java.lang.NoSuchMethodError: com.yammer.metrics.core.TimerContext.stop()J
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:36)
	at kafka.log.FileMessageSet.flush(FileMessageSet.scala:164)
	at kafka.log.FileMessageSet.close(FileMessageSet.scala:173)
	at kafka.log.LogSegment$$anonfun$close$2.apply$mcV$sp(LogSegment.scala:161)
	at kafka.utils.Utils$.swallow(Utils.scala:186)
	at kafka.utils.Logging$class.swallowWarn(Logging.scala:91)
	at kafka.utils.Utils$.swallowWarn(Utils.scala:45)
	at kafka.utils.Logging$class.swallow(Logging.scala:93)
	at kafka.utils.Utils$.swallow(Utils.scala:45)
	at kafka.log.LogSegment.close(LogSegment.scala:161)
	at kafka.log.Log$$anonfun$close$2.apply(Log.scala:248)
	at kafka.log.Log$$anonfun$close$2.apply(Log.scala:247)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
	at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
	at kafka.log.Log.close(Log.scala:247)
	at kafka.log.LogManager$$anonfun$shutdown$2.apply(LogManager.scala:290)
	at kafka.log.LogManager$$anonfun$shutdown$2.apply(LogManager.scala:290)
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)
	at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:474)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
	at scala.collection.JavaConversions$JCollectionWrapper.foreach(JavaConversions.scala:495)
	at kafka.log.LogManager.shutdown(LogManager.scala:290)
	at kafka.server.KafkaServer$$anonfun$shutdown$7.apply$mcV$sp(KafkaServer.scala:125)
	at kafka.utils.Utils$.swallow(Utils.scala:186)
	at kafka.utils.Logging$class.swallowWarn(Logging.scala:91)
	at kafka.utils.Utils$.swallowWarn(Utils.scala:45)
	at kafka.utils.Logging$class.swallow(Logging.scala:93)
	at kafka.utils.Utils$.swallow(Utils.scala:45)
	at kafka.server.KafkaServer.shutdown(KafkaServer.scala:125)
	at kafka.server.KafkaServerStartable.shutdown(KafkaServerStartable.scala:46)
	at kafka.Kafka$$anon$1.run(Kafka.scala:42)

Steps to reproduce:
git checkout  remotes/origin/0.8
./sbt update
./sbt +package
./sbt assembly-package-dependency
./sbt eclipse

Now create 3 server.properties under config as described in the quick-start for 0.8
Additionally comment out the JMX_PORT in bin/kafka-server-start.sh.
Now start up servers and create a topic

bin/zookeeper-server-start.sh config/zookeeper.properties&
bin/kafka-server-start.sh config/server0.properties&
bin/kafka-server-start.sh config/server1.properties&
bin/kafka-server-start.sh config/server2.properties&
bin/kafka-create-topic.sh --topic mytopic --replica 3 --zookeeper localhost:2181
bin/kafka-console-producer.sh --broker-list localhost:9092,localhost:9093,localhost:9094 --sync --topic mytopic

Confirm that kafka servers are running as well as QuorumPeerMain
$ jps
43668 QuorumPeerMain
43669 Jps
43666 Kafka
43667 Kafka
43665 Kafka

Now stop the brokers and zookeeper
kill $(ps -eaf|grep Kafka|grep -v grep|awk '{print $2}')
kill $(ps -eaf|grep QuorumPeerMain|grep -v grep|awk '{print $2}')

You should see the above stack trace.",MAC OSX 10.8.3,kkasravi,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,325306,,,Sat Apr 27 04:42:50 UTC 2013,,,,,,,,,,"0|i1k4fz:",325651,,,,,,,,,,,,,,,,,,,,"26/Apr/13 21:47;nehanarkhede;We recently downgraded from metrics-3.x to metrics-2.2.0. Please check your classpath.;;;","27/Apr/13 04:42;kkasravi;That resolved it. Thanks Neha.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoLeaderPartitionSet should be cleared before leader finder thread is started up,KAFKA-880,12644755,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,jjkoshy,jjkoshy,26/Apr/13 00:13,26/Apr/13 02:03,14/Jul/23 05:39,26/Apr/13 02:03,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"This was a recent regression.

This could prevent the consumer from progressing because fetchers for the currently owned partitions may not be added (depending on the order that the map iterator yields).

I think the fix should be simple - just clear the set after stopping the leader finder thread and stopping fetchers.

[2013-04-25 17:06:38,377] WARN [sometopic-somehost-1366909575615-f801367d-leader-finder-thread]
, Failed to find leader for Set([sometopic,11], [sometopic,25], [sometopic,24]) (kafka.consumer.ConsumerFetcherManager$Lead
erFinderThread)
java.util.NoSuchElementException: key not found: [sometopic,24]
        at scala.collection.MapLike$class.default(MapLike.scala:223)
        at scala.collection.immutable.Map$Map2.default(Map.scala:110)
        at scala.collection.MapLike$class.apply(MapLike.scala:134)
        at scala.collection.immutable.Map$Map2.apply(Map.scala:110)
        at kafka.consumer.ConsumerFetcherManager$LeaderFinderThread$$anonfun$doWork$4.apply(ConsumerFetcherManager.scala:81)
        at kafka.consumer.ConsumerFetcherManager$LeaderFinderThread$$anonfun$doWork$4.apply(ConsumerFetcherManager.scala:79)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
        at kafka.consumer.ConsumerFetcherManager$LeaderFinderThread.doWork(ConsumerFetcherManager.scala:79)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)
",,jjkoshy,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/13 01:43;junrao;kafka-880.patch;https://issues.apache.org/jira/secure/attachment/12580647/kafka-880.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,325118,,,Fri Apr 26 02:03:51 UTC 2013,,,,,,,,,,"0|i1k3af:",325463,,,,,,,,,,,,,,,,,,,,"26/Apr/13 01:43;junrao;Good catch. Attach a patch.;;;","26/Apr/13 01:48;nehanarkhede;+1;;;","26/Apr/13 02:03;junrao;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In system test, read the new leader from zookeeper instead of broker log on completion of become-leader state transition",KAFKA-879,12644749,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jfung,swapnilghike,swapnilghike,25/Apr/13 23:13,23/Jul/13 16:36,14/Jul/23 05:39,23/Jul/13 16:36,,,,0.8.0,,,,,,,,,,0,bugs,,,"Currently the system test reads the new leader from a broker log statement on completion of become-leader state transition. The log statements can change with time and can also move around.

We can read the leader info from zookeeper given a topic, partition. The path is #2 at https://cwiki.apache.org/confluence/display/KAFKA/Kafka+data+structures+in+Zookeeper. The Zookeeper data structures are unlikely to change in foreseeable future.

",,jfung,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/13 21:30;jfung;kafka-879-v1.patch;https://issues.apache.org/jira/secure/attachment/12590308/kafka-879-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,325112,,,Tue Jul 23 16:36:18 UTC 2013,,,,,,,,,,"0|i1k393:",325457,,,,,,,,,,,,,,,,,,,,"01/Jul/13 21:30;jfung;Uploaded kafka-879-v1.patch;;;","01/Jul/13 21:34;jfung;kafka-879-v1.patch contains the change to query zookeeper to find out the leader change.

However, the info provided by zookeeper does not include the timestamp detailed enough to determine the leader election latency. Therefore, the leader election latency validation is disabled in the patch.

Please note that the leader election latency retrieved from the log messages is not accurate though.;;;","23/Jul/13 16:36;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Socket server does not set send/recv buffer sizes,KAFKA-872,12644154,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jjkoshy,jjkoshy,jjkoshy,23/Apr/13 17:00,23/Apr/13 17:41,14/Jul/23 05:39,23/Apr/13 17:41,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,The socket server should set its send and receive socket buffer sizes - this is important in cross-DC mirroring setups where large buffer sizes are essential to enable the mirror-maker processes to do bulk consumption. ,,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/13 17:02;jjkoshy;KAFKA-872-v1.patch;https://issues.apache.org/jira/secure/attachment/12580076/KAFKA-872-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,324521,,,Tue Apr 23 17:41:47 UTC 2013,,,,,,,,,,"0|i1jzlr:",324866,,,,,,,,,,,,,,,,,,,,"23/Apr/13 17:21;junrao;Thanks for the patch. Looks good and +1.;;;","23/Apr/13 17:41;jjkoshy;Thanks for the review. Applied to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename ZkConfig properties,KAFKA-871,12644034,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,23/Apr/13 01:31,26/Apr/13 01:58,14/Jul/23 05:39,26/Apr/13 01:58,,,,0.8.0,,,,,,,,,,0,kafka-0.8,p1,,For clarity. Renaming these properties should help in migration from 0.7 to 0.8.,,jfung,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/13 01:33;swapnilghike;kafka-871-v1.patch;https://issues.apache.org/jira/secure/attachment/12579956/kafka-871-v1.patch","24/Apr/13 01:39;swapnilghike;kafka-871-v2.patch;https://issues.apache.org/jira/secure/attachment/12580210/kafka-871-v2.patch","25/Apr/13 02:39;swapnilghike;kafka-871-v3.patch;https://issues.apache.org/jira/secure/attachment/12580454/kafka-871-v3.patch","25/Apr/13 23:02;swapnilghike;kafka-871-v4.patch;https://issues.apache.org/jira/secure/attachment/12580611/kafka-871-v4.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,324401,,,Fri Apr 26 01:58:30 UTC 2013,,,,,,,,,,"0|i1jyvb:",324746,,,,,,,,,,,,,,,,,,,,"23/Apr/13 01:33;swapnilghike;Unit tests pass.;;;","23/Apr/13 01:33;swapnilghike;[~jfung]: Could you take a look at this patch? Since it touches a lot of the system test files.;;;","23/Apr/13 17:05;junrao;Thanks for the patch. For clarity, should we rename broker.list in ProducerConfig to sth like bootstrap.broker.list? In 0.8, broker.list is only used by the metadata request for bootstrapping, not for the actual produce requests.;;;","23/Apr/13 18:23;swapnilghike;How about using metadata.broker.list or metadata.brokers? ;;;","23/Apr/13 18:31;junrao;metadata.broker.list is fine.;;;","23/Apr/13 18:45;jfung;Hi Swapnil,

In system_test/migration_tool_testsuite, the source cluster are running in 0.7. It is necessary to distinguish between 0.7 & 0.8 config in the testcase_xxxx_properties.json file. Let's walk through the changes needed.;;;","24/Apr/13 01:39;swapnilghike;Renamed broker.list to metadata.broker.list. 

Discussed the system test changes with John, they are included in this patch. Unfortunately I could not run the sanity test locally, because it kept prompting for password. I will work with John tomorrow to solve this issue. 

As a by product of the discussion with John, we have filed KAFKA-874 and KAFKA-875.

Testing: All unit tests pass.;;;","24/Apr/13 06:20;swapnilghike;We should hold off checking this patch in until we have verified that the system test works. I will also include a little JSON parsing related bug fix in v3.;;;","24/Apr/13 15:53;junrao;Some comments on patch v2. There are a few places that we changed ""--brokerinfo broker.list"" or ""--brokerinfo zk.connect"" in the 0.7 kafka.perf.ProducerPerformance to use the new 0.8 property name (the ""--brokerinfo"" option is only available in 0.7). These include
system_test/producer_perf/bin/run-compression-test.sh
system_test/producer_perf/bin/run-test.sh
system_test/broker_failure/bin/run-test.sh
system_test/mirror_maker/bin/run-test.sh 

It's weird that we include the 0.7 ProducerPerformance in system_test/broker_failure/bin/run-test.sh and system_test/mirror_maker/bin/run-test.sh since they are supposed to only test 0.8.;;;","24/Apr/13 17:18;swapnilghike;According to [~jfung], anything under system_test that does not have ""_testsuite"" at the end is not being used currently. So, I did not take the pains to clean up the changes that you mentioned, I believe those directories should be deleted as part of KAFKA-875 anyways.;;;","24/Apr/13 17:18;swapnilghike;I will exclude them from v3 to reduce the noise in the patch.;;;","24/Apr/13 17:45;jfung;The following are older system testing scripts in bash before system test is ported to Python. It is probably safe to be removed from the branch to avoid confusion.

1. system_test/broker_failure
2. system_test/mirror_maker  (NOT mirror_maker_testsuite)
3. system_test/producer_perf
4. system_test/common;;;","25/Apr/13 02:38;swapnilghike;1. Removed the aforementioned noise from patch v3.
2. The additional interesting change is a one character fix to JSON parsing in seqToJson in Utils.scala. We were not building the comma separated string correctly earlier, and this bug was not exposed because that particular condition was not exercised.
3. Added zookeeper.connect and zookeeper.connection.timeout.ms to migration_tool_testsuite/config/server.properties, because we are currently using only 1 server.properties file for both kafka 0.7 and 0.8 brokers. KAFKA-874 will create two separate config files for 0.7 and 0.8 brokers.

Testing:
1. Unit tests pass.
2. As [~jfung] suggested, tested migration tool test case 9001. It passed:

_test_case_name  :  testcase_9001
_test_class_name  :  MigrationToolTest
arg : bounce_migration_tool  :  false
arg : message_producing_free_time_sec  :  30
arg : num_iteration  :  1
arg : num_messages_to_produce_per_producer_call  :  50
arg : num_partition  :  1
arg : replica_factor  :  3
arg : sleep_seconds_between_producer_calls  :  1
validation_status  : 
     Unique messages from consumer on [test_1]  :  0
     Unique messages from producer on [test_1]  :  0

Total failures count : 0

;;;","25/Apr/13 16:47;junrao;Thanks for patch v3. Looks good. The following config is weird. It seems to be an 0.7 producer config. However, in 0.7, the broker list format is brokerid:host:port. Is it still being used?

./system_test/migration_tool_testsuite/config/migration_producer.properties:broker.list=localhost:9094,localhost:9095,localhost:9096
;;;","25/Apr/13 17:15;jfung;[~swapnilghike] : The test result is showing 0 messages for both producer & consumer. There may be some settings which are configured incorrectly.

[~junrao] : The broker.list property line is also used in the Hudson nightly job.;;;","25/Apr/13 23:02;swapnilghike;Attached patch v4. 

I think v4 fixes the system test. Ran 3 test cases according to [~jfung]'s advice:

_test_case_name  :  testcase_5001
_test_class_name  :  MirrorMakerTest
arg : bounce_leader  :  false
arg : bounce_mirror_maker  :  false
arg : message_producing_free_time_sec  :  15
arg : num_iteration  :  1
arg : num_messages_to_produce_per_producer_call  :  50
arg : num_partition  :  1
arg : replica_factor  :  3
arg : sleep_seconds_between_producer_calls  :  1
validation_status  :-
     Unique messages from consumer on [test_1]  :  500
     Unique messages from producer on [test_1]  :  500
     Validate for data matched on topic [test_1]  :  PASSED
     Validate for merged log segment checksum in cluster [source]  :  PASSED
     Validate for merged log segment checksum in cluster [target]  :  PASSED

_test_case_name  :  testcase_0001
_test_class_name  :  ReplicaBasicTest
arg : bounce_broker  :  false
arg : broker_type  :  leader
arg : message_producing_free_time_sec  :  15
arg : num_iteration  :  1
arg : num_messages_to_produce_per_producer_call  :  50
arg : num_partition  :  1
arg : replica_factor  :  3
arg : sleep_seconds_between_producer_calls  :  1
validation_status  :-
     Leader Election Latency MAX  :  None
     Leader Election Latency MIN  :  None
     No. of messages from consumer on [test_1] at simple_consumer_test_1-0_r1.log  :  500
     No. of messages from consumer on [test_1] at simple_consumer_test_1-0_r2.log  :  500
     No. of messages from consumer on [test_1] at simple_consumer_test_1-0_r3.log  :  500
     Unique messages from consumer on [test_1]  :  500
     Unique messages from producer on [test_1]  :  500
     Validate for data matched on topic [test_1]  :  PASSED
     Validate for data matched on topic [test_1] across replicas  :  PASSED
     Validate for merged log segment checksum in cluster [source]  :  PASSED
     Validate index log in cluster [source]  :  PASSED

_test_case_name : testcase_9001
_test_class_name : MigrationToolTest
validation_status : 
     Unique messages from consumer on [test_1] : 10500
     Unique messages from producer on [test_1] : 10500

Unit tests pass.
;;;","26/Apr/13 01:58;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"hadoop-producer KafkaRecordWriter writes entire input buffer capacity, even when intended payload is smaller",KAFKA-870,12644023,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,dstein,dstein,23/Apr/13 00:01,23/Apr/13 02:23,14/Jul/23 05:39,23/Apr/13 02:22,0.7,0.8.0,,,,,,,,,contrib,,,0,patch,,,"KafkaRecordWriter incorrectly gets the entire buffer capacity of the input BytesWritable.

If the capacity is larger than the underlying represented byte array, then the message will be padded with garbage bytes at the end.",,dstein,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/13 00:05;dstein;kafka-0.7.patch;https://issues.apache.org/jira/secure/attachment/12579938/kafka-0.7.patch","23/Apr/13 00:45;dstein;kafka-0.8.patch;https://issues.apache.org/jira/secure/attachment/12579949/kafka-0.8.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,324390,,,Tue Apr 23 02:22:11 UTC 2013,,,,,,,,,,"0|i1jysv:",324735,,,,,,,,,,,,,,,,,,,,"23/Apr/13 00:05;dstein;fix for 0.7 branch;;;","23/Apr/13 00:20;nehanarkhede;Thanks for the patch, David! Do you mind providing a patch for 0.8 branch as well ?;;;","23/Apr/13 00:45;dstein;fix for 0.8 / origin;;;","23/Apr/13 02:15;nehanarkhede;+1, thanks for the patch!;;;","23/Apr/13 02:22;nehanarkhede;Checked in patches to 0.7 and 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Recover segment does shallow iteration to fix index causing inconsistencies,KAFKA-866,12643607,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriramsub,sriramsub,sriramsub,19/Apr/13 15:10,22/Apr/13 23:51,14/Jul/23 05:39,22/Apr/13 23:51,0.8.0,,,0.8.0,,,,,,,,,,0,kafka-0.8,p1,,"When we recover a segment, we do a shallow iteration to fix the index. This creates entries in the index with the last logical offset in a compressed messageSet. However, during log append we use the starting offset in a compressed message set to populate the index (We changed to starting offset recently). This causes inconsistencies. ",,junrao,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/13 22:54;sriramsub;KAFKA-866-v2.patch;https://issues.apache.org/jira/secure/attachment/12579923/KAFKA-866-v2.patch","22/Apr/13 21:31;sriramsub;KAFKA-866.patch;https://issues.apache.org/jira/secure/attachment/12579909/KAFKA-866.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,323974,,,Mon Apr 22 23:51:17 UTC 2013,,,,,,,,,,"0|i1jw8n:",324319,,,,,,,,,,,,,,,,,,,,"22/Apr/13 21:31;sriramsub;1. Recovery decompresses the message to get the starting offset
2. AbstractFetcherThread should do deep iteration. This was missed in the IndexOutOfBounds fix.;;;","22/Apr/13 22:54;sriramsub;On Jun's suggestion reverting the fetcher thread to perform shallow iteration for performance. We could hit corrupt messages but this should be rare.;;;","22/Apr/13 23:51;junrao;Thanks for the patch. +1. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error incrementing leader high watermark,KAFKA-862,12641931,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,10/Apr/13 21:40,11/Apr/13 18:55,14/Jul/23 05:39,11/Apr/13 18:55,0.8.0,,,,,,,,,,replication,,,0,kafka-0.8,p1,,"2013/04/10 20:04:31.529 ERROR [KafkaApis] [kafka-request-handler-6] [kafka] [] [KafkaApi-270] Error processing ProducerRequest with correlation id 242757885 from client null-13 on foo:3
java.lang.UnsupportedOperationException: empty.min
        at scala.collection.TraversableOnce$class.min(TraversableOnce.scala:317)
        at scala.collection.immutable.Set$EmptySet$.min(Set.scala:47)
        at kafka.cluster.Partition.maybeIncrementLeaderHW(Partition.scala:263)
        at kafka.server.KafkaApis$$anonfun$appendToLocalLog$2.apply(KafkaApis.scala:195)
        at kafka.server.KafkaApis$$anonfun$appendToLocalLog$2.apply(KafkaApis.scala:186)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.mutable.HashMap.map(HashMap.scala:39)
        at kafka.server.KafkaApis.appendToLocalLog(KafkaApis.scala:186)
        at kafka.server.KafkaApis.handleProducerRequest(KafkaApis.scala:120)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:60)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:41)
        at java.lang.Thread.run(Thread.java:619)
",,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-860,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,322346,,,Thu Apr 11 18:55:11 UTC 2013,,,,,,,,,,"0|i1jm73:",322691,,,,,,,,,,,,,,,,,,,,"10/Apr/13 22:32;nehanarkhede;This is caused by a race condition while updating inSyncReplicaSet. Root cause is Partition.maybeIncrementLeaderHW() does not synchronize on the leaderAndIsrUpdateLock causing it to read an empty in sync replica set. Now, the in sync replica set is empty because the broker is becoming a follower for the same partition at that time. The fix should include two things -

1. synchronize access to inSyncReplicaSet
2. fix the behavior of Partition.maybeIncrementLeaderHW() if the inSyncReplicaSet is empty. This should never happen since at least the leader should be in the set at all times.;;;","10/Apr/13 23:53;nehanarkhede;The fix for KAFKA-860 will probably fix this one as well;;;","11/Apr/13 18:55;nehanarkhede;Fix for KAFKA-860 fixes this bug;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBoundsException while fetching data from leader,KAFKA-861,12641925,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriramsub,sriramsub,sriramsub,10/Apr/13 21:20,16/Apr/13 01:12,14/Jul/23 05:39,16/Apr/13 01:12,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"2013-04-09 16:36:50,051] ERROR [ReplicaFetcherThread-0-261], Error due to  (kafka.server.ReplicaFetcherThread)
kafka.common.KafkaException: error processing data for topic firehoseUpdates partititon 14 offset 53531364
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$4.apply(AbstractFetcherThread.scala:136)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$4.apply(AbstractFetcherThread.scala:113)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:125)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:113)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:89)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)
Caused by: java.lang.IndexOutOfBoundsException
        at java.nio.Buffer.checkIndex(Buffer.java:512)
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:121)
        at kafka.message.Message.compressionCodec(Message.scala:202)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNextOuter(ByteBufferMessageSet.scala:174)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:197)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:145)
        at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:61)
        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:53)
        at scala.collection.IterableLike$class.isEmpty(IterableLike.scala:92)
        at kafka.message.MessageSet.isEmpty(MessageSet.scala:67)
        at scala.collection.TraversableLike$class.lastOption(TraversableLike.scala:512)
        at kafka.message.MessageSet.lastOption(MessageSet.scala:67)",,junrao,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/13 00:33;sriramsub;KAFKA-861-v2.patch;https://issues.apache.org/jira/secure/attachment/12578846/KAFKA-861-v2.patch","15/Apr/13 20:48;sriramsub;KAFKA-861.patch;https://issues.apache.org/jira/secure/attachment/12578813/KAFKA-861.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,322340,,,Tue Apr 16 01:12:55 UTC 2013,,,,,,,,,,"0|i1jm5r:",322685,,,,,,,,,,,,,,,,,,,,"11/Apr/13 21:23;sriramsub;This happens when an existing follower becomes the new leader and the current leader starts following the new leader. 

The existing follower closes the fetcher thread and transitions to become a leader
The current leader truncates its log to high water mark and starts following the new leader

The messageset that is received by the old follower during this transition contains only zero bytes. When we try to iterate through this messageset, we fail and throw the above exception.

What causes these zero bytes to be present in the messageset? It looks like when the old leader truncated its log, it was also trying to send bytes to the follower. These bytes were outside the truncated region. Somehow, the bytes after the highwatermark all became zeros. 

It turns out that in jdk 1.6 there is a bug in truncateTo that truncates the file but does not update the postion of the file. This is fixed in kafka by explicitly setting the position after the truncate call. However, a simple program below verifies that reading the file channel after the truncated region (without setting the position) is totally fine and does not return any bytes

1.
    // create a channel for a file
    val path = ""/home/myid/outfile1""
    val fileAccess = new RandomAccessFile(path, ""rw"")
    val fc = fileAccess.getChannel

2. 
    // create random buffer
    val b = ByteBuffer.allocate(100)
    new Random().nextBytes(b.array())
   
    // write the buffer to the channel
    fc.write(b)
    var pos = fc.position() // position is 100
    var size = fc.size() // size is 100

3.
    // truncate the channel
    fc.truncate(50)
    size = fc.size() // size is 50
    pos = fc.position() // position is 100

4. 
    // transfer the truncated portition to a channel
    val path1 = ""/home/myid/outfile2""
    val f2 = new RandomAccessFile(path1, ""rw"")
    val fc1 = f2.getChannel
    val transferred = fc.transferTo(50, 50, fc1) // transferred is 0

Further, if we add the 3"" step below after step 3 above, it can be seen that step 4 does return non zero bytes and they all contain 0 bytes.

3""

   // write more bytes
    b.rewind()
    fc.write(b)
    pos = fc.position() // position is 200
    size = fc.size() // size is 200

The code above shows that appending to a file without setting the position after truncate does expose the zero bytes to the reader. But in kafka, truncate/set position and append are all synchronized. This means we should not hit the issue above. 

This could mean there is a race condition in FileChannelImpl that could somehow cause this. The code snippet below from transferTo method from FileChannelImpl might explain what we see. 

        long sz = size();   -- > checks size. size() is synchronized with other FileChannelImpl methods
        if (position > sz)
            return 0; --> This is what is returned in step 4 above in the first case. The size is smaller than the position requested. However, truncate can happen after this line.
        int icount = (int)Math.min(count, Integer.MAX_VALUE);
        if ((sz - position) < icount)
            icount = (int)(sz - position);

        long n;

        // Attempt a direct transfer, if the kernel supports it
        if ((n = transferToDirectly(position, icount, target)) >= 0) // the size check above could have been good above but at this point the size is smaller than the requested 
            return n;                                                                 // position. transferToDirectly calls transferTo0 which could just read the zero bytes written by truncate.


Few open questions
1. Does truncate zero out the bytes synchronously or lazily? If it is lazy, we could also get junk bytes instead of zeros
2. How to fix it in kafka. One possible fix is to ensure that the MessageSet iterator throws invalid message when it encounters 0 byte size or if crc does not match the message. The follower can then try to refetch the offset for that topic partition or just fail (atleast we know the cause). 

    

   ;;;","15/Apr/13 20:48;sriramsub;Fixes the size = 0 in the iterator.The fetcher thread logs and does not update the offset and retries again in the next loop for that topic partition.;;;","15/Apr/13 23:56;junrao;Thanks for the patch. A couple of comments:

1. ByteBufferMessageSet: Instead of check for size <= 0, it's probably better to check for size < Message.MinHeaderSize.

2. AbstractFetcher: Could we add a comment when we catch InvalidMessageException to explain the particular problem that we have seen and why refetching the data will likely solve the issue?;;;","16/Apr/13 00:17;nehanarkhede;Thanks for the patch. Good catch!

1. AbstractFetcherThread -
1.1 Let's change the logging to ""[%s,%d]"" for printing the topic partition. This is what most of the code is standardized upon right now. It will make it easier to grep through the logs.
1.2 While you're in there, do you mind fixing this typo - ""partititon""

2 ByteBufferMessageSet: +1 on Jun's suggestion above.

;;;","16/Apr/13 00:41;nehanarkhede;+1 on v2;;;","16/Apr/13 01:12;junrao;Thanks for patch v2. We should do ""size < Message.MinHeaderSize"" instead of ""size <= Message.MinHeaderSize"" in ByteBufferMessagesSet since it's possible to have a valid message whose size is exactly Message.MinHeaderSize. Committed to 0.8 with the change.

Great investigation.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replica fetcher thread errors out and dies during rolling bounce of cluster,KAFKA-860,12641921,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,10/Apr/13 21:12,30/Apr/13 17:14,14/Jul/23 05:39,30/Apr/13 17:14,0.8.0,,,,,,,,,,replication,,,0,kafka-0.8,p1,,"2013/04/10 20:04:32.071 ERROR [ReplicaFetcherThread] [ReplicaFetcherThread-0-272] [kafka] [] [ReplicaFetcherThread-0-272], Error due to 
kafka.common.KafkaException: error processing data for topic PageViewEvent partititon 3 offset 2482625623
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$4.apply(AbstractFetcherThread.scala:135)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$4.apply(AbstractFetcherThread.scala:113)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:113)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:89)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)
Caused by: java.lang.RuntimeException: Offset mismatch: fetched offset = 2482625623, log end offset = 2482625631.
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:49)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$4.apply(AbstractFetcherThread.scala:132)
        ... 5 more

This causes replica fetcher thread to shut down",,aozeritsky,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-862,,,,,,,,,,,,,,,,,,,,,,"11/Apr/13 00:46;nehanarkhede;kafka-860-v1.patch;https://issues.apache.org/jira/secure/attachment/12578121/kafka-860-v1.patch","11/Apr/13 01:42;nehanarkhede;kafka-860-v2.patch;https://issues.apache.org/jira/secure/attachment/12578131/kafka-860-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,322336,,,Tue Apr 30 17:14:17 UTC 2013,,,,,,,,,,"0|i1jm4v:",322681,,,,,,,,,,,,,,,,,,,,"10/Apr/13 22:40;nehanarkhede;This is caused by a race condition between the old leader's local append to the log and the new follower's log truncation. Specifically, the following causes the bug-

1. Current leader receives a produce request.
2. Broker receives leader and isr request making it a follower now
3. Broker starts become follower and truncates log 
4. Broker, not knowing it is not the leader anymore, continues with the produce request and appends some data to the log
5. Become follower starts a fetcher with the old log end offset

At step 5, it runs into the error;;;","11/Apr/13 00:46;nehanarkhede;The root cause is that during produce request handling, we acquire different locks to check if the broker is a leader and then append messages atomically. The fix is to move the append to Partition, so that either it is the leader and it finishes the append or it rejects the produce request since it is becoming a follower. No interleaving should happen.;;;","11/Apr/13 01:42;nehanarkhede;Jun made a good point when we discussed this offline. The solution is correct but there is a performance hit. Basically, the only requirement is to have become-leader/become-follower/update-isr block the appends. But we shouldn't let 2 appends block each other. Implemented that using a read-write lock;;;","11/Apr/13 05:10;junrao;Thanks for patch v2. This turns out to be a bit more tricky.

1. First of all, instead of using ""leaderIsrReadLock synchronized"", we should do ""leaderIsrReadLock.lock()"".

2. Second, we should use a fair readWriteLock. Otherwise, some threads may be indefinitely postponed. 

3. Third, from java doc, ReentrantReadWriteLock doesn't support upgrading from read lock to write loc.
"" Lock downgrading
Reentrancy also allows downgrading from the write lock to a read lock, by acquiring the write lock, then the read lock and then releasing the write lock. However, upgrading from a read lock to the write lock is not possible. ""

This means that if we need to call updateIsr(), we have to first release the read lock and require the read lock again when done. See the following example. However, this means that we are still vulnerable to the issue in maybeIncrementLeaderHW() (kafka-862). We probably can change the logic in maybeIncrementLeaderHW() so that it can handle empty set. We will need to think a bit more how to write the logic in a clean way.

http://codereview.stackexchange.com/questions/12939/reentrantreadwritelock-lock-upgrade-method

Another possibility is to just take v1 patch. All producers to the same log will sync on the leaderIsrUpdateLock. In log.append(), the only code outside the log lock are analyzeAndValidateMessageSet() and maybeFlush(). The former is cheap since it does shallow iteration. The latter re-requires the log lock if flush if needed.;;;","11/Apr/13 05:50;nehanarkhede;I like the option of picking a simpler solution for now and filing a performance improvement bug to come back and do it properly. ;;;","11/Apr/13 16:22;jkreps;The reason for the more granular locking in Log was to avoid locking around the flush. However we since learned that the flush effectively locks the log no matter what, so it doesn't make any difference. So I am not sure that V1 will be a performance hit. What would be really nice would be automated perf tests that checked this kind of thing so we could spot regressions.;;;","11/Apr/13 18:54;nehanarkhede;I checked in patch v1 and will see how that goes.;;;","30/Apr/13 17:14;nehanarkhede;v1 fixes the issue;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
High watermark values can be overwritten during controlled shutdown,KAFKA-858,12641669,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,09/Apr/13 17:28,09/Apr/13 22:01,14/Jul/23 05:39,09/Apr/13 22:01,0.8.0,,,,,,,,,,replication,,,0,kafka-0.8,p1,,"Race condition between controlled shutdown, actual process shutdown and high watermark checkpoint thread frequency can cause high watermark values for a subset of partitions to be overwritten. So even if the controller sends a complete list of partitions to the broker, the highwatermark for some partitions is still 0. This causes the follower to fetch from the leader's start offset.",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/13 20:38;nehanarkhede;kafka-858-v2.patch;https://issues.apache.org/jira/secure/attachment/12577886/kafka-858-v2.patch","09/Apr/13 18:12;nehanarkhede;kafka-858.patch;https://issues.apache.org/jira/secure/attachment/12577845/kafka-858.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,322085,,,Tue Apr 09 22:01:27 UTC 2013,,,,,,,,,,"0|i1jkl3:",322430,,,,,,,,,,,,,,,,,,,,"09/Apr/13 18:12;nehanarkhede;-This is a corner case bug that shows up only if, after the controlled shutdown is issued to the broker and before the broker actually shuts down, the high watermark checkpoint thread wakes up and writes the high watermarks.
- Since we depend on allPartitions map to checkpoint high watermarks, I think the safest and least intrusive fix is to never remove partitions from that map. If we do, then we run into a risk of overwriting high watermark values for a partition.
- If we do the above, we have to take care that the partition is essentially dormant from the point of view of the broker. So getPartition() should not return partitions that have been stopped during controlled shutdown. This will take care of failing future produce/fetching or having purgatory return the proper error code. So I added a stoppedPartitions set that maintains the list of stopped partitions. 
- PartitionCount mbean will show only the alive partitions
- Stop replica request is half baked right now, but we don't need to focus on fixing it since those changes will be pretty big, intrusive and risky. All we need to make sure is that when stop replica request is received, we stop the fetcher and remove the partition from leaderPartitions, so the broker does not attempt to shrink isr for such partitions.
- Apart from the fix, I cleaned up logging in two places -
1. adding/removing fetchers to print partition the way the rest of the code does. This is required for ease of grepping for a particular partition
2. state change log during leader/follower request to remove redundant entries
;;;","09/Apr/13 19:38;junrao;Thanks for the patch. Great catch of this bug.

Not sure about the change in ReplicaManager though. It seems that a simpler fix is to in stopReplica(), only remove a partition from allPartitions if deletePartition is true. A partition still exists if its fetcher is stopped. It only stops to exist if the controller decides to delete it (currently, only because of partition reassignment). At this point, it can be taken out of allPartitions.;;;","09/Apr/13 20:38;nehanarkhede;Thanks for the review, Jun! Your suggestion is simpler, included that in the v2 patch.;;;","09/Apr/13 21:09;junrao;Thanks for patch v2. +1.;;;","09/Apr/13 22:01;nehanarkhede;Checked in patch v2, thanks for the reviews;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correlation id for OffsetFetch request (#2) always responds with 0,KAFKA-856,12641386,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,mtanski,mtanski,08/Apr/13 16:01,30/May/13 04:44,14/Jul/23 05:39,30/May/13 04:44,0.8.0,0.8.1,,0.8.0,,,,,,,network,,,1,,,,"The in the new Kafka when making an OffsetFetch request the correlation id always response is always sent as 0. It doesn't matter if the client request specifies a correlation id other then 0.

Example wireshark capture:

00000000  00 00 00 31 00 07 00 00  00 00 00 2a 00 03 66 6f ...1.... ...*..fo
00000010  6f 00 0a 74 65 73 74 2d  67 72 6f 75 70 00 00 00 o..test- group...
00000020  01 00 0a 74 65 73 74 5f  74 6f 70 69 63 00 00 00 ...test_ topic...
00000030  01 00 00 00 00                                   .....
 
Request #1
--------
len:    00 00 00 31
api:    00 07
ver:    00 00
cor:    00 00 00 2a
cid:    2a 00 03 66 6f 6f
....
 
00000000  00 00 00 2d 00 00 00 2a  00 03 66 6f 6f 00 00 00 ...-...* ..foo...
00000010  01 00 0a 74 65 73 74 5f  74 6f 70 69 63 00 00 00 ...test_ topic...
00000020  01 00 00 00 00 ff ff ff  ff ff ff ff ff 00 00 00 ........ ........
00000030  03
 
Response #1
--------
len:    00 00 00 2d
cor:    00 00 00 2a
cid:    2a 00 03 66 6f 6f
....
 
00000035  00 00 00 35 00 02 00 00  00 00 00 2a 00 03 66 6f ...5.... ...*..fo
00000045  6f ff ff ff ff 00 00 00  01 00 0a 74 65 73 74 5f o....... ...test_
00000055  74 6f 70 69 63 00 00 00  01 00 00 00 00 ff ff ff topic... ........
00000065  ff ff ff ff fe 00 00 00  01                      ........ .
 
Request #2
---------
len:    00 00 00 35
api:    00 02
ver:    00 00
cor:    00 00 00 2a
cid:    00 03 66 6f 6f
 
00000031  00 00 00 2a 00 00 00 00  00 00 00 01 00 0a 74 65 ...*.... ......te
00000041  73 74 5f 74 6f 70 69 63  00 00 00 01 00 00 00 00 st_topic ........
00000051  00 00 00 00 00 01 00 00  00 00 00 00 00 00       ........ ......
 
Response #2:
----------
len:    00 00 00 2a
cor:    00 00 00 00
alen:   00 00 00 01
....",Linux & Mac,junrao,mtanski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-642,,,,,,,,,,,,,"08/Apr/13 17:13;mtanski;KAFKA-856.patch;https://issues.apache.org/jira/secure/attachment/12577569/KAFKA-856.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,321802,,,Thu May 30 04:44:49 UTC 2013,,,,,,,,,,"0|i1jiu7:",322147,,,,,,,,,,,,,,,,,,,,"08/Apr/13 17:12;mtanski;I have a patch to fix this;;;","08/Apr/13 17:13;mtanski;The mentioned patch.;;;","15/May/13 23:39;lanzaa;This would be nice to have in a 0.8 release, so client writers would not have to special case offsetrequest / offsetresponse not maintaining the correlationid. If this will not make it into 0.8, the protocol doc should be updated to reflect this oddity.;;;","15/May/13 23:56;lanzaa;Kafka-642 was where the correlation id was added to the protocol. However, in the readFrom method the correlation id was not passed to the constructor.;;;","30/May/13 04:44;junrao;Thanks for the patch. +1. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove clientId from OffsetFetchResponse and OffsetCommitResponse,KAFKA-852,12641052,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mumrah,mumrah,mumrah,05/Apr/13 14:26,11/Jul/13 22:37,14/Jul/23 05:39,11/Jul/13 22:37,0.8.1,,,0.8.1,,,,,,,core,,,0,,,,These are not needed and conflict with the API documentation. Should be removed to be consistent with other APIs,,jkreps,junrao,mtanski,mumrah,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/13 18:41;mumrah;0001-KAFKA-852-remove-clientId-from-Offset-Fetch-Commit-R.patch;https://issues.apache.org/jira/secure/attachment/12577257/0001-KAFKA-852-remove-clientId-from-Offset-Fetch-Commit-R.patch","08/Apr/13 17:28;mumrah;KAFKA-852.diff;https://issues.apache.org/jira/secure/attachment/12577573/KAFKA-852.diff","14/Jun/13 14:28;mumrah;KAFKA-852v2.diff;https://issues.apache.org/jira/secure/attachment/12587818/KAFKA-852v2.diff",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,321468,,,Thu Jul 11 22:37:49 UTC 2013,,,,,,,,,,"0|i1jgrz:",321813,,,,,,,,,,,,,,,,,,,,"05/Apr/13 18:41;mumrah;v1 of patch attached, simple change;;;","08/Apr/13 16:35;nehanarkhede;Thanks for the patch, David. However, I'm not able to apply it using the following commands -

patch -p1 kafka-852.patch
patch -p0 kafka-852.patch


Please create a patch using the following command -

git diff <local-0.8-branch> > kafka-852.patch
OR
git remote update
git diff origin/0.8 > kafka-852.patch;;;","08/Apr/13 17:30;mumrah;@Neha, I attached a diff created using git-diff. The previous patch was generated using git format-patch.

Also, this issue is targeting 0.8.1 (which I'm assuming is trunk) not 0.8. The Offset fetch/commit APIs are not slated for the 0.8 release AFAIK;;;","10/Apr/13 00:47;nehanarkhede;I think you meant to remove correlation id right ? ;;;","10/Apr/13 12:38;mumrah;The responses should include correlation, but not client id. The client id is just used for logging purposes on the broker, but correlation is used for clients to do async request/response handling. From the protocol wiki page: ""Response => CorrelationId ResponseMessage""

I had incorrectly included the client id in these two new responses (I originally was including version id as well, which was also a mistake - KAFKA-759).

https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-Responses;;;","30/May/13 04:48;junrao;Sorry for the late review. The patch no longer applies to trunk. Could you rebase? Thanks,;;;","14/Jun/13 14:26;mumrah;v2 of patch rebased against current trunk.;;;","14/Jun/13 14:28;mumrah;Meant ""diff"" not ""patch"";;;","11/Jul/13 22:37;jkreps;Sorry for lagging on this. Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken handling of leader and isr request leads to incorrect high watermark checkpoint file,KAFKA-851,12640888,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,04/Apr/13 22:35,05/Apr/13 04:28,14/Jul/23 05:39,05/Apr/13 04:28,0.8.0,,,,,,,,,,replication,,,0,kafka-0.8,p1,,"The broker depends on receiving a list of *all* partitions from the controller on startup. It uses this information to create a list of partitions that will get check pointed to the high watermark checkpoint file. However, during a make follower operation, it adds a partition to the high watermark checkpoint list only if its leader is alive. Due to this, even if the controller sends a full list of partitions to the broker, replica manager filters it to keep only those partitions whose leader is alive. This leads to the high watermark value for the rest of those partitions to reset to 0. Which, in turn, leads to the follower to fetch from the beginning of leader's log instead of min(log end offset, high watermark). The effect of this is very long lag on the replica fetchers leading to high under replicated partition count",,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/13 23:51;nehanarkhede;kafka-851-v1.patch;https://issues.apache.org/jira/secure/attachment/12577112/kafka-851-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,321347,,,Fri Apr 05 04:28:26 UTC 2013,,,,,,,,,,"0|i1jg1j:",321692,,,,,,,,,,,,,,,,,,,,"04/Apr/13 23:51;nehanarkhede;Fixed Partition.makeFollower() to do the following irrespective of whether the leader is alive or not -

getOrCreateReplica: This creates a local replica, if one does not exist. This ensures that, on the very first leader and isr request, the broker will create a replica object for every partition in the leader and isr request. As part of this, it reads the previous high watermark value for that partition and creates a replica object with that high watermark value. This ensures that the right high watermark value gets checkpointed by the checkpoint thread. This will also ensure that all partitions will get checkpointed to the file
;;;","05/Apr/13 01:27;junrao;Good catch. +1.;;;","05/Apr/13 02:01;swapnilghike;+1 Great catch!;;;","05/Apr/13 04:28;nehanarkhede;Thanks for the quick review, guys! Checked in patch to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in controller's startup/failover logic fails to update in memory leader and isr cache causing other state changes to work incorrectly,KAFKA-849,12640707,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,04/Apr/13 06:12,05/Apr/13 04:27,14/Jul/23 05:39,05/Apr/13 04:27,0.8.0,,,,,,,,,,controller,,,0,kafka-0.8,p1,,"partitionLeadershipInfo is the in memory cache of the controller that keeps track of every partition's ""last elected"" leader and isr. On controller startup/failover, this cache is bootstrapped only with those partitions whose leader is alive. This causes the leader and isr cache to be initialized incorrectly causing other state transitions related to new broker startup, existing broker failure to not work correctly. For instance, it does not allow the controller to send the list of *all* replicas that exist on a broker to it during startup.

Another bug during controller startup is that it invokes OnlinePartition state change before OnlineReplica state change. This also breaks the guarantee that the controller sends a full list of replicas to a broker on startup",,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/13 23:15;nehanarkhede;kafka-849-v1.patch;https://issues.apache.org/jira/secure/attachment/12577105/kafka-849-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,321166,,,Fri Apr 05 04:27:49 UTC 2013,,,,,,,,,,"0|i1jexb:",321511,,,,,,,,,,,,,,,,,,,,"04/Apr/13 23:13;nehanarkhede;Fixed the bug so that leader and isr cache is updated whether or not the leader is alive. This is the right thing to do since the purpose of the cache is to record the last decision made. On controller failover, this is the value read from zookeeper.

Other than that, fixed couple other issues -

1. Changed list topics tool to also print whether or not the partition is under replicated. This makes it very easy to script the output of list topics to show only partitions that are under replicated
2. Reduced the noise in the logs due to failed metadata requests. There is not much value in logging this since when some brokers are down, the stack trace just complains that those brokers are down. We still return the correct error code to the client, so turned this error message to debug;;;","05/Apr/13 01:16;junrao;Thanks for the patch. +1. The changes related to list topic are not sufficient though. The problem is that if a broker is down, AdminUtils.fetchTopicMetadataFromZk returns an empty replica list. There is a patch in kafka-850 that fixes this issue more completely.;;;","05/Apr/13 01:26;swapnilghike;+1 Both were great catches!;;;","05/Apr/13 03:55;nehanarkhede;Thanks Jun and Swapnil for the quick review! Jun, I agree that these changes are not complete. I saw that you have it covered in 850 so left it from here.;;;","05/Apr/13 04:27;nehanarkhede;Committed to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StackOverFlowError,KAFKA-848,12640652,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,siningma,siningma,03/Apr/13 23:13,07/Feb/15 23:43,14/Jul/23 05:39,07/Feb/15 23:43,0.7.1,,,,,,,,,,producer ,,,0,,,,"
I am using kafka 0.7.1 right now.
I am using the following log4j properties file and trying to send some log 
information to kafka server.



log4j.rootLogger=INFO,file,stdout

log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=[%d] %p %t %m (%c)%n

log4j.appender.file=org.apache.log4j.RollingFileAppender
#log4j.appender.file.FileNamePattern=c:\\development\\producer-agent_%d{yyyy-MM-dd}.log
log4j.appender.file.File=${AC_DATA_HOME}\\lmservice\\tailer-aggregator.log
log4j.appender.file.MaxFileSize=100MB
log4j.appender.file.MaxBackupIndex=1
log4j.appender.file.layout=org.apache.log4j.PatternLayout
#log4j.appender.file.layout.ConversionPattern= %-4r [%t] %-5p %c %x - %m%n
log4j.appender.file.layout.ConversionPattern=[%d] %p %t %m (%c)%n

log4j.appender.KAFKA=kafka.producer.KafkaLog4jAppender
log4j.appender.KAFKA.layout=org.apache.log4j.PatternLayout
log4j.appender.KAFKA.layout.ConversionPattern=[%d] %p %t %m (%c)%n
log4j.appender.KAFKA.BrokerList=0:localhost:9092
log4j.appender.KAFKA.SerializerClass=kafka.serializer.StringEncoder
log4j.appender.KAFKA.Topic=test.topic


# Turn on all our debugging info
log4j.logger.kafka=INFO, KAFKA
log4j.logger.org=INFO, KAFKA
log4j.logger.com=INFO, KAFKA

When I run the program without KafkaLog4jAppender, everything is fine. Then I add KafkaLog4jAppender.
The producer can send messages through KafkaLog4jAppender to Kafka server.
However, after the producer send messages for a while. I see StackOverflowError. It seems that the producer or something else is recursively calling some functions.
This error will come out from many threads. I only copy 3 threads call stack below.

CallStack:
Exception in thread ""Process_Manager"" java.lang.StackOverflowError
        at java.util.regex.Pattern$1.isSatisfiedBy(Unknown Source)
        at java.util.regex.Pattern$5.isSatisfiedBy(Unknown Source)
        at java.util.regex.Pattern$5.isSatisfiedBy(Unknown Source)
        at java.util.regex.Pattern$CharProperty.match(Unknown Source)
        at java.util.regex.Pattern$GroupHead.match(Unknown Source)
        at java.util.regex.Pattern$Branch.match(Unknown Source)
        at java.util.regex.Pattern$Branch.match(Unknown Source)
        at java.util.regex.Pattern$Branch.match(Unknown Source)
        at java.util.regex.Pattern$BranchConn.match(Unknown Source)
        at java.util.regex.Pattern$GroupTail.match(Unknown Source)
        at java.util.regex.Pattern$Curly.match0(Unknown Source)
        at java.util.regex.Pattern$Curly.match(Unknown Source)
        at java.util.regex.Pattern$GroupHead.match(Unknown Source)
        at java.util.regex.Pattern$Branch.match(Unknown Source)
        at java.util.regex.Pattern$Branch.match(Unknown Source)
        at java.util.regex.Pattern$BmpCharProperty.match(Unknown Source)
        at java.util.regex.Pattern$Start.match(Unknown Source)
        at java.util.regex.Matcher.search(Unknown Source)
        at java.util.regex.Matcher.find(Unknown Source)
        at java.util.Formatter.parse(Unknown Source)
        at java.util.Formatter.format(Unknown Source)
        at java.util.Formatter.format(Unknown Source)
        at java.lang.String.format(Unknown Source)
        at scala.collection.immutable.StringLike$class.format(StringLike.scala:251)
        at scala.collection.immutable.StringOps.format(StringOps.scala:31)
        at kafka.utils.Logging$class.msgWithLogIdent(Logging.scala:28)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)

Exception in thread ""Thread-3"" java.lang.StackOverflowError
        at sun.nio.ch.SocketDispatcher.writev0(Native Method)
        at sun.nio.ch.SocketDispatcher.writev(Unknown Source)
        at sun.nio.ch.IOUtil.write(Unknown Source)
        at sun.nio.ch.SocketChannelImpl.write(Unknown Source)
        at java.nio.channels.SocketChannel.write(Unknown Source)
        at kafka.network.BoundedByteBufferSend.writeTo(BoundedByteBufferSend.scala:49)
        at kafka.network.Send$class.writeCompletely(Transmission.scala:73)
        at kafka.network.BoundedByteBufferSend.writeCompletely(BoundedByteBufferSend.scala:25)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:95)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)
        at org.apache.log4j.Category.info(Category.java:666)
        at kafka.utils.Logging$class.info(Logging.scala:58)
        at kafka.producer.SyncProducer.info(SyncProducer.scala:39)
        at kafka.producer.SyncProducer.disconnect(SyncProducer.scala:153)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:99)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:94)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:125)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:114)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:100)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:100)
        at kafka.producer.Producer.configSend(Producer.scala:159)
        at kafka.producer.Producer.send(Producer.scala:100)
        at kafka.producer.KafkaLog4jAppender.append(KafkaLog4jAppender.scala:83)
        at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
        at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
        at org.apache.log4j.Category.callAppenders(Category.java:206)
        at org.apache.log4j.Category.forcedLog(Category.java:391)

",Win7 64 bit,siningma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,321111,,,2013-04-03 23:13:01.0,,,,,,,,,,"0|i1jel3:",321456,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka appender layout does not work for kafka 0.7.1,KAFKA-847,12640651,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,peterp,siningma,siningma,03/Apr/13 23:11,22/Sep/14 18:03,14/Jul/23 05:39,15/Sep/14 03:24,0.7.1,0.8.0,,0.8.2.0,,,,,,,producer ,,,4,easyfix,newbie,,"I am using kafka 0.7.1 right now.
I am using the following log4j properties file and trying to send some log information to kafka server.

log4j.rootLogger=INFO,file,stdout

log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=[%d] %p %t %m (%c)%n

log4j.appender.file=org.apache.log4j.RollingFileAppender
#log4j.appender.file.FileNamePattern=c:\\development\\producer-agent_%d{yyyy-MM-dd}.log
log4j.appender.file.File=${AC_DATA_HOME}\\lmservice\\tailer-aggregator.log
log4j.appender.file.MaxFileSize=100MB
log4j.appender.file.MaxBackupIndex=1
log4j.appender.file.layout=org.apache.log4j.PatternLayout
#log4j.appender.file.layout.ConversionPattern= %-4r [%t] %-5p %c %x - %m%n
log4j.appender.file.layout.ConversionPattern=[%d] %p %t %m (%c)%n

log4j.appender.KAFKA=kafka.producer.KafkaLog4jAppender
log4j.appender.KAFKA.layout=org.apache.log4j.PatternLayout
log4j.appender.KAFKA.layout.ConversionPattern=[%d] %p %t %m (%c)%n
log4j.appender.KAFKA.BrokerList=0:localhost:9092
log4j.appender.KAFKA.SerializerClass=kafka.serializer.StringEncoder
log4j.appender.KAFKA.Topic=test.topic

# Turn on all our debugging info
log4j.logger.kafka=INFO, KAFKA
log4j.logger.org=INFO, KAFKA
log4j.logger.com=INFO, KAFKA

However, I find that the messages send to kafka server are not formatted as my defined layout. 
KafkaLog4jAppender just sends messages(%m), and there is no other conversion patterns.",Win7 64 bit,grace.huang,junrao,lotka,nehanarkhede,peterp,siningma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/14 04:31;peterp;KAFKA-847-v1.patch;https://issues.apache.org/jira/secure/attachment/12628429/KAFKA-847-v1.patch","27/Aug/14 13:54;lotka;KAFKA-847-v2.patch;https://issues.apache.org/jira/secure/attachment/12664646/KAFKA-847-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,321110,,,Mon Sep 15 03:24:24 UTC 2014,,,,,,,,,,"0|i1jekv:",321455,,nehanarkhede,,,,,,,,,,,,,,,,,,"12/Feb/14 04:31;peterp;Set requiresLayout to true so that the log4j Kafka appender accepts layout parameters set in a log4j.properties;;;","26/Aug/14 11:14;lotka;Is this patch going to be included in the next release? The task is not assigned to any version. ;;;","26/Aug/14 14:37;junrao;Yes, we can. The patch seems to contain more stuff than just setting requiresLayout to true. Do you want to submit a new patch to trunk?;;;","27/Aug/14 13:54;lotka;A patch file containing changes only in KafkaLog4jAppender;;;","15/Sep/14 03:24;nehanarkhede;Pushed v2 to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractFetcherThread should do shallow instead of deep iteration,KAFKA-846,12640533,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,junrao,junrao,03/Apr/13 15:20,04/Apr/13 16:55,14/Jul/23 05:39,04/Apr/13 00:16,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"Deep iteration does decompression, which introduces unnecessary overhead.",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/13 15:21;junrao;kafka-846.patch;https://issues.apache.org/jira/secure/attachment/12576795/kafka-846.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,320993,,,Thu Apr 04 00:16:53 UTC 2013,,,,,,,,,,"0|i1jdtz:",321334,,,,,,,,,,,,,,,,,,,,"03/Apr/13 15:21;junrao;Attach a patch.;;;","03/Apr/13 20:58;nehanarkhede;Thanks for the patch! +1 
Minor comment => Typo -> parititon in AbstractFetcherThread;;;","04/Apr/13 00:16;junrao;Thanks for the review. Committed to 0.8 after fixing the typo. Also fixed an issue with type mismatch in the logging statement.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mirror maker can lose some messages during shutdown,KAFKA-842,12640228,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,02/Apr/13 03:58,03/Apr/13 20:45,14/Jul/23 05:39,03/Apr/13 20:45,0.8.0,,,,,,,,,,tools,,,0,,,,"On shutdown, mirror maker can lose a few messages.",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/13 15:55;nehanarkhede;kafka-842.patch;https://issues.apache.org/jira/secure/attachment/12576594/kafka-842.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,320691,,,Wed Apr 03 20:45:47 UTC 2013,,,,,,,,,,"0|i1jbyv:",321032,,,,,,,,,,,,,,,,,,,,"02/Apr/13 15:55;nehanarkhede;Mirror maker was losing few messages even on a clean shutdown. This is because, on shutdown, we only waited for the embedded producer to send data to an async producer. The messages were lost from the async producer's queue since we didn't call producer.close() that flushes the outstanding messages in the producer's queue. 

This patch fixes the bug. The system test failed due to this bug and another bug in the system test itself. Will file a separate bug to fix the test.;;;","03/Apr/13 16:21;junrao;Thanks for the patch. +1. Just one minor comment. Could we put the following line in ProducerThread.run() inside a ""if(logger.isDebugEnabled())"" test?
            logger.debug(""Sending message %s"".format(new String(data.message())));
;;;","03/Apr/13 20:45;nehanarkhede;Thanks for the review, Jun! Made that change and committed to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller tries to perform preferred replica election on failover before state machines have started up,KAFKA-840,12639679,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,28/Mar/13 21:10,04/Apr/13 06:43,14/Jul/23 05:39,01/Apr/13 16:17,0.8.0,,,0.8.0,,,,,,,controller,,,0,bugs,,,"If the admin/preferred_replica_election path is non-empty when a new controller starts, the controller attempts to perform preferred replica election before the partition and replica state machine have been initialized. In this case, the controller will try to make invalid state transitions. ",,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/13 21:18;swapnilghike;kafka-840-after-commit-v1.patch;https://issues.apache.org/jira/secure/attachment/12576438/kafka-840-after-commit-v1.patch","02/Apr/13 19:50;swapnilghike;kafka-840-after-commit-v2.patch;https://issues.apache.org/jira/secure/attachment/12576640/kafka-840-after-commit-v2.patch","30/Mar/13 00:39;swapnilghike;kafka-840-v2.patch;https://issues.apache.org/jira/secure/attachment/12576206/kafka-840-v2.patch","28/Mar/13 21:14;swapnilghike;kafka-840.patch;https://issues.apache.org/jira/secure/attachment/12575951/kafka-840.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,320148,,,Thu Apr 04 06:43:40 UTC 2013,,,,,,,,,,"0|i1j8m7:",320489,,,,,,,,,,,,,,,,,,,,"28/Mar/13 21:14;swapnilghike;I think the right solution is to move   initializeAndMaybeTriggerPartitionReassignment() and initializeAndMaybeTriggerPreferredReplicaElection() after the controller mbean is registered and the controller logs that it is ready to serve. Attached a patch.;;;","29/Mar/13 02:21;nehanarkhede;Thanks for the patch. In addition to fixing the issue at hand, I think it will be good to throw an IllegalOperationException if handleStateChanges() is invoked before startup() or after shutdown() in both PartitionStateMachine and ReplicaStateMachine;;;","30/Mar/13 00:39;swapnilghike;1. Renamed inShuttingdown in partition/replica state machines to isRunning, because the former does not let us distinguish between a state machine that has not started, and a state machine that has started and is not shutting down. Made changes at other places accordingly.

2. With this patch, if the state machine has not started, then handleStateChange() throws a StateChangeFailedException with a message ""because the state machine has not started"". ;;;","01/Apr/13 16:16;nehanarkhede;Cool fix for handling the ""not started"", ""started but shutdown"" conditions. +1
It's ok to not respond to new topic/new broker zookeeper watches, since on controller failover, the new controller will read the new topics/brokers and put them in New state.;;;","01/Apr/13 16:17;nehanarkhede;Patch v2 is checked in, thanks Swapnil!;;;","01/Apr/13 21:18;swapnilghike;I made a mistake. Patch v2 ignores a broker/topic change listener callback if the state machines have not started up. However, if such a callback occurs after the controller context has been initialized in controller failover, the topic/broker change will not be noticed by the state machines.

The right fix is to have to two separate flags in the state machines - hasStarted and hasShutdown.;;;","02/Apr/13 16:31;nehanarkhede;Thanks for the patch! One issue is that the !hasShutdown check should be within the lock, otherwise the state machine can end up trying to respond to a zookeeper listener even after it has shutdown. Today, this cannot happen since we close the zkclient before the controller shuts down, but this is a defensive check which will protect us if the shutdown order changes in the future.
;;;","02/Apr/13 19:31;swapnilghike;Makes sense. Should we also de-register the topic/broker change listeners when the partition/replica state machines are shutting down?;;;","02/Apr/13 19:35;nehanarkhede;That would make sense, but with the current shutdown logic of the server, zkclient is closed before controller is shutdown. So doing that will raise an error. ;;;","02/Apr/13 19:50;swapnilghike;Cool, attached a new patch 'after-commit-v2'.;;;","02/Apr/13 19:52;nehanarkhede;+1 on the latest patch, thanks Swapnil!;;;","04/Apr/13 06:19;swapnilghike;Did we check in the 'after-commit-v2' patch? Thanks.;;;","04/Apr/13 06:43;nehanarkhede;Committed 'after-commit-v2' patch to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0.8 Consumer prevents rebalance if consumer thread is blocked or slow,KAFKA-832,12639623,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,nehanarkhede,nehanarkhede,28/Mar/13 17:05,29/Mar/13 01:42,14/Jul/23 05:39,28/Mar/13 23:35,0.8.0,,,0.8.0,,,,,,,consumer,,,0,kafka-0.8,p2,,"0.8 consumer involves multiple locks across the fetcher manager, leader finder thread and the rebalance watcher executor that prevent rebalance from interrupting the consumer. Rebalance should always take precedence over consumption, not the other way around. The effect of this bug is that if a consumer is lagging and the fetcher thread is blocked on adding the next fetched data chunk to the internal blocking queue, rebalance is halted.",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/13 18:40;junrao;kafka-832.patch;https://issues.apache.org/jira/secure/attachment/12575910/kafka-832.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,320092,,,Thu Mar 28 23:35:54 UTC 2013,,,,,,,,,,"0|i1j89r:",320433,,,,,,,,,,,,,,,,,,,,"28/Mar/13 18:40;junrao;Attach a patch, which addresses the deadlock in the following way.

1. AbstractFetcherThread: make the lock acquiring in add/remove partition interruptible.

2. ConsumerFetcherManager: To stop connections, we first stop the leaderFinder thread and then stop all fetchers. The stopping of the leaderFinder thread uses interrupt and is expected to be non-blocking because of the changes make in item 1.;;;","28/Mar/13 22:36;nehanarkhede;+1, very well thought out. I think shutting down the leader finder thread which will then interrupt the partitionMapLock (if it is blocked on that) will work. ;;;","28/Mar/13 23:35;junrao;Thanks for the review. Committed to 0.8 after adding a trace logging when there is no partition for leader election.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller does not send the complete list of partitions to a newly started broker,KAFKA-831,12639613,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,28/Mar/13 16:12,29/Mar/13 15:27,14/Jul/23 05:39,28/Mar/13 23:30,0.8.0,,,,,,,,,,controller,,,0,kafka-0.8,p1,,"On a new broker startup, the controller is supposed to send the entire list of partitions that the new broker is supposed to host in one leader and isr request. In this request, it specifies if the new broker is the leader or follower for each of those partitions. For most of the partitions, the new broker will be a follower. However, for some partitions that don't have any other broker alive, the new broker will be the leader. The bug is that the controller first elects the leaders for offline partitons to see if the new broker is the leader for any of those and sends only those partitions for which leader was elected in the first leader and isr request. Right after that, it does send a leader and isr request with all partitions, but that breaks the guarantee that the very first leader and isr request has all partitions. As a result of this bug, the high watermark checkpointing logic behaves incorrectly.",,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/13 23:07;nehanarkhede;kafka-831-v1.patch;https://issues.apache.org/jira/secure/attachment/12575969/kafka-831-v1.patch","29/Mar/13 02:16;nehanarkhede;kafka-831-v2.patch;https://issues.apache.org/jira/secure/attachment/12576009/kafka-831-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,320082,,,Fri Mar 29 15:27:26 UTC 2013,,,,,,,,,,"0|i1j87j:",320423,,,,,,,,,,,,,,,,,,,,"28/Mar/13 23:07;nehanarkhede;- The fix is to switch the order of leader election and sending the list of all partitions to the newly restarted broker. 
- This is ok since the impact of doing that is that the newly started broker could be asked to become follower of a dead leader. On the ReplicaManager side, we actually abort a become follower if the leader is not alive anymore.
- Right after sending the list of all partitions, the controller will trigger leader election for all new and offline partitions. This will take care of the case when the newly started broker is the next leader of such partitions;;;","28/Mar/13 23:21;junrao;Thanks for the patch. +1.;;;","28/Mar/13 23:30;nehanarkhede;Thanks for the review, checked in!;;;","28/Mar/13 23:42;swapnilghike;This patch may not fix the issue. In the OfflineReplica->OnlineReplica state transition, we send a leaderAndIsr request that includes only those partitions whose leader is alive. So if another broker was down which was leading the partition that the broker that has recently started was following, the 1st leaderAndIsr request will miss that partition.;;;","28/Mar/13 23:49;swapnilghike;One way to avoid it could be that in the Online/OfflineReplica -> OnlineReplica state transition, send a leaderAndIsr request that contains all partitions. If the leader of the partition is offline, then the ReplicaManager will drop the request.

The replica manager will also log ""the leader for this partition went down during the state transition"", so perhaps we can change this log statement to say ""the leader for this partition is down"".;;;","29/Mar/13 02:16;nehanarkhede;Ack, you're right. The optimization of sending the leader and isr request only if the leader is alive is flawed. This is because even if the leader is alive at the time of the check, it can immediately go offline by the time the broker is ready to act on the request. In any case, the replica manager does the appropriate check just before executing the leader/follower state change on its end. 

- This patch removes the above optimization and sends the leader and isr request for all partitions in Offline/Online state
- Note that, in the first request, the partitions in NewPartition state are not included since they never had a leader. This is ok since it means that the broker never started a log for such partitions. So there is no worry of previously written checkpoint getting lost
;;;","29/Mar/13 15:15;junrao;+1 on patch v2.;;;","29/Mar/13 15:27;nehanarkhede;Thank you for the follow up review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
partition replica assignment map in the controller should be a Set,KAFKA-830,12639232,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,27/Mar/13 00:07,27/Mar/13 21:02,14/Jul/23 05:39,27/Mar/13 20:34,,,,0.8.0,,,,,,,controller,,,0,kafka-0.8,p1,,"partitionReplicaAssignment currently stores the list of assigned replicas as a sequence. When a broker comes online, the replica state machine adds the broker to the list of assigned replicas. It should do that only if the replica is already not in the list of assigned replicas. This causes the replication factor to be incorrectly calculated",,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/13 15:29;nehanarkhede;kafka-830-v1.patch;https://issues.apache.org/jira/secure/attachment/12575719/kafka-830-v1.patch","27/Mar/13 17:21;nehanarkhede;kafka-830-v2.patch;https://issues.apache.org/jira/secure/attachment/12575733/kafka-830-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,319702,,,Wed Mar 27 21:02:12 UTC 2013,,,,,,,,,,"0|i1j5v3:",320043,,,,,,,,,,,,,,,,,,,,"27/Mar/13 15:29;nehanarkhede;- Changed any replica list to a set. This makes sense since replica ids can never legitimately repeat.;;;","27/Mar/13 16:24;junrao;Thanks for the patch. One problem with changing assigned replicas from a list to a set is that it breaks the preferred replica logic. We assume that the first replica is the preferred one. So the ordering of the replicas is important. Perhaps we should keep replicas as a list and make sure that we don't add duplicates when changing it.;;;","27/Mar/13 17:21;nehanarkhede;Good catch! However, it doesn't make sense to keep it sequence since by definition those ids are unique. As for preferred replica logic, currently it is just the smallest replica id . So there are 2 options -

1. Store replica ids in a sorted set
2. For preferred replica, pick the smallest id from a traditional hash set

But in patch v2, I stuck to your suggestion since it is a much smaller change for 0.8. Will fix this in 0.8.1;;;","27/Mar/13 18:17;nehanarkhede;Actually, thinking about it more, even picking the smallest replica id does not make sense. This is because if we pick the first replica to be the largest broker id, the rest of the replica ids are smaller than the preferred replica in this case. We will have to keep this a sequence and be careful while adding a replica id to the assigned replica list;;;","27/Mar/13 19:19;swapnilghike;collection.mutable.LinkedHashSet will iterate the elements in the same sequence they were inserted. But using it will require changes at more places than the one line change in patch v2.;;;","27/Mar/13 20:18;junrao;+1 on patch v2.;;;","27/Mar/13 20:34;nehanarkhede;Thanks for the review;;;","27/Mar/13 21:02;nehanarkhede;Swapnil, thanks for pointing that out, we can remember to do this in 0.8.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mirror maker needs to share the migration tool request channel ,KAFKA-829,12639224,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,26/Mar/13 23:18,28/Mar/13 17:00,14/Jul/23 05:39,28/Mar/13 17:00,0.8.0,,,,,,,,,,tools,,,0,kafka-0.8,p1,,"0.8 mirror maker suffers from the same drawbacks that the original version of the migration tool did. The consumer gets blocked if its producer queue is full, even if there are other idle producers.",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/13 23:08;nehanarkhede;kafka-829-v1.patch;https://issues.apache.org/jira/secure/attachment/12575796/kafka-829-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,319694,,,Thu Mar 28 17:00:12 UTC 2013,,,,,,,,,,"0|i1j5tb:",320035,,,,,,,,,,,,,,,,,,,,"27/Mar/13 23:08;nehanarkhede;- Mirror maker shares the producer data channel and producer thread from migration tool. 
- Not sure what is the best place to put producer data channel and producer thread since it is relevant only to migration tool and mirror maker. So left it inside migration tool for now
- Same caveat as migration tool. If there are failures that causes producer threads to exit, the mirror maker will not shut down cleanly and will have to be killed;;;","28/Mar/13 16:42;junrao;Thanks for the patch. +1. Just one minor comment. Should ProducerDataChannel be protected at the package level like ProducerThread?;;;","28/Mar/13 17:00;nehanarkhede;Changed the visibility of ProducerDataChannel as per your suggestion. Thanks for the review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Preferred Replica Election does not delete the admin path on controller failover,KAFKA-828,12639173,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,26/Mar/13 19:56,28/Mar/13 15:51,14/Jul/23 05:39,28/Mar/13 15:51,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,kafka-0.8,p1,,,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/13 22:35;swapnilghike;kafka-828-v1.patch;https://issues.apache.org/jira/secure/attachment/12575600/kafka-828-v1.patch","26/Mar/13 22:46;swapnilghike;kafka-828-v2.patch;https://issues.apache.org/jira/secure/attachment/12575601/kafka-828-v2.patch","27/Mar/13 02:09;swapnilghike;kafka-828-v3.patch;https://issues.apache.org/jira/secure/attachment/12575639/kafka-828-v3.patch","28/Mar/13 06:28;swapnilghike;kafka-828-v4.patch;https://issues.apache.org/jira/secure/attachment/12575835/kafka-828-v4.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,319643,,,Thu Mar 28 15:51:23 UTC 2013,,,,,,,,,,"0|i1j5hz:",319984,,,,,,,,,,,,,,,,,,,,"26/Mar/13 22:35;swapnilghike;1. Moving removePartitionsFromPreferredReplicaElection() to onPreferredReplicaElection() so that when a controller fails over and tries to complete the preferred replica election, the admin path is deleted.

2. Moved the parsePreferredReplicaElectionData() to ZkUtils from the admin.PreferredReplicaLeaderElectionCommand. There was also another function in ZkUtils for the same purpose and it was outdated, removed it.;;;","26/Mar/13 22:46;swapnilghike;Converted KAFKA-814 to subtask of this as Sriram asked to. Added a one line change to PreferredReplicaLeaderSelector to log instead of throwing an exception if the preferred leader is already the current leader.;;;","27/Mar/13 02:09;swapnilghike;Actually since preferred replica election is not a heavy weight operation like partition reassignment (where we need to wait until the new followers catch up etc), we can move reading of the controllerContext.partitionsUndergoingPreferredReplicaElection to inside the controllerLock. Attached patch v3.;;;","27/Mar/13 15:01;junrao;Thanks for patch v3. A couple of comments:

1. There is a compilation error
[error] /Users/jrao/intellij_workspace/kafka_git/core/src/main/scala/kafka/controller/PartitionLeaderSelector.scala:129: type mismatch;
[error]  found   : Unit
[error]  required: (kafka.api.LeaderAndIsr, Seq[Int])
[error]       info(""Preferred replica %d is already the current leader for partition %s"".format(preferredReplica, topicAndPartition))
[error]           ^
[error] one error found

2. PreferredReplicaPartitionLeaderSelector: Is there any value in logging the following?
      info(""Preferred replica %d is already the current leader for partition %s"".format(preferredReplica, topicAndPartition))
The same info will be logged in KafkaController.removePartitionsFromPreferredReplicaElection().

3. ZkUtils.parsePreferredReplicaElectionData: This is very specific to preferred replica election. So, it's better to move it to  PreferredReplicaLeaderElectionCommand.
;;;","27/Mar/13 15:49;nehanarkhede;Agree with Jun on 1, 2 and 3.

The issue with your change to the leader selector is that it doesn't achieve the objective of KAFKA-814 completely. If a preferred replica is already the leader, we *do not* want to update zookeeper and send the become leader request to the replica again. It should just continue to work on the next partition. It doesn't achieve that. One way of handling that is throwing a special exception LeaderAlreadyElectedException (or a better name) and then catching and swallowing it inside electLeaderForPartition() ;;;","28/Mar/13 06:28;swapnilghike;Agree with all comments, uploading patch v4. 

Defined a new LeaderElectionNotNeededException that is thrown in selectLeader() when the current leader is the same as the preferred replica. electLeaderForPartition() swallows it.

Thanks for the reviews and sorry, I was quite lousy while including the patch for 814.;;;","28/Mar/13 15:44;nehanarkhede;Thanks for the patch ! +1;;;","28/Mar/13 15:51;nehanarkhede;Committed patch v4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
improve list topic output format,KAFKA-827,12639119,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,junrao,junrao,26/Mar/13 15:48,01/Apr/13 17:42,14/Jul/23 05:39,01/Apr/13 17:42,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,We need to make the output of list topic command more readable.,,jjkoshy,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/13 17:34;junrao;kafk-827_v3.patch;https://issues.apache.org/jira/secure/attachment/12576404/kafk-827_v3.patch","26/Mar/13 15:49;junrao;kafka-827.patch;https://issues.apache.org/jira/secure/attachment/12575531/kafka-827.patch","29/Mar/13 22:47;junrao;kafka-827_v2.patch;https://issues.apache.org/jira/secure/attachment/12576189/kafka-827_v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,319589,,,Mon Apr 01 17:42:38 UTC 2013,,,,,,,,,,"0|i1j55z:",319930,,,,,,,,,,,,,,,,,,,,"26/Mar/13 15:49;junrao;Attach a patch.;;;","26/Mar/13 18:58;jjkoshy;While you are touching this, would it be reasonable to also switch from using the AdminUtil to a blank TopicMetadataRequest? It runs a lot quicker if there are a large number of topics and you run the tool from outside the ZK cluster's DC. Also, the topicOpt description has been misleading for a while.;;;","26/Mar/13 19:54;jkreps;This is already done on trunk, please backport that.;;;","29/Mar/13 22:47;junrao;Attach patch v2. 

1. Back ported the fix from trunk and made a slight change to print each partition in a single line. 
2. Fixed the description of the topic option.
3. Didn't make the change to use TopicMetadataRequest since it's not a blocker. Also, in trunk, list topic is merged into a single tool that requires ZK.;;;","01/Apr/13 16:01;nehanarkhede;Overall, good change. 2 suggestions -

1. Can we not make this change as part of overriding the toString() API of PartitionMetadata ?
2. For the ease of scripting, please can we 
2.1 reduce 2 tabs to 1 tab ? 
2.2 remove the space after the comma ?;;;","01/Apr/13 17:34;junrao;Thanks for the review. These are good suggestions. Attach patch v3 that addresses those issues.;;;","01/Apr/13 17:36;nehanarkhede;Looks great, thanks for the patch ! 
+1;;;","01/Apr/13 17:42;junrao;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Kafka 0.8 depend on metrics 2.2.0 instead of 3.x,KAFKA-826,12638970,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,dragosm,nehanarkhede,nehanarkhede,25/Mar/13 21:19,02/Feb/16 23:09,14/Jul/23 05:39,18/Apr/13 04:20,0.8.0,,,0.8.0,,,,,,,core,,,0,build,kafka-0.8,metrics,"In order to mavenize Kafka 0.8, we have to depend on metrics 2.2.0 since metrics 3.x is a huge change as well as not an officially supported release.",,brugidou,clehene,dragosm,githubbot,jjkoshy,junrao,nehanarkhede,nikore,otis,rektide,scott_carey,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-133,KAFKA-833,,,,,,,,,,,,,KAFKA-960,,,,,,,,,,,,,"08/Apr/13 18:05;dragosm;kafka-fix-for-826-complete.patch;https://issues.apache.org/jira/secure/attachment/12577577/kafka-fix-for-826-complete.patch","01/Apr/13 21:07;dragosm;kafka-fix-for-826-take2.patch;https://issues.apache.org/jira/secure/attachment/12576434/kafka-fix-for-826-take2.patch","29/Mar/13 20:36;dragosm;kafka-fix-for-826.patch;https://issues.apache.org/jira/secure/attachment/12576156/kafka-fix-for-826.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,319440,,,Tue Feb 02 23:09:20 UTC 2016,,,,,,,,,,"0|i1j48n:",319781,,,,,,,,,,,,,,,,,,,,"25/Mar/13 21:47;scott_carey;Thank you!  We will be able to test and validate this quickly once there is a patch.

Metrics 3.0.x has hit its first snapshot recently:
https://groups.google.com/forum/#!topic/metrics-user/c4sPUhLjHEQ

However, it looks like it won't be done in time for Kafka 0.8.  It now at least does not conflict  with a copy of 2.2.x as badly as it did a couple months ago.;;;","25/Mar/13 22:36;dragosm;Good to see this pop on your radar screen... I looked at what it would take to move from 3.0.0-c0c8be71 to SNAPSHOT and there were quite a few changes. I've just forked the project, if I make progress I'll send a pull request.;;;","28/Mar/13 18:14;nehanarkhede;We need to remove the dependency on the custom metrics jar in order to mavenize kafka;;;","28/Mar/13 21:39;dragosm;diff --git a/core/lib/metrics-annotation-3.0.0-c0c8be71.jar b/core/lib/metrics-annotation-3.0.0-c0c8be71.jar
deleted file mode 100644
index dba9d2b..0000000
Binary files a/core/lib/metrics-annotation-3.0.0-c0c8be71.jar and /dev/null differ
diff --git a/core/lib/metrics-core-3.0.0-c0c8be71.jar b/core/lib/metrics-core-3.0.0-c0c8be71.jar
deleted file mode 100644
index 529a69b..0000000
Binary files a/core/lib/metrics-core-3.0.0-c0c8be71.jar and /dev/null differ
diff --git a/core/src/main/scala/kafka/cluster/Partition.scala b/core/src/main/scala/kafka/cluster/Partition.scala
index 367ccd5..7788b30 100644
--- a/core/src/main/scala/kafka/cluster/Partition.scala
+++ b/core/src/main/scala/kafka/cluster/Partition.scala
@@ -60,7 +60,7 @@ class Partition(val topic: String,
   newGauge(
     topic + ""-"" + partitionId + ""-UnderReplicated"",
     new Gauge[Int] {
-      def getValue = {
+      def value = {
         if (isUnderReplicated) 1 else 0
       }
     }
diff --git a/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala b/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala
index 972d33d..51b9c35 100644
--- a/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala
+++ b/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala
@@ -653,7 +653,7 @@ private[kafka] class ZookeeperConsumerConnector(val config: ConsumerConfig,
       newGauge(
         config.clientId + ""-"" + config.groupId + ""-"" + topicThreadId._1 + ""-"" + topicThreadId._2 + ""-FetchQueueSize"",
         new Gauge[Int] {
-          def getValue = q.size
+          def value = q.size
         }
       )
     })
diff --git a/core/src/main/scala/kafka/controller/KafkaController.scala b/core/src/main/scala/kafka/controller/KafkaController.scala
index 48eae7e..9c4c8d1 100644
--- a/core/src/main/scala/kafka/controller/KafkaController.scala
+++ b/core/src/main/scala/kafka/controller/KafkaController.scala
@@ -95,7 +95,7 @@ class KafkaController(val config : KafkaConfig, zkClient: ZkClient) extends Logg
   newGauge(
     ""ActiveControllerCount"",
     new Gauge[Int] {
-      def getValue() = if (isActive) 1 else 0
+      def value() = if (isActive) 1 else 0
     }
   )
 
diff --git a/core/src/main/scala/kafka/log/Log.scala b/core/src/main/scala/kafka/log/Log.scala
index 631953f..b7b266e 100644
--- a/core/src/main/scala/kafka/log/Log.scala
+++ b/core/src/main/scala/kafka/log/Log.scala
@@ -77,10 +77,10 @@ class Log(val dir: File,
   debug(""Completed load of log %s with log end offset %d"".format(name, logEndOffset))
 
   newGauge(name + ""-"" + ""NumLogSegments"",
-           new Gauge[Int] { def getValue = numberOfSegments })
+           new Gauge[Int] { def value = numberOfSegments })
 
   newGauge(name + ""-"" + ""LogEndOffset"",
-           new Gauge[Long] { def getValue = logEndOffset })
+           new Gauge[Long] { def value = logEndOffset })
 
   /** The name of this log */
   def name  = dir.getName()
diff --git a/core/src/main/scala/kafka/network/RequestChannel.scala b/core/src/main/scala/kafka/network/RequestChannel.scala
index 209fdfa..c0e0dfc 100644
--- a/core/src/main/scala/kafka/network/RequestChannel.scala
+++ b/core/src/main/scala/kafka/network/RequestChannel.scala
@@ -99,7 +99,7 @@ class RequestChannel(val numProcessors: Int, val queueSize: Int) extends KafkaMe
   newGauge(
     ""RequestQueueSize"",
     new Gauge[Int] {
-      def getValue = requestQueue.size
+      def value = requestQueue.size
     }
   )
 
diff --git a/core/src/main/scala/kafka/producer/async/ProducerSendThread.scala b/core/src/main/scala/kafka/producer/async/ProducerSendThread.scala
index 6691147..090400d 100644
--- a/core/src/main/scala/kafka/producer/async/ProducerSendThread.scala
+++ b/core/src/main/scala/kafka/producer/async/ProducerSendThread.scala
@@ -36,7 +36,7 @@ class ProducerSendThread[K,V](val threadName: String,
 
   newGauge(clientId + ""-ProducerQueueSize"",
           new Gauge[Int] {
-            def getValue = queue.size
+            def value = queue.size
           })
 
   override def run {
diff --git a/core/src/main/scala/kafka/server/AbstractFetcherThread.scala b/core/src/main/scala/kafka/server/AbstractFetcherThread.scala
index a7d39b1..006d573 100644
--- a/core/src/main/scala/kafka/server/AbstractFetcherThread.scala
+++ b/core/src/main/scala/kafka/server/AbstractFetcherThread.scala
@@ -203,7 +203,7 @@ class FetcherLagMetrics(metricId: ClientIdBrokerTopicPartition) extends KafkaMet
   newGauge(
     metricId + ""-ConsumerLag"",
     new Gauge[Long] {
-      def getValue = lagVal.get
+      def value = lagVal.get
     }
   )
 
diff --git a/core/src/main/scala/kafka/server/ReplicaManager.scala b/core/src/main/scala/kafka/server/ReplicaManager.scala
index 765d3cb..e1d5bd8 100644
--- a/core/src/main/scala/kafka/server/ReplicaManager.scala
+++ b/core/src/main/scala/kafka/server/ReplicaManager.scala
@@ -56,19 +56,19 @@ class ReplicaManager(val config: KafkaConfig,
   newGauge(
     ""LeaderCount"",
     new Gauge[Int] {
-      def getValue = leaderPartitions.size
+      def value = leaderPartitions.size
     }
   )
   newGauge(
     ""PartitionCount"",
     new Gauge[Int] {
-      def getValue = allPartitions.size
+      def value = allPartitions.size
     }
   )
   newGauge(
     ""UnderReplicatedPartitions"",
     new Gauge[Int] {
-      def getValue = {
+      def value = {
         leaderPartitionsLock synchronized {
           leaderPartitions.count(_.isUnderReplicated)
         }
diff --git a/core/src/main/scala/kafka/server/RequestPurgatory.scala b/core/src/main/scala/kafka/server/RequestPurgatory.scala
index afe9e22..c064c5c 100644
--- a/core/src/main/scala/kafka/server/RequestPurgatory.scala
+++ b/core/src/main/scala/kafka/server/RequestPurgatory.scala
@@ -72,14 +72,14 @@ abstract class RequestPurgatory[T <: DelayedRequest, R](brokerId: Int = 0, purge
   newGauge(
     ""PurgatorySize"",
     new Gauge[Int] {
-      def getValue = watchersForKey.values.map(_.numRequests).sum + expiredRequestReaper.numRequests
+      def value = watchersForKey.values.map(_.numRequests).sum + expiredRequestReaper.numRequests
     }
   )
 
   newGauge(
     ""NumDelayedRequests"",
     new Gauge[Int] {
-      def getValue = expiredRequestReaper.unsatisfied.get()
+      def value = expiredRequestReaper.unsatisfied.get()
     }
   )
 
diff --git a/core/src/test/scala/unit/kafka/metrics/KafkaTimerTest.scala b/core/src/test/scala/unit/kafka/metrics/KafkaTimerTest.scala
index a3f85cf..fe5bc09 100644
--- a/core/src/test/scala/unit/kafka/metrics/KafkaTimerTest.scala
+++ b/core/src/test/scala/unit/kafka/metrics/KafkaTimerTest.scala
@@ -35,20 +35,20 @@ class KafkaTimerTest extends JUnit3Suite {
     timer.time {
       clock.addMillis(1000)
     }
-    assertEquals(1, metric.getCount())
-    assertTrue((metric.getMax() - 1000).abs <= Double.Epsilon)
-    assertTrue((metric.getMin() - 1000).abs <= Double.Epsilon)
+    assertEquals(1, metric.count())
+    assertTrue((metric.max() - 1000).abs <= Double.Epsilon)
+    assertTrue((metric.min() - 1000).abs <= Double.Epsilon)
   }
 
   private class ManualClock extends Clock {
 
     private var ticksInNanos = 0L
 
-    override def getTick() = {
+    override def tick() = {
       ticksInNanos
     }
 
-    override def getTime() = {
+    override def time() = {
       TimeUnit.NANOSECONDS.toMillis(ticksInNanos)
     }
 
diff --git a/project/Build.scala b/project/Build.scala
index facca79..bc3bc0c 100644
--- a/project/Build.scala
+++ b/project/Build.scala
@@ -17,7 +17,6 @@
 
 import sbt._
 import Keys._
-import java.io.File
 
 import scala.xml.{Node, Elem}
 import scala.xml.transform.{RewriteRule, RuleTransformer}
@@ -35,7 +34,9 @@ object KafkaBuild extends Build {
       ""log4j""                 % ""log4j""        % ""1.2.15"",
       ""net.sf.jopt-simple""    % ""jopt-simple""  % ""3.2"",
       ""org.slf4j""             % ""slf4j-simple"" % ""1.6.4"",
-      ""com.101tec""            % ""zkclient""     % ""0.2""
+      ""com.101tec""            % ""zkclient""     % ""0.2"",
+      ""com.yammer.metrics"" % ""metrics-core"" % ""2.2.0"",
+      ""com.yammer.metrics"" % ""metrics-annotation"" % ""2.2.0""
     ),
     // The issue is going from log4j 1.2.14 to 1.2.15, the developers added some features which required
     // some dependencies on various sun and javax packages.
diff --git a/project/build/KafkaProject.scala b/project/build/KafkaProject.scala
index 1660fb8..853a45c 100644
--- a/project/build/KafkaProject.scala
+++ b/project/build/KafkaProject.scala
@@ -74,7 +74,7 @@ class KafkaProject(info: ProjectInfo) extends ParentProject(info) with IdeaProje
       <dependency>
         <groupId>com.yammer.metrics</groupId>
         <artifactId>metrics-core</artifactId>
-        <version>3.0.0-c0c8be71</version>
+        <version>2.2.0</version>
         <scope>compile</scope>
       </dependency>
 
@@ -82,7 +82,7 @@ class KafkaProject(info: ProjectInfo) extends ParentProject(info) with IdeaProje
       <dependency>
         <groupId>com.yammer.metrics</groupId>
         <artifactId>metrics-annotation</artifactId>
-        <version>3.0.0-c0c8be71</version>
+        <version>2.2.0</version>
         <scope>compile</scope>
       </dependency>
 
;;;","28/Mar/13 23:12;jjkoshy;Thank you for looking into this. Metrics 2.x had a few minor issues with the CsvReporter (which we use in the system tests) and this is why we
used 3.x.

The fixes that I'm aware of are:
- https://github.com/codahale/metrics/pull/225
- https://github.com/codahale/metrics/pull/290
- If a CSV file already exists, metrics throws an IOException and does not resume CSV reporting. This would be the case on a broker bounce for example. Someone put out a patch for this (https://github.com/adagios/metrics/compare/2.x-maintenance...2.x-epoch-in-csv) but I'd have to check if that was pulled into metrics-3.x

Unfortunately, although the above are small fixes, if we want to use the official 2.x metrics release we would need to copy over
the code of the metrics CsvReporter (i.e., into a new implementation of metrics' AbstractReporter), patch in those fixes and plug
that into KafkaMetricsCsvReporter. I don't think it is difficult, but a bit clunky (which is why at the time we preferred using 3.x).
;;;","29/Mar/13 17:26;scott_carey;The only real trouble with 3.0.x is if:
* it conflicts with 2.2.x if in the same classloader
* there is no released artifact in a public maven repo to provide repeatable builds.

We might be able to find out when a 3.0-alpha may be available and pushed to a public maven repo.  It does  not appear that a final version is due out in time.
;;;","29/Mar/13 19:42;swapnilghike;[~dragosm]: Could you upload a patch using the attach files options above? I tried applying your patch using ""patch -p1"", but it could not be applied. Thanks.;;;","29/Mar/13 20:36;dragosm;Alternatively this code is checked into my fork of the project, https://github.com/polymorphic/kafka, the metrics2 branch;;;","29/Mar/13 22:27;swapnilghike;Hi Dragos, I am still not able to cleanly apply the patch. Is it created off 0.8 HEAD? Could you rebase in case it helps? Thanks.

sghike@machine:~/kafka-local/kafka$ patch -p1 --dry-run < ../kafka-fix-for-826.patch 
patching file core/src/main/scala/kafka/cluster/Partition.scala
patching file core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala
Hunk #1 succeeded at 650 (offset -3 lines).
patching file core/src/main/scala/kafka/controller/KafkaController.scala
Hunk #1 succeeded at 97 (offset 2 lines).
patching file core/src/main/scala/kafka/log/Log.scala
Hunk #1 succeeded at 130 with fuzz 2 (offset 53 lines).
patching file core/src/main/scala/kafka/network/RequestChannel.scala
patching file core/src/main/scala/kafka/producer/async/ProducerSendThread.scala
patching file core/src/main/scala/kafka/server/AbstractFetcherThread.scala
Hunk #1 succeeded at 195 (offset -8 lines).
patching file core/src/main/scala/kafka/server/ReplicaManager.scala
Hunk #1 FAILED at 56.
1 out of 1 hunk FAILED -- saving rejects to file core/src/main/scala/kafka/server/ReplicaManager.scala.rej
patching file core/src/main/scala/kafka/server/RequestPurgatory.scala
patching file core/src/test/scala/unit/kafka/metrics/KafkaTimerTest.scala
patching file project/Build.scala
Hunk #2 FAILED at 34.
1 out of 2 hunks FAILED -- saving rejects to file project/Build.scala.rej
patching file project/build/KafkaProject.scala
Hunk #1 FAILED at 74.
Hunk #2 FAILED at 82.
2 out of 2 hunks FAILED -- saving rejects to file project/build/KafkaProject.scala.rej;;;","01/Apr/13 20:32;dragosm;Let's try this again, I have another patch ready.;;;","01/Apr/13 20:37;dragosm;diff --git a/core/src/main/scala/kafka/cluster/Partition.scala b/core/src/main/scala/kafka/cluster/Partition.scala
index 2ca7ee6..e49bdae 100644
--- a/core/src/main/scala/kafka/cluster/Partition.scala
+++ b/core/src/main/scala/kafka/cluster/Partition.scala
@@ -60,7 +60,7 @@ class Partition(val topic: String,
   newGauge(
     topic + ""-"" + partitionId + ""-UnderReplicated"",
     new Gauge[Int] {
-      def getValue = {
+      def value = {
         if (isUnderReplicated) 1 else 0
       }
     }
diff --git a/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala b/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala
index 9a5fbfe..398618f 100644
--- a/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala
+++ b/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala
@@ -650,7 +650,7 @@ private[kafka] class ZookeeperConsumerConnector(val config: ConsumerConfig,
       newGauge(
         config.clientId + ""-"" + config.groupId + ""-"" + topicThreadId._1 + ""-"" + topicThreadId._2 + ""-FetchQueueSize"",
         new Gauge[Int] {
-          def getValue = q.size
+          def value = q.size
         }
       )
     })
diff --git a/core/src/main/scala/kafka/controller/KafkaController.scala b/core/src/main/scala/kafka/controller/KafkaController.scala
index 74614d8..5f6eb3c 100644
--- a/core/src/main/scala/kafka/controller/KafkaController.scala
+++ b/core/src/main/scala/kafka/controller/KafkaController.scala
@@ -97,14 +97,14 @@ class KafkaController(val config : KafkaConfig, zkClient: ZkClient) extends Logg
   newGauge(
     ""ActiveControllerCount"",
     new Gauge[Int] {
-      def getValue() = if (isActive) 1 else 0
+      def value() = if (isActive) 1 else 0
     }
   )
 
   newGauge(
     ""OfflinePartitionsCount"",
     new Gauge[Int] {
-      def getValue: Int = {
+      def value(): Int = {
         controllerContext.controllerLock synchronized {
           controllerContext.partitionLeadershipInfo.count(p => !controllerContext.liveBrokerIds.contains(p._2.leaderAndIsr.leader))
         }
diff --git a/core/src/main/scala/kafka/log/Log.scala b/core/src/main/scala/kafka/log/Log.scala
index 7d71451..451775b 100644
--- a/core/src/main/scala/kafka/log/Log.scala
+++ b/core/src/main/scala/kafka/log/Log.scala
@@ -130,10 +130,10 @@ private[kafka] class Log(val dir: File,
   debug(""Completed load of log %s with log end offset %d"".format(name, logEndOffset))
 
   newGauge(name + ""-"" + ""NumLogSegments"",
-           new Gauge[Int] { def getValue = numberOfSegments })
+           new Gauge[Int] { def value = numberOfSegments })
 
   newGauge(name + ""-"" + ""LogEndOffset"",
-           new Gauge[Long] { def getValue = logEndOffset })
+           new Gauge[Long] { def value = logEndOffset })
 
   /* The name of this log */
   def name  = dir.getName()
diff --git a/core/src/main/scala/kafka/network/RequestChannel.scala b/core/src/main/scala/kafka/network/RequestChannel.scala
index 209fdfa..c0e0dfc 100644
--- a/core/src/main/scala/kafka/network/RequestChannel.scala
+++ b/core/src/main/scala/kafka/network/RequestChannel.scala
@@ -99,7 +99,7 @@ class RequestChannel(val numProcessors: Int, val queueSize: Int) extends KafkaMe
   newGauge(
     ""RequestQueueSize"",
     new Gauge[Int] {
-      def getValue = requestQueue.size
+      def value = requestQueue.size
     }
   )
 
diff --git a/core/src/main/scala/kafka/producer/async/ProducerSendThread.scala b/core/src/main/scala/kafka/producer/async/ProducerSendThread.scala
index 6691147..090400d 100644
--- a/core/src/main/scala/kafka/producer/async/ProducerSendThread.scala
+++ b/core/src/main/scala/kafka/producer/async/ProducerSendThread.scala
@@ -36,7 +36,7 @@ class ProducerSendThread[K,V](val threadName: String,
 
   newGauge(clientId + ""-ProducerQueueSize"",
           new Gauge[Int] {
-            def getValue = queue.size
+            def value = queue.size
           })
 
   override def run {
diff --git a/core/src/main/scala/kafka/server/AbstractFetcherThread.scala b/core/src/main/scala/kafka/server/AbstractFetcherThread.scala
index 087979f..2e026e6 100644
--- a/core/src/main/scala/kafka/server/AbstractFetcherThread.scala
+++ b/core/src/main/scala/kafka/server/AbstractFetcherThread.scala
@@ -195,7 +195,7 @@ class FetcherLagMetrics(metricId: ClientIdBrokerTopicPartition) extends KafkaMet
   newGauge(
     metricId + ""-ConsumerLag"",
     new Gauge[Long] {
-      def getValue = lagVal.get
+      def value = lagVal.get
     }
   )
 
diff --git a/core/src/main/scala/kafka/server/ReplicaManager.scala b/core/src/main/scala/kafka/server/ReplicaManager.scala
index 68e712c..44ad562 100644
--- a/core/src/main/scala/kafka/server/ReplicaManager.scala
+++ b/core/src/main/scala/kafka/server/ReplicaManager.scala
@@ -57,7 +57,7 @@ class ReplicaManager(val config: KafkaConfig,
   newGauge(
     ""LeaderCount"",
     new Gauge[Int] {
-      def getValue = {
+      def value = {
         leaderPartitionsLock synchronized {
           leaderPartitions.size
         }
@@ -67,13 +67,13 @@ class ReplicaManager(val config: KafkaConfig,
   newGauge(
     ""PartitionCount"",
     new Gauge[Int] {
-      def getValue = allPartitions.size
+      def value = allPartitions.size
     }
   )
   newGauge(
     ""UnderReplicatedPartitions"",
     new Gauge[Int] {
-      def getValue = {
+      def value = {
         leaderPartitionsLock synchronized {
           leaderPartitions.count(_.isUnderReplicated)
         }
diff --git a/core/src/main/scala/kafka/server/RequestPurgatory.scala b/core/src/main/scala/kafka/server/RequestPurgatory.scala
index afe9e22..c064c5c 100644
--- a/core/src/main/scala/kafka/server/RequestPurgatory.scala
+++ b/core/src/main/scala/kafka/server/RequestPurgatory.scala
@@ -72,14 +72,14 @@ abstract class RequestPurgatory[T <: DelayedRequest, R](brokerId: Int = 0, purge
   newGauge(
     ""PurgatorySize"",
     new Gauge[Int] {
-      def getValue = watchersForKey.values.map(_.numRequests).sum + expiredRequestReaper.numRequests
+      def value = watchersForKey.values.map(_.numRequests).sum + expiredRequestReaper.numRequests
     }
   )
 
   newGauge(
     ""NumDelayedRequests"",
     new Gauge[Int] {
-      def getValue = expiredRequestReaper.unsatisfied.get()
+      def value = expiredRequestReaper.unsatisfied.get()
     }
   )
 
diff --git a/core/src/test/scala/unit/kafka/metrics/KafkaTimerTest.scala b/core/src/test/scala/unit/kafka/metrics/KafkaTimerTest.scala
index a3f85cf..fe5bc09 100644
--- a/core/src/test/scala/unit/kafka/metrics/KafkaTimerTest.scala
+++ b/core/src/test/scala/unit/kafka/metrics/KafkaTimerTest.scala
@@ -35,20 +35,20 @@ class KafkaTimerTest extends JUnit3Suite {
     timer.time {
       clock.addMillis(1000)
     }
-    assertEquals(1, metric.getCount())
-    assertTrue((metric.getMax() - 1000).abs <= Double.Epsilon)
-    assertTrue((metric.getMin() - 1000).abs <= Double.Epsilon)
+    assertEquals(1, metric.count())
+    assertTrue((metric.max() - 1000).abs <= Double.Epsilon)
+    assertTrue((metric.min() - 1000).abs <= Double.Epsilon)
   }
 
   private class ManualClock extends Clock {
 
     private var ticksInNanos = 0L
 
-    override def getTick() = {
+    override def tick() = {
       ticksInNanos
     }
 
-    override def getTime() = {
+    override def time() = {
       TimeUnit.NANOSECONDS.toMillis(ticksInNanos)
     }
 
diff --git a/project/Build.scala b/project/Build.scala
index 4bbdfee..b8b476b 100644
--- a/project/Build.scala
+++ b/project/Build.scala
@@ -17,7 +17,6 @@
 
 import sbt._
 import Keys._
-import java.io.File
 
 import scala.xml.{Node, Elem}
 import scala.xml.transform.{RewriteRule, RuleTransformer}
@@ -34,7 +33,10 @@ object KafkaBuild extends Build {
     libraryDependencies ++= Seq(
       ""log4j""                 % ""log4j""        % ""1.2.15"",
       ""net.sf.jopt-simple""    % ""jopt-simple""  % ""3.2"",
-      ""org.slf4j""             % ""slf4j-simple"" % ""1.6.4""
+      ""org.slf4j""             % ""slf4j-simple"" % ""1.6.4"",
+      ""com.101tec""            % ""zkclient""     % ""0.2"",
+      ""com.yammer.metrics""    % ""metrics-core"" % ""2.2.0"",
+      ""com.yammer.metrics""    % ""metrics-annotation"" % ""2.2.0""
     ),
     // The issue is going from log4j 1.2.14 to 1.2.15, the developers added some features which required
     // some dependencies on various sun and javax packages.
diff --git a/project/build/KafkaProject.scala b/project/build/KafkaProject.scala
index fac723a..853a45c 100644
--- a/project/build/KafkaProject.scala
+++ b/project/build/KafkaProject.scala
@@ -74,7 +74,7 @@ class KafkaProject(info: ProjectInfo) extends ParentProject(info) with IdeaProje
       <dependency>
         <groupId>com.yammer.metrics</groupId>
         <artifactId>metrics-core</artifactId>
-        <version>3.0.0-SNAPSHOT</version>
+        <version>2.2.0</version>
         <scope>compile</scope>
       </dependency>
 
@@ -82,7 +82,7 @@ class KafkaProject(info: ProjectInfo) extends ParentProject(info) with IdeaProje
       <dependency>
         <groupId>com.yammer.metrics</groupId>
         <artifactId>metrics-annotation</artifactId>
-        <version>3.0.0-SNAPSHOT</version>
+        <version>2.2.0</version>
         <scope>compile</scope>
       </dependency>
 
;;;","01/Apr/13 20:39;dragosm;I'm sorry you've had problems with this patch. You should be good to go:

seac02jh0rjdkq4 ~/Repos/kafka% git status                                                                                                                                  [92]
# On branch 0.8
# Untracked files:
#   (use ""git add <file>..."" to include in what will be committed)
#
#	.idea_modules/
nothing added to commit but untracked files present (use ""git add"" to track)
mseac02jh0rjdkq4 ~/Repos/kafka% patch -p1 --dry-run < ../kafka-fix-for-826-take2.patch                                                                                      [93]
patching file core/src/main/scala/kafka/cluster/Partition.scala
patching file core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala
patching file core/src/main/scala/kafka/controller/KafkaController.scala
patching file core/src/main/scala/kafka/log/Log.scala
patching file core/src/main/scala/kafka/network/RequestChannel.scala
patching file core/src/main/scala/kafka/producer/async/ProducerSendThread.scala
patching file core/src/main/scala/kafka/server/AbstractFetcherThread.scala
patching file core/src/main/scala/kafka/server/ReplicaManager.scala
patching file core/src/main/scala/kafka/server/RequestPurgatory.scala
patching file core/src/test/scala/unit/kafka/metrics/KafkaTimerTest.scala
patching file project/Build.scala
patching file project/build/KafkaProject.scala;;;","01/Apr/13 20:57;swapnilghike;Hi Dragos, sorry for bugging you again. I think if I copy paste your new patch, I am not able to apply it again. Do you mind attaching it as a file? Thanks for the help Dragos!;;;","01/Apr/13 21:07;dragosm;Here it is, attached. Please LMK if you still have problems.;;;","01/Apr/13 21:15;swapnilghike;The take2 patch applies quite smoothly! Thanks a ton, will check today how it works out.;;;","02/Apr/13 03:21;otis;Ah, what timing!  Metrics 3.0.0 BETA1 has just been pushed to Maven Central with goodies:
Coda Hale <coda.hale@gmail.com> Apr 01 02:02PM -0700  

Metrics 3.0.0-BETA1 is on its way to Maven Central! It includes the following changes:
 
• Total overhaul of most of the core Metrics classes:
• Metric names are now just dotted paths like com.example.Thing, allowing for very flexible scopes, etc.
• Meters and timers no longer have rate or duration units; those are properties of reporters.
• Reporter architecture has been radically simplified, fixing many bugs.
• Histograms and timers can take arbitrary reservoir implementations.
• Added sliding window reservoir implementations.
• Added MetricSet for sets of metrics.
• Changed package names to be OSGi-compatible and added OSGi bundling.
• Extracted JVM instrumentation to metrics-jvm.
• Extracted Jackson integration to metrics-json.
• Removed metrics-guice, metrics-scala, and metrics-spring.
• Renamed metrics-servlet to metrics-servlets.
• Renamed metrics-web to metrics-servlet.
• Renamed metrics-jetty to metrics-jetty8.
• Many more small changes!

Wouldn't it make sense to invest the effort into moving to 3.0.0 instead now?  Otherwise, if the gap between 0.8 and post-0.8 release is as big as the gap between 0.7.2 and 0.8 it will be a while before moving to this new metrics package.
;;;","02/Apr/13 22:35;junrao;Otis,

The reason that we want to downgrade metrics to 2.x is because quite a few people feel this is the most stable version that they can depend on. Do you know how stable is this beta release? Will there be any API or bean name change btw the final 3.x release and this beta release?

In general, do people feel comfortable about using the metrics 3.0 beta release?;;;","03/Apr/13 00:06;swapnilghike;Dragos, thanks for the input. We can definitely use your patch if we decide to downgrade metrics to 2.2.0. Could you please address the following comments and re-submit the patch?

1. Could you move the metrics-core, metrics-annotations and zkclient library dependences to core/build.sbt? You will need to replace the old organization and version number of zkclient there with the one you added to KafkaBuild.
2. Could you append the corresponding paths to class path in bin/kafka-run-class.sh? Also please remove core/lib/*.jar from class path since we are moving the three jars there to ivy.
3. git rm core/lib for the same reason mentioned in 2.

Apart from your patch, we will need to fix the csv reporter separately.;;;","03/Apr/13 03:34;otis;[~junrao] - I'm on the ML for that metrics lib and a few weeks (1-2 months?) ago Coda wrote an email saying he was not happy with a number of things in 2.x that he completely reworked in 3.x.  I don't recall the details, but I recall feeling like 3.x is the version I'd use if I had to pick between 2.x and 3.x.  Unless Kafka 0.8 is going to be released within the next week or two, I'd personally go with 3.x, unless you are open moving from 2.x to 3.x in the near future in 0.8.1.

;;;","03/Apr/13 22:19;scott_carey;My main complaint prior to this was two-fold:

* Early versions of metrics 3.x had classpath collisions with 2.2.x , so we would be unable to have both in the same application and we already use 2.2.
* There was no official version of metrics 3.x published anywhere to consume, no roadmap, and the developer was MIA for months.  Kafka would have had to publish their own artifact version which gets messy fast and would have likely api compatibility issues with any future final metrics 3.0.x release and therefore be very difficult to use kafka 0.8 and the final metrics 3.0 in the same application.

If 3.x is stable enough API wise, I'd be fine with keeping kafka on it.   I believe 3.x no longer collides with 2.2.x in a classpath, but have not tested that recently, meaning we could migrate to 3.x at our own pace and not have to time it to be in sync with use of Kafka 0.8.;;;","03/Apr/13 22:29;scott_carey;Given the volume of work that has been done on metrics 3 between the snapshot on March 12 and the recent BETA1, I am comfortable with it -- a full release is likely to be complete before Kafka 0.8 is through its beta.;;;","04/Apr/13 11:47;brugidou;Yes there has been some work and it breaks when I try to use 3.0.0-BETA1 from 0.8 branch. We need to have a MetricsRegistry (static or injected) and use the new register() method to add gauges/metrics. Also the core namespace has been removed

Anyone working on a patch to get the BETA1 ? (and remove the core/lib/metrics*.jar files? )

By the way I don't think we use metrics-annotation, just metrics-core right?;;;","04/Apr/13 16:07;junrao;I pinged Coda and the following are the answers that I got.

1. How stable is 3.0.0-Beta1?
The code is actually less complicated than Metrics 2 and far better tested: no thread pools, no lifecycles, no static references, just simple objects.

2. Is it true that 3.0.0-Beta1 has no classpath conflict with 2.x? In other words, can an application use both jars in the same JVM?
Many of the classes have changed names and/or packages, but I didn't intend for there to be no classpath conflicts. You might be able to get away with it, but someone somewhere will step on something sharp, I'm sure.

3. Will there be any api change btw 3.0.0-Beta1 and the final 3.0 release?
Probably, yes.

4. Were there any api changes btw 3.0.0-Beta1 and 3.0.0-SNAPSHOT (the one that we are currently using in Kafka 0.8)?
I don't know which snapshot build you're using, but I would imagine there are a lot of changes.

Based on the above, my feeling is that Kafka 0.8 release should use metrics 2.x since (1) it has been stable; (2) people who depend on 2.x have no risk in using Kafka 0.8; (3) the api of 3.x is likely going to change again. In a post 0.8 release, we can upgrade to metrics 3.x when the final version is released and is deemed stable. Any concern with this approach?;;;","04/Apr/13 16:16;dragosm;Thank you Jun for the analysis! From my perspective the approach you suggested is sound. FWIW a couple of weeks ago I ""ported"" the Kafka code to the latest (at the time) metrics 3.0.0 APIs. The code compiled and tests passed, but I don't know the extent that demonstrates that the instrumentation and reporting worked as it should. I'm bringing it up because I feel that once 3.0.0 is released the port is mechanical.;;;","04/Apr/13 16:59;nehanarkhede;I agree with Jun that we should release Kafka 0.8 with metrics 2.x. Let's wait until metrics 3.x APIs are stable.;;;","04/Apr/13 18:47;scott_carey;Sounds good to me. ;;;","04/Apr/13 20:22;nikore;I also like this, allows this to be integrated in with our other metrics we are already collecting under 2.2 and report it all to ganglia. ;;;","04/Apr/13 21:39;swapnilghike;Dragos, it seems like we are headed towards agreeing on using 2.2.0. Do you mind including the review suggestions in your patch? If you are busy, I can finish it up too. Thanks.;;;","05/Apr/13 22:51;dragosm;I'm just starting to work on it, will try to finish it over the weekend.;;;","06/Apr/13 23:28;dragosm;Here's the patch for metrics 2.2.0 with (i) code updates for the metrics API, (ii) build updates to reference zkclient 0.2 and metrics 2.2.0, and (iii) script updates with the appropriate classpaths. I have verified it and all tests pass:


[info] Passed: : Total 175, Failed 0, Errors 0, Passed 175, Skipped 0
[success] Total time: 166 s, completed Apr 6, 2013 4:19:27 PM

Please LMK if you run into problems.;;;","08/Apr/13 07:58;swapnilghike;Thanks, the unit tests were ok. But while trying to boot up a kafka server, I got the following error.

This is what I did, 
rm -rf ~/.ivy2
patch -p1 < kafka-fix-for-826-complete.patch
git rm -r core/lib
./sbt clean
./sbt update
./sbt package
bin/zookeeper-server-start.sh config/zookeeper.properties
bin/kafka-server-start.sh config/server.properties

[2013-04-08 00:48:29,743] FATAL  (kafka.Kafka$)
java.lang.NoClassDefFoundError: com/yammer/metrics/reporting/CsvReporter
	at kafka.metrics.KafkaCSVMetricsReporter.init(KafkaCSVMetricsReporter.scala:53)
	at kafka.metrics.KafkaMetricsReporter$$anonfun$startReporters$1.apply(KafkaMetricsReporter.scala:60)
	at kafka.metrics.KafkaMetricsReporter$$anonfun$startReporters$1.apply(KafkaMetricsReporter.scala:58)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32)
	at kafka.metrics.KafkaMetricsReporter$.startReporters(KafkaMetricsReporter.scala:58)
	at kafka.Kafka$.main(Kafka.scala:36)
	at kafka.Kafka.main(Kafka.scala)
Caused by: java.lang.ClassNotFoundException: com.yammer.metrics.reporting.CsvReporter
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	... 8 more

I am not currently sure what's causing this issue.;;;","08/Apr/13 18:05;dragosm;Hmm, it looks like for some reason the metrics JARs didn't end up on your CLASSPATH.

Here's the same patch with one additional update that packages the required libraries into a single jar, kafka-assembly-0.8-SNAPSHOT-deps.jar. This change removes ALL references to ~/.ivy2/cache from the script and requires one additional step: execute the ""assembly-package-dependency"" sbt task (provided by the sbt-assembly plugin that was already included).

Here are the steps:

   rm -rf ~/.ivy2/cache/
   find . -name target | xargs rm -rf
   bash sbt update
   bash sbt package
   bash sbt assembly-package-dependency # NEW STEP
   bin/zookeeper-server-start.sh config/zookeeper.properties 
   bin/kafka-server-start.sh config/server.properties

I verified and the server starts up fine. Please LMK if you run into problems.

Here's the output from assembly-package-dependency: 

% bash sbt assembly-package-dependency                                                                                                       
[info] Loading global plugins from /Users/dragos.manolescu/.sbt/plugins
[info] Loading project definition from /Users/dragos.manolescu/Repos/kafka/project
[info] Set current project to Kafka (in build file:/Users/dragos.manolescu/Repos/kafka/)
[info] Including metrics-core-2.2.0.jar
[info] Including scala-compiler.jar
[info] Including zkclient-0.2.jar
[info] Including metrics-annotation-2.2.0.jar
[info] Including snappy-java-1.0.4.1.jar
[info] Including log4j-1.2.15.jar
[info] Including slf4j-api-1.7.2.jar
[info] Including zookeeper-3.3.4.jar
[info] Including jopt-simple-3.2.jar
[info] Including slf4j-simple-1.6.4.jar
[info] Including scala-library.jar
[warn] Merging 'META-INF/NOTICE' with strategy 'rename'
[warn] Merging 'org/xerial/snappy/native/README' with strategy 'rename'
[warn] Merging 'META-INF/maven/org.xerial.snappy/snappy-java/LICENSE' with strategy 'rename'
[warn] Merging 'LICENSE.txt' with strategy 'rename'
[warn] Merging 'META-INF/LICENSE' with strategy 'rename'
[warn] Merging 'META-INF/MANIFEST.MF' with strategy 'discard'
[warn] Strategy 'discard' was applied to a file
[warn] Strategy 'rename' was applied to 5 files
[info] Packaging /Users/dragos.manolescu/Repos/kafka/core/target/scala-2.8.0/kafka-assembly-0.8-SNAPSHOT-deps.jar ...
[info] Done packaging.
[success] Total time: 44 s, completed Apr 8, 2013 10:58:33 AM;;;","09/Apr/13 02:25;scott_carey;Can a committer look at this?  This would be great to get in the 0.8 branch soon and looks good to me. 
I have a cleanup of sbt to significantly simplify it further that I'd like to do as part of KAFKA-854 but it will fail to merge with this change.;;;","09/Apr/13 06:42;nikore;I made my own branch and applyed this patch, in all my testing it seems like its doing great. I would also love to see it committed. ;;;","09/Apr/13 08:16;swapnilghike;Thanks Matt for verifying! This patch works well. Perhaps a committer who is familiar with sbt should take a look. ;;;","09/Apr/13 22:55;nehanarkhede;Thanks a lot for the patches! +1 on the complete patch. In addition to that, I will check in a change that deletes core/lib;;;","09/Apr/13 23:37;dragosm;You're welcome; thank you all for verifying independently.;;;","10/Apr/13 00:13;junrao;Dragos,

Thanks for the patch. It seems that you merged all dependant jars into a fat jar kafka-assembly-0.8-SNAPSHOT-deps.jar. Is it possible to keep individual jars in target/scala-2.8.0? This way, if people want to release the binary, it's clear for them what are the dependant jars and their version.;;;","10/Apr/13 22:13;dragosm;Jun, I understand what you're after. The sbt-assembly task lists the JARs it packages together when executed:

> assembly-package-dependency
[info] Including metrics-core-2.2.0.jar
[info] Including metrics-annotation-2.2.0.jar
[info] Including snappy-java-1.0.4.1.jar
[info] Including slf4j-api-1.7.2.jar
[info] Including log4j-1.2.15.jar
[info] Including zkclient-0.2.jar
[info] Including scala-compiler.jar
[info] Including zookeeper-3.3.4.jar
[info] Including slf4j-simple-1.6.4.jar
[info] Including scala-library.jar
[info] Including jopt-simple-3.2.jar

Following the execution of the update task these JARs will be available from the ivy cache (rather than the target folder, which if my understanding is correct holds only artifacts generated while compiling and building).

I am not aware of a mechanism to have sbt-assembly place the dependencies in the target folder. As plugins like sbt-assembly, sbt-proguard, sbt-onejar and others indicate fat JARs are the typical packaging mechanism for releasing JVM bytecode.;;;","03/Jul/13 13:13;clehene;Now that metrics 3.0 has been released (http://metrics.codahale.com/about/release-notes/) perhaps we should consider upgrading it back?;;;","03/Jul/13 16:55;junrao;Could you file a new jira to track this?;;;","03/Jul/13 17:13;clehene;https://issues.apache.org/jira/browse/KAFKA-960;;;","02/Feb/16 23:09;githubbot;Github user stumped2 closed the pull request at:

    https://github.com/apache/kafka/pull/3
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaController.isActive() needs to be synchronized,KAFKA-825,12638921,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,junrao,junrao,25/Mar/13 17:25,25/Mar/13 18:03,14/Jul/23 05:39,25/Mar/13 17:47,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,KafkaController.isActive() is not synchronized right now. This means that it could read an outdated controllerContext.controllerChannelManager.,,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/13 17:26;junrao;kafka-825.patch;https://issues.apache.org/jira/secure/attachment/12575350/kafka-825.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,319391,,,Mon Mar 25 17:47:55 UTC 2013,,,,,,,,,,"0|i1j3xr:",319732,,,,,,,,,,,,,,,,,,,,"25/Mar/13 17:26;junrao;Attach a patch.;;;","25/Mar/13 17:31;nehanarkhede;Thanks for the patch, +1;;;","25/Mar/13 17:47;junrao;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.NullPointerException in commitOffsets ,KAFKA-824,12638781,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,parth.brahmbhatt,yozhao,yozhao,25/Mar/13 05:59,01/Dec/15 16:42,14/Jul/23 05:39,02/Sep/15 20:48,0.7.2,0.8.2.0,,0.9.0.0,,,,,,,consumer,,,6,newbie,,,"Neha Narkhede

""Yes, I have. Unfortunately, I never quite around to fixing it. My guess is
that it is caused due to a race condition between the rebalance thread and
the offset commit thread when a rebalance is triggered or the client is
being shutdown. Do you mind filing a bug ?""


2013/03/25 12:08:32.020 WARN [ZookeeperConsumerConnector] [] 0_lu-ml-test10.bj-1364184411339-7c88f710 exception during commitOffsets
java.lang.NullPointerException
        at org.I0Itec.zkclient.ZkConnection.writeData(ZkConnection.java:111)
        at org.I0Itec.zkclient.ZkClient$10.call(ZkClient.java:813)
        at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)
        at org.I0Itec.zkclient.ZkClient.writeData(ZkClient.java:809)
        at org.I0Itec.zkclient.ZkClient.writeData(ZkClient.java:777)
        at kafka.utils.ZkUtils$.updatePersistentPath(ZkUtils.scala:103)
        at kafka.consumer.ZookeeperConsumerConnector$$anonfun$commitOffsets$2$$anonfun$apply$4.apply(ZookeeperConsumerConnector.scala:251)
        at kafka.consumer.ZookeeperConsumerConnector$$anonfun$commitOffsets$2$$anonfun$apply$4.apply(ZookeeperConsumerConnector.scala:248)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:549)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
        at scala.collection.JavaConversions$JCollectionWrapper.foreach(JavaConversions.scala:570)
        at kafka.consumer.ZookeeperConsumerConnector$$anonfun$commitOffsets$2.apply(ZookeeperConsumerConnector.scala:248)
        at kafka.consumer.ZookeeperConsumerConnector$$anonfun$commitOffsets$2.apply(ZookeeperConsumerConnector.scala:246)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at kafka.utils.Pool$$anon$1.foreach(Pool.scala:53)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
        at kafka.utils.Pool.foreach(Pool.scala:24)
        at kafka.consumer.ZookeeperConsumerConnector.commitOffsets(ZookeeperConsumerConnector.scala:246)
        at kafka.consumer.ZookeeperConsumerConnector.autoCommit(ZookeeperConsumerConnector.scala:232)
        at kafka.consumer.ZookeeperConsumerConnector$$anonfun$1.apply$mcV$sp(ZookeeperConsumerConnector.scala:126)
        at kafka.utils.Utils$$anon$2.run(Utils.scala:58)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)


",,adenysenko,danehammer,guozhang,junrao,lishuming,noslowerdna,oae,parth.brahmbhatt,rmetzger,techwhizbang,w00te,yozhao,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-2169,,,,,,,,,,,,,,,,,,,FLINK-3067,,,,,,,,,,"24/Apr/14 03:43;adenysenko;ZkClient.0.3.txt;https://issues.apache.org/jira/secure/attachment/12641643/ZkClient.0.3.txt","24/Apr/14 03:43;adenysenko;ZkClient.0.4.txt;https://issues.apache.org/jira/secure/attachment/12641644/ZkClient.0.4.txt","05/May/14 05:01;adenysenko;screenshot-1.jpg;https://issues.apache.org/jira/secure/attachment/12643326/screenshot-1.jpg",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,319256,,,Wed Sep 02 20:48:14 UTC 2015,,,,,,,,,,"0|i1j33z:",319597,,,,,,,,,,,,,,,,,,,,"23/Apr/14 19:52;adenysenko;I have it as well. No special test case - but it fails periodically ~1 in 10 cases.
My configuration:
- zkclient: 0.4
- kafka: 2.9.2-0.8.1
- zookeeper: 3.4.5
;;;","23/Apr/14 22:50;guozhang;Currently we are only using zkclient 0.3. Could you check if this problem still persists in that version?

Guozhang;;;","24/Apr/14 03:43;adenysenko;It fails for both zkclient 0.3 and 0.4. Please see attached files.;;;","24/Apr/14 04:10;junrao;Do you see ZK session expiration around that time?;;;","24/Apr/14 05:28;adenysenko;I think it's a case then ""_connection"" is null for some reason: https://github.com/sgroschupf/zkclient/blob/master/src/main/java/org/I0Itec/zkclient/ZkClient.java#L308
Without going deep into hardcore I would override ""ZkClient.create"" with my method checking ""_connection"" for null:

{code:java}
public class KafkaZkClient extends ZkClient{
    public String create(final String path, Object data, final CreateMode mode) throws ZkInterruptedException, IllegalArgumentException, ZkException, RuntimeException {
        if (path == null) {
            throw new NullPointerException(""path must not be null."");
        }
        final byte[] bytes = data == null ? null : serialize(data);

        return retryUntilConnected(new Callable<String>() {

            @Override
            public String call() throws Exception {
                if(_connection==null) throw new ConnectionLossException(); // FIX HERE <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
                return _connection.create(path, bytes, mode);
            }
        });
    }
}
{code} ;;;","24/Apr/14 05:44;adenysenko;The root of the problem is here: https://github.com/sgroschupf/zkclient/blob/master/src/main/java/org/I0Itec/zkclient/ZkClient.java#L690
The first ""return callable.call();"" potentially can be called  even before ""_connection""  is initialized.

So it's better to fix the root.;;;","24/Apr/14 14:46;junrao;Hmm, not sure if this can happen. The following is what happens in the constructor. _connection is set there and the underlying _zk in ZkConnection is also set.

    public ZkClient(IZkConnection zkConnection, int connectionTimeout, ZkSerializer zkSerializer) {
        _connection = zkConnection;
        _zkSerializer = zkSerializer;
        connect(connectionTimeout, this);
    };;;","24/Apr/14 15:22;adenysenko;Could it be by re-using closed ZkClient? https://github.com/sgroschupf/zkclient/blob/master/src/main/java/org/I0Itec/zkclient/ZkClient.java#L941
It happens in some way.;;;","24/Apr/14 15:47;junrao;ZkClient is only closed when shutdown the consumer connector.;;;","05/May/14 05:01;adenysenko;Any progress?
Have a look: !screenshot-1.jpg!;;;","05/Jun/14 21:09;noslowerdna;We are seeing a similar NPE in our application's direct usage of the ZkClient, unrelated to commitOffsets.

Here are a couple example stack traces.
{noformat}
Caused by: java.lang.NullPointerException
	at org.I0Itec.zkclient.ZkConnection.create(ZkConnection.java:87)
	at org.I0Itec.zkclient.ZkClient$1.call(ZkClient.java:308)
	at org.I0Itec.zkclient.ZkClient$1.call(ZkClient.java:304)
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)
	at org.I0Itec.zkclient.ZkClient.create(ZkClient.java:304)
	at org.I0Itec.zkclient.ZkClient.createPersistent(ZkClient.java:213)
{noformat}

{noformat}
Caused by: java.lang.NullPointerException
	at org.I0Itec.zkclient.ZkConnection.writeDataReturnStat(ZkConnection.java:115)
	at org.I0Itec.zkclient.ZkClient$10.call(ZkClient.java:817)
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)
	at org.I0Itec.zkclient.ZkClient.writeDataReturnStat(ZkClient.java:813)
	at org.I0Itec.zkclient.ZkClient.writeData(ZkClient.java:808)
	at org.I0Itec.zkclient.ZkClient.writeData(ZkClient.java:777)
{noformat}

ZkClient implements the org.apache.zookeeper.Watcher interface, so the process() callback can be invoked at any time by the background ZK event thread [1]. This callback method is not synchronized against the other ZkClient public methods, however. So if a state change event occurs that requires a reconnection [2], the internal ZkConnection is closed while reconnecting [3] which sets its org.apache.zookeeper.ZooKeeper to null [4] resulting in the NullPointerException if the process is concurrently using the ZkClient to read or write data.

[1] http://zookeeper.apache.org/doc/r3.3.4/zookeeperProgrammers.html#Java+Binding
[2] https://github.com/sgroschupf/zkclient/blob/master/src/main/java/org/I0Itec/zkclient/ZkClient.java#L457
[3] https://github.com/sgroschupf/zkclient/blob/master/src/main/java/org/I0Itec/zkclient/ZkClient.java#L953-954
[4] https://github.com/sgroschupf/zkclient/blob/master/src/main/java/org/I0Itec/zkclient/ZkConnection.java#L79;;;","05/Jun/14 21:16;noslowerdna;Here's another example stack trace that we have seen.

{noformat}
java.lang.NullPointerException
	at org.I0Itec.zkclient.ZkConnection.exists(ZkConnection.java:95)
	at org.I0Itec.zkclient.ZkClient$3.call(ZkClient.java:439)
	at org.I0Itec.zkclient.ZkClient$3.call(ZkClient.java:436)
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)
	at org.I0Itec.zkclient.ZkClient.exists(ZkClient.java:436)
	at org.I0Itec.zkclient.ZkClient$12.call(ZkClient.java:846)
	at org.I0Itec.zkclient.ZkClient$12.call(ZkClient.java:843)
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)
	at org.I0Itec.zkclient.ZkClient.watchForChilds(ZkClient.java:843)
	at org.I0Itec.zkclient.ZkClient.subscribeChildChanges(ZkClient.java:114)
	at kafka.consumer.ZookeeperConsumerConnector.kafka$consumer$ZookeeperConsumerConnector$$reinitializeConsumer(ZookeeperConsumerConnector.scala:713)
	at kafka.consumer.ZookeeperConsumerConnector$WildcardStreamsHandler.(ZookeeperConsumerConnector.scala:756)
	at kafka.consumer.ZookeeperConsumerConnector.createMessageStreamsByFilter(ZookeeperConsumerConnector.scala:145)
	at kafka.javaapi.consumer.ZookeeperConsumerConnector.createMessageStreamsByFilter(ZookeeperConsumerConnector.scala:96)
{noformat};;;","06/Jun/14 17:24;noslowerdna;I have reported this issue to the zkclient project: https://github.com/sgroschupf/zkclient/issues/25;;;","30/Apr/15 14:48;w00te;Has any progress been made on this, or do any workarounds exist yet?  Has anyone determined what causes it in particular?

We're getting it fairly regularly and I would love to know how to mitigate the issue.;;;","30/Apr/15 15:47;oae;Hi guys,

just had a look at this.
Think there are only 2 possibilities that such an exception can occur:
- 1) a null zkConnection is passed in
- 2) a retryUntilConnected action wakes up and the client was closed in meantime

I could reproduce the NPE for case 2 and changed the code to throw an clear exception instead of risking unclear follow up exception like the NPE's.
See https://github.com/sgroschupf/zkclient/commit/0630c9c6e67ab49a51e80bfd939e4a0d01a69dfe

HTH

PS: this is part of the zkclient-0.5 release which should be online in a few hours!;;;","30/Apr/15 16:28;w00te;Awesome, thank you for the quick follow-up :).;;;","17/Jul/15 20:05;techwhizbang;Can someone confirm that upgrading to zkclient-0.5 did fix the NPE?;;;","20/Jul/15 16:23;parth.brahmbhatt;[~techwhizbang] I upgraded to zkClient-0.5 so I will verify this is fixed and update the jira.;;;","21/Jul/15 00:25;parth.brahmbhatt;[~junrao] Looked at the fix code and ensure that it was part of 0.5 release. The fix also had a unit test so I believe this is ok to resolve. ;;;","02/Sep/15 20:48;junrao;[~parth.brahmbhatt], thanks for confirming this. Resolving this jira.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Topic metadata request handling fails to return all metadata about replicas,KAFKA-820,12637866,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,20/Mar/13 00:32,22/Mar/13 16:13,14/Jul/23 05:39,22/Mar/13 16:13,0.8.0,,,,,,,,,,core,,,0,kafka-0.8,,,"The admin utility that fetches topic metadata needs improvement in error handling. While fetching replica and isr broker information, if one of the replicas is offline, it fails to fetch the replica and isr info for the rest of them. This creates confusion on the client since it seems to the client the rest of the brokers are offline as well.",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/13 01:03;nehanarkhede;kafka-820-v1.patch;https://issues.apache.org/jira/secure/attachment/12574467/kafka-820-v1.patch","20/Mar/13 18:10;nehanarkhede;kafka-820-v2.patch;https://issues.apache.org/jira/secure/attachment/12574588/kafka-820-v2.patch","21/Mar/13 15:30;nehanarkhede;kafka-820-v3.patch;https://issues.apache.org/jira/secure/attachment/12574812/kafka-820-v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,318346,,,Fri Mar 22 16:13:11 UTC 2013,,,,,,,,,,"0|i1ixhr:",318687,,,,,,,,,,,,,,,,,,,,"20/Mar/13 01:03;nehanarkhede;Two changes -

1. Modified topic metadata request handling to return the broker metadata for the rest of the brokers, even if one of them is offline. Also, modified it to throw an exception if even one of the brokers is offline so that error code will get returned to the client
2. ReplicaNotAvailableException is irrelevant to the producer. It should not log the stack trace for it inside BrokerPartitionInfo.updateInfo(). This should only happen for UnknownTopicOrPartitionException and LeaderNotAvailableException;;;","20/Mar/13 16:39;junrao;Thanks for the patch. Not sure that I understand the changes in AdminUtils.getBrokerInfoFromCache(). It seems to me that with or w/o the patch, the method will throw an exception if at least one of the items in brokerIds can't be converted to a Broker object, in which case the return value is irrelevant.;;;","20/Mar/13 17:10;nehanarkhede;The return value is not irrelevant. As I explained, even if one broker is down, it aborts sending the broker data for the rest of the brokers. The impression it gives the client is that all the brokers are dead. The reason it throws the exception at the end is because we need to send the appropriate error code to the client if at least one broker is down.;;;","20/Mar/13 17:21;junrao;When an exception is thrown in getBrokerInfoFromCache(), no value is returned, right?;;;","20/Mar/13 18:10;nehanarkhede;You are right, attached v2 patch to fix the issue;;;","21/Mar/13 15:30;nehanarkhede;- Fixed a bug in BrokerPartitionInfo while printing error code. In v2, I was printing the topic's error code instead of the partition's error code
- As pointed out by Jun offline, dropWhile does not work the way I thought it did. Replaced that with filterNot.;;;","21/Mar/13 17:05;junrao;Thanks for patch v3. A few minor comments. Once addressed, the patch can be checked in.

30. AdminUtil.getBrokerInfoFromCache(): The following line uses double negation
    brokerMetadata.filterNot(!_.isDefined).map(_.get)
It will be easier to read if we do
    brokerMetadata.filter(_.isDefined).map(_.get)

31. AdminUtil.fetchTopicMetadataFromZk():
31.1 In the following statement, should the catch clause be moved to after the first two statements inside try? Otherwise, we will unnecessarily wrap a ReplicaNotAvailableException over another ReplicaNotAvailableException.
          try {
            replicaInfo = getBrokerInfoFromCache(zkClient, cachedBrokerInfo, replicas.map(id => id.toInt))
            isrInfo = getBrokerInfoFromCache(zkClient, cachedBrokerInfo, inSyncReplicas)
            if(replicaInfo.size < replicas.size)
              throw new ReplicaNotAvailableException(""Replica information not available for following brokers: "" +
                replicas.filterNot(replicaInfo.map(_.id).contains(_)).mkString("",""))
            if(isrInfo.size < inSyncReplicas.size)
              throw new ReplicaNotAvailableException(""In Sync Replica information not available for following brokers: "" +
                inSyncReplicas.filterNot(isrInfo.map(_.id).contains(_)).mkString("",""))
          } catch {
            case e => throw new ReplicaNotAvailableException(e)
          }
31.2 Similarly, in the following try/catch clause, should we move the try/catch clause inside case Some(l)? Otherwise, we will be wrapping a LeaderNotAvailableException over another LeaderNotAvailableException.
          try {
            leaderInfo = leader match {
              case Some(l) => Some(getBrokerInfoFromCache(zkClient, cachedBrokerInfo, List(l)).head)
              case None => throw new LeaderNotAvailableException(""No leader exists for partition "" + partition)
            }
          } catch {
            case e => throw new LeaderNotAvailableException(""Leader not available for topic %s partition %d"".format(topic, partition), e)
          }

;;;","22/Mar/13 16:13;nehanarkhede;Checked in after suggested fixes;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce noise in Kafka server logs due to NotLeaderForPartitionException,KAFKA-816,12637838,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,19/Mar/13 22:09,29/May/13 16:59,14/Jul/23 05:39,22/Mar/13 16:32,0.8.0,,,,,,,,,,core,,,0,kafka-0.8,p2,,NotLeaderForPartitionException is logged at the ERROR level with a full stack trace. But really this is just an informational message on the server when a client with stale metadata sends requests to the wrong leader for a partition. This floods the logs either if there are many clients or few clients sending many topics (migration tool or mirror maker). This should probably be logged at WARN and without the stack trace,,balaji.seshadri@dish.com,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,KAFKA-877,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/13 03:45;balaji.seshadri@dish.com;KAFKA-816.jpg;https://issues.apache.org/jira/secure/attachment/12580654/KAFKA-816.jpg","22/Mar/13 16:17;nehanarkhede;kafka-816-v2.patch;https://issues.apache.org/jira/secure/attachment/12575033/kafka-816-v2.patch","19/Mar/13 23:19;nehanarkhede;kafka-816.patch;https://issues.apache.org/jira/secure/attachment/12574441/kafka-816.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,318318,,,Wed May 29 16:59:27 UTC 2013,,,,,,,,,,"0|i1ixbj:",318659,,,,,,,,,,,,,,,,,,,,"19/Mar/13 23:19;nehanarkhede;Special cased NotLeaderForPartitionException and UnknownTopicOrPartitionException to do the following -

- Print a warn message, not a stack trace at ERROR level
- Not increment the failed request metric since that is meant to track a broker's failure to serve requests for a partition for which it is the leader;;;","20/Mar/13 00:43;junrao;Thanks for the patch. Instead of duplicating the comments for each type of exception, is it better to add a comment at the beginning of catch to explain how different exceptions are handled? The comment should probably be added to handleOffsetRequest() too. Also, there is a grammar mistake in ""Failed produce requests is "".;;;","20/Mar/13 00:50;nehanarkhede;Sure. And the comment is not added to handleOffsetRequest since there is no failed offset request rate that we track. Will fix the grammar while checkin.;;;","22/Mar/13 16:17;nehanarkhede;Moved the comments as per your suggestion;;;","22/Mar/13 16:26;junrao;+1 on patch v2. To make the comment clearer, suggest that we change the comment ""// NOTE: Failed produce requests is not incremented here"" to ""// NOTE: Failed produce requests is not incremented for UnknownTopicOrPartitionException and NotLeaderForPartitionException"" (ditto for the comments in readMessageSets()) before checking in.;;;","22/Mar/13 16:32;nehanarkhede;checked in after making the suggested change;;;","25/Apr/13 22:51;balaji.seshadri@dish.com;Using the below trunk and i still see error happening.Please let us know if this can be fixed.


https://github.com/apache/kafka.git

[2013-04-25 16:47:08,924] WARN [console-consumer-24019_MERD7-21964-1366930009136-8b7f9eb7-leader-finder-thread], Failed to add fetcher for [mytopic,0] to broker id:0,host:MERD7-21964.echostar.com,port:9092 (kafka.consumer.ConsumerFetcherManager$$anon$1)
kafka.common.NotLeaderForPartitionException
        at sun.reflect.GeneratedConstructorAccessor1.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
        at java.lang.Class.newInstance0(Class.java:372)
        at java.lang.Class.newInstance(Class.java:325)
        at kafka.common.ErrorMapping$.exceptionFor(ErrorMapping.scala:72)
        at kafka.consumer.SimpleConsumer.earliestOrLatestOffset(SimpleConsumer.scala:163)
        at kafka.consumer.ConsumerFetcherThread.handleOffsetOutOfRange(ConsumerFetcherThread.scala:61)
        at kafka.server.AbstractFetcherThread.addPartition(AbstractFetcherThread.scala:167)
        at kafka.server.AbstractFetcherManager.addFetcher(AbstractFetcherManager.scala:48)
        at kafka.consumer.ConsumerFetcherManager$$anon$1$$anonfun$doWork$3.apply(ConsumerFetcherManager.scala:79)
        at kafka.consumer.ConsumerFetcherManager$$anon$1$$anonfun$doWork$3.apply(ConsumerFetcherManager.scala:75)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:95)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:95)
        at scala.collection.Iterator$class.foreach(Iterator.scala:772)
        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:45)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:95)
        at kafka.consumer.ConsumerFetcherManager$$anon$1.doWork(ConsumerFetcherManager.scala:75)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)

We are evaluating Kafka for our new messaging system and we had tough time running in windows.

We somehow managed to run 0.8 using cygwin but when we run the console producer/consumer,we are not getting messages from consumer.

Please help us to fix this issue,this might not be related but its keeping on throwing this error on consumer side.
;;;","25/Apr/13 22:55;nehanarkhede;All 0.8 development is happening on the 0.8 branch. Please can you retry on that branch ?;;;","25/Apr/13 23:00;balaji.seshadri@dish.com;Please let me know the Git URL.;;;","25/Apr/13 23:02;balaji.seshadri@dish.com;I see fix merged in the show log but it is not working.

SHA-1: 51421fcc0111031bb77f779a6f6c00520d526a34

* KAFKA-816 Reduce noise in Kafka server logs due to NotLeaderForPartitionException; reviewed by Jun Rao
;;;","26/Apr/13 03:46;balaji.seshadri@dish.com;Error occuring still in 0.8 branch,pls refer screen shot attached.;;;","29/May/13 16:59;nehanarkhede;Balaji, 

That fix was to reduce the noise in the kafka server logs. The error you see is on the consumer when the leader for some partitions is failing over to another replica. Can you please move the issue to a separate JIRA?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve SimpleConsumerShell to take in a max messages config option,KAFKA-815,12637829,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,19/Mar/13 21:43,08/Apr/15 01:44,14/Jul/23 05:39,08/Apr/15 01:44,0.8.0,,,,,,,,,,tools,,,0,,,,It's useful to have a max-messages option on the SimpleConsumerShell similar to other tools.,,gwenshap,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/13 22:05;nehanarkhede;kafka-815.patch;https://issues.apache.org/jira/secure/attachment/12574419/kafka-815.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,318309,,,Wed Apr 08 01:44:24 UTC 2015,,,,,,,,,,"0|i1ix9j:",318650,,,,,,,,,,,,,,,,,,,,"19/Mar/13 22:05;nehanarkhede;- Added a max-messages option to SimpleConsumerShell
- Improved some logging
- Very often, I've wanted to use a no op message formatter instead of having the messages go to standard out on every invocation. So added that.;;;","19/Mar/13 23:49;junrao;Thanks for the patch. +1.;;;","08/Apr/15 01:44;gwenshap;was committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor cleanup in Controller,KAFKA-813,12637693,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,19/Mar/13 01:04,25/Mar/13 16:48,14/Jul/23 05:39,25/Mar/13 16:32,0.8.0,,,0.8.0,,,,,,,controller,,,0,kafka-0.8,,,"Before starting work on delete topic support, uploading a patch first to address some minor hiccups that touch a bunch of files:

1. Change PartitionOfflineException to PartitionUnavailableException because in the partition state machine we mark a partition offline when its leader is down, whereas the PartitionOfflineException is thrown when all the assigned replicas of the partition are down.
2. Change PartitionOfflineRate to UnavailablePartitionRate
3. Remove default leader selector from partition state machine's handleStateChange. We can specify null as default when we don't need to use a leader selector.
4. Include controller info in the client id of LeaderAndIsrRequest.
5. Rename controllerContext.allleaders to something more meaningful - partitionLeadershipInfo.
6. We don't need to put partition in OnlinePartition state in partition state machine initializeLeaderAndIsrForPartition, the state change occurs in handleStateChange.
7. Add todo in handleStateChanges
8. Left a comment above ReassignedPartitionLeaderSelector that reassigned replicas are already in the ISR (this is not true for other leader selectors), renamed the vals in the selector.
",,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/13 01:07;swapnilghike;kafka-813-v1.patch;https://issues.apache.org/jira/secure/attachment/12574284/kafka-813-v1.patch","20/Mar/13 01:39;swapnilghike;kafka-813-v2.patch;https://issues.apache.org/jira/secure/attachment/12574473/kafka-813-v2.patch","21/Mar/13 01:22;swapnilghike;kafka-813-v3.patch;https://issues.apache.org/jira/secure/attachment/12574701/kafka-813-v3.patch","24/Mar/13 06:27;swapnilghike;kafka-813-v4.patch;https://issues.apache.org/jira/secure/attachment/12575210/kafka-813-v4.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,318173,,,Mon Mar 25 16:32:34 UTC 2013,,,,,,,,,,"0|i1iwfb:",318514,,,,,,,,,,,,,,,,,,,,"19/Mar/13 01:07;swapnilghike;Unit tests pass.;;;","19/Mar/13 17:07;junrao;Thanks for the patch. Some comments:

1. For 1 and 2, it seems that in this patch, we are treating PartitionOfflineException and PartitionUnavailableException the same. Is it better to just pick PartitionOfflineException (and remove PartitionUnavailableException) since it introduces fewer changes?

2. PartiitonStateChange.handleStateChanges(): Is it a good idea to allow leaderSelector to be null? We will have to check and make sure that leaderSelector is null every time we use it.;;;","19/Mar/13 18:37;nehanarkhede;Thanks for the cleanup patch, Swapnil. Here are some minor suggestions after which we can check it in -


1. ControllerBrokerRequestBatch
1.1 Can we explicitly pass in controller id and controller client id ? It is better to avoid object passing if we can. 
1.2 How about clientId() instead of toString(). This is because it makes it very explicit the purpose of the usage of that API. It is unlikely anything else would want to call toString() on KafkaController

2. PartitionStateMachine
Let's not set the leaderSelector to null as default. Each invocation of handleStateChanges should pass in the correct leader selector for better code readability.

;;;","19/Mar/13 18:58;swapnilghike;@Neha: 2. If the target state is not OnlinePartition, then we don't need to pass in any leader selector. ;;;","19/Mar/13 19:11;nehanarkhede;Correct, another way to do this is to defined a NoOpLeaderSelector and pass that in. The NoOpLeaderSelector can just return the leadership info from partitionLeadershipInfo. At least, that way, the leader selector logic is explicit which was the purpose of the change.;;;","20/Mar/13 01:39;swapnilghike;Addressing comments: 
[~junrao]: 
1. Renamed the exception to PartitionNoReplicaOnlineException, since 
i. the old name caused confusion with OfflinePartition state of the partition state machine 
ii. this exception is thrown when all the replicas assigned to a topic-partition are down, or there are no replicas assigned to a topic-partition

2. Created a new NoOpLeaderSelector and set it as the default.

[~nehanarkhede]: 
1.1. Passing the controllerId and clientId separately to ControllerBrokerRequestBatch
1.2. Renamed KafkaController.toString to clientId

2. Created a new NoOpLeaderSelector, thanks for the great suggestion, and set it as the default. 
i. It returns the existing leader and Isr and replica assignment, so the logs will first contain a warning issued by selectLeader() and then the Zk write will log an error.
ii. I think it is ok to set the no-op leader selector as the default, because we don't need to pass any leader selector when the target state is not OnlinePartition, and there could be little confusion in there about why no leader selector was passed. Earlier we had cases where the target state was OnlinePartition, but it was not clear which leader selector was used, since we provided OfflinePartitionLeaderSelector as the default to handleStateChanges().

Additional changes:
1. Removed OfflinePartitionRate as discussed offline. Instead, created a new gauge OfflinePartitionsCount, which would measure the count of partitions whose individual ""leaders"" are down. The reason to make this change is that OfflinePartitionsRate would provide information about how many partitions go Offline in a given time window, but does not give information about how many partitions don't come back to Online State and stay Offline since the last time window.

Marking the jira as blocker because of the metric change.;;;","20/Mar/13 15:12;junrao;Thanks for patch v2. Some more comments.

20. PartitionStateMachine.initializeLeaderAndIsrForPartition(): why is the following line removed?
          partitionState.put(topicAndPartition, OnlinePartition)

21. PartitionNoReplicaOnlineException seems long. Is it better to use NoReplicaOnlineException?

22. KafkaController: To compute the gauge OfflinePartitionsCount, we need to synchronize on controller lock.

;;;","20/Mar/13 15:19;nehanarkhede;Thanks for patch v2 -

1. KafkaController
1.1 The change to OfflinePartitionsCount looks good. However, there is a distinction between a partition whose leader is not alive but other replicas are so leader election will happen and a partition for which all replicas are dead. In the latter case, there can be no leader for that partition which is a much more dangerous state for a partition to be in. I suggest two metrics, OfflinePartitionsCount to indicate the former and UnavailablePartitionsCount to indicate the latter
1.2 I wonder why ActiveControllerCount and OfflinePartitionsCount are not part of ControllerStats ?

2. NoOpLeaderSelector
Minor code style suggestion - return is not required here.

3. PartitionStateMachine
We don't have to define the noOpLeaderSelector in the controller since it is used only in PartitionStateMachine.handleStateChanges(). Let's move it there. The reason offlineLeaderSelector is there since both the controller and the partition state machine access it.

Nit pick - Can we change PartitionNoReplicaOnlineException to NoReplicaOnlineForPartitionException or simply NoReplicaOnlineException ? :);;;","20/Mar/13 20:16;swapnilghike;A couple of comments/questions before uploading the next patch:

[~junrao]: 
20. Because initializeLeaderAndIsrForPartition() is called at only one place, in handleStateChange(). And handleStateChange() puts the partition in Online state.
21. Ok, renaming to NoReplicaOnlineException.
22. Ok, also please read below.

[~nehanarkhede]: 
1.1 I agree that having all the replicas assigned to a partition down is a dangerous situation. I guess that will be reflected if the OfflinePartitionsCount is non-zero for a while, and then we can dig into Zk. An OfflinePartitionsCount that remains non-zero for a while could also indicate another dangerous situation where the replicas are up but leader is not being assigned. So, i think that clues provided by an OfflinePartitionsCount that remains non-zero for a few minutes subsume the clues given by UnavailablePartitionsCount, and we may not specifically need the latter. Thoughts?

2. Argh, yes, thanks.

3. I agree that Controller does not need to know about a useless leader selector. Moving NoOpLeaderSelector object to partition state machine. 

[~junrao],[~nehanarkhede]: :1.2 I believe the reason was to initialize the gauges when the controller object is created. However, we can move the gauges to ControllerStats and force their initialization like server.registerStats(). So perhaps it would be good if we could decide which jmx beans need to be force-initialized and at which places in the code. Accordingly I can make the relevant changes. 
;;;","21/Mar/13 00:39;nehanarkhede;1.1 Makes sense, we can just live with the existing counter for now.
Also, let's register the controller stats at startup. That way, they at least show a value of 0 instead of nan.;;;","21/Mar/13 01:22;swapnilghike;Patch v3 takes care of the comments made above.;;;","21/Mar/13 15:34;junrao;Thanks for patch v3. Looks good. Just one more comment:

30. KafkaController: Is it really useful to include port in clientId?;;;","21/Mar/13 15:36;nehanarkhede;+1 on patch v3;;;","22/Mar/13 10:23;swapnilghike;30. port has been included to maintain consistency with other such clientIds. I agree that it's not specifically useful since the hostname gives enough information for log debugging etc. ;;;","22/Mar/13 16:06;junrao;Ok. That's fine then. One more comment:

31. PartitionStateMachine.electLeaderForPartition(): In the following code, is there any value in wrapping a NoReplicaOnlineException over another NoReplicaOnlineException?
      case noReplicaOnlineEx: NoReplicaOnlineException =>
        val failMsg = ""All replicas %s for partition %s are dead. Marking this partition offline.""
                        .format(controllerContext.partitionReplicaAssignment(topicAndPartition).mkString("",""), topicAndPartition)
        throw new NoReplicaOnlineException(failMsg, noReplicaOnlineEx)
;;;","24/Mar/13 06:27;swapnilghike;Thanks for pointing that out. Attached patch v4. When the leader goes down, the state change log will show the OfflinePartition state change. So yes, there is no need to wrap it in another NoReplicaOnlineException. 
;;;","25/Mar/13 16:32;junrao;Thanks for patch v4. +1 and committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix clientId in migration tool,KAFKA-811,12637638,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,18/Mar/13 20:51,20/Mar/13 16:45,14/Jul/23 05:39,18/Mar/13 21:34,0.8.0,,,0.8.0,,,,,,,,,,0,kafka-0.8,,,Append producer threadId to the clientId passed by the user.,,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Mar/13 19:03;swapnilghike;kafka-811-v2.patch;https://issues.apache.org/jira/secure/attachment/12574381/kafka-811-v2.patch","18/Mar/13 20:54;swapnilghike;kafka-811.patch;https://issues.apache.org/jira/secure/attachment/12574213/kafka-811.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,318118,,,Wed Mar 20 16:45:01 UTC 2013,,,,,,,,,,"0|i1iw33:",318459,,,,,,,,,,,,,,,,,,,,"18/Mar/13 21:32;nehanarkhede;+1, thanks for the patch!;;;","19/Mar/13 19:03;swapnilghike;Can we check in this patch ? The 1st one had a real stupid mistake.;;;","19/Mar/13 19:12;nehanarkhede;it looks like the same, what is the change ?;;;","19/Mar/13 19:17;swapnilghike;The 1st patch will pass client ids like abc-0, abc-0-1, abc-0-1-2,... (where abc is the clientId set by the user in their config file)

The 2nd patch will pass client ids like abc-0, abc-1, abc-2;;;","20/Mar/13 16:45;junrao;Committed v2 to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LineMessageReader doesn't correctly parse the key separator,KAFKA-807,12637055,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,dragosm,dragosm,dragosm,14/Mar/13 17:16,29/Mar/13 15:35,14/Jul/23 05:39,29/Mar/13 15:35,0.8.0,,,0.8.0,,,,,,,tools,,,0,patch,producer,,"Typo in key name prevents extracting the key separator. The patch follows; what's the recommended way to submit patches?

Index: core/src/main/scala/kafka/producer/ConsoleProducer.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>/**\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \""License\""); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n * \n *    http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \""AS IS\"" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage kafka.producer\n\nimport scala.collection.JavaConversions._\nimport joptsimple._\nimport java.util.Properties\nimport java.io._\nimport kafka.common._\nimport kafka.message._\nimport kafka.serializer._\n\nobject ConsoleProducer { \n\n  def main(args: Array[String]) { \n    val parser = new OptionParser\n    val topicOpt = parser.accepts(\""topic\"", \""REQUIRED: The topic id to produce messages to.\"")\n                           .withRequiredArg\n                           .describedAs(\""topic\"")\n                           .ofType(classOf[String])\n    val brokerListOpt = parser.accepts(\""broker-list\"", \""REQUIRED: The broker list string in the form HOST1:PORT1,HOST2:PORT2.\"")\n                           .withRequiredArg\n                           .describedAs(\""broker-list\"")\n                           .ofType(classOf[String])\n    val syncOpt = parser.accepts(\""sync\"", \""If set message send requests to the brokers are synchronously, one at a time as they arrive.\"")\n    val compressOpt = parser.accepts(\""compress\"", \""If set, messages batches are sent compressed\"")\n    val batchSizeOpt = parser.accepts(\""batch-size\"", \""Number of messages to send in a single batch if they are not being sent synchronously.\"")\n                             .withRequiredArg\n                             .describedAs(\""size\"")\n                             .ofType(classOf[java.lang.Integer])\n                             .defaultsTo(200)\n    val sendTimeoutOpt = parser.accepts(\""timeout\"", \""If set and the producer is running in asynchronous mode, this gives the maximum amount of time\"" + \n                                                   \"" a message will queue awaiting suffient batch size. The value is given in ms.\"")\n                               .withRequiredArg\n                               .describedAs(\""timeout_ms\"")\n                               .ofType(classOf[java.lang.Long])\n                               .defaultsTo(1000)\n    val queueSizeOpt = parser.accepts(\""queue-size\"", \""If set and the producer is running in asynchronous mode, this gives the maximum amount of \"" + \n                                                   \"" messages will queue awaiting suffient batch size.\"")\n                               .withRequiredArg\n                               .describedAs(\""queue_size\"")\n                               .ofType(classOf[java.lang.Long])\n                               .defaultsTo(10000)\n    val queueEnqueueTimeoutMsOpt = parser.accepts(\""queue-enqueuetimeout-ms\"", \""Timeout for event enqueue\"")\n                               .withRequiredArg\n                               .describedAs(\""queue enqueuetimeout ms\"")\n                               .ofType(classOf[java.lang.Long])\n                               .defaultsTo(0)\n    val requestRequiredAcksOpt = parser.accepts(\""request-required-acks\"", \""The required acks of the producer requests\"")\n                               .withRequiredArg\n                               .describedAs(\""request required acks\"")\n                               .ofType(classOf[java.lang.Integer])\n                               .defaultsTo(0)\n    val requestTimeoutMsOpt = parser.accepts(\""request-timeout-ms\"", \""The ack timeout of the producer requests. Value must be non-negative and non-zero\"")\n                               .withRequiredArg\n                               .describedAs(\""request timeout ms\"")\n                               .ofType(classOf[java.lang.Integer])\n                               .defaultsTo(1500)\n    val valueEncoderOpt = parser.accepts(\""value-serializer\"", \""The class name of the message encoder implementation to use for serializing values.\"")\n                                 .withRequiredArg\n                                 .describedAs(\""encoder_class\"")\n                                 .ofType(classOf[java.lang.String])\n                                 .defaultsTo(classOf[StringEncoder].getName)\n    val keyEncoderOpt = parser.accepts(\""key-serializer\"", \""The class name of the message encoder implementation to use for serializing keys.\"")\n                                 .withRequiredArg\n                                 .describedAs(\""encoder_class\"")\n                                 .ofType(classOf[java.lang.String])\n                                 .defaultsTo(classOf[StringEncoder].getName)\n    val messageReaderOpt = parser.accepts(\""line-reader\"", \""The class name of the class to use for reading lines from standard in. \"" + \n                                                          \""By default each line is read as a separate message.\"")\n                                  .withRequiredArg\n                                  .describedAs(\""reader_class\"")\n                                  .ofType(classOf[java.lang.String])\n                                  .defaultsTo(classOf[LineMessageReader].getName)\n    val socketBufferSizeOpt = parser.accepts(\""socket-buffer-size\"", \""The size of the tcp RECV size.\"")\n                                  .withRequiredArg\n                                  .describedAs(\""size\"")\n                                  .ofType(classOf[java.lang.Integer])\n                                  .defaultsTo(1024*100)\n    val propertyOpt = parser.accepts(\""property\"", \""A mechanism to pass user-defined properties in the form key=value to the message reader. \"" +\n                                                 \""This allows custom configuration for a user-defined message reader.\"")\n                            .withRequiredArg\n                            .describedAs(\""prop\"")\n                            .ofType(classOf[String])\n\n\n    val options = parser.parse(args : _*)\n    for(arg <- List(topicOpt, brokerListOpt)) {\n      if(!options.has(arg)) {\n        System.err.println(\""Missing required argument \\\""\"" + arg + \""\\\""\"")\n        parser.printHelpOn(System.err)\n        System.exit(1)\n      }\n    }\n\n    val topic = options.valueOf(topicOpt)\n    val brokerList = options.valueOf(brokerListOpt)\n    val sync = options.has(syncOpt)\n    val compress = options.has(compressOpt)\n    val batchSize = options.valueOf(batchSizeOpt)\n    val sendTimeout = options.valueOf(sendTimeoutOpt)\n    val queueSize = options.valueOf(queueSizeOpt)\n    val queueEnqueueTimeoutMs = options.valueOf(queueEnqueueTimeoutMsOpt)\n    val requestRequiredAcks = options.valueOf(requestRequiredAcksOpt)\n    val requestTimeoutMs = options.valueOf(requestTimeoutMsOpt)\n    val keyEncoderClass = options.valueOf(keyEncoderOpt)\n    val valueEncoderClass = options.valueOf(valueEncoderOpt)\n    val readerClass = options.valueOf(messageReaderOpt)\n    val socketBuffer = options.valueOf(socketBufferSizeOpt)\n    val cmdLineProps = parseLineReaderArgs(options.valuesOf(propertyOpt))\n    cmdLineProps.put(\""topic\"", topic)\n\n    val props = new Properties()\n    props.put(\""broker.list\"", brokerList)\n    val codec = if(compress) DefaultCompressionCodec.codec else NoCompressionCodec.codec\n    props.put(\""compression.codec\"", codec.toString)\n    props.put(\""producer.type\"", if(sync) \""sync\"" else \""async\"")\n    if(options.has(batchSizeOpt))\n      props.put(\""batch.num.messages\"", batchSize.toString)\n    props.put(\""queue.buffering.max.ms\"", sendTimeout.toString)\n    props.put(\""queue.buffering.max.messages\"", queueSize.toString)\n    props.put(\""queue.enqueue.timeout.ms\"", queueEnqueueTimeoutMs.toString)\n    props.put(\""request.required.acks\"", requestRequiredAcks.toString)\n    props.put(\""request.timeout.ms\"", requestTimeoutMs.toString)\n    props.put(\""key.serializer.class\"", keyEncoderClass)\n    props.put(\""serializer.class\"", valueEncoderClass)\n    props.put(\""send.buffer.bytes\"", socketBuffer.toString)\n    val reader = Class.forName(readerClass).newInstance().asInstanceOf[MessageReader[AnyRef, AnyRef]]\n    reader.init(System.in, cmdLineProps)\n\n    try {\n        val producer = new Producer[AnyRef, AnyRef](new ProducerConfig(props))\n\n        Runtime.getRuntime.addShutdownHook(new Thread() {\n          override def run() {\n            producer.close()\n          }\n        })\n\n        var message: KeyedMessage[AnyRef, AnyRef] = null\n        do {\n          message = reader.readMessage()\n          if(message != null)\n            producer.send(message)\n        } while(message != null)\n    } catch {\n      case e: Exception =>\n        e.printStackTrace\n        System.exit(1)\n    }\n    System.exit(0)\n  }\n\n  def parseLineReaderArgs(args: Iterable[String]): Properties = {\n    val splits = args.map(_ split \""=\"").filterNot(_ == null).filterNot(_.length == 0)\n    if(!splits.forall(_.length == 2)) {\n      System.err.println(\""Invalid line reader properties: \"" + args.mkString(\"" \""))\n      System.exit(1)\n    }\n    val props = new Properties\n    for(a <- splits)\n      props.put(a(0), a(1))\n    props\n  }\n\n  trait MessageReader[K,V] { \n    def init(inputStream: InputStream, props: Properties) {}\n    def readMessage(): KeyedMessage[K,V]\n    def close() {}\n  }\n\n  class LineMessageReader extends MessageReader[String, String] {\n    var topic: String = null\n    var reader: BufferedReader = null\n    var parseKey = false\n    var keySeparator = \""\\t\""\n    var ignoreError = false\n    var lineNumber = 0\n\n    override def init(inputStream: InputStream, props: Properties) {\n      topic = props.getProperty(\""topic\"")\n      if(props.containsKey(\""parse.key\""))\n        parseKey = props.getProperty(\""parse.key\"").trim.toLowerCase.equals(\""true\"")\n      if(props.containsKey(\""key.seperator\""))\n        keySeparator = props.getProperty(\""key.separator\"")\n      if(props.containsKey(\""ignore.error\""))\n        ignoreError = props.getProperty(\""ignore.error\"").trim.toLowerCase.equals(\""true\"")\n      reader = new BufferedReader(new InputStreamReader(inputStream))\n    }\n\n    override def readMessage() = {\n      lineNumber += 1\n      (reader.readLine(), parseKey) match {\n        case (null, _) => null\n        case (line, true) =>\n          line.indexOf(keySeparator) match {\n            case -1 =>\n              if(ignoreError)\n                new KeyedMessage(topic, line)\n              else\n                throw new KafkaException(\""No key found on line \"" + lineNumber + \"": \"" + line)\n            case n =>\n              new KeyedMessage(topic,\n                             line.substring(0, n), \n                             if(n + keySeparator.size > line.size) \""\"" else line.substring(n + keySeparator.size))\n          }\n        case (line, false) =>\n          new KeyedMessage(topic, line)\n      }\n    }\n  }\n}\n
===================================================================
--- core/src/main/scala/kafka/producer/ConsoleProducer.scala	(revision 290d5e0eac38e9917c64353a131154821b899f26)
+++ core/src/main/scala/kafka/producer/ConsoleProducer.scala	(revision )
@@ -196,7 +196,7 @@
       topic = props.getProperty(""topic"")
       if(props.containsKey(""parse.key""))
         parseKey = props.getProperty(""parse.key"").trim.toLowerCase.equals(""true"")
-      if(props.containsKey(""key.seperator""))
+      if(props.containsKey(""key.separator""))
         keySeparator = props.getProperty(""key.separator"")
       if(props.containsKey(""ignore.error""))
         ignoreError = props.getProperty(""ignore.error"").trim.toLowerCase.equals(""true"")
",,dragosm,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,317547,,,Fri Mar 29 15:35:17 UTC 2013,,,,,,,,,,"0|i1isk7:",317888,,,,,,,,,,,,,,,,,,,,"26/Mar/13 16:51;dragosm;Index: core/build.sbt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- core/build.sbt	(date 1364252653000)
+++ core/build.sbt	(date 1364254450000)
@@ -18,8 +18,9 @@
 
 libraryDependencies <<= (scalaVersion, libraryDependencies) { (sv, deps) =>
   deps :+ (sv match {
-    case ""2.8.0"" => ""org.scalatest"" %  ""scalatest"" % ""1.2"" % ""test""
+    case ""2.8.0"" => ""org.scalatest"" %  ""scalatest"" % ""1.2""   % ""test""
+    case ""2.9.2"" => ""org.scalatest"" %% ""scalatest"" % ""1.9.1"" % ""test""
-    case _       => ""org.scalatest"" %% ""scalatest"" % ""1.8"" % ""test""
+    case _       => ""org.scalatest"" %% ""scalatest"" % ""1.8""   % ""test""
   })
 }
 
Index: core/src/main/scala/kafka/producer/ConsoleProducer.scala
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- core/src/main/scala/kafka/producer/ConsoleProducer.scala	(date 1364252653000)
+++ core/src/main/scala/kafka/producer/ConsoleProducer.scala	(date 1364254450000)
@@ -196,7 +196,7 @@
       topic = props.getProperty(""topic"")
       if(props.containsKey(""parse.key""))
         parseKey = props.getProperty(""parse.key"").trim.toLowerCase.equals(""true"")
-      if(props.containsKey(""key.seperator""))
+      if(props.containsKey(""key.separator""))
         keySeparator = props.getProperty(""key.separator"")
       if(props.containsKey(""ignore.error""))
         ignoreError = props.getProperty(""ignore.error"").trim.toLowerCase.equals(""true"")
;;;","29/Mar/13 15:35;junrao;Thanks for the patch. Committed the change to ConsoleProducer to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log.append() may fail with a compressed messageset containing no uncompressed messages ,KAFKA-805,12636907,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,junrao,junrao,14/Mar/13 00:12,05/Sep/17 06:38,14/Jul/23 05:39,05/Sep/17 06:38,0.8.0,,,,,,,,,,core,,,0,,,,"If a broker receives a compressed messageset that contains no uncompressed message, the log.append() logic may fail.",,junrao,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,317399,,,Tue Sep 05 06:38:35 UTC 2017,,,,,,,,,,"0|i1irnb:",317740,,,,,,,,,,,,,,,,,,,,"05/Sep/17 06:38;omkreddy;Not able to reproduce the issue. looks like this got fixed in newer versions. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect index in the log of a follower,KAFKA-804,12636905,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,junrao,junrao,14/Mar/13 00:05,19/Mar/13 20:35,14/Jul/23 05:39,18/Mar/13 22:57,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"In the follower, in log.append(), we use the offset of the 1st compressed message as the offset for appending index entries. This means that the index is pointing to an earlier position in the file than it should.  Instead, it should use the offset of the 1st uncompressed message for appending index.",,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/13 00:06;junrao;kafka-804.patch;https://issues.apache.org/jira/secure/attachment/12573626/kafka-804.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,317397,,,Mon Mar 18 22:57:52 UTC 2013,,,,,,,,,,"0|i1irmv:",317738,,,,,,,,,,,,,,,,,,,,"14/Mar/13 00:06;junrao;Attach a patch. It always uses the nextOffset before append as the 1st offset.;;;","14/Mar/13 05:03;nehanarkhede;+1;;;","14/Mar/13 16:45;jkreps;I am not sure about this change. There are two choices to add to the index: the offset of the wrapper message which is the offset of the last message in the compressed set or the offset of the first message in the compressed set. Either of these are correct since the index entry needs to be a lower bound on the position, so this is not a bug. I do agree that we should ideally    produce the same index entries on the leader and follower, but I am not sure if we should choose the first or last message. The problem with choosing the first message is that the integrity of the index can only be validated using deep iteration on the log and the index then points to a message which doesn't have the index entry given in the index which is very odd.

This code looks very confusing to me. I can't tell if this change is making it worse or the code is already just very confusing. I recommend we leave it as is on 0.8 and fix it on trunk where that code has been cleaned up. Let's also figure out the pros and cons of which offset the index should contain. ;;;","14/Mar/13 17:23;junrao;Yes, the log code is a bit more complicated now to address a few recent issues that we found :
kafka-802: flush interval based on compressed message set
kafka-765: corrupted messages cause broker to shut down
kafka-767: message size check needs to be done after offset assignment

My main concern of not addressing this in 0.8 is that this makes it hard to do index verification in our system tests.

Indexing the first message seems to make the most intuitive sense. Yes, we do need to patch DumpLogSegments to support deep message iteration, in order to do index verification.;;;","14/Mar/13 18:57;jkreps;So does this patch have the fix for all those?;;;","18/Mar/13 19:33;jkreps;- I only see stats being updated in the case where assignOffsets is true.
- Please fix the logging to be human readable.

Can you give a patch against trunk too?;;;","18/Mar/13 20:11;jkreps;Also: 
- the offsets tuple gets recreated which isnt needed
- the count that is being done in analyzeAndValidateMessageSet is wrong and not being used. It should be removed (this can happen on trunk).;;;","18/Mar/13 22:57;junrao;Thanks for the review. Committed to 0.8 by addressing the following issues.

1. The stat for message rate is actually meant for monitoring data rates from producers. So it's correct to only account for it in the leader during offset assignment.

2. Fixed the log.

3. Fixed the issue with unnecessarily recreating the tuple.

4. Didn't fix count in analyzeAndValidateMessageSet in this patch. Will fix it when merging to trunk. Will create a separate jira for review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Offset returned to producer is not consistent,KAFKA-803,12636880,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,junrao,junrao,13/Mar/13 22:15,17/May/16 14:11,14/Jul/23 05:39,04/Apr/14 15:02,,,,0.8.1.1,,,,,,,,,,0,,,,"Currently, the offset that we return to the producer is the offset of the first message if the producer request doesn't go through purgatory, and the offset of the last message, if otherwise. We need to make this consistent.",,guozhang,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,317372,,,Fri Apr 04 15:02:27 UTC 2014,,,,,,,,,,"0|i1irhb:",317713,,,,,,,,,,,,,,,,,,,,"13/Mar/13 22:16;junrao;Since in 0.8, we haven't exposed the offset in the Producer api, we can probably fix this issue post 0.8.;;;","02/Apr/14 18:08;guozhang;I think this issue has already been solved in the new producer release process. [~junrao] could you confirm that?;;;","02/Apr/14 18:20;jjkoshy;Yes - I think this was fixed as part of KAFKA-1260;;;","04/Apr/14 15:02;junrao;Fixed in KAFKA-1260.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flush message interval is based on compressed message count,KAFKA-802,12636725,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,junrao,junrao,13/Mar/13 04:35,19/Mar/13 17:01,14/Jul/23 05:39,19/Mar/13 17:01,0.8.0,,,,,,,,,,core,,,0,,,,"In Log.append(), we use compressed message count to determine whether to flush the log or not. We should use uncompressed message count instead.",,junrao,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/13 17:38;nehanarkhede;kafka-802-v2.patch;https://issues.apache.org/jira/secure/attachment/12573538/kafka-802-v2.patch","13/Mar/13 18:01;nehanarkhede;kafka-802-v3.patch;https://issues.apache.org/jira/secure/attachment/12573544/kafka-802-v3.patch","13/Mar/13 16:24;nehanarkhede;kafka-802.patch;https://issues.apache.org/jira/secure/attachment/12573530/kafka-802.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,317217,,,Wed Mar 13 18:40:56 UTC 2013,,,,,,,,,,"0|i1iqiv:",317558,,,,,,,,,,,,,,,,,,,,"13/Mar/13 04:38;junrao;This is easy to fix on the leader since it iterates uncompressed messages to assign offsets. It's a bit hard to fix in the follower. We could iterate uncompressed messages. This will add CPU overhead though.;;;","13/Mar/13 16:24;nehanarkhede;Two changes -

1. Changed the maybeFlush() call from Log.append() to take in number of appended messages. On the leader, this will be the uncompressed message count, but on the follower, this will still be the compressed message count since we don't recompress data on the follower. I think this is fine.

2. Changed the default of log.flush.interval.messages to 10000 instead of 500. With replication, a very low default for this config doesn't make sense;;;","13/Mar/13 16:29;sriramsub;Personally, having different meaning to flushed messages (compressed for follower Vs uncompressed for leader) will make it tough to reason out things. ;;;","13/Mar/13 17:28;junrao;Sriram, agreed that it's better if the meaning for flush interval is consistent btw the leader and the follower. After thinking a bit more, I think this is doable. Basically, during shallow iteration, the offset returned for the compressed message is the offset of the last uncompressed message in it. So, the last offset is actually correct even if messages are compressed. The problem is with the first offset. I think we can set first offset to nextOffset at the beginning of append(). This way, whether we need to assign offsets or not, we can alway obtain the right first offset. We can then use different between last and first offset to drive log flush.;;;","13/Mar/13 17:38;nehanarkhede;That is a good insight, Jun. Included that fix in v2;;;","13/Mar/13 17:48;junrao;Thanks for patch v2. We need a few tweaks.

20. The assignment to firstOffset and the computation of numAppendedMessages have to be done inside the synchronization of the lock.

21. The following statements in append are no longer valid and can be removed.
          // return the offset at which the messages were appended
          offsets

;;;","13/Mar/13 18:40;junrao;Thanks for patch v3. +1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix MessagesInPerSec mbean to count uncompressed message rate,KAFKA-801,12636492,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,nehanarkhede,nehanarkhede,12/Mar/13 01:30,13/Mar/13 17:14,14/Jul/23 05:39,13/Mar/13 17:11,0.8.0,,,0.8.0,,,,,,,core,,,0,kafka-0.8,p2,,"Today, BrokerTopicMetrics.MessagesInPerSec does shallow iteration and hence counts the number of compressed messages in every request. This does not make sense, we should count the number of uncompressed messages stored on the leader. Also, this metric makes sense only on the leader.",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/13 04:32;junrao;kafka-801.patch;https://issues.apache.org/jira/secure/attachment/12573472/kafka-801.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,316984,,,Wed Mar 13 17:11:27 UTC 2013,,,,,,,,,,"0|i1ip3b:",317325,,,,,,,,,,,,,,,,,,,,"13/Mar/13 04:32;junrao;Attach a patch.;;;","13/Mar/13 15:40;nehanarkhede;+1. Minor simplification -

Can we do the following instead of subtracting 1 and then adding 1 to compute the number of messages ?
              val numMessages = offsetCounter.get - firstOffset 
;;;","13/Mar/13 16:02;junrao;We could do that. However, the meaning of offsetCounter is not clear while the meaning of lastOffset is.;;;","13/Mar/13 16:15;nehanarkhede;Agree, I guess we can pass on that suggestion;;;","13/Mar/13 17:11;junrao;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
inSyncReplica in Partition needs some tweaks,KAFKA-800,12636450,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriramsub,sriramsub,sriramsub,11/Mar/13 21:06,12/Mar/13 18:22,14/Jul/23 05:39,12/Mar/13 18:22,0.8.0,,,0.8.0,,,,,,,,,,0,kafka-0.8,p1,,"1. isUnderReplicated method in Partition.scala needs to take the leaderIsrUpdateLock before reading the inSyncReplica structure. We suspect that this could cause the gauge to read stale values from the cache.

2. toString api should take the leaderIsrUpdateLock to prevent anything from blowing up when it iterates the data structures",,junrao,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/13 18:12;sriramsub;KAFKA-800-v2.patch;https://issues.apache.org/jira/secure/attachment/12573379/KAFKA-800-v2.patch","12/Mar/13 17:17;sriramsub;KAFKA-800.patch;https://issues.apache.org/jira/secure/attachment/12573370/KAFKA-800.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,316942,,,Tue Mar 12 18:22:12 UTC 2013,,,,,,,,,,"0|i1iou7:",317284,,,,,,,,,,,,,,,,,,,,"11/Mar/13 21:34;junrao;Also, the leaderCount gauge in ReplicaManager needs to use the lock too.;;;","12/Mar/13 17:25;junrao;Thanks for the patch. Just one comment.

1. ReplicaManager: There is no need to sync on the leaderPartitionsLock to access allPartitions. It uses Pool, which is backed by a ConcurrentHashMap already.;;;","12/Mar/13 18:12;nehanarkhede;Thanks for the patch, a few questions/comments -
ReplicaManager
1. Like Jun said, we don't need to synchronize on leaderPartitionsLock inside getOrCreatePartition and getPartition()
2. I don't think we need to synchronize on leaderPartitionsLock inside checkpointHighWatermarks too, that lock is protecting the leaderPartitions data structure.
3. In stopReplica(), we need to synchronize only around leaderPartitions, not allPartitions and the info statement following that.;;;","12/Mar/13 18:12;sriramsub;You are right. I was being too paranoid there.;;;","12/Mar/13 18:22;nehanarkhede;+1 on v2;;;","12/Mar/13 18:22;nehanarkhede;v2 checked in;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use biased histograms instead of uniform histograms in KafkaMetricsGroup,KAFKA-798,12636156,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,09/Mar/13 01:19,11/Mar/13 18:05,14/Jul/23 05:39,09/Mar/13 22:53,0.8.0,,,0.8.0,,,,,,,,,,0,kafka-0.8,p1,,"A biased histogram produces quantiles which are representative of (roughly) the last five minutes of data. On the other hand, a uniform histogram produces quantiles which are valid for the entirely of the histogram’s lifetime. More discussion at http://metrics.codahale.com/manual/core/#man-core-histograms-uniform
",,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/13 01:24;swapnilghike;kafka-798.patch;https://issues.apache.org/jira/secure/attachment/12572878/kafka-798.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,316648,,,Mon Mar 11 18:05:25 UTC 2013,,,,,,,,,,"0|i1in0v:",316990,,,,,,,,,,,,,,,,,,,,"09/Mar/13 01:24;swapnilghike;One word fix.;;;","09/Mar/13 22:52;nehanarkhede;+1, thanks for catching the problem and fixing it!;;;","09/Mar/13 22:53;nehanarkhede;Fix is checked in to 0.8;;;","11/Mar/13 15:11;junrao;Thanks for the patch. What about Timer? Does it include a biased histogram?;;;","11/Mar/13 18:05;swapnilghike;Yes, timer includes a biased histogram by default.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include controllerId in all requests sent by controller,KAFKA-793,12635659,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,06/Mar/13 20:57,08/Mar/13 19:32,14/Jul/23 05:39,08/Mar/13 02:21,0.8.0,,,0.8.0,,,,,,,,,,0,kafka-0.8,p1,,"As part of KAFKA-513, we added controllerId in LeaderAndIsrRequest. We should add that to all requests sent by the controller, such as StopReplicaRequest.

Other small issues that can be taken care of by this jira:
1. The string ""state.change.logger"" is used in many places. We should create a constant val and reuse the val to avoid human mistakes.
2. Remove the private constructor from ControllerChannelManager.
3. Remove default from jsonFileOpt in PreferredReplicaLeaderElectionCommand, because the default is only used to indicate an absense of a legitimate jsonFile. We can check the same using if(options.has(jsonFileOpt))

",,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/13 00:57;swapnilghike;kafka-793-v1.patch;https://issues.apache.org/jira/secure/attachment/12572662/kafka-793-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,316152,,,Fri Mar 08 02:21:10 UTC 2013,,,,,,,,,,"0|i1ijyv:",316495,,,,,,,,,,,,,,,,,,,,"08/Mar/13 00:57;swapnilghike;Patch v1. Created a val for ""state.change.logger"" in KafkaController object.;;;","08/Mar/13 02:21;junrao;Thanks for the patch. +1. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simple Consumer connecting to Broker that is not the Leader generates wrong error,KAFKA-787,12635374,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,chriscurtin,chriscurtin,05/Mar/13 17:18,08/Mar/13 02:32,14/Jul/23 05:39,08/Mar/13 02:32,0.8.0,,,,,,,,,,core,,,0,kafka-0.8,p2,,"0.8.0 HEAD from 3/4/2013

Using a SimpleConsumer, connect to a Broker that is NOT the leader for the topic/partition you want to fetch, but IS a replica.

On fetch the error returned is '5' 'ErrorMapping.LeaderNotAvailableCode'

Per email thread with Neha:

""Ideally, you should get back  ErrorMapping.NotLeaderForPartitionCode""

Key part of the test is the Broker must be a replica. If the topic/partition is not on the Broker you get a different (correct) error.",,chriscurtin,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/13 23:17;nehanarkhede;kafka-787.patch;https://issues.apache.org/jira/secure/attachment/12572193/kafka-787.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,315867,,,Fri Mar 08 02:01:06 UTC 2013,,,,,,,,,,"0|i1ii7j:",316210,,,,,,,,,,,,,,,,,,,,"05/Mar/13 23:17;nehanarkhede;Minor fix for the correct error code;;;","08/Mar/13 02:01;junrao;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Use ""withRequiredArg"" while parsing jopt options in all tools",KAFKA-786,12635260,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,05/Mar/13 00:45,06/Mar/13 18:27,14/Jul/23 05:39,06/Mar/13 18:13,,,,0.8.0,,,,,,,,,,0,kafka-0.8,p2,,"While parsing jopt Options in our tools, we sometimes use withRequiredArg() and sometimes use withOptionalArg(). I think this confusing and we should always use withRequiredArg().

withOptionalArg() allows you to provide an option without an argument. For instance, the following commands will yield the same result if xyz was a parser option that accepted an optional argument and was provided a default in the tool's code:
kafka-tool.sh --xyz 
kafka-tool.sh 

I don't quite see the need to allow the 1st command, think that writing code will be less confusing if we allowed only the second command. To do that, we can make all options require arguments. These arguments will need to be given via command line or via a default in the code. So if xyz was an option that required an argument then you will see the following:

kafka-tool.sh --xyz 
Option ['xyz'] requires an argument //printed by jOpt

kafka-tool.sh --xyz argumentVal
// Kafka tool proceeds

If you want to use a default value specified for xyz in the code, then simply run ./kafka-tool.sh.",,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/13 23:51;swapnilghike;kafka-786-v2.patch;https://issues.apache.org/jira/secure/attachment/12572205/kafka-786-v2.patch","05/Mar/13 01:03;swapnilghike;kafka-786.patch;https://issues.apache.org/jira/secure/attachment/12572008/kafka-786.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,315753,,,Wed Mar 06 18:13:17 UTC 2013,,,,,,,,,,"0|i1ihi7:",316096,,,,,,,,,,,,,,,,,,,,"05/Mar/13 00:45;swapnilghike;Also note that withRequiredArg() does not check if xyz has a non-null value, we still need to perform that check ourselves in the code. So, jopt will do nothing to prevent you from running into a NPE, if you ran
kafka.tool.sh and xyz was an option that required an argument, but was not provided a default value in the code.;;;","05/Mar/13 01:03;swapnilghike;Attached a patch.;;;","05/Mar/13 23:51;swapnilghike;Jun raised a good question: What do we do for options like ""print"" or ""verify"" or ""enable"" etc? These options typically don't expect a boolean argument, it should be enough to specify these options on the command line to sort of enable them in the program.

The answer is to not use any of withRequiredArg() or withOptionalArg(). We can simply write something like the following:
val printOpt = parser.accepts(""print-data-log"", ""if set, printing the messages content when dumping data logs"")

Later we should check in the program like the following:
if(options.has(printOpt) print() else doNothing();;;","06/Mar/13 18:13;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resolve bugs in PreferredReplicaLeaderElection admin tool,KAFKA-785,12635222,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriramsub,swapnilghike,swapnilghike,04/Mar/13 22:11,05/Mar/13 17:39,14/Jul/23 05:39,05/Mar/13 17:39,0.8.0,,,0.8.0,,,,,,,,,,0,kafka-0.8,p2,,"1. Since we run the preferred replica election on all partitions if the jsonFile is empty, the jsonFileOpt should not be checked in the code as a required option. 
2. We should not pass """" to Utils.readFileAsString
3. KAFKA-780",,nehanarkhede,sriramsub,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/13 00:07;sriramsub;KAFKA-785.patch;https://issues.apache.org/jira/secure/attachment/12571994/KAFKA-785.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,315715,,,Tue Mar 05 17:39:05 UTC 2013,,,,,,,,,,"0|i1ih9r:",316058,,,,,,,,,,,,,,,,,,,,"05/Mar/13 00:07;sriramsub;1. Made partition input file optional for PreferredReplicaLeaderElectionCommand
2. Removed shutdown hook from PreferredReplicaLeaderElectionCommand and ReassignPartitionsCommand as it was causing NPE and it was not useful since the operations on the controller are asynchronous.;;;","05/Mar/13 00:12;swapnilghike;Can we use val jsonString = if (options.has(jsonFileOpt)) Utils.readFileAsString(jsonFile) else """" ?
This way we don't need to provide a default to the required arg.;;;","05/Mar/13 00:17;sriramsub;It does not simplify anything though. Just two different ways of doing things.;;;","05/Mar/13 00:20;swapnilghike;Well, you avoid a small jump in logic (jsonFile == """" => option not provided an argument). Also we have been using options.has() in other tools.;;;","05/Mar/13 17:38;nehanarkhede;+1;;;","05/Mar/13 17:39;nehanarkhede;Checked in.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
replicas not being displayed in TopicMetadataResponse when replica's Broker is shutdown,KAFKA-782,12635168,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,chriscurtin,chriscurtin,04/Mar/13 17:58,01/May/13 14:26,14/Jul/23 05:39,01/May/13 14:26,0.8.0,,,,,,,,,,replication,,,0,kafka-0.8,p2,,"Setup
- 4 nodes in the cluster
- topic has 10 partitions, numbered 0-9 in the output below
- configure the cluster so one of the nodes doesn't hold any leader copies of the partition. I did this by shutting down a node, waiting for reassignment of leaders and starting again

Output of the TopicMetadataResponse call:

Partition: 0:vrd01.atlnp1 R:[  vrd03.atlnp1 vrd04.atlnp1 vrd01.atlnp1] I:[ vrd01.atlnp1 vrd03.atlnp1 vrd04.atlnp1]
Partition: 1:vrd01.atlnp1 R:[  vrd04.atlnp1 vrd01.atlnp1 vrd02.atlnp1] I:[ vrd01.atlnp1 vrd04.atlnp1 vrd02.atlnp1]
Partition: 2:vrd01.atlnp1 R:[  vrd01.atlnp1 vrd02.atlnp1 vrd03.atlnp1] I:[ vrd01.atlnp1 vrd03.atlnp1 vrd02.atlnp1]
Partition: 3:vrd03.atlnp1 R:[  vrd02.atlnp1 vrd03.atlnp1 vrd04.atlnp1] I:[ vrd03.atlnp1 vrd04.atlnp1 vrd02.atlnp1]
Partition: 4:vrd01.atlnp1 R:[  vrd03.atlnp1 vrd01.atlnp1 vrd02.atlnp1] I:[ vrd01.atlnp1 vrd03.atlnp1 vrd02.atlnp1]
Partition: 5:vrd03.atlnp1 R:[  vrd04.atlnp1 vrd02.atlnp1 vrd03.atlnp1] I:[ vrd03.atlnp1 vrd04.atlnp1 vrd02.atlnp1]
Partition: 6:vrd01.atlnp1 R:[  vrd01.atlnp1 vrd03.atlnp1 vrd04.atlnp1] I:[ vrd01.atlnp1 vrd03.atlnp1 vrd04.atlnp1]
Partition: 7:vrd01.atlnp1 R:[  vrd02.atlnp1 vrd04.atlnp1 vrd01.atlnp1] I:[ vrd01.atlnp1 vrd04.atlnp1 vrd02.atlnp1]
Partition: 8:vrd03.atlnp1 R:[  vrd03.atlnp1 vrd02.atlnp1 vrd04.atlnp1] I:[ vrd03.atlnp1 vrd04.atlnp1 vrd02.atlnp1]
Partition: 9:vrd01.atlnp1 R:[  vrd04.atlnp1 vrd03.atlnp1 vrd01.atlnp1] I:[ vrd01.atlnp1 vrd03.atlnp1 vrd04.atlnp1]

Note that vrd02.atlnp1 does not contain any leader replicas.

Shutdown vrd02 normally. Run TopicMetadataResponse again:

Partition: 0:vrd01.atlnp1 R:[  vrd03.atlnp1 vrd04.atlnp1 vrd01.atlnp1] I:[ vrd01.atlnp1 vrd03.atlnp1 vrd04.atlnp1]
Partition: 1:vrd01.atlnp1 R:[ ] I:[]
Partition: 2:vrd01.atlnp1 R:[ ] I:[]
Partition: 3:vrd03.atlnp1 R:[ ] I:[]
Partition: 4:vrd01.atlnp1 R:[ ] I:[]
Partition: 5:vrd03.atlnp1 R:[ ] I:[]
Partition: 6:vrd01.atlnp1 R:[  vrd01.atlnp1 vrd03.atlnp1 vrd04.atlnp1] I:[ vrd01.atlnp1 vrd03.atlnp1 vrd04.atlnp1]
Partition: 7:vrd01.atlnp1 R:[ ] I:[]
Partition: 8:vrd03.atlnp1 R:[ ] I:[]
Partition: 9:vrd01.atlnp1 R:[  vrd04.atlnp1 vrd03.atlnp1 vrd01.atlnp1] I:[ vrd01.atlnp1 vrd03.atlnp1 vrd04.atlnp1]

Note that the partitions where vrd02 was in the replica set no longer show any replicas.

Not clear if the list of replicas isn't being set correctly or the replicas aren't associated with the partition any longer. 


Java code:

  kafka.javaapi.consumer.SimpleConsumer consumer  = new SimpleConsumer(""vrd01.atlnp1"",
                9092,
                100000,
                64 * 1024, ""test"");

        List<String> topics2 = new ArrayList<String>();
        topics2.add(""storm-anon"");
        TopicMetadataRequest req = new TopicMetadataRequest(topics2);
        kafka.javaapi.TopicMetadataResponse resp = consumer.send(req);

        List<kafka.javaapi.TopicMetadata> data3 =  resp.topicsMetadata();

        for (kafka.javaapi.TopicMetadata item : data3) {

           for (kafka.javaapi.PartitionMetadata part: item.partitionsMetadata() ) {
               String replicas = """";
               String isr = """";

               for (kafka.cluster.Broker replica: part.replicas() ) {
                   replicas += "" "" + replica.host();
               }

               for (kafka.cluster.Broker replica: part.isr() ) {
                   isr += "" "" + replica.host();
               }

              System.out.println( ""Partition: "" +   part.partitionId()  + "":"" + part.leader().host() + "" R:[ "" + replicas + ""] I:["" + isr + ""]"");
           }
        }","
$ uname -a
Linux vrd01.atlnp1 2.6.32-279.el6.x86_64 #1 SMP Fri Jun 22 12:19:21 UTC 2012 x86_64 x86_64 x86_64 GNU/Linux

$ java -version
java version ""1.6.0_25""
Java(TM) SE Runtime Environment (build 1.6.0_25-b06)
Java HotSpot(TM) 64-Bit Server VM (build 20.0-b11, mixed mode)

Kafka 0.8.0 loaded from HEAD on 1/29/2013",chriscurtin,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,315661,,,Wed May 01 13:17:49 UTC 2013,,,,,,,,,,"0|i1igxr:",316004,,,,,,,,,,,,,,,,,,,,"04/Mar/13 19:53;chriscurtin;Confirmed same behavior with 0.8.0 HEAD as of 3/4/2013;;;","30/Apr/13 17:17;nehanarkhede;This is probably fixed by now, can you retry ?;;;","01/May/13 13:17;chriscurtin;I tested with the 0.8.0 HEAD as of 4/30/2013 and the issue is resolved. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standardize Zk data structures for Re-assign partitions and Preferred replication election,KAFKA-779,12634941,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,02/Mar/13 00:57,13/Mar/13 02:04,14/Jul/23 05:39,06/Mar/13 20:55,0.8.0,,,0.8.0,,,,,,,,,,0,kafka-0.8,p2,,Follow the schema at https://cwiki.apache.org/confluence/display/KAFKA/Kafka+data+structures+in+Zookeeper,,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/13 04:27;swapnilghike;kafka-779-v1.patch;https://issues.apache.org/jira/secure/attachment/12571841/kafka-779-v1.patch","04/Mar/13 21:54;swapnilghike;kafka-779-v2.patch;https://issues.apache.org/jira/secure/attachment/12571963/kafka-779-v2.patch","06/Mar/13 20:24;swapnilghike;kafka-779-v3.patch;https://issues.apache.org/jira/secure/attachment/12572385/kafka-779-v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,315434,,,Sat Mar 09 12:48:15 UTC 2013,,,,,,,,,,"0|i1ifjr:",315778,,,,,,,,,,,,,,,,,,,,"04/Mar/13 04:27;swapnilghike;Changes:

1. Zk data format standardization for admin/reassign_partitions and admin/preffered_replica_election. (These changes still hit the bug mentioned in KAFKA-780 which occurred without the changes in this patch.)

2. I changed Json manipulation related functions in Utils, because mergeJsonObjects had a bug which could screw up a JSON string that included a nested object or an array, while attempting to sort the fields in the string. I replaced it with a new function mergeJsonFields and created a new function mapToJsonFields. As it turned out, these two functions also helped reduce some lines of code in other JSON related functions in Utils. 

3. Ran optimize imports on the entire source code. Three files unrelated to this patch have been touched.

Testing done - 
i. Unit tests pass
ii. Verified the format of all Zk data structures as mentioned in the wiki to ensure 2 above didn't break anything. Verified that the fields are sorted.
iii. Tested reassign-partitions tool and preferred-replica-election admin tools, but they hit the exception mentioned in KAFKA-780.;;;","04/Mar/13 21:54;swapnilghike;Modified the description of the --path-to-json-file option required by the tools.;;;","06/Mar/13 18:48;junrao;Thanks for patch v2. Looks good overall. A couple of comments:

1. PreferredReplicaLeaderElectionCommand: We shouldn't let the user put in the version # in path-to-json-file. The tool knows the version and should fill it in. Ditto for ReassignPartitionsCommand.

2. I had the following unit test failure.
[error] Test Failed: testPartitionReassignmentWithLeaderInNewReplicas(kafka.admin.AdminTest)
junit.framework.AssertionFailedError: Partition should have been reassigned to 0, 2, 3 expected:<List(0, 2, 3)> but was:<List(0, 1, 2)>
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.failNotEquals(Assert.java:277)
	at junit.framework.Assert.assertEquals(Assert.java:64)
	at kafka.admin.AdminTest.testPartitionReassignmentWithLeaderInNewReplicas(AdminTest.scala:232)

 ;;;","06/Mar/13 20:24;swapnilghike;Patch v3:

1. Ok, not asking the user to provide version in both the tools.

2. Seems like this was a transient failure. I ran it multiple times on my machine, it passed. We can take a look at it if it fails more often than not. ;;;","06/Mar/13 20:55;junrao;Thanks for patch v3. Committed to 0.8.;;;","09/Mar/13 12:48;swapnilghike;Jay had a great suggestion about combining Json encoding utils into one single util. He has already checked in the code to trunk at KAFKA-554. So, perhaps we should make the related refactoring in trunk. I would be happy to do it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing ZK format breaks some tools,KAFKA-776,12634505,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,sriramsub,sriramsub,27/Feb/13 22:42,28/Feb/13 03:18,14/Jul/23 05:39,28/Feb/13 03:18,0.8.0,,,0.8.0,,,,,,,,,,0,kafka-0.8,p1,,"There are some tools that parse the zk output and they might break.1 has been verified to break.

Few that read the zk output are 
1. Shutdown Broker
2. PreferredReplicaLeader
3. ConsumerOffsetChecker

There could be few others but I have not checked everything",,nehanarkhede,sriramsub,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/13 01:34;swapnilghike;kafka-776-v1.patch;https://issues.apache.org/jira/secure/attachment/12571332/kafka-776-v1.patch","28/Feb/13 02:43;swapnilghike;kafka-776-v2.patch;https://issues.apache.org/jira/secure/attachment/12571344/kafka-776-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,314998,,,Thu Feb 28 03:18:21 UTC 2013,,,,,,,,,,"0|i1icuv:",315342,,,,,,,,,,,,,,,,,,,,"27/Feb/13 23:46;swapnilghike;From what I see in the code, kafka.admin.ShutdownBroker and kafka.tools.ConsumerOffsetChecker should break because of KAFKA-755.

Unrelated to KAFKA-755, kafka.tools.{ExportZkOffsets, VerifyConsumerRebalance} should break because we don't seem to have tested them for a while and there has been an API change in ZkUtils.readDataMaybeNull.

Sriram, which tool did you verify to break? ;;;","28/Feb/13 01:34;swapnilghike;Attached a patch to fix the 4 tools.;;;","28/Feb/13 02:21;nehanarkhede;Thanks for the patch. Overall looks good, few review comments -
1. ConsumerOffsetChecker
Fix error() statement in getConsumer() to include the throwable e as the 2nd argument to error(). That will ensure that the stack trace is pretty printed

2. ExportZkOffsets
Maybe the following 2 lines should be in the ""case: Some(m)"" block or protected by if(offsetVal != null)
 fileWriter.write(offsetPath + "":"" + offsetVal + ""\n"")
            debug(offsetPath + "" => "" + offsetVal)

3. ShutdownBroker
Would you mind changing the error() line to the following -
error(""Operation failed due to controller failure"", t);;;","28/Feb/13 02:43;swapnilghike;Attached the changes.;;;","28/Feb/13 03:18;nehanarkhede;+1 on patch v2;;;","28/Feb/13 03:18;nehanarkhede;Checked in v2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Very long error message on the producer during produce requests failures,KAFKA-775,12634320,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,27/Feb/13 00:00,27/Feb/13 00:46,14/Jul/23 05:39,27/Feb/13 00:45,0.8.0,,,,,,,,,,producer ,,,0,kafka-0.8,p1,,"Saw this on the producer during a cluster upgrade -

2013-02-26 23:19:37,386 - ERROR [ProducerSendThread-1413963846:Logging$class@96] - Failed to send requests for topics LixTreatmentEvent,LixTreatmentEvent,LixTreatmentEvent,LixTreatmentEvent,LixTreatmentEvent,LixTreatmentEvent,LixTreatmentEvent,LixTreatmentEvent,LixTreatmentEvent,LixTreatmentEvent,LixTreatmentEvent,LixTreatmentEvent,LixTreatmentEvent,LixTreatmentEvent,... super long line)",,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,314813,,,Wed Feb 27 00:45:54 UTC 2013,,,,,,,,,,"0|i1ibpr:",315157,,,,,,,,,,,,,,,,,,,,"27/Feb/13 00:45;nehanarkhede;diff --git a/core/src/main/scala/kafka/producer/async/DefaultEventHandler.scala b/core/src/main/scala/kafka/producer/async/DefaultEventHandler.scala
index 4cfe2a4..dab1b9c 100644
--- a/core/src/main/scala/kafka/producer/async/DefaultEventHandler.scala
+++ b/core/src/main/scala/kafka/producer/async/DefaultEventHandler.scala
@@ -82,7 +82,8 @@ class DefaultEventHandler[K,V](config: ProducerConfig,
       if(outstandingProduceRequests.size > 0) {
         producerStats.failedSendRate.mark()
         val correlationIdEnd = correlationId.get()
-        error(""Failed to send requests for topics %s with correlation ids in [%d,%d]"".format(outstandingProduceRequests.map(_.topic).mkString("",""),
+        error(""Failed to send requests for topics %s with correlation ids in [%d,%d]""
+          .format(outstandingProduceRequests.map(_.topic).toSet.mkString("",""),
           correlationIdStart, correlationIdEnd-1))
         throw new FailedToSendMessageException(""Failed to send messages after "" + config.messageSendMaxRetries + "" tries."", null)
       }
;;;","27/Feb/13 00:45;nehanarkhede;Checked in without review since it's a minor patch. Before looking into, wasn't sure if this was a legitimate bug or not;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Periodic refresh of topic metadata on the producer does not check for error code in the response,KAFKA-774,12634295,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,26/Feb/13 21:44,27/Feb/13 00:41,14/Jul/23 05:39,27/Feb/13 00:40,0.8.0,,,,,,,,,,producer ,,,0,p1,,,"The producer does a periodic refresh of the metadata but marks the attempt as successful based only on whether the response was received or not. It does not check for error codes in the response. This is probably ok since those produce requests will fail and the required topics will enter the list for topic metadata refresh in the next attempt. However, it will be good to log this since it looks like the metadata was refreshed but actually there could be failures.",,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/13 23:03;nehanarkhede;kafka-774-v2.patch;https://issues.apache.org/jira/secure/attachment/12571083/kafka-774-v2.patch","27/Feb/13 00:16;nehanarkhede;kafka-774-v3.patch;https://issues.apache.org/jira/secure/attachment/12571094/kafka-774-v3.patch","26/Feb/13 21:51;nehanarkhede;kafka-774.patch;https://issues.apache.org/jira/secure/attachment/12571068/kafka-774.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,314788,,,Wed Feb 27 00:40:58 UTC 2013,,,,,,,,,,"0|i1ibk7:",315132,,,,,,,,,,,,,,,,,,,,"26/Feb/13 21:51;nehanarkhede;Minor patch, probably could even skip review. The error code for partition metadata is changed from debug to warn. This is useful in investigating incorrect metadata issues which is what I observe on one of our clusters, but not sure at what point it started since the error code is in debug;;;","26/Feb/13 22:02;swapnilghike;Useful stuff, +1.;;;","26/Feb/13 23:03;nehanarkhede;Thanks for the quick review, Swapnil. While reviewing the server side metadata code, I found another problem. We were sending error codes only for LeaderNotAvailableException and ReplicaNotAvailableException, but if there are other errors while fetching metadata, those were just logged on the server and the client will not know. Fixed that in this patch. ;;;","26/Feb/13 23:34;swapnilghike;I doubt if the v2 patch will do what it intends to. In the big try catch block that you idented, we catch all exceptions and throw LeaderNotAvailableException and ReplicaNotAvailableException, which is what is caught by the last catch block.

Apart from that, it was unnecessary to have two separate cases in the last catch block, we can consolidate them into one.;;;","26/Feb/13 23:41;nehanarkhede;Not sure if I understood your point about consolidating catch blocks, doesn't this consolidate everything -

          case e =>
            new PartitionMetadata(partition, leaderInfo, replicaInfo, isrInfo,
                                  ErrorMapping.codeFor(e.getClass.asInstanceOf[Class[Throwable]]))

It is true that we explicitly throw only those 2 exceptions, but we could hit few others as well. This will catch all of those.;;;","26/Feb/13 23:54;swapnilghike;Yes, your change consolidates everything, which is what I meant. The indentation change to the big try block is good as well.

If we threw any other exceptions (which I think can be thrown only from getBrokerInfoFromCache), wouldn't the inner catch blocks convert them to LeaderNotAvailableException or ReplicaNotAvailableException? So the question would be which other exceptions are we trying to catch here and can we really send error codes to client except those for LeaderNotAvailableException or ReplicaNotAvailableException.

Another question would be in getBrokerInfoFromCache, if we hit a Zk related exception do we want to convert it to one of LeaderNotAvailableException or ReplicaNotAvailableException?
;;;","27/Feb/13 00:16;nehanarkhede;I see your concern here. The reason we convert those exceptions to one of LeaderNotAvailable or ReplicaNotAvailable is so that the client can appropriately understand the nature of the error. In other words, if there was ""some"" error while fetching the leader for a partiton, the client will only care about LeaderNotAvailable as far as topic metadata request is concerned. What we need to think is whether the client will understand any other type of internal Kafka exception. If yes, we should cover that, if not we can send whatever level of detail is easy for clients to understand. Hence, the conversion.

But you raised a good point, just from troubleshooting perspective, it might be worth at least logging the root cause on the server. This patch adds that.;;;","27/Feb/13 00:30;swapnilghike;Yup, looks good. +1;;;","27/Feb/13 00:40;nehanarkhede;Thanks for the reviews, checked in v3 after fixing indentation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka.integration.PrimitiveApiTest fails intermittently,KAFKA-773,12633965,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,25/Feb/13 18:45,26/Feb/13 23:23,14/Jul/23 05:39,26/Feb/13 23:23,0.8.0,,,,,,,,,,,,,0,,,,"
junit.framework.AssertionFailedError: 
Expected :List(a_test3, b_test3)
Actual   :List()
 <Click to see difference>
	at kafka.integration.PrimitiveApiTest$$anonfun$testPipelinedProduceRequests$6.apply(PrimitiveApiTest.scala:340)
	at kafka.integration.PrimitiveApiTest$$anonfun$testPipelinedProduceRequests$6.apply(PrimitiveApiTest.scala:338)
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
	at scala.collection.immutable.List.foreach(List.scala:45)
	at kafka.integration.PrimitiveApiTest.testPipelinedProduceRequests(PrimitiveApiTest.scala:338)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at org.junit.internal.runners.OldTestClassRunner.run(OldTestClassRunner.java:35)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:121)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:76)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:195)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:63)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)


Process finished with exit code 255",,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/13 22:07;swapnilghike;kafka-773-v2.patch;https://issues.apache.org/jira/secure/attachment/12571072/kafka-773-v2.patch","26/Feb/13 03:26;swapnilghike;kafka-773.patch;https://issues.apache.org/jira/secure/attachment/12570920/kafka-773.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,314459,,,Tue Feb 26 23:23:07 UTC 2013,,,,,,,,,,"0|i1i9j3:",314803,,,,,,,,,,,,,,,,,,,,"26/Feb/13 03:26;swapnilghike;This test fails when highWatermark for a partition is updated after its log segment is read. Doesn't look like a real issue, we can fix the test by making the main thread wait until all highwatermarks have been updated.

Some debugging: 

Highwatermark for topic test4 partition 0 updated to 2
Highwatermark for topic test1 partition 0 updated to 2
Highwatermark for topic test2 partition 0 updated to 2

Reading 10000 bytes from offset 0 in log test2-0 of length 76 bytes, maxOffset = 2
Attempt to read with a maximum offset (2) less than the start offset (0).
inside logsegment messageset sizeinBytes = 76 length = 76 startposition = 0

Reading 10000 bytes from offset 0 in log test3-0 of length 76 bytes, maxOffset = 0
Attempt to read with a maximum offset (0) less than the start offset (0).
inside logsegment messageset sizeinBytes = 76 length = 0 startposition = 0

Reading 10000 bytes from offset 0 in log test4-0 of length 76 bytes, maxOffset = 2
Attempt to read with a maximum offset (2) less than the start offset (0).
inside logsegment messageset sizeinBytes = 76 length = 76 startposition = 0

Reading 10000 bytes from offset 0 in log test1-0 of length 76 bytes, maxOffset = 2
Attempt to read with a maximum offset (2) less than the start offset (0).
inside logsegment messageset sizeinBytes = 76 length = 76 startposition = 0

bytesReadable = 228

Highwatermark for topic test3 partition 0 updated to 2

junit.framework.AssertionFailedError: 
Expected :List(a_test3, b_test3)
Actual   :List();;;","26/Feb/13 03:47;nehanarkhede;Thanks for the patch. Adding a sleep seems to fix it, but will be timing dependent. How about using waitUntilTrue to check for the highwatermark values ? It will be faster than a sleep since at most times it will finish quickly.;;;","26/Feb/13 22:07;swapnilghike;Yes you're right. Attached patch v2.;;;","26/Feb/13 23:16;nehanarkhede;Looks good, +1;;;","26/Feb/13 23:23;nehanarkhede;Thanks for patch v3, checked in;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System Test Transient Failure on testcase_0122,KAFKA-772,12633953,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriramsub,jfung,jfung,25/Feb/13 17:34,05/Mar/13 16:26,14/Jul/23 05:39,05/Mar/13 06:41,0.8.0,,,0.8.0,,,,,,,,,,0,kafka-0.8,p1,,"* This test case is failing randomly in the past few weeks. Please note there is a small % data loss allowance for the test case with Ack = 1. But the failure in this case is the mismatch of log segment checksum across the replicas.

* Test description:
3 brokers cluster
Replication factor = 3
No. topic = 2
No. partitions = 3
Controlled failure (kill -15)
Ack = 1

* Test case output
_test_case_name  :  testcase_0122
_test_class_name  :  ReplicaBasicTest
arg : auto_create_topic  :  true
arg : bounce_broker  :  true
arg : broker_type  :  leader
arg : message_producing_free_time_sec  :  15
arg : num_iteration  :  3
arg : num_partition  :  3
arg : replica_factor  :  3
arg : sleep_seconds_between_producer_calls  :  1
validation_status  : 
     Leader Election Latency - iter 1 brokerid 3  :  377.00 ms
     Leader Election Latency - iter 2 brokerid 1  :  374.00 ms
     Leader Election Latency - iter 3 brokerid 2  :  384.00 ms
     Leader Election Latency MAX  :  384.00
     Leader Election Latency MIN  :  374.00
     Unique messages from consumer on [test_1] at simple_consumer_test_1-0_r1.log  :  1750
     Unique messages from consumer on [test_1] at simple_consumer_test_1-0_r2.log  :  1750
     Unique messages from consumer on [test_1] at simple_consumer_test_1-0_r3.log  :  1750
     Unique messages from consumer on [test_1] at simple_consumer_test_1-1_r1.log  :  1750
     Unique messages from consumer on [test_1] at simple_consumer_test_1-1_r2.log  :  1750
     Unique messages from consumer on [test_1] at simple_consumer_test_1-1_r3.log  :  1750
     Unique messages from consumer on [test_1] at simple_consumer_test_1-2_r1.log  :  1500
     Unique messages from consumer on [test_1] at simple_consumer_test_1-2_r2.log  :  1500
     Unique messages from consumer on [test_1] at simple_consumer_test_1-2_r3.log  :  1500
     Unique messages from consumer on [test_2]  :  5000
     Unique messages from consumer on [test_2] at simple_consumer_test_2-0_r1.log  :  1714
     Unique messages from consumer on [test_2] at simple_consumer_test_2-0_r2.log  :  1714
     Unique messages from consumer on [test_2] at simple_consumer_test_2-0_r3.log  :  1680
     Unique messages from consumer on [test_2] at simple_consumer_test_2-1_r1.log  :  1708
     Unique messages from consumer on [test_2] at simple_consumer_test_2-1_r2.log  :  1708
     Unique messages from consumer on [test_2] at simple_consumer_test_2-1_r3.log  :  1708
     Unique messages from consumer on [test_2] at simple_consumer_test_2-2_r1.log  :  1469
     Unique messages from consumer on [test_2] at simple_consumer_test_2-2_r2.log  :  1469
     Unique messages from consumer on [test_2] at simple_consumer_test_2-2_r3.log  :  1469
     Unique messages from producer on [test_2]  :  4900
     Validate for data matched on topic [test_1] across replicas  :  PASSED
     Validate for data matched on topic [test_2]  :  FAILED
     Validate for data matched on topic [test_2] across replicas  :  FAILED
     Validate for merged log segment checksum in cluster [source]  :  FAILED
     Validate leader election successful  :  PASSED
",,jfung,junrao,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/13 22:44;sriramsub;KAFKA-772.patch;https://issues.apache.org/jira/secure/attachment/12571974/KAFKA-772.patch","25/Feb/13 17:35;jfung;testcase_0122.tar.gz;https://issues.apache.org/jira/secure/attachment/12570815/testcase_0122.tar.gz","01/Mar/13 18:03;jfung;testcase_0125.tar.gz;https://issues.apache.org/jira/secure/attachment/12571621/testcase_0125.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,314447,,,Tue Mar 05 16:26:48 UTC 2013,,,,,,,,,,"0|i1i9gf:",314791,,,,,,,,,,,,,,,,,,,,"25/Feb/13 17:35;jfung;Attached a tar file for all log4j messages and data log files;;;","28/Feb/13 21:59;sriramsub;There are two issues with the given logs. Both the issues are for topic 2 - partition 0 on broker 3.

1. Segment 1 starting with logical offset 0 on broker 3 does not have continuous logical offsets. Logical offset 699 is followed by 734. 
2. Segment 2 starting with logical offset 974 on broker 3 is 0 bytes while that in broker 2 has values from 974 to 1713. Broker 3 has segment 3 starting with logical offset 1012 to 1713. Broker 2 does not have any third segment.

We have run the test in a loop multiple times for a day but have not been able to repro this on the local box. I am still investigating how the logs could end up in this state during continuous restarts with ack = 0 and replication factor = 3 ;;;","01/Mar/13 17:54;jfung;There is a similar failure in testcase_0125 yesterday in our distributed environment. Attached the log4j messages and data log segment files for reference.

The failure is as follows (similar to testcase_0122):

     Unique messages from consumer on [test_1] at simple_consumer_test_1-0_r1.log  :  1715
     Unique messages from consumer on [test_1] at simple_consumer_test_1-0_r2.log  :  1715
     Unique messages from consumer on [test_1] at simple_consumer_test_1-0_r3.log  :  1715
     Unique messages from consumer on [test_1] at simple_consumer_test_1-1_r1.log  :  1711
     Unique messages from consumer on [test_1] at simple_consumer_test_1-1_r2.log  :  1711
     Unique messages from consumer on [test_1] at simple_consumer_test_1-1_r3.log  :  1711
     Unique messages from consumer on [test_1] at simple_consumer_test_1-2_r1.log  :  1469
     Unique messages from consumer on [test_1] at simple_consumer_test_1-2_r2.log  :  1469
     Unique messages from consumer on [test_1] at simple_consumer_test_1-2_r3.log  :  1469
     Unique messages from consumer on [test_2]  :  4895
     Unique messages from consumer on [test_2] at simple_consumer_test_2-0_r1.log  :  1715
     Unique messages from consumer on [test_2] at simple_consumer_test_2-0_r2.log  :  1715
     Unique messages from consumer on [test_2] at simple_consumer_test_2-0_r3.log  :  1682
     Unique messages from consumer on [test_2] at simple_consumer_test_2-1_r1.log  :  1708
     Unique messages from consumer on [test_2] at simple_consumer_test_2-1_r2.log  :  1708
     Unique messages from consumer on [test_2] at simple_consumer_test_2-1_r3.log  :  1708
     Unique messages from consumer on [test_2] at simple_consumer_test_2-2_r1.log  :  1467
     Unique messages from consumer on [test_2] at simple_consumer_test_2-2_r2.log  :  1467
     Unique messages from consumer on [test_2] at simple_consumer_test_2-2_r3.log  :  1467
     Unique messages from producer on [test_2]  :  4900
     Validate for data matched on topic [test_1] across replicas  :  PASSED
     Validate for data matched on topic [test_2]  :  PASSED
     Validate for data matched on topic [test_2] across replicas  :  FAILED
     Validate for merged log segment checksum in cluster [source]  :  FAILED
     Validate leader election successful  :  PASSED
;;;","05/Mar/13 00:03;nehanarkhede;It would be useful to maybe add a WARN message and log the topic, partition, replica id, current offset, fetch offset when this happens. Other than that, this fix looks good.;;;","05/Mar/13 00:14;sriramsub;I would like WARN to be actionable. Do you think it would be useful in this case? I am thinking what we would do if we saw this message in the log now that we know this is a valid case.;;;","05/Mar/13 00:29;sriramsub;The test failed on Monday and then again failed on Friday. It was clear that the issue was timing related. We tried to reproduce the failure on the local box (repeatedly running the test) but could not reproduce it. I did some code browsing but did not have much luck. So I decided to setup tracing and run the test repeatedly in a distributed environment over the weekend and was hoping that it would fail. Luckily, it did and the trace logs proved to be useful in identifying the issue. Thanks to John for setting this up.

What you see below are excerpts from the trace log which pertain to this failure at different points in time. In this particular failure, topic_2 / partitions 2 had missing logical offsets from 570 to 582 on broker 3 (3 brokers in total).

current fetch offset = 582 
current HW = 570
Leader for topic_2/partition 2 = broker 2

1. The lines below show the Fetch request that was issued by broker 3 to broker 2 just before broker 1 was shutdown. The requested offset is 582 for [test_2,2].

[2013-03-02 12:37:56,034] TRACE [ReplicaFetcherThread-0-2], issuing to broker 2 of fetch request Name: FetchRequest; Version: 0; CorrelationId: 121; ClientId: ReplicaFetcherThread-0-2; ReplicaId: 3; MaxWait: 500 ms; MinBytes: 4096 bytes; RequestInfo: [test_1,0] -> PartitionFetchInfo(700,1048576),[test_2,1] -> PartitionFetchInfo(677,1048576),[test_2,2] -> PartitionFetchInfo(582,1048576),[test_2,0] -> PartitionFetchInfo(679,1048576),[test_1,2] -> PartitionFetchInfo(600,1048576),[test_1,1] -> PartitionFetchInfo(699,1048576) (kafka.server.ReplicaFetcherThread)

2. Broker 1 is shutdown and broker 3 handles leader and isr request. Note that [test_2,2] still follows broker 2 but we still issue a makefollower call for it.

[2013-03-02 12:37:56,086] INFO Replica Manager on Broker 3: Handling leader and isr request Name: LeaderAndIsrRequest; Version: 0; CorrelationId: 2; ClientId: ; AckTimeoutMs: 1000 ms; ControllerEpoch: 2; PartitionStateInfo: (test_1,0) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""2,1,3"", ""leader"":""2"", ""leaderEpoch"":""1"" },1),3),(test_2,1) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""2,3"", ""leader"":""2"", ""leaderEpoch"":""2"" },2),3),(test_2,2) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""2,1,3"", ""leader"":""2"", ""leaderEpoch"":""1"" },1),3),(test_2,0) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""2,3"", ""leader"":""2"", ""leaderEpoch"":""2"" },2),3),(test_1,2) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""2,3"", ""leader"":""2"", ""leaderEpoch"":""2"" },2),3),(test_1,1) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""2,1,3"", ""leader"":""2"", ""leaderEpoch"":""1"" },1),3); Leaders: id:2,host:xxxx(kafka.server.ReplicaManager)

3. The leader and isr request results in removing the fetcher to broker 2 for [test_2,2], truncating the log to high watermark (570) and then adding back the fetcher to the same broker.

[2013-03-02 12:37:56,088] INFO [ReplicaFetcherManager on broker 3] removing fetcher on topic test_2, partition 2 (kafka.server.ReplicaFetcherManager)
[2013-03-02 12:37:56,088] INFO [Kafka Log on Broker 3], Truncated log segment /tmp/kafka_server_3_logs/test_2-2/00000000000000000000.log to target offset 570 (kafka.log.Log)
[2013-03-02 12:37:56,088] INFO [ReplicaFetcherManager on broker 3] adding fetcher on topic test_2, partion 2, initOffset 570 to broker 2 with fetcherId 0 (kafka.server.ReplicaFetcherManager)

4. The leader and isr request is completed at this point of time.

[2013-03-02 12:37:56,090] INFO Replica Manager on Broker 3: Completed leader and isr request Name: LeaderAndIsrRequest; Version: 0; CorrelationId: 2; ClientId: ; AckTimeoutMs: 1000 ms; ControllerEpoch: 2; PartitionStateInfo: (test_1,0) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""2,1,3"", ""leader"":""2"", ""leaderEpoch"":""1"" },1),3),(test_2,1) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""2,3"", ""leader"":""2"", ""leaderEpoch"":""2"" },2),3),(test_2,2) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""2,1,3"", ""leader"":""2"", ""leaderEpoch"":""1"" },1),3),(test_2,0) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""2,3"", ""leader"":""2"", ""leaderEpoch"":""2"" },2),3),(test_1,2) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""2,3"", ""leader"":""2"", ""leaderEpoch"":""2"" },2),3),(test_1,1) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""2,1,3"", ""leader"":""2"", ""leaderEpoch"":""1"" },1),3); Leaders: id:2,host:xxxx (kafka.server.ReplicaManager)


5.  A log append happens at offset 582 though the nextOffset for the log is at 570. This append actually pertains to the fetch request at step 1. This explains the gap in the log.

[2013-03-02 12:37:56,098] TRACE [Kafka Log on Broker 3], Appending message set to test_2-2 offset: 582 nextOffset: 570 messageSet: ByteBufferMessageSet(MessageAndOffset(Message(magic = 0, attributes = 0, crc = 1408289663, key = null, payload = java.nio.HeapByteBuffer[pos=0 lim=500 cap=500]),582), MessageAndOffset(Message(magic = 0, attributes = 0, crc = 3696400058, key = null, payload = java.nio.HeapByteBuffer[pos=0 lim=500 cap=500]),583), MessageAndOffset(Message(magic = 0, attributes = 0, crc = 2403920749, key = null, payload = java.nio.HeapByteBuffer[pos=0 lim=500 cap=500]),584), ) (kafka.log.Log)

From the set of steps above, it is clear that some thing is causing the fetch request at step 1 to complete even though step 2 and 3 removed the fetcher for that topic,partition.

Looking at the code now it becomes obvious. The race condition is between the thread that removes the fetcher, truncates the log and adds the fetcher back and the thread that fetches bytes from the leader. Follow the steps below to understand what is happening.

Partition.Scala

          replicaFetcherManager.removeFetcher(topic, partitionId)           --> step 2 : Removes the topic,partition – offset mapping from partitionMap in AbstractFetcherThread
          // make sure local replica exists
          val localReplica = getOrCreateReplica()
          localReplica.log.get.truncateTo(localReplica.highWatermark)    --> step 3 : Truncates to offset 570
          inSyncReplicas = Set.empty[Replica]
          leaderEpoch = leaderAndIsr.leaderEpoch
          zkVersion = leaderAndIsr.zkVersion
          leaderReplicaIdOpt = Some(newLeaderBrokerId)
          // start fetcher thread to current leader
          replicaFetcherManager.addFetcher(topic, partitionId, localReplica.logEndOffset, leaderBroker)    --> step 4: Sets the new fetcher to fetch from the log end offset which is at 570 at this point

AbstractFetcherThread.Scala

private def processFetchRequest(fetchRequest: FetchRequest) {
    val partitionsWithError = new mutable.HashSet[TopicAndPartition]
    var response: FetchResponse = null
    try {
      trace(""issuing to broker %d of fetch request %s"".format(sourceBroker.id, fetchRequest))
      response = simpleConsumer.fetch(fetchRequest)
    } catch {
      case t =>
        debug(""error in fetch %s"".format(fetchRequest), t)
        if (isRunning.get) {
          partitionMapLock synchronized {
            partitionsWithError ++= partitionMap.keys
          }
        }
    }
    fetcherStats.requestRate.mark()   -->  step 1 : Fetch completes. Fetch request is from offset 582.

    if (response != null) {
      // process fetched data 
      partitionMapLock.lock()     ---> step 5: This is where the fetch request is waiting when the addFetcher in Partition.Scala is executing above
      try {
        response.data.foreach {
          case(topicAndPartition, partitionData) =>
            val (topic, partitionId) = topicAndPartition.asTuple
            val currentOffset = partitionMap.get(topicAndPartition)
            if (currentOffset.isDefined) {
              partitionData.error match {
                case ErrorMapping.NoError =>
                  val messages = partitionData.messages.asInstanceOf[ByteBufferMessageSet]
                  val validBytes = messages.validBytes
                  val newOffset = messages.lastOption match {          -->  step 6: The newOffset is set to 587 and partitionMap is updated
                    case Some(m: MessageAndOffset) => m.nextOffset
                    case None => currentOffset.get
                  }
                  partitionMap.put(topicAndPartition, newOffset)
                  fetcherLagStats.getFetcherLagStats(topic, partitionId).lag = partitionData.hw - newOffset
                  fetcherStats.byteRate.mark(validBytes)
                  // Once we hand off the partition data to the subclass, we can't mess with it any more in this thread
                  processPartitionData(topicAndPartition, currentOffset.get, partitionData)    --> step 7: This appends data to the log with logical offsets from 582 – 587. Note that the offset passed to this method is 570 (currentOffset). Hence all offset validation checks in processPartitionData passes.
                case ErrorMapping.OffsetOutOfRangeCode =>
                  try {
                    val newOffset = handleOffsetOutOfRange(topicAndPartition)
                    partitionMap.put(topicAndPartition, newOffset)
                    warn(""current offset %d for topic %s partition %d out of range; reset offset to %d""
                      .format(currentOffset.get, topic, partitionId, newOffset))
                  } catch {
                    case e =>
                      warn(""error getting offset for %s %d to broker %d"".format(topic, partitionId, sourceBroker.id), e)
                      partitionsWithError += topicAndPartition
                  }
                case _ =>
                  warn(""error for %s %d to broker %d"".format(topic, partitionId, sourceBroker.id),
                    ErrorMapping.exceptionFor(partitionData.error))
                  partitionsWithError += topicAndPartition
              }
            }
        }
      } finally {
        partitionMapLock.unlock()
      }
    };;;","05/Mar/13 06:41;junrao;Thanks for the patch. Committed to 0.8.;;;","05/Mar/13 16:26;nehanarkhede;Yeah, probably ok to skip the message;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in handleOffsetCommitRequest,KAFKA-771,12633731,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mumrah,mumrah,mumrah,23/Feb/13 03:35,17/May/16 14:10,14/Jul/23 05:39,25/Feb/13 02:11,0.10.1.0,,,0.8.1,,,,,,,core,,,0,,,,"If the metadata string is null, we get an NPE on the length check",,mumrah,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/13 03:51;mumrah;0001-KAFKA-771-Add-null-check-to-offset-metadata.patch;https://issues.apache.org/jira/secure/attachment/12570597/0001-KAFKA-771-Add-null-check-to-offset-metadata.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,314226,,,Mon Feb 25 02:11:16 UTC 2013,,,,,,,,,,"0|i1i83j:",314571,,,,,,,,,,,,,,,,,,,,"23/Feb/13 03:55;mumrah;Isn't there a more ""Scala-y"" way to do this sort of thing? Perhaps if ApiUtils#readShortString returned Option(String) instead of just String? OffsetMetadataAndError would need to do the same thing. This would be a large refactoring due to the ubiquity of readShortString;;;","24/Feb/13 20:27;nehanarkhede;Is this a patch on 0.8 since it doesn't apply cleanly, please can you rebase ?;;;","24/Feb/13 23:36;mumrah;Sorry, my mistake. Should be applied to trunk (the new commit/fetch offset APIs are not on 0.8);;;","25/Feb/13 02:11;nehanarkhede;Ah, that was my bad. +1 on your patch, looks good.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaConfig properties should be verified in the constructor,KAFKA-770,12633561,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,swapnilghike,swapnilghike,22/Feb/13 07:15,22/Feb/13 16:15,14/Jul/23 05:39,22/Feb/13 16:15,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,,,"To maintain consistency with ProducerConfig and ConsumerConfig, KafkaConfig properties should be verified in the constructor.",,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/13 07:16;swapnilghike;kafka-770.patch;https://issues.apache.org/jira/secure/attachment/12570434/kafka-770.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,314056,,,Fri Feb 22 15:35:07 UTC 2013,,,,,,,,,,"0|i1i71r:",314401,,,,,,,,,,,,,,,,,,,,"22/Feb/13 07:16;swapnilghike;Attached a patch.;;;","22/Feb/13 15:35;nehanarkhede;Thanks for the patch, +1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"On startup, a brokers highwatermark for every topic partition gets reset to zero",KAFKA-769,12633528,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriramsub,sriramsub,sriramsub,22/Feb/13 01:32,22/Feb/13 23:26,14/Jul/23 05:39,22/Feb/13 23:26,0.8.0,,,0.8.0,,,,,,,,,,0,p1,,,"There is a race condition between the highwatermark thread and the handleLeaderAndIsrRequest call of the request handler thread. When a broker starts, the highwatermark thread tries to persist all the checkpoints of the partitions in ReplicaManager. This partition map in ReplicaManager is initially empty. When the leaderAndIsrRequest runs, it updates each partition and if the highwatermark thread runs during this interval, it is essentially going to overwrite the highwatermark file to an inconsistent state. The read of the highwatermark reads from the file each time and hence would return the inconsistent state.",,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/13 20:07;sriramsub;KAFKA-769-v1.patch;https://issues.apache.org/jira/secure/attachment/12570525/KAFKA-769-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,314023,,,Fri Feb 22 23:26:31 UTC 2013,,,,,,,,,,"0|i1i6uf:",314368,,,,,,,,,,,,,,,,,,,,"22/Feb/13 01:49;nehanarkhede;I will take a look at this.;;;","22/Feb/13 01:54;sriramsub;I have a fix for this. Do you still want to take a look?;;;","22/Feb/13 01:56;nehanarkhede;Sure, go ahead and attach a patch, if you already have it ready :);;;","22/Feb/13 03:50;sriramsub;Assigning this back based on the email thread;;;","22/Feb/13 18:35;nehanarkhede;An easy way of resolving this would be to start the highwatermark thread only after the first leader and isr request is completed on a newly restarted broker. This is easy to keep track of since the initial leader and isr request has a special init flag turned on. This will ensure that there is no inconsistent state checkpointed to disk since we will wait until the replica manager has finished initializing the highwatermark for all its replicas from disk. Also, this logic will become trickier when we add the features to change the number of replicas online or change the number of partitions online, but we don't have to worry about that right now.;;;","22/Feb/13 20:07;sriramsub;The patch ensures that highwatermark thread is initialized only after the first leaderIsrRequest batch.;;;","22/Feb/13 20:07;sriramsub;.../main/scala/kafka/server/ReplicaManager.scala   |    9 +++++++--
 1 files changed, 7 insertions(+), 2 deletions(-)
;;;","22/Feb/13 23:24;nehanarkhede;+1;;;","22/Feb/13 23:26;nehanarkhede;Checked in patch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
broker should exit if hitting exceptions durin startup,KAFKA-768,12633469,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,junrao,junrao,21/Feb/13 21:01,14/Jan/15 18:48,14/Jul/23 05:39,22/Feb/13 05:49,0.8.0,,,0.8.0,,,,,,,core,,,0,p1,,,"A broker hit the following exception, but didn't exit.

2013/02/20 01:54:21.341 FATAL [KafkaServerStartable] [main] [kafka] []  Fatal error during KafkaServerStable startup. Prepare to shutdown
kafka.common.KafkaException: Failed to create data directory /export/content/kafka/i001_caches
        at kafka.log.LogManager$$anonfun$createAndValidateLogDirs$2.apply(LogManager.scala:77)
        at kafka.log.LogManager$$anonfun$createAndValidateLogDirs$2.apply(LogManager.scala:72)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32)
        at kafka.log.LogManager.createAndValidateLogDirs(LogManager.scala:72)
        at kafka.log.LogManager.<init>(LogManager.scala:60)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:59)
        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:34)
",,guozhang,junrao,nehanarkhede,sriramsub,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/13 21:09;junrao;kafka-768.patch;https://issues.apache.org/jira/secure/attachment/12570354/kafka-768.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,313964,,,Wed Jan 14 18:48:44 UTC 2015,,,,,,,,,,"0|i1i6hb:",314309,,,,,,,,,,,,,,,,,,,,"21/Feb/13 21:09;junrao;Attach a patch.;;;","21/Feb/13 21:13;sriramsub;+1;;;","21/Feb/13 21:55;swapnilghike;+1;;;","21/Feb/13 21:58;swapnilghike;Also to maintain consistency with Producer/ConsumerConfigs, perhaps we should verify KafkaConfig properties in the secondary constructor instead of having a separate verify method.;;;","21/Feb/13 23:40;nehanarkhede;+1;;;","22/Feb/13 05:49;junrao;Thanks for the review. Committed to 0.8.

Swapnil, 

I agree. Could you file a separate jira to track that?;;;","22/Feb/13 07:16;swapnilghike;Filed KAFKA-770 with a patch.;;;","14/Jan/15 18:48;guozhang;We encountered a general deaklock issue with System.exit(), where a startup / shutdown aware container wrapping KafkaServerStartable that has shutdown hook executed by another thread is blocked waiting for the main thread in startup() to set the startup_finished flag while the main thread is blocked on waiting for the shutdown hook thread to join.

So I would like to propose a bit different solution:

1. Replace System.exit(1) in startup with the shutdown() call.
2. In KafkaServer.shutdown, move

{code}
        brokerState.newState(NotRunning)
        shutdownLatch.countDown()
        startupComplete.set(false)
{code}

to the final block.

3. Remove the System.exit(1) in shutdown() call.

This will resolve this issue but as well works with a startup / shutdown aware container that has a shutdown hook depending on the startup logic to be complete.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Message Size check should be done after assigning the offsets,KAFKA-767,12633276,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriramsub,sriramsub,sriramsub,20/Feb/13 21:15,22/Feb/13 23:03,14/Jul/23 05:39,22/Feb/13 23:03,0.8.0,,,0.8.0,,,,,,,log,,,0,p1,,,"Replica fetcher thread fails with messageSizeTooLarge exception. One theory is that this check is happening before decompress - assign offsets - compress phase. Hence the final compressed size can be different from that obtained from the produce request. This causes replica fetcher thread to be permanently down and prevents the broker from being in sync.

2013/02/20 02:19:25.447 ERROR [ReplicaFetcherThread] [ReplicaFetcherThread-0-274] [kafka] []  [ReplicaFetcherThread-0-274], Error due to
kafka.common.MessageSizeTooLargeException: Message size is 1000028 bytes which exceeds the maximum configured message size of 1000000.
        at kafka.log.Log$$anonfun$analyzeAndValidateMessageSet$1.apply(Log.scala:353)
        at kafka.log.Log$$anonfun$analyzeAndValidateMessageSet$1.apply(Log.scala:339)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:32)
        at kafka.log.Log.analyzeAndValidateMessageSet(Log.scala:339)
        at kafka.log.Log.append(Log.scala:262)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:52)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$4.apply(AbstractFetcherThread.scala:130)
        at kafka.server.AbstractFetcherThread$$anonfun$processFetchRequest$4.apply(AbstractFetcherThread.scala:113)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:125)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)
        at kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:113)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:89)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:51)
",,junrao,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/13 07:43;sriramsub;KAFKA-767-v1.patch;https://issues.apache.org/jira/secure/attachment/12570440/KAFKA-767-v1.patch","22/Feb/13 18:45;sriramsub;KAFKA-767-v2.patch;https://issues.apache.org/jira/secure/attachment/12570509/KAFKA-767-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,313772,,,Fri Feb 22 23:03:28 UTC 2013,,,,,,,,,,"0|i1i5an:",314117,,,,,,,,,,,,,,,,,,,,"22/Feb/13 07:44;sriramsub;Changes

core/src/main/scala/kafka/log/Log.scala |   18 +++++++++++-------
 1 file changed, 11 insertions(+), 7 deletions(-);;;","22/Feb/13 15:28;junrao;Thanks for the patch. It looks good. Should we put the size verifying logic in a separate private method?

Also, the patch doesn't apply to 0.8. Could you rebase?;;;","22/Feb/13 15:33;nehanarkhede;Thanks for the patch, it  looks good. Just one comment, can you break long log4j line ?
;;;","22/Feb/13 18:45;sriramsub;Neha - fixed
Jun - I dont see a strong reason to make it a method. It neither seems reusable nor large enough to wrap it in a method.;;;","22/Feb/13 23:03;nehanarkhede;committed v2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Isr shrink/expand check is fragile,KAFKA-766,12633256,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,sriramsub,sriramsub,20/Feb/13 19:05,08/Apr/15 21:06,14/Jul/23 05:39,08/Apr/15 21:06,0.8.0,,,0.9.0.0,,,,,,,,,,1,,,,"Currently the isr check is coupled tightly with the produce batch size. For example, if the producer batch size is 10000 messages and isr check is 4000 messages, we continuously oscillate between shrinking isr and expanding isr every second. This is because a single produce request throws the replica out of the isr. This results in hundreds of calls to ZK (we still dont have multi write). This can be alleviated by making the producer batch size smaller than the isr check size. 

Going forward, we should try to not have this coupling. It is worth investigating if we can make the check more robust under such scenarios. 
",,aauradkar,abraithwaite,diederik,fullung,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,313752,,,Wed Apr 08 21:06:59 UTC 2015,,,,,,,,,,"0|i1i567:",314097,,,,,,,,,,,,,,,,,,,,"08/Apr/15 21:06;aauradkar;Fixed in KAFKA-1546;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corrupted messages in produce request could shutdown the broker,KAFKA-765,12633244,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,junrao,junrao,20/Feb/13 18:16,25/Feb/13 20:13,14/Jul/23 05:39,25/Feb/13 08:14,0.8.0,,,,,,,,,,core,,,0,p1,,,"In kafka.log.append(), we convert all IOException to KafkaStorageException. This will cause the caller to shutdown the broker. However, if there is a corrupted compressed message, validMessages.assignOffsets() in append() could also throw an IOException when decompressing the message. In this case, we shouldn't shut down the broker and should just fail this particular produce request.",,junrao,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/13 02:24;nehanarkhede;KAFKA-765.patch;https://issues.apache.org/jira/secure/attachment/12570704/KAFKA-765.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,313740,,,Mon Feb 25 20:13:09 UTC 2013,,,,,,,,,,"0|i1i53j:",314085,,,,,,,,,,,,,,,,,,,,"20/Feb/13 18:18;junrao;One way to fix this is to only convert IOException from segment.append and maybeFlush to KafkaStorageException.;;;","22/Feb/13 03:52;sriramsub;Assigning to Neha as per mail conversation;;;","25/Feb/13 02:24;nehanarkhede;Fix is to catch the IOException for assignOffsets() and rethrow a KafkaException describing the validation error;;;","25/Feb/13 05:58;junrao;Thanks for the patch. +1.;;;","25/Feb/13 06:19;sriramsub;The change seems to cause a leak. The IOException during decompression now does not shutdown the broker (ByteBufferMessageSet.Scala). If the read below throws, close is never called on the input stream. You would need to wrap the code in try - finally.

val compressed = CompressionFactory(message.compressionCodec, inputStream)
    Stream.continually(compressed.read(intermediateBuffer)).takeWhile(_ > 0).foreach { dataRead =>
      outputStream.write(intermediateBuffer, 0, dataRead)
    }
compressed.close();;;","25/Feb/13 08:14;nehanarkhede;Thanks for the review, checked in after implementing Sriram's suggestion.;;;","25/Feb/13 15:27;junrao;Shouldn't we do the same in the compressing logic too, i.e, always closes output?;;;","25/Feb/13 20:13;nehanarkhede;Yes we should. Checked that in as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
building a version other than 2.8.0 with package breaks scripts when running them,KAFKA-760,12632787,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,charmalloc,charmalloc,18/Feb/13 05:40,08/Feb/15 00:00,14/Jul/23 05:39,08/Feb/15 00:00,,,,,,,,,,,,,,1,,,,target paths in the scripts are not moving along with the sbt build target,,ashokg,charmalloc,erikdw,phargett,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,313283,,,2013-02-18 05:40:42.0,,,,,,,,,,"0|i1i29z:",313628,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commit/FetchOffset APIs should not return versionId,KAFKA-759,12632720,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mumrah,mumrah,mumrah,16/Feb/13 20:22,01/Mar/13 21:54,14/Jul/23 05:39,01/Mar/13 21:54,0.8.0,,,0.8.1,,,,,,,core,,,0,,,,,,mumrah,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/13 20:50;mumrah;0001-KAFKA-759-Remove-versionId-from-OffsetCommitResponse.patch;https://issues.apache.org/jira/secure/attachment/12569686/0001-KAFKA-759-Remove-versionId-from-OffsetCommitResponse.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,313216,,,Fri Mar 01 21:54:30 UTC 2013,,,,,,,,,,"0|i1i1vb:",313562,,,,,,,,,,,,,,,,,,,,"16/Feb/13 20:50;mumrah;Small fix, remove versionId from response APIs and from handle* methods in KafkaAPIs. ;;;","01/Mar/13 21:50;nehanarkhede;+1, looks good;;;","01/Mar/13 21:54;nehanarkhede;Checked into trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
startHighWaterMarksCheckPointThread is never called,KAFKA-758,12632673,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriramsub,sriramsub,sriramsub,15/Feb/13 23:49,22/Feb/13 16:17,14/Jul/23 05:39,18/Feb/13 22:55,0.8.0,,,0.8.0,,,,,,,,,,0,p1,,,"startHighWaterMarksCheckPointThread is never called during startup and hence we only persist the highwater mark file on a clean shutdown. With an unclean shutdown, the highwater marks are not persisted. This causes long recovery time for replicas after unclean shutdowns.",,junrao,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/13 00:16;sriramsub;KAFKA-758.patch;https://issues.apache.org/jira/secure/attachment/12569627/KAFKA-758.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,313169,,,Mon Feb 18 22:55:00 UTC 2013,,,,,,,,,,"0|i1i1kv:",313515,,,,,,,,,,,,,,,,,,,,"16/Feb/13 00:16;sriramsub;Call startHighWaterMarkCheck on startup;;;","18/Feb/13 22:55;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"System Test Hard Failure cases : ""Fatal error during KafkaServerStable startup"" when hard-failed broker is re-started",KAFKA-757,12632021,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,jfung,jfung,12/Feb/13 18:16,08/Nov/13 13:21,14/Jul/23 05:39,13/Feb/13 22:02,,,,0.8.0,,,,,,,,,,0,0.8,replication-testing,,,,jfung,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1112,,,,,,,,,,"13/Feb/13 09:50;swapnilghike;kafka-757-v1.patch;https://issues.apache.org/jira/secure/attachment/12569165/kafka-757-v1.patch","13/Feb/13 10:05;swapnilghike;kafka-757-v2.patch;https://issues.apache.org/jira/secure/attachment/12569167/kafka-757-v2.patch","13/Feb/13 20:17;swapnilghike;kafka-757-v3.patch;https://issues.apache.org/jira/secure/attachment/12569257/kafka-757-v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,312517,,,Wed Feb 13 22:02:39 UTC 2013,,,,,,,,,,"0|i1hxjr:",312863,,,,,,,,,,,,,,,,,,,,"12/Feb/13 18:20;jfung;This is happening in all Hard-failure testcases in System Test: testcase_015[1-9] 

The following error is thrown when a hard-failed broker (kill -9) is re-started:

[2013-02-12 10:07:15,015] INFO Started Kafka CSV metrics reporter with polling period 5 seconds (kafka.metrics.KafkaCSVMetricsReporter)
[2013-02-12 10:07:15,020] INFO [Kafka Server 3], starting (kafka.server.KafkaServer)
[2013-02-12 10:07:15,046] INFO [Log Manager on Broker 3] Loading log 'test_1-2' (kafka.log.LogManager)
[2013-02-12 10:07:15,062] INFO Creating or reloading log segment /tmp/kafka_server_3_logs/test_1-2/00000000000000000000.log (kafka.log.FileMessageSet)
[2013-02-12 10:07:15,066] INFO Loaded index file /tmp/kafka_server_3_logs/test_1-2/00000000000000000000.index with maxEntries = 1310720, maxIndexSize = 10485760, entries = 1310720, lastOffset = 0, file position = 10485760 (kafka.log.OffsetIndex)
[2013-02-12 10:07:15,068] FATAL Fatal error during KafkaServerStable startup. Prepare to shutdown (kafka.server.KafkaServerStartable)
java.lang.IllegalArgumentException: requirement failed: Corrupt index found, index file (/tmp/kafka_server_3_logs/test_1-2/00000000000000000000.index) has non-zero size but last offset is 0.
        at scala.Predef$.require(Predef.scala:145)
        at kafka.log.OffsetIndex.<init>(OffsetIndex.scala:95)
        at kafka.log.LogSegment.<init>(LogSegment.scala:36)
        at kafka.log.Log$$anonfun$loadSegments$2.apply(Log.scala:163)
        at kafka.log.Log$$anonfun$loadSegments$2.apply(Log.scala:147)
        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:827)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:826)
        at kafka.log.Log.loadSegments(Log.scala:147)
        at kafka.log.Log.<init>(Log.scala:125)
        at kafka.log.LogManager$$anonfun$loadLogs$1$$anonfun$apply$3.apply(LogManager.scala:115)
        at kafka.log.LogManager$$anonfun$loadLogs$1$$anonfun$apply$3.apply(LogManager.scala:109)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
        at kafka.log.LogManager$$anonfun$loadLogs$1.apply(LogManager.scala:109)
        at kafka.log.LogManager$$anonfun$loadLogs$1.apply(LogManager.scala:101)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32)
        at kafka.log.LogManager.loadLogs(LogManager.scala:101)
        at kafka.log.LogManager.<init>(LogManager.scala:62)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:59)
        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:34)
        at kafka.Kafka$.main(Kafka.scala:46)
        at kafka.Kafka.main(Kafka.scala)
[2013-02-12 10:07:15,069] INFO [Kafka Server 3], shutting down (kafka.server.KafkaServer)
;;;","13/Feb/13 09:50;swapnilghike;There are two parts to this patch :

A. Move the sanity check to detect corrupt index files from OffsetIndex constructor to Log constructor below the recovery logic. In case of a hard kill, checking for corrupt index files before the last segment has been recovered will fail the require() assertion.

B. The following corner case is possible:
1. A broker rolled a new log segment file and an index file of non-zero size, and got hard killed before any appends to the index file were flushed. 
2. When the broker reboots and tries to load existing log segments, it will encounter this index file that has non-zero size, but has no data. 
3. Since the broker was hard killed, it will enter the recovery logic in Log.loadLogSegments(). 
4. The recovery logic will try to truncate the index file to the base offset of the segment. It will try to find the indexSlotFor(baseOffset). indexForSlot() will return a non- zero value, because the relativeOffset(idx, mid) == relOffset == 0. 
5. This will set the size of index file to a non-zero value (which will be half of its original size which was maxIndexSize * 8). 
6. Thus, the require() check for corrupted index file in Log constructor will not pass since we have #entries == size != 0 && lastOffset == baseOffset. 

The solution is to modify indexSlotFor() such that it returns -1 for non–zero sized index file whose lastOffset is 0 (assuming that setLength() will set empty bytes to 0), so that the index file is truncated to #entries == size == 0. 


Testing done: 
1. Unit tests passed.
2. Change the flush interval and index append interval to really low values. Produce data using console producer (index file will have flushed entries), hard kill the broker, restart the broker. Should see the exception without A. Should pass with A, ctrl+C the broker.
3. Cleanup the kafka-logs directory, don't cleanup the zookeeper. Restart the broker (to create empty log and index files for topics created in 2 above), it will boot up, hard kill it. Restart the broker again, it should fail without B, should boot successfully with B.
;;;","13/Feb/13 10:05;swapnilghike;Sorry, in indexSlotFor() we should check if lastOffset is the same as or less than baseOffset. Attached patch v2.;;;","13/Feb/13 16:23;junrao;Thanks for the patch. Good catch. I think problem B can explain item 2 in KAFKA-750.

I am not so sure about the fix in OffsetIndex though. indexSlotFor() assumes that the index is valid. However, when truncate() is called, the index may not be valid. Instead of changing the assumption in indexSlotFor(), it's probably better to implement truncate() directly without relying on index lookups.;;;","13/Feb/13 20:17;swapnilghike;Yes, your point is valid. Jay also suggested to implement truncate() directly without calling indexSlotFor(). Patch v3 contains the change.;;;","13/Feb/13 22:02;junrao;Thanks for patch v3. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Processor thread blocks due to infinite loop during fetch response send,KAFKA-756,12631817,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriramsub,sriramsub,sriramsub,11/Feb/13 21:37,22/Feb/13 05:31,14/Jul/23 05:39,12/Feb/13 20:31,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,p1,,"This looks to be because of an infinite loop during fetch response send. This happens because we try to send bytes from a log which has been truncated during send. The total size to send is calculated at the beginning of the iteration and it does not take into account the change in log size during send. When send happens, it uses the size calculated at the start and loops continuously hoping to send more data.",,jkreps,junrao,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/13 19:30;sriramsub;KAFKA-756-v3.patch;https://issues.apache.org/jira/secure/attachment/12569047/KAFKA-756-v3.patch","11/Feb/13 23:10;sriramsub;KAFKA-756.patch;https://issues.apache.org/jira/secure/attachment/12568898/KAFKA-756.patch","12/Feb/13 00:37;sriramsub;Kafka-756-v2.patch;https://issues.apache.org/jira/secure/attachment/12568911/Kafka-756-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,312313,,,Tue Feb 12 20:31:58 UTC 2013,,,,,,,,,,"0|i1hwaf:",312659,,,,,,,,,,,,,,,,,,,,"11/Feb/13 23:10;sriramsub;This does two things
- Checks during writeTo in FileMessageSet to ensure the underlying view has not changed. If it has, we throw an exception and SocketServer will close the key
- We do not write the fetch response for all the topics to the channel in a blocking manner. We give a chance to the processor thread to address other waiting requests. This change is to mainly to get feedback.;;;","11/Feb/13 23:38;sriramsub;- Ignore the second change. Will have an updated patch.;;;","12/Feb/13 00:32;sriramsub;writeTo of multiSend is modified to keep writing till we cannot write any more to the socket.;;;","12/Feb/13 01:07;junrao;Thanks for patch v2. Looks good overall. A few minor comments:

1. SocketServer: Could we add a space at the beginning and the end of ""using key""?

2. MultiSend.writeTo(): totalWrite is hard to distinguish from totalWritten. Could we change it to sth like totalWrittenPerCall instead?

3. Should we add some trace level logging in FileMessageSet.writeTo() and MultiSend.writeTo()?;;;","12/Feb/13 04:03;jkreps;I don't understand the intention of the mutlisend change. Can you give a little background...?

The one change seems to remove a while loop, the other adds one...

Also, did you do any testing on this?;;;","12/Feb/13 05:59;sriramsub;There are two parts to this change.

1. The change in FileMessageSet is the actual fix that prevents the infinite loop.
2. The change in MultiSend is subtle and is more of an optimization. A fetchresponse can typically take a while to transmit all the bytes to the fetcher today (depends on the fetch size and number of topics). This blocks the processor thread today and thereby all the responses in that processor queue. We had two possible solutions for this. 
a. A solution I had proposed is to have a configurable maxsize value that would be used by Processor::write. Once the maxsize is reached, the write would return and will try to write the remaining in the next processor thread iteration. This would also enable other requests and responses to be handled that are ready.
b. Jun's proposal is to fill up the socket buffer as much as possible and stop the iteration when it cannot take more data. The patch v2 does that. It essentially continues to write to the buffer till an incomplete write happens. In such a scenario we return to the processor iteration loop and give a chance to process the other requests and responses.

The 2nd change was made to get feedbacks from others on their thoughts. We are testing the change as part of system tests and are running it currently in the test cluster to monitor how it performs.;;;","12/Feb/13 15:43;jkreps;Ah, I get it. I was wondering about the loop. Usually you just do a write and then wait for the event loop to come around again. I think the subtlty here is that an individual send may be much smaller than the socket buffer size so you need to keep writing. Nice.;;;","12/Feb/13 17:37;jkreps;+1

It would be good to understand how you tested this. Can we reproduce the blocking behavior w/o this patch (i.e. add a println and send some stuff with a larger fetch size than socket size). Can we do the same after the patch and validate that it is behaving as we expect...;;;","12/Feb/13 19:30;sriramsub;Added some more tracing;;;","12/Feb/13 20:31;junrao;Thanks for patch v3. Committed to 0.8 with the following minor changes.

1. Added file name in the trace log in FileMessageSet.

2. Rename a local var in Transmission to make it consistent with existing vars.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
standardizing json values stored in ZK,KAFKA-755,12631761,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,junrao,junrao,11/Feb/13 17:48,24/Feb/13 20:18,14/Jul/23 05:39,23/Feb/13 00:44,0.8.0,,,0.8.0,,,,,,,core,,,0,p1,,,"Currently, we have the following paths in ZK that stores non-singleton values.

1. Topic assignment value:
/brokers/topics/topic
{ ""0"": [""0""] }

2. LeaderAndISR info:
/brokers/topics/test/partitions/0/leaderAndISR
{ ""ISR"":""0,1"",""leader"":""0"",""controllerEpoch"":""1"",""leaderEpoch"":""0"" }

3. broker registration:
/brokers/ids/0
192.168.1.148:9092:9999

4. partition reassignment path

It would be good if we do the following:
a. make them true json (e.g., using number as the value for broker/partition, instead of string).
b. add version support for future growth.


",,jkreps,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/13 07:52;swapnilghike;kafka-755-v1.patch;https://issues.apache.org/jira/secure/attachment/12570277/kafka-755-v1.patch","22/Feb/13 07:12;swapnilghike;kafka-755-v2.patch;https://issues.apache.org/jira/secure/attachment/12570433/kafka-755-v2.patch","23/Feb/13 00:09;swapnilghike;kafka-755-v3.patch;https://issues.apache.org/jira/secure/attachment/12570562/kafka-755-v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,312257,,,Sat Feb 23 00:44:39 UTC 2013,,,,,,,,,,"0|i1hvxz:",312603,,,,,,,,,,,,,,,,,,,,"11/Feb/13 20:02;jkreps;Also
c. Think through how any generalization would work (e.g. (1) can't really be extended beyond partitions because the partitions are at the top level of the object.
d. Standardize capitalization: we have dashes, underscores, camel case and pretty much everything else
e: Document them and going forward add to the documentation. This will ensure future additions are thought through holistically with all the other structures.;;;","13/Feb/13 00:31;junrao;Proposed the new format in https://cwiki.apache.org/confluence/display/KAFKA/Kafka+data+structures+in+Zookeeper
;;;","15/Feb/13 18:14;junrao;Updated the wiki. We need to patch value for the following path too.

Consumer registration:

/consumers/[groupId]/ids/[consumerId];;;","21/Feb/13 01:52;nehanarkhede;I updated the wiki with the proposed zookeeper path and format changes for preferred replica election and partition reassignment;;;","21/Feb/13 07:52;swapnilghike;This patch standardizes the reading and writing to zookeeper values according to the JSON schemas 1 thru 8 defined in the aforementioned wiki. 

The changes in this patch involve a bunch of refactoring to make this standardization possible. 

This patch does not touch any other use of JSON-like structures in the code that does not involve interacting with zookeeper.

Testing done:
- Unit tests pass.
- Nuke all the zookeeper data. Start kafka server, produce data using a producer and start a consumer. Observe the zookeeper data.

It would be good if we could check in this 1st patch and let the system test run over it. I will create another patch to make the changes for item 9 and 10 in the wiki as and when my hands are free of other blockers. 9 and 10 will be used only by our tools and are standalone changes in the sense that we can simply delete and recreate the related zookeeper namespaces without disturbing our clients.;;;","22/Feb/13 02:24;junrao;Thanks for the patch. Some comments.

1. Broker.createBroker(): We should catch all json parsing related exceptions and throw a KafkaException with the problematic json string in it.

2. ZkUtils.partitionReplicasZkData: should we rename it to replicaAssignmentZkData?

3. TopicCount: Instead of throwing RuntimeException when hitting json errors, it's probably better to throw KafkaException.

4. Utils:
4.1 I recommend rename putStringValuesInJsonObject() to mapToJson().
4.2 I recommend rename putStringSeqInJsonArray() to arrayToJson() and add a valueAsString flag too.
4.3 recommend rename putIntSeqValuesInJson() to mapWithSeqValuesToJson()

5. ZkUtils:
5.1 registerBrokerInZK(): broker is no longer referenced
5.2 There are 2 getReplicaAssignmentForTopics methods and the following one is not used. Let's remove it.
  def getReplicaAssignmentForTopics(zkClient: ZkClient, topics: Iterator[String]):

6. ZookeeperConsumerConnector: the line calling mergeJsonObjects is too long

7.ZookeeperConsumerConnectorTest: no real change

8. The following map is not sorted by field names.
/consumers/console-consumer-8915/ids/console-consumer-8915_jrao-ld-1361498895428-3268a767
{ ""subscription"":{ ""*test"" : 1 }, ""version"":1, ""pattern"":""white_list"" }
;;;","22/Feb/13 07:12;swapnilghike;Addressed all points.

i. Renamed valueAsString flag to valueInQuotes for clarity.
ii. Json fields should appear in sorted order now.
iii. Changed the whitelist/blacklist detection to use the ""pattern"" field in the zk json. Accordingly modified the createTopicCount and dbString methods.

Testing done -
1. Testing done for v1 patch.
2. Tried all of non-wildcard, whitelist and blacklist topics with console consumer. Zk data looks ok (it does not include * for non-wildcard topics). Also changed the wiki to reflect this, previously it had entries like ""*abc"" and ""!abc"" for wildcard consumer subscription fields.

;;;","22/Feb/13 15:59;junrao;Thanks for patch v2. A few more comments.

20. Broker: Can we add brokerInfoString in the following exception?
throw new KafkaException(""Failed to parse the broker info from zookeeper"", t)

21. TopicCount:
21.1 Can we define ""white_list"" and ""black_list"" as constants and reference only the constants instead of the strings?
21.2 Can we rename specialList to subscriptionPattern?

22. Utils: The comment for mapWithSeqValuesToJson() needs to be changed.

Could you rebase too?

;;;","23/Feb/13 00:09;swapnilghike;Made the changes and rebased. 

writePreferredReplicaElectionData is a bit clunky to read currently, I will take care of it while completing the remaining changes for items 9 adn 10.;;;","23/Feb/13 00:44;junrao;Thanks for patch v3. +1 and committed to 0.8 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable log4j logging in command line tools,KAFKA-754,12631529,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jkreps,jkreps,08/Feb/13 22:26,07/Feb/15 23:24,14/Jul/23 05:39,07/Feb/15 23:24,,,,,,,,,,,,,,0,,,,"The command line tools use the same log4j configuration as the server, so they produce all kinds of logging to standard out. This is annoying. Administrative tools and performance tests should only produce expected output.

We should use a different log4j configuration for these that turns off logging.",,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,312025,,,2013-02-08 22:26:37.0,,,,,,,,,,"0|i1huif:",312371,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix windows build script - kafka-run-class.bat,KAFKA-751,12630946,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,mdevilliers,mdevilliers,05/Feb/13 20:40,14/Feb/13 16:24,14/Jul/23 05:39,14/Feb/13 16:24,0.8.0,,,0.8.0,,,,,,,tools,,,1,patch,,,"I've updated the kafka-run-class.bat to keep it in line (and working) with the kafka-run-class.sh.

I'll attach a diff file to the issue.
",,junrao,mdevilliers,zayeem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/13 20:43;mdevilliers;kafka-run-class.diff;https://issues.apache.org/jira/secure/attachment/12568081/kafka-run-class.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,311442,,,Thu Feb 14 16:24:34 UTC 2013,,,,,,,,,,"0|i1hqx3:",311788,,,,,,,,,,,,,,,,,,,,"14/Feb/13 08:11;zayeem;With the patch, the server starts without any issues.

Previously, it runs into kafka.Kafka main class issues due to the non-availability of jars in the classpath.;;;","14/Feb/13 16:24;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in socket server shutdown logic makes the broker hang on shutdown until it has to be killed,KAFKA-749,12630779,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,04/Feb/13 23:14,06/Feb/13 04:11,14/Jul/23 05:39,06/Feb/13 04:11,0.8.0,,,,,,,,,,network,,,0,bugs,p1,,"The current shutdown logic of the server shuts down the io threads first, followed by acceptor and finally processor threads. The shutdown API of io threads enqueues a special AllDone command into the common request queue. It shuts down the io thread when it dequeues this special all done command. What can happen is that while this shutdown command processing is happening on the io threads, the network/processor threads can still accept new connections and requests and will add those new requests to the request queue. That means, more requests can be enqueued after the AllDone command. What happens is that after the io threads have shutdown, there is no thread available to dequeue from the request queue. So the processor threads can hang while adding new requests to a full request queue, thereby blocking the server from shutting down.",,jkreps,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/13 01:09;nehanarkhede;kafka-749-v1.patch;https://issues.apache.org/jira/secure/attachment/12567920/kafka-749-v1.patch","05/Feb/13 18:02;nehanarkhede;kafka-749-v2.patch;https://issues.apache.org/jira/secure/attachment/12568043/kafka-749-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,311275,,,Wed Feb 06 04:11:24 UTC 2013,,,,,,,,,,"0|i1hpvz:",311621,,,,,,,,,,,,,,,,,,,,"05/Feb/13 01:09;nehanarkhede;Bug fix includes the following changes -

1. KafkaRequestHandler
- Removed AllDone, instead the shutdown() command will set isRunning to false and wait for the request handler thread to finish processing existing request and then stop

2. RequestChannel
- Modified receiveRequest to wait on a condition variable if the queue is empty. The purpose is to introduce a clean way to wake it up when it is time to shutdown. 
- Added a close() API that will set isShuttingDown to true and signal the condition so all io threads waiting to receiveRequest() will return null
- Did not clear the queue since the io threads shutdown after the socket server. If we clear the queue, all io threads will try to get the next request and will get null until it shuts down. This time period should hopefully be very short, but it is still inefficient. This is ok for the io thread shutdown logic since it will just process one request before it shuts down as well. So its ok to not clear the queue. 

3. SocketServer
- Invoke close on request channel after the acceptor and processor threads are shutdown

4. KafkaServer
- Shutdown the socket server before the request handler. This ensures we don't accept and enqueue more requests that will timeout anyway.;;;","05/Feb/13 05:43;sriramsub;Took a look at this. 

1. This adds another layer of locking to the Request queue which in itself is a Blocking queue implementation. This does not seem very efficient. Either we implement our blocking queue or use the api.
2. Having said that, it seems a lot easier to just purge the queue. Is this not an option? 
3. If 2 is not possible, a more elegant way is to use the poison pill approach. We shutdown the socket server, and enqueue a shutdown request to the queue. Each request handler thread dequeues the request, checks it is is shutdown request, if so re-queues it and exits.;;;","05/Feb/13 05:56;jkreps;The ugly part here is the extra layer of synchronization and signally around the already synchronized blocking queue. This code is a bit hard to validate (for example shouldn't it be signal instead of signalAll--since only one thing was added?) so it tends to quickly get broken by later people who don't understand it.

I think I don't quite understand why we can't just call clear on the queue and enqueue the AllDone object to achieve this. The uglinesses of the previous implementation where that AllDone actually came out of the RequestChannel and that it was a ProducerRequest. This is easily fixed. There is no reason it should be a Producer request, and the check for eq AllDone can be done in receiveRequest.;;;","05/Feb/13 06:02;nehanarkhede;The only problem with the AllDone approach is that we need to ensure that the request queue size is atleast as big as the number of io threads we have. If it is less than that, we have the same problem where shutdown will hang.;;;","05/Feb/13 14:46;jkreps;So what if we just do

while(true) {
  queue.clear()
  queue.offer(AllDone)
  return
};;;","05/Feb/13 14:46;jkreps;Err, should be

while(true) {
  queue.clear()
  if(queue.offer(AllDone))
    return
};;;","05/Feb/13 16:21;nehanarkhede;Is this the logic for the shutdown() API in KafkaRequestHandler ? If yes, then I don't think we can keep clearing the queue in a loop. What can happen is that AllDone requests for the rest of the request handlers might still be in the queue. If we clear those, then request handlers won't shutdown and some of the request handlers will just busy wait trying to clear and offer AllDone in a loop.

I'm thinking an easier solution might be to just require the request queue size as large as the number of io threads. This makes sense so that io threads are never underutilized and there is space for one request per io thread in the request queue. Thoughts ?;;;","05/Feb/13 17:48;jkreps;That makes sense, good solution.;;;","05/Feb/13 17:56;sriramsub;If that works fine. But it looks like there really is a solution that does not need any constraints. 

1. SocketServer on shutdown closes the acceptor and processor threads. It can then add a alldone request to the queue.
2. Each requesthandler thread just dequeues from the request queue. If it is alldone, it just re-enqueues it and exits.
3. requesthandler shutdown just waits for all threads to exit (this is same as today).

Will this not work? ;;;","05/Feb/13 18:02;nehanarkhede;Thanks for the review ! I think I was over thinking the issue of request queue size having to be larger than io threads. Even if  it is smaller, some io thread's shutdown will wait for some space to free up. Space will free up since some other io thread will dequeue the AllDone command. 

This patch is very simple. It changed the shutdown logic of the Kafka server to go through following steps -
1. Shutdown acceptor, so no new connections are accepted
2. Shutdown processor threads, they will enqueue the currently selected keys' requests in the request queue. This is fine since io threads are alive and will dequeue requests. So this step will not block
3. Request channel shutdown will clear the queue. At this time, no thread is enqueuing more data. IO threads trying to dequeue data will hang on the receiveRequest
4. Shutdown io threads, this will enqueue AllDone command in the queue. And all io threads will shutdown one after the other. Even if the request queue is smaller than # of io threads, it will eventually shutdown

;;;","05/Feb/13 18:07;nehanarkhede;Sriram, That is what I had described in my simpler solution and is what patch v2 does.;;;","05/Feb/13 18:12;sriramsub;Awesome. looks good to me. Nit - are we planning to fix getShutdownReceive to not use produceRequest as part of this patch or is that a separate jira?;;;","05/Feb/13 18:19;nehanarkhede;KAFKA-745 is filed for that. I prefer to not touch it here, it is a much bigger change;;;","05/Feb/13 18:52;jkreps;Why is that a big change?;;;","05/Feb/13 18:55;jkreps;Also, not sure if I get it. If the I/O threads are able to make progress and the response queue is unlimited in size then shouldn't requestQueue.put always succeed eventually? Even if the queue is currently full some requests will get processed and free up some space...?;;;","05/Feb/13 19:19;nehanarkhede;That is correct. Also, removing the getShutdownReceive() is part of the refactoring in KAFKA-745. This patch includes the bug fix itself, I would prefer not to extend the scope of this bug fix. ;;;","06/Feb/13 04:11;nehanarkhede;Thanks for the reviews, checked in patch v2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Append to index fails due to invalid offset,KAFKA-748,12630726,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jkreps,sriramsub,sriramsub,04/Feb/13 19:14,22/Feb/13 05:32,14/Jul/23 05:39,08/Feb/13 18:58,0.8.0,,,0.8.0,,,,,,,,,,0,p1,,,"We seem to be appending to the index and it checks to make sure we do not insert an entry with an offset that is larger than the actual offset of the message. We seem to be trying to insert an offset = 1 in the index while lastOffset is 24463. This seems to get fixed on restarting the broker.

java.lang.IllegalArgumentException: Attempt to append an offset (1) to position 21703 no larger than the last offset appended (24463).
at kafka.log.OffsetIndex.append(OffsetIndex.scala:183)
at kafka.log.LogSegment.append(LogSegment.scala:60)
at kafka.log.Log.append(Log.scala:286)
at kafka.server.KafkaApis$$anonfun$appendToLocalLog$2.apply(KafkaApis.scala:188)
at kafka.server.KafkaApis$$anonfun$appendToLocalLog$2.apply(KafkaApis.scala:181)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)",,jkreps,junrao,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/13 00:15;jkreps;KAFKA-748-v1.patch;https://issues.apache.org/jira/secure/attachment/12568498/KAFKA-748-v1.patch","04/Feb/13 23:29;sriramsub;outindex;https://issues.apache.org/jira/secure/attachment/12567905/outindex","04/Feb/13 23:29;sriramsub;outmsg;https://issues.apache.org/jira/secure/attachment/12567906/outmsg",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,311222,,,Fri Feb 08 18:58:33 UTC 2013,,,,,,,,,,"0|i1hpk7:",311568,,,,,,,,,,,,,,,,,,,,"04/Feb/13 23:29;sriramsub;The last few lines in the index file are below. Looks like the offsets are not monotonically increasing. Note that these are the last lines that the DumpLogSegment tool could dump since it fails due to the mismatch. This still does not explain why the append is happening to offset 1. Adding some logging on startup, append and truncate code paths to ensure sanity and shutting down if there is inconsistency should help to get more info.


offset: 24878 position: 278006637
offset: 24879 position: 278012656
offset: 24880 position: 278050159
offset: 24313 position: 249363783
;;;","07/Feb/13 22:54;jkreps;This is actually very easy to reproduce. Just do a clean shutdown on the broker while running the producer perf test. Haven't debugged it yet.;;;","07/Feb/13 23:50;jkreps;Okay the problem here is the resize method. We generalized index.trimInvalid to index.resize to be able to enlarge the index when we load a segment. This was done to avoid rolling the log on a full index, I think, which had other problems with empty indexes. However it looks like this never actually was tried, because doing this resets the position in the mmap to 0, so we start overwriting the entries in the index from the beginning but expanding what we think the valid segment of the memory map is. When we close we then have a bunch of zeros at the end of the index and have overwritten the front of the file.;;;","08/Feb/13 00:15;jkreps;Attached is a patch that retains the index position on resize. It also includes a few useful logging and assert statements.;;;","08/Feb/13 18:12;junrao;Thanks for the patch. It looks good. So +1. I suggest that we add one more sanity check after loading all logs during startup: make sure that the position pointed to by the last index entry is less than FileMessageSet.sizeInBytes(). We can probably just do the check for the active log segment.

Also, do you think this can cause KAFKA-753 too?;;;","08/Feb/13 18:58;jkreps;Checked in.

Jun, I think we effectively do the check you describe already because we run translateOffset on the last index entry in order to initialize the log end offset.

I am so sure that this can cause the negative position issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Snappy compression isn't applied to messages written to disk on broker,KAFKA-746,12630154,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,junrao,parantumaton,parantumaton,31/Jan/13 15:21,05/Dec/16 12:52,14/Jul/23 05:39,05/Dec/16 12:52,0.7.2,,,,,,,,,,producer ,,,0,,,,"We recently tested 0.7.2 performance using different compression codecs for messages and noticed that when using Snappy (compression.codec = 2 in Producer configuration) the messages written to topic partitions were written without compression. Compared to GZIP (compression.codec = 1) this is especially peculiar since with GZIP the partition files themselves were compressed as expected.

I did walk through the 0.8 code briefly and didn't spot any obvious reasons why this would be happening so I'm now handing this issue completely into your able hands.",,ijuma,parantumaton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,310650,,,Mon Dec 05 12:52:50 UTC 2016,,,,,,,,,,"0|i1hm13:",310995,,,,,,,,,,,,,,,,,,,,"05/Dec/16 12:52;ijuma;I am going to mark this as fixed as there have been no reports of this for several years.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove getShutdownReceive() and other kafka specific code from the RequestChannel,KAFKA-745,12630021,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,sriramsub,nehanarkhede,nehanarkhede,30/Jan/13 22:54,31/Aug/17 16:58,14/Jul/23 05:39,31/Aug/17 16:58,0.8.0,,,,,,,,,,network,,,0,,,,Jay's suggestion from KAFKA-736 is to get rid of getShutdownReceive() and kafka request specific code from the generic requestchannel,,nehanarkhede,omkreddy,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/13 11:10;sriramsub;KAFKA-745-v1.patch;https://issues.apache.org/jira/secure/attachment/12568209/KAFKA-745-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,310517,,,Thu Aug 31 16:58:49 UTC 2017,,,,,,,,,,"0|i1hl7r:",310862,,,,,,,,,,,,,,,,,,,,"31/Jan/13 16:28;sriramsub;This bug also tracks decoupling requestObj from RequestChannel if possible.;;;","05/Feb/13 20:58;sriramsub;I would like to take this and complete it. Let me know if you want to work on it and I will reassign.;;;","06/Feb/13 06:56;sriramsub;Took a look at this. The main issue is that we seem to be tracking a bunch of metrics per request type and they all end up being tracked in the network layer. This has really made the network layer ugly. These metrics are really of two types Network level metrics and Request level metrics. 

1. Metrics that can be tracked at the network level are those that do not have any large difference based on the request type. For example, queueTime is a metric that is supposed to be the amount of time a request spends in the request queue. Tracking this for each request type does not really make sense. This is a network level property and should be tracked at that level.

2. Metrics that can be tracked at the request level have a large difference based on the request type. For example, local time is the amount of time the KafkaApi handle method takes to complete. This largely depends on the request type and should be tracked at the request level.

To summarize, with the decoupling specified above we have the following metrics at the two levels

Network metrics  
-----------------------
 // time a request spent in a request queue
  val queueTimeHist = newHistogram(name + ""-QueueTimeMs"")

// time to send the response to the requester from the response queue
  val responseSendTimeHist = newHistogram(name + ""-ResponseSendTimeMs"")

// total time taken for the request to be served 
  val totalTimeHist = newHistogram(name + ""-TotalTimeMs"")
 
Request metrics
------------------------

  // request rate by type
  val requestRate = newMeter(name + ""-RequestsPerSec"",  ""requests"", TimeUnit.SECONDS)   

  // time a request takes to be processed at the local broker
  val localTimeHist = newHistogram(name + ""-LocalTimeMs"")

  // time a request takes to wait on remote brokers (only relevant to fetch and produce requests)
  val remoteTimeHist = newHistogram(name + ""-RemoteTimeMs"")
   
With the separation specified above, any request can be defined as

queueTime + localTime + remoteTime + responseSendTime = totalTime

We can totally remove any kafkaapi dependency in the network layer with the proposed separation.;;;","06/Feb/13 11:10;sriramsub;This is a first version.

1. Removes all the metric stuff,requestObj etc from Request.
2.Adds Network metrics to track network level parameters
3. Any form of delayed request now uses a trimmed version of the Request object. 
4. Added Request metrics to the kafka layer to track kafka specific parameters

TODO - Need to add logging.
          - Some more cleanup

- We will not get all the different times for a given request as part of one log statement anymore. I think this is fine for the ability to maintain clean code.
- W.r.t the logging in the network layer, we would get info from the Request object (need to add a toString here to ignore the buffer) and nothing from the actual request. I think this is fine since knowing the request type is not useful at this layer based on the explanation above.;;;","31/Aug/17 16:58;omkreddy;These changes were done in newer Kafka versions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PreferredReplicaLeaderElectionCommand has command line error,KAFKA-743,12629926,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,junrao,junrao,30/Jan/13 16:14,19/Feb/13 18:07,14/Jul/23 05:39,12/Feb/13 15:53,0.8.0,,,0.8.0,,,,,,,tools,,,0,,,,"bin/kafka-preferred-replica-election.sh 
Exception in thread ""main"" joptsimple.IllegalOptionSpecificationException: ' ' is not a legal option character
	at joptsimple.ParserRules.ensureLegalOptionCharacter(ParserRules.java:81)
	at joptsimple.ParserRules.ensureLegalOption(ParserRules.java:71)
	at joptsimple.ParserRules.ensureLegalOptions(ParserRules.java:76)
	at joptsimple.OptionParser.acceptsAll(OptionParser.java:309)
	at joptsimple.OptionParser.accepts(OptionParser.java:271)
	at kafka.admin.PreferredReplicaLeaderElectionCommand$.main(PreferredReplicaLeaderElectionCommand.scala:29)
	at kafka.admin.PreferredReplicaLeaderElectionCommand.main(PreferredReplicaLeaderElectionCommand.scala)
",,brugidou,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/13 16:17;junrao;kafka-743.patch;https://issues.apache.org/jira/secure/attachment/12567152/kafka-743.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,310422,,,Tue Feb 12 15:53:30 UTC 2013,,,,,,,,,,"0|i1hkmn:",310767,,,,,,,,,,,,,,,,,,,,"30/Jan/13 16:17;junrao;Attach a fix.;;;","30/Jan/13 16:19;brugidou;Same thing for CheckReassignmentStatus and ReassignPartitionsCommand;;;","12/Feb/13 15:53;junrao;Thanks for the review. Committed to 0.8 after fixing the same problem in the other 2 command line tools.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Existing directories under the Kafka data directory without any data cause process to not start,KAFKA-742,12629756,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,singhashish,chriscurtin,chriscurtin,29/Jan/13 19:27,20/May/16 21:23,14/Jul/23 05:39,17/Dec/14 04:34,0.8.0,,,0.9.0.0,,,,,,,config,,,0,,,,"I incorrectly setup the configuration file to have the metrics go to /var/kafka/metrics while the logs were in /var/kafka. On startup I received the following error then the daemon exited:

30   [main] INFO  kafka.log.LogManager  - [Log Manager on Broker 0] Loading log 'metrics'
32   [main] FATAL kafka.server.KafkaServerStartable  - Fatal error during KafkaServerStable startup. Prepare to shutdown
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at java.lang.String.substring(String.java:1937)
        at kafka.log.LogManager.kafka$log$LogManager$$parseTopicPartitionName(LogManager.scala:335)
        at kafka.log.LogManager$$anonfun$loadLogs$1$$anonfun$apply$3.apply(LogManager.scala:112)
        at kafka.log.LogManager$$anonfun$loadLogs$1$$anonfun$apply$3.apply(LogManager.scala:109)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
        at kafka.log.LogManager$$anonfun$loadLogs$1.apply(LogManager.scala:109)
        at kafka.log.LogManager$$anonfun$loadLogs$1.apply(LogManager.scala:101)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32)
        at kafka.log.LogManager.loadLogs(LogManager.scala:101)
        at kafka.log.LogManager.<init>(LogManager.scala:62)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:59)
        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:34)
        at kafka.Kafka$.main(Kafka.scala:46)
        at kafka.Kafka.main(Kafka.scala)
34   [main] INFO  kafka.server.KafkaServer  - [Kafka Server 0], shutting down

This was on a brand new cluster so no data or metrics logs existed yet.

Moving the metrics to their own directory (not a child of the logs) allowed the daemon to start.

Took a few minutes to figure out what was wrong.",,chriscurtin,dbtucker,gwenshap,jkreps,nehanarkhede,noslowerdna,qwertymaniac,singhashish,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/14 18:29;singhashish;KAFKA-742.1.patch;https://issues.apache.org/jira/secure/attachment/12687528/KAFKA-742.1.patch","14/Dec/14 19:36;singhashish;KAFKA-742.patch;https://issues.apache.org/jira/secure/attachment/12687132/KAFKA-742.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,310252,,,Fri May 20 21:23:00 UTC 2016,,,,,,,,,,"0|i1hjkv:",310597,,,,,,,,,,,,,,,,,,,,"29/Jan/13 19:34;jkreps;Getting an error is not really bad since we assume we have control of everything under the log directory (it is obviously hard for us to distinguish a log directory from a non-log directory and silently ignoring could be worse than an error). So I think there are two problems here:
1. Are we enabling metrics logging by default? Are we creating the metrics dir even if metrics logging is not enabled. This needs to be sanity checked...
2. If there are bogus directories under the log directory I think the right thing to do is to give a better error message (something like ""Found directory /x/y/z, 'z' is not in the form topic-partition"").

 If you agree with those fixes I will take this on.;;;","29/Jan/13 19:43;chriscurtin;Thanks Jay, a better error is a good idea for this. ;;;","02/Dec/14 00:11;singhashish;[~jkreps], [~chriscurtin], I would like to take a stab at this. Assigning it to myself.;;;","11/Dec/14 19:44;singhashish;[~jkreps] Now that I actually started to work on this. I re-read your comment above and realized you mentioned that you intend to work on this. I missed the line when I assigned the JIRA to myself. My apologies for the same. Kindly feel free to take it on and assign it to yourself. However, if you are not planning to work on this, then let me know and then I can work on this. My apologies for the confusion.;;;","11/Dec/14 22:10;jkreps;No, I said that but then never did any work. Definitely take it!;;;","12/Dec/14 00:56;singhashish;[~jkreps] ok, I am on it then :);;;","14/Dec/14 19:33;singhashish;Created RB: https://reviews.apache.org/r/29030/

bq. Are we enabling metrics logging by default? Are we creating the metrics dir even if metrics logging is not enabled. This needs to be sanity checked...

metrics dir is created by a metrics reporter during its init, which is called by {{KafkaMetricsReporter}} only if the reported is registered.

bq. If there are bogus directories under the log directory I think the right thing to do is to give a better error message (something like ""Found directory /x/y/z, 'z' is not in the form topic-partition"")

{{parseTopicPartitionName}} only gets log directory name and not the path. I can make changes for it to receive directory path rather than just the name. However, I think just specifying that dir ""some_dir"" is not in the form topic-partition should be good enough. Let me know if you guys think otherwise. Other possible solution is I can catch the exception thrown by {{parseTopicPartitionName}} in the caller, which has the path for dir, and throw exception with full path of the dir.;;;","15/Dec/14 17:40;gwenshap;I think we need to print the full path to the directory we can't parse - I've met at least two customers who installed Kafka through a distribution and had no idea where their Kafka log directory is.

Also, I'd add something to the error message along lines of ""If a directory does not contain Kafka topic data it should not exist in Kafka's log directory. Please move it elsewhere.""

;;;","16/Dec/14 18:29;singhashish;Makes sense. Updated patch.;;;","17/Dec/14 04:34;nehanarkhede;Thanks for the patch, [~singhashish]. Pushed to trunk;;;","30/Dec/14 20:08;noslowerdna;Just a side note, we've seen this problem on Redhat Linux when the OS creates a special directory named ""lost+found"" in a mounted filesystem.;;;","20/May/16 21:23;dbtucker;And the ""lost+found"" issue still exists in Kafka 0.9 even when a non-root user is given ownership of the mount point.   There is a workaround : create a single sub-directory and point to that.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle null values in Message payload,KAFKA-739,12629629,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,29/Jan/13 03:14,03/Jul/13 04:02,14/Jul/23 05:39,03/Jul/13 04:02,,,,0.8.1,,,,,,,,,,0,,,,"Add tests for null message payloads in producer, server, and consumer.
Ensure log cleaner treats these as deletes.
Test that null keys are rejected on dedupe logs.",,jkreps,junrao,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-741,,,,,,,,,,,,,,,,,,,,,"04/Mar/13 22:55;jkreps;KAFKA-739-v1.patch;https://issues.apache.org/jira/secure/attachment/12571977/KAFKA-739-v1.patch","08/Mar/13 19:21;jkreps;KAFKA-739-v2.patch;https://issues.apache.org/jira/secure/attachment/12572801/KAFKA-739-v2.patch","08/Mar/13 23:31;jkreps;KAFKA-739-v3.patch;https://issues.apache.org/jira/secure/attachment/12572856/KAFKA-739-v3.patch","09/Mar/13 04:44;jkreps;KAFKA-739-v4.patch;https://issues.apache.org/jira/secure/attachment/12572892/KAFKA-739-v4.patch","12/Mar/13 16:46;jkreps;KAFKA-739-v5.patch;https://issues.apache.org/jira/secure/attachment/12573365/KAFKA-739-v5.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,310125,,,Tue Mar 12 17:38:58 UTC 2013,,,,,,,,,,"0|i1hisn:",310470,,,,,,,,,,,,,,,,,,,,"04/Mar/13 22:55;jkreps;This patch is more extensive than I expected because I found a hole in the logic when handling deletes in the log compactor. The changes are as follows:

1. Handle null properly in Message.scala and miscellaneous other places.
2. Fix the logic for handling deletes. Previously we guaranteed that we would retain delete records only in the dirty section of the log. This is not sufficient, because a bootstrapping consumer might see a message, but the subsequent delete message might be gc'd before the consumer sees it.
3. OffsetMap.scala: make the map exact using a probing scheme. This means that the tail of the log is actually now fully deduplicated. The motivation for this is making delete-handling easier since to remove a delete tombstone you need to ensure that there are no prior occurrences of that message. Also added a counter on the number of collisions, just to help with any debugging.
4. Added a new configuration log.cleaner.delete.retention.ms that controls the length of time for which delete records are retained. This is implicitly a limit on the amount of time the consumer can spend bootstrapping and still get a consistent bootstrap. Once the topic-level config patch goes in, this will be made available at the topic level and can be set with the create topic tool
5. Added a peek() method to iterator template. Didn't end up using it, but it is a useful feature
6. Changed the integration test tool to issue deletes and changed the verification to handle delete records properly. Redid testing now with deletes included.
7. Added a variety of unit tests for null messages
;;;","05/Mar/13 06:37;sriramsub;I am not able to apply this patch using git apply. Was this created using git format-patch?;;;","05/Mar/13 16:09;jkreps;No, actually it is just git diff against the base revision.;;;","06/Mar/13 19:26;nehanarkhede;There was a conflict on DefaultEventHandler, but I reviewed the patch. 

1. KafkaConfig
Should the default for log.cleaner.delete.retention.ms be 24 hours instead of 1 hour ?

2. LogCleaner
2.1 Should the check for dedup buffer be 
config.dedupeBufferSize / config.numThreads > Int.MaxValue

3. DefaultEventHandler (There was a conflict, maybe you already handled this)
Need to check for null payload in the following trace- 
              trace(""Successfully sent message: %s"".format(Utils.readString(message.message.payload)))))

4. DumpLogSegments
Should this be reading message.key instead ?
          print("" key: "" + Utils.readString(messageAndOffset.message.payload, ""UTF-8""))

5. SimpleKafkaETLMapper
Should probably check for null here in getData well -
                ByteBuffer buf = message.payload();

6. OffsetMap6.1 If I understand correctly from getPosition(), it seems that the probe length will change arbitrarily each time. What is the advantage of doing this VS picking a fixed probe length that is relatively prime to the total number of entries that the hash table can fit in ? The purpose of this property is so that every slot in the hash table can be eventually traversed.
6.2 Why does attempts increment by 1 and not by 4 ?

7. TestLogCleaning
The purpose of dumpLogs config is not clear from the command line option description.;;;","08/Mar/13 19:21;jkreps;New patch rebased to trunk and addresses Neha's comments:

1. Changed delete retention to 24 hours
2. Fixed broken logic in warning statement so it warns when your buffer is too big.
3. Yes, that was in the patch, just got lost in the conflict?
4. Dump log segments was printing the value as the key, fixed.
5. SimpleKafkaETLMapper didn't handle null. This isn't an easy fix since the text format doesn't have an out of range marker to represent null. Returning empty string which is ambiguous but better than crashing.
6. Linear probing has the problem that it tends to lead to ""runs"". I.e. if you have a fixed probing step size of N then if you have a collision the probability that the spot M slots over is full is going to be higher. So the ideal probing approach would be a sequence of fully random hashes which were completely uncorrelated with one another. That is the motivation for using the rest of the md5 before degrading to linear probing since we have already computed 16 bytes of random hash. The second question is wether it is legit to increment byte by byte or not since this effectively reuses bytes of the hash. I agree it is a little sketchy, though it does seem to work.
7. Clarified the purpose of dump logs.;;;","08/Mar/13 23:31;jkreps;Patch version V3:
1. Rebased to include the dynamic config change.
2. Made delete retention a per-topic config ;;;","09/Mar/13 01:02;sriramsub;1. OffsetMap
    a. The way the probe is calculated, we could end up having the same probe multiple times. Starting with attempt = hashSize - 4 to attempt = hashSize the probe would be the same. 
    val probe = Utils.readInt(hash, math.min(attempt, hashSize-4)) + math.max(0, attempt - hashSize)

2. LogCleaner
    a. Cleaner doc comments need to be updated
   
Will look at the test changes and provide comments if any.
;;;","09/Mar/13 04:44;jkreps;Nice catch Sriram, that actually drops the collision rate by 7%.

Here is a new patch that fixes that bug, fixes the docs, exposes the cleaner buffer load factor as a configuration parameter.;;;","11/Mar/13 16:02;nehanarkhede;+1 on patch v4;;;","11/Mar/13 21:29;junrao;Thanks for patch v4. Looks good. Some minor comments:

40. IteratorTemplate: Not sure that I understand how peek() is different from next(). If both cases, they call hasNext() and therefore move nextItem to the next item, right?

41. LogCleaner:
41.1 Could you add some comments in the header to describe how delete retention works?
41.2 cleanSegments(): val now not used.

42. Decoder:
42.1 Could we add a comment in the trait saying that bytes can be null?
42.2 We need to fix StringDecoder to return null if input is null.
;;;","12/Mar/13 16:46;jkreps;Jun, attached v5 patch to address your comments.
40. This is actually right, both peek, hasNext, and next will all call makeNext() if there isn't an item ready. But peek and hasNext are idempotent and next() isn't--it advances the iterator. I wrote a unit test that demonstrates this.
41. Added the comment and removed the stray variable.
42. Actually the handling of nulls is not done in the serializers, it is done in Kafka. That is no matter what serializer you use, null always deserializes to null. You could argue either way whether this is a good thing. The downside to pushing it isn't the serializer is that all serializers have to remember to handle null. The advantage is that the serializer could yield a different value for null if it wanted. Couldn't think of a use for the later so I went with the simple thing.;;;","12/Mar/13 17:38;junrao;Thanks for patch v5. +1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
correlationId is not set in FetchRequest in AbstractFetcherThread,KAFKA-738,12629462,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,junrao,junrao,27/Jan/13 23:17,30/Jan/13 01:14,14/Jul/23 05:39,30/Jan/13 01:01,0.8.0,,,0.8.0,,,,,,,core,,,0,p2,,,"correlationId is always 0 in FetchRequest in AbstractFetcherThread.

",,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/13 18:20;junrao;kafka-738.patch;https://issues.apache.org/jira/secure/attachment/12567018/kafka-738.patch","28/Jan/13 22:45;junrao;kafka-738.patch;https://issues.apache.org/jira/secure/attachment/12566842/kafka-738.patch","29/Jan/13 00:08;junrao;kafka-738_v2.patch;https://issues.apache.org/jira/secure/attachment/12566857/kafka-738_v2.patch","29/Jan/13 20:47;junrao;kafka-738_v3.patch;https://issues.apache.org/jira/secure/attachment/12567044/kafka-738_v3.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,309958,,,Wed Jan 30 01:01:45 UTC 2013,,,,,,,,,,"0|i1hhrb:",310302,,,,,,,,,,,,,,,,,,,,"28/Jan/13 00:30;swapnilghike;Probably because we create a new FetchRequestBuilder object in AbstractFetcherThread.doWork() and create the fetchRequest as 

val fetchRequest = fetchRequestBuilder.build() // this sets correlationId in fetchRequest to 0.

;;;","28/Jan/13 22:45;junrao;Attach a patch. 

Yes, we have to create a new FetchRequestBuilder every time, since some of the old values need to be cleared.;;;","28/Jan/13 23:15;swapnilghike;With this patch, all fetchRequests in SimpleConsumerShell will have correlationId = 0. This will happen in a bunch of unit tests too.
A solution could be to use the same fetchRequestBuilder in AbstractFetcherThread, but clear its old values every time in doWork(). This way we can make sure that a fetchRequestBuilder can increment its own correlationId with every call to build().;;;","29/Jan/13 00:08;junrao;Attach patch v2 that fixes the simpleConsumerShell issue. Overall, it seems it's safer if we don't reuse the request builder.;;;","29/Jan/13 00:21;swapnilghike;I see. Actually the same issue is present in hadoop-consumer's use of builder.build(). Unit tests look fine with patch v2.;;;","29/Jan/13 00:27;nehanarkhede;We don't use the builder pattern the way it is supposed to be used. The builder is created once, setting the required values and calling build() happens multiple times. In our case, this is complicated because we maintain some state in the request map, but this can easily be resolved by providing a resetBuilder() API. That way, it will work for SimpleConsumer and high level consumer;;;","29/Jan/13 18:20;junrao;Attach patch v3. Having a separate reset() method adds more work to the client. Changed it to reset the map after build().;;;","29/Jan/13 18:32;nehanarkhede;Thanks for the new patch. Some more comments -

1. FetchRequest
1.1 Remove unused import import java.util.concurrent.atomic.AtomicInteger
1.2 I thought you wanted to clear the map in the build method. Don't see that change included in this patch ?

2. FetchRequestBuilder
2.1 It is not ideal to have the SimpleConsumer set the correlation id correctly on each fetch request. It is better to leave it as it was done before (increment the correlation id in build()) and just change it to use Utils.getNextNonNegativeInt.
;;;","29/Jan/13 20:47;junrao;Sorry, attached the wrong patch. Attach the correct one v3 this time.;;;","29/Jan/13 22:30;nehanarkhede;+1 on the latest patch;;;","29/Jan/13 23:31;swapnilghike;Though with this patch, the requestMap will be cleared in SimpleConsumerShell and other places that use build(), which is not the case currently.;;;","30/Jan/13 00:12;swapnilghike;+1;;;","30/Jan/13 01:01;junrao;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Migration tool needs a revamp, it was poorly written and has many performance bugs",KAFKA-734,12629185,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,24/Jan/13 23:32,24/Feb/13 22:30,14/Jul/23 05:39,24/Feb/13 22:30,0.8.0,,,,,,,,,,tools,,,0,p1,,,Migration tool has a number of problems ranging from poor logging to poor design. This needs to be thought through again,,jfung,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/13 23:35;nehanarkhede;kafka-734-v1.patch;https://issues.apache.org/jira/secure/attachment/12566851/kafka-734-v1.patch","29/Jan/13 17:48;nehanarkhede;kafka-734-v2.patch;https://issues.apache.org/jira/secure/attachment/12567013/kafka-734-v2.patch","01/Feb/13 17:32;nehanarkhede;kafka-734-v3.patch;https://issues.apache.org/jira/secure/attachment/12567608/kafka-734-v3.patch","08/Feb/13 05:42;nehanarkhede;kafka-734-v4.patch;https://issues.apache.org/jira/secure/attachment/12568535/kafka-734-v4.patch","22/Feb/13 23:15;nehanarkhede;kafka-734-v5.patch;https://issues.apache.org/jira/secure/attachment/12570554/kafka-734-v5.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,309029,,,Sun Feb 24 22:30:02 UTC 2013,,,,,,,,,,"0|i1dyin:",289689,,,,,,,,,,,,,,,,,,,,"28/Jan/13 23:35;nehanarkhede;Few changes to the migration tool -

1. Do not share the list of producers amongst all the consumer threads. Since every consumer thread used the circular iterator over the same ordered list of producers, they spent most of their time locking the producer queues. This is fixed by partitioning the list of producers amongst the consumer threads. Intuitively, it seems that it is enough to have one consumer thread use on producer, but since the producer spends a significant amount of time waiting for an ack from the broker, it is useful to have multiple producers per consumer thread on the migration tool. 
2. There was a debug statement that was unprotected in the MigrationThread that caused the thread to spent time blocked on log4j. 
3. There is still another problem that is not related directly to the migration tool, but is a problem with the rebalancing logic in the consumer for wildcard subscription. I tried firing up 2 migration tool instances, each with 32 consumer threads to migrate ~300 topics, most of which had 1 partition. Since the rebalancing logic is per topic, it causes the 1st migration tool to take almost all of the load. This is a bug in the rebalancing logic which disallows us from horizontally scaling out consumption in the presence of multiple topics, each with a small number of partitions. Will file another bug to track this.;;;","29/Jan/13 02:15;junrao;Thanks for the patch. Some comments:

1. Utils.partition(): It's probably better to move this method to MigrationTool since it's customized for its use case. Also, it would be great if we can add some examples of how this method works.

2. Producer: asyncProducerID is no longer used.;;;","29/Jan/13 17:48;nehanarkhede;Thanks for the review, Jun!

1. Moved it to the migration tool. However, didn't find a good place to stick the unit test. So left it in utils test, let me know if you can think of a better place.
2. Removed.;;;","01/Feb/13 17:32;nehanarkhede;- KafkaMigrationTool

Simplified the partitioning logic of the producers and changed partitionProducers to return a list of producer ids per consumer

- UtilsTest

Added a unit test for the partitioning logic for the producers, that covers all the cases -
producers = consumers
producers < consumers
producers > consumers

- Tested this on a backlog of roughly 100s of GB. Prior to the changes, a migration tool couldn't keep up with production traffic. After applying this patch, it performs well, catches up and can keep up as well;;;","08/Feb/13 05:42;nehanarkhede;More improvements to migration tool -

1. Added a shutdown hook and shutdown logic

2. Changed the design of migration tool as per Jun's suggestion. Basically, it looks more like the request channel idea from the socket server. The migration threads are consumers that add to a common producer channel. The producer threads pull from the common channel and send data across. This ensures that if one of the producers slow down, the data keeps flowing through rest of the producers.

3. Didn't get a chance to test this on a large workload, there might be bugs.;;;","08/Feb/13 05:43;nehanarkhede;Few problems that are not solved yet -

1. If consumer threads shutdown on their own, there is no way to shutdown the entire tool
2. If producer threads shutdown, there is no way to shutdown the entire tool;;;","15/Feb/13 19:26;jfung;** Tried the latest patch v4 and got the following FATAL error:

[2013-02-15 17:36:01,324] INFO Property queue.enqueue.timeout.ms is overridden to -1 (kafka.utils.VerifiableProperties)
[2013-02-15 17:36:01,358] ERROR Kafka migration tool failed:  (kafka.tools.KafkaMigrationTool)
java.lang.IllegalArgumentException: Topic cannot be null.
        at kafka.producer.KeyedMessage.<init>(KeyedMessage.scala:25)
        at kafka.tools.KafkaMigrationTool$ProducerThread.<init>(KafkaMigrationTool.java:349)
        at kafka.tools.KafkaMigrationTool.main(KafkaMigrationTool.java:236)
[2013-02-15 17:36:01,359] FATAL Migration thread failure due to  (kafka.tools.KafkaMigrationTool$MigrationThread)
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at kafka.tools.KafkaMigrationTool$MigrationThread.run(KafkaMigrationTool.java:310)
Caused by: java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1961)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1996)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
        at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:60)
        at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:32)
        at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:59)
        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:51)
        ... 5 more
;;;","22/Feb/13 23:15;nehanarkhede;Fixed 2 bugs -

- The shutdown message does not have a null topic anymore
- I was initially shutting down the migration and producer threads in the finally block at the end of the main method, which is pretty stupid of me. Fixed that 
- Caught InvocationTargetException so that we log the root cause. Otherwise java only throws the invocationtargetexception;;;","23/Feb/13 19:41;junrao;Thanks for patch v5. Just some minor comments. Once they are addressed, the patch can be checked in.

50. KafkaMigrationTool:
50.1 Should client.id for producer be set to the client.id in the producer property + the producer thread id?
50.2 In the shutdown hook, we need to call consumerConnector_07.shutdown first.
50.3 In MigrationThread, we don't need isRunning since it's never used.
50.4 In the catch clause of main(), it's probably better to print out the error and stacktrace, in addition to the logging in log4j. This way, if people forget to set log4j properly, they can still see why the tool failed.
50.5 At the end of shutdown hook, it's probably useful to print out sth like ""migration tool shuts down successfully"".



;;;","24/Feb/13 22:30;nehanarkhede;isRunning is required. Other that, incorporated other changes and fixed a producer thread shutdown bug;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MirrorMaker with shallow.iterator.enable=true produces unreadble messages,KAFKA-732,12629028,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,brugidou,brugidou,24/Jan/13 08:54,03/Apr/13 13:53,14/Jul/23 05:39,07/Mar/13 00:09,0.8.0,0.8.1,,0.8.0,,,,,,,core,producer ,,3,,,,"Trying to use MirrorMaker between two 0.8 clusters

When using shallow.iterator.enable=true on the consumer side, the performance gain is big (when incoming messages are compressed) and the producer does not complain but write the messages uncompressed without the compression flag.

If you try:
- enable compression on the producer, it obviously makes things worse since the data get double-compressed (the wiki warns about this)
- disable compression and the compressed messages are written in bulk in an uncompressed message, thus making it unreadable.

If I follow correctly the current state of code from MirrorMaker to the produce request, there is no way for the producer to know whether the message is deep or not. So I wonder how it worked on 0.7?

Here is the code as i read it (correct me if i'm wrong):

1. MirrorMakerThread.run(): create KeyedMessage[Array[Byte],Array[Byte]](topic, message)
2. Producer.send() -> DefaultEventHandler.handle()
3. DefaultEventHandler.serialize(): use DefaultEncoder for the message (does nothing)
4. DefaultEventHandler.dispatchSerializedData():
4.1 DefaultEventHandler.partitionAndCollate(): group messages by broker/partition/topic
4.2 DefaultEventHandler.dispatchSerializeData(): cycle through each broker
4.3 DefaultEventHandler.groupMessagesToSet(): Create a ByteBufferMessageSet for each partition/topic grouping all the messages together, and compressing them if needed
4.4 DefaultEventHandler.send(): send the ByteBufferMessageSets for this broker in one ProduceRequest

The gist is that in DEH.groupMessagesToSet(), you don't know wether the raw message in KeyedMessage.message is shallow or not. So I think I missed something... Also it doesn't seem possible to send batch of deep messages in one ProduceRequest.

I would love to provide a patch (or if you tell me that i'm doing it wrong, it's even better), since I can easily test it on my test clusters but I will need guidance here.",,brugidou,jfilipiak,junrao,kamaradclimber,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-845,,,,,,,,,,,,,"06/Mar/13 23:40;junrao;kafka-732.patch;https://issues.apache.org/jira/secure/attachment/12572439/kafka-732.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,308646,,,Thu Mar 07 00:09:40 UTC 2013,,,,,,,,,,"0|i1cqyf:",282498,,,,,,,,,,,,,,,,,,,,"19/Feb/13 01:24;junrao;I seems that it's non-trivial to support shallow iteration in 0.8. The main reason is that the encoder api is changed to encode(event: T) => byte[], from encode(event: T) => Message. So, in 0.7, we can get a compressed message from the source and simply pass it to the producer in mirrorMaker. In 0.8, there is no easy way that we can do that.

So, I suggest that we remove the shallowIteration option in ConsumerConfig and revisit this issue post 0.8.;;;","22/Feb/13 05:36;nehanarkhede;Moving this out of 0.8 as per discussion on mailing list;;;","06/Mar/13 23:40;junrao;Attach a patch that removes the shallow iteration option in the consumer.;;;","06/Mar/13 23:47;nehanarkhede;+1;;;","07/Mar/13 00:09;junrao;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gzip compression codec complains about missing SnappyInputStream,KAFKA-729,12628935,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,jfung,jfung,23/Jan/13 18:53,03/Jul/13 21:57,14/Jul/23 05:39,03/Jul/13 21:57,,,,,,,,,,,,,,0,,,,"$ bin/kafka-run-class.sh kafka.perf.ProducerPerformance --broker-list localhost:9092 --topic test_1 --messages 10 --batch-size 1 --compression-codec 1


java.lang.NoClassDefFoundError: org/xerial/snappy/SnappyInputStream
        at kafka.message.ByteBufferMessageSet$.kafka$message$ByteBufferMessageSet$$create(ByteBufferMessageSet.scala:41)
        at kafka.message.ByteBufferMessageSet.<init>(ByteBufferMessageSet.scala:98)
        at kafka.producer.async.DefaultEventHandler$$anonfun$4.apply(DefaultEventHandler.scala:291)
        at kafka.producer.async.DefaultEventHandler$$anonfun$4.apply(DefaultEventHandler.scala:279)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.mutable.HashMap.map(HashMap.scala:39)
        at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$groupMessagesToSet(DefaultEventHandler.scala:279)
        at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$1.apply(DefaultEventHandler.scala:102)
        at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$1.apply(DefaultEventHandler.scala:98)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
        at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:98)
        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:72)
        at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:104)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:87)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:67)
        at scala.collection.immutable.Stream.foreach(Stream.scala:254)
        at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:66)
        at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:44)
",,jfung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,308431,,,Wed Jul 03 21:57:33 UTC 2013,,,,,,,,,,"0|i1c7fz:",279336,,,,,,,,,,,,,,,,,,,,"03/Jul/13 21:57;jfung;Not an issue any more.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System Test (with compression on) Failures due to missing snappy jar path in kafka-run-class.sh,KAFKA-728,12628933,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jfung,jfung,jfung,23/Jan/13 18:47,24/Jan/13 04:37,14/Jul/23 05:39,24/Jan/13 02:04,,,,,,,,,,,,,,0,,,,,,charmalloc,jfung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/13 19:03;jfung;kafka-728.patch;https://issues.apache.org/jira/secure/attachment/12566164/kafka-728.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,308429,,,Thu Jan 24 02:04:40 UTC 2013,,,,,,,,,,"0|i1c7fj:",279334,,,,,,,,,,,,,,,,,,,,"23/Jan/13 19:03;jfung;Uploaded kafka-728.patch to add the snappy jar path to kafka-run-class.sh such that we can run system test with compression enabled.;;;","24/Jan/13 02:04;charmalloc;looks good, committed ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
broker can still expose uncommitted data to a consumer,KAFKA-727,12628852,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jkreps,junrao,junrao,23/Jan/13 05:22,23/Dec/14 17:57,14/Jul/23 05:39,23/Jan/13 22:10,0.8.0,,,,,,,,,,core,,,0,p1,,,"Even after kafka-698 is fixed, we still see consumer clients occasionally see uncommitted data. The following is how this can happen.

1. In Log.read(), we pass in startOffset < HW and maxOffset = HW.
2. Then we call LogSegment.read(), in which we call translateOffset on the maxOffset. The offset doesn't exist and translateOffset returns null.
3. Continue in LogSegment.read(), we then call messageSet.sizeInBytes() to fetch and return the data.

What can happen is that between step 2 and step 3, a new message is appended to the log and is not committed yet. Now, we have exposed uncommitted data to the client.",,jkreps,junrao,lokeshbirla,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-725,,,,,,,,,,,,,,,,,,,,,"23/Jan/13 20:29;jkreps;KAFKA-727-v1.patch;https://issues.apache.org/jira/secure/attachment/12566183/KAFKA-727-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,308332,,,Tue Dec 23 17:57:21 UTC 2014,,,,,,,,,,"0|i1b6jr:",273359,,,,,,,,,,,,,,,,,,,,"23/Jan/13 05:29;junrao;One way to fix this is for FileMessageSet.searchFor() to return OffsetPosition(-1L, the value of size) if offset is not found, instead of returning null. In LogSegment.read(), we can use the returned position to guard the length of messageSet.read.  ;;;","23/Jan/13 05:41;jkreps;Fantastic catch.

I think another fix is to just save the size of the log prior to translating the hw mark and use this rather than dynamically checking log.sizeInBytes later in the method. This will effectively act as a valid lower bound.

It might also be worthwhile to write a throw away torture test that has one thread do appends and another thread do reads and check that this condition is not violated in case there are any more of these subtleties. 

Happy to take this one on since it is my bad.;;;","23/Jan/13 05:49;junrao;Jay, sure, you can take this on. The way we saw this is that we had a consumer client that uses minBytes=1 and a producer that produces data once every couple of secs.;;;","23/Jan/13 16:30;nehanarkhede;>> I think another fix is to just save the size of the log prior to translating the hw mark and use this rather than dynamically checking log.sizeInBytes later in the method. This will effectively act as a valid lower bound.

+1

>> It might also be worthwhile to write a throw away torture test that has one thread do appends and another thread do reads and check that this condition is not violated in case there are any more of these subtleties.

This will be really good to have going forward;;;","23/Jan/13 20:29;jkreps;Patch v1.
1. Snapshot the size of the log prior to translating the maxOffset to a file position to give a consistent end point.
2. Fix docs on read so they match the code
3. Add a stress test that does reads and writes at the same point to validate fix;;;","23/Jan/13 22:02;nehanarkhede;+1. Minor change before checkin
Remove unused import ""import joptsimple._"" from StressTestLog;;;","23/Jan/13 22:10;nehanarkhede;Checked in patch v1 to proceed with build.;;;","04/Dec/14 21:46;lokeshbirla;Hi,

Is this really fixed? I still see this issue when I am using 4 topics, 3 partitions and 3 replication factor.  I am using kafka_2.9.2-0.8.1.1.
Currently I am using 3 node broker and 1 zookeeper. I did not see this issue when I used 1,2 or 3 topics. 



2014-08-18 06:43:58,356] ERROR [KafkaApi-1] Error when processing fetch request for partition [mmetopic4,2] offset 1940029 from consumer with correlation id 21 (kafka.server.Kaf
kaApis)
java.lang.IllegalArgumentException: Attempt to read with a maximum offset (1818353) less than the start offset (1940029).
        at kafka.log.LogSegment.read(LogSegment.scala:136)
        at kafka.log.Log.read(Log.scala:386)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSet(KafkaApis.scala:530)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:476)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:471)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:233)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:119)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
        at scala.collection.immutable.Map$Map1.map(Map.scala:107)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSets(KafkaApis.scala:471)
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:783)
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:765)
        at kafka.server.RequestPurgatory$ExpiredRequestReaper.run(RequestPurgatory.scala:216)
        at java.lang.Thread.run(Thread.java:745)


THanks for your help. 
;;;","12/Dec/14 18:37;junrao;Is there an easy way to reproduce this issue? Thanks,;;;","22/Dec/14 20:27;lokeshbirla;Hi Jun and Jay,
Also, I can see this issue with producer itself without running any consumer. 

I filed https://issues.apache.org/jira/browse/KAFKA-1806 for this issue. Could you please have a look? 
https://issues.apache.org/jira/browse/KAFKA-1806?focusedCommentId=14254551&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14254551

Please check my latest comment. 

Lokesh
;;;","23/Dec/14 17:57;nehanarkhede;[~lokeshbirla] Again, pasting [~junrao]'s comment here again
bq. Is there an easy way to reproduce this issue?

What all of us are looking for is steps (a reproducible test case) that we can run through on trunk, to see the same problem and error you do. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ReplicaFetcherThread name to mbean names,KAFKA-726,12628828,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,23/Jan/13 00:26,24/Jan/13 04:38,14/Jul/23 05:39,23/Jan/13 20:06,0.8.0,0.8.1,,0.8.0,,,,,,,,,,0,bugs,,,"1. Add ReplicaFetcherThread name to mbean names via clientId.

2. Use a uniform format for thread names in ConsumerFetcherThread and ReplicaFetcherThread.

3. Modify the way kafka.server.FetcherStats are created so that the thread name is not appended twice to the mbean name.",,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/13 19:48;swapnilghike;KAFKA-726-v2.patch;https://issues.apache.org/jira/secure/attachment/12566177/KAFKA-726-v2.patch","23/Jan/13 01:11;swapnilghike;KAFKA-726.patch;https://issues.apache.org/jira/secure/attachment/12566062/KAFKA-726.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,308306,,,Wed Jan 23 20:06:50 UTC 2013,,,,,,,,,,"0|i1b1mv:",272563,,,,,,,,,,,,,,,,,,,,"23/Jan/13 01:11;swapnilghike;As described in the description above.;;;","23/Jan/13 06:22;junrao;Thanks for the patch. In ReplicaFetcherManager, it's actually useful to include source broker id in the thread name since there are typically multiple ReplicaFetcherThreads on a broker. Perhaps, we can pass in clientId to ReplicaFetcherThread.;;;","23/Jan/13 06:29;swapnilghike;I actually deleted brokerConfig.brokerId. ReplicaFetcherManager still passes the sourceBroker.id in the name of ReplicaFetcherThread. 

I did not understand your comment about the clientId though.;;;","23/Jan/13 06:38;junrao;Sorry, I missed the brokerId change. Yes, you are right.

The clientId in ReplicaFetcherThread is now replica-fetcher-ReplicaFetcherThread-fetcherId-brokerId. It probably should just be replica-fetcher-fetcherId-brokerId. We can pass that into ReplicaFetcherThread.;;;","23/Jan/13 18:49;nehanarkhede;In ReplicaFetcherThread.scala,

the clientId can just be name, we don't need FetchRequest.ReplicaFetcherClientId anymore.;;;","23/Jan/13 19:48;swapnilghike;Set the clientId to name in ReplicaFetcherThread and removed ReplicaFetcherClientId from FetchRequest since it is not used anywhere else.;;;","23/Jan/13 20:06;junrao;Thanks for patch v2. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker Exception: Attempt to read with a maximum offset less than start offset,KAFKA-725,12628820,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,srdo,criccomini,criccomini,22/Jan/13 23:16,06/May/16 12:59,14/Jul/23 05:39,06/May/16 12:59,0.8.0,,,0.10.0.0,,,,,,,log,,,0,,,,"I have a simple consumer that's reading from a single topic/partition pair. Running it seems to trigger these messages on the broker periodically:

2013/01/22 23:04:54.936 ERROR [KafkaApis] [kafka-request-handler-4] [kafka] []  [KafkaApi-466] error when processing request (MyTopic,4,7951732,2097152)
java.lang.IllegalArgumentException: Attempt to read with a maximum offset (7951715) less than the start offset (7951732).
        at kafka.log.LogSegment.read(LogSegment.scala:105)
        at kafka.log.Log.read(Log.scala:390)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSet(KafkaApis.scala:372)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:330)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:326)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map1.map(Map.scala:93)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSets(KafkaApis.scala:326)
        at kafka.server.KafkaApis$$anonfun$maybeUnblockDelayedFetchRequests$2.apply(KafkaApis.scala:165)
        at kafka.server.KafkaApis$$anonfun$maybeUnblockDelayedFetchRequests$2.apply(KafkaApis.scala:164)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.server.KafkaApis.maybeUnblockDelayedFetchRequests(KafkaApis.scala:164)
        at kafka.server.KafkaApis$$anonfun$handleProducerRequest$2.apply(KafkaApis.scala:186)
        at kafka.server.KafkaApis$$anonfun$handleProducerRequest$2.apply(KafkaApis.scala:185)
        at scala.collection.immutable.Map$Map2.foreach(Map.scala:127)
        at kafka.server.KafkaApis.handleProducerRequest(KafkaApis.scala:185)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:58)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:41)
        at java.lang.Thread.run(Thread.java:619)

When I shut the consumer down, I don't see the exceptions anymore.

This is the code that my consumer is running:
          while(true) {
            // we believe the consumer to be connected, so try and use it for a fetch request
            val request = new FetchRequestBuilder()
              .addFetch(topic, partition, nextOffset, fetchSize)
              .maxWait(Int.MaxValue)
              // TODO for super high-throughput, might be worth waiting for more bytes
              .minBytes(1)
              .build

            debug(""Fetching messages for stream %s and offset %s."" format (streamPartition, nextOffset))
            val messages = connectedConsumer.fetch(request)
            debug(""Fetch complete for stream %s and offset %s. Got messages: %s"" format (streamPartition, nextOffset, messages))
            if (messages.hasError) {
              warn(""Got error code from broker for %s: %s. Shutting down consumer to trigger a reconnect."" format (streamPartition, messages.errorCode(topic, partition)))
              ErrorMapping.maybeThrowException(messages.errorCode(topic, partition))
            }
            messages.messageSet(topic, partition).foreach(msg => {
              watchers.foreach(_.onMessagesReady(msg.offset.toString, msg.message.payload))
              nextOffset = msg.nextOffset
            })
          }

Any idea what might be causing this error?",,becket_qin,chienle,criccomini,diwakar,githubbot,guozhang,gwenshap,ijuma,junrao,lokeshbirla,mauzhang,muditcse,nehanarkhede,rmetzger,srdo,wushujames,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-727,,,,,,,,,FLINK-3288,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,308298,,,Fri May 06 12:59:09 UTC 2016,,,,,,,,,,"0|i1b1l3:",272555,,guozhang,,,,,,,,,,,,,,,,,,"23/Jan/13 00:43;nehanarkhede;This looks exactly like KAFKA-698. Chris, did you try with a Kafka cluster that includes a fix for KAFKA-698 ?;;;","05/Dec/14 17:45;lokeshbirla;Neha,

I still see this issue in 0.8.1.1.

https://issues.apache.org/jira/browse/KAFKA-1806;;;","12/Jan/15 18:01;diwakar;Neha,

we have 6 brokers and 131 partitions per topic(replication factor : 3 ) and recently updated to kafka_2.10-0.8.2.0 and facing similar issue causing lot of below errors.Due to this it seems like producers are unable to produce to kafka successfully.

[2015-01-11 05:21:56.604-0700] ERROR [Replica Manager on Broker 2]: Error when processing fetch request for partition [application-access,13] offset 42748276 from consumer with correlation id 4974. Possible cause: Attempt to read with a maximum offset (42748275) less than the start offset (42748276). 


Any solution available to fix this.

Thanks
Diwakar
;;;","22/Jan/16 00:25;mauzhang;Is anyone still looking at this issue ? We have run into this exception on a 4-node kafka_2.10-0.8.2.1 cluster where 4 producers produce data with throughput of 17k messages/s on each node.
;;;","22/Jan/16 01:29;gwenshap;KAFKA-2477 has somewhat similar symptoms. Perhaps you are running into that? You can try applying the patch and checking if it fixes your issue.;;;","22/Jan/16 06:17;becket_qin;[~gwenshap] [~mauzhang] Not sure if it is related to KAFKA-2477. KAFKA-2477 should only affect replica fetchers, and we never set maxOffset for replica fetchers. The error log here seems caused by a regular consumers trying to fetch beyond high watermark. But this should not affect producing.;;;","17/Feb/16 05:19;mauzhang;I can reproduce this on 0.9.0.0. The error log is 

[2016-01-28 16:12:32,840] ERROR [Replica Manager on Broker 1]: Error processing fetch operation on partition [ad-events,1] offset 75510318 (kafka.server.ReplicaManager)

I also print the sent offset from producer 

time   partition offset 
16:12:32.840   1   75510318

It seems the offset is produced and consumed at the same time. 
;;;","17/Feb/16 06:22;mauzhang;I can reproduce this on 0.9.0.0. The error log is 

[2016-01-28 16:12:32,840] ERROR [Replica Manager on Broker 1]: Error processing fetch operation on partition [ad-events,1] offset 75510318 (kafka.server.ReplicaManager)

I also print the sent offset from producer 

time   partition offset 
16:12:32.840   1   75510318

It seems the offset is produced and consumed at the same time. 
;;;","01/Apr/16 13:14;srdo;We're seeing this on 8.2.2. I'm not sure Log/LogSegment handles the high watermark as gracefully as they maybe could.

My guess at how it's happening:
Assume a replica set of at least 2.
A consumer (in our case the Storm KafkaSpout) reads up to the end of the committed log, say up to message 5. 
The leader for the relevant partition then receives one or more messages (6). 
Before the new message(s) are replicated, the consumer increments its offset and fetches (from 6). 
The leader receives the fetch, sets the maxOffset for read to the high watermark (5), and compares the end of the log to the requested offset (see https://github.com/apache/kafka/blob/0.9.0.1/core/src/main/scala/kafka/log/Log.scala#L482). This check passes because the end of the log is at 6.
When the read on LogSegment is reached, it will error out when the maxOffset is smaller than the start offset, which causes this error log. https://github.com/apache/kafka/blob/0.9.0.1/core/src/main/scala/kafka/log/LogSegment.scala#L146

Maybe the check in Log should include whether the maxOffset is larger than offset as well?

Edit: Or maybe Kafka should allow the fetch to wait until the requested offset is available, similar to how minBytes can be waited for?;;;","02/Apr/16 14:56;githubbot;GitHub user srdo opened a pull request:

    https://github.com/apache/kafka/pull/1178

    KAFKA-725: Change behavior of Log/LogSegment when attempting read on an offset that's above high watermark.

    This should make Log.read act the same when startOffset is larger than maxOffset as it would if startOffset was larger than logEndOffset. The current behavior can result in an IllegalArgumentException from LogSegment if a consumer attempts to fetch an offset above the high watermark which is present in the leader's log. It seems more correct if Log.read presents the view of the log to consumers as if it simply ended at maxOffset (high watermark).
    
    I've tried to describe an example scenario of this happening here https://issues.apache.org/jira/browse/KAFKA-725?focusedCommentId=15221673&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15221673
    
    I'm not sure I understand why ReplicaManager sets maxOffset to the high watermark, and not high watermark + 1. Isn't the high watermark the last committed message, and readable by consumers?
    
    Tests passed for me locally on second try, seems like it just hit a flaky test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/srdo/kafka KAFKA-725

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1178.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1178
    
----
commit 5c7d583ec1af0892e9fadc4bbdcbeaa94390524e
Author: Stig Rohde Døssing <sdo@it-minds.dk>
Date:   2016-04-02T12:20:50Z

    KAFKA-725: Throw OffsetOutOfRangeException when reading from Log with maxOffset > startOffset

commit 5546433916d49b30b0869964a779e1af189be0ce
Author: Stig Rohde Døssing <sdo@it-minds.dk>
Date:   2016-04-02T13:37:22Z

    KAFKA-725: Return empty message set if reading from Log with maxOffset+1 == startOffset

commit 5808b31828d3703729569476217880971bf279af
Author: Stig Rohde Døssing <sdo@it-minds.dk>
Date:   2016-04-02T14:09:40Z

    KAFKA-725: Return only message offset when reading one beyond maxOffset

----
;;;","02/Apr/16 18:27;srdo;Nevermind this, I misunderstood the high watermark to be the last committed offset. It seems to be the last committed offset + 1. There's still a minor issue if a consumer requests an offset that's in the log but above the high watermark, which the PR should fix.;;;","03/Apr/16 18:47;githubbot;Github user srdo closed the pull request at:

    https://github.com/apache/kafka/pull/1178
;;;","03/Apr/16 18:47;githubbot;GitHub user srdo reopened a pull request:

    https://github.com/apache/kafka/pull/1178

    KAFKA-725: Change behavior of Log/LogSegment when attempting read on an offset that's above high watermark.

    This should make Log.read act the same when startOffset is larger than maxOffset as it would if startOffset was larger than logEndOffset. The current behavior can result in an IllegalArgumentException from LogSegment if a consumer attempts to fetch an offset above the high watermark which is present in the leader's log. It seems more correct if Log.read presents the view of the log to consumers as if it simply ended at maxOffset (high watermark).
    
    I've tried to describe an example scenario of this happening here https://issues.apache.org/jira/browse/KAFKA-725?focusedCommentId=15221673&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15221673
    
    I'm not sure I understand why ReplicaManager sets maxOffset to the high watermark, and not high watermark + 1. Isn't the high watermark the last committed message, and readable by consumers?
    
    Tests passed for me locally on second try, seems like it just hit a flaky test.

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/srdo/kafka KAFKA-725

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1178.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1178
    
----
commit c7bab99b77b71c73380d473facda1138799e42a6
Author: Stig Rohde Døssing <sdo@it-minds.dk>
Date:   2016-04-02T12:20:50Z

    KAFKA-725: Throw OffsetOutOfRangeException when reading from Log with maxOffset > startOffset

commit 4f5b415651ec45d3040c22393d24293de4f2cfd0
Author: Stig Rohde Døssing <sdo@it-minds.dk>
Date:   2016-04-02T23:29:02Z

    KAFKA-725: Put check for HW from consumer in ReplicaManager.readFromLocalLog instead of Log.read

----
;;;","08/Apr/16 04:37;guozhang;[~Srdo] I think your reasoning still makes sense, because a producer usually produces in batches. So following your case, after receiving a batch of 5 messages, the log end offset could then be 5 + 5 + 1 = 11, but before it was replicated to follower the high watermark is still 5 + 1 = 6. Hence this check will fail.;;;","08/Apr/16 08:23;muditcse;Hi,

We are seeing below exception in our kafka logs on one of the broker id.

[2016-04-08 07:59:58,486] ERROR [Replica Manager on Broker 3]: Error processing fetch operation on partition [subscribed_product_logs,17] offset 483780 (kafka.server.ReplicaManager)
java.lang.IllegalStateException: Failed to read complete buffer for targetOffset 483780 startPosition 958861516 in /kafka/kafka-logs/subscribed_product_logs-17/00000000000000378389.log
        at kafka.log.FileMessageSet.searchFor(FileMessageSet.scala:133)
        at kafka.log.LogSegment.translateOffset(LogSegment.scala:105)
        at kafka.log.LogSegment.read(LogSegment.scala:126)
        at kafka.log.Log.read(Log.scala:506)
        at kafka.server.ReplicaManager$$anonfun$readFromLocalLog$1.apply(ReplicaManager.scala:536)
        at kafka.server.ReplicaManager$$anonfun$readFromLocalLog$1.apply(ReplicaManager.scala:507)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)
        at kafka.server.ReplicaManager.readFromLocalLog(ReplicaManager.scala:507)
        at kafka.server.ReplicaManager.fetchMessages(ReplicaManager.scala:462)
        at kafka.server.KafkaApis.handleFetchRequest(KafkaApis.scala:431)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:69)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:60)
        at java.lang.Thread.run(Thread.java:745)
[2016-04-08 07:59:58,486] ERROR [Replica Manager on Broker 3]: Error processing fetch operation on partition [subscribed_product_logs,8] offset 592637 (kafka.server.ReplicaManager)
java.lang.IllegalStateException: Failed to read complete buffer for targetOffset 592637 startPosition 780606424 in /kafka/kafka-logs/subscribed_product_logs-8/00000000000000505731.log
        at kafka.log.FileMessageSet.searchFor(FileMessageSet.scala:133)
        at kafka.log.LogSegment.translateOffset(LogSegment.scala:105)
        at kafka.log.LogSegment.read(LogSegment.scala:126)
        at kafka.log.Log.read(Log.scala:506)
        at kafka.server.ReplicaManager$$anonfun$readFromLocalLog$1.apply(ReplicaManager.scala:536)
        at kafka.server.ReplicaManager$$anonfun$readFromLocalLog$1.apply(ReplicaManager.scala:507)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
        at scala.collection.AbstractTraversable.map(Traversable.scala:104)
        at kafka.server.ReplicaManager.readFromLocalLog(ReplicaManager.scala:507)
        at kafka.server.ReplicaManager.fetchMessages(ReplicaManager.scala:462)
        at kafka.server.KafkaApis.handleFetchRequest(KafkaApis.scala:431)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:69)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:60)
        at java.lang.Thread.run(Thread.java:745)

seems this is related to same bug.Any update on this?;;;","08/Apr/16 16:45;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1178
;;;","08/Apr/16 16:45;guozhang;Issue resolved by pull request 1178
[https://github.com/apache/kafka/pull/1178];;;","08/Apr/16 17:14;srdo;[~guozhang] Makes sense. I'm wondering if it would be better for the request to be put into purgatory then? If the request hits inbetween the high watermark and the end of the log, we can reasonably expect that offset to be readable shortly, while if the client gets a general OffsetOutOfRangeException, it might make more sense for the client to restart at either end of the log.

What I mean is basically, what does proper handling of this exception look like on the client-side now?;;;","08/Apr/16 17:22;guozhang;Usually client code should gracefully handle OffsetOutOfRangeException by requesting the current log end offset and retry fetching. Note that this handling would just make this exception potentially being triggered multiple times until the data is replicated complete and HW advanced; in most cases this is fine as it is only transient. But I agree that a more ideal solution is to park the request in purgatory so that we can reduce round trips retrying in this case as well.;;;","11/Apr/16 12:39;srdo;[~guozhang] Okay, returning the error should be fine then. I can't really think of a case where this error can happen if the client is well behaved and unclean leader election is turned off. If the client never increments its offset by more than 1 past the most recently consumed message, it shouldn't be possible for it to request an offset higher than the high watermark, since the most recent offset it can have consumed is HW - 1.;;;","04/May/16 15:10;junrao;[~Srdo], thanks for the patch. It's still not very clear to me how a consumer can trigger the IllegalArgumentException even without the patch. The broker only returns messages up to the high watermark (HW) to the consumer. So, the offset from a consumer should always be <= HW. The problem can only occur if a consumer uses an offset > HW, but <= the log end offset, which should never happen in a normal consumer.;;;","04/May/16 17:39;becket_qin;[~junrao] I asked this question in the RB. It seems the consumer in this case is not the Java consumer. Theoretically a java consumer can only fetch beyond HW when unclean leader election occurs.;;;","05/May/16 04:06;junrao;Reopen this jira since the fix exposes a new issue. When the leader switches (say due to leader balancing), the new leader's HW can actually be smaller than the previous leader's HW since HW is propagated asynchronously. The new leader's log end offset is >= than the previous leader's HW and eventually its HW will move to its log end offset. Before that happens, if a consumer fetches data using previous leader's HW, with the patch, the consumer will get OffsetOutOfRangeException and thus has to reset the offset, which is bad. Without the patch, the consumer will get an empty response instead.

So, it seems that we should revert the changes in this patch.;;;","05/May/16 15:09;guozhang;Jun, thanks for pointing it out. While reverting this change, I'm thinking we should change https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/log/LogSegment.scala#L147 to return empty response instead of throw exceptions as well. What do you think?;;;","05/May/16 16:37;junrao;Yes, I agree. If the requested offset is > MaxOffset, it's better to just return an empty response instead of throwing an IllegalStateException. We can add a comment on why we want to do that. Also, while you are there, could you fix the following comment above LogSegment.read()? maxPosition is not optional.

   * @param maxPosition An optional maximum position in the log segment that should be exposed for read.
;;;","05/May/16 19:04;githubbot;GitHub user guozhangwang opened a pull request:

    https://github.com/apache/kafka/pull/1327

    HOTFIX: follow-up on KAFKA-725 to remove the check and return empty response instead of throw exceptions

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/guozhangwang/kafka K725r

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/1327.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #1327
    
----
commit 9fdfe9ce1a0242f78775cbc5e24fc4a059a07296
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2016-05-05T19:03:30Z

    follow-up on KAFKA-725

----
;;;","05/May/16 19:54;srdo;Thanks for fixing this. The scenario Jun describes is probably a better match for the times we saw the exception originally. We're using Storm's storm-kafka component to consume, and it shouldn't go beyond the HW if the HW never moves backwards. It seems plausible that the logs coincided with leader failover for us.;;;","05/May/16 23:55;githubbot;Github user asfgit closed the pull request at:

    https://github.com/apache/kafka/pull/1327
;;;","06/May/16 12:59;ijuma;[~guozhang]'s PR was merged to trunk and 0.10.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala's default case class toString() is very inefficient,KAFKA-723,12628772,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,nehanarkhede,nehanarkhede,22/Jan/13 19:24,22/Feb/15 05:37,14/Jul/23 05:39,22/Feb/15 05:37,0.8.0,,,0.8.1,,,,,,,core,,,0,,,,Request logging is in the critical path of processing requests and we use Scala's default toString() API to log the requests. We should override the toString() in these case classes and log only what is useful.,,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,308250,,,Sun Feb 22 05:37:08 UTC 2015,,,,,,,,,,"0|i1b1af:",272507,,,,,,,,,,,,,,,,,,,,"07/Feb/15 22:29;jkreps;[~nehanarkhede] I think we did this, no?;;;","22/Feb/15 05:37;nehanarkhede;As [~jkreps] pointed out, this is already fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Path typos in kafka-run-class.sh,KAFKA-722,12628769,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jfung,jfung,jfung,22/Jan/13 19:05,22/Jan/13 20:17,14/Jul/23 05:39,22/Jan/13 20:17,,,,,,,,,,,,,,0,,,,,,jfung,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/13 19:14;jfung;kafka-722.patch;https://issues.apache.org/jira/secure/attachment/12565993/kafka-722.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,308247,,,Tue Jan 22 20:16:56 UTC 2013,,,,,,,,,,"0|i1b19r:",272504,,,,,,,,,,,,,,,,,,,,"22/Jan/13 19:14;jfung;Upload kafka-722.patch;;;","22/Jan/13 19:16;jfung;Fixing the path from scala_2.8.0 => scala-2.8.0;;;","22/Jan/13 20:16;nehanarkhede;+1
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migration tool halts,KAFKA-720,12628763,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriramsub,nehanarkhede,nehanarkhede,22/Jan/13 18:45,29/Jan/13 20:12,14/Jul/23 05:39,28/Jan/13 23:11,0.8.0,,,,,,,,,,core,,,0,p1,,,"Migration tool halts, attaching a thread dump for further investigation",,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/13 18:45;nehanarkhede;migration-tool-halts;https://issues.apache.org/jira/secure/attachment/12565986/migration-tool-halts","24/Jan/13 18:25;sriramsub;migrationtoolfix-2.patch;https://issues.apache.org/jira/secure/attachment/12566337/migrationtoolfix-2.patch","24/Jan/13 03:38;sriramsub;migrationtoolfix.patch;https://issues.apache.org/jira/secure/attachment/12566247/migrationtoolfix.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,308241,,,Thu Jan 24 18:47:42 UTC 2013,,,,,,,,,,"0|i1b18f:",272498,,,,,,,,,,,,,,,,,,,,"24/Jan/13 03:38;sriramsub;When one of the migration tool thread dies, we were just eating up the exception. Also all the fetcher threads were trying to push data to the queue of the thread that died causing the tool to stall. We now catch exceptions and log them and continue. For any error belonging to lang.Error we log a fatal error.;;;","24/Jan/13 03:54;nehanarkhede;Thanks for the patch. There are some compile time errors. Few comments -

1. Each log4j statement using the helper APIs in Logging.scala should be of the form (""error message"", cause). Let's fix that.
2. Inside the while loop, catching throwable will also catching all Exceptions, so we only need to catch Throwable and exit. This is because, in the migration tool case, only a Kafka bug will throw an exception. In other cases, we might know the exact type of exception that is data dependent and expected, so it makes sense to catch those separately. ;;;","24/Jan/13 05:12;sriramsub;The patch seems to have conflicts with the changes I have. Will fix that.

2. Exception and Throwable are specifically caught separately. Throwable includes critical errors which should never be swallowed. In case of Exceptions caused mainly by kafka errors we are logging an error and continuing with consumption. If you are suggesting we exit for any exceptions it is no different from what is there already,;;;","24/Jan/13 18:25;sriramsub;- for now changed the catch statement to log a fatal error instead of directing to stdout. Let us look at the logs if the issue reproduces and find what the exceptions are.;;;","24/Jan/13 18:47;nehanarkhede;Checked in v2. We'll see how it goes;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka broker shuts down due to irrecoverable IO error,KAFKA-719,12628760,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,nehanarkhede,nehanarkhede,22/Jan/13 18:41,29/Jan/13 20:27,14/Jul/23 05:39,29/Jan/13 20:27,0.8.0,,,,,,,,,,log,,,0,bugs,p1,,"2013/01/22 03:48:18.144 ERROR [KafkaApis] [request-expiration-task] [kafka] []  [KafkaApi-277] error when processing request (service_metrics,2,1155,2000000)
java.nio.channels.ClosedChannelException
        at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)
        at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:613)
        at kafka.log.FileMessageSet.searchFor(FileMessageSet.scala:83)
        at kafka.log.LogSegment.translateOffset(LogSegment.scala:76)
        at kafka.log.LogSegment.read(LogSegment.scala:90)
        at kafka.log.Log.read(Log.scala:390)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSet(KafkaApis.scala:372)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:330)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:326)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map2.foreach(Map.scala:127)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map2.map(Map.scala:110)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSets(KafkaApis.scala:326)
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:528)
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:510)
        at kafka.server.RequestPurgatory$ExpiredRequestReaper.run(RequestPurgatory.scala:222)
        at java.lang.Thread.run(Thread.java:619)
2013/01/22 03:48:18.989 INFO [Processor] [kafka-processor-10251-2] [kafka] []  Closing socket connection to /172.18.146.132.
2013/01/22 03:48:18.989 INFO [Processor] [kafka-processor-10251-1] [kafka] []  Closing socket connection to /172.18.146.132.
2013/01/22 03:48:18.998 FATAL [KafkaApis] [kafka-request-handler-4] [kafka] []  [KafkaApi-277] Halting due to unrecoverable I/O error while handling produce request: 
kafka.common.KafkaStorageException: I/O exception in append to log 'service_metrics-2'
        at kafka.log.Log.append(Log.scala:301)
        at kafka.server.KafkaApis$$anonfun$appendToLocalLog$2.apply(KafkaApis.scala:249)
        at kafka.server.KafkaApis$$anonfun$appendToLocalLog$2.apply(KafkaApis.scala:242)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map1.map(Map.scala:93)
        at kafka.server.KafkaApis.appendToLocalLog(KafkaApis.scala:242)
        at kafka.server.KafkaApis.handleProducerRequest(KafkaApis.scala:181)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:58)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:41)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.nio.channels.ClosedChannelException
        at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)
        at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:189)
        at kafka.message.ByteBufferMessageSet.writeTo(ByteBufferMessageSet.scala:128)
        at kafka.log.FileMessageSet.append(FileMessageSet.scala:146)
        at kafka.log.LogSegment.append(LogSegment.scala:64)
        at kafka.log.Log.append(Log.scala:286)",,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-695,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,308238,,,Tue Jan 22 18:54:10 UTC 2013,,,,,,,,,,"0|i1b17r:",272495,,,,,,,,,,,,,,,,,,,,"22/Jan/13 18:54;jkreps;Isn't this the same as KAFKA-695?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-run-class.sh should use reasonable gc settings,KAFKA-718,12628759,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,22/Jan/13 18:40,05/Aug/13 17:16,14/Jul/23 05:39,05/Aug/13 17:16,,,,0.8.0,,,,,,,,,,0,,,,"Our start script seems to use the default ""stop the world"" collector. It would be good to default to well tuned gc settings including gc logging, CMS, etc. Whatever we are using in prod and perf lab...

Many people who want to use kafka basically don't know java well so they won't succeed in figuring this stuff out on their own and just think it is broken and timing out if we don't have good defaults.",,ashwanthfernando@gmail.com,jkreps,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/13 00:22;ashwanthfernando@gmail.com;718-v1.patch;https://issues.apache.org/jira/secure/attachment/12591330/718-v1.patch","11/Jul/13 21:28;jkreps;KAFKA-718-v2.patch;https://issues.apache.org/jira/secure/attachment/12591911/KAFKA-718-v2.patch","11/Jul/13 23:32;jkreps;KAFKA-718-v3.patch;https://issues.apache.org/jira/secure/attachment/12591933/KAFKA-718-v3.patch","03/Aug/13 17:47;jkreps;KAFKA-718-v4.patch;https://issues.apache.org/jira/secure/attachment/12595759/KAFKA-718-v4.patch","04/Aug/13 21:33;jkreps;KAFKA-718-v5.patch;https://issues.apache.org/jira/secure/attachment/12595831/KAFKA-718-v5.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,308237,,,Mon Aug 05 17:16:53 UTC 2013,,,,,,,,,,"0|i1b17j:",272494,,,,,,,,,,,,,,,,,,,,"03/Jul/13 04:08;ashwanthfernando@gmail.com;Hi, This bug seems to be open for a long time. I can take this up if its not been paid attention.;;;","03/Jul/13 04:20;jkreps;That would be great.;;;","04/Jul/13 08:52;ashwanthfernando@gmail.com;I checked out 0.8, committed my changes and while rebasing with the trunk (which I believe holds 0.7), there are a whole lot of inbound patches which have conflicts with 0.8 code. Do I try to manual merge these conflicts? I have no clue what those patches are.

Followed the simple contributor workflow here - https://cwiki.apache.org/confluence/display/KAFKA/Git+Workflow except that for ""git checkout -b xyz remotes/origin/trunk"" I did git checkout -b 718 remotes/origin/0.8
;;;","08/Jul/13 03:43;junrao;Since this is not critical for 0.8, I suggest that we make the patch for trunk.;;;","08/Jul/13 04:41;ashwanthfernando@gmail.com;ok. I will do that and get back. Tx.;;;","08/Jul/13 18:10;jkreps;Hey Jun, this should be a trivial change and will likely catch a lot of people (most people don't check the gc settings). GC leads to all kinds of irritiating problems so i think it would be good to get this in 0.8 if possible.;;;","09/Jul/13 00:18;ashwanthfernando@gmail.com;The change is trivial. I guess where I am having problems in sending out a patch is that I checked out 0.8, made my changes, committed to my local git repo. While rebasing with the trunk, there are a whole lot of inbound patches that need to be added to 0.8 branch. Do I merge these patches, because I don't feel comfortable dealing with the conflicts, because these patches are not authored by me.

Or, do I rebase with 0.8 instead of the trunk, if I am working on 0.8?;;;","09/Jul/13 00:22;ashwanthfernando@gmail.com;Eitherways, I have a patch that cleanly applies on trunk.

Basically the kafka-run-class.sh has a lot of .sh clients which use it. A majority of these clients are non-daemon and a couple (zk and kafkaServer) are daemons. For the non daemons, we don't need gc logs so I have the following settings:

-XX:+AggressiveOpts -XX:+UseCompressedOops -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC

For the daemon processes (zk and kafkaServer) I have the above options set + the options to emit GC Logs.

-Xloggc:$base_dir/$GC_LOG_FILE_NAME -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps

Where GC_LOG_FILE_NAME is either zookeeper_gc.log or kafkaServer_gc.log depending on which is the daemon that is run. This is done, so that when developers run both zk and kafkaServer on the same filesystem, the two processes don't clobber the same file.;;;","11/Jul/13 21:28;jkreps;Hey Ashwanth, thanks for the patch.

I applied it to 0.8 and I want to abuse this ticket to include a bunch of trivial ""good housekeeping"" fixes I think we need for the 0.8 final release that probably don't warrant a ticket of thier own.

This includes:
1. Your gc settings, but minus the +AggressiveOpts which doesn't seem to work on all versions of java 1.6
2. Clean up the README which has a number of outdated things in it.
3. Remove unused properties from server.properties to fix WARN messages.
4. Change to use log.dirs in the example server config so people know about multiple log directories
5. Remove incorrect description of flush.interval in example server config.;;;","11/Jul/13 23:32;jkreps;Separate out a few more command line variables (GC, GC logging, SCALA_VERSION, etc).

Also remove delete_topic.sh command since that doesn't work yet.;;;","11/Jul/13 23:39;ashwanthfernando@gmail.com;Thanks Jay !! Can we mark this as resolved or do you have some more?;;;","11/Jul/13 23:46;jkreps;Since 0.8 is somewhat locked down I am just waiting on close sanity checking from a few others before I check in. I'll close it up then.;;;","03/Aug/13 17:47;jkreps;New patch. Rebased. Fixed logging in tools so that we no longer spew crazy log messages intermingled with the tool output.;;;","04/Aug/13 21:33;jkreps;Patch v5: move all logs to a logs/ directory to avoid polluting the main directory with a half dozen rolling logs.;;;","04/Aug/13 22:29;junrao;Thanks for patch v4. Got the following exception running kafka-console-producer.sh (ditto for kafka-console-consumer.sh)

bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
log4j:ERROR Could not read configuration file from URL [file:bin/../config/tools-log4j.properties].
java.io.FileNotFoundException: bin/../config/tools-log4j.properties (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:120)
	at java.io.FileInputStream.<init>(FileInputStream.java:79)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:70)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:161)
	at java.net.URL.openStream(URL.java:1010)
	at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:459)
	at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:471)
	at org.apache.log4j.LogManager.<clinit>(LogManager.java:125)
	at org.apache.log4j.Logger.getLogger(Logger.java:105)
	at kafka.utils.Logging$class.logger(Logging.scala:24)
	at kafka.utils.VerifiableProperties.logger(VerifiableProperties.scala:23)
	at kafka.utils.Logging$class.info(Logging.scala:66)
	at kafka.utils.VerifiableProperties.info(VerifiableProperties.scala:23)
	at kafka.utils.VerifiableProperties.verify(VerifiableProperties.scala:180)
	at kafka.producer.ProducerConfig.<init>(ProducerConfig.scala:57)
	at kafka.producer.ConsoleProducer$.main(ConsoleProducer.scala:147)
	at kafka.producer.ConsoleProducer.main(ConsoleProducer.scala)
;;;","05/Aug/13 02:21;jkreps;Sorry, I had missed that file in v4. I had fixed it in v5, can you try that?;;;","05/Aug/13 15:16;junrao;+1 on v5.;;;","05/Aug/13 17:16;jkreps;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleConsumerPerformance does not consume all available messages,KAFKA-716,12628637,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,jfung,jfung,21/Jan/13 23:26,03/Jul/13 21:47,14/Jul/23 05:39,03/Jul/13 21:47,,,,,,,,,,,,,,0,p2,,,"To reproduce the issue:

1. Start 1 zookeeper

2. Start 1 broker

3. Send some messages

4. Start SimpleConsumerPerformance to consume messages. The only way to consume all messages is to set the fetch-size to be greater than the log segment file size.

5. This output shows that SimpleConsumerPerformance consumes only 6 messages:

$ bin/kafka-run-class.sh kafka.perf.SimpleConsumerPerformance --server kafka://host1:9092 --topic topic_001 --fetch-size 2048 --partition 0
start.time, end.time, fetch.size, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec
2013-01-21 15:09:21:124, 2013-01-21 15:09:21:165, 2048, 0.0059, 0.1429, 6, 146.3415

6. This output shows that ConsoleConsumer consumes all 5500 messages (same test as the above)

$ bin/kafka-run-class.sh kafka.consumer.ConsoleConsumer --zookeeper host2:2181 --topic topic_001 --consumer-timeout-ms 5000   --formatter kafka.consumer.ChecksumMessageFormatter  --from-beginning | grep ^checksum | wc -l
Consumed 5500 messages
5500
",,jfung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,307127,,,Wed Jul 03 21:47:52 UTC 2013,,,,,,,,,,"0|i1a5fz:",267348,,,,,,,,,,,,,,,,,,,,"03/Jul/13 21:47;jfung;Not an issue;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsoleConsumer throws SocketTimeoutException when fetching topic metadata,KAFKA-714,12628588,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,jfung,jfung,21/Jan/13 18:35,23/Jan/13 22:24,14/Jul/23 05:39,23/Jan/13 22:24,0.8.0,,,,,,,,,,core,,,0,bugs,p1,,"Test Description:

1. 1 zookeeper
2. 3 brokers
3. Replication factor = 3
4. Partions = 2
5. No. of topics = 250

There is no problem sending messages to brokers. But ConsoleConsumer is throwing SocketTimeoutException when fetching topic metadata. Currently, ConsoleConsumer doesn't provide a command line argument to configure Socket timeout.

Exception:

[2013-01-21 10:10:08,915] WARN Fetching topic metadata with correlation id 0 for topics [Set(topic_0219, topic_0026, topic_0160, topic_0056, topic_0100, topic_0146, topic_0103, topic_0179, topic_0078, topic_0098, topic_0102, topic_0028, topic_0060, topic_0218, topic_0210, topic_0161, topic_0144, topic_0101, topic_0104, topic_0186, topic_0040, topic_0027, topic_0093, topic_0147, topic_0080, topic_0211, topic_0089, topic_0177, topic_0220, topic_0097, topic_0079, topic_0187, topic_0105, topic_0178, topic_0096, topic_0108, topic_0095, topic_0065, topic_0066, topic_0021, topic_0023, topic_0109, topic_0058, topic_0092, topic_0149, topic_0150, topic_0250, topic_0022, topic_0227, topic_0145, topic_0063, topic_0094, topic_0216, topic_0185, topic_0057, topic_0141, topic_0215, topic_0184, topic_0024, topic_0214, topic_0140, topic_0217, topic_0228, topic_0025, topic_0064, topic_0044, topic_0043, topic_0152, topic_0009, topic_0029, topic_0151, topic_0142, topic_0041, topic_0164, topic_0077, topic_0062, topic_0163, topic_0046, topic_0061, topic_0190, topic_0162, topic_0143, topic_0165, topic_0148, topic_0042, topic_0087, topic_0223, topic_0182, topic_0008, topic_0132, topic_0204, topic_0007, topic_0067, topic_0181, topic_0169, topic_0203, topic_0180, topic_0224, topic_0183, topic_0048, topic_0107, topic_0069, topic_0130, topic_0106, topic_0047, topic_0068, topic_0222, topic_0189, topic_0221, topic_0131, topic_0134, topic_0156, topic_0111, topic_0246, topic_0110, topic_0245, topic_0171, topic_0240, topic_0010, topic_0122, topic_0201, topic_0135, topic_0196, topic_0034, topic_0241, topic_0012, topic_0230, topic_0082, topic_0188, topic_0195, topic_0166, topic_0088, topic_0036, topic_0099, topic_0172, topic_0112, topic_0085, topic_0202, topic_0123, topic_0011, topic_0115, topic_0084, topic_0121, topic_0243, topic_0086, topic_0192, topic_0035, topic_0191, topic_0200, topic_0242, topic_0231, topic_0133, topic_0229, topic_0116, topic_0167, topic_0244, topic_0032, topic_0168, topic_0157, topic_0118, topic_0209, topic_0045, topic_0226, topic_0119, topic_0076, topic_0117, topic_0006, topic_0129, topic_0225, topic_0033, topic_0159, topic_0037, topic_0197, topic_0030, topic_0049, topic_0205, topic_0238, topic_0004, topic_0153, topic_0074, topic_0127, topic_0083, topic_0003, topic_0126, topic_0249, topic_0158, topic_0005, topic_0081, topic_0155, topic_0031, topic_0198, topic_0206, topic_0020, topic_0154, topic_0075, topic_0239, topic_0128, topic_0212, topic_0017, topic_0054, topic_0174, topic_0073, topic_0072, topic_0173, topic_0039, topic_0213, topic_0138, topic_0059, topic_0015, topic_0055, topic_0052, topic_0237, topic_0038, topic_0091, topic_0236, topic_0053, topic_0234, topic_0070, topic_0193, topic_0051, topic_0090, topic_0248, topic_0125, topic_0002, topic_0050, topic_0247, topic_0137, topic_0124, topic_0014, topic_0001, topic_0071, topic_0235, topic_0194, topic_0120, topic_0232, topic_0175, topic_0208, topic_0170, topic_0114, topic_0016, topic_0139, topic_0013, topic_0136, topic_0113, topic_0018, topic_0233, topic_0019, topic_0176, topic_0199, topic_0207)] from broker [id:1,host:esv4-app19.corp.linkedin.com,port:9091] failed (kafka.client.ClientUtils$)
java.net.SocketTimeoutException
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:201)
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:86)
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:221)
        at kafka.utils.Utils$.read(Utils.scala:393)
        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)
        at kafka.network.Receive$class.readCompletely(Transmission.scala:56)
        at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29)
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:100)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:73)
        at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:71)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:105)
        at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:33)
        at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:66)
        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$rebalance(ZookeeperConsumerConnector.scala:403)
        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener$$anonfun$syncedRebalance$1.apply$mcVI$sp(ZookeeperConsumerConnector.scala:373)
        at scala.collection.immutable.Range$ByOne$class.foreach$mVc$sp(Range.scala:282)
        at scala.collection.immutable.Range$$anon$2.foreach$mVc$sp(Range.scala:265)
        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.syncedRebalance(ZookeeperConsumerConnector.scala:368)
        at kafka.consumer.ZookeeperConsumerConnector.kafka$consumer$ZookeeperConsumerConnector$$reinitializeConsumer(ZookeeperConsumerConnector.scala:674)
        at kafka.consumer.ZookeeperConsumerConnector$WildcardStreamsHandler.<init>(ZookeeperConsumerConnector.scala:709)
        at kafka.consumer.ZookeeperConsumerConnector.createMessageStreamsByFilter(ZookeeperConsumerConnector.scala:141)
        at kafka.consumer.ConsoleConsumer$.main(ConsoleConsumer.scala:183)
        at kafka.consumer.ConsoleConsumer.main(ConsoleConsumer.scala)
",,jfung,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/13 18:42;jfung;kafka-714-reproduce-issue.patch;https://issues.apache.org/jira/secure/attachment/12565820/kafka-714-reproduce-issue.patch","21/Jan/13 21:49;junrao;kafka-714.patch;https://issues.apache.org/jira/secure/attachment/12565857/kafka-714.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,307039,,,Wed Jan 23 22:24:33 UTC 2013,,,,,,,,,,"0|i1a3zz:",267114,,,,,,,,,,,,,,,,,,,,"21/Jan/13 18:42;jfung;Uploaded kafka-714-reproduce-issue.patch to reproduce the issue.

Please note this issue should be reproduced in a distributed environment. Therefore, system_test/cluster_config.json should be updated with the corresponding remote brokers machine in the entity hostname accordingly.;;;","21/Jan/13 21:49;junrao;Thanks for opening the jira. Attach a patch that allows socket timeout to be passed to  getMetadata requests on the consumer side.;;;","21/Jan/13 21:50;junrao;Mark it as an 0.8 blocker since it affects mirrorMaker.;;;","21/Jan/13 21:52;nehanarkhede;+1 on Jun's patch;;;","22/Jan/13 23:14;jfung;Thanks Jun for the patch. I have verified it with the latest 0.8 branch with 250 topics & 3 partitions in a 3-broker clusters (Replication factor = 3) and it works fine. FYI: the request-timeout-ms is set to 90,000ms (I think it is not necessary to be this high);;;","23/Jan/13 22:24;nehanarkhede;Thanks for the patch, checked in;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Hadoop producer for Kafka 0.8 changes,KAFKA-713,12628348,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,shahsam,shahsam,shahsam,18/Jan/13 23:46,01/Feb/13 18:52,14/Jul/23 05:39,01/Feb/13 18:47,0.8.0,,,0.8.0,,,,,,,,,,0,hadoop,,,"With the changes in Kafka 0.8, the Hadoop producer (in contrib) is busted due to changes in the way KeyedMessages are now handled. I will fix.",,nehanarkhede,shahsam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/13 18:42;shahsam;KAFKA-713.patch;https://issues.apache.org/jira/secure/attachment/12567181/KAFKA-713.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,305478,,,Fri Feb 01 18:47:25 UTC 2013,,,,,,,,,,"0|i18crj:",256870,,,,,,,,,,,,,,,,,,,,"18/Jan/13 23:53;shahsam;Here's a patch for the following changes to the Hadoop producer:

* Now works in 0.8 (fixed the KeyedMessage wrapping)
* Adds support for semantic partitioning
* Provides a much cleaner way of specifying configuration to Kafka
* Removes legacy ZK-based producer from the docs
* Removes one byte buffer copy on each message publish so is infinitesimally faster 
* Updates the build dependencies to use Pig 0.10;;;","19/Jan/13 02:45;nehanarkhede;Sam, thanks for the patch. I tried applying it on a fresh checkout of 0.8 and also on trunk, but it fails applying it to KafkaOutputFormat -
Hunk #4 FAILED at 99.
1 out of 4 hunks FAILED -- saving rejects to file contrib/hadoop-producer/src/main/java/kafka/bridge/hadoop/KafkaOutputFormat.java.rej

I also tried merging the changes, but there are many changes to this file and I'm not sure it would result in the set of changes you intended to make. Could you please rebase ?;;;","20/Jan/13 03:45;shahsam;Oops, here's the patch rebased to trunk.;;;","23/Jan/13 18:43;nehanarkhede;Thanks for the rebased patch, Sam. A few minor review suggestions -

1. KafkaOutputFormat

This will be a good time to add a few useful defaults to the Hadoop producer -
1.1 Remove max.message.size since it is obsolete in 0.8 as message size checks moved to the server
1.2 buffer.size is now send.buffer.bytes
1.3 Add compression.codec (0 = no compression, 1 = GZIP, 2 = Snappy). The Kafka producer defaults to no compression, but for Hadoop->Kafka pushes, compression will be useful to have. Should probably default to gzip/snappy

2. README
    REGISTER zkclient-20120522.jar; is no longer required
;;;","30/Jan/13 18:42;shahsam;Updated patch with Neha's suggestions. I default to GZip compression with the Hadoop producer.;;;","01/Feb/13 18:47;nehanarkhede;+1;;;","01/Feb/13 18:47;nehanarkhede;Thanks for the updated patch, Sam. Just checked it in.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some arguments are always set to default in ProducerPerformance,KAFKA-710,12628107,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jfung,jfung,jfung,17/Jan/13 20:23,21/Jan/13 16:09,14/Jul/23 05:39,21/Jan/13 04:55,,,,0.8.0,,,,,,,,,,0,,,,,,jfung,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/13 20:26;jfung;kafka-710-v1.patch;https://issues.apache.org/jira/secure/attachment/12565370/kafka-710-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,304958,,,Mon Jan 21 16:09:48 UTC 2013,,,,,,,,,,"0|i17x4f:",254336,,,,,,,,,,,,,,,,,,,,"17/Jan/13 20:25;jfung;The following arguments are always set to default values:
--request-num-acks
--compression-codec
--request-timeout-ms;;;","17/Jan/13 20:26;jfung;Uploaded kafka-710-v1.patch to remove the additional variables;;;","21/Jan/13 04:55;junrao;Thanks for the patch. Committed to 0.8 by adding a better description in the compress-codec option.;;;","21/Jan/13 16:09;junrao;Just to clarify. The problem is that there are duplicated command line options. After this patch, those options are still available, but work correctly.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default queue.enqueue.timeout.ms to -1,KAFKA-709,12627975,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,criccomini,criccomini,17/Jan/13 02:11,21/Jan/13 03:15,14/Jul/23 05:39,18/Jan/13 05:31,0.8.0,,,,,,,,,,producer ,,,0,,,,"Hey Guys,

It seems that, by default, producers in 0.8 are async, and have a default queue.enqueue.timeout.ms of 0. This means that anyone who reads messages faster than they're producing them will likely end up eventually hitting this exception:

Exception in thread ""Thread-3"" kafka.common.QueueFullException: Event queue is full of unsent messages, could not send event: KeyedMessage(PageViewEventByGroupJson,Missing Page Group,java.nio.HeapByteBuffer[pos=0 lim=125 cap=125])
        at kafka.producer.Producer$$anonfun$asyncSend$1.apply(Producer.scala:111)
        at kafka.producer.Producer$$anonfun$asyncSend$1.apply(Producer.scala:89)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32)
        at kafka.producer.Producer.asyncSend(Producer.scala:89)
        at kafka.producer.Producer.send(Producer.scala:77)

As it says in https://cwiki.apache.org/KAFKA/kafka-mirroring.html, this can result in losing messages, and nasty exceptions in the logs. I think the better default is setting queue.enqueue.timeout.ms to -1, which will just block until the queue frees up.",,charmalloc,criccomini,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/13 05:13;nehanarkhede;kafka-709.patch;https://issues.apache.org/jira/secure/attachment/12565262/kafka-709.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,304823,,,Mon Jan 21 03:15:44 UTC 2013,,,,,,,,,,"0|i17w6v:",254184,,,,,,,,,,,,,,,,,,,,"17/Jan/13 03:04;jkreps;Yeah I agree. Everyone who tries to use it hits this and then basically just thinks kafka is broken because the error message isn't very clear.;;;","17/Jan/13 05:13;nehanarkhede;Minor one liner change to fix the default for queue.enqueue.timeout.ms;;;","18/Jan/13 05:21;jkreps;+1;;;","18/Jan/13 05:31;nehanarkhede;Thanks for the review;;;","21/Jan/13 03:15;charmalloc;found a test failing related to this

the fix I think is best to just update the AsyncProducerTest case for if someone changes the new default value to something like 0 so it goes to offer instead of put

adding the line props.put(""queue.enqueue.timeout.ms"", ""0"")

going to commit this change to AsyncProducerTest.testProducerQueueSize (line 68) so it passes again;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ISR becomes empty while marking a partition offline,KAFKA-708,12627970,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,swapnilghike,swapnilghike,17/Jan/13 01:40,23/Jan/13 22:06,14/Jul/23 05:39,23/Jan/13 22:06,0.8.0,0.8.1,,0.8.0,,,,,,,,,,0,bugs,p1,,Attached state change log shows that ISR becomes empty when a partition is being marked as offline.,,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-715,,,,,,,,,,,,,,,,,,,,,"21/Jan/13 22:37;nehanarkhede;kafka-708-v1.patch;https://issues.apache.org/jira/secure/attachment/12565869/kafka-708-v1.patch","22/Jan/13 17:29;nehanarkhede;kafka-708-v2.patch;https://issues.apache.org/jira/secure/attachment/12565974/kafka-708-v2.patch","17/Jan/13 01:40;swapnilghike;kafka-request.log.2013-01-16-15;https://issues.apache.org/jira/secure/attachment/12565237/kafka-request.log.2013-01-16-15",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,304818,,,Wed Jan 23 22:06:10 UTC 2013,,,,,,,,,,"0|i17w5r:",254179,,,,,,,,,,,,,,,,,,,,"21/Jan/13 22:37;nehanarkhede;Changes in this patch include -

1. Allowing the isr list to be empty. This can happen if all the brokers in the isr fall out which can happen if all the replicas that host the partition are down. In other words, the partition is offline. The problem was that if the controller moves when the isr is empty or you restart the entire cluster, it relied on a non-empty isr.

2. Marking the partition offline by setting the leader to -1 in zookeeper. This is because, today there is no way for an external tool to figure out the list of all offline partitions. If we were to build a Kafka cluster dashboard and list partitions and their metadata, we would want to know the leader for each partition. Until a new leader is elected, we continue to store the previous leader in zookeeper. If the partition goes offline and no new leader will ever come up, we still store the previous leader. This is not ideal and it might be worth to store some value like -1 to denote an offline partition

3. Cleaned up logging for a partition. There were several places in the code that used a custom string like ""[%s, %d]"" or ""(%s, %d)"" to print a partition. This makes it very hard to trace the state changes on a partition while troubleshooting. I changed everything in kafka.controller to standardize on the toString() API of TopicAndPartition. I'm assuming the rest of the code will get cleaned up as part of KAFKA-649;;;","22/Jan/13 03:11;junrao;Thanks for the patch. Overall, looks good. A couple of minor comments:

1. PartitionStateMachine: The patch sets the ISR in ZK to empty every time the partition goes offline. This means that, in the most common case when we can elect another broker as the leader, we will need to update ISR in ZK twice when the leader gone, the first time to set it to empty and the second time to set it to the new leader. I am wondering if we should set the ISR in ZK to empty in OfflinePartitionLeaderSelector.selectLeader() just before we throw a PartitionOfflineException. This way, in the common case, we avoid an extra ZK write.

2. UtilTest,testCsvList():     assertTrue(emptyStringList!=null) should probably be  assertTrue(emptyList!=null)
;;;","22/Jan/13 17:29;nehanarkhede;1. Good point. I moved setting the leader to -1 to the removeReplicaFromIsr API. Basically, if you are removing a leader from the isr, then just set the leader field to -1 and remove it from the isr

2. Included the fix;;;","23/Jan/13 19:54;junrao;There is a typo logIndent in Logging that causes a compilation error. Other than that, +1 for patch v2. Once the typo is fixed, the patch can be checked in.;;;","23/Jan/13 22:06;nehanarkhede;Fixed the typo, checked in patch v2. Thanks for the review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"broker appears to be encoding ProduceResponse, but never sending it",KAFKA-706,12627841,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,sriramsub,benfleis,benfleis,16/Jan/13 15:30,02/May/13 02:30,14/Jul/23 05:39,25/Feb/13 18:53,0.8.0,,,,,,,,,,core,,,0,,,,"By all appearances, I seem to be able to convince a broker to periodically encode, but never transmit, a ProduceResponse.  Unfortunately my client is proprietary, but I will share it with Neha via LI channels.  But I will describe what's going on in the hopes that there's another trivial way to reproduce it.  (I did search through JIRA, and haven't found anything that looks like this.)

I am running a single instance zookeeper and single broker.  I have a client that generates configurable amounts of data, tracking what is produced (both sent and ACK'd), and what is consumed.  I was noticing that when using high transfer rates via high frequency single messages, my unack'd queue appeared to be getting continuously larger.  So, I outfitted my client to log more information about correlation ids at various stages, and modified the kafka ProducerRequest/ProducerResponse to log (de)serialization of the same.  I then used tcpdump to intercept all communications between my client and the broker.  Finally, I configured my client to generate 1 message per ~10ms, each payload being approximately 33 bytes; requestAckTimeout was set to 2000ms, and requestAcksRequired was set to 1.  I used 10ms as I found that 5ms or less caused my unacked queue to build up due to system speed -- it simply couldn't keep up.  10ms keeps the load high, but just manageable.  YMMV with that param.  All of this is done on a single host, over loopback.  I ran it on both my airbook, and a well setup RH linux box, and found the same problem.

At startup, my system logged ""expired"" requests - meaning reqs that were sent, but for which no ACK, positive or negative, was seen from the broker, within 1.25x the requestAckTimeout (ie, 2500ms).  I would let it settle until the unacked queue was stable at or around 0.

What I found is this: ACKs are normally generated within milliseconds.  This was demonstrated by my logging added to the scala ProducerRe* classes, and they are normally seen quickly by my client.  But when the actual error occurs, namely that a request is ignored, the ProducerResponse class *does* encode the correct correlationId; however, a response containing that ID is never sent over the network, as evidenced by my tcpdump traces.  In my experience this would take anywhere from 3-15 seconds to occur after the system was warm, meaning that it's 1 out of several hundred on average that shows the condition.

While I can't attach my client code, I could attach logs; but since my intention is to share the code with LI people, I will wait to see if that's useful here.
","reproduced on both Mac OS and RH linux, via private node.js client",benfleis,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-736,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-736,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,304629,,,Mon Feb 25 18:53:59 UTC 2013,,,,,,,,,,"0|i17nsv:",252825,,,,,,,,,,,,,,,,,,,,"08/Feb/13 00:11;sriramsub;I think I know what is happening here.

Our current server is not suitable for async io yet from the client. As part of the processor thread, we continuously invoke processNewResponse on each iteration. processNewResponse does the following

1. dequeue the response from the response queue
2. set the interest bit of the selector key to write 
3. attach the response to the key

The problem is that we dont check if the previous response attached to the key has already been sent or not. We just replace the response and hence drop arbitrary responses. This should not happen with the v2 patch for KAFKA-736 since we would serialize the requests from a client.;;;","08/Feb/13 05:47;nehanarkhede;Great catch, Sriram ! I think the v2 patch on KAFKA-736 might solve this problem.;;;","22/Feb/13 07:49;sriramsub;It has been verified that v2 patch fixes this issue.Thank you Ben for your help with this bug.;;;","22/Feb/13 15:18;nehanarkhede;This bug will be fixed by the v2 patch for KAFKA-736;;;","25/Feb/13 18:53;sriramsub;The dependent bug has been fixed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controlled shutdown doesn't seem to work on more than one broker in a cluster,KAFKA-705,12627749,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,jjkoshy,nehanarkhede,nehanarkhede,15/Jan/13 22:57,16/Jul/13 06:43,14/Jul/23 05:39,16/Jul/13 06:43,0.8.0,,,,,,,,,,core,,,0,bugs,,,"I wrote a script (attached here) to basically round robin through the brokers in a cluster doing the following 2 operations on each of them -

1. Send the controlled shutdown admin command. If it succeeds
2. Restart the broker

What I've observed is that only one broker is able to finish the above successfully the first time around. For the rest of the iterations, no broker is able to shutdown using the admin command and every single time it fails with the error message stating the same number of leaders on every broker. 

",,jjkoshy,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/13 21:43;jjkoshy;kafka-705-incremental-v2.patch;https://issues.apache.org/jira/secure/attachment/12565853/kafka-705-incremental-v2.patch","18/Jan/13 21:37;jjkoshy;kafka-705-v1.patch;https://issues.apache.org/jira/secure/attachment/12565560/kafka-705-v1.patch","15/Jan/13 22:59;nehanarkhede;shutdown-command;https://issues.apache.org/jira/secure/attachment/12565029/shutdown-command","15/Jan/13 22:59;nehanarkhede;shutdown_brokers_eat.py;https://issues.apache.org/jira/secure/attachment/12565028/shutdown_brokers_eat.py",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,304537,,,Tue Jul 16 06:43:28 UTC 2013,,,,,,,,,,"0|i17n7z:",252731,,,,,,,,,,,,,,,,,,,,"15/Jan/13 22:59;nehanarkhede;Run it with the --help option to list the description of the command line options. ;;;","16/Jan/13 19:15;jjkoshy;I set up a local cluster of three brokers and created a bunch of topics, replication factor = 2. I was able to do multiple iterations of rolling bounces without
issue. Since this was local, I did not use your py script as it kills pid's returned by ps.

Would you by any chance be able to provide a scenario to reproduce this locally? That said, I believe John Fung also tried to reproduce this in a
distributed environment but was unable to do so; so I'll probably need to take a look at logs in your environment.
;;;","16/Jan/13 19:17;nehanarkhede;>> Would you by any chance be able to provide a scenario to reproduce this locally?

I would suggest you try out on a distributed environment that is setup on a large amount of partitions and traffic. Since it is internal, I can pass on the connection url to you.;;;","18/Jan/13 19:00;jjkoshy;I think this is why it happens:

https://github.com/apache/kafka/blob/03eb903ce223ab55c5acbcf4243ce805aaaf4fad/core/src/main/scala/kafka/controller/ReplicaStateMachine.scala#L150

It could occur as follows. Suppose there's a partition 'P' assigned to brokers x and y; leaderAndIsr = y, {x, y}
1. Controlled shutdown of broker x; leaderAndIsr -> y, {y}
2. After above completes, kill -15 and then restart broker x
3. Immediately do a controlled shutdown of broker y; so now y is in the list of shutting down brokers.

Due to the above, x will not start its follower to 'P' on broker y.

Adding sufficient wait time between (2) and (3) seems to address the issue (in your script there's no sleep), but we should handle it properly in the shutdown code.
Will think about a fix for that.
;;;","18/Jan/13 21:37;jjkoshy;Here's a simple fix.

I don't really see any good reason why we shouldn't allow starting
a fetcher to a broker that is shutting down but not completely
shut down yet if a leader still exists on that broker.
;;;","20/Jan/13 18:45;nehanarkhede;+1 on the fix. And there is a problem with the script I wrote. This fix is correct, but the script will fail because it uses the shutdown command in a way that is not recommended or intended. It shuts down one broker, restarts it, doesn't wait until the restart is completed and the first broker re-registers itself in zookeeper and proceeds to shutting down the next broker. Since the replication factor is 2, if both these brokers were the replicas for some partitions, they go into the under replicated state and the script is never able to shut any other broker down after that.

I think we should include this fix.;;;","21/Jan/13 21:37;jjkoshy;I committed the fix to 0.8 with a small edit: used the liveOrShuttingDownBrokers field.

Another small issue is that we send a stop replica fetchers to the shutting down broker even if
controlled shutdown did not complete. This ""prematurely"" forces the broker out of the ISR of those
partitions. I think it should be safe to avoid sending the stop replica request if controlled shutdown
has not completely moved leadership of partitions off the shutting down broker.
;;;","21/Jan/13 21:43;jjkoshy;Here is what I meant in my last comment.;;;","22/Jan/13 17:34;nehanarkhede;+1;;;","22/Jan/13 20:14;jjkoshy;Thanks for reviewing. I checked-in the incremental patch as well. Will leave this jira open for now until it can be verified.;;;","11/Jul/13 22:13;jkreps;Joel, this is done, no?;;;","16/Jul/13 06:43;jjkoshy;Yes we can close this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A fetch request in Fetch Purgatory can double count the bytes from the same delayed produce request,KAFKA-703,12627496,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriramsub,sriramsub,sriramsub,14/Jan/13 21:07,04/Sep/14 21:56,14/Jul/23 05:39,04/Sep/14 21:56,0.8.1,,,0.8.2.0,,,,,,,purgatory,,,0,,,,"When a producer request is handled, the fetch purgatory is checked to ensure any fetch requests are satisfied. When the produce request is satisfied we do the check again and if the same fetch request was still in the fetch purgatory it would end up double counting the bytes received.

Possible Solutions

1. In the delayed produce request case, do the check only after the produce request is satisfied. This could potentially delay the fetch request from being satisfied.
2. Remove dependency of fetch request on produce request and just look at the last logical log offset (which should mostly be cached). This would need the replica.fetch.min.bytes to be number of messages rather than bytes. This also helps KAFKA-671 in that we would no longer need to pass the ProduceRequest object to the producer purgatory and hence not have to consume any memory.",,guozhang,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1430,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,304280,,,Thu Sep 04 21:56:26 UTC 2014,,,,,,,,,,"0|i17l6v:",252402,,,,,,,,,,,,,,,,,,,,"21/Jan/13 19:31;sriramsub;Can we move this jira to the next version since we have decided to punt this?;;;","01/Feb/14 19:37;nehanarkhede;Moving to 0.8.2;;;","04/Sep/14 21:56;guozhang;This problem is resolved in the purgatory / API redesign: KAFKA-1583. Closing now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock between request handler/processor threads,KAFKA-702,12627479,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jkreps,jjkoshy,jjkoshy,14/Jan/13 18:58,16/Jan/13 21:42,14/Jul/23 05:39,16/Jan/13 18:01,0.8.0,,,0.8.0,,,,,,,network,,,0,bugs,,,"We have seen this a couple of times in the past few days in a test cluster. The request handler and processor threads deadlock on the request/response queues bringing the server to a halt

""kafka-processor-10251-7"" prio=10 tid=0x00007f4a0c3c9800 nid=0x4c39 waiting on condition [0x00007f46f698e000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f48c9dd2698> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
        at java.util.concurrent.ArrayBlockingQueue.put(ArrayBlockingQueue.java:252)
        at kafka.network.RequestChannel.sendRequest(RequestChannel.scala:107)
        at kafka.network.Processor.read(SocketServer.scala:321)
        at kafka.network.Processor.run(SocketServer.scala:231)
        at java.lang.Thread.run(Thread.java:619)


""kafka-request-handler-7"" daemon prio=10 tid=0x00007f4a0c57f000 nid=0x4c47 waiting on condition [0x00007f46f5b80000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f48c9dd6348> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
        at java.util.concurrent.ArrayBlockingQueue.put(ArrayBlockingQueue.java:252)
        at kafka.network.RequestChannel.sendResponse(RequestChannel.scala:112)
        at kafka.server.KafkaApis.handleProducerRequest(KafkaApis.scala:198)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:58)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:41)
        at java.lang.Thread.run(Thread.java:619)

This is because there is a cycle in the wait-for graph of processor threads and request handler threads. If the request handling slows down on a busy server, the request queue fills up. All processor threads quickly block on adding incoming requests to the request queue. Due to this, those threads do not processes responses filling up their response queues. At this moment, the request handler threads start blocking on adding responses to the respective response queues. This can lead to a deadlock where every thread is holding a lock on one queue and asking a lock for the other queue. This brings the server to a halt where it accepts connections but every request gets timed out.

One way to resolve this is by breaking the cycle in the wait-for graph of the request handler and processor threads. Instead of having the processor threads dispatching the responses, we can have one or more dedicated response handler threads that dequeue responses from the queue and write those on the socket. One downside of this approach is that now access to the selector will have to be synchronized.",,jjkoshy,jkreps,junrao,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/13 05:35;jkreps;KAFKA-702-v1.patch;https://issues.apache.org/jira/secure/attachment/12565076/KAFKA-702-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,304263,,,Wed Jan 16 19:13:43 UTC 2013,,,,,,,,,,"0|i17kun:",252347,,,,,,,,,,,,,,,,,,,,"15/Jan/13 05:23;junrao;I am wondering if the following approach will break the deadlock.

In RequestChannel.sendRequest(), instead of doing a blocking put(), we do offer() and return false if the queue is full. We propagate this flag to Processor.read(). Then, In Processor.run(), we only remove the selected key if read returns true. This way, read() will never block, which allows us to handle the responses even when the request queue is full.;;;","15/Jan/13 05:33;nehanarkhede;I like this idea better than having separate response threads and locking between the processor and response handler threads. In the non blocking request put approach, we will have to be careful when the queue is full to not select new ready keys when the previous ones have not been successfully enqueued. This can be a little tricky.;;;","16/Jan/13 05:35;jkreps;TLDR: attached one-line patch should fix the deadlock, but there is a larger issue (though not a blocker).

I don't like the idea of splitting response and request between different threads. The reason our socket server is simple is because each network thread is totally independent of all others, so there are no threading issues. Mixing these is slow (because of all the locking) and error prone. I have seen this done before and it is a big mess.

Jun's second idea is better, but its not as simple as described. We have to put the newly attached request somewhere and trigger a second attempt on adding it to the queue. Registering again for reading doesn't really work because there won't be more data to read. Registering for writing doesn't work because sockets are always writable so we would end up busy waiting. So to make this work we would need some kind of list where we stored requests that had been read but didn't fit in the queue. But we need something that will check this list periodically and it is hard to guarantee that that would happen with any more frequency that the poll timeout.

But I think we are muddling things a bit. Let's step back and think about this from first principles.

Why do queues have limits? The reason is to bound memory usage. So taking data off the socket and putting it in a list is silly, that defeats the original purpose of having the bound (the queue after all is just a list).

But think about this more. Why are we blocking adding responses to the response queue? The reason would be to bound memory usage. But the response queue doesn't actually bound memory usage. Things going into the response queue come either directly from processors or from purgatory, and in either case they are taking up memory there. Preventing responses from going out isn't helping anything.

So the short term fix is just to remove the bound on the response queue.

The larger problem is that regardless of this change in 0.8 *we aren't effectively bounding memory usage*. The reason is the purgatory. The purgatory will accumulate requests any time expiration gets slow. This could be due to a misconfigured client or due to a slow broker.

So the error is that we are using queue size to indicate ""backlog"" but really the proper measure of backlog is the total number of requests in flight including all requests in queues OR in purgatory.

But even once we understand the correct limit, it isn't clear what to do once we hit that limit. There are two choices: (1) stop taking new requests, (2) prematurely start responding to requests in the purgatory. Neither of these is great. Consider the case where one broker gets slow and umpteen produce requests pile up in purgatory. If we stop taking new requests that is like a GC pause, but since the timeout could be 30 seconds away it will be a long one. If we start dumping the purgatory prematurely we will have to respond with an error because we lack sufficient acknowledgements.;;;","16/Jan/13 07:20;nehanarkhede;Agree that a longer term fix is to be able to bound the total memory usage by counting the requests in flight in the purgatory. But when we do reach this limit, we should load shed from the purgatory like you suggested. One way of doing that is by implementing client quotas and shedding requests coming from the faulty clients. This might require more thought and probably a big refactoring. I think we should checkin this patch and tune GC and see how that goes.

+1 on patch v1;;;","16/Jan/13 15:56;junrao;First of all, +1 on the simple patch. I think it solves the immediate problem.

For socket selector, my understanding after reading the java doc is that the selected keys are always there unless you explicitly remove it. In other words, those selected keys won't magically go away after the socket key is being consumed. Everytime we call select(), only newly available keys are added and existing selected keys are untouched. So, even if we have finished reading from a socket, if the key is not automatically removed and select() will still give the same key back. However, my suggestion is probably worse than this patch. Before the request queue has space again, the processor thread could be doing the busy loop by keeping trying to add the same request from a socket to the request queue.

A second thing is that, currently, the number of outstanding requests on the broker is bounded by the number of clients since each client can have at most one outstanding request. So, if we bound the number of clients, we can somewhat bound the memory used by outstanding requests. This limit is probably useful for not running out of open file handlers too.;;;","16/Jan/13 17:08;sriramsub;I would like to add my thoughts to this. 

1. Load shedding arbitrary clients will bound the memory but would essentially cause the system to fail most of the requests and not recover from it till the load goes down. We have quite a few inter-dependencies between requests (producer depends on replica requests, replica depends on produce requests and consumer requests depend on produce requests) and dropping requests would essentially cause the requests depending on it to stay longer in the purgatory and fail. 

2. Having client quotas may not work because we do not have one faulty client. Each client can at most have only one request.

Few improvements might reduce the failure scenarios

1. Currently replica request wait on a hard limit (min bytes). Instead they could be made to return earlier to free the purgatory and accept more requests during high load scenarios.
2. Direct consumers to read from other replicas in the isr that have lesser load. This is going to be harder.

;;;","16/Jan/13 18:01;nehanarkhede;Checked this in to proceed with deployment;;;","16/Jan/13 19:13;nehanarkhede;>> 2. Having client quotas may not work because we do not have one faulty client. Each client can at most have only one request.

We understand that. Client quotas are better done probably in terms of expirations per second. Basically, if you setup your partitions with a large replication factor (let's say 6) and set the num.acks in your producer to -1. At the same time, if you set your timeout too low, all requests will timeout and expire. This will allow your client to send many requests that all timeout.

Load shedding needs more thought. It is not as straightforward and when we scope it out, we will need to obviously keep in mind consequences of load shedding.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsoleProducer does not exit correctly and fix some config properties following KAFKA-648,KAFKA-701,12627454,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,brugidou,brugidou,brugidou,14/Jan/13 16:14,14/Jan/13 17:22,14/Jul/23 05:39,14/Jan/13 17:22,0.8.0,,,0.8.0,,,,,,,config,core,,0,,,,"Just added a proper try/catch around the ConsoleProducer so that when an exception is thrown, the system exits (with error code 1)

In addition, KAFKA-648 broker some configs like request.enqueue.timeout.ms and zk.connection.timeout.ms that I fixed",,brugidou,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/13 16:14;brugidou;KAFKA-701.patch;https://issues.apache.org/jira/secure/attachment/12564729/KAFKA-701.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,304238,,,Mon Jan 14 17:22:44 UTC 2013,,,,,,,,,,"0|i17kgn:",252284,,,,,,,,,,,,,,,,,,,,"14/Jan/13 17:22;junrao;Thanks for the patch. Committed to 0.8 after reverting the change in system_test/ (the consumer property file there is used for an 0.7 consumer).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
broker may expose uncommitted data to a consumer,KAFKA-698,12627366,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jkreps,junrao,junrao,14/Jan/13 01:24,20/Jan/13 18:42,14/Jul/23 05:39,16/Jan/13 04:40,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"We saw the following error in the log during testing. The problem seems to be that when the high watermark was at offset 39021, the broker incorrectly exposed an uncommitted message (at offset 39022) to the client. This doesn't always happen, but can happen when certain conditions are met, which I should explain in the comments.

2013/01/11 00:54:42.059 ERROR [KafkaApis] [kafka-request-handler-2] [kafka] []  [KafkaApi-277] error when processing request (service_metrics,2,39022,2000000)
java.lang.IllegalArgumentException: Attempt to read with a maximum offset (39021) less than the start offset (39022).
        at kafka.log.LogSegment.read(LogSegment.scala:105)
        at kafka.log.Log.read(Log.scala:386)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSet(KafkaApis.scala:369)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:327)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:323)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map1.map(Map.scala:93)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSets(KafkaApis.scala:323)
        at kafka.server.KafkaApis$$anonfun$maybeUnblockDelayedFetchRequests$2.apply(KafkaApis.scala:165)
        at kafka.server.KafkaApis$$anonfun$maybeUnblockDelayedFetchRequests$2.apply(KafkaApis.scala:164)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.server.KafkaApis.maybeUnblockDelayedFetchRequests(KafkaApis.scala:164)
        at kafka.server.KafkaApis$$anonfun$handleProducerRequest$3.apply(KafkaApis.scala:186)
        at kafka.server.KafkaApis$$anonfun$handleProducerRequest$3.apply(KafkaApis.scala:185)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at kafka.server.KafkaApis.handleProducerRequest(KafkaApis.scala:185)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:58)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:41)
        at java.lang.Thread.run(Thread.java:619)
",,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/13 17:34;jkreps;KAFKA-698-v1.patch;https://issues.apache.org/jira/secure/attachment/12564738/KAFKA-698-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,304139,,,Tue Jan 15 01:40:35 UTC 2013,,,,,,,,,,"0|i17jnz:",252155,,,,,,,,,,,,,,,,,,,,"14/Jan/13 01:25;junrao;My analysis is the following. In LogSegment.read(), we have the following code that calculates the end position of the maxOffset. When maxOffset is the high watermark, we won't find its position in the file (since the offset is exclusive). Therefore, we read the size of the segment file as the end position. What can happen is that a new message gets appended btw we call translateOffset and messageSet.sizeInBytes(). That message (may be uncommitted) will then be incorrectly returned to the consumer.

    // calculate the length of the message set to read based on whether or not they gave us a maxOffset
    val length = 
      maxOffset match {
        case None =>
          // no max offset, just use the max size they gave unmolested
          maxSize
        case Some(offset) => {
          // there is a max offset, translate it to a file position and use that to calculate the max read size
          if(offset < startOffset)
            throw new IllegalArgumentException(""Attempt to read with a maximum offset (%d) less than the start offset (%d)."".format(offset, startOffset))
          val mapping = translateOffset(offset)
          val endPosition = 
            if(mapping == null)
              messageSet.sizeInBytes() // the max offset is off the end of the log, use the end of the file
            else
              mapping.position
          min(endPosition - startPosition.position, maxSize) 
        }
      }

I think the actual problem seems to be that in log.append, we advance nextoffset before the data is actually appended to the log. This causes the problem in log.read since it may see an offset that doesn't exist in the segment file yet. To fix this, we will need to advance nextOffset after the data is appended to the log.
;;;","14/Jan/13 17:34;jkreps;Attached is a hacky but small patch that should fix this.

The proper fix is to refactor ByteBufferMessageSet to no longer take an AtomicLong directly, but this is not very straight-forward  and will be very conflict prone with trunk. So let's hack it for now and I will file a follow up ticket to refactor this.;;;","14/Jan/13 19:32;junrao;+1 on the patch.;;;","15/Jan/13 01:40;nehanarkhede;+1 on the patch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ConsoleConsumer throws InvalidConfigException for ""."" in client id",KAFKA-697,12627316,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,jfung,jfung,12/Jan/13 16:21,15/Jan/13 17:47,14/Jul/23 05:39,15/Jan/13 17:46,,,,,,,,,,,,,,0,,,,"kafka.common.InvalidConfigException: client.id console-consumer-16946-ConsumerFetcherThread-console-consumer-16946_XXXX-host18.corp-1358006528116-991e3f2d-0-1 is illegal, contains a character other than ASCII alphanumerics, _ and -
        at kafka.common.Config$class.validateChars(Config.scala:32)
        at kafka.consumer.ConsumerConfig$.validateChars(ConsumerConfig.scala:25)
        at kafka.consumer.ConsumerConfig$.validateClientId(ConsumerConfig.scala:55)
        at kafka.consumer.SimpleConsumer.<init>(SimpleConsumer.scala:88)
        at kafka.server.AbstractFetcherThread.<init>(AbstractFetcherThread.scala:44)
        at kafka.consumer.ConsumerFetcherThread.<init>(ConsumerFetcherThread.scala:27)
        at kafka.consumer.ConsumerFetcherManager.createFetcherThread(ConsumerFetcherManager.scala:93)
        at kafka.server.AbstractFetcherManager.addFetcher(AbstractFetcherManager.scala:44)
        at kafka.consumer.ConsumerFetcherManager$$anon$1$$anonfun$doWork$3.apply(ConsumerFetcherManager.scala:75)
        at kafka.consumer.ConsumerFetcherManager$$anon$1$$anonfun$doWork$3.apply(ConsumerFetcherManager.scala:72)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
        at kafka.consumer.ConsumerFetcherManager$$anon$1.doWork(ConsumerFetcherManager.scala:72)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)
",,jfung,junrao,nehanarkhede,sriramsub,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-704,,,,,,,,,,,,,,,,,,,"15/Jan/13 01:38;swapnilghike;kafka-697-allow-dot.patch;https://issues.apache.org/jira/secure/attachment/12564837/kafka-697-allow-dot.patch","13/Jan/13 21:51;junrao;kafka-697.patch;https://issues.apache.org/jira/secure/attachment/12564635/kafka-697.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,304089,,,Tue Jan 15 17:47:36 UTC 2013,,,,,,,,,,"0|i17jaf:",252094,,,,,,,,,,,,,,,,,,,,"12/Jan/13 22:09;swapnilghike;Currently we allow only alphanumerics, - and _ in the clientId, topic name and groupId. The topic name validation criteria were decided in KAFKA-495 and later while deciding to validate clientId and groupId in KAFKA-646, we tried to maintain consistency with the topic name validation. 

As discussed in KAFKA-495, we don't have any strong reason to disallow dot in the validation, but we did not see any strong reason to make special case exceptions at the time as well and decided to make the validation criteria strict.

For now, can we change the system test to not use dot? We can add it to the set of legal characters later though if more use cases arise.;;;","13/Jan/13 21:51;junrao;In kafka-683 we changed client.id for a consumer to be config.clientId + ""-"" + consumerIdString + fetcherId + sourceBroker.id. ConsumerIdString has the format of consumerHostName + currentTimeMillis + randomUUID. The problem is that consumerHostName may have ""."", which breaks the naming convention. However, such a client.id is too verbose.  For debugging, we need to know the client IP address. We can fix that for every type of request in a separate jira. With client IP address and correlation id, we should have enough information for debugging. So, for now, just attach a patch to revert client.id to just config.ClientId.;;;","14/Jan/13 16:44;jfung;Tested kafka-697.patch and it fixes the exception. Ran a full System Test and looks good.;;;","15/Jan/13 00:39;nehanarkhede;Unfortunately, this rule of not allowing a '.' in the clientid seems very restrictive. Here, it is useful for the client id to have the hostname of the fetcher thread. This allows us to look at the kafka request log, take that client id, go the box, take a thread dump and correlate the behavior of the thread from the thread dump and the log4j using the client id string. This saves time required for troubleshooting. Since we have this restriction, we will have to take the client IP from the request log, convert it to a hostname, construct the thread name from this information and then try to understand the thread behavior from the thread dump and log4j. Also, one needs to know Kafka code to figure out how to construct the thread name from all this information, which is even worse.

I think this is making things more difficult for us, which is a down side to disallowing '.' in the client id.;;;","15/Jan/13 00:45;sriramsub;+1 on Neha's comment. It is simpler to allow '.' rather than getting the ip and adding code to pass that as part of the request object.;;;","15/Jan/13 01:38;swapnilghike;Thanks for your comments. I agree that allowing dot will make our lives easier.

In the attached patch, I have added dot to the regexes in Topic and Config classes. The regexes have also been modified to escape the last hyphen so that in the future if someone accidentally added a character at the end of the regex group, then the hyphen will not be treated as a range identifier.

Topic has been modified to disallow ""."" and "".."" as topic names.

TopicTest and ConfigTest have been modified accordingly.

After including this patch, we can keep Jun's patch or discard it depending on what is convenient to keep in clientId.

John, can you please check if this patch solves the problem (after reverting Jun's patch so that the root cause is reintroduced)? ;;;","15/Jan/13 16:35;junrao;The new patch looks good to me. We can just apply this patch once John verifies the system tests are ok.;;;","15/Jan/13 17:05;jfung;Thanks Swapnil for the latest patch.

The patch is sanity tested in a single box in which the hostname doesn't have a ""."" and in a distributed env where the host names have a ""."" (besides the parent domain name). In both cases, the patch works fine.;;;","15/Jan/13 17:35;nehanarkhede;+1 on the new patch, thanks Swapnil!;;;","15/Jan/13 17:46;nehanarkhede;Committed patch to 0.8 branch;;;","15/Jan/13 17:47;junrao;Thanks for the latest patch. Committed to 0.8 and merged to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix toString() API for all requests to make logging easier to read,KAFKA-696,12627266,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,sriramsub,nehanarkhede,nehanarkhede,11/Jan/13 22:08,24/Jan/13 19:52,14/Jul/23 05:39,24/Jan/13 19:52,0.8.0,,,,,,,,,,,,,0,,,,"It will be useful to have consistent logging styles for all requests. Right now, we depend on the default toString implementation and the problem is that it is very hard to read and prints out unnecessary information like the ByteBuffer.",,junrao,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/13 00:09;sriramsub;cleanup-v1.patch;https://issues.apache.org/jira/secure/attachment/12565044/cleanup-v1.patch","22/Jan/13 00:33;sriramsub;cleanup-v2.patch;https://issues.apache.org/jira/secure/attachment/12565881/cleanup-v2.patch","24/Jan/13 19:38;sriramsub;cleanup-v3.patch;https://issues.apache.org/jira/secure/attachment/12566355/cleanup-v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,304036,,,Thu Jan 24 19:52:25 UTC 2013,,,,,,,,,,"0|i17hrj:",251847,,,,,,,,,,,,,,,,,,,,"16/Jan/13 00:09;sriramsub;- Added ToString overrides to all the request object to make logging more readable and ignore things that we dont care
- Errors are handled by each request and removed the case statements in kafkaapi.;;;","20/Jan/13 23:46;junrao;Thanks for the patch. Some comments:

1. RequestOrResponse.handleError: We probably shouldn't couple the error response generation and the sending of the response through RequestChannel. It's probably better if we change handleError() to sth like generateErrorResponse() that only takes an error code and generates a response. The sending of the response will be done in KafkaApis. Similarly, I think the error logging should be done in KafkaApis.handle() too.

2. ProducerRequest: When printing out data, in addition to the key, it would be useful to print out messageSet.sizeInBytes().

3. KafkaApis: 
3.1 Doesn't compile.
3.2 There are statements like the following in the handling of each type of request. Could we move them to handle() and remove the special printing for versionId, correlationId, and clientId?
    if(requestLogger.isTraceEnabled)
      requestLogger.trace(""Handling LeaderAndIsrRequest v%d with correlation id %d from client %s: %s""
            .format(leaderAndIsrRequest.versionId, leaderAndIsrRequest.correlationId, leaderAndIsrRequest.clientId, leaderAndIsrRequest.toString))

4. RequestChannel.Request: Could you remove versionId, correlationId, clientId in the trace statement in updateRequestMetrics() and remove the corresponding deserialization logic?
;;;","22/Jan/13 00:33;sriramsub;1. The reason why I did not do that was because ReplicaRequest does not extend from RequestReponse. I kept what was there as is but moved the logging of the request outside to kafkaapi.
2. ProducerRequest logs the size for each message set.
3.1 It was an old patch and looks like there were conflicts.
3.2 Each request also prints the request name now. Hence moved these lines to the parent method.
4. Removed the deserializations and just log the request object.;;;","24/Jan/13 04:16;nehanarkhede;Sorry for a late review. Following are a few review suggestions on patch v2 -
1. FetchRequest
1.1 Remove unused imports ""Response"" and ""BoundedByteBufferSend""
1.2 In the toString() API, for MaxWait, lets append ""ms"" after the value. Also for MinBytes, lets append ""bytes"" after the value

2. KafkaApis
2.1 In handle() APIs, let's move the .format(request.requestObj)) to the previous line

3. LeaderAndIsrRequest
Let's include ""ms"" after the value for AckTimeoutMs

4. ProducerRequest
Let's include ""ms"" after the value for AckTimeoutMs

5. StopReplicaRequest
5.1 Let's include ""ms"" after the value for AckTimeoutMs
5.2 Looks like we haven't standardized StopReplicaRequest to use TopicAndPartition instead of a custom Tuple for representing partitions. This messes up the log4j logging style and the toString() API will not print partitions the way rest of the code does. You can either include it in your patch or leave it for KAFKA-649
;;;","24/Jan/13 19:38;sriramsub;- Added units to the logging
- Did not change StopReplicaRequest to using TopicAndPartition. Leaving it to 649.;;;","24/Jan/13 19:52;nehanarkhede;Committed v3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker shuts down due to attempt to read a closed index file,KAFKA-695,12627225,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,nehanarkhede,nehanarkhede,11/Jan/13 18:17,06/Feb/13 04:02,14/Jul/23 05:39,06/Feb/13 04:02,0.8.0,,,0.8.0,,,,,,,log,,,0,p1,,,"Broker shuts down with the following error message -


013/01/11 01:43:51.320 ERROR [KafkaApis] [request-expiration-task] [kafka] []  [KafkaApi-277] error when processing request (service_metrics,2,39192,2000000)
java.nio.channels.ClosedChannelException
        at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)
        at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:613)
        at kafka.log.FileMessageSet.searchFor(FileMessageSet.scala:82)
        at kafka.log.LogSegment.translateOffset(LogSegment.scala:76)
        at kafka.log.LogSegment.read(LogSegment.scala:106)
        at kafka.log.Log.read(Log.scala:386)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSet(KafkaApis.scala:369)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:327)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:323)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map1.map(Map.scala:93)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSets(KafkaApis.scala:323)
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:519)
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:501)
        at kafka.server.RequestPurgatory$ExpiredRequestReaper.run(RequestPurgatory.scala:222)
        at java.lang.Thread.run(Thread.java:619)
2013/01/11 01:43:52.815 INFO [Processor] [kafka-processor-10251-2] [kafka] []  Closing socket connection to /172.20.72.244.
2013/01/11 01:43:54.286 INFO [Processor] [kafka-processor-10251-3] [kafka] []  Closing socket connection to /172.20.72.243.
2013/01/11 01:43:54.385 ERROR [LogManager] [kafka-logflusher-1] [kafka] []  [Log Manager on Broker 277] Error flushing topic service_metrics
java.nio.channels.ClosedChannelException
        at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)
        at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:349)
        at kafka.log.FileMessageSet$$anonfun$flush$1.apply$mcV$sp(FileMessageSet.scala:154)
        at kafka.log.FileMessageSet$$anonfun$flush$1.apply(FileMessageSet.scala:154)
        at kafka.log.FileMessageSet$$anonfun$flush$1.apply(FileMessageSet.scala:154)
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
        at kafka.log.FileMessageSet.flush(FileMessageSet.scala:153)
        at kafka.log.LogSegment.flush(LogSegment.scala:151)
        at kafka.log.Log.flush(Log.scala:493)
        at kafka.log.LogManager$$anonfun$kafka$log$LogManager$$flushDirtyLogs$2.apply(LogManager.scala:319)
        at kafka.log.LogManager$$anonfun$kafka$log$LogManager$$flushDirtyLogs$2.apply(LogManager.scala:310)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:474)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
        at scala.collection.JavaConversions$JCollectionWrapper.foreach(JavaConversions.scala:495)
        at kafka.log.LogManager.kafka$log$LogManager$$flushDirtyLogs(LogManager.scala:310)
        at kafka.log.LogManager$$anonfun$startup$2.apply$mcV$sp(LogManager.scala:144)
        at kafka.utils.Utils$$anon$2.run(Utils.scala:66)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
2013/01/11 01:43:54.447 FATAL [LogManager] [kafka-logflusher-1] [kafka] []  [Log Manager on Broker 277] Halting due to unrecoverable I/O error while flushing logs: null
java.nio.channels.ClosedChannelException
        at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)
        at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:349)
        at kafka.log.FileMessageSet$$anonfun$flush$1.apply$mcV$sp(FileMessageSet.scala:154)
        at kafka.log.FileMessageSet$$anonfun$flush$1.apply(FileMessageSet.scala:154)
        at kafka.log.FileMessageSet$$anonfun$flush$1.apply(FileMessageSet.scala:154)
        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
        at kafka.log.FileMessageSet.flush(FileMessageSet.scala:153)
        at kafka.log.LogSegment.flush(LogSegment.scala:151)
        at kafka.log.Log.flush(Log.scala:493)

       at kafka.log.LogSegment.flush(LogSegment.scala:151)
        at kafka.log.Log.flush(Log.scala:493)
        at kafka.log.LogManager$$anonfun$kafka$log$LogManager$$flushDirtyLogs$2.apply(LogManager.scala:319)
        at kafka.log.LogManager$$anonfun$kafka$log$LogManager$$flushDirtyLogs$2.apply(LogManager.scala:310)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.JavaConversions$JIteratorWrapper.foreach(JavaConversions.scala:474)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
        at scala.collection.JavaConversions$JCollectionWrapper.foreach(JavaConversions.scala:495)
        at kafka.log.LogManager.kafka$log$LogManager$$flushDirtyLogs(LogManager.scala:310)
        at kafka.log.LogManager$$anonfun$startup$2.apply$mcV$sp(LogManager.scala:144)
        at kafka.utils.Utils$$anon$2.run(Utils.scala:66)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
2013/01/11 01:43:54.512 INFO [ComponentsContextLoaderListener] [Thread-2] [kafka] []  Shutting down...
",,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-719,,,,,,,,,,,,,,,,,,,,,"27/Jan/13 23:10;junrao;kafka-695.patch;https://issues.apache.org/jira/secure/attachment/12566696/kafka-695.patch","30/Jan/13 19:06;junrao;kafka-695_followup.patch;https://issues.apache.org/jira/secure/attachment/12567187/kafka-695_followup.patch","05/Feb/13 09:08;junrao;kafka-695_followup_v2.patch;https://issues.apache.org/jira/secure/attachment/12567980/kafka-695_followup_v2.patch","28/Jan/13 22:18;junrao;kafka-695_v2.patch;https://issues.apache.org/jira/secure/attachment/12566836/kafka-695_v2.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,303995,,,Tue Feb 05 16:46:55 UTC 2013,,,,,,,,,,"0|i17hd3:",251782,,,,,,,,,,,,,,,,,,,,"13/Jan/13 04:00;jkreps;Hmm, so this can happen. Currently in 0.8 when we delete a file there is nothing that block reads on that file. The assumption is that this only happens at the tail of the log and is rare. This bug problem is fixed on trunk by the async delete patch.

However the question is whether this is really what is happening...?;;;","13/Jan/13 22:36;junrao;The weird thing is that the file actually exists on disk (this is a low volume topic that has only 1 segment) and there was no logging that shows the segment has been deleted.;;;","22/Jan/13 19:45;jkreps;Some research:
1. Taking also KAFKA-719 we have examples of this happening in the background flush, the read, and in append(). Flush and append only happen on the active segment so that complicates things.
2. The FileChannel in FileMessageSet is private so the only way to close it is to either call close() or delete() on the message set.
3. Since delete() first calls close() it is hard to say which code path was taken (you would get the same error message).
4. The only call to close() is LogSegment.close() the only call to that is in Log.close() so that's not it.
5. There are several calls to delete() but all are inside the lock except for deleteSegments.
6. deleteSegments should, intuitively, not delete the active segment since we force a segment roll if needed inside the lock inside markDeleteWhile()
7. But it is possible for deleteSegments to collide with truncateTo, but not sure how feasible this is.;;;","24/Jan/13 19:30;junrao;Another piece of info is that the log segment seems to exist after the broker is shut down (it's loaded on broker restart). So, it didn't seem that the segment was deleted. After restart, the same problem shows up after the broker is running for about 1 day.;;;","27/Jan/13 23:10;junrao;I have a theory of what's happening here. What we overlooked is that there is another possibility for us to get a closed channel, other than explicitly closing it. If a thread is in the middle of a read/write of a file channel and the thread is interrupted. The channel will be closed automatically. I guess the following is what has happened.

The ExpiredRequestReaper thread is in the middle of expiring a FetchRequest and gets interrupted by ExpiredRequestReaper.forcePurge(). When the interruption occurs, the reaper thread could be reading the file channel (see stracktrace below). This will cause the file channel to be closed. All subsequent reads and writes on this file channel will fail due to ClosedChannelException.

java.nio.channels.ClosedChannelException
        at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)
        at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:613)
        at kafka.log.FileMessageSet.searchFor(FileMessageSet.scala:83)
        at kafka.log.LogSegment.translateOffset(LogSegment.scala:76)
        at kafka.log.LogSegment.read(LogSegment.scala:91)
        at kafka.log.Log.read(Log.scala:390)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSet(KafkaApis.scala:372)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:330)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:326)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map1.map(Map.scala:93)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSets(KafkaApis.scala:326)
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:528)
        at kafka.server.KafkaApis$FetchRequestPurgatory.expire(KafkaApis.scala:510)
        at kafka.server.RequestPurgatory$ExpiredRequestReaper.run(RequestPurgatory.scala:222)
        at java.lang.Thread.run(Thread.java:619)

Attach a quick fix. The problem is that we shouldn't be using interrupt to communicate with the ExpiredRequestReaper thread since it has dangerous side effects. The patch basically uses a boolean flag to indicate that a full purge is needed and changes the ExpiredRequestReaper thread not to block for more than 500ms (so that it gets a chance to do the full purge).

Not sure what's the best way to unit test this though.;;;","28/Jan/13 04:20;jkreps;This is a pretty brilliant catch.

That code should not be there. It looks like the interrupt from shutdown (which is okay maybe cause you are shutting down) somehow got pasted into elsewhere.;;;","28/Jan/13 16:37;junrao;So, do you think the patch is ok or do you plan to provide a better patch?;;;","28/Jan/13 16:44;jkreps;I'm not sure. I think I need to rewind back to what forcePurge is trying to do. From what I see I think we are replaying an immediate interrupt with a 500ms check which may be fine, though obviously its higher latency. The purpose of forcePurge is to try to clean out dead memory?;;;","28/Jan/13 18:10;nehanarkhede;Great catch! One question just to see if I understood the problem correctly. So the problem happens if the forcePurge() interrupts the expired request reaper while reading message set from the log. In that case, we should also see ""error when processing request.."" in the kafka log, because we catch all throwables in the readMessageSet() API. Do you see that ?

>> The purpose of forcePurge is to try to clean out dead memory?

Yes, this was added as part of fixing KAFKA-664

I think the patch looks good, it is fine to delay the full purge by a few ms.;;;","28/Jan/13 18:21;nehanarkhede;The LogRecoveryTest throws the following new error messages now -

[2013-01-28 10:15:54,534] ERROR ExpiredRequestReaper-0 Error in long poll expiry thread:  (kafka.server.RequestPurgatory$ExpiredRequestReaper:102)
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1961)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2038)
	at java.util.concurrent.DelayQueue.poll(DelayQueue.java:209)
	at kafka.server.RequestPurgatory$ExpiredRequestReaper.pollExpired(RequestPurgatory.scala:269)
	at kafka.server.RequestPurgatory$ExpiredRequestReaper.run(RequestPurgatory.scala:221)
	at java.lang.Thread.run(Thread.java:680)
[2013-01-28 10:15:54,534] ERROR ExpiredRequestReaper-0 Error in long poll expiry thread:  (kafka.server.RequestPurgatory$ExpiredRequestReaper:102)
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:1961)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2038)
	at java.util.concurrent.DelayQueue.poll(DelayQueue.java:209)
	at kafka.server.RequestPurgatory$ExpiredRequestReaper.pollExpired(RequestPurgatory.scala:269)
	at kafka.server.RequestPurgatory$ExpiredRequestReaper.run(RequestPurgatory.scala:221)
	at java.lang.Thread.run(Thread.java:680)
;;;","28/Jan/13 22:18;junrao;Attach patch v2. Simplified the logic a bit. ExpiredRequestReaper now checks if we need to do a full purge directly (since it wakes up periodically) and RequestPurgatory.watch() just increments the request count. Also, reduced the wait from 500ms to 200ms.

The new error in LogRecoveryTest happens when we interrupt the ExpiredRequestReaper thread during shutdown. So, this is fine. The test still fails transiently, but the problem has been there for some time.;;;","28/Jan/13 23:14;nehanarkhede;+1;;;","29/Jan/13 16:57;junrao;Just realized there are a couple of other corner cases that we need to handle. First, we shut down ExpiredRequestReaper by interrupting the thread. This of course, can close a filechannel, which can cause a KafkaStorageException during a subsequent log append. This means that we may unnecessarily do an unclean shutdown. Second, the replicaFetcher thread is shut down through interruption too. When the thread is interrupted, it can be in the middle of a log append. Similarly, this means another unnecessary unclean shutdown.

We can remove interruption during shutdown and just rely on the isRunning flag. This is fine for the first case since ExpiredRequestReaper wakes up every 200ms. In the second case, this means that we may have to wait for the last outstanding fetch request to complete. This can potentially make shutdown longer. Not sure if this is a big concern.;;;","30/Jan/13 19:06;junrao;Attach a follow up patch.

1. Make ExpiredRequestReaper a non-daemon thread since we want to shut it down explicitly. Remove interrupt since the thread now wakes up periodically.

2. AbstractFetcher: Make it non-interruptible. Just wait for the last fetch request to complete.

3. ReplicaFetcher: handle KafkaStorageException properly.;;;","30/Jan/13 19:06;junrao;reopen it to deal with followup issue.s;;;","01/Feb/13 18:15;nehanarkhede;Thanks for the follow up patch, most of the changes are good. I have a few questions -

1. ReplicaFetcherThread
Shouldn't we also catch Throwable and at least log an error saying why the fetcher thread died ? Otherwise, if there is some code bug, it will die anyway, but we will not know the reason why.

2. AbstractFetcherThread
Why are we overriding isInterruptible in AbstractFetcherThread. I think you want ReplicaFetcherThread to be uninterruptible, but ConsumerThread can be interruptible, no ?

3. RequestPurgatory
Shouldn't expiration thread be a daemon thread ? We don't really want the expiration thread to block the JVM from shutting down, do we ?;;;","05/Feb/13 09:08;junrao;Attach followup patch v2.;;;","05/Feb/13 09:15;junrao;1. Any unexpected exception in ReplicaFetcherThread will kill the thread and the error will be logged in ShutdownableThread.

2. This is a good point and is fixed in the v2 patch.

3. We are shutting down the expiration thread explicitly. If the thread can't be shut down, it indicates a bug. It's probably better to expose the bug than making this a daemon thread.

So, the only change in patch v2 is to address #2.;;;","05/Feb/13 16:11;nehanarkhede;1,3. Makes sense and the follow up patch v2 looks good. 
+1;;;","05/Feb/13 16:22;jkreps;Can we just remove the interrupt entirely instead of making it optional. Unless we want to write tests for both cases and maintain this functionality in perpetuity...seems like we should just stop using interrupt?;;;","05/Feb/13 16:46;junrao;It's possible. However, some subclasses of ShutdownableThread such as ConsumerFetchThread does blocking writes to a queue. So, if the queue is full, not sure if we can shut down the consumer thread without sending interrupts.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ProducerPerformance doesn't take ""compression-codec"" from command line",KAFKA-694,12627217,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jfung,jfung,jfung,11/Jan/13 17:37,11/Jan/13 18:11,14/Jul/23 05:39,11/Jan/13 18:08,,,,,,,,,,,,,,0,,,,The argument value from command line is passed into a different variable.,,jfung,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/13 17:40;jfung;kafka-694-v1.patch;https://issues.apache.org/jira/secure/attachment/12564455/kafka-694-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,303987,,,Fri Jan 11 18:11:08 UTC 2013,,,,,,,,,,"0|i17hbb:",251774,,,,,,,,,,,,,,,,,,,,"11/Jan/13 17:40;jfung;Uploaded kafka-694-v1.patch to remove variable compressionCodecOption which will make compression-codec always assigned to default;;;","11/Jan/13 17:53;nehanarkhede;I suggest we take a string compression code that should be either none,gzip or snappy. This will make it easier for the users to run producer performance with compression;;;","11/Jan/13 18:11;nehanarkhede;oops, wrong jira;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer rebalance fails if no leader available for a partition and stops all fetchers,KAFKA-693,12627146,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,brugidou,brugidou,brugidou,11/Jan/13 09:22,24/Jan/13 04:53,14/Jul/23 05:39,17/Jan/13 18:16,0.8.0,,,0.8.0,,,,,,,core,,,1,p2,,,"I am currently experiencing this with the MirrorMaker but I assume it happens for any rebalance. The symptoms are:

I have replication factor of 1

1. If i start the MirrorMaker (bin/kafka-run-class.sh kafka.tools.MirrorMaker --consumer.config mirror-consumer.properties  --producer.config mirror-producer.properties --blacklist 'xdummyx' --num.streams=1 --num.producers=1) with a broker down
1.1 I set the refresh.leader.backoff.ms to 600000 (10min) so that the ConsumerFetcherManager doesn't retry to often to get the unavailable partitions
1.2 The rebalance starts at the init step and fails: Exception in thread ""main"" kafka.common.ConsumerRebalanceFailedException: KafkaMirror_mirror-01-1357893495345-fac86b15 can't rebalance after 4 retries
1.3 After the exception, everything stops (fetchers and queues)
1.4 I attached the full logs (info & debug) for this case

2. If i start the MirrorMaker with all the brokers up and then kill a broker
2.1 The first rebalance is successful
2.2 The consumer will handle correctly the broker down and stop the associated ConsumerFetcherThread
2.3 The refresh.leader.backoff.ms to 600000 works correctly
2.4 If something triggers a rebalance (new topic, partition reassignment...), then we go back to 1., the rebalance fails and stops everything.

I think the desired behavior is to consumer whatever is available, and try later at some intervals. I would be glad to help on that issue although the Consumer code seems a little tough to get on.",,brugidou,junrao,kamaradclimber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-691,,,,,,,,,,"16/Jan/13 08:48;brugidou;KAFKA-693-v2.patch;https://issues.apache.org/jira/secure/attachment/12565097/KAFKA-693-v2.patch","17/Jan/13 13:52;brugidou;KAFKA-693-v3.patch;https://issues.apache.org/jira/secure/attachment/12565320/KAFKA-693-v3.patch","15/Jan/13 12:41;brugidou;KAFKA-693.patch;https://issues.apache.org/jira/secure/attachment/12564916/KAFKA-693.patch","11/Jan/13 09:22;brugidou;mirror.log;https://issues.apache.org/jira/secure/attachment/12564382/mirror.log","11/Jan/13 09:22;brugidou;mirror_debug.log;https://issues.apache.org/jira/secure/attachment/12564383/mirror_debug.log",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,303915,,,Thu Jan 17 18:16:17 UTC 2013,,,,,,,,,,"0|i17guv:",251700,,,,,,,,,,,,,,,,,,,,"13/Jan/13 22:32;junrao;Ok. This is actually a real problem. During rebalance, we actually try to get the leader even though we don't really need it at rebalancing time. The fix seems easy.

In ZKRebalancerListener.addPartitionTopicInfo(), we don't really need to get the leaderId, which is not used in PartitionTopicInfo. So, we can just get rid of that code. We can also get rid of the code in rebalance() that computes leaderIdForPartitionsMap.;;;","14/Jan/13 15:27;brugidou;I looked up the code in details and I am stuck because during the rebalance() operation, the ZookeeperConsumerConnector's topicRegistry is updated with some PartitionTopicInfo that needs to store the consumerOffset and fetchOffset. During addPartitionTopicInfo(), the consumer offset is read from Zookeeper, however it needs to be initialized if no offsetString is available on Zookeeper (first time starting a consumer), and we need to access the broker/leader to get the start offset (using SimpleConsumer.earliestOrLatestOffset() in addPartitionTopicInfo()).

I digged a bit and we could probably initialize the offset later in the ConsumerFetcherManager? I could help with a patch if i get general directions because i'm not 100% familiar with the codebase yet.;;;","14/Jan/13 16:37;junrao;That's a good point. I overlooked this. Your understanding is correct. We could move the offset initialization logic into AbstractFetcherThread. The following is one way to do this. Not sure if this is the best way.

1. In AbstractFetcher: 
Change addPartition to pass in initialOffset: Option[Long].
If initialOffset is none, we call handleOffsetOutOfRange to get the offset. If we hit any exception while doing this, we pass the exception to the caller without adding the partition to partitionMap.

2. In ConsumerFetcherManager.doWork():
If we hit any exception when calling addFetcher, we add the partition back to noLeaderPartitionSet.

3. In ConsumerFetcherThread.handleOffsetOutOfRange():
We need to check if the offset response has any error. If so, we throw an exception to the caller.

4. In ZookeeperConsumerConnector.addPartitionTopicInfo(): If initial offset doesn't exist in ZK, we pass in none to PartitionTopicInfo.

5. In PartitionTopicInfo: Make fetchedOffset Option[AtomicLong].
;;;","14/Jan/13 18:18;brugidou;Looks good, It should work but I still have a pain point about PartitionTopicInfo that uses AtomicLong to track consume/fetch offsets. Using Option[AtomicLong] looks strange, because I have to change the 2 counters and make them variables... And it's probably not thread safe at all so I would need some sort of lock to ""initialize"" the counters.;;;","14/Jan/13 21:29;junrao;Another quick thought on this. Instead of using Option for offset, we could still use AtomicLong and pass in sth like -1 to indicate a non-exist offset.;;;","15/Jan/13 12:54;brugidou;Here is a patch:

1. AbstractFetchThread.addPartition(): call handleOffsetOutOfRange if initialOffset < 0

2. I didnt touch ConsumerFetcherManager.doWork() since addFetcher() is called for partitions with leaders only (which is why 3 is unnecessary).

3. ConsumerFetcherThrad.handleOffsetOutOfRange: check partitionErrorAndOffset.error and throw appropriate exception (which should have been done anyway, I don't think this is necessary for the patch)
3.1 Note: this should probably be done in the ReplicaFetcherThread too?

4. ZookeeperConsumerConnector.ZkRebalanceListener: Do not compute leaderIdForPartitionMap in rebalance() and set PartitionTopicInfo offsets to -1 if not in Zk (new consumer)

5. PartitionTopicInfo: removed brokerId

6. Fixed tests for compilation (I am having a hard time running tests since ./sbt test does not seem to work for me very well)

7. Should we increase the default refresh.leader.backoff.ms ? It's tradeoff between being able to pick fast a new leader to consume (useful when replication is on) and not flooding the broker when there is no leader (or replication is off). 200ms is very short, but something hybrid like ""try 5 times at 200ms backoff, then every 5min"" would get all use cases.

I am running this on test clusters with a mirrormaker andthe error that I had in my initial test case (in the description) does not occur anymore.;;;","15/Jan/13 18:12;junrao;Thanks for the patch. Some comments:

10. ZookeeperConsumerConnector: Let's define a constant InvalidOffset, instead of using -1 directly.

11. ConsumerFetcherManager.doWork(): After we identify the leader of a partition, the leader could change immediately. So, we may hit the exception when calling addFetcher(). When this happens, we haven't added the partition to the fetcher and we don't want to lose it. So, we should add it back to noLeaderPartitionSet so that we can find the new leader later.

12. ReplicaFetcherThread: Yes, it should also throw an exception if getOffsetBefore returns an error.

13. AbstractFetcherThread.doWork(): We need to handle the exception when calling handleOffsetOutOfRange(). If we get an exception, we should add the partition to partitionsWithError. This will cover both ConsumerFetcherThread and ReplicaFetcherThread.
;;;","16/Jan/13 08:50;brugidou;10. Created PartitionTopicInfo.InvalidOffset

11. In ConsumerFetcherManager.doWork(), I believe that addFetcher() is called before the partition is removed from noLeaderPartitionSet, if an exception is caught the partition will still be in the noLeaderPartitionSet, so I didn't change anything

12. done

13. done;;;","16/Jan/13 17:46;junrao;Thanks for patch v2. Looks good. Some minor comments:

11. I think we still need to change ConsumerFetcherManager.doWork(): Currently, if we hit an exception when calling addFetcher(), we won't remove any partition from noLeaderPartitionSet, include those that have been processed successfully. We can change it so that we remove each partition from noLeaderPartitionSet after calling addFetcher() successfully.

20. AbstractFetcherThread: Instead of doing initialOffset < 0, could we define an isOffsetInvalid() method?
;;;","17/Jan/13 13:52;brugidou;Added v3 with your remarks;;;","17/Jan/13 18:16;junrao;Thanks for the patch. Committed to 0.8 with the following minor changes.

1. ConsumerFetcherManager: fixed the bug in the new warn logging.
2. AbstractFetcherThread: moved isOffsetInvalid() to where InvalidOffset is defined.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsoleConsumer outputs diagnostic message to stdout instead of stderr,KAFKA-692,12626920,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,benfleis,benfleis,benfleis,10/Jan/13 11:33,13/Jan/13 14:35,14/Jul/23 05:39,11/Jan/13 18:12,0.8.0,,,0.8.0,,,,,,,clients,,,0,,,,"At the end of its handling loop, ConsoleConsumer prints ""Consumed %d messages"" to standard out.  Clients who use custom formatters, and read this output, shouldn't need to special case this line, or accept a parse error.

It should instead go (as all diagnostics should) to stderr.

patch attached.",,benfleis,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/13 11:34;benfleis;kafka_692_v1.diff;https://issues.apache.org/jira/secure/attachment/12564153/kafka_692_v1.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,303611,,,Sun Jan 13 14:35:55 UTC 2013,,,,,,,,,,"0|i17d5r:",251101,,,,,,,,,,,,,,,,,,,,"10/Jan/13 11:34;benfleis;stdout -> stderr;;;","10/Jan/13 23:33;nehanarkhede;+1. Thanks for the patch;;;","11/Jan/13 18:12;nehanarkhede;Checked in ;;;","13/Jan/13 03:52;jkreps;Is this the right fix, I don't think the console consumer should output anything but messages or errors. This diagnostic should not have been added, perhaps we should just remove it?;;;","13/Jan/13 14:35;benfleis;Agreed - I just patched it so it didn't get in my way on my setup.  In general, it's not clear why it's particular useful/helpful.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fault tolerance broken with replication factor 1,KAFKA-691,12626789,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,brugidou,jkreps,jkreps,09/Jan/13 16:18,17/Jan/13 17:12,14/Jul/23 05:39,10/Jan/13 19:09,0.8.0,,,0.8.0,,,,,,,,,,2,,,,"In 0.7 if a partition was down we would just send the message elsewhere. This meant that the partitioning was really more of a ""stickiness"" then a hard guarantee. This made it impossible to depend on it for partitioned, stateful processing.

In 0.8 when running with replication this should not be a problem generally as the partitions are now highly available and fail over to other replicas. However in the case of replication factor = 1 no longer really works for most cases as now a dead broker will give errors for that broker.

I am not sure of the best fix. Intuitively I think this is something that should be handled by the Partitioner interface. However currently the partitioner has no knowledge of which nodes are available. So you could use a random partitioner, but that would keep going back to the down node.

",,brugidou,jkreps,junrao,kamaradclimber,rangadi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-693,,,,,,,,,,,,,"10/Jan/13 14:37;brugidou;KAFKA-691-v1.patch;https://issues.apache.org/jira/secure/attachment/12564173/KAFKA-691-v1.patch","10/Jan/13 18:38;brugidou;KAFKA-691-v2.patch;https://issues.apache.org/jira/secure/attachment/12564212/KAFKA-691-v2.patch","17/Jan/13 00:00;junrao;kafka-691_extra.patch;https://issues.apache.org/jira/secure/attachment/12565216/kafka-691_extra.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,303407,,,Thu Jan 17 17:12:46 UTC 2013,,,,,,,,,,"0|i17azj:",250748,,,,,,,,,,,,,,,,,,,,"09/Jan/13 16:44;brugidou;I think the work-around is not really acceptable for me since it will consume 3x the resources (because replication of 3 is the minimum acceptable) and it will still make the cluster less available anyway (unless i have only 3 brokers).

The thing is that 0.7 was making the cluster 100% available (for my use case, accepting data loss) as long a single broker was alive.

A way to handle this would be to:
1. Have a lot of partitions per topic (more than the # of brokers)
2. Have something that rebalances the partitions and make sure a broker has a at least a partition for each topic (to make every topic ""available"")
3. Have a setting in the consumer/producer that say ""I don't care about partitioning, just produce/consume wherever you can"";;;","09/Jan/13 19:14;junrao;One thing we can do is to change the partitioner api so that it takes # of partitions and for each partition, an indicator whether a partition is available or not. The we can change the default partitioner to only route a message to the available partitions, if a key is not provided.;;;","09/Jan/13 22:27;brugidou;I agree with Jun solution, this would solve 3 (1 and 2 can be done manualy already -- just send a ReassignPartition command when you add a broker)

I could probably implement this very quickly, I'm just not sure of how you get the availability of a partition, but i'll try to figure it out and submit a first patch tomorrow.;;;","10/Jan/13 01:37;jkreps;That would be awesome. If you don't mind just give the proposed set of changes on the JIRA first and lets get everyone on board with how it should work since it is a reasonably important change (or, if you don't mind revising your patch we can start with that).;;;","10/Jan/13 05:44;junrao;DefaultEventHander.getPartitionListForTopic() returns Seq[PartitionAndLeader]. If PartitionAndLeader.leaderBrokerIdOpt is none, the partition is not available. 

There is another tricky issue. If a partition is not available, when do we refresh the metadata to check if the partition becomes available again? Currently, we refresh the metadata if we fail to send the data. However, if we always route the messages to available partitions, we may never fail to send. One possible solution is that if there is at least one partition not available in Seq[PartitionAndLeader], we refresh the metadata if a configurable amount of time has passed (e.g., 10 mins).;;;","10/Jan/13 14:37;brugidou;Here is a first draft (v1) patch.

1. Added the consumer property ""producer.metadata.refresh.interval.ms"" defaults to 600000 (10min)

2. The metadata is refreshed every 10min (only if a message is sent), and the set of topics to refresh is tracked in the topicMetadataToRefresh Set (cleared after every refresh) - I think the added value of refreshing regardless of partition availability is to detect new partitions

3. The good news is that I didn't touch the Partitioner API, I only changed the code to use available partitions if the key is null (as suggested by Jun), it will also throw a UnknownTopicOrPartitionException(""No leader for any partition"") if no partition is available at all

Let me know what you think about this patch. I ran a producer with that code successfully and tested with a broker down.

I now have some concerns about the consumer: the refresh.leader.backoff.ms config could help me (if i increase it to say, 10min) BUT the rebalance fails in any case since there is no leader for some partitions

I don't have a good workaround yet for that, any help/suggestion appreciated.;;;","10/Jan/13 17:59;junrao;Thanks for the patch. Overall, the patch is pretty good and is well thought out. Some comments:

1. DefaultEventHandler:
1.1 In handle(), I don't think we need to add the if test in the following statement. The reason is that a message could fail to be sent because the leader changes immediately after the previous metadata refresh. Normally, leaders are elected very quickly. So, it makes sense to refresh the metadata again.
          if (topicMetadataToRefresh.nonEmpty)
              Utils.swallowError(brokerPartitionInfo.updateInfo(outstandingProduceRequests.map(_.topic).toSet))
1.2 In handle(), it seems that it's better to call the following code before dispatchSerializedData().
        if (topicMetadataRefreshInterval >= 0 &&
            SystemTime.milliseconds - lastTopicMetadataRefresh > topicMetadataRefreshInterval) {
          Utils.swallowError(brokerPartitionInfo.updateInfo(topicMetadataToRefresh.toSet))
          topicMetadataToRefresh.clear
          lastTopicMetadataRefresh = SystemTime.milliseconds
        }
1.3 getPartition(): If none of the partitions is available, we should throw LeaderNotAvailableException, instead of UnknownTopicOrPartitionException.

2. DefaultPartitioner: Since key is not expected to be null, we should remove the code that deals with null key. 

3. The consumer side logic is fine. The consumer rebalance is only triggered when there are changes in partitions, not when there are changes in the availability of the partition. The rebalance logic doesn't depend on a partition being available. If a partition is not available, ConsumerFetcherManager will keep refreshing metadata. If you have a replication factor of 1, you will need to set a larger refresh.leader.backoff.ms, if a broker is expected to go down for a long time. ;;;","10/Jan/13 18:38;brugidou;Thanks for your feedback, I updated it (v2) according to your notes (1. and 2.).

for 3. I believe you are right, except that:
3.1 It seems (correct me if i'm wrong) that a rebalance happen at the consumer initialization, so that means a consumer can't start if a broker is down
3.2 Can a rebalance be triggered when a partition is added or moved? Having a broker down shouldn't prevent me from reassigning partitions or adding partitions.
;;;","10/Jan/13 19:09;junrao;Thanks for patch v2. Committed to 0.8 by renaming lastTopicMetadataRefresh to lastTopicMetadataRefreshTime and removing an unused comment.

3.1 Rebalance happens during consumer initialization. It only needs the partition data to be in ZK and doesn't require all brokers to be up. Of course, if a broker is not up, the consumer may not be able to consume data from it. ConsumerFetcherManager is responsible for checking if a partition becomes available again.

3.2 If the partition path changes in ZK, a rebalance will be triggered.;;;","10/Jan/13 19:57;brugidou;Thanks for committing the patch.

3.1 Are you sure that the rebalance doesn't require all partitions to have a leader? My experience earlier today was that the rebalance would fail and throw ConsumerRebalanceFailedException after having stopped all fetchers and cleared all queues. If you are sure then i'll try to reproduce the behavior I encountered, and maybe open a separate JIRA?;;;","10/Jan/13 21:31;junrao;3.1 It shouldn't. However, if you can reproduce this problem, please file a new jira.;;;","12/Jan/13 16:58;junrao;Another potential issue is that for producers that produce many topics (like migrationTool and mirrorMaker), the time-based refreshing may need to get the metadata for many topics. This means that the metadata request is likely to timeout. One solution is to break topics into batches in BrokerPartitionInfo.updateInfo() and issue a metadata request per batch. ;;;","12/Jan/13 18:02;brugidou;Should i make another patch? I'll try on Monday.

1. It would probably require yet another config variable like ""producer.metadata.request.batch.size"" or something like that.
2. Should it be batched for every updateInfo() or just during the metadata refresh? It could help if we do the former because failing messages from many different topics could probably never go through if the metadata request timeouts.
3. Isn'it getting a little convoluted? Maybe i am missing something but the producer side is getting trickier.
4. Please note that I also opened KAFKA-693 about the consumer side. And I'd love to submit a patch but the rebalance logic seems complex so I'd prefer to have some insights first before going in the wrong direction.;;;","13/Jan/13 22:11;junrao;It would be great if you can provide a patch.

1,2,3. Yes, we will need a new config. We should do batching in updateinfo(). This does make the producer side logic a bit more complicated. We have been thinking about making getMetadata faster. When we get there, we can revisit the batching logic.;;;","14/Jan/13 00:34;jkreps;Does batching make sense versus just having people increase the timeout?;;;","14/Jan/13 01:00;junrao;That's a good point. Increasing the timeout will work for most cases. If a broker goes down, the client request will get a socket exception immediately, independent of the timeout. So setting a large timeout doesn't hurt. When the broker host goes down and the client is waiting for a response from the server, I think the client will have to wait until the timeout. If we set a larger timeout, it means that the client has to wait longer before realizing the broker is down. However, since this is a rarer case, I think setting a larger timeout for now is probably good enough.;;;","14/Jan/13 14:53;brugidou;So I wait for your feedback first, but I guess that increasing the time out is good enough, although it's 1500ms by default which is very short.;;;","16/Jan/13 22:53;junrao;The last patch introduced a bug. DefaultEventHander.getPartition() is expected to return the index of the partitionList, instead of the actual partition id. Attach a patch that fixes the issue.;;;","17/Jan/13 00:00;junrao;Attach the right patch (kafka-691_extra.patch).;;;","17/Jan/13 17:12;junrao;Actually, the current code works since partitionId is always btw 0 and num.partition-1 and therefore it happens to also be the index of the partitionList. This patch just makes the code a bit better to understand.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TopicMetadataRequest throws exception when no topics are specified,KAFKA-690,12626691,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mumrah,mumrah,mumrah,09/Jan/13 00:50,09/Jan/13 16:49,14/Jul/23 05:39,09/Jan/13 16:49,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"If no topics are sent in a TopicMetadataRequest, `readFrom` throws an exception when trying to get the the head of the topic list for a debug statement.

java.util.NoSuchElementException: head of empty list
	at scala.collection.immutable.Nil$.head(List.scala:386)
	at scala.collection.immutable.Nil$.head(List.scala:383)
	at kafka.api.TopicMetadataRequest$$anonfun$readFrom$2.apply(TopicMetadataRequest.scala:43)
	at kafka.api.TopicMetadataRequest$$anonfun$readFrom$2.apply(TopicMetadataRequest.scala:43)
	at kafka.utils.Logging$class.debug(Logging.scala:51)
	at kafka.api.TopicMetadataRequest$.debug(TopicMetadataRequest.scala:25)
	at kafka.api.TopicMetadataRequest$.readFrom(TopicMetadataRequest.scala:43)
	at kafka.api.RequestKeys$$anonfun$4.apply(RequestKeys.scala:37)
	at kafka.api.RequestKeys$$anonfun$4.apply(RequestKeys.scala:37)
	at kafka.network.RequestChannel$Request.<init>(RequestChannel.scala:47)
	at kafka.network.Processor.read(SocketServer.scala:320)
	at kafka.network.Processor.run(SocketServer.scala:231)
	at java.lang.Thread.run(Thread.java:680)
",,brugidou,jkreps,mumrah,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-653,,,,,,,,,,,,,,,,,,,,,,"09/Jan/13 01:22;mumrah;KAFKA-690.patch;https://issues.apache.org/jira/secure/attachment/12563864/KAFKA-690.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,303299,,,Wed Jan 09 16:49:40 UTC 2013,,,,,,,,,,"0|i179a7:",250472,,,,,,,,,,,,,,,,,,,,"09/Jan/13 00:58;mumrah;Removing the debug statement fixes the issue. However, it seems instead of returning all of the topic metadata - I get none.

Here are some trace logs when I send a TopicMetadataRequest with no topics

[2013-01-08 19:53:34,463] TRACE 26 bytes read from /127.0.0.1:61558 (kafka.network.Processor)
[2013-01-08 19:53:34,465] TRACE Received request: TopicMetadataRequest(0,kafka-python,List(),0) (kafka.network.RequestChannel$)
[2013-01-08 19:53:34,466] TRACE Recieved request, sending for processing by handler: Request(1,sun.nio.ch.SelectionKeyImpl@785e8d7d,java.nio.HeapByteBuffer[pos=0 lim=26 cap=26],1357692814463) (kafka.network.Processor)
[2013-01-08 19:53:34,466] DEBUG [Kafka Request Handler 1 on Broker 0], handles request Request(1,sun.nio.ch.SelectionKeyImpl@785e8d7d,java.nio.HeapByteBuffer[pos=0 lim=26 cap=26],1357692814463) (kafka.server.KafkaRequestHandler)
[2013-01-08 19:53:34,466] TRACE Handling topic metadata request TopicMetadataRequest(0,kafka-python,List(),0) (kafka.request.logger)
[2013-01-08 19:53:34,466] TRACE [KafkaApi-0] Handling topic metadata request TopicMetadataRequest(0,kafka-python,List(),0) (kafka.server.KafkaApis)
[2013-01-08 19:53:34,475] TRACE Socket server received response to send, registering for write: Response(1,Request(1,sun.nio.ch.SelectionKeyImpl@785e8d7d,java.nio.HeapByteBuffer[pos=0 lim=26 cap=26],1357692814463),kafka.network.BoundedByteBufferSend@2e82674b) (kafka.network.Processor)
[2013-01-08 19:53:34,476] TRACE 16 bytes written to /127.0.0.1:61558 (kafka.network.Processor)
[2013-01-08 19:53:34,476] TRACE Completed request: TopicMetadataRequest(0,kafka-python,List(),0) totalTime:13 queueTime:3 localTime:9 remoteTime:0 sendTime:1 (kafka.network.RequestChannel$)

I would expect this to return info about the one existing topic ""test"".

Here are the logs for when I send a TopicMetadataRequest with the ""test"" topic (this gives the expected response):

[2013-01-08 19:55:26,247] TRACE 32 bytes read from /127.0.0.1:61719 (kafka.network.Processor)
[2013-01-08 19:55:26,247] TRACE Received request: TopicMetadataRequest(0,kafka-python,List(test),0) (kafka.network.RequestChannel$)
[2013-01-08 19:55:26,247] TRACE Recieved request, sending for processing by handler: Request(0,sun.nio.ch.SelectionKeyImpl@30d647d8,java.nio.HeapByteBuffer[pos=0 lim=32 cap=32],1357692926247) (kafka.network.Processor)
[2013-01-08 19:55:26,247] DEBUG [Kafka Request Handler 0 on Broker 0], handles request Request(0,sun.nio.ch.SelectionKeyImpl@30d647d8,java.nio.HeapByteBuffer[pos=0 lim=32 cap=32],1357692926247) (kafka.server.KafkaRequestHandler)
[2013-01-08 19:55:26,247] TRACE Handling topic metadata request TopicMetadataRequest(0,kafka-python,List(test),0) (kafka.request.logger)

[2013-01-08 19:55:26,247] TRACE [KafkaApi-0] Handling topic metadata request TopicMetadataRequest(0,kafka-python,List(test),0) (kafka.server.KafkaApis)

[2013-01-08 19:55:26,248] DEBUG Reading reply sessionid:0x13c15a1d848003f, packet:: clientPath:null serverPath:null finished:false header:: 30,3  replyHeader:: 30,354,0  request:: '/kafka/brokers/topics/test,F  response:: s{320,320,1357689749289,1357689749289,0,1,0,0,14,1,323}  (org.apache.zookeeper.ClientCnxn)
[2013-01-08 19:55:26,249] DEBUG Reading reply sessionid:0x13c15a1d848003f, packet:: clientPath:null serverPath:null finished:false header:: 31,4  replyHeader:: 31,354,0  request:: '/kafka/brokers/topics/test,F  response:: #7b202230223a205b2230225d207d,s{320,320,1357689749289,1357689749289,0,1,0,0,14,1,323}  (org.apache.zookeeper.ClientCnxn)
[2013-01-08 19:55:26,252] DEBUG Partition map for /brokers/topics/test is Map(0 -> List(0)) (kafka.utils.ZkUtils$)
[2013-01-08 19:55:26,255] DEBUG Reading reply sessionid:0x13c15a1d848003f, packet:: clientPath:null serverPath:null finished:false header:: 32,4  replyHeader:: 32,354,0  request:: '/kafka/brokers/topics/test/partitions/0/leaderAndISR,F  response:: #7b2022495352223a2230222c226c6561646572223a2230222c22636f6e74726f6c6c657245706f6368223a2232222c226c656164657245706f6368223a223022207d,s{325,325,1357689749406,1357689749406,0,0,0,0,66,0,325}  (org.apache.zookeeper.ClientCnxn)
[2013-01-08 19:55:26,266] DEBUG Reading reply sessionid:0x13c15a1d848003f, packet:: clientPath:null serverPath:null finished:false header:: 33,4  replyHeader:: 33,354,0  request:: '/kafka/brokers/topics/test/partitions/0/leaderAndISR,F  response:: #7b2022495352223a2230222c226c6561646572223a2230222c22636f6e74726f6c6c657245706f6368223a2232222c226c656164657245706f6368223a223022207d,s{325,325,1357689749406,1357689749406,0,0,0,0,66,0,325}  (org.apache.zookeeper.ClientCnxn)
[2013-01-08 19:55:26,277] DEBUG replicas = List(0), in sync replicas = ArrayBuffer(0), leader = Some(0) (kafka.admin.AdminUtils$)
[2013-01-08 19:55:26,278] DEBUG Reading reply sessionid:0x13c15a1d848003f, packet:: clientPath:null serverPath:null finished:false header:: 34,4  replyHeader:: 34,354,0  request:: '/kafka/brokers/ids/0,F  response:: #3139322e3136382e322e313a393039323a39393939,s{352,352,1357692804787,1357692804787,0,0,0,88969877503082559,21,0,352}  (org.apache.zookeeper.ClientCnxn)

>>> The TopicMetadataResponse <<<
[2013-01-08 19:55:26,282] TRACE [KafkaApi-0] Sending topic metadata TopicMetadata(test,List(PartitionMetadata(0,Some(id:0,host:192.168.2.1,port:9092),List(id:0,host:192.168.2.1,port:9092),ArrayBuffer(id:0,host:192.168.2.1,port:9092),0)),0) (kafka.server.KafkaApis)

[2013-01-08 19:55:26,284] TRACE Socket server received response to send, registering for write: Response(0,Request(0,sun.nio.ch.SelectionKeyImpl@30d647d8,java.nio.HeapByteBuffer[pos=0 lim=32 cap=32],1357692926247),kafka.network.BoundedByteBufferSend@3ddfd90f) (kafka.network.Processor)
[2013-01-08 19:55:26,284] TRACE 75 bytes written to /127.0.0.1:61719 (kafka.network.Processor)
[2013-01-08 19:55:26,284] TRACE Completed request: TopicMetadataRequest(0,kafka-python,List(test),0) totalTime:37 queueTime:0 localTime:37 remoteTime:0 sendTime:0 (kafka.network.RequestChannel$)
[2013-01-08 19:55:26,285] INFO Closing socket connection to /127.0.0.1. (kafka.network.Processor)
[2013-01-08 19:55:26,285] DEBUG Closing connection from /127.0.0.1:61719 (kafka.network.Processor)

;;;","09/Jan/13 01:22;mumrah;This patch will return all topic metadata if none are specified in the TopicMetadataRequest. Also fixes that debug statement;;;","09/Jan/13 05:42;nehanarkhede;+1. Thanks for the patch !;;;","09/Jan/13 12:13;brugidou;this would resolve KAFKA-653;;;","09/Jan/13 13:59;mumrah;Ah, I didn't see that JIRA. I was following https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-MetadataRequest and read ""TopicName -> The topics to produce metadata for. If empty the request will yield metadata for all topics"", so I assumed this was a bug instead of a TODO;;;","09/Jan/13 16:22;jkreps;+1 Thanks David!

Folks, objections to putting this on 0.8?;;;","09/Jan/13 16:49;nehanarkhede;The protocol was meant to do this, so checked it on 0.8 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsumerOffsetChecker does not work with 0.8,KAFKA-685,12626320,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,brugidou,brugidou,brugidou,07/Jan/13 09:42,08/Jan/13 21:49,14/Jul/23 05:39,08/Jan/13 21:49,0.8.0,,,,,,,,,,core,,,0,,,,"The ConsumerOffsetChecker does not work anymore with 0.8, this tool is very useful when used with the MirrorMaker.

Here is a patch to make it work with some cosmetic changes:

* script-friendly formatting (one line per partition)
* offsets do not correspond to bytes anymore (so the lag is in number of messages, not GiB)
* --broker-info optional option to print the broker list at the end (like the previous version)

Example: 

bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --group KafkaMirror --zkconnect zoo.example.org:2181
Group           Topic           Pid Offset          logSize         Lag             Owner
KafkaMirror     test       0  215385          215385          0               Some(KafkaMirror_broker01-1379350-71cf9117-0)
KafkaMirror     test       1  683564          683564          0               Some(KafkaMirror_broker03-1379351-71cf9117-0)
KafkaMirror     test2     0  176943          176943          0               Some(KafkaMirror_broker05-1379353-71cf91
",,brugidou,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/13 08:22;brugidou;KAFKA-685-2.patch;https://issues.apache.org/jira/secure/attachment/12563719/KAFKA-685-2.patch","07/Jan/13 09:47;brugidou;KAFKA-685.patch;https://issues.apache.org/jira/secure/attachment/12563540/KAFKA-685.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,302906,,,Tue Jan 08 21:49:00 UTC 2013,,,,,,,,,,"0|i176bz:",249994,,,,,,,,,,,,,,,,,,,,"07/Jan/13 23:13;junrao;Thanks for the patch.  The code looks good. Got the following error on latest 0.8 branch. This is likely due to the recent change in KAFKA-668. So you need to change the regex a bit.

ERROR Could not parse broker info Some(jrao-ld.linkedin.biz:9092:9999) with regex ^([^:]+):(\d+)$ (kafka.tools.ConsumerOffsetChecker$);;;","08/Jan/13 08:22;brugidou;New patch, updated the regex to ignore JMX port;;;","08/Jan/13 21:49;junrao;Thanks for patch v2. Committed to 0.8 with the following tweaks.

1. Print owner better since it's an Option.

2. Increased the width of 2 fields in the output.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsoleProducer does not have the queue-size option,KAFKA-684,12626311,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,brugidou,brugidou,brugidou,07/Jan/13 09:02,08/Jan/13 21:57,14/Jul/23 05:39,08/Jan/13 21:57,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"When using the kafka ConsoleProducer (from script kafka-console-producer.sh), you cannot set the queue.size, which gets very annoying when  you want to produce quickly a lot of messages. You definitely need to increase the queue.size (or decrease the send timeout).

Here is a simple patch to add the option.",,brugidou,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/13 09:30;brugidou;KAFKA-684-2.patch;https://issues.apache.org/jira/secure/attachment/12563727/KAFKA-684-2.patch","08/Jan/13 10:07;brugidou;KAFKA-684-3.patch;https://issues.apache.org/jira/secure/attachment/12563731/KAFKA-684-3.patch","07/Jan/13 09:04;brugidou;KAFKA-684.patch;https://issues.apache.org/jira/secure/attachment/12563528/KAFKA-684.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,302897,,,Tue Jan 08 21:57:55 UTC 2013,,,,,,,,,,"0|i1769z:",249985,,,,,,,,,,,,,,,,,,,,"07/Jan/13 21:37;junrao;Thanks for the patch. It looks good. While you are here, could you also expose the following properties (from SyncProducerConfig) in ConsoleProducer?
producer.request.timeout.ms
producer.request.required.acks;;;","08/Jan/13 09:30;brugidou;I added:

queue.enqueueTimeout.ms
producer.request.required.acks
producer.request.timeout.ms
;;;","08/Jan/13 10:07;brugidou;While I'm at it, I added this small feature:

Exit at end of input stream (so you can do echo ""test"" | ./kafka-console-producer.sh or ./kafka-console-producer.sh < test without stopping the producer manually;;;","08/Jan/13 21:57;junrao;Thanks for patch v3. +1. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.OutOfMemoryError: Java heap space,KAFKA-682,12626080,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,rngadam,rngadam,04/Jan/13 18:14,11/Jul/13 22:17,14/Jul/23 05:39,11/Jul/13 22:17,0.8.0,,,,,,,,,,core,,,0,,,,"git pull (commit 32dae955d5e2e2dd45bddb628cb07c874241d856)

...build...

./sbt update
./sbt package

...run...

bin/zookeeper-server-start.sh config/zookeeper.properties
bin/kafka-server-start.sh config/server.properties

...then configured fluentd with kafka plugin...

gem install fluentd --no-ri --no-rdoc
gem install fluent-plugin-kafka
fluentd -c ./fluent/fluent.conf -vv

...then flood fluentd with messages inputted from syslog and outputted to kafka.

results in (after about 10000 messages of 1K each in 3s):

[2013-01-05 02:00:52,087] ERROR Closing socket for /127.0.0.1 because of error (kafka.network.Processor)
java.lang.OutOfMemoryError: Java heap space
    at kafka.api.ProducerRequest$$anonfun$1$$anonfun$apply$1.apply(ProducerRequest.scala:45)
    at kafka.api.ProducerRequest$$anonfun$1$$anonfun$apply$1.apply(ProducerRequest.scala:42)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
    at scala.collection.immutable.Range$ByOne$class.foreach(Range.scala:282)
    at scala.collection.immutable.Range$$anon$1.foreach(Range.scala:274)
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
    at scala.collection.immutable.Range.map(Range.scala:39)
    at kafka.api.ProducerRequest$$anonfun$1.apply(ProducerRequest.scala:42)
    at kafka.api.ProducerRequest$$anonfun$1.apply(ProducerRequest.scala:38)
    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:227)
    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:227)
    at scala.collection.immutable.Range$ByOne$class.foreach(Range.scala:282)
    at scala.collection.immutable.Range$$anon$1.foreach(Range.scala:274)
    at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:227)
    at scala.collection.immutable.Range.flatMap(Range.scala:39)
    at kafka.api.ProducerRequest$.readFrom(ProducerRequest.scala:38)
    at kafka.api.RequestKeys$$anonfun$1.apply(RequestKeys.scala:32)
    at kafka.api.RequestKeys$$anonfun$1.apply(RequestKeys.scala:32)
    at kafka.network.RequestChannel$Request.<init>(RequestChannel.scala:47)
    at kafka.network.Processor.read(SocketServer.scala:298)
    at kafka.network.Processor.run(SocketServer.scala:209)
    at java.lang.Thread.run(Thread.java:722)
","$ uname -a
Linux rngadam-think 3.5.0-17-generic #28-Ubuntu SMP Tue Oct 9 19:32:08 UTC 2012 i686 i686 i686 GNU/Linux
$ java -version
java version ""1.7.0_09""
OpenJDK Runtime Environment (IcedTea7 2.3.3) (7u9-2.3.3-0ubuntu1~12.04.1)
OpenJDK Server VM (build 23.2-b09, mixed mode)",jjkoshy,jkreps,junrao,nehanarkhede,rngadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/13 08:14;rngadam;java_pid22281.hprof.gz;https://issues.apache.org/jira/secure/attachment/12563466/java_pid22281.hprof.gz","06/Jan/13 08:14;rngadam;java_pid22281_Leak_Suspects.zip;https://issues.apache.org/jira/secure/attachment/12563467/java_pid22281_Leak_Suspects.zip",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,302663,,,Thu Jul 11 22:17:15 UTC 2013,,,,,,,,,,"0|i174lj:",249713,,,,,,,,,,,,,,,,,,,,"04/Jan/13 19:13;jjkoshy;You might need to increase your heap size. What do you have it set to right now? Would you be able to run the broker with -XX:+HeapDumpOnOutOfMemoryError to get a heap-dump?

In case you are overriding defaults - what's the replication factor for the topic, num-required-acks for the producer requests, and producer request timeout? Are any requests going through or are the produce requests expiring?
;;;","04/Jan/13 23:04;junrao;That commit is in trunk. Could you try the current head in 0.8 (which fixed one OOME issue KAFKA-664)?;;;","05/Jan/13 01:35;jjkoshy;I think that fix was merged into trunk (before 32da) so it should be there in trunk as well.;;;","06/Jan/13 08:12;rngadam;After filing the bug initially, I switched to these settings (and then added the HeapDump directive):

bin/kafka-run-class.sh

KAFKA_OPTS=""-server -Xms1024m -Xmx1024m -XX:NewSize=256m -XX:MaxNewSize=256m -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -Xloggc:logs/gc.log -Djava.awt.headless=true -Dlog4j.configuration=file:$base_dir/config/log4j.properties -XX:+HeapDumpOnOutOfMemoryError""

Shouldn't these be set more aggressively as per the operational suggestions? It's probably better to have the user lower them than to have to make them higher.

I've downloaded MAT for Eclipse and ran it on the hprof. It points out two issues of which this is the more noticeable:

One instance of ""java.nio.HeapByteBuffer"" loaded by ""<system class loader>"" occupies 8,404,016 (58.22%) bytes. The instance is referenced by kafka.network.BoundedByteBufferReceive @ 0x7ad6a038 , loaded by ""sun.misc.Launcher$AppClassLoader @ 0x7ad00d40"". The memory is accumulated in one instance of ""byte[]"" loaded by ""<system class loader>""

;;;","06/Jan/13 08:14;rngadam;the hprof dump;;;","07/Jan/13 21:19;junrao;BoundedByteBufferReceive is used for receiving client requests. Most of the space is likely taken by ProducerRequest. If you are sending many large ProducerRequests, the result in the head dump makes sense. Do you still see OOME with the new JVM setting? You heap size seems small. I would try 3-4GBs.;;;","07/Jan/13 21:30;nehanarkhede;I think this is the cause - https://issues.apache.org/jira/browse/KAFKA-671;;;","08/Jan/13 19:11;jjkoshy;That's why I asked  for the configured ""num-required-acks for the producer requests"". If it is the default (0) then it shouldn't be added to the request purgatory which rule out KAFKA-671 no?;;;","18/Jan/13 05:25;jkreps;Yes, Joel, that makes sense. Ricky, do you know the ""acks"" setting being used in the requests the ruby client is sending?;;;","11/Jul/13 22:17;jkreps;Marking resolved as we fixed a 0.8 bug that impacted memory and improved the default GC settings.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unclean shutdown testing - truncateAndStartWithNewOffset is not invoked when it is expected to,KAFKA-681,12625896,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,jfung,jfung,03/Jan/13 19:31,04/Jan/13 17:19,14/Jul/23 05:39,04/Jan/13 17:19,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,,,,,jfung,jjkoshy,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/13 19:43;jfung;kafka-681-reproduce-issue.patch;https://issues.apache.org/jira/secure/attachment/12563138/kafka-681-reproduce-issue.patch","04/Jan/13 02:14;junrao;kafka-681_v1.patch;https://issues.apache.org/jira/secure/attachment/12563214/kafka-681_v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,302477,,,Fri Jan 04 17:19:39 UTC 2013,,,,,,,,,,"0|i173fr:",249524,,,,,,,,,,,,,,,,,,,,"03/Jan/13 19:41;jfung;Test description (compression off):
1. Start 2 brokers B1 & B2
2. Send 5 messages and sleep 5 sec
3. Stop B2 and sleep 5 sec
4. Send 20 messages and sleep 10 sec
5. Stop B1 and sleep 5 sec
6. Start B2 and sleep 5 sec
7. Send 5 messages and sleep 5 sec
8. Start B1 and sleep 5 sec
9. Check B1's log4j messages for ""Truncate and start log"" which is invoked from ""truncateAndStartWithNewOffset"" inside Log.scala;;;","03/Jan/13 19:43;jfung;Uploaded kafka-681-reproduce-issue.patch;;;","03/Jan/13 19:47;jfung;To reproduce the issue:
1. Download the latest 0.8 branch and apply kafka-681-reproduce-issue.patch
2. In <kafka_home>/system_test, execute this command: python -u -B system_test_runner.py 2>&1 | tee system_test_output_`date +%s`.log
3. The test suite will execute testcase_9071
4. When the test is completed, check the file <kafka_home>/system_test/unclean_shutdown_testsuite/testcase_9071/logs/broker-1/kafka_server_9091.log for the message ""Truncate and start log"";;;","04/Jan/13 02:14;junrao;This is actually caused by a real bug. When reading high watermarks from the file, we can incorrectly recover the topic name. Attach a fix.;;;","04/Jan/13 05:08;jjkoshy;+1;;;","04/Jan/13 07:18;nehanarkhede;+1 ;;;","04/Jan/13 16:32;jfung;Thanks Jun for the patch.

* The patch is applied with the latest 0.8 branch and tested under the same conditions and the followings are the output:

* Searching for the message and the timestamp is after broker 1 is re-started
        kafka/system_test/unclean_shutdown_testsuite/testcase_9071/logs $ grep -rs -i ""truncate and start"" *
        broker-1/kafka_server_9091.log:[2013-01-04 08:25:24,416] INFO [Kafka Log on Broker 1], Truncate and start log 't001-0' to 0 (kafka.log.Log)

* The patch seems to be working as expected.;;;","04/Jan/13 17:19;junrao;Thanks for the review. Committed to 0.8 with minor cleanup.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ApiUtils#writeShortString uses String length instead of byte length,KAFKA-680,12625881,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mumrah,mumrah,mumrah,03/Jan/13 17:48,04/Jan/13 21:46,14/Jul/23 05:39,04/Jan/13 21:46,,,,0.8.0,,,,,,,clients,core,,0,,,,"Instead of using the length of the encoded bytes, writeShortString is using String#length. If non single-byte characters are encoded, then things go wrong then decoding the string with readShortString",,junrao,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/13 17:51;mumrah;KAFKA-680.patch;https://issues.apache.org/jira/secure/attachment/12563110/KAFKA-680.patch","04/Jan/13 20:51;mumrah;KAFKA-680v1.patch;https://issues.apache.org/jira/secure/attachment/12563363/KAFKA-680v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,302462,,,Fri Jan 04 21:46:09 UTC 2013,,,,,,,,,,"0|i173cf:",249509,,,,,,,,,,,,,,,,,,,,"04/Jan/13 20:51;mumrah;Attaching v1 of patch, included some unit tests;;;","04/Jan/13 21:46;junrao;Thanks for the patch. +1. Committed to 0.8 and merged into trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We should default to NullEncoder for producer,KAFKA-678,12625036,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,21/Dec/12 18:14,09/Feb/14 23:59,14/Jul/23 05:39,09/Feb/14 23:59,0.8.0,,,,,,,,,,core,,,0,,,,"Currently we default to using whatever serializer you set for your value to also work for your key. This works if the serializer is of a generic sort (Avro, Java serialization, etc) and both key and value map into this. However if you have a custom serializer this is not the right thing to do.

I think it would be better to default this to NullEncoder which defaults to maintain the pre-0.8 behavior of not retaining the key.",,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,301572,,,Sun Feb 09 23:59:36 UTC 2014,,,,,,,,,,"0|i16ur3:",248114,,,,,,,,,,,,,,,,,,,,"09/Feb/14 23:59;jkreps;Instead we are removing serializers altogether.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broker recovery check logic is reversed,KAFKA-673,12623824,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,jkreps,jkreps,jkreps,13/Dec/12 20:50,16/Dec/12 19:12,14/Jul/23 05:39,16/Dec/12 19:12,0.8.0,,,0.8.0,,,,,,,,,,0,,,,We are currently running recovery when there IS a clean shutdown and not recovering when there isn't.,,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/12 20:55;jkreps;KAFKA-673.patch;https://issues.apache.org/jira/secure/attachment/12560847/KAFKA-673.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,297544,,,Thu Dec 13 20:57:09 UTC 2012,,,,,,,,,,"0|i14ran:",235887,,,,,,,,,,,,,,,,,,,,"13/Dec/12 20:55;jkreps;Oopsy daisy.;;;","13/Dec/12 20:57;nehanarkhede;+1 :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merge 0.8 changes to trunk,KAFKA-672,12623643,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,12/Dec/12 23:50,18/Dec/12 17:51,14/Jul/23 05:39,18/Dec/12 17:51,,,,,,,,,,,,,,0,,,,,,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/12 23:53;jkreps;KAFKA-672.patch;https://issues.apache.org/jira/secure/attachment/12560680/KAFKA-672.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,297367,,,Wed Dec 12 23:53:28 UTC 2012,,,,,,,,,,"0|i14o7j:",235387,,,,,,,,,,,,,,,,,,,,"12/Dec/12 23:53;jkreps;Here is the merge changes in case anyone wants to take a look. I will probably proceed with the merge tomorrow if I don't hear back.

Conflicts:
 core/src/main/scala/kafka/log/Log.scala
 core/src/test/scala/unit/kafka/admin/AdminTest.scala
 core/src/test/scala/unit/kafka/server/LogOffsetTest.scala

Full change list:
# On branch trunk-0.8-merge
# Changes to be committed:
#
#	deleted:    bin/kafka-producer-shell.sh
#	new file:   bin/windows/kafka-server-stop.bat
#	new file:   bin/windows/zookeeper-server-stop.bat
#	modified:   contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLContext.java
#	modified:   core/src/main/scala/kafka/admin/AdminUtils.scala
#	modified:   core/src/main/scala/kafka/api/FetchRequest.scala
#	modified:   core/src/main/scala/kafka/api/FetchResponse.scala
#	modified:   core/src/main/scala/kafka/api/LeaderAndIsrRequest.scala
#	modified:   core/src/main/scala/kafka/api/LeaderAndIsrResponse.scala
#	modified:   core/src/main/scala/kafka/api/OffsetRequest.scala
#	modified:   core/src/main/scala/kafka/api/OffsetResponse.scala
#	modified:   core/src/main/scala/kafka/api/ProducerRequest.scala
#	modified:   core/src/main/scala/kafka/api/ProducerResponse.scala
#	modified:   core/src/main/scala/kafka/api/StopReplicaRequest.scala
#	modified:   core/src/main/scala/kafka/api/StopReplicaResponse.scala
#	modified:   core/src/main/scala/kafka/api/TopicMetadata.scala
#	modified:   core/src/main/scala/kafka/api/TopicMetadataRequest.scala
#	modified:   core/src/main/scala/kafka/api/TopicMetadataResponse.scala
#	modified:   core/src/main/scala/kafka/client/ClientUtils.scala
#	modified:   core/src/main/scala/kafka/cluster/Broker.scala
#	renamed:    core/src/main/scala/kafka/producer/async/AsyncProducerStats.scala -> core/src/main/scala/kafka/common/InvalidClientIdException.scala
#	new file:   core/src/main/scala/kafka/common/TopicExistsException.scala
#	modified:   core/src/main/scala/kafka/consumer/ConsoleConsumer.scala
#	modified:   core/src/main/scala/kafka/consumer/ConsumerFetcherManager.scala
#	modified:   core/src/main/scala/kafka/consumer/ConsumerIterator.scala
#	modified:   core/src/main/scala/kafka/consumer/ConsumerTopicStat.scala
#	renamed:    core/src/main/scala/kafka/consumer/ConsumerTopicStat.scala -> core/src/main/scala/kafka/consumer/ConsumerTopicStats.scala
#	modified:   core/src/main/scala/kafka/consumer/KafkaStream.scala
#	modified:   core/src/main/scala/kafka/consumer/PartitionTopicInfo.scala
#	modified:   core/src/main/scala/kafka/consumer/SimpleConsumer.scala
#	modified:   core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala
#	modified:   core/src/main/scala/kafka/controller/KafkaController.scala
#	modified:   core/src/main/scala/kafka/controller/PartitionLeaderSelector.scala
#	modified:   core/src/main/scala/kafka/controller/PartitionStateMachine.scala
#	modified:   core/src/main/scala/kafka/controller/ReplicaStateMachine.scala
#	modified:   core/src/main/scala/kafka/javaapi/TopicMetadataRequest.scala
#	modified:   core/src/main/scala/kafka/javaapi/consumer/SimpleConsumer.scala
#	modified:   core/src/main/scala/kafka/javaapi/consumer/ZookeeperConsumerConnector.scala
#	modified:   core/src/main/scala/kafka/message/Message.scala
#	modified:   core/src/main/scala/kafka/metrics/KafkaCSVMetricsReporter.scala
#	modified:   core/src/main/scala/kafka/producer/BrokerPartitionInfo.scala
#	modified:   core/src/main/scala/kafka/producer/ConsoleProducer.scala
#	modified:   core/src/main/scala/kafka/producer/DefaultPartitioner.scala
#	modified:   core/src/main/scala/kafka/producer/Producer.scala
#	modified:   core/src/main/scala/kafka/producer/ProducerConfig.scala
#	modified:   core/src/main/scala/kafka/producer/ProducerPool.scala
#	modified:   core/src/main/scala/kafka/producer/SyncProducer.scala
#	modified:   core/src/main/scala/kafka/producer/SyncProducerConfig.scala
#	modified:   core/src/main/scala/kafka/producer/async/AsyncProducerStats.scala
#	modified:   core/src/main/scala/kafka/producer/async/DefaultEventHandler.scala
#	modified:   core/src/main/scala/kafka/producer/async/ProducerSendThread.scala
#	modified:   core/src/main/scala/kafka/serializer/Decoder.scala
#	modified:   core/src/main/scala/kafka/serializer/Encoder.scala
#	modified:   core/src/main/scala/kafka/server/AbstractFetcherThread.scala
#	modified:   core/src/main/scala/kafka/server/HighwaterMarkCheckpoint.scala
#	modified:   core/src/main/scala/kafka/server/KafkaApis.scala
#	modified:   core/src/main/scala/kafka/server/KafkaRequestHandler.scala
#	modified:   core/src/main/scala/kafka/server/KafkaServer.scala
#	modified:   core/src/main/scala/kafka/server/KafkaZooKeeper.scala
#	modified:   core/src/main/scala/kafka/server/ReplicaFetcherThread.scala
#	modified:   core/src/main/scala/kafka/tools/ConsumerOffsetChecker.scala
#	modified:   core/src/main/scala/kafka/tools/DumpLogSegments.scala
#	modified:   core/src/main/scala/kafka/tools/GetOffsetShell.scala
#	modified:   core/src/main/scala/kafka/tools/MirrorMaker.scala
#	deleted:    core/src/main/scala/kafka/tools/ProducerShell.scala
#	modified:   core/src/main/scala/kafka/tools/ReplayLogProducer.scala
#	modified:   core/src/main/scala/kafka/tools/SimpleConsumerShell.scala
#	modified:   core/src/main/scala/kafka/tools/UpdateOffsetsInZK.scala
#	renamed:    core/src/main/scala/kafka/utils/Topic.scala -> core/src/main/scala/kafka/utils/ClientIdAndTopic.scala
#	modified:   core/src/main/scala/kafka/utils/Topic.scala
#	modified:   core/src/main/scala/kafka/utils/Utils.scala
#	modified:   core/src/main/scala/kafka/utils/ZkUtils.scala
#	modified:   core/src/test/scala/other/kafka/TestKafkaAppender.scala
#	modified:   core/src/test/scala/other/kafka/TestZKConsumerOffsets.scala
#	modified:   core/src/test/scala/unit/kafka/admin/AdminTest.scala
#	modified:   core/src/test/scala/unit/kafka/api/RequestResponseSerializationTest.scala
#	modified:   core/src/test/scala/unit/kafka/consumer/ConsumerIteratorTest.scala
#	modified:   core/src/test/scala/unit/kafka/integration/AutoOffsetResetTest.scala
#	modified:   core/src/test/scala/unit/kafka/integration/FetcherTest.scala
#	modified:   core/src/test/scala/unit/kafka/integration/LazyInitProducerTest.scala
#	modified:   core/src/test/scala/unit/kafka/integration/PrimitiveApiTest.scala
#	modified:   core/src/test/scala/unit/kafka/integration/ProducerConsumerTestHarness.scala
#	modified:   core/src/test/scala/unit/kafka/javaapi/consumer/ZookeeperConsumerConnectorTest.scala
#	modified:   core/src/test/scala/unit/kafka/log4j/KafkaLog4jAppenderTest.scala
#	modified:   core/src/test/scala/unit/kafka/network/SocketServerTest.scala
#	modified:   core/src/test/scala/unit/kafka/producer/AsyncProducerTest.scala
#	modified:   core/src/test/scala/unit/kafka/producer/ProducerTest.scala
#	modified:   core/src/test/scala/unit/kafka/producer/SyncProducerTest.scala
#	modified:   core/src/test/scala/unit/kafka/server/LeaderElectionTest.scala
#	modified:   core/src/test/scala/unit/kafka/server/LogOffsetTest.scala
#	modified:   core/src/test/scala/unit/kafka/server/LogRecoveryTest.scala
#	modified:   core/src/test/scala/unit/kafka/server/ServerShutdownTest.scala
#	new file:   core/src/test/scala/unit/kafka/utils/ClientIdTest.scala
#	modified:   core/src/test/scala/unit/kafka/utils/TestUtils.scala
#	modified:   examples/src/main/java/kafka/examples/SimpleConsumerDemo.java
#	modified:   perf/src/main/scala/kafka/perf/ProducerPerformance.scala
#	modified:   perf/src/main/scala/kafka/perf/SimpleConsumerPerformance.scala
#	modified:   system_test/migration_tool_testsuite/migration_tool_test.py
#	modified:   system_test/mirror_maker_testsuite/mirror_maker_test.py
#	modified:   system_test/replication_testsuite/replica_basic_test.py
#	modified:   system_test/replication_testsuite/testcase_9051/cluster_config.json
#	modified:   system_test/replication_testsuite/testcase_9051/testcase_9051_properties.json
#	modified:   system_test/testcase_to_run.json
#	modified:   system_test/utils/kafka_system_test_utils.py
#
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DelayedProduce requests should not hold full producer request data,KAFKA-671,12623612,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriramsub,jjkoshy,jjkoshy,12/Dec/12 19:50,27/Feb/13 01:36,14/Jul/23 05:39,27/Feb/13 01:36,0.8.0,,,0.8.1,,,,,,,,,,0,bugs,p1,,"Per summary, this leads to unnecessary memory usage.",,jjkoshy,jkreps,junrao,nehanarkhede,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/13 02:09;sriramsub;outOfMemFix-v1.patch;https://issues.apache.org/jira/secure/attachment/12564849/outOfMemFix-v1.patch","29/Jan/13 18:09;sriramsub;outOfMemFix-v2-rebase.patch;https://issues.apache.org/jira/secure/attachment/12567017/outOfMemFix-v2-rebase.patch","29/Jan/13 02:00;sriramsub;outOfMemFix-v2.patch;https://issues.apache.org/jira/secure/attachment/12566885/outOfMemFix-v2.patch","30/Jan/13 00:14;sriramsub;outOfMemFix-v3.patch;https://issues.apache.org/jira/secure/attachment/12567082/outOfMemFix-v3.patch","25/Feb/13 18:55;sriramsub;outOfMemFix-v4-rebase.patch;https://issues.apache.org/jira/secure/attachment/12570837/outOfMemFix-v4-rebase.patch","22/Feb/13 23:59;sriramsub;outOfMemFix-v4.patch;https://issues.apache.org/jira/secure/attachment/12570559/outOfMemFix-v4.patch","26/Feb/13 08:30;sriramsub;outOfMemFix-v5.patch;https://issues.apache.org/jira/secure/attachment/12570949/outOfMemFix-v5.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,297330,,,Wed Feb 27 01:36:36 UTC 2013,,,,,,,,,,"0|i14nxj:",235342,,,,,,,,,,,,,,,,,,,,"22/Dec/12 01:01;jkreps;I am concerned this may be a blocker for production usage. If this adds 200k per request with 5000 outstanding requests that is 1G of memory. This is not too far out there for production usage with a high number of producers.;;;","15/Jan/13 02:09;sriramsub;This is one possible fix. The fix would largely depend on the decision for Kafka 703 but I just wanted one possible fix to be available to make better decision on the other bug.

1. We now create a map of topic partion -> messagesizes and DelayedProduce uses this instead of the entire data payload. 
2. We reset the data payload at the end of HandleProduceRequest. 

The only way I have found to reset the immutable maps is to make them vars (Other option is to convert them to mutable maps but that involves a ton of code changes (all the place where the data objects are passed around need to be changed to mutable maps). Let me know of a better option in Scala.

The expectation is that the data object will not be used after the reset method is called.;;;","15/Jan/13 18:53;nehanarkhede;Overall, the changes look good. Minor suggestions -

1. KafkaApis
1.1 Can we rename messageSizeInBytes to messageSetSize ?
1.2 Similarly rename messageSizes to messageSetSize

2. ProducerRequest
I'm guessing the resetData() API is added with the goal of helping with garbage collection ? I think this might not be required since once the produce request is handled, whether or not it entered the purgatory, it will be eligible for garbage collection;;;","15/Jan/13 18:59;sriramsub;2. I don't think so since DelayedProduce is still holding on to the RequestChannel's Request object which is the produceRequest. Let me know if that is not the case.;;;","15/Jan/13 19:10;nehanarkhede;2. You maybe right. However, given that you set a large enough young gen compared to the total heap size, I wonder what is the impact of nullifying the data. It will be worth doing a quick experiment to study GC patterns and confirm that it does help. The only downside of this approach is that it looks ugly.;;;","15/Jan/13 19:15;sriramsub;2. I am not sure if it is an issue in the first place. Depends on how long these objects stay in the queue. If 5000 outstanding requests can be reached as suggested by Jay above then it does seem like an issue. Let me know what you find in your GC investigation. I agree it is ugly. Another option is to totally remove the request object dependency from DelayedItems but that would be a larger change.;;;","20/Jan/13 22:59;junrao;Thanks for the patch. I agree with Neha that ProducerRequest.resetData() seems a bit hacky. Another thing is that currently, we log the request object in trace in RequestChannel.Request when processing the response. Resetting the data in the produce request may break the logic there.

Maybe we should test it out and see if this is really a problem. It's true that there could be many producers. However, it may take some time for them to generate 1GB of data.;;;","20/Jan/13 23:59;junrao;Thinking about this a bit more. There is another approach. In KafkaApis.handle(), after the request is handled, we set RequestChannel.Request.requestObj to null. At this point, the response is not sent yet. Currently, requestObj is needed when sending a response is in RequestChannel.Request.updateRequestMetrics() where we need to print requestObj (in trace). What we can do is that in RequestChannel.Request, after requestObj is constructed, get and cache the string value of request (set the string to null if not trace enabled). We also need to cache isFromFollower which is obtained from requestObj. This way, we don't need ProducerRequest.resetData() and we can guarantee that after KafkaApis.handle(), each requestObj can be GC-ed.;;;","21/Jan/13 00:36;sriramsub;I think to do this right we should limit the change to produceRequest instead of getting rid of the requestobj completely. The reason is as you mention you would need to cache a bunch of fields from requestObj to be used in updateRequestMetrics while sending the response. This can be two fields today but going forward you may need more fields and this would need to be constantly maintained. I think if produceRequest handles data being null, the fix would be more robust and isolates the changes within produce request.;;;","21/Jan/13 18:09;nehanarkhede;Nullifying the request object seems like a bigger change. I'm wondering about the precise impact to GC that these changes will introduce. If the impact is just that the objects get garbage collected within 1 iteration of the young gen collector VS 2-3 iterations, I would say the performance upside of this change is not worth the risk. But if it significantly reduces garbage collected overhead, it might be worth looking further into. Even if we have to do this, I agree with Sriram that his earlier change is smaller impact than nullifying request object and caching a bunch of things to get around it. ;;;","21/Jan/13 18:33;nehanarkhede;Thinking about this a little more, the real problem seems to be that we hang onto the request object in DelayedProduce until we send out the response. There are 2 reasons for this -
1. The request latency metrics are part of the request object. These need to be updated when the response is created.
2. To send out the response, we need the selector key, which is inside the request object.

To handle delayed produce requests without hanging onto the produce request data, we will need to -
1. Remove the request object from DelayedProduce 
2. Pass in the selector key into DelayedProduce
3. Define the request metrics in a separate object and remove those from the Request object. Pass in the new RequestMetrics object into DelayedProduce

Since this requires changing the DelayedRequest object as well, it will affect all requests. My guess is that this refactoring is not that big of a change, but I could be wrong.;;;","29/Jan/13 02:00;sriramsub;- Ignore the changes in bin and system tests
- this change is cleaner and a lot safer. 
  - the data is now an mutable map
  - we just cache the topicpartition - message size in producer request
  - we clear the map after handling the request
  - the toString implementation of produceRequest uses the cached map instead of the data
  - the byteBuffer in RequestChannel.Request is now made private and is set to null after deserialization.
  - update test cases to work with these changes
  - I will be updating the thread with how the heap characteristics look before and after this change.;;;","29/Jan/13 18:09;sriramsub;rebased;;;","29/Jan/13 19:21;nehanarkhede;Thanks for the v2 patch, few review comments -

1. kafka-run-class
Revert these changes, the default heap size of 5g is too large. Also for the GC configs, there is another JIRA tracking it.
2. ProducerRequest
How about renaming emptyData() to clear() for consistency ?
3. What is the purpose of the mutable map changes in DefaultEventHandler ?
;;;","29/Jan/13 23:51;sriramsub;1. You can ignore the changes in system test and bin. I will update with a patch that does not have them.
2. emptyData seems more clear in what is happening in the produceRequest object Vs clear. Let me know if you think otherwise.
3. You need them since ProduceRequest data gets set from the methods in DefaultEventHandler. Is there any other way specific to Scala?;;;","30/Jan/13 05:42;jkreps;Took a look at this. Looks reasonable.

Other atrocities have occurred inside RequestQueue, but they aren't from this patch. Re-opened KAFKA-683. :-)

For maps it is nicer to import scala.collection and then refer to mutable.Map rather than scala.mutable.Map.

I think the question is why do we need to hang onto the ProduceRequest object at all? We are doing work to null it out, but why can't we just take the one or two fields we need from that in the delayed produce? If we do that then won't the producerequest be out of scope after handleProduce and get gc'd? Is the root cause of this the fact that we moved deserialization into the network thread and shoved the api object into the request?;;;","30/Jan/13 06:40;sriramsub;The issue is that even if we pass only the required fields from produceRequest to DelayedProduce, we also pass the actual Request itself to delayedProduce which is used in multiple places. Now that contains the requestObj and hence there is a non zero reference to it still. Further, when sending the response we depend on the requestObj at multiple places in updateRequestMetrics, one of them being the requirement to log the complete request. We would have to do some non trivial changes to be able to get rid of the request object completely which would probably need to wait till a later time.;;;","30/Jan/13 07:05;jkreps;Okay let's sync up. I think requestObj is the devil. :-);;;","31/Jan/13 18:48;sriramsub;KAFKA-745 is tracking the cleanup of RequestChannel. ;;;","05/Feb/13 06:32;sriramsub;Jay - are you fine with checking this in? We would like to see the impact in shadow. We are using Kafka-745 for tracking the cleanup for RequestChannel.;;;","05/Feb/13 14:52;jkreps;I think hacking requestObj is fine as an intermediate step but we need a plan to get rid of it in 0.8. Jun what's the plan? I am +1 when we have figured that out.

One nit pick in the current code is:
     val requestObj: RequestOrResponse = RequestKeys.deserializerForKey(requestId)(buffer)
     buffer.rewind()
+    buffer = null

Why are we keeping the buffer around at all if we immediately null it out (and we probably don't need to rewind it first)...?;;;","22/Feb/13 23:59;sriramsub;rebased;;;","23/Feb/13 19:46;junrao;Patch v4 doesn't apply. 0.8 has moved since the patch is uploaded. Could you rebase again?;;;","25/Feb/13 18:55;sriramsub;rebased;;;","25/Feb/13 21:04;nehanarkhede;Looks good overall. Just one comment -

In the toString(), the map operation is not correct. If we just want to print a list of topic partitions, shouldn't it just be topicPartitionMessageSizeMap.keys.mkString("","")  ?

Other than that, I'm +1 on this. If others don't have any concerns, I can check this in after the above change is made
;;;","25/Feb/13 21:13;sriramsub;We want to print the topicPartition and the size.;;;","25/Feb/13 21:28;nehanarkhede;I see, in that case, I still don't see the use of the map(). It is not transforming anything -
topicPartitionMessageSizeMap.map(r => r._1 -> r._2).toMap.mkString("","");;;","26/Feb/13 08:30;sriramsub;Yes it can be simplified. ;;;","26/Feb/13 21:02;nehanarkhede;+1. If others don't have any other input, I will go ahead and check this in;;;","27/Feb/13 01:36;nehanarkhede;Checked in patch v5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup spurious .index files if present in the log directory on initialization,KAFKA-670,12623596,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jkreps,jkreps,jkreps,12/Dec/12 17:50,13/Dec/12 21:22,14/Jul/23 05:39,13/Dec/12 21:22,,,,,,,,,,,,,,0,,,,"It is possible that an index file could somehow be left on the filesystem with no corresponding log file. This is not currently handled well. If the .index file happens to fall on the same offset as a new log segment, then when that segment is created terrible things will happen.

We should check this condition on initialization and add some unit tests against it.",,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/12 20:17;jkreps;KAFKA-670-v1.patch;https://issues.apache.org/jira/secure/attachment/12560629/KAFKA-670-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,297314,,,Thu Dec 13 21:22:43 UTC 2012,,,,,,,,,,"0|i14ntz:",235326,,,,,,,,,,,,,,,,,,,,"12/Dec/12 20:17;jkreps;Cleanup any bogus index files (those without a corresponding .log file) during log initialization. Also adds a unit test to cover this case.;;;","13/Dec/12 00:15;nehanarkhede;+1. Do you think this would resolve KAFKA-669 as well ?;;;","13/Dec/12 20:56;nehanarkhede;I checked in v1 as part of testing if the git repo works.;;;","13/Dec/12 21:22;jkreps;Closing since Neha checked in the patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controlled shutdown admin tool should not require controller JMX url/port to be supplied,KAFKA-668,12623441,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jjkoshy,jjkoshy,11/Dec/12 19:37,02/Jan/13 22:09,14/Jul/23 05:39,02/Jan/13 22:09,0.8.0,0.8.1,,0.8.0,,,,,,,,,,0,,,,The controlled shutdown admin command takes a zookeeper string and also requires the user to supply the controller's jmx url/port. This is a bit annoying since the purpose of the zookeeper string is to discover the controller. The tool should require exactly one of these options. If zookeeper is supplied then discover the controller and its jmx port (which means we will need to add the jmx port information to zk).,,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-652,,,,,,,,,,"22/Dec/12 01:16;jjkoshy;KAFKA-668-v1.patch;https://issues.apache.org/jira/secure/attachment/12562196/KAFKA-668-v1.patch","02/Jan/13 19:41;jjkoshy;KAFKA-668-v2.patch;https://issues.apache.org/jira/secure/attachment/12562947/KAFKA-668-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,297140,,,Wed Jan 02 22:09:37 UTC 2013,,,,,,,,,,"0|i14lmv:",234969,,,,,,,,,,,,,,,,,,,,"22/Dec/12 01:16;jjkoshy;This is a pretty straightforward change. It's slightly hacky in that I'm appending the :jmxport to the zk string, and it is effectively ignored in the Broker class. I preferred this over adding a jmxPort field to the Broker class as that would be cause wide-spread edits.;;;","02/Jan/13 19:27;junrao;The patch doesn't seem to apply on 0.8 for me.

git apply -p1 ~/Downloads/KAFKA-668-v1.patch 
error: patch failed: core/src/main/scala/kafka/server/KafkaZooKeeper.scala:43
error: core/src/main/scala/kafka/server/KafkaZooKeeper.scala: patch does not apply
;;;","02/Jan/13 19:41;jjkoshy;Needed a rebase.;;;","02/Jan/13 22:03;junrao;Thanks for patch v2. +1;;;","02/Jan/13 22:09;jjkoshy;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Outgoing responses delayed on a busy Kafka broker ,KAFKA-665,12622980,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,nehanarkhede,nehanarkhede,07/Dec/12 22:24,14/May/16 12:42,14/Jul/23 05:39,07/Feb/15 22:34,0.8.0,,,0.9.0.0,,,,,,,,,,0,replication-performance,,,"In a long running test, I observed that after a few hours of operation, few requests start timing out, mainly because they spent very long time sitting in the response queue -

[2012-12-07 22:05:56,670] TRACE Completed request with correlation id 3965966 and client : TopicMetadataRequest:4009, queueTime:1, localTime:28, remoteTime:0, sendTime:3980 (kafka.network.RequestChannel$)
[2012-12-07 22:04:12,046] TRACE Completed request with correlation id 3962561 and client : TopicMetadataRequest:3449, queueTime:0, localTime:29, remoteTime:0, sendTime:3420 (kafka.network.RequestChannel$)
[2012-12-07 22:05:56,670] TRACE Completed request with correlation id 3965966 and client : TopicMetadataRequest:4009, queueTime:1, localTime:28, remoteTime:0, sendTime:3980 (kafka.network.RequestChannel$)

We might have a problem in the way we process outgoing responses. Basically, if the processor thread blocks on enqueuing requests in the request queue, it doesn't come around to processing its responses which are ready to go out. ",,donnchadh,guozhang,jkreps,mumrah,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1043,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,296567,,,Sat Feb 07 22:34:09 UTC 2015,,,,,,,,,,"0|i14a93:",233124,,,,,,,,,,,,,,,,,,,,"02/Apr/13 15:15;mumrah;Maybe RequestChannel.sendRequest and RequestChannel.requestQueue should have a timeout instead of blocking forever?;;;","04/Sep/14 22:00;guozhang;Moving out of 0.8.2 for now.;;;","07/Feb/15 22:34;jkreps;We solved this by making the response queue unbounded. When the request queue is full the network thread will block until some space frees up, but there is no deadlock here, processors can still make progress. We do need to block the processors to avoid continuing to read and enqueue an unbounded number of new requests. You could imagine being smarter about this but I don't think the current situation is too bad.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka server threads die due to OOME during long running test,KAFKA-664,12622938,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jkreps,nehanarkhede,nehanarkhede,07/Dec/12 18:07,06/Jan/16 19:49,14/Jul/23 05:39,18/Dec/12 00:42,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,,,"I set up a Kafka cluster with 5 brokers (JVM memory 512M) and set up a long running producer process that sends data to 100s of partitions continuously for ~15 hours. After ~4 hours of operation, few server threads (acceptor and processor) exited due to OOME -

[2012-12-07 08:24:44,355] ERROR OOME with size 1700161893 (kafka.network.BoundedByteBufferReceive)
java.lang.OutOfMemoryError: Java heap space
[2012-12-07 08:24:44,356] ERROR Uncaught exception in thread 'kafka-acceptor': (kafka.utils.Utils$)
java.lang.OutOfMemoryError: Java heap space
[2012-12-07 08:24:44,356] ERROR Uncaught exception in thread 'kafka-processor-9092-1': (kafka.utils.Utils$)
java.lang.OutOfMemoryError: Java heap space
[2012-12-07 08:24:46,344] INFO Unable to reconnect to ZooKeeper service, session 0x13afd0753870103 has expired, closing socket connection (org.apache.zookeeper.ClientCnxn)
[2012-12-07 08:24:46,344] INFO zookeeper state changed (Expired) (org.I0Itec.zkclient.ZkClient)
[2012-12-07 08:24:46,344] INFO Initiating client connection, connectString=eat1-app309.corp:12913,eat1-app310.corp:12913,eat1-app311.corp:12913,eat1-app312.corp:12913,eat1-app313.corp:12913 sessionTimeout=15000 watcher=org.I0Itec.zkclient.ZkClient@19202d69 (org.apache.zookeeper.ZooKeeper)
[2012-12-07 08:24:55,702] ERROR OOME with size 2001040997 (kafka.network.BoundedByteBufferReceive)
java.lang.OutOfMemoryError: Java heap space
[2012-12-07 08:25:01,192] ERROR Uncaught exception in thread 'kafka-request-handler-0': (kafka.utils.Utils$)
java.lang.OutOfMemoryError: Java heap space
[2012-12-07 08:25:08,739] INFO Opening socket connection to server eat1-app311.corp/172.20.72.75:12913 (org.apache.zookeeper.ClientCnxn)
[2012-12-07 08:25:14,221] INFO Socket connection established to eat1-app311.corp/172.20.72.75:12913, initiating session (org.apache.zookeeper.ClientCnxn)
[2012-12-07 08:25:17,943] INFO Client session timed out, have not heard from server in 3722ms for sessionid 0x0, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2012-12-07 08:25:19,805] ERROR error in loggedRunnable (kafka.utils.Utils$)
java.lang.OutOfMemoryError: Java heap space
[2012-12-07 08:25:23,528] ERROR OOME with size 1853095936 (kafka.network.BoundedByteBufferReceive)
java.lang.OutOfMemoryError: Java heap space


It seems like it runs out of memory while trying to read the producer request, but its unclear so far. ",,bhaskarv82@gmail.com,ijuma,jjkoshy,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/12 19:20;jjkoshy;KAFKA-664-v3.patch;https://issues.apache.org/jira/secure/attachment/12560617/KAFKA-664-v3.patch","14/Dec/12 23:50;jjkoshy;KAFKA-664-v4.patch;https://issues.apache.org/jira/secure/attachment/12561061/KAFKA-664-v4.patch","09/Dec/12 19:33;nehanarkhede;Screen Shot 2012-12-09 at 11.22.50 AM.png;https://issues.apache.org/jira/secure/attachment/12560112/Screen+Shot+2012-12-09+at+11.22.50+AM.png","09/Dec/12 19:33;nehanarkhede;Screen Shot 2012-12-09 at 11.23.09 AM.png;https://issues.apache.org/jira/secure/attachment/12560113/Screen+Shot+2012-12-09+at+11.23.09+AM.png","09/Dec/12 19:33;nehanarkhede;Screen Shot 2012-12-09 at 11.31.29 AM.png;https://issues.apache.org/jira/secure/attachment/12560114/Screen+Shot+2012-12-09+at+11.31.29+AM.png","10/Dec/12 19:50;jjkoshy;kafka-664-draft-2.patch;https://issues.apache.org/jira/secure/attachment/12560258/kafka-664-draft-2.patch","08/Dec/12 01:55;nehanarkhede;kafka-664-draft.patch;https://issues.apache.org/jira/secure/attachment/12560004/kafka-664-draft.patch","07/Dec/12 18:08;nehanarkhede;thread-dump.log;https://issues.apache.org/jira/secure/attachment/12559911/thread-dump.log","08/Dec/12 01:55;nehanarkhede;watchersForKey.png;https://issues.apache.org/jira/secure/attachment/12560005/watchersForKey.png",,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,296525,,,Wed Jan 06 19:49:51 UTC 2016,,,,,,,,,,"0|i149yn:",233077,,,,,,,,,,,,,,,,,,,,"07/Dec/12 18:08;nehanarkhede;Attaching a thread dump that shows -

1. 4 processor threads and the acceptor threads are dead
2. Rest of the processor threads have a full request queue, and they are waiting to add to the request queue.;;;","07/Dec/12 18:12;nehanarkhede;Another observation - The server is probably GCing quite a lot, since I see the following in the server logs -

[2012-12-07 09:32:14,742] INFO Client session timed out, have not heard from server in 1204905ms for sessionid 0x23afd074d6600ea, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)

The zookeeper session timeout is pretty high (15secs) and it is in the same DC as the Kafka cluster and the producer;;;","07/Dec/12 19:46;jkreps;One pain of oom is that the thing leaking the memory is not necessarily the thing that gets the exception. Can you rerun with -XX:+HeapDumpOnOutOfMemoryError;;;","07/Dec/12 19:51;nehanarkhede;Heap dump is here - http://people.apache.org/~nehanarkhede/kafka-misc/kafka-0.8/heap-dump.tar.gz
Almost all the largest objects trace back to RequestPurgatory$ExpiredRequestReaper as the GC root.;;;","07/Dec/12 19:51;nehanarkhede;I'm re-running the tests with that option now;;;","07/Dec/12 21:46;jkreps;Looks like the problem is in request purgatory--watchers aren't getting removed.;;;","07/Dec/12 21:47;nehanarkhede;The root cause seems to be that watchersForKey map keeps growing. I see that we add keys to the map, but never actually delete them.;;;","07/Dec/12 23:31;jjkoshy;To clarify, the map itself shouldn't grow indefinitely right? - i.e., if there are no new partitions the number of keys should be the same. I think the issue is that expired requests (for a key) are not removed from the list of outstanding requests for that key.;;;","07/Dec/12 23:38;jjkoshy;Okay I'm slightly confused. Even on expiration the request is marked as satisfied. So even if it is not removed from the watcher's list during expiration it will be removed on the next call to collectSatisfiedRequests - which in this case will be when the next produce request arrives to that partition. Which means this should only be due to low-volume partitions that are no longer growing. i.e., the replica fetcher would keep issuing fetch requests that keep expiring but never get removed from the list of pending requests in watchersFor(the-low-volume-partition).;;;","07/Dec/12 23:44;jkreps;Another issue is that we are saving the full producer request in memory for as long as it is in purgatory. Not sure that is causing this, but that is pretty bad.;;;","08/Dec/12 01:55;nehanarkhede;The problem was ever increasing requests in the watchersForKey map. Please look at the graph attached. In merely 40 minutes of running the broker, the number of requests in the purgatory map shot upto 4 million.
This can happen for very low volume topics since the replica fetcher requests keep entering this map, and since there are no more produce requests coming for those topics/partitions, no one ever removes those requests from the map. 

With Joel's help, hacked RequestPurgatory to force the cleanup of expired/satisfied requests by the expiry thread inside purgeSatisfied. Of course, a better solution is re-designing the purgatory data structure to point from the queue to the map, but that is a bigger change. I just want to get around this issue and continue performance testing.
;;;","08/Dec/12 05:19;jjkoshy;+1 

Some minor comments:
- We can probably remove the WatchersForKey gauge (or maybe keep it until the RequestPurgatory refactoring is done).
- While I agree we should definitely refactor the RequestPurgatory to fix the inefficient scan, I think this approach is not as hacky as it sounds. i.e., on fetch request expiration, this now does what would have been done if a produce request to that key had arrived; so we can consider the overhead of this approach as sending additional produce requests to the affected partition at the rate of fetch expirations (which by default is 2/sec). We can optimize a bit more, by adding a threshold for cleanup. i.e., do the iteration and check/removal only if watchers.requests.size > threshold.
;;;","09/Dec/12 19:33;nehanarkhede;Joel,

Thanks for the quick review. I wasn't intending to checkin the draft patch, just threw it up here for people to take a look and give feedback. I'm not sure I understand the purgatory enough to checkin the right fix, mostly interested in crossing this hurdle and proceed with perf testing :)

Having said that, I tested the draft patch and as you can see from the attached screenshots, it doesn't seem to solve the problem. The memory consumption due to the requests in the watchersFor map and the corresponding GC activity is very high and keeps on increasing. ;;;","10/Dec/12 19:50;jjkoshy;Can you try this instead? The update should run as requests expire, not in the purge step - which only runs when a cleanup threshold is reached.;;;","10/Dec/12 23:30;nehanarkhede;Joel, patch v2 works better and I see why v1 didn't work out. Thanks for the quick turnaround !;;;","11/Dec/12 06:08;junrao;If the problem is due to an expired request not being removed from the request LinkedList in the watcher, then there should be at most 1 such outstanding request per topic/partition. So, if the number of topic/partition is fixed, the memory space taken by those outstanding requests should be bounded too, right? Not sure why this causes memory usage to keep going up.;;;","11/Dec/12 06:17;nehanarkhede;For the current implementation, that is true only for the DelayQueue but not for the watchersForKey map. The memory consumption keeps on increasing because we never circle around to cleaning up the entries in the map. When entries are satisfied/expired, they only exit the queue and are ""marked"" as expired/satisfied.;;;","11/Dec/12 06:34;junrao;Got it. So the issue is that for low volume topics, the fetch requests made by the followers keep got timed out. Those timeouted requests won't be removed from the request LinkedList in the watcher until the next produce request for that topic comes, which could be a long time.;;;","11/Dec/12 06:44;nehanarkhede;That's correct. I'm tempted to checkin v2 for now and wait for the purgatory refactor patch. Until then, we can probably keep the JIRA open. Thoughts ?;;;","11/Dec/12 13:17;jkreps;This seems to iterate the whole list for every expiration. That seems a bit excessive, no? What about just doing a ""full clean"" every 100 expirations or something like that...?

I agree we should do something like this rather than attempt a rewrite.;;;","11/Dec/12 20:28;jjkoshy;Agreed that we should checkin this fix + the throttling (forgot to add that in v2) and open a separate jira to refactor the purgatory a bit.;;;","11/Dec/12 23:42;jjkoshy;One problem with the updated patch is the following: multi-fetch requests could be satisfied (since minBytes is 1).
i.e., the request will be marked as satisfied and the expiration code path will not be executed. We should do the update
on expiration even if the request has been satisfied.

So an additional ""catch-all"" that may make life easier for us is to have a global threshold of the requestsFor map - i.e.,
if its size exceeds a threshold (which will be checked on both expiration/checkSatisfied) then trigger a full cleanup - i.e.,
iterate over all entries in watchersFor and remove those that are satisfied.;;;","12/Dec/12 19:20;jjkoshy;Here is a simpler and hopefully safer approach of using a global request counter and purging both the delay queue and watcher map. Also, I changed the existing gauge to just count the number of requests in purgatory (i.e., over both the delay queue and the map).
;;;","12/Dec/12 19:51;jjkoshy;Also, just filed KAFKA-671 to address the issue that Jay pointed out.;;;","13/Dec/12 05:53;junrao;Thanks for patch v3. +1 from me. One minor comment: Should we make CleanupInterval configurable?;;;","13/Dec/12 19:03;nehanarkhede;Thanks for patch v3, Joel ! It works correctly, here are a few review suggestions -

1. Can we bring back the exact count of delayed requests sitting in the purgatory ? Your patch removed that, added a metric for the total size of purgatory, which is good to have as well. But its inaccuracy is inversely proportional to the time till the next cleanup interval.
2. It seems like the cleanup is happening on the request handler thread. Can it happen on the background expiry reaper thread instead ? I understand the latency impact is minimal, but it still seems like a good idea to put extra processing on a background thread, which we already have
3. CleanupThreshold can be hard coded in the future. Until then, for performance tuning, it will be very convenient to have it configurable :)

;;;","13/Dec/12 19:13;jkreps;+1 on doing cleanup in the expiry thread--expiry is a lot less latency sensitive.;;;","14/Dec/12 23:50;jjkoshy;All good points - here is v4 with those changes.;;;","17/Dec/12 22:40;nehanarkhede;+1 on v4;;;","18/Dec/12 00:42;jjkoshy;Committed on 0.8;;;","06/Jan/16 10:42;bhaskarv82@gmail.com;I am seeing similar errors on Kafka which is running on version 0.8.2.;;;","06/Jan/16 11:25;ijuma;Vijay, you should probably file a new issue and include as much information as possible. Also, it would be interesting to know if this still happens with 0.9.0.0 (if possible).;;;","06/Jan/16 19:49;bhaskarv82@gmail.com;Thanks for the suggestion, Ismael.
I have created KAFKA-3071 ID.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent a shutting down broker from re-entering the ISR,KAFKA-661,12622841,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jjkoshy,jjkoshy,07/Dec/12 01:14,30/Dec/19 22:54,14/Jul/23 05:39,30/Dec/19 22:54,0.8.0,0.8.1,,,,,,,,,replication,,,0,,,,"There is a timing issue in controlled shutdown that affects low-volume topics. The leader that is being shut down receives a leaderAndIsrRequest informing it is no longer the leader and thus starts up a follower which starts issuing fetch requests to the new leader. We then shrink the ISR and send a StopReplicaRequest to the shutting down broker. However, the new leader upon receiving the fetch request expands the ISR again.

This does not really have critical impact in the sense that it can cause producers to that topic to timeout. However, there are probably very few or no produce requests coming in as it primarily affects low-volume topics. The shutdown logic itself seems to be working correctly in that the leader has been successfully moved.

One possible approach would be to use the callback feature in the ControllerBrokerRequestBatch and wait until the StopReplicaRequest has been processed by the shutting down broker before shrinking the ISR; and there are probably other ways as well.",,bkirwi,boniek,hachikuji,jjkoshy,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-7395,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,296414,,,Mon Dec 30 22:54:50 UTC 2019,,,,,,,,,,"0|i1497r:",232956,,,,,,,,,,,,,,,,,,,,"01/Feb/14 19:57;nehanarkhede;0.8.1 will add callbacks as part of delete topic. These could be used to fix the issue described here.;;;","10/May/18 15:14;bkirwi;{quote}The leader that is being shut down receives a leaderAndIsrRequest informing it is no longer the leader and thus starts up a follower which starts issuing fetch requests to the new leader. We then shrink the ISR and send a StopReplicaRequest to the shutting down broker. However, the new leader upon receiving the fetch request expands the ISR again.
{quote}
This seems to happen when the dying broker is a follower as well, for similar reasons: it can send a fetch request after the controlled shutdown request is complete, which re-expands the ISR to include the dying broker.

I'm having a look at what it will take to use the stop-replica callbacks to implement this suggestion. Hopefully not too complicated!;;;","30/Dec/19 22:54;hachikuji;This patch is fixed by KIP-320, which adds leader epoch validation to the replica fetchers. When the ISR is shrunk for the shutting down broker, the leader epoch will be bumped. Followers fetching with an older epoch will be fenced. This, along with the leader and ISR znode version checking ensures that a shutting down replica cannot be added back to the ISR.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Change (topic, partition) tuples to TopicAndPartition",KAFKA-660,12622732,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,swapnilghike,swapnilghike,06/Dec/12 22:40,07/Feb/15 23:36,14/Jul/23 05:39,07/Feb/15 23:36,0.8.1,,,,,,,,,,,,,0,,,,"For any use cases like the following :
 private val allPartitions = new Pool[(String, Int), Partition]

we should convert (topic, partition) tuples to TopicAndPartition objects.",,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,296397,,,2012-12-06 22:40:36.0,,,,,,,,,,"0|i1493j:",232937,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Irrecoverable error while trying to roll a segment that already exists,KAFKA-654,12618991,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,06/Dec/12 06:19,19/Dec/17 23:43,14/Jul/23 05:39,10/Dec/12 18:24,0.8.0,,,,,,,,,,,,,0,,,,"I tried setting up a 5 broker 0.8 cluster and sending messages to 100s of topics on it. For a couple of topic partitions, the produce requests never succeed since they fail on the leader with the following error - 

[2012-12-05 22:54:05,711] WARN [Kafka Log on Broker 2], Newly rolled segment file 0000000000000000000
0.log already exists; deleting it first (kafka.log.Log)
[2012-12-05 22:54:05,711] WARN [Kafka Log on Broker 2], Newly rolled segment file 0000000000000000000
0.index already exists; deleting it first (kafka.log.Log)
[2012-12-05 22:54:05,715] ERROR [ReplicaFetcherThread-1-0-on-broker-2], Error due to  (kafka.server.R
eplicaFetcherThread)
kafka.common.KafkaException: Trying to roll a new log segment for topic partition NusWriteEvent-4 with start offset 0 while it already exsits
        at kafka.log.Log.rollToOffset(Log.scala:456)
        at kafka.log.Log.roll(Log.scala:434)
        at kafka.log.Log.maybeRoll(Log.scala:423)
        at kafka.log.Log.append(Log.scala:257)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:51)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:125)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:108)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:125)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:108)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)

",,jjkoshy,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-6388,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/12 20:00;nehanarkhede;kafka-654-v1.patch;https://issues.apache.org/jira/secure/attachment/12560116/kafka-654-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,296279,,,Mon Dec 10 18:24:00 UTC 2012,,,,,,,,,,"0|i1486n:",232789,,,,,,,,,,,,,,,,,,,,"06/Dec/12 18:17;jjkoshy;FWIW, it should be easy to reproduce this - I just saw it on a two node cluster / one topic but am looking at another issue right now.

[2012-12-06 10:10:18,380] ERROR [ReplicaFetcherThread-0-0-on-broker-1], Error due to  (kafka.server.ReplicaFetcherThread)
kafka.common.KafkaException: Trying to roll a new log segment for topic partition abc-0 with start offset 0 while it already exsits
        at kafka.log.Log.rollToOffset(Log.scala:456)
        at kafka.log.Log.roll(Log.scala:434)
        at kafka.log.Log.maybeRoll(Log.scala:423)
        at kafka.log.Log.append(Log.scala:257)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:51)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:125)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:108)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:108)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)
[2012-12-06 10:10:18,382] INFO [ReplicaFetcherThread-0-0-on-broker-1], Stopped  (kafka.server.ReplicaFetcherThread);;;","06/Dec/12 18:37;jkreps;I think this is almost certainly a bug in the log. I will come by and discuss.;;;","06/Dec/12 20:31;nehanarkhede;The bug is inside Log.truncateAndStartWithNewOffset

      val deletedSegments = segments.trunc(segments.view.size)
      segments.append(new LogSegment(dir,
                                     newOffset,
                                     indexIntervalBytes = indexIntervalBytes, 
                                     maxIndexSize = maxIndexSize))
      deleteSegments(deletedSegments)

The order of deleteSegments and segments.append is reversed. Due to this, we end up adding an already full index as the last entry in the segments array, but delete it later from disk. During maybeRoll, it finds the index to be full and errors on the rolling of the new log segment.

However, in my testing, I saw this on the leader as well. I've not been able to fix that yet.;;;","09/Dec/12 20:00;nehanarkhede;Patch to fix the problem on the follower as explained above. However, I couldn't reproduce this issue on the leader. I guess we can file another JIRA if we are able to find that issue again;;;","10/Dec/12 17:34;jkreps;+1

I had fixed this on trunk:
http://svn.apache.org/repos/asf/kafka/trunk/core/src/main/scala/kafka/log/Log.scala
Double checked that it didn't get reintroduced in the async delete patch:
https://issues.apache.org/jira/secure/attachment/12559981/KAFKA-636-v1.patch

So it looks like it was only on 0.8.;;;","10/Dec/12 18:24;nehanarkhede;Thanks for the review, committed patch v1 to 0.8 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use uniform convention for naming properties keys ,KAFKA-648,12618781,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,sriramsub,swapnilghike,swapnilghike,04/Dec/12 20:04,14/Jan/13 05:52,14/Jul/23 05:39,12/Jan/13 00:14,0.8.0,,,0.8.0,0.8.1,,,,,,,,,0,,,,"Currently, the convention that we seem to use to get a property value in *Config is as follows:

val configVal = property.getType(""config.val"", ...) // dot is used to separate two words in the key and the first letter of second word is capitalized in configVal.

We should use similar convention for groupId, consumerId, clientId, correlationId.

This change will probably be backward non-compatible.",,jfung,jkreps,junrao,nehanarkhede,sriramsub,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/13 22:15;sriramsub;configchanges-1.patch;https://issues.apache.org/jira/secure/attachment/12563174/configchanges-1.patch","08/Jan/13 01:44;sriramsub;configchanges-v2.patch;https://issues.apache.org/jira/secure/attachment/12563666/configchanges-v2.patch","08/Jan/13 23:05;sriramsub;configchanges-v3.patch;https://issues.apache.org/jira/secure/attachment/12563847/configchanges-v3.patch","09/Jan/13 19:53;sriramsub;configchanges-v4.patch;https://issues.apache.org/jira/secure/attachment/12564016/configchanges-v4.patch","11/Jan/13 00:05;sriramsub;configchanges-v5.patch;https://issues.apache.org/jira/secure/attachment/12564296/configchanges-v5.patch","11/Jan/13 23:39;sriramsub;configchanges-v6-rebased.patch;https://issues.apache.org/jira/secure/attachment/12564516/configchanges-v6-rebased.patch","11/Jan/13 20:18;sriramsub;configchanges-v6.patch;https://issues.apache.org/jira/secure/attachment/12564480/configchanges-v6.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,296029,,,Sat Jan 12 02:27:04 UTC 2013,,,,,,,,,,"0|i144s7:",232238,,,,,,,,,,,,,,,,,,,,"04/Dec/12 20:09;jkreps;word.;;;","04/Dec/12 20:20;nehanarkhede;Not sure I got the part about config.val, where first letter of second word is capitalized. Mind giving an example ?;;;","04/Dec/12 20:54;jkreps;A bunch of the config values are non standard. For example KafkaConfig.brokerId is given by the property ""brokerid"" which should be ""broker.id"". What I take Swapnil to be saying is that all properties should be in the form
  a.config.name
and the scala variable should always be the same thing, but in camel case,
  aConfigName
Would be great to do this in 0.8 with the other compatibility changes.;;;","04/Dec/12 20:59;jkreps;I actually recommend that we do a full config review and just change not only things that are poorly capitalized but also things which have inconsistent naming conventions or that are just confusing. For example 100% of properties related to segment retention are called log.retention.x EXCEPT for log.cleanup.interval.mins--why???? Also some times are given in mins or hours when a more granular time interval might be more useful. Or another: log.file.size should really be log.segment.size to make it clear that it is the size of the log segments not the total size of the log. I recommend we just make a full list of all configs and review them all. It will be boring but worth it.;;;","03/Jan/13 22:15;sriramsub;1. Ensured that all config names have uniform naming format
2. Changed some config names to better reflect their usage

I did not modify any time formats as part of this change. Let me know which configs need more fine grained time specifications.;;;","04/Jan/13 02:41;junrao;Thanks for the patch. Some comments:

1. ConsumerConfig: We have both enable.shallow.iterator and auto.commit.enable. Need to standardize.

2. We need to make a pass of the names in SyncProducerConfigShared and AsyncProducerConfig too.

3. Could you change the affected property names in other directories (config/, system_tests/, examples/, perf/, contrib/) too?;;;","08/Jan/13 01:44;sriramsub;1. Standardized usage of enable
2. Fixed config names in SyncPRoducerConfigShared and AsyncProducerConfig
3. Made the property changes in system_tests, perf;;;","08/Jan/13 02:33;junrao;Thanks for patch v2. I missed a few items in the previous review. Sorry about that. The following are some more comments.

20. We need to standardize properties in ZKConfig too. 

21. KafkaConfig: The following names are pretty long.
      replica.fetch.max.wait.time.ms
      replica.fetch.min.expected.bytes
  Would it be better to change them to
      replica.fetch.max.wait.ms
      replica.fetch.min.bytes

22. config/producer.properties: The following properties no longer exist.
# the callback handler for one or multiple events 
#callback.handler=

# properties required to initialize the callback handler 
#callback.handler.props=

# the handler for events 
#event.handler=

# properties required to initialize the event handler 
#event.handler.props=

23. config/server.properties: The following property no longer exists.
# Overrides for for the default given by num.partitions on a per-topic basis
#topic.partition.count.map=topic1:3, topic2:4
;;;","08/Jan/13 18:05;jfung;The testcase_xxxx_properties.json files in System Test will need to be updated for the following changes:

brokerid => broker.id
log.file.size => log.segment.size
groupid => group.id

A separate JIRA KAFKA-688 is created for this task.;;;","08/Jan/13 23:05;sriramsub;- made a scan through ZKConfigs
- There are many other properties that seem to be longer than these. I have fixed the two mentioned.
- Removed the unused configs;;;","09/Jan/13 16:34;jfung;Hi Sriram,

The change in system_test/migration_tool_testsuite/config/migration_consumer.properties is not needed because the migration tool consumer is using 0.7 library and would not be aware of this naming convention change.

Other than that, changes in the System Test part looks good.;;;","09/Jan/13 19:53;sriramsub;Reverted the change from migrationtool/consumer.properties;;;","09/Jan/13 20:55;jfung;Tested v4 patch with KAFKA-688-v1.patch with the latest patch and it works fine.;;;","09/Jan/13 22:11;junrao;Thanks for patch v4. Made another pass. Should we make the following changes?

40. KafkaConfig:
max.message.size => max.message.bytes
socket.send.buffer => socket.send.buffer.bytes
socket.receive.buffer => socket.receive.buffer.bytes
log.segment.size => log.segment.bytes
log.retention.size => log.retention.bytes
log.flush.interval => log.flush.interval.messages
log.default.flush.interval.ms => log.flush.interval.ms
log.flush.intervals.ms.per.topic => log.flush.interval.ms.per.topic
replica.socket.buffersize => replica.socket.receive.buffer.bytes
replica.fetch.size => replica.max.fetch.bytes
fetch.request.purgatory.purge.interval => fetch.purgatory.purge.interval.requests
producer.request.purgatory.purge.interval => producer.purgatory.purge.interval.requests
replica.max.lag.time.ms => max.replica.lag.time.ms
replica.max.lag.bytes => max.replica.lag.bytes
replica.fetch.max.wait.ms => max.replica.fetch.wait.ms
replica.fetch.min.bytes => min.replica.fetch.bytes

41. ProducerConfig: producer.retry.count. In other configs we have num.network.threads. To be consistent, shouldn't we use num.producer.retries?

42. AsyncProducerConfig:
queue.time => queue.time.ms
buffer.size => socket.send.buffer.bytes

43. ConsumerConfig:
socket.buffer.size => socket.receive.buffer.bytes
fetch.size => max.fetch.bytes
;;;","11/Jan/13 00:05;sriramsub;40 / 42/ 43
Accepted the suggestions but handled them differently. Specifying max and min at the beginning will cause configs related to the same feature to not look similar. For Example, 

max.log.Index.size and log.roll.hours are both configs related to logs but end up looking different. 

Instead, the configs use the following format - 

  ConfigName => ComponentName AnyString [Max/Min] [Unit]

  FeatureName => Name of the  component/feature this config is used for. Example - log, replica, etc.

  AnyString => A string that represents what this config is used for

  Max/Min => Optional. Used if the config represents a max or min value. For example, replicaLagTimeMaxMs

  Unit => Optional. The unit of the value the config represents. For example, replicaLagMaxBytes for value specified in bytes.

41 Removed the producer prefix in producer configs. 

John you may have to fix the json files once more to work with the new changes.;;;","11/Jan/13 16:34;junrao;Thanks for patch v5. Some of the comments in those config files are outdated or are missing. Made another pass.

50. SyncProducerConfig: max.message.size => max.message.bytes

51. KafkaConfig: 
51.1 auto.create.topics => auto.create.topics.enable
51.2 We probably should remove the support of ""log.dir"" and default ""log.dirs"" to /tmp/kafka-logs (defaulting to """" is confusing).

52. ConsumerConfig: Fix the following grammar.
""to immediate satisfy min.fetch.bytes"" => ""to immediately satisfy min.fetch.bytes""

53. KafkaConfig:
53.1 replicaLagTimeMaxMs: add the following comments 
//If a follower hasn't sent any fetch requests during this time,  the leader will remove the follower from isr.
53.2 replica.lag.max.bytes: The name is out dated since offsets are now logical, instead of physical. So we need to change this property to replica.lag.max.messages and add the following comment.
//If the lag in messages between a leader and a follower exceeds this number, the leader will remove the follower from isr.
We also need to change the input parameter name in maybeShrinkIsr() and getOutOfSyncReplicas() in Partition from bytes to messages too.
53.3 auto.create.topics => auto.create.topics.enable
""highwater mark"" => ""high watermark""

54. ProducerConfig:
54.1 messageSendMaxRetries: The comment is outdated. Let's change it to the following:
//The leader may be unavailable transiently, which can fail the sending of a message. This property specifies the number of retries when such failures occur.
54.2 retryBackoffMs: Let's add the following comment.
//Before each retry, the producer refreshes the metadata of relevant topics. Since leader election takes a bit of time, this property specifies the amount of time that the producer waits before refreshing the metadata.;;;","11/Jan/13 20:18;sriramsub;- Updated the comments.
- Did not remove log.dir as there are systems tests depending on it. Just used a default path instead of """".
- Removed messageMaxSize from producer as it is no longer used.;;;","11/Jan/13 23:39;sriramsub;rebased;;;","11/Jan/13 23:56;swapnilghike;Will it make sense to create a Utils function to get the value of logDirs and move the require(logDirs.size > 0) inside this function? Currently, that's the only require() call in KafkaConfig. This new function will probably be too specific to logDirs though.;;;","12/Jan/13 00:14;junrao;Thanks for the last patch. +1. Committed to 0.8 after fixing clienid in ProducerPerformance. ;;;","12/Jan/13 02:27;junrao;Merged to trunk and resolved conflicts.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide aggregate stats at the high level Producer and ZookeeperConsumerConnector level,KAFKA-646,12618775,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,04/Dec/12 19:44,17/Dec/12 22:07,14/Jul/23 05:39,17/Dec/12 22:07,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,,,"WIth KAFKA-622, we measure ProducerRequestStats and FetchRequestAndResponseStats at the SyncProducer and SimpleConsumer level respectively. We could also aggregate them in the high level Producer and ZookeeperConsumerConnector level to provide an overall sense of request/response rate/size at the client level. Currently, I am not completely clear about the math that might be necessary for such  aggregation or if metrics already provides an API for aggregating stats of the same type.

We should also address the comments by Jun at KAFKA-622, I am copy pasting them here:

60. What happens if have 2 instances of Consumers with the same clientid in the same jvm? Does one of them fail because it fails to register metrics? Ditto for Producers.
61. ConsumerTopicStats: What if a topic is named AllTopics? We use to handle this by adding a - in topic specific stats.
62. ZookeeperConsumerConnector: Do we need to validate groupid?
63. ClientId: Does the clientid length need to be different from topic length?
64. AbstractFetcherThread: When building a fetch request, do we need to pass in brokerInfo as part of the client id? BrokerInfo contains the source broker info and the fetch requests are always made to the source broker.",,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/12 01:06;swapnilghike;kafka-646-patch-num1-v1.patch;https://issues.apache.org/jira/secure/attachment/12559995/kafka-646-patch-num1-v1.patch","08/Dec/12 06:16;swapnilghike;kafka-646-patch-num1-v2.patch;https://issues.apache.org/jira/secure/attachment/12560022/kafka-646-patch-num1-v2.patch","10/Dec/12 21:16;swapnilghike;kafka-646-patch-num1-v3.patch;https://issues.apache.org/jira/secure/attachment/12560279/kafka-646-patch-num1-v3.patch","10/Dec/12 22:17;swapnilghike;kafka-646-patch-num1-v4.patch;https://issues.apache.org/jira/secure/attachment/12560292/kafka-646-patch-num1-v4.patch","11/Dec/12 19:48;swapnilghike;kafka-646-patch-num1-v5.patch;https://issues.apache.org/jira/secure/attachment/12560434/kafka-646-patch-num1-v5.patch","12/Dec/12 08:31;swapnilghike;kafka-646-patch-num1-v6.patch;https://issues.apache.org/jira/secure/attachment/12560529/kafka-646-patch-num1-v6.patch","12/Dec/12 19:04;swapnilghike;kafka-646-patch-num1-v7.patch;https://issues.apache.org/jira/secure/attachment/12560615/kafka-646-patch-num1-v7.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,296023,,,Mon Dec 17 22:07:44 UTC 2012,,,,,,,,,,"0|i144qv:",232232,,,,,,,,,,,,,,,,,,,,"08/Dec/12 01:06;swapnilghike;This patch has a bunch of refactoring changes and a couple of new additions. 

Addressing Jun's comments: 
These are all great catches! Thanks for being so thorough.

60. By default, metrics-core will return an existing metric object of the same name using a getOrCreate() like functionality. As discussed offline, we should fail the clients that use an already registered clientId name. We will need to create two objects thaty contain hashmaps to record the existing producer and consumer clientIds and methods to throw an exception if a client attempts to use an existing clientId. I worked on this change a bit, but it breaks a lot of our unit tests (about half) and the refactoring will take some time. Hence, I think it will be better if I submit a patch for all other changes and create another patch for this issue under this jira. Until then we can keep this jira open.

61. For recording stats about all topics, I am now using a string ""All.Topics"". Since '.' is not allowed in the legal character set for topic names, this will differentiate from a topic named AllTopics.

62. Yes, we should validate groupId. Added the functionality and a unit test. It has the same validation rules as ClientId.

63. A metric name is something like (clientId + topic + some string) and this entire string is limited by fillename size. We already allow topic name to be at most 255 bytes long. We could fix max lengths for each of clientId, groupId, topic name so that the metric name never exceeds filename size. But those lengths will be quite arbitrary, perhaps we should skip the check on the length of clientId and groupId. 

64. Removed brokerInfo from the clientId used to instantiate FetchRequestBuilder.


Refactoring: 
1. Moved validation of clientId at the end of instantiation of ProducerConfig and ConsumerConfig. 
- Created static objects ProducerConfig and ConsumerConfig which contain a validate() method.

2. Created global *Registry objects in which each high level Producer and Consumer can register their *stats objects.
- These objects are registered in the static object only once using utils.Pool.getAndMaybePut functionality. 
- This will remove the need to pass *stats objects around the code in constructors (I thought having the metrics objects right up in the constructors was a bit intrusive, since one doesn't quite always think about the monitoring mechanism while instantiating various modules of the program, for example while unit testing.)
- Instead of the constructor, each concerned class obtains the *Stats objects from the global registry object.
- This cleans up any metrics objects created in the unit tests.
- Special mention: The producer constructors are back to the old themselves. With clientId validation moved to *Config objects, the intermediate Producer constructor that merely separated the parameters of a quadruplet is gone.

3. Created separate files
-  for ProducerStats, ProducerTopicStats, ProducerRequestStats in kafka.producer package and for FetchRequestAndResponseStats in kafka.consumer package. Thought it was appropriate given that we already had ConsumerTopicStats in a separate file, and since the code for metrics had increased in size due to addition of *Registry and Aggregated* objects. Added comments.
- for objects Topic, ClientId and GroupId in kafka.utils package.
- to move the helper case classes ClientIdAndTopic, ClientIdAndBroker to kafka.common package. 

4. Renamed a few variables to easier names (anyOldName to ""metricId"" change).


New additions: 
1. Added two objects to aggregate metrics recorded by SyncProducers and SimpleConsumers at the high level Producer and Consumer. 
- For this, changed KafkaTimer to accept a list of Timers. Typically we will pass a specificTimer and a globalTimer to this KafkaTimer class. Created a new KafkaHistogram in a similar way.

2. Validation of groupId.


Issues:
1. Initializing the aggregator metrics with default values: For example, let's say that a syncProducer could be created (which will register a ProducerRequestStats mbean for this syncProducer). However, if no request is sent by this syncProducer then the absense of its data is not reflected in the aggregator histogram. For instance, the min requestSize for the syncProducer that never sent a request will be 0, but this won't be accurately represented in the aggregator histogram. Thus, we need to understand that if the request count of a syncProducer is 0, then its data will not be accurately reflected in the aggregator histogram.

The question is whether it is possible to inform the aggregator histogram of some default values without increasing the request count of any syncProducer or the aggregated stats.


Further proposed changes: 
Another patch under this jira to address comment 60 by Jun.;;;","08/Dec/12 03:12;swapnilghike;Actually I just realized that the Aggregated*Stats objects that I have created are not consistent with the way ""All.Topics-MessageRate"" is measured. It is possible to measure ""All.Brokers-producerRequestSize"" in a similar way in ProducerRequestStats.
But it's not possible to measure ""All.Brokers-ProduceRequestRateAndTimeMs"" in the same manner since we use a timer block. 

To make everything look consistent, I can delete the aggregator objects from my v1 patch and create a KafkaMeter class that accepts a list of meters. Will upload another version of patch.

;;;","08/Dec/12 06:16;swapnilghike;Attached patch v2. 

The changes from patch v1:
1. Deleted KafkaHistogram class. (There is also no need for KafkaMeter class.)

2. Deleted the Aggregated*Stats objects. 
- The metrics of SyncProducer and SimpleConsumer for different brokers are aggregated together using the same way the producerTopicStats are aggregated for ""All.Topics"". 
- Measuring the time for produce requests and fetch requests is achieved by passing a list of timers to KafkaTimer.;;;","10/Dec/12 19:07;nehanarkhede;Patch v2 looks good to me. Few minor questions -
1. Producer
You probably don't validate the client id anymore in the secondary constructor. Shouldn't we do that ?

2. ZookeeperConsumerConnector
consumerTopicStats is unused

3. Do the singleton validate() APIs need to be synchronized ? 
;;;","10/Dec/12 21:16;swapnilghike;Thanks for reviewing.

Patch v3:
1. Oh, that's because clientId is validated at the end of ProducerConfig constructor.

2. Removed it. 

3. Currently the validate() APIs only check for illegal chars and they don't yet check whether the incoming clientId has already been taken. (I am planning to do it in a separate patch in the same jira, after this patch has been checked in). ;;;","10/Dec/12 22:17;swapnilghike;Fixed a typo in FetchRequestAndResponseStats mbean creation.;;;","11/Dec/12 01:18;junrao;Thanks for patch v4. Looks good overall. A few minor comments:

40. GroupId,ClientId: The validation code is identical. Could we combine them into one utility? We can throw a generic InvalidConfigurationException with the right text.

41. The patch does apply because of changes in system_test/testcase_to_run.json. Do you actually intend to change this file?;;;","11/Dec/12 19:48;swapnilghike;Patch v5:

40. I agree with you. Created a new trait Config in common package, it has a method that can validate a clientId or a groupId. ProducerConfig and ConsumerConfig extend this trait and they have their additional methods to validate config values that are specific to themselves.
- Moved config value validations in ZookeeperConsumerConnector and Producer to ConsumerConfig and ProducerConfig objects respectively, since we can throw an InvalidConfigException when the Config is getting instantiated.
- Moved Topic and TopicTest to common package.
- Added a ConfigTest to common package.
- Removed InvalidClientException and InvalidGroupException. Instead I am now re-using the existing InvalidConfigException to print the appropriate message.

41. It automatically got changed when I ran sanity test, John probably has a patch that will add another file run_test, he says we can use it once it is checked in.;;;","12/Dec/12 01:54;nehanarkhede;+1 on v5. I think we can check this in and wait for v6 addressing rest of the issues ?;;;","12/Dec/12 05:47;junrao;Thanks for patch v5. A few more comments:

50. Config: 
50.1 Could we rename validateClientIdOrGroupId to sth more general like validateAlphaNumericString?
50.2 We should add a separator btw prop and value in the message string of InvalidConfigException.

51. ConsumerConfig: In validateAutoOffsetReset(), let's add the autoOffsetReset value in the message string of InvalidConfigException.

52. ProducerConfig: In validateProducerType(), let's add the producerType value in the message string of InvalidConfigException.;;;","12/Dec/12 08:31;swapnilghike;Made the changes.;;;","12/Dec/12 19:04;swapnilghike;Including in patch v7 Joel's suggestion at KAFKA-604 to measure time as following :

aggregatekafkatimer.time {
  specifickafkatimer.time {
    <code block>
  }
}

This style is probably ok, since we don't have a use case where we use more than two timers to time the same block. This also removes the need to modify KafkaTimer.;;;","17/Dec/12 22:07;nehanarkhede;Thanks for v7. Just checked it in.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Protocol tweaks for 0.8,KAFKA-642,12618254,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jkreps,jkreps,jkreps,30/Nov/12 00:49,15/May/13 23:56,14/Jul/23 05:39,06/Dec/12 04:24,0.8.0,,,,,,,,,,core,,,0,,,,"There are a couple of things in the protocol that are not idea. It would be good to tweak these for 0.8 so we start clean.

Here is a set of problems and proposals:

Problems:
1. Correlation id is not used across all the requests. I don't think it can work as intended because of this.
2. On reflection I am not sure that we need a correlation id field. I think that since we need to guarantee that processing is sequential on any particular socket we can correlate with a simple queue. (e.g. as the client sends messages it adds them to a queue and as it receives responses it just correlates to whatever is at the head of the queue).
3. The metadata response seems to have a number of problems. Among them is that it weirdly repeats all the broker information many times. The response includes the ISR, leader (maybe), and the replicas. Each of these repeat all the broker information. This is super weird. I think what we should be doing here is including all broker information for all brokers and then just having the appropriate ids for the isr, leader, and replicas.
4. For topic discovery I think we need to support the case where no topics are specified in the metadata request and for this return information about all topics. I don't think we do this now.
5. I don't understand what the creator id is.
6. The offset request and response is not fully thought through and should be generalized.

Proposals:
1, 2. Correlation id. This is not strictly speaking needed, but it is maybe useful for debugging to be able to trace a particular request from client to server. So we will extend this across all the requests.
3. For metadata response I will try to fix this up by normalizing out the broker list and having the isr, replicas, and leader field just have the node id.
4. This should be uncontroversial and easy to add.
5. Let's remove creator id, it isn't used.
6. Let's generalize offset request. My proposal is below:

Rename TopicMetadata API to ClusterMetadata, as this will contain all the data that is known cluster-wide. Then let's generalize the offset request to be PartitionMetadata--namely stuff about a particular partition on a particular server.

The format of PartitionMetdata would be the following:

PartitionMetadataRequest => [TopicName [PartitionId MinSegmentTime MaxSegmentInfos]]
  TopicName => string
  PartitionId => uint32
  MinSegmentTime => uint64
  MaxSegmentInfos => int32

PartitionMetadataResponse => [TopicName [PartitionMetadata]]
  TopicName => string
  PartitionMetadata => PartitionId LogSize NumberOfSegments LogEndOffset HighwaterMark [SegmentData]
  SegmentData => StartOffset LastModifiedTime
  LogSize => uint64
  NumberOfSegments => int32
  LogEndOffset => int64
  HighwaterMark => int64

This would be general enough that we could continue to add to it for any new pieces of data we need.",,jjkoshy,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-856,,,,,,,,,,"07/Dec/12 03:54;jkreps;KAFKA-642-remove-response-versions.patch;https://issues.apache.org/jira/secure/attachment/12559804/KAFKA-642-remove-response-versions.patch","30/Nov/12 23:43;jkreps;KAFKA-642-v1.patch;https://issues.apache.org/jira/secure/attachment/12555588/KAFKA-642-v1.patch","03/Dec/12 19:52;jkreps;KAFKA-642-v2.patch;https://issues.apache.org/jira/secure/attachment/12555815/KAFKA-642-v2.patch","03/Dec/12 20:17;jkreps;KAFKA-642-v3.patch;https://issues.apache.org/jira/secure/attachment/12555821/KAFKA-642-v3.patch","04/Dec/12 20:15;jkreps;KAFKA-642-v4.patch;https://issues.apache.org/jira/secure/attachment/12555979/KAFKA-642-v4.patch","05/Dec/12 18:15;jkreps;KAFKA-642-v6.patch;https://issues.apache.org/jira/secure/attachment/12556127/KAFKA-642-v6.patch",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,292883,,,Fri Dec 07 03:56:53 UTC 2012,,,,,,,,,,"0|i0sg6f:",164098,,,,,,,,,,,,,,,,,,,,"30/Nov/12 23:43;jkreps;This patch implements the changes described above with the following exceptions:
1. I punted on fixing OffsetRequest. This change kind of depends on the log refactoring and is somewhat larger than I expected. It would be nice to fix it but I was going to do that as a separate patch and maybe not for 0.8.
2. I also changed instances where were using shorts for array lengths. There were a few of these and it complicates the protocol definition since you can't have a general definition of an array.
3. I changed ClientUtils to not require Broker instances, since that is crazy.

OffsetRequest, TopicMetadataRequest
 - Add correlation id. Not all are being set, but the point is just to get it in the protocol

TopicMetadata
 - Change the serialization format so that we store only broker ids, not full brokers
 - ""no leader"" is encoded as leader_id=-1
 - The object itself doesn't change
 - Change sizes to all be 4 bytes to be consistent with all other arrays

TopicMetadataResponse
 - Add broker list to response. This is guaranteed to have all ""relevant"" brokers--i.e. all leaders and replicas for topics included in the request
 - Add correlation id 

ClientUtils
 - fetchTopicMetadata should take a list of addresses not a list of brokers

Broker
 - remove creatorid

Other files
 - carry through the above changes (i.e. pass in the new argument) ;;;","30/Nov/12 23:57;jkreps;Appears to pass basic system tests.;;;","01/Dec/12 01:03;nehanarkhede;I guess we want to include this on the 0.8 branch as well right ? The patch is probably created off of trunk, so it doesn't apply on the 0.8 branch. Do you mind uploading one that will apply on 0.8 ?;;;","01/Dec/12 02:42;nehanarkhede;Thanks for the patch, reviewed the one that applies on trunk. 
1. Broker
I guess we can delete the debug statement in sizeInBytes(). It's wierd that we have it.

2. KafkaApis
2.1 All responses objects besides OffsetRequest, LeaderIsrResponse and StopReplicaResponse have a correlation id. I'm guessing correlation id can be a request level thing that is included on every Kafka request-response object. Another benefit of doing that is the ability to log the correlation id as part of the trace statement in RequestChannel - 
      trace(""Completed request: %s totalTime:%d queueTime:%d localTime:%d remoteTime:%d sendTime:%d""
        .format(requestObj, totalTime, queueTime, apiLocalTime, apiRemoteTime, responseSendTime))
This will greatly simplify troubleshooting. Thoughts ?
2.2 The OffsetRequest takes in correlation id but we don't return it as part of the OffsetResponse.

3. OffsetRequest
3.1 The correlationId is not passed into the constructor after being read from the byte buffer.
3.2 Probably better to define a DefaultCorrelationId somewhere. It will useful elsewhere in the code.

4. correlation id is a per request level id, does it make sense for it to be of type long instead ?

5. SyncProducerConfig
I think we should get rid of producer.request.correlation_id

6. ProducerPool
6.1 Remove unused import import java.net.InetSocketAddress

7. SimpleConsumer
7.1 It probably makes sense to allow earlierOrLatestOffset to take in the correlation id that defaults to 0. This is because ZookeeperConsumerConnector should use a correlation id that increments every time it sends a fetch/offset reques
t to the Kafka brokers.

8. ZookeeperConsumerConnector & DefaultEventHandler
It will be nice if these classes set an ever increasing correlation id as part of every request they send to Kafka

9. TopicMetadata
9.1 Remove unused import KafkaException
9.2 The documentation of the topic metadata request format is broken.
9.3 Rename TopicMetadata API to ClusterMetadata ?

10. PartitionMetadata
10.1 In the constructor, it makes sense for the list of replicas to just be the broker ids, not the full Broker object. This will simplify PartitionMetadata.readFrom() and TopicMetadata.readFrom() as well.

11. TopicMetadataRequest
11.1 Can we let the following constructor take in the correlation id instead of hardcoding to 0 ?
def this(topics: Seq[String])
The reason is this is used by ClientUtils.fetchTopicMetadata, which in truen, is used elsewhere in ZookeeperConsumerConnector and Producer, where we'd like to track the requests by correlation id.
11.2 The order of the constructor arguments in the scala & java api TopicMetadataRequest is very different. Elsewhere, in the code, versionId is first, followed by correlationId.
;;;","03/Dec/12 19:52;jkreps;I should clarify that my goal here is to make the most minimal change that fixes the client protocol for at least the user-facing apis. I also wussed out on trying to generalize offsetrequest/response. Basically I think trying those things at these point would just take and we are trying to stabalize and release.

So I made some of the changes you recommend, but some I think are bigger, and my hope was to hold off on those.

For example my goal is not to implement correlation id, just add it to the protocol. To  properly handle correlation id we need to make it so that we have a single counter across all requests on a single connection which is hard to do right now.  I have some thoughts on generalizing some of our serialization and request handling stuff which I started to discuss in KAFKA-643. All I want to do now is fix as much of the protocol as I can while breaking as little as possible in the process.

1. Agreed, fixed.

2. Ack, I missed the correlation id in OffsetResponse. I had intended to leave it out of the non-public apis since this was meant to be a minimal change, but it is easy to add so i will do so. This should simplify future upgrades.

3.1 Yeah, but see above comment.
3.2 I mean properly speaking having a default correlation id doesn't really make sense does it? Anything other than a per-connection counter is basically a bug...

4. No, it is a signed int so it should be fine for it to roll over every 4 billion requests per connection, that will take a while.

5. Good point.

6. Done

7. See above comment on correlationId

8. Did it for DefaultEventHandler as that is easy, cowardly not attempting for consumer.

9.1 Done.
9.2  Deleted, should not duplicate protocol docs.
9.3 I chickened out on this. We will have to do it as a follow-up post 0.8 item.

10.1 Agreed, but this is several weeks of work I think. This is a pretty big refactoring. Some thoughts on KAFKA-643.

11. Yeah, I mean basically that constructor shouldn't exist at all since it isn't setting client id either. ;;;","03/Dec/12 19:53;jkreps;Updated patch addresses issues as per points above. Still not rebased...await love from apache infra folks to revive our git.;;;","03/Dec/12 20:17;jkreps;Missed two files on that last patch...;;;","04/Dec/12 20:15;jkreps;Updated patch. This patch also makes the message format consistent with the other serialization formats for both key and value. Now both are a 4 byte size followed by an N byte payload.

I have intentionally not made everything null safe since I think that is a larger change. We can do that in 0.8.1. This is just the binary format change.

I also haven't rebased yet, will do that when this passes review since git is still down and I don't want to do two svn rebases.;;;","05/Dec/12 18:15;jkreps;Patch v6.
- Rebased to 0.8 head.
- Removed changes to ClientUtils since they conflict and aren't really related to protocol changes.
;;;","05/Dec/12 22:57;nehanarkhede;+1. Minor observations -

12. In OffsetRequest, I thought you didn't want to default correlationId to 0, no ?
13. In TopicMetadataRequest.sizeInBytes, there is a bug in the way we compute the length for clientId. I believe it should use shortStringLength(client) instead of clientId.length;;;","06/Dec/12 01:51;jjkoshy;+1

BTW, is there a separate jira open on fetching metadata for all topics if none are given to the metadata request?;;;","06/Dec/12 03:53;jkreps;Neha:
12. No, that should be fine, right? The alternative is actually trying to make it work properly everywhere but that seems pointless since it involves grandfathering up through lots of oddball apis like the two random variants on earliestOrLatestOffset()...I think we should just clean all that up later rather than half-ass it now.
13. Oooh, very nice catch. Would have been super awesome to be debugging that the first time someone plugged in a non-ascii client id.

Joel:
Ach, thanks for reminding me. Will file a ticket.

Checking in with a fix for 13.;;;","06/Dec/12 06:05;junrao;Thanks for the patch. A couple of minor comments:

60. TopicMetadata: The following statement works as magic. It relies on an implicit conversion from int to Broker based on the map (brokers). 
    val replicas = replicaIds.map(brokers)
Is this the recommended scala way? Is it more easily understood if we write it in the following way?
    val replicas = replicaIds.map(id => brokers(id))

61. TopicMetadataResponse: In the following statement, do we know how the precedence of ++ over . work? Should we add brackets to make it clearer?
    val brokers = parts.flatMap(_.replicas) ++ parts.map(_.leader).collect{case Some(l) => l}
;;;","06/Dec/12 06:46;junrao;Another thing.

62. FetchResponsePartitionData.initialOffset is redundant (since it's specified in FetchRequest) and is not really used. I am wondering if we should get rid of it from the wire protocol.;;;","06/Dec/12 16:10;jkreps;60. This isn't an implict conversion. In java the method to get something from a map would be myMap.get(key). In scala the same is myMap(key) because is uses the apply() method. So ids.map(brokers) means the same as ids.map(brokers.get) would in java (if java had a map() method). I think this is pretty natural and not magical, but I share your distrust of magic and am opening to changing it if you object.
61. Yes, I think that all operators have lower precedence then method calls. This works for the same reason a.toInt + b.toInt works. But I agree parens make it explicit, I will add them.
62. This is a very good catch, I had meant to propose that and I just forgot. I will do it.;;;","06/Dec/12 17:38;junrao;60. What you said makes sense. But from IDE, that call maps to the following api
   def map[B, That](f: (A) ⇒ B)(implicit bf: CanBuildFrom[Repr, B, That]): That
instead of 
   def map[B](f: (A) ⇒ B): Traversable[B]
So, it seems that some soft of implicit conversion is happening. We can keep the code as it as, as long as we understand how this conversion works.;;;","06/Dec/12 18:07;jkreps;Ack, you're right.;;;","07/Dec/12 03:54;jkreps;This patch removes the response version. There was a discussion on this on the mailing list. This is fairly straight-forward patch so I am going to go ahead and check it in, but throwing it up here for review.;;;","07/Dec/12 03:56;jkreps;It also baselines all versions, including the message magic byte back to 0 for consistency.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System Test Failures : kafka.common.InvalidClientIdException in broker log4j messages,KAFKA-640,12618184,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,jfung,jfung,29/Nov/12 17:22,30/Nov/12 17:50,14/Jul/23 05:39,30/Nov/12 17:50,,,,,,,,,,,,,,0,replication-testing,,,"* To reproduce the issue, download and build the latest Kafka 0.8 branch and execute this command: ""<kafka_home>/system_test $ python -B system_test_runner.py""

* The following exception is found in the broker log4j messages in most System Test cases: 

[2012-11-29 09:06:21,322] WARN No previously checkpointed highwatermark value found for topic test_1 partition 1. Returning 0 as the highwatermark (kafka.server.HighwaterMarkCheckpoint)
[2012-11-29 09:06:21,326] INFO [Kafka Log on Broker 1], Truncated log segment /tmp/kafka_server_1_logs/test_1-1/00000000000000000000.log to target offset 0 (kafka.log.Log)
[2012-11-29 09:06:21,333] ERROR Replica Manager on Broker 1: Error processing leaderAndISR request LeaderAndIsrRequest(1,,1000,Map((test_1,1) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""2,3,1"",""leader"":""2"",""leaderEpoch"":""0"" },1),3), (test_1,0) -> PartitionStateInfo(LeaderIsrAndControllerEpoch({ ""ISR"":""1,2,3"",""leader"":""1"",""leaderEpoch"":""0"" },1),3)),Set(id:2,creatorId:127.0.0.1-1354208764997,host:127.0.0.1,port:9092, id:1,creatorId:127.0.0.1-1354208760105,host:127.0.0.1,port:9091),1) (kafka.server.ReplicaManager)
kafka.common.InvalidClientIdException: ClientId replica-fetcher-host_127.0.0.1-port_9092 is illegal, contains a character other than ASCII alphanumerics, _ and -
        at kafka.utils.ClientId$.validate(ClientIdAndTopic.scala:36)
        at kafka.consumer.SimpleConsumer.<init>(SimpleConsumer.scala:81)
        at kafka.server.AbstractFetcherThread.<init>(AbstractFetcherThread.scala:44)
        at kafka.server.ReplicaFetcherThread.<init>(ReplicaFetcherThread.scala:26)
        at kafka.server.ReplicaFetcherManager.createFetcherThread(ReplicaFetcherManager.scala:26)
        at kafka.server.AbstractFetcherManager.addFetcher(AbstractFetcherManager.scala:44)
        at kafka.cluster.Partition.makeFollower(Partition.scala:190)
        at kafka.server.ReplicaManager.kafka$server$ReplicaManager$$makeFollower(ReplicaManager.scala:236)
        at kafka.server.ReplicaManager$$anonfun$becomeLeaderOrFollower$3.apply(ReplicaManager.scala:201)
        at kafka.server.ReplicaManager$$anonfun$becomeLeaderOrFollower$3.apply(ReplicaManager.scala:191)
        at scala.collection.immutable.Map$Map2.foreach(Map.scala:127)
        at kafka.server.ReplicaManager.becomeLeaderOrFollower(ReplicaManager.scala:191)
        at kafka.server.KafkaApis.handleLeaderAndIsrRequest(KafkaApis.scala:129)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:60)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:41)
        at java.lang.Thread.run(Thread.java:662)
",,jfung,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/12 22:06;swapnilghike;kafka-640.patch;https://issues.apache.org/jira/secure/attachment/12555413/kafka-640.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,292806,,,Fri Nov 30 17:50:06 UTC 2012,,,,,,,,,,"0|i0sefj:",163814,,,,,,,,,,,,,,,,,,,,"29/Nov/12 22:06;swapnilghike;This happened because ClientId.validate(clientId) in SimpleConsumer did not validate ""."" in the clientId passed from ReplicaFetcherThread. 

This patch fixes another bug - AbstractFetcherThread would create SimpleConsumer and pass ""%s-host_%s-port_%s"" as the clientId to the SimpleConsumer. SimpleConsumer would append the host-port string again to the clientId while instantiating FetchRequestAndResponseStats. 

Changes: 
- The fix is to not include the host-port string while initializing AbstractFetcherThread.clientId in ReplicaFetcherThread, the host and port are already passed through the sourceBroker argument. This new clientId is passed to SimpleConsumer in AbstractFetcherThread and it should validate successfully.
- Pass clientId + host-port string while instantiating *Stats in AbstractFetcherThread and SimpleConsumer. Since the host-port string gives information about the client, I think it should be ok to append it to clientId and not create a separate case class like ClientIdAndTopic. 
- Pass clientId + host-port string while instantiating FetchRequestBuilder in AbstractFetcherThread.
- Pass clientId + host-port string to the constructors of ProducerRequestStats, FetchRequestAndResponseStats, FetcherStats and FetcherLagStats to maintain uniformity with passing clientId + host-port to FetchRequestBuilder in AbstractFetcherThread.doWork().
- Removed a line in ClientIdTest.scala, it was redundant.

The validation criteria for clientId string that comes from the client is unchanged. Ideally I would like to validate the clientId that *includes* the host-port string, but that would require an introduction of '.' in the legal characters set which would be inconsistent with legal chars for Topic. Instead, we can maintain the same legal chars set and take care that the host-port string doesn't change format within the code.;;;","30/Nov/12 00:36;jfung;Thanks Swapnil for the patch. It is working with the latest 0.8 branch.;;;","30/Nov/12 17:39;nehanarkhede;+1, LGTM;;;","30/Nov/12 17:50;nehanarkhede;Thanks for the patch, just committed it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make log segment delete asynchronous,KAFKA-636,12617922,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,28/Nov/12 03:51,11/Dec/12 19:47,14/Jul/23 05:39,11/Dec/12 19:47,,,,,,,,,,,,,,0,,,,"We have a few corner-case bugs around delete of segment files:
1. It is possible for delete and truncate to kind of cross streams and end up with a case where you have no segments.
2. Reads on the log have no locking (which is good) but as a result deleting a segment that is being read will result in some kind of I/O exception.
3. We can't easily fix the synchronization problems without deleting files inside the log's write lock. This can be a problem as deleting a 2GB segment can take a couple of seconds even on an unloaded system.

The proposed fix for these problems is to make file removal asynchronous using the following scheme as the new delete scheme:
1. Immediately remove the file from segment map and rename the~ file from X to X.deleted (e.g. 0000000.log to 000000.log.deleted. We think renaming a file will not impact reads since the file is already open and hence the name is irrelevant. This will always be O(1) and can be done inside the write lock.
2. Schedule a future operation to delete the file. The time to wait would be configurable but we would just default it to 60 seconds and probably no one would ever change it.
3. On startup we would delete any files with the .deleted suffix as they would have been pending deletes that didn't take place.

I plan to do this soon working against the refactored log (KAFKA-521). We can opt to back port the patch for 0.8 if we are feeling daring.",,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/12 23:32;jkreps;KAFKA-636-v1.patch;https://issues.apache.org/jira/secure/attachment/12559981/KAFKA-636-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,292502,,,Tue Dec 11 19:47:41 UTC 2012,,,,,,,,,,"0|i0s39z:",162007,,,,,,,,,,,,,,,,,,,,"07/Dec/12 23:32;jkreps;This patch implements asynchronous delete in the log.

To do this Log.scala now requires a scheduler to be used for scheduling the deletions.

The deletion works as described above.

The locking for segment deletion can now be more aggressive since the file renames are assumed to be fast they can be inside the lock.

As part of testing this I also found a problem with MockScheduler, namely that it does not reentrant. That is, if scheduled tasks themselves create scheduled tasks it misbehaves. To fix this I rewrote MockScheduler to use a priority queue. The code is simpler and more correct since it now performs all executions in the correct order too.;;;","11/Dec/12 17:25;junrao;Thanks for the patch. +1. Just one minor comment:

1. Log.deleteSegment(): In asyncDeleteFiles(), is it better to log the deleting of a log segment in info instead of debug?
;;;","11/Dec/12 19:03;nehanarkhede;+1. Minor comments -

- Typos in the description of the deleteSegment API in Log.scala. 
- I agree with Jun on the info log level for the delete messages.;;;","11/Dec/12 19:47;jkreps;Checked in with the suggested changes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsoleProducer compresses messages and ignores the --compress flag,KAFKA-634,12617847,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,anentropic,anentropic,27/Nov/12 18:11,09/Jan/13 12:19,14/Jul/23 05:39,09/Jan/13 12:19,0.7,,,0.8.0,,,,,,,core,,,0,console,producer,,"I am using the kafka-producer-shell.sh script without the --compress option

however my messages seem to be gzipped

the docs say compression is off by default:
http://incubator.apache.org/kafka/configuration.html

The only producer.properties file I can find is at:
/home/ubuntu/kafka-0.7.2-incubating-src/config/producer.properties

In there is:
compression.codec=0

My process looks like:

root      1748  1746  0 Nov19 ?        00:02:37 java -Xmx512M -server -Dlog4j.configuration=file:/usr/local/bin/kafka/../config/log4j.properties -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -cp :/usr/local/bin/kafka/../project/boot/scala-2.8.0/lib/scala-compiler.jar:/usr/local/bin/kafka/../project/boot/scala-2.8.0/lib/scala-library.jar:/usr/local/bin/kafka/../core/target/scala_2.8.0/kafka-0.7.2.jar:/usr/local/bin/kafka/../core/lib/*.jar:/usr/local/bin/kafka/../perf/target/scala_2.8.0/kafka-perf-0.7.2.jar:/usr/local/bin/kafka/../core/lib_managed/scala_2.8.0/compile/jopt-simple-3.2.jar:/usr/local/bin/kafka/../core/lib_managed/scala_2.8.0/compile/log4j-1.2.15.jar:/usr/local/bin/kafka/../core/lib_managed/scala_2.8.0/compile/snappy-java-1.0.4.1.jar:/usr/local/bin/kafka/../core/lib_managed/scala_2.8.0/compile/zkclient-0.1.jar:/usr/local/bin/kafka/../core/lib_managed/scala_2.8.0/compile/zookeeper-3.3.4.jar kafka.producer.ConsoleProducer --topic logtail --zookeeper x.x.x.x:2181

But the messages come out gobbledegook unless I use a client that understands compressed messages, and in that client it identifies the bit as set to 1, gzip compression.


Jun Rao junrao@gmail.com via incubator.apache.org 
Nov 26 (1 day ago)
to kafka-users 

This seems to be a bug in ConsoleProducer. It also compresses messages and
ignores the --compress flag. Could you file a jira?

Thanks,
Jun",,anentropic,brugidou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-506,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,292413,,,Wed Jan 09 12:18:34 UTC 2013,,,,,,,,,,"0|i0ry6n:",161182,,,,,,,,,,,,,,,,,,,,"09/Jan/13 12:18;brugidou;KAFKA-506 fixed this (commit f64fd3dcbaace1dba7bbd72398bb3e7d28b41d61 in the 0.8 branch)

This will be fixed in 0.8 I guess;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdminTest.testShutdownBroker fails,KAFKA-633,12617740,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jjkoshy,junrao,junrao,27/Nov/12 05:43,07/Dec/12 01:10,14/Jul/23 05:39,07/Dec/12 01:09,0.8.0,,,,,,,,,,core,,,0,,,,"0m[ [31merror [0m]  [0mTest Failed: testShutdownBroker(kafka.admin.AdminTest) [0m
junit.framework.AssertionFailedError: expected:<2> but was:<3>
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.failNotEquals(Assert.java:277)
	at junit.framework.Assert.assertEquals(Assert.java:64)
	at junit.framework.Assert.assertEquals(Assert.java:195)
	at junit.framework.Assert.assertEquals(Assert.java:201)
	at kafka.admin.AdminTest.testShutdownBroker(AdminTest.scala:381)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:228)
	at junit.framework.TestSuite.run(TestSuite.java:223)
	at junit.framework.TestSuite.runTest(TestSuite.java:228)
	at junit.framework.TestSuite.run(TestSuite.java:223)
	at org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)
	at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
	at sbt.TestRunner.run(TestFramework.scala:53)
	at sbt.TestRunner.runTest$1(TestFramework.scala:67)
	at sbt.TestRunner.run(TestFramework.scala:76)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.NamedTestTask.run(TestFramework.scala:92)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
	at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
	at sbt.impl.RunTask.runTask(RunTask.scala:85)
	at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Control$.trapUnit(Control.scala:19)
	at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)",,jjkoshy,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/12 23:06;jjkoshy;KAFKA-633-v1.patch;https://issues.apache.org/jira/secure/attachment/12559761/KAFKA-633-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,292255,,,Fri Dec 07 01:10:20 UTC 2012,,,,,,,,,,"0|i0rsuv:",160319,,,,,,,,,,,,,,,,,,,,"28/Nov/12 00:27;jkreps;Another annoying thing is that after AdminTest runs you often see bazillions of these errors on stdout:
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:599)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)

Not sure what the cause is, something in the test harness is not shutting down cleanly but I couldn't figure it out.;;;","29/Nov/12 22:04;nehanarkhede;That happens due to the test failure. If the test fails, it doesn't close down the zookeeper connections leading to these errors. Once we fix the root cause, these will disappear;;;","29/Nov/12 22:29;jjkoshy;I'll take a look into this.;;;","29/Nov/12 23:06;jkreps;But that's a problem, no? Those test harnesses are supposed to clean up after themselves robustly--even if the test fails.;;;","29/Nov/12 23:27;jjkoshy;Yes it's definitely annoying (apart from the test failure itself). I remember seeing something similar with 0.7 a while ago and might have a fix for it lying around.
;;;","06/Dec/12 23:06;jjkoshy;This is a timing issue that affects low-volume topics (and in this case an empty topic). The issue is that the leader that is being shut down receives a leaderAndIsrRequest informing it is no longer the leader and thus starts up a follower which starts issuing fetch requests to the new leader. We then shrink the ISR and send a StopReplicaRequest to the shutting down broker. However, the new leader upon receiving the fetch request expands the ISR again.

The shutdown itself is working correctly in that the leader has been successfully moved. This patch fixes the assertion by checking the controller's cached ISR instead of ZooKeeper. The patch also fixes the annoying zookeeper-related messages if the test fails - the problem was that the brokers were not getting shut down and were trying to talk to the torn down zookeeper.

I think it would be better to fix the corner case in a separate non-blocker jira. One possible approach would be to use the callback feature in the ControllerBrokerRequestBatch and wait until the StopReplicaRequest has been processed by the shutting down broker before shrinking the ISR; and there are probably other ways as well.
;;;","07/Dec/12 00:00;junrao;Thanks for the patch. +1.;;;","07/Dec/12 01:10;jjkoshy;Checked into 0.8. Will file separate jira to protect the ISR from re-expansion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System Test Failure Case 5005 (Mirror Maker bouncing) - Data Loss in ConsoleConsumer,KAFKA-628,12616852,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jfung,jfung,19/Nov/12 22:44,14/Dec/12 18:18,14/Jul/23 05:39,14/Dec/12 18:18,,,,,,,,,,,,,,0,,,,,,jfung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/12 22:55;jfung;kafka-628-reproduce-issue.patch;https://issues.apache.org/jira/secure/attachment/12554250/kafka-628-reproduce-issue.patch","19/Nov/12 23:49;jfung;log4j_and_data_logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12554259/log4j_and_data_logs.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,258724,,,Fri Dec 14 18:18:06 UTC 2012,,,,,,,,,,"0|i0l2o7:",121083,,,,,,,,,,,,,,,,,,,,"19/Nov/12 22:55;jfung;* There are data loss in testcase_5005 as shown below.

validation_status  : 
     Unique messages from consumer on [test_1] at console_consumer.log  :  1550
     Unique messages from consumer on [test_2] at console_consumer.log  :  1650
     Unique messages from producer on [test_1]  :  1700
     Unique messages from producer on [test_2]  :  1700
     Validate for data matched on topic [test_1]  :  FAILED
     Validate for data matched on topic [test_2]  :  FAILED
     Validate for merged log segment checksum in cluster [source]  :  PASSED
     Validate for merged log segment checksum in cluster [target]  :  PASSED

* The issue is reproducible in latest 0.8 branch:
  1. Download the latest 0.8 branch.
  2. Apply the attached patch from <kafka_home>: patch -p0 -i kafka-628-reproduce-issue.patch
  3. Build Kafka from <kafka_home>: ./sbt update package
  4. Run the testcase from <kafka_home>/system_test : python -u -B system_test_runner.py 2>&1 | tee system_test_output_`date +%s`.log;;;","29/Nov/12 19:37;jfung;This test case is consistently failing in System Test. The followings are the errors found in the log4j messages from Broker & MirrorMaker:

* Logs Archive location:  /mnt/u001/hudson_kafka_replication_system_test_archives/test_1354190469/testcase_5005/logs

===================
* Errors in Source Broker : 
===================
$ less broker-5/kafka_server_5.log (source)
[2012-11-29 10:18:14,307] ERROR [KafkaApi-2] error when processing request (test_2,0,0,1048576) (kafka.server.KafkaApis)
kafka.common.UnknownTopicOrPartitionException: Topic test_2 partition 0 doesn't exist on 2
        at kafka.server.ReplicaManager.getLeaderReplicaIfLocal(ReplicaManager.scala:163)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSet(KafkaApis.scala:359)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:325)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:321)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map1.map(Map.scala:93)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSets(KafkaApis.scala:321)
        at kafka.server.KafkaApis.handleFetchRequest(KafkaApis.scala:289)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:57)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:41)
        at java.lang.Thread.run(Thread.java:619)

===================
* Errors in Target Broker :
===================
$ less broker-7/kafka_server_7.log (target)
[2012-11-29 10:18:29,711] INFO [KafkaApi-4] Auto creation of topic test_1 with 5 partitions and replication factor 1 is successful! (kafka.server.KafkaApis)
[2012-11-29 10:18:29,713] ERROR [KafkaApi-4] Error while retrieving topic metadata (kafka.server.KafkaApis)
kafka.admin.AdministrationException: topic test_1 already exists
        at kafka.admin.AdminUtils$.createTopicPartitionAssignmentPathInZK(AdminUtils.scala:85)
        at kafka.admin.CreateTopicCommand$.createTopic(CreateTopicCommand.scala:95)
        at kafka.server.KafkaApis$$anonfun$handleTopicMetadataRequest$2.apply(KafkaApis.scala:437)
        at kafka.server.KafkaApis$$anonfun$handleTopicMetadataRequest$2.apply(KafkaApis.scala:430)
        at scala.collection.immutable.Set$Set1.foreach(Set.scala:81)
        at kafka.server.KafkaApis.handleTopicMetadataRequest(KafkaApis.scala:429)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:59)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:41)
        at java.lang.Thread.run(Thread.java:619)
. . .
[2012-11-29 10:19:48,714] ERROR [Partition state machine on Controller 5]: State change for partition [test_2, 0] from OfflinePa
rtition to OnlinePartition failed (kafka.controller.PartitionStateMachine)
kafka.common.PartitionOfflineException: All replicas for partition [test_2, 0] are dead. Marking this partition offline
        at kafka.controller.PartitionStateMachine.electLeaderForPartition(PartitionStateMachine.scala:300)
        at kafka.controller.PartitionStateMachine.kafka$controller$PartitionStateMachine$$handleStateChange(PartitionStateMachin
e.scala:141)
        at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$1.apply(PartitionStateMachine.scala:86)
        at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$1.apply(PartitionStateMachine.scala:84)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
        at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:84)
        at kafka.controller.PartitionStateMachine.startup(PartitionStateMachine.scala:59)
        at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:221)
        at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:85)
        at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:53)
        at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:106)
        at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)
        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
Caused by: kafka.common.PartitionOfflineException: No replica for partition ([test_2, 0]) is alive. Live brokers are: [Set(5, 6)], Assigned replicas are: [List(4)]
        at kafka.controller.OfflinePartitionLeaderSelector.selectLeader(PartitionLeaderSelector.scala:62)
        at kafka.controller.PartitionStateMachine.electLeaderForPartition(PartitionStateMachine.scala:283)
        ... 18 more

===================
* Errors in Mirror Maker :
===================
$ less mirror_maker-14/mirror_maker_14.log
[2012-11-29 10:18:29,738] ERROR Failed to collate messages by topic, partition due to (kafka.producer.async.DefaultEventHandler)
kafka.common.KafkaException: Failed to fetch topic metadata for topic: test_1
        at kafka.producer.BrokerPartitionInfo.getBrokerPartitionInfo(BrokerPartitionInfo.scala:53)
        at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$getPartitionListForTopic(DefaultEventHandler.scala:170)
        at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:131)
        at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:130)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.async.DefaultEventHandler.partitionAndCollate(DefaultEventHandler.scala:130)
        at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:76)
        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:57)
        at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:103)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:86)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:66)
        at scala.collection.immutable.Stream.foreach(Stream.scala:254)
        at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:65)
        at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:43)
[2012-11-29 10:18:29,740] ERROR Error in handling batch of 200 events (kafka.producer.async.ProducerSendThread)
kafka.common.KafkaException: Failed to fetch topic metadata for topic: test_1
        at kafka.producer.BrokerPartitionInfo.getBrokerPartitionInfo(BrokerPartitionInfo.scala:53)
        at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$getPartitionListForTopic(DefaultEventHandler.scala:170)
        at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:131)
        at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:130)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.async.DefaultEventHandler.partitionAndCollate(DefaultEventHandler.scala:130)
        at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:76)
        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:57)
        at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:103)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:86)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:66)
        at scala.collection.immutable.Stream.foreach(Stream.scala:254)
        at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:65)
        at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:43)
;;;","14/Dec/12 18:18;jfung;This issue is not showing any more in 0.8 branch. So mark this close now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make UnknownTopicOrPartitionException a WARN in broker,KAFKA-627,12616769,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,criccomini,criccomini,19/Nov/12 17:34,12/Jul/19 22:15,14/Jul/23 05:39,24/Aug/17 15:30,0.8.0,,,,,,,,,,core,,,0,,,,"Currently, when sending messages to a topic that doesn't yet exist, the broker spews out these ""errors"" as it tries to auto-create new topics. I spoke with Neha, and she said that this should be a warning, not an error.

Could you please change it to something less scary, if, in fact, it's not scary.

2012/11/14 22:38:53.238 INFO [LogManager] [kafka-request-handler-6] [kafka] []  [Log Manager on Broker 464] Created log for 'firehoseReads'-5
2012/11/14 22:38:53.241 WARN [HighwaterMarkCheckpoint] [kafka-request-handler-6] [kafka] []  No previously checkpointed highwatermark value found for topic firehoseReads partition 5. Returning 0 as the highwatermark
2012/11/14 22:38:53.242 INFO [Log] [kafka-request-handler-6] [kafka] []  [Kafka Log on Broker 464], Truncated log segment /export/content/kafka/i001_caches/firehoseReads-5/00000000000000000000.log to target offset 0
2012/11/14 22:38:53.242 INFO [ReplicaFetcherManager] [kafka-request-handler-6] [kafka] []  [ReplicaFetcherManager on broker 464] adding fetcher on topic firehoseReads, partion 5, initOffset 0 to broker 466 with fetcherId 0
2012/11/14 22:38:53.248 ERROR [ReplicaFetcherThread] [ReplicaFetcherThread-466-0-on-broker-464] [kafka] []  [ReplicaFetcherThread-466-0-on-broker-464], error for firehoseReads 5 to broker 466
kafka.common.UnknownTopicOrPartitionException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at java.lang.Class.newInstance0(Class.java:355)
        at java.lang.Class.newInstance(Class.java:308)
        at kafka.common.ErrorMapping$.exceptionFor(ErrorMapping.scala:68)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5$$anonfun$apply$3.apply(AbstractFetcherThread.scala:124)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5$$anonfun$apply$3.apply(AbstractFetcherThread.scala:124)
        at kafka.utils.Logging$class.error(Logging.scala:102)
        at kafka.utils.ShutdownableThread.error(ShutdownableThread.scala:23)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:123)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:99)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:125)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:344)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:99)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)","Kafka 0.8, RHEL6, Java 1.6",criccomini,githubbot,jkreps,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,258638,,,Thu Aug 24 15:30:03 UTC 2017,,,,,,,,,,"0|i0l1qv:",120933,,,,,,,,,,,,,,,,,,,,"19/Nov/12 17:41;jkreps;Since this is expected behavior it seems like this shouldn't have a stack trace or even be a WARNing, just an INFO message.;;;","14/Feb/16 20:48;githubbot;GitHub user kichristensen opened a pull request:

    https://github.com/apache/kafka/pull/913

    KAFKA-627: Make UnknownTopicOrPartitionException a WARN in broker

    

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/kichristensen/kafka KAFKA-627

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/kafka/pull/913.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #913
    
----
commit 9dc94fc95ceed94bd14cffa321023c902b11e19d
Author: Kim Christensen <kich@mvno.dk>
Date:   2016-02-14T20:45:13Z

    UnknownTopicOrPartition should be logged as warn

----
;;;","24/Aug/17 15:30;omkreddy;Not observed on latest versions;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Produce requests dropped due to socket timeouts on get metadata requests,KAFKA-626,12616759,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,19/Nov/12 16:50,13/Dec/12 00:23,14/Jul/23 05:39,13/Dec/12 00:22,0.8.0,,,,,,,,,,,,,0,bugs,,,"The setup of the test includes 2 servers with the following properties overridden -

num.partitions=10
default.replication.factor=2

Ran producer performance to send 1000 messages to 8 topics in async mode. Each of the topics are auto created on the broker and default to 10 partitions. No broker was bounced during this test. 

The producer log has the following errors -

[2012-11-18 17:44:04,622] WARN fetching topic metadata for topics [Set(test1114, test1117, test1115, test1116, test1118)] from broker [id:0,creatorId:localhost-1353289442325,host:localhost,port:9091] failed (kafka.client.ClientUtils$)
java.net.SocketTimeoutException
        at sun.nio.ch.SocketAdaptor$SocketInputStream.read(SocketAdaptor.java:201)
        at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:86)
        at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:221)
        at kafka.utils.Utils$.read(Utils.scala:393)
        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)
        at kafka.network.Receive$class.readCompletely(Transmission.scala:56)
        at kafka.network.BoundedByteBufferReceive.readCompletely(BoundedByteBufferReceive.scala:29)
        at kafka.network.BlockingChannel.receive(BlockingChannel.scala:100)
        at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:76)
        at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:74)
        at kafka.producer.SyncProducer.send(SyncProducer.scala:101)
        at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:25)
        at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:75)
        at kafka.producer.async.DefaultEventHandler$$anonfun$handle$1.apply$mcV$sp(DefaultEventHandler.scala:62)
        at kafka.utils.Utils$.swallow(Utils.scala:185)
        at kafka.utils.Logging$class.swallowError(Logging.scala:105)
        at kafka.utils.Utils$.swallowError(Utils.scala:44)
        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:62)
        at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:103)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:86)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:66)
        at scala.collection.immutable.Stream.foreach(Stream.scala:254)
        at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:65)
        at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:43)
[2012-11-18 17:44:04,624] INFO Fetching metadata for topic Set(test1114, test1117, test1115, test1116, test1118) (kafka.client.ClientUtils$)
[2012-11-18 17:44:04,624] INFO Connected to localhost:9092 for producing (kafka.producer.SyncProducer)
[2012-11-18 17:44:04,805] INFO Disconnecting from localhost:9092 (kafka.producer.SyncProducer)
[2012-11-18 17:44:04,806] INFO Disconnecting from 127.0.0.1:9091 (kafka.producer.SyncProducer)
[2012-11-18 17:44:04,806] INFO Disconnecting from 127.0.0.1:9092 (kafka.producer.SyncProducer)
[2012-11-18 17:44:04,815] INFO Connected to 127.0.0.1:9092 for producing (kafka.producer.SyncProducer)
[2012-11-18 17:44:04,910] INFO Connected to 127.0.0.1:9091 for producing (kafka.producer.SyncProducer)
[2012-11-18 17:44:05,048] INFO Fetching metadata for topic Set(test1115, test1118) (kafka.client.ClientUtils$)
[2012-11-18 17:44:05,049] INFO Connected to localhost:9091 for producing (kafka.producer.SyncProducer)
[2012-11-18 17:44:05,111] INFO Disconnecting from localhost:9091 (kafka.producer.SyncProducer)
[2012-11-18 17:44:05,112] INFO Disconnecting from 127.0.0.1:9091 (kafka.producer.SyncProducer)
[2012-11-18 17:44:05,112] INFO Disconnecting from 127.0.0.1:9092 (kafka.producer.SyncProducer)
[2012-11-18 17:44:05,114] ERROR Failed to send the following requests: ArrayBuffer(KeyedMessage(test1115,1,Message(magic = 2, attributes = 0, crc = 1950606895, key = null, payload = java.nio.HeapByteBuffer[pos=0 lim=100 cap=100])), KeyedMessage(test1115,11,Message(magic = 2, attributes = 0, crc = 1950606895, key = null, payload = java.nio.HeapByteBuffer[pos=0 lim=100 cap=100])), KeyedMessage(test1115,21,Message(magic = 2, attributes = 0, crc = 1950606895, key = null, payload = java.nio.HeapByteBuffer[pos=0 lim=100 cap=100])), KeyedMessage(test1118,5,Message(magic = 2, attributes = 0, crc = 1950606895, key = null, payload = java.nio.HeapByteBuffer[pos=0 lim=100 cap=100])), KeyedMessage(test1118,15,Message(magic = 2, attributes = 0, crc = 1950606895, key = null, payload = java.nio.HeapByteBuffer[pos=0 lim=100 cap=100]))) (kafka.producer.async.DefaultEventHandler)
[2012-11-18 17:44:05,122] ERROR Error in handling batch of 200 events (kafka.producer.async.ProducerSendThread)
kafka.common.FailedToSendMessageException: Failed to send messages after 3 tries.
        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:70)
        at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:103)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:86)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:66)
        at scala.collection.immutable.Stream.foreach(Stream.scala:254)
        at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:65)
        at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:43)


These errors don't happen when I run producer performance on fewer topics. 

Also, the consumer receives 8995 messages, the expected messages is 8000 (1000/topic). Since the producer failed to send a request, most of these messages could be duplicates from previous requests.",,criccomini,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,258624,,,Thu Dec 13 00:16:48 UTC 2012,,,,,,,,,,"0|i0l1lr:",120910,,,,,,,,,,,,,,,,,,,,"19/Nov/12 17:42;nehanarkhede;nnarkhed-ld:apache-kafka-0.8 nnarkhed$ grep MessageID producer-with-ids.log | awk '{print $4}' | sed ""s/Topic://g"" | sed ""s/:.*MessageID//g"" | sort -k1 | uniq -w7 -c
   1000 topic11:0000000000:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   1000 topic12:0000000000:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   1000 topic13:0000000000:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   1000 topic14:0000000000:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   1000 topic15:0000000000:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   1000 topic16:0000000000:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   1000 topic17:0000000000:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   1000 topic18:0000000000:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
nnarkhed-ld:apache-kafka-0.8 nnarkhed$ 
nnarkhed-ld:apache-kafka-0.8 nnarkhed$ grep MessageID consumer.log | sed ""s/Topic://g"" | sed ""s/:.*MessageID//g"" | sort -k1 | uniq -w7 -c
   2388 topic11:0000000000:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   2387 topic12:0000000000:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   2388 topic13:0000000000:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   2387 topic14:0000000000:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   2007 topic15:0000000000:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   2386 topic16:0000000000:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   2008 topic17:0000000000:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   1255 topic18:0000000001:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
;;;","19/Nov/12 17:57;jkreps;Suspect the root cause is KAFKA-608. I am changing that to a blocker because our default is insanely low. In general I don't think timing out like this is very useful. I think it is better to default higher and let people tune it down as they run into some need. Otherwise it is a bit of a time bomb since under normal usage you will definitely eventually hit a slow request and die.;;;","13/Dec/12 00:04;jkreps;So the only bug was KAFKA-608, right? I mean it remains true that fetching metadata is slow because of lack of zk multiget, but there is nothing else to do on this right?;;;","13/Dec/12 00:16;nehanarkhede;Correct. The right fix is to forward the request to the controller, which will be much faster than trying to read zookeeper, but I think that is a bigger change. So, for now, we have nothing more to do on this JIRA;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create mbeans per client ,KAFKA-622,12616485,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,16/Nov/12 21:51,04/Dec/12 20:06,14/Jul/23 05:39,29/Nov/12 01:31,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,improvement,,"Currently we create one mbean of each type for a given mbean server, regardless of the number of clients. We should create MBeans per client for both producer and consumer. To do that we need to introduce clientId in mbean names.",,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/12 02:51;swapnilghike;kafka-622-v1.patch;https://issues.apache.org/jira/secure/attachment/12553880/kafka-622-v1.patch","21/Nov/12 02:09;swapnilghike;kafka-622-v2.patch;https://issues.apache.org/jira/secure/attachment/12554445/kafka-622-v2.patch","28/Nov/12 01:25;swapnilghike;kafka-622-v3.patch;https://issues.apache.org/jira/secure/attachment/12555097/kafka-622-v3.patch","29/Nov/12 00:50;swapnilghike;kafka-622-v4.patch;https://issues.apache.org/jira/secure/attachment/12555278/kafka-622-v4.patch","29/Nov/12 01:08;swapnilghike;kafka-622-v5.patch;https://issues.apache.org/jira/secure/attachment/12555285/kafka-622-v5.patch","29/Nov/12 01:21;swapnilghike;kafka-622-v6.patch;https://issues.apache.org/jira/secure/attachment/12555287/kafka-622-v6.patch",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,258351,,,Tue Dec 04 20:06:17 UTC 2012,,,,,,,,,,"0|i0krkn:",119284,,,,,,,,,,,,,,,,,,,,"17/Nov/12 02:51;swapnilghike;The major change in this patch is to convert a bunch of *Stat objects to classes, and instantiating one instance of each such class at the high level Producer, ZookeeperConsumerConnector and AbstractFetcherThread level. This will ensure that clients within the same service will have their own mbeans. 

The mbean name will contain clientId, and the clientId is validated at the high level Producer, ZookeeperConsumerConnector and AbstractFetcherThread level.

I have set max length of clientId to 200, because it will be appended with some string while creating mbean, and the mbean name might be limited by filename length. 

Other changes are a bunch of import optimizations, and a test case for clientId validation.;;;","18/Nov/12 18:07;nehanarkhede;1. AbstractFetcherThread
1.1 Maybe the clientid validation could happen once on process startup (while instantiating the config) instead of on every fetcher thread. In other words, would it make sense to let the broker startup
 if the config doesn't comply with the allowed values ?
1.2 How about renaming FetcherLagStat to FetcherLagStats, same for FetcherStat and ConsumerTopicStat and any APIs that are get*Stat.
1.3 The change to FetcherLagStat.getFetcherLagMetrics serves the purpose but seems slightly hacky. Instead of changing the name of the topic by prepending the clientid to it, what do you think about ch
anging FetcherLagMetrics take in clientId, (topic, partition) ? The valueFactory can be changed to pass in clientid to the constructor of FetcherLagMetrics.
1.4 In other parts of code, we've been trying to move away from using a tuple of topic, partition as the key to a map and using TopicPartition instead. The reason being its unsafe since scala doesn't do a good job of doing the right thing on map/fold operations on such a map. Wondering if you were up for making that change in this patch ? If not, then lets file another JIRA to fix that.

2. ConsumerTopicStat.scala
2.1 Change the name to ConsumerTopicStats
2.2 Why does clientId default to empty string ?
2.3 Same change as 1.3 to ConsumerTopicMetrics
2.4 I wonder if it makes sense to allow creating ConsumerIterator without passing in ConsumerTopicStat ? ConsumerTopicStat is instantiated in ZookeeperConsumerConnector on startup. The same object need
s to be passed around everywhere else in the consumer. However, the code is allowing ConsumerTopicStat to be re-instantiated in ConsumerIterator with an empty clientId. Isn't that going to create new m
etrics objects tracking different values for the same class of metrics ? I guess the right thing is to not provide it a default

3. DefaultEventHandler
3.1 Same as 2.4 here for ProducerStats and ProducerTopicStat. 3.2 Let's standardize the naming here. One is ProducerStats and the other is ProducerTopicStat. How about renaming ProducerTopicStat to ProducerTopicStats ?

4. PartitionTopicInfo
Same as 2.4 here 

5. Producer 
5.1 What is the point of passing in empty string to ProducerTopicStat ? If that is testing only, how about passing in a clientid like ""test-producer"". I also think it is better to not change the constr
uctutor in this way and instead have the test pass in the producerStats object.
5.2 The clientId is validated after it is passed in to ProducerSendThread. Let's move the clientId validation all the way up in the constructor.
5.3 Rename getProducer*Stat to getProducer*Stats 
5.4 Same as 1.3 here as well
5.5 Having a separate AsyncProducerStats object seems a little clunky and is probably a remnant from the time when we explicitly exposed AsyncProducer and SyncProducer as public APIs. I'm guessing movi
ng this to ProducerStats would be better. That way you have only 2 metrics to worry about on the producer, one is ProducerStats and the other is per-topic stats exposed through ProducerTopicStats
 
6. ProducerPool
6.1 Same as 5.1 here, let's not change the constructor of ProducerPool in this manner, its ugly 

7. ProducerSendThread 
7.1 Let's not allow empty clientId as the default value. It should *always* be passed in correctly.
 
8. SimpleConsumer
8.1 I wonder what is the value of letting one instantiate a FetchRequestAndResponseStat with an empty clientId ?
8.2 Instead of adding a stat object to the constructor of SimpleConsumer, let's just add the clientId. It seems unnatural to have the user of SimpleConsumer to know how to instantiate the stat object. 
On the other hand, the purpose of clientId might be much easier to explain and understand.
8.3 Let's not change the static SimpleConsumer APIs to have the user pass in a stat object. These are fire and forget queries. At best, let's have the user pass in clientId or default it to be the host
 name or something meaningful but simple. ZookeeperConsumerConnector will pass in the right value for clientId

9. SyncProducer
9.1 Same as 8.2. Let's not change the constructor in this manner
9.2 Rename ProducerRequestStat to ProducerRequestStats

10. Is TopicTest deleted on purpose ?
;;;","21/Nov/12 02:09;swapnilghike;1.2, 2.1, 3.2, 5.3, 9.2:  Renamed *Stat to *Stats and get*Stat to get*Stats. Looks better.

1.3, 2.3, 5.4: Yes, I agree. Changed.

2.2, 2.4, 3.1, 4, 5.1, 6.1, 7.1, 8.1: I was not sure if I should modify a whole bunch of unit tests or provide defaults in constructors. The v2 patch touches a whole bunch of unit tests and some tools. 

1.1 Yes, clientId is validated in ZookeeperConsumerConnector which is eventually passed to ConsumerFetcherThread (which extends AbstractFetcherThread). We control the clientId for ReplicaManagerThread (which also extends AbstractFetcherThread) in code, so I guess we can safely remove clientId validation from AbstractFetcherThread. 
1.4 I can file another Jira to take a look at this.

5.2 Yes, my bad. To correct this, I had to hack around the Producer constructor. The same *Stats objects need to be passed to DefaultEventHandler and Producer, and they can be constructed only after validating the clientId. Scala does not allow you to put any statement (ClientId.validate(clientId) in our case) before calling a (n-1) level constructor, so you need to put the clientId validation in a block and return a quadruplet from the block which is evaluated as the constructor argument. This constructor acts as an intermediary to separate out the elements of the quadruplet. Suggestions are welcome.
5.5 Moved DroppedMessagePerSec to ProducerStats and deleted AsyncProducerStats.

10 That was because I was trying to modify and rename a file at the same time, and svn has issues cleanly applying such patch. On a second thought, separated TopicTest and ClientIdTest. This patch contains deletion of ConsumerTopicStat.scala and addition of ConsumerTopicStats.scala for the same reason.

Change of plan: 
8.2, 8.3, 9.1 Now, each SyncProducer and SimpleConsumer will register its own ProducerRequestStats/ConsumerRequestStats. That removes the need to pass a *Stats object via their constructors. Aggregation of ProducerRequestStats/ConsumerRequestStats at the high level Producer or ZookeeperConsumerConnector level will probably require an aggregator object to be passed to the constructors of SyncProducer/SimpleConsumer. So, I have left that part out.

Additional things:
a. Not quite sure what should go as the clientId in KafkaETLContext. 
b. Overloaded the static createSyncProducer API. The two separate APIs remove the need to use Option and also make it easy to pass ""FetchMetadata"" as the clientId from ClientUtils.
c. Renamed kafka.controller.ControllerStat to ControllerStats and kafka.server.BrokerTopicStat to BrokerTopicStats.;;;","22/Nov/12 18:29;nehanarkhede;Thanks for incorporating suggestions in the first review!

1. AbstractFetcherThread.scala

FetcherLagMetrics now takes in a nested tuples. Scala's support for tuples makes it easier to express short lived data structures that need to be paired conveniently, but they tend to reduce code readability. In addition to this, we've been bitten by incomplete Scala semantics when comparing two tuples or using them for some of the functional programming collections APIs. Over time, as we understood this more, we now encourage use of tuples only for expressing short lived objects within maybe the same API. We haven't changed all the past usages to conform to this yet, but we do hope we can avoid this in the future usage. Having said that, I think here it is a good idea to change this to accept two arguments - String and TopicAndPartition.

2. ClientUtils

Both producers and consumers use this API to fetch metadata and it seems like we have hardcoded the clientId. The clientId to create sync producer should be the one that the consumer/producer is instantiated with. 

3. Producer.scala

3.1 I think you are validating the clientId twice in the constructor that takes in just the ProducerConfig. 
3.2 Maybe this was done for convenience, what is the reason that the constructor argument to ProducerTopicMetrics is a tuple vs 2 separate strings ? 
3.3 We definitely want to avoid this - 
  private val stats = new Pool[(String, String), ProducerTopicMetrics](Some(valueFactory))
Change this to use a case class TopicAndClientId here as well as ConsumerTopicStats.scala.

4. ProducerPool.scala

createSyncProducer is overloaded to create 2 very similar but confusing methods. One takes Broker as the second argument, the other takes it as the first argument. I see why you need two, but it will be great to add some docs to explain where each of those is used.

5. ProducerSendThread.scala

There needs to be a delimiter (""-"") between the clientId and ProducerQueueSize 

6. ProducerConfig

Not sure which patch introduced this property, but it is inconsistent with the property naming conventions. It happens to be the only one that uses an underscore in the property name. Besides, the one in the consumer is called just clientid. Let's please change this to clientid. 

val clientId = props.getString(""producer.request.client_id"",SyncProducerConfig.DefaultClientId)

7. ZookeeperConsumerConnector

It will be good to have some summary stats here that aggregates from individual simple consumers. Often users wouldn't want to do this math themselves. 

8. What happened to ConsumerTopicStats ?

;;;","28/Nov/12 01:25;swapnilghike;1. I see your point, that's good to know. Now using a case class with three constructor arguments. Using two separate arguments String and TopicAndPartition would mean that the key of the stats Pool will be a touple. 
2. Yup.
3.1 Yes, the primary constructor is only used for unit testing. So we can skip clientId validation.
3.2, 3.3 using a case class object to avoid using a tuple as the constructor argument.
4. Added a comment above both methods, also changed the order of arguments so that broker is the second argument in both methods.
5. Aye.
6. Changed it to clientId = props.getString(""clientid"",SyncProducerConfig.DefaultClientId)
7. Aggregation of ConsumerRequestStats at the ZookeeperConsumerConnector level will need us to pass an aggregator object to the constructor of SimpleConsumer. Then we will either need to provide this aggregator object a default value in the constructor because we can't expect the users of SimpleConsumer to know how to instantiate that object. Thoughts?
8. ConsumerTopicStat.scala was modified and renamed to ConsumerTopicStats.scala. Due to svn issues, deleted the old file, added the modified file as a new file. 
;;;","28/Nov/12 21:47;nehanarkhede;v3 looks much better and almost ready for check in. Few minor cleanup comments -

9. Minor naming convention suggestion - Probably better to rename ClientIdTopicPartition to ClientIdAndPartition or ClientIdAndTopicPartition
10. ProducerPool Minor documentation nitpick - The API docs for createSyncProducer doesn't match the commenting style for documenting APIs. We use C style comments while this patch has added C++ style comments.
11. SimpleConsumerPerformance - Remove unused import FetchRequestAndResponseStats
12. BrokerPartitionInfo - Not introduced by your patch, but will be good to fix. Typo in the name of the param. Change it to topics instead of topic
13.1 Producer - Again not introduced by your patch, just happened to notice it right now. Typo in the name of the param in send API docs. It says producerData now but should be messages
13.2 Minor nitpick - Rest of the metrics classes have name as their constructor argument, except ProducerTopicMetrics which says tuple and its not a tuple.
14. SyncProducerConfig - Elsewhere, we don't use camel case for config names. Probably best to change it to clientid like in ConsumerConfig
15. What happened to Topic.scala ?

7. For now, it seems that the messages rate, both per topic and global should suffice. We can, however, think about how/if the aggregation metrics for fetch response rate and size makes sense at the ZookeeperConsumerConnector level. But I d
on't think we should hold up this patch for it. Please can you file a JIRA to track that ?

;;;","29/Nov/12 00:50;swapnilghike;9 - I am not convinced that ""ClientIdAndTopicPartition"" is the right way to name it, because the class has 3 separate constructor arguments and it does not use a TopicAndPartition constructor argument. Also I did not want to include too many ""And""s in the name. Let me know what you think and I can change it accordingly.
Fixed 10, 11, 12, 13.1. 
13.2 - name was used to refer to the topic name. Replaced it with a more straightforward clientIdTopic of the type ClientIdAndTopic.
14. A more uniform scheme would be to use clientId = ""client.id"" in both SyncProducerConfigShared and ConsumerConfig.  This way we don't have any confusion in using $word+""id"", or $word+""_id"", or $word+""Id"". Similarly, we could change groupid, consumerid in ConsumerConfig and producer.request.correlation_id in SyncProducerConfigShared. I haven't changed them in this patch because changing groupid might have impact on any tools/tests that are creating consumers. Should we change them in another/this jira?
15. Grouped object ClientId, object Topic and case class ClientIdAndTopic together in one file ClientIdAndTopic.scala.
7. Ok.
;;;","29/Nov/12 01:08;swapnilghike;Quick fix to interchange the order of constructor arguments of ClientIdAndTopic.;;;","29/Nov/12 01:21;swapnilghike;Oops, another quick fix. Didn't realize that syncproducer was not getting clientId while fetching metadata. ;;;","29/Nov/12 01:22;nehanarkhede;9. Sounds good.
14. That's a good suggestion but bigger change and backwards incompatible as well. Let's think it through more. For now, let's leave clientid as it is. 

+1;;;","29/Nov/12 01:31;nehanarkhede;Committed patch v6 after making the following minor changes-
1. Changed the config name to clientid until we change the naming convention across the board
2. Removed unused variables from SyncProducer

Please file the JIRAs to track the metrics aggregation and the config naming convention. ;;;","03/Dec/12 23:43;junrao;Sorry for the late review. Have a few minor questions/comments.

60. What happens if have 2 instances of Consumers with the same clientid in the same jvm? Does one of them fail because it fails to register metrics? Ditto for Producers.

61. ConsumerTopicStats: What if a topic is named AllTopics? We use to handle this by adding a - in topic specific stats.

62. ZookeeperConsumerConnector: Do we need to validate groupid?

63. ClientId: Does the clientid length need to be different from topic length?

64. AbstractFetcherThread: When building a fetch request, do we need to pass in brokerInfo as part of the client id? BrokerInfo contains the source broker info and the fetch requests are always made to the source broker.

;;;","04/Dec/12 20:06;swapnilghike;Created KAFKA-646 to address your comments.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System Test 9051 : ConsoleConsumer doesn't receives any data for 20 topics but works for 10,KAFKA-621,12616447,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jfung,jfung,16/Nov/12 17:39,03/Jul/13 21:38,14/Jul/23 05:39,03/Jul/13 21:38,,,,,,,,,,,,,,0,,,,"* This issue may be related to KAFKA-618

* To reproduce the issue:
    1. Download the latest 0.8 branch and apply the attached patch
    2. In <kafka_home>, execute ""./sbt update package""
    3. In <kafka_home>/system_test, execute ""python -B system_test_runner.py"" and it will execute testcase_9051

* The validation output would be as follows:
validation_status  : 
     Unique messages from consumer on [t001]  :  0
     Unique messages from consumer on [t002]  :  0
     . . .
     Unique messages from consumer on [t019]  :  0
     Unique messages from consumer on [t020]  :  0
     Unique messages from producer on [t001]  :  1000
     Unique messages from producer on [t002]  :  1000
     . . .
     Unique messages from producer on [t018]  :  1000
     Unique messages from producer on [t019]  :  1000
     Unique messages from producer on [t020]  :  1000
     Validate for data matched on topic [t001]  :  FAILED
     Validate for data matched on topic [t002]  :  FAILED
     . . .
     Validate for data matched on topic [t019]  :  FAILED
     Validate for data matched on topic [t020]  :  FAILED
     Validate for merged log segment checksum in cluster [source]  :  PASSED

* However, it will work fine if there are only 10 topics
     In system_test/replication_testsuite/testcase_9051/testcase_9051_properties.json, update the following line to 10 topics:

      ""topic"": ""t001,t002,t003,t004,t005,t006,t007,t008,t009,t010,t011,t012,t013,t014,t015,t016,t017,t018,t019,t020"",",,jfung,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-618,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/12 17:42;jfung;kafka-621-reproduce-issue.patch;https://issues.apache.org/jira/secure/attachment/12553804/kafka-621-reproduce-issue.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,258289,,,Wed Jul 03 21:38:43 UTC 2013,,,,,,,,,,"0|i0kr5r:",119217,,,,,,,,,,,,,,,,,,,,"07/Jan/13 04:39;nehanarkhede;Can we check if this is still a problem ?;;;","03/Jul/13 21:38;jfung;Not an issue any more.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regression : System Test 900x (Migration Tool) - java.lang.ClassCastException: kafka.message.Message cannot be cast to [B,KAFKA-619,12616430,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jfung,jfung,16/Nov/12 16:35,03/Jul/13 21:40,14/Jul/23 05:39,16/Nov/12 21:57,,,,,,,,,,,,,,0,,,,"This error is happening in : testcase_900x (migration tool test group). The issue is that no data is received by ConsoleConsumer. All Migration Tool log4j messages are showing the following error. 

. . .
[2012-11-16 08:28:55,361] INFO FetchRunnable-1 start fetching topic: test_1 part: 3 offset: 0 from 127.0.0.1:9093 (kafka.consumer.FetcherRunnable)
[2012-11-16 08:28:55,361] INFO FetchRunnable-0 start fetching topic: test_1 part: 3 offset: 0 from 127.0.0.1:9092 (kafka.consumer.FetcherRunnable)
[2012-11-16 08:28:55,361] INFO FetchRunnable-2 start fetching topic: test_1 part: 0 offset: 0 from 127.0.0.1:9091 (kafka.consumer.FetcherRunnable)
Migration thread failure due to java.lang.ClassCastException: kafka.message.Message cannot be cast to [B
java.lang.ClassCastException: kafka.message.Message cannot be cast to [B
        at kafka.serializer.DefaultEncoder.toBytes(Encoder.scala:36)
        at kafka.producer.async.DefaultEventHandler$$anonfun$serialize$1.apply(DefaultEventHandler.scala:111)
        at kafka.producer.async.DefaultEventHandler$$anonfun$serialize$1.apply(DefaultEventHandler.scala:106)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.mutable.WrappedArray.map(WrappedArray.scala:32)
        at kafka.producer.async.DefaultEventHandler.serialize(DefaultEventHandler.scala:106)
        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:47)
        at kafka.producer.Producer.send(Producer.scala:75)
        at kafka.javaapi.producer.Producer.send(Producer.scala:32)
        at kafka.tools.KafkaMigrationTool$MigrationThread.run(KafkaMigrationTool.java:287)
[2012-11-16 08:30:04,854] INFO test-consumer-group_jfung-ld-1353083318174-2b62271b begin rebalancing consumer test-consumer-group_jfung-ld-1353083318174-2b62271b try #0 (kafka.consumer.ZookeeperConsumerConnector)
[2012-11-16 08:30:04,858] INFO Constructing topic count for test-consumer-group_jfung-ld-1353083318174-2b62271b from *2*.* using \*(\p{Digit}+)\*(.*) as pattern. (kafka.consumer.TopicCount$)
. . .",,jfung,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,258271,,,Fri Nov 16 21:57:05 UTC 2012,,,,,,,,,,"0|i0kr1j:",119198,,,,,,,,,,,,,,,,,,,,"16/Nov/12 21:57;jkreps;I believe I have fixed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock between leader-finder-thread and consumer-fetcher-thread during broker failure,KAFKA-618,12616351,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,jjkoshy,jjkoshy,16/Nov/12 01:20,17/Nov/12 01:39,14/Jul/23 05:39,17/Nov/12 01:39,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"This causes the test failure reported in KAFKA-607. This affects high-level consumers - if they hit the deadlock then they would get wedged (or at least until the consumer timeout).

Here is the threaddump output that shows the issue:

Found one Java-level deadlock:
=============================
""ConsumerFetcherThread-console-consumer-41755_jkoshy-ld-1353026496639-b0e24a70-0-1"":
  waiting for ownable synchronizer 0x00007f2283ad0000, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),
  which is held by ""console-consumer-41755_jkoshy-ld-1353026496639-b0e24a70-leader-finder-thread""
""console-consumer-41755_jkoshy-ld-1353026496639-b0e24a70-leader-finder-thread"":
  waiting to lock monitor 0x00007f2288297190 (object 0x00007f2283ab01d0, a java.lang.Object),
  which is held by ""ConsumerFetcherThread-console-consumer-41755_jkoshy-ld-1353026496639-b0e24a70-0-1""

Java stack information for the threads listed above:
===================================================
""ConsumerFetcherThread-console-consumer-41755_jkoshy-ld-1353026496639-b0e24a70-0-1"":
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f2283ad0000> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
        at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
        at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
        at kafka.consumer.ConsumerFetcherManager.getPartitionTopicInfo(ConsumerFetcherManager.scala:131)
        at kafka.consumer.ConsumerFetcherThread.processPartitionData(ConsumerFetcherThread.scala:43)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:116)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:99)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:99)
        - locked <0x00007f2283ab01d0> (a java.lang.Object)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)
""console-consumer-41755_jkoshy-ld-1353026496639-b0e24a70-leader-finder-thread"":
        at kafka.server.AbstractFetcherThread.addPartition(AbstractFetcherThread.scala:142)
        - waiting to lock <0x00007f2283ab01d0> (a java.lang.Object)
        at kafka.server.AbstractFetcherManager.addFetcher(AbstractFetcherManager.scala:49)
        - locked <0x00007f2283ab0338> (a java.lang.Object)
        at kafka.consumer.ConsumerFetcherManager$$anon$1$$anonfun$doWork$5.apply(ConsumerFetcherManager.scala:81)
        at kafka.consumer.ConsumerFetcherManager$$anon$1$$anonfun$doWork$5.apply(ConsumerFetcherManager.scala:76)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
        at kafka.consumer.ConsumerFetcherManager$$anon$1.doWork(ConsumerFetcherManager.scala:76)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)

Found 1 deadlock.
",,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-621,,,,,,,,,,,,,,,,,,,,,,,KAFKA-607,,,,"17/Nov/12 00:31;jjkoshy;KAFKA-618-v1.patch;https://issues.apache.org/jira/secure/attachment/12553868/KAFKA-618-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,258113,,,Sat Nov 17 01:39:30 UTC 2012,,,,,,,,,,"0|i0kmy7:",118535,,,,,,,,,,,,,,,,,,,,"16/Nov/12 03:11;junrao;This is a very good finding. The following is one way of breaking the deadlock.

In ConsumerFetcherManager, don't expose getPartitionTopicInfo(). Instead, pass partitionMap (which is immutable) to each newly created ConsumerFetcherThread. This way, ConsumerFetcherThread.processPartitionData() and ConsumerFetcherThread.handleOffsetOutOfRange() won't depend on ConsumerFetcherManager any more. If we do that, we can improve ConsumerFetcherManager.stopAllConnections() a bit too. The clearing of noLeaderPartitionSet and partitionMap can be done together before calling closeAllFetchers(). Before, we have to clear partitionMap last because before all fetchers are stopped, the processing of the fetch request still needs to read partitionMap and expects it to be non-null. ;;;","17/Nov/12 00:31;jjkoshy;Ran 20 iterations of testcase 4011 and they all pass.

One potential concern is partitionMap in the ConsumerFetcherThread being null/inconsistent wrt the partitionMap in ConsumerFetcherManager, but I looked at it closely and don't think it is possible.;;;","17/Nov/12 01:31;junrao;Thanks for the patch. +1;;;","17/Nov/12 01:39;jjkoshy;Committed to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement acks=0,KAFKA-616,12616322,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,jkreps,jkreps,15/Nov/12 22:11,19/Jun/14 05:11,14/Jul/23 05:39,03/Jul/13 04:02,0.8.0,,,,,,,,,,,,,0,newbie,,,"For completeness it would be nice to handle the case where acks=0 in the produce request. The meaning of this would be that the broker immediately responds without blocking even on the local write. The advantage of this is that it would often isolate the producer from any latency in the local write (which we have occasionally seen).

Since we don't block on the append the response would contain a placeholder for all the fields--e.g. offset=-1 and no error.

This should be pretty easy to implement, just an if statement in KafkaApis.handleProduceRequest to send the response immediately in this case (and again to avoid sending a second response later).",,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,258083,,,Wed Jul 03 04:02:49 UTC 2013,,,,,,,,,,"0|i0kmrj:",118505,,,,,,,,,,,,,,,,,,,,"03/Jul/13 04:02;jkreps;Neha did it :-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DumpLogSegment offset verification is incorrect for compressed messages,KAFKA-614,12616271,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,junrao,junrao,15/Nov/12 18:00,29/Nov/12 00:10,14/Jul/23 05:39,17/Nov/12 00:29,0.8.0,,,0.8.0,,,,,,,core,,,0,newbie,,,"During verification, DumpLogSegment tries to make sure that offsets are consecutive. However, this won't be true for compressed messages since FileMessageSet only does shallow iteration. The simplest fix is to skip the verification for compressed messages.",,junrao,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/12 22:44;yeyangever;dump_fix.diff;https://issues.apache.org/jira/secure/attachment/12555258/dump_fix.diff","17/Nov/12 00:04;yeyangever;kafka_614_v1.patch;https://issues.apache.org/jira/secure/attachment/12553864/kafka_614_v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,258030,,,Thu Nov 29 00:10:05 UTC 2012,,,,,,,,,,"0|i0km33:",118395,,,,,,,,,,,,,,,,,,,,"17/Nov/12 00:29;junrao;Thanks for the patch. +1 and committed to 0.8.;;;","28/Nov/12 22:44;yeyangever;
Dump Log segment too continue patch;;;","29/Nov/12 00:10;junrao;Thanks for the second patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MigrationTool should disable shallow iteration in the 0.7 consumer,KAFKA-613,12616249,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,junrao,junrao,15/Nov/12 15:09,17/Nov/12 01:34,14/Jul/23 05:39,17/Nov/12 01:34,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"If shallow iteration is enabled, we should override it and log a warning.",,junrao,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/12 23:58;yeyangever;kafka_613_v1.patch;https://issues.apache.org/jira/secure/attachment/12553862/kafka_613_v1.patch","17/Nov/12 00:36;yeyangever;kafka_613_v2.patch;https://issues.apache.org/jira/secure/attachment/12553871/kafka_613_v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,258007,,,Sat Nov 17 01:34:23 UTC 2012,,,,,,,,,,"0|i0klxb:",118369,,,,,,,,,,,,,,,,,,,,"16/Nov/12 23:58;yeyangever;
disable shallow iteration ;;;","17/Nov/12 00:25;junrao;Thanks for patch v1. It would be good to also log a warning that ""shallow iteration is not supported"" if shallowiteration is set to true in the 0.7 consumer property file.;;;","17/Nov/12 00:36;yeyangever;
add logging;;;","17/Nov/12 01:34;junrao;Thanks for patch v2. +1. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
move shutting down of fetcher thread out of critical path,KAFKA-612,12616127,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,14/Nov/12 21:35,19/Nov/12 05:52,14/Jul/23 05:39,19/Nov/12 05:52,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"Shutting down a fetch thread seems to take more than 200ms since we need to interrupt the thread. Currently, we shutdown fetcher threads while processing a leaderAndIsr request. This can delay some of the partitions to become a leader.",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/12 23:44;junrao;kafka-612-v2.patch;https://issues.apache.org/jira/secure/attachment/12554099/kafka-612-v2.patch","14/Nov/12 21:42;junrao;kafka-612.patch;https://issues.apache.org/jira/secure/attachment/12553566/kafka-612.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,257873,,,Mon Nov 19 05:52:48 UTC 2012,,,,,,,,,,"0|i0kkkn:",118150,,,,,,,,,,,,,,,,,,,,"14/Nov/12 21:42;junrao;Attach a patch. It introduces a separate method for shutting down empty fetcher threads and that method will be called at the end of processing a leaderAndIsr request. Because of this change, a fetcher thread could have an empty fetch map. To avoid sending empty fetch requests to the broker, the patch uses a condition to coordinate the fetch. Also fixed a bug when removing items from a scala map (can't do the removal while iterating the map since the behaviour is not deterministic).;;;","18/Nov/12 01:02;nehanarkhede;Overall, the fix looks good, here are a few review comments -

1. AbstractFetcherThread
1.1 I'm trying to understand what can cause a particular topic partition from the response object to have no offset in the fetchMap. Right now, we ignore this case altogether. I'm guessing if the leader changes after the broker sends the fetch response, this could happen, which is ok. Wondering if this is what was intended or if we should log and handle the error case ?
1.2 hasPartition and partCount could use a refactor. You can return a value from a try catch block, the finally block will still execute and unlock the fetch map lock.

2. ReplicaManager
The right way to measure leader election latency is at the controller by measuring the time after it sent a leader/isr request to the time it received the corresponding ACK fro
m the broker. Given that, we should probably move the following lines at the end of handleLeaderAndIsrRequest in KafkaApis.
    info(""Completed leader and isr request %s"".format(leaderAndISRRequest))
    replicaFetcherManager.shutdownEmptyFetcherThread()
 
3. AbstractFetcherThread 
3.1 fetchMap is a confusing name, this map is keeping track of fetch offsets per topic partition. I haven't looked at this code in much detail before, but when I read it while 
reviewing this patch, I thought the names of the maps could be improved. For example, the map in AbstractFetcherManager is called fetcherThreadMap and the map in this class is 
called fetcherMap.
3.2 How about renaming shutdownEmptyFetcherThread to shutdownIdleFetcherThreads ?
;;;","18/Nov/12 23:44;junrao;Thanks for the review. Attach patch v2.

1.1 This is to handle the case that a partition is removed from the fetchMap while a fetch request is issued to the broker (since we don't hold the lock when making fetch requests). When this happens, we just need to ignore this partition.
1.2 Good suggestion. Fixed.

2. That info message is really used to track how long it takes a broker to complete the leaderAndIsr request. From the broker's perspective, once the leader is set in a partition, the partition can serve read/write requests.

3.1 Agreed that the naming is confusing. Changed it to partitionMap.
3.2 Done.;;;","18/Nov/12 23:52;nehanarkhede;Minor nitpick before you checkin v2 - Please could you rename shutdownIdleFetcherThread to shutdownIdlFetcherThreads since typically there are multiple fetcher threads :)

+1 on v2;;;","19/Nov/12 05:52;junrao;Thanks for the review. Changed the method name and committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"System Test Transient Failure 9001 (Migration tool) - ConsoleConsumer terminates after ""can't rebalance after 4 retries""",KAFKA-609,12615928,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jfung,jfung,13/Nov/12 18:02,03/Jul/13 21:41,14/Jul/23 05:39,16/Nov/12 23:17,,,,,,,,,,,,,,0,,,,,,jfung,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,257508,,,Fri Nov 16 23:17:24 UTC 2012,,,,,,,,,,"0|i0k2wv:",115289,,,,,,,,,,,,,,,,,,,,"13/Nov/12 18:04;jfung;* Testcase : 9001

* Hudson Build : # 250

* Functional Test Group : Migration Tool (No bouncing)

* Failure : ConsoleConsumer doesn't receive any messages as it terminated after 4 ""Rebalancing attempt failed"":

* Testcase archive : <host_0996>:/export/home/kafka/kafka_hudson_test_output_logs/test_1352766410/testcase_9001/logs


1. ConsoleConsumer log4j messages showing 4 attempts failed:

 [~/kafka_hudson_test_output_logs/test_1352766410/testcase_9001/logs] grep ""Rebalancing attempt failed"" console_consumer-8/console_consumer.log
[2012-11-13 00:31:25,017] INFO [console-consumer-15395_host0997-1352766684787-46c22f38], Rebalancing attempt failed. Clearing the cache before the next rebalancing operation is triggered (kafka.consumer.ZookeeperConsumerConnector)
[2012-11-13 00:31:27,052] INFO [console-consumer-15395_host0997-1352766684787-46c22f38], Rebalancing attempt failed. Clearing the cache before the next rebalancing operation is triggered (kafka.consumer.ZookeeperConsumerConnector)
[2012-11-13 00:31:29,064] INFO [console-consumer-15395_host0997-1352766684787-46c22f38], Rebalancing attempt failed. Clearing the cache before the next rebalancing operation is triggered (kafka.consumer.ZookeeperConsumerConnector)
[2012-11-13 00:31:31,076] INFO [console-consumer-15395_host0997-1352766684787-46c22f38], Rebalancing attempt failed. Clearing the cache before the next rebalancing operation is triggered (kafka.consumer.ZookeeperConsumerConnector)
. . .
[2012-11-13 00:31:31,076] INFO [console-consumer-15395_host0997-1352766684787-46c22f38], Cleared all relevant queues for this fetcher (kafka.consumer.ZookeeperConsumerConnector)
[2012-11-13 00:31:31,076] INFO [console-consumer-15395_host0997-1352766684787-46c22f38], Cleared the data chunks in all the consumer message iterators (kafka.consumer.ZookeeperConsumerConnector)
[2012-11-13 00:31:31,076] INFO [console-consumer-15395_host0997-1352766684787-46c22f38], Committing all offsets after clearing the fetcher queues (kafka.consumer.ZookeeperConsumerConnector)
[2012-11-13 00:31:33,078] ERROR Error processing message, stopping consumer:  (kafka.consumer.ConsoleConsumer$)
kafka.common.ConsumerRebalanceFailedException: console-consumer-15395_host0997-1352766684787-46c22f38 can't rebalance after 4 retries
        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.syncedRebalance(ZookeeperConsumerConnector.scala:388)
        at kafka.consumer.ZookeeperConsumerConnector.kafka$consumer$ZookeeperConsumerConnector$$reinitializeConsumer(ZookeeperConsumerConnector.scala:690)
        at kafka.consumer.ZookeeperConsumerConnector$WildcardStreamsHandler.<init>(ZookeeperConsumerConnector.scala:720)
        at kafka.consumer.ZookeeperConsumerConnector.createMessageStreamsByFilter(ZookeeperConsumerConnector.scala:133)
        at kafka.consumer.ConsoleConsumer$.main(ConsoleConsumer.scala:182)
        at kafka.consumer.ConsoleConsumer.main(ConsoleConsumer.scala)
[2012-11-13 00:31:33,079] INFO [console-consumer-15395_host0997-1352766684787-46c22f38], ZKConsumerConnector shutting down (kafka.consumer.ZookeeperConsumerConnector)
[2012-11-13 00:31:33,080] INFO Forcing shutdown of Kafka scheduler (kafka.utils.KafkaScheduler)
[2012-11-13 00:31:33,080] INFO [ConsumerFetcherManager-1352766684865] shutting down (kafka.consumer.ConsumerFetcherManager)


2. All 3 brokers started without any errors nor exceptions until stopped by System Test:

. . .
[2012-11-13 00:31:20,807] INFO Closing socket connection to /x.x.x.x. (kafka.network.Processor)
[2012-11-13 00:31:20,807] INFO Closing socket connection to /x.x.x.x. (kafka.network.Processor)
[2012-11-13 00:31:20,807] INFO Closing socket connection to /x.x.x.x. (kafka.network.Processor)
[2012-11-13 00:31:44,529] INFO Shutting down Kafka server (kafka.server.KafkaServer)
[2012-11-13 00:31:44,530] INFO shutdown scheduler kafka-logcleaner- (kafka.utils.KafkaScheduler)
[2012-11-13 00:31:44,537] INFO shutdown scheduler kafka-logflusher- (kafka.utils.KafkaScheduler)
[2012-11-13 00:31:44,538] INFO Closing zookeeper client... (kafka.server.KafkaZooKeeper)


3. Zookeeper log4j messages in Source cluster works fine until terminated by System Test:

[2012-11-13 00:30:39,802] INFO Got user-level KeeperException when processing sessionid:0x13af72bbc290003 type:setData cxid:0xb1 zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a Error Path:/consumers/test-consumer-group/offsets/test_1/2-3 Error:KeeperErrorCode = NoNode for /consumers/test-consumer-group/offsets/test_1/2-3 (org.apache.zookeeper.server.PrepRequestProcessor)
[2012-11-13 00:30:39,803] INFO Got user-level KeeperException when processing sessionid:0x13af72bbc290003 type:create cxid:0xb2 zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a Error Path:/consumers/test-consumer-group/offsets/test_1 Error:KeeperErrorCode = NodeExists for /consumers/test-consumer-group/offsets/test_1 (org.apache.zookeeper.server.PrepRequestProcessor)
[2012-11-13 00:31:44,540] INFO Processed session termination for sessionid: 0x13af72bbc290000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2012-11-13 00:31:44,542] INFO Closed socket connection for client /x.x.x.x:46477 which had sessionid 0x13af72bbc290000 (org.apache.zookeeper.server.NIOServerCnxn)
[2012-11-13 00:31:44,736] INFO Processed session termination for sessionid: 0x13af72bbc290002 (org.apache.zookeeper.server.PrepRequestProcessor)
[2012-11-13 00:31:44,737] INFO Closed socket connection for client /x.x.x.125:39520 which had sessionid 0x13af72bbc290002 (org.apache.zookeeper.server.NIOServerCnxn)
[2012-11-13 00:31:44,855] INFO Processed session termination for sessionid: 0x13af72bbc290001 (org.apache.zookeeper.server.PrepRequestProcessor)
[2012-11-13 00:31:44,856] INFO Closed socket connection for client /x.x.x.124:36697 which had sessionid 0x13af72bbc290001 (org.apache.zookeeper.server.NIOServerCnxn)


4. Zookeeper log4j messages in Target cluster :

[2012-11-13 00:31:24,904] INFO Processed session termination for sessionid: 0x13af72bd8660001 (org.apache.zookeeper.server.PrepRequestProcessor)
[2012-11-13 00:31:24,906] INFO Closed socket connection for client /x.x.x.123:51194 which had sessionid 0x13af72bd8660001 (org.apache.zookeeper.server.NIOServerCnxn)
[2012-11-13 00:31:24,933] INFO Got user-level KeeperException when processing sessionid:0x13af72bd8660000 type:create cxid:0x2 zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a Error Path:/consumers/console-consumer-15395/ids Error:KeeperErrorCode = NoNode for /consumers/console-consumer-15395/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2012-11-13 00:31:24,934] INFO Got user-level KeeperException when processing sessionid:0x13af72bd8660000 type:create cxid:0x3 zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a Error Path:/consumers/console-consumer-15395 Error:KeeperErrorCode = NoNode for /consumers/console-consumer-15395 (org.apache.zookeeper.server.PrepRequestProcessor)
[2012-11-13 00:31:24,936] INFO Got user-level KeeperException when processing sessionid:0x13af72bd8660000 type:create cxid:0x4 zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NoNode for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2012-11-13 00:31:33,085] INFO Processed session termination for sessionid: 0x13af72bd8660000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2012-11-13 00:31:33,088] INFO Closed socket connection for client /x.x.x.123:51193 which had sessionid 0x13af72bd8660000 (org.apache.zookeeper.server.NIOServerCnxn)
[2012-11-13 00:31:33,964] INFO Accepted socket connection from /x.x.x.123:51196 (org.apache.zookeeper.server.NIOServerCnxn)
[2012-11-13 00:31:33,965] INFO Client attempting to establish new session at /x.x.x.123:51196 (org.apache.zookeeper.server.NIOServerCnxn)
[2012-11-13 00:31:33,967] INFO Established session 0x13af72bd8660002 with negotiated timeout 30000 for client /x.x.x.123:51196 (org.apache.zookeeper.server.NIOServerCnxn)
[2012-11-13 00:31:33,977] INFO Processed session termination for sessionid: 0x13af72bd8660002 (org.apache.zookeeper.server.PrepRequestProcessor)
[2012-11-13 00:31:33,979] INFO Closed socket connection for client /x.x.x.123:51196 which had sessionid 0x13af72bd8660002 (org.apache.zookeeper.server.NIOServerCnxn)


5. In 0.7 producer side, the log segment files have data:

[~/kafka_hudson_test_output_logs/test_1352766410/testcase_9001/logs] .../kafka_08_replication_system_test/system_test/migration_tool_testsuite/0.7/bin/kafka-run-class.sh kafka.tools.DumpLogSegments -noprint broker-3/kafka_server_3_logs/test_1-4/00000000000000170959.kafka

Starting offset: 170959
offset: 170959 isvalid: true payloadsize: 76 magic: 1 compresscodec: NoCompressionCodec
offset: 171045 isvalid: true payloadsize: 61 magic: 1 compresscodec: NoCompressionCodec
offset: 171116 isvalid: true payloadsize: 160 magic: 1 compresscodec: NoCompressionCodec
offset: 171286 isvalid: true payloadsize: 253 magic: 1 compresscodec: NoCompressionCodec
offset: 171549 isvalid: true payloadsize: 419 magic: 1 compresscodec: NoCompressionCodec
. . .

;;;","14/Nov/12 22:59;yeyangever;John, do you have full log kept for this test. It seems hard to find out the cause without the log.;;;","16/Nov/12 23:17;yeyangever;We found this is not a real issue.  The reason is simply because that the target (08) brokers didn't actually start up. We see following log in all three target brokers:

[2012-11-13 00:30:09,655] INFO [Log Manager on Broker 6] Starting log flusher every 1000 ms with the following overrides Map() (kafka.log.LogManager)
[2012-11-13 00:30:09,685] ERROR Uncaught exception in thread 'kafka-acceptor': (kafka.utils.Utils$)
java.net.BindException: Address already in use
        at sun.nio.ch.Net.bind(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:119)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)
        at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:52)
        at kafka.network.Acceptor.run(SocketServer.scala:134)
        at java.lang.Thread.run(Thread.java:619)
[2012-11-13 00:31:45,327] INFO [Kafka Server 6], shutting down (kafka.server.KafkaServer)
[2012-11-13 00:31:45,330] INFO Shutdown Kafka scheduler (kafka.utils.KafkaScheduler)
[2012-11-13 00:31:45,331] INFO [Socket Server on Broker 6], shutting down (kafka.network.SocketServer)


That's because the java process last run didn't get closed completely and the socket is still been used. No wonder no data is produced to the target cluster.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getTopicMetadata does not respect producer config settings,KAFKA-608,12615925,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,jkreps,jkreps,13/Nov/12 17:58,05/Dec/12 18:32,14/Jul/23 05:39,03/Dec/12 23:26,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"ProducerPool.scala contains the following code:
object ProducerPool{
  def createSyncProducer(configOpt: Option[ProducerConfig], broker: Broker): SyncProducer = {
    val props = new Properties()
    props.put(""host"", broker.host)
    props.put(""port"", broker.port.toString)
    if(configOpt.isDefined)
      props.putAll(configOpt.get.props.props)
    new SyncProducer(new SyncProducerConfig(props))
  }
}

Note also, that ClientUtils.getTopicMetadata() does the following:
   ProducerPool.createSyncProducer(None, brokers(i))

As a result there is no way to control the socket settings for the get metadata request.

My recommendation is that we require the config to be specified in the 

Note that this creates a new sync producer without using ANY of the settings the user had given for the producer. In particular the socket timeout is defaulted to 500ms. 

This causes unit tests to fail a lot since a newly started test may easily timeout on a 500ms request.",,criccomini,jkreps,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-630,,,,,,,,,,,,,,,,,,,,,,"29/Nov/12 23:08;nehanarkhede;kafka-608-v1.patch;https://issues.apache.org/jira/secure/attachment/12555429/kafka-608-v1.patch","30/Nov/12 18:00;nehanarkhede;kafka-608-v2.patch;https://issues.apache.org/jira/secure/attachment/12555538/kafka-608-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,257505,,,Wed Dec 05 18:32:13 UTC 2012,,,,,,,,,,"0|i0k2w7:",115286,,,,,,,,,,,,,,,,,,,,"29/Nov/12 23:08;nehanarkhede;Attaching a patch that makes the following changes -

1. Fixes the metadata api calls to use the right producer config
2. Fixes the auto create topic issue to ignore the exception that complains that the topic was already created. This is fine since many brokers might be creating the topic at the same time. 
3. Bumped up the ack timeout on the producer to 1500 as default;;;","30/Nov/12 17:49;junrao;Thanks for the patch. Looks good overall. Some comments:

1. TopicExistsException: The scala file is missing from the patch.

2. DefaultEventHandler.send(): Do we want to prepend some text to the payload in the trace logging below?
        if (logger.isTraceEnabled) {
          val successfullySentData = response.status.filter(_._2.error == ErrorMapping.NoError)
          successfullySentData.foreach(m => messagesPerTopic(m._1).foreach(message => trace(""%s"".format(Utils.readString(message.message.payload)))))
        }

3. ProducerPool: Should we removed the commented out createSyncProducer()?

4. ConsoleConsumer: Should numMessages be long instead of int?

5. I was thinking of having a separate config for metadata request timeout. On second thought, this may not be necessary. The apps that really need a large metadata request timeout are migration tool and mirror maker since either of them could be mirroring a large number of topics. However, these apps can live with a large producer request timeout (which controls the timeout for both metadata and send requests). So, what this patch does is probably fine.
;;;","30/Nov/12 18:00;nehanarkhede;1. My bad, included in this patch
2. Absolutely, prepended ""Message: "". Once this is checked in, we should update the system tests to use it. Will file another JIRA to cover that.
3,4. Makes sense, done.
5. Thought about it and am still investigating some bugs related to topic metadata request timeouts and large number of topics. Ultimately, in order to allow clients to scale the # of topics they produce to/consume from, we need to allow a much larger timeout to accomodate the get metadata request. This is true until we move to zookeeper 3.4.x that allows us to use the multi-get APIs, which will reduce the latency of fetching metadata from zookeeper from large number of zk paths. However, I'm waiting for that investigation to complete and have KAFKA-626 to cover that issue.
;;;","30/Nov/12 18:54;swapnilghike;Is including broker.list in properties necessary in ClientUtils.fetchTopicMetadata that is used by non-producer clients?;;;","30/Nov/12 19:00;nehanarkhede;yes, you can't instantiate a ProducerConfig object without a broker.list, it is mandatory. Think about it this way, how will a producer send a request if it doesn't know who to talk to.;;;","01/Dec/12 01:31;junrao;+1. For #2, may be we should prepend a bit more text like ""Successfully sent message:"" to make it more easily identifiable.;;;","05/Dec/12 04:26;jkreps;Guys, previously we had a terrible api to fetch metadata. This is a public api, and it was checked in without being at all thought through. With this patch now we have two bad apis. How is this happening?;;;","05/Dec/12 18:32;junrao;I guess that you are talking about ClientUtils. Currently, for java users, they only need to use the getMetadata api directly if they use SimpleConsumer (which is a public api). For that purpose, we added an api in SimpleConsumer to send TopicMetadataRequest. So, I am not sure if ClientUtils is really intended as a public api. We could make it a public api and get rid of the support of TopicMetadataRequest from SimpleConsumer, if people feel that's better.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System Test Transient Failure (case 4011 Log Retention) - ConsoleConsumer receives less data,KAFKA-607,12615664,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jfung,jfung,11/Nov/12 22:32,03/Jul/13 21:43,14/Jul/23 05:39,03/Jul/13 21:43,,,,,,,,,,,,,,0,,,,,,jfung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-618,,"11/Nov/12 22:40;jfung;testcase_4011_data_and_log4j.tar.gz;https://issues.apache.org/jira/secure/attachment/12553053/testcase_4011_data_and_log4j.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,256965,,,Wed Jul 03 21:43:35 UTC 2013,,,,,,,,,,"0|i0j11j:",109098,,,,,,,,,,,,,,,,,,,,"11/Nov/12 22:44;jfung;• This issue is transient and is showing randomly in this functional test group. The test case is failing because ConsoleConsumer is receiving less data than producer produced.

• Transient Failure Testcase: 4011

• Functional Test Group: Log Retention (Size) / 2 topics / 2 partitions / Replica Factor = 2 / No. of brokers = 3 / One Leader bouncing
     Unique messages from consumer on [test_1]  :  500
     Unique messages from consumer on [test_1] at simple_consumer_test_1-0_r1.log  :  500
     Unique messages from consumer on [test_1] at simple_consumer_test_1-0_r2.log  :  0
     Unique messages from consumer on [test_1] at simple_consumer_test_1-0_r3.log  :  500
     Unique messages from consumer on [test_1] at simple_consumer_test_1-1_r1.log  :  500
     Unique messages from consumer on [test_1] at simple_consumer_test_1-1_r2.log  :  500
     Unique messages from consumer on [test_1] at simple_consumer_test_1-1_r3.log  :  0
     Unique messages from consumer on [test_2]  :  295
     Unique messages from consumer on [test_2] at simple_consumer_test_2-0_r1.log  :  500
     Unique messages from consumer on [test_2] at simple_consumer_test_2-0_r2.log  :  0
     Unique messages from consumer on [test_2] at simple_consumer_test_2-0_r3.log  :  500
     Unique messages from consumer on [test_2] at simple_consumer_test_2-1_r1.log  :  500
     Unique messages from consumer on [test_2] at simple_consumer_test_2-1_r2.log  :  500
     Unique messages from consumer on [test_2] at simple_consumer_test_2-1_r3.log  :  0
     Unique messages from producer on [test_1]  :  1000
     Unique messages from producer on [test_2]  :  1000
     Validate for data matched on topic [test_1]  :  FAILED
     Validate for data matched on topic [test_1] across replicas  :  PASSED
     Validate for data matched on topic [test_2]  :  FAILED
     Validate for data matched on topic [test_2] across replicas  :  PASSED
     Validate leader election successful  :  PASSED


• By checking the broker and ConsoleConsumer log4j messages, it appears that when leader is stopped, ConsoleConsumer is disconnected from Zookeeper.

• Broker log4j messages:

2012-11-10 04:52:52,857 - INFO - found leader in entity [3] with brokerid [3] for partition [0] (kafka_system_test_utils)
2012-11-10 04:52:52,857 - INFO - stopping leader in entity 3 with pid 16146 (kafka_system_test_utils)
2012-11-10 04:52:53,036 - DEBUG - executing command [ssh ela4-app0999.prod 'kill -15 16148'] (system_test_utils)
2012-11-10 04:52:53,055 - DEBUG - executing command [ssh ela4-app0999.prod 'kill -15 16146'] (system_test_utils)

• Log4j messages in the Broker that was bounced:

[2012-11-10 04:52:53,200] INFO Replica Manager on Broker 3: Shutted down completely (kafka.server.ReplicaManager)
[2012-11-10 04:52:53,201] INFO [Socket Server on Broker 3], shutting down (kafka.network.SocketServer)
[2012-11-10 04:52:53,205] INFO [Socket Server on Broker 3], shutted down completely (kafka.network.SocketServer)
[2012-11-10 04:52:53,212] INFO [Kafka Server 3], shut down completed (kafka.server.KafkaServer)

[2012-11-10 04:53:09,497] INFO Started Kafka CSV metrics reporter with polling period 5 seconds (kafka.metrics.KafkaCSVMetricsReporter)
[2012-11-10 04:53:09,506] INFO [Kafka Server 3], starting (kafka.server.KafkaServer)
[2012-11-10 04:53:09,535] INFO [Log Manager on Broker 3] Loading log 'test_2-0' (kafka.log.LogManager)
[2012-11-10 04:53:09,579] INFO [Log Manager on Broker 3] Loading log 'test_1-0' (kafka.log.LogManager)
[2012-11-10 04:53:09,583] INFO [Log Manager on Broker 3] Starting log cleaner every 60000 ms (kafka.log.LogManager)
[2012-11-10 04:53:09,588] INFO [Log Manager on Broker 3] Starting log flusher every 1000 ms with the following overrides Map() (kafka.log.LogManager)
. . .
[2012-11-10 04:53:09,692] INFO zookeeper state changed (SyncConnected) (org.I0Itec.zkclient.ZkClient)
[2012-11-10 04:53:09,694] INFO Registering broker /brokers/ids/3 (kafka.server.KafkaZooKeeper)
[2012-11-10 04:53:09,705] INFO Registering broker /brokers/ids/3 succeeded with id:3,creatorId:172.17.166.125-1352523189694,host:172.17.166.125,port:9093 (kafka.utils.ZkUtils$)
[2012-11-10 04:53:09,705] INFO [Kafka Server 3], Connecting to ZK: ela4-app0996.prod:2188 (kafka.server.KafkaServer)
[2012-11-10 04:53:09,772] INFO Will not load MX4J, mx4j-tools.jar is not in the classpath (kafka.utils.Mx4jLoader$)
[2012-11-10 04:53:09,774] INFO [Controller 3]: Controller starting up (kafka.controller.KafkaController)
[2012-11-10 04:53:09,781] ERROR [KafkaApi-3] error when processing request (test_2,0,150,1048576) (kafka.server.KafkaApis)
kafka.common.UnknownTopicOrPartitionException: Topic test_2 partition 0 doesn't exist on 3
        at kafka.server.ReplicaManager.getLeaderReplicaIfLocal(ReplicaManager.scala:142)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSet(KafkaApis.scala:363)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:329)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:325)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map1.map(Map.scala:93)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSets(KafkaApis.scala:325)
        at kafka.server.KafkaApis.handleFetchRequest(KafkaApis.scala:293)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:58)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:41)
        at java.lang.Thread.run(Thread.java:619)[2012-11-10 04:53:09,781] ERROR [KafkaApi-3] error when processing request (test_1,0,250,1048576) (kafka.server.KafkaApis)
kafka.common.UnknownTopicOrPartitionException: Topic test_1 partition 0 doesn't exist on 3
        at kafka.server.ReplicaManager.getLeaderReplicaIfLocal(ReplicaManager.scala:142)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSet(KafkaApis.scala:363)
        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:329)        at kafka.server.KafkaApis$$anonfun$kafka$server$KafkaApis$$readMessageSets$1.apply(KafkaApis.scala:325)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.immutable.Map$Map1.map(Map.scala:93)
        at kafka.server.KafkaApis.kafka$server$KafkaApis$$readMessageSets(KafkaApis.scala:325)        at kafka.server.KafkaApis.handleFetchRequest(KafkaApis.scala:293)
        at kafka.server.KafkaApis.handle(KafkaApis.scala:58)
        at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:41)
        at java.lang.Thread.run(Thread.java:619)
[2012-11-10 04:53:09,806] INFO conflict in /controller data: 3 stored data: 1 (kafka.utils.ZkUtils$)
[2012-11-10 04:53:09,808] INFO [Controller 3]: Controller startup complete (kafka.controller.KafkaController)
[2012-11-10 04:53:09,825] INFO [Kafka Server 3], started (kafka.server.KafkaServer)
[2012-11-10 04:53:09,826] INFO Verifying properties (kafka.utils.VerifiableProperties)

• ConsoleConsumer log4j messages:

[2012-11-10 04:52:53,550] INFO Reconnect due to socket error:  (kafka.consumer.SimpleConsumer)
java.io.EOFException: Received -1 when reading from channel, socket has likely been closed.
        at kafka.utils.Utils$.read(Utils.scala:388)
        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)
. . .
[2012-11-10 04:52:53,558] INFO Fetching metadata for topic Set(test_1) (kafka.client.ClientUtils$)
[2012-11-10 04:52:53,558] INFO Connected to 172.17.166.123:9091 for producing (kafka.producer.SyncProducer)
[2012-11-10 04:52:53,582] INFO Disconnecting from 172.17.166.123:9091 (kafka.producer.SyncProducer)
[2012-11-10 04:53:36,054] ERROR Error processing message, stopping consumer:  (kafka.consumer.ConsoleConsumer$)
kafka.consumer.ConsumerTimeoutException
        at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:67)
        at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:33)
. . .
[2012-11-10 04:53:36,055] INFO [console-consumer-84076_ela4-app0997.prod-1352523123516-afb2c948], ZKConsumerConnector shutting down (kafka.consumer.ZookeeperConsum
erConnector)
[2012-11-10 04:53:36,055] INFO Shutting down topic event watcher. (kafka.consumer.ZookeeperTopicEventWatcher)
[2012-11-10 04:53:36,055] INFO Terminate ZkClient event thread. (org.I0Itec.zkclient.ZkEventThread)[2012-11-10 04:53:36,057] INFO Session: 0x13ae8a88aae000a closed (org.apache.zookeeper.ZooKeeper)
[2012-11-10 04:53:36,057] INFO EventThread shut down (org.apache.zookeeper.ClientCnxn)[2012-11-10 04:53:36,058] INFO Forcing shutdown of Kafka scheduler (kafka.utils.KafkaScheduler)
[2012-11-10 04:53:36,059] INFO [ConsumerFetcherManager-1352523123614] shutting down (kafka.consumer.ConsumerFetcherManager)
[2012-11-10 04:53:36,059] INFO [console-consumer-84076_ela4-app0997.prod-1352523123516-afb2c948-leader-finder-thread], Shutting down (kafka.consumer.ConsumerFetche
rManager$$anon$1)
[2012-11-10 04:53:36,703] INFO [console-consumer-84076_ela4-app0997.prod-1352523123516-afb2c948], stopping watcher executor thread for consumer console-consumer-84
076_ela4-app0997.prod-1352523123516-afb2c948 (kafka.consumer.ZookeeperConsumerConnector)
[2012-11-10 04:55:02,312] INFO Reconnect due to socket error:  (kafka.consumer.SimpleConsumer)
java.io.EOFException: Received -1 when reading from channel, socket has likely been closed.
        at kafka.utils.Utils$.read(Utils.scala:388)
        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)
. . .
[2012-11-10 04:55:03,252] INFO Unable to read additional data from server sessionid 0x13ae8a88aae0005, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2012-11-10 04:55:03,353] INFO zookeeper state changed (Disconnected) (org.I0Itec.zkclient.ZkClient)
;;;","11/Nov/12 22:46;jfung;log4j messages files for brokers and zookeepers are not uploaded to this JIRA but are available at host 0996 @ /export/home/kafka/kafka_hudson_test_output_logs/test_1352505679_KEEP/testcase_4011/logs;;;","03/Jul/13 21:43;jfung;Not an issue any more.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System Test Transient Failure (case 0302 GC Pause) - Log segments mismatched across replicas,KAFKA-606,12615661,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jfung,jfung,11/Nov/12 21:54,03/Jul/13 21:43,14/Jul/23 05:39,03/Jul/13 21:43,,,,,,,,,,,,,,0,,,,,,jfung,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/12 21:55;jfung;testcase_0302_data_and_log4j.tar.gz;https://issues.apache.org/jira/secure/attachment/12553050/testcase_0302_data_and_log4j.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,256962,,,Wed Jul 03 21:43:02 UTC 2013,,,,,,,,,,"0|i0j10v:",109095,,,,,,,,,,,,,,,,,,,,"11/Nov/12 22:02;jfung;• This issue is transient and is showing randomly in this functional test group. The test case is failing because the log segment checksums are not matching across the replicas. When a DumpLogSegment is run on one of the log segment files and indexes, it gives error:
    Non-secutive offsets in :00000000000000000405.log
    Exception in thread ""main"" java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer

• Transient Failure Testcase: 0302, 0308

• Functional Test Group: Replication with Simulated Leader GC Pause / 1 topic / 3 partitions / Replica Factor = 3 / No. of brokers = 3

• validation_status  : 
     Leader Election Latency MAX  :  None
     Leader Election Latency MIN  :  None
     Unique messages from consumer on [test_1] at simple_consumer_test_1-0_r1.log  :  971
     Unique messages from consumer on [test_1] at simple_consumer_test_1-0_r2.log  :  967
     Unique messages from consumer on [test_1] at simple_consumer_test_1-0_r3.log  :  971
     Unique messages from consumer on [test_1] at simple_consumer_test_1-1_r1.log  :  961
     Unique messages from consumer on [test_1] at simple_consumer_test_1-1_r2.log  :  961
     Unique messages from consumer on [test_1] at simple_consumer_test_1-1_r3.log  :  961
     Unique messages from consumer on [test_1] at simple_consumer_test_1-2_r1.log  :  825
     Unique messages from consumer on [test_1] at simple_consumer_test_1-2_r2.log  :  825
     Unique messages from consumer on [test_1] at simple_consumer_test_1-2_r3.log  :  825
     Validate for data matched on topic [test_1] across replicas  :  FAILED
     Validate for merged log segment checksum in cluster [source]  :  FAILED

• Failure is due to log segment checksum doesn't match.

• Leader with brokerID 2 is GC paused:

2012-11-11 04:03:50,967 - DEBUG - brokerid: [2] entity_id: [2] (kafka_system_test_utils)
2012-11-11 04:03:50,968 - INFO - ======================================================
2012-11-11 04:03:50,968 - INFO - Found leader with entity id: 2
2012-11-11 04:03:50,968 - INFO - ======================================================
2012-11-11 04:03:50,968 - DEBUG - executing command [ssh ela4-app0998.prod 'pid=17256; prev_pid=""""; echo $pid; while [[ ""x$pid"" != ""x"" ]]; do prev_pid=$pid;   for child in $(ps -o pid,ppid ax | awk ""{ if ( \$2 == $pid ) { print \$1 }}"");     do ech
o $child; pid=$child;   done;   if [ $prev_pid == $pid ]; then     break;   fi; done' 2> /dev/null (system_test_utils)
2012-11-11 04:03:51,120 - DEBUG - executing command [ssh ela4-app0998.prod 'kill -SIGSTOP 17258'] (system_test_utils)2012-11-11 04:03:51,136 - DEBUG - executing command [ssh ela4-app0998.prod 'kill -SIGSTOP 17256'] (system_test_utils)
2012-11-11 04:03:56,159 - DEBUG - executing command [ssh ela4-app0998.prod 'kill -SIGCONT 17256'] (system_test_utils)
2012-11-11 04:03:56,176 - DEBUG - executing command [ssh ela4-app0998.prod 'kill -SIGCONT 17258'] (system_test_utils)

• Log segment files in all brokers

kafka@ela4-app0996 [~/kafka_hudson_test_output_logs/test_1352592075/testcase_0302/logs] ./ls_log_segments.sh 

-rw-rw-r-- 1 kafka kafka 105966 Nov 11 05:30 broker-1/kafka_server_1_logs/test_1-0/00000000000000000000.log
-rw-rw-r-- 1 kafka kafka 105444 Nov 11 05:30 broker-1/kafka_server_1_logs/test_1-0/00000000000000000203.log
-rw-rw-r-- 1 kafka kafka 103878 Nov 11 05:30 broker-1/kafka_server_1_logs/test_1-0/00000000000000000405.log
-rw-rw-r-- 1 kafka kafka 103878 Nov 11 05:30 broker-1/kafka_server_1_logs/test_1-0/00000000000000000604.log
-rw-rw-r-- 1 kafka kafka  87696 Nov 11 05:30 broker-1/kafka_server_1_logs/test_1-0/00000000000000000803.log
. . .
======================================================

-rw-rw-r-- 1 kafka kafka 105966 Nov 11 05:30 broker-2/kafka_server_2_logs/test_1-0/00000000000000000000.log
-rw-rw-r-- 1 kafka kafka 105444 Nov 11 05:30 broker-2/kafka_server_2_logs/test_1-0/00000000000000000203.log
-rw-rw-r-- 1 kafka kafka 105444 Nov 11 05:30 broker-2/kafka_server_2_logs/test_1-0/00000000000000000405.log
-rw-rw-r-- 1 kafka kafka 114840 Nov 11 05:30 broker-2/kafka_server_2_logs/test_1-0/00000000000000000611.log    
-rw-rw-r-- 1 kafka kafka  73080 Nov 11 05:30 broker-2/kafka_server_2_logs/test_1-0/00000000000000000831.log
. . .
======================================================

-rw-rw-r-- 1 kafka kafka 105966 Nov 11 05:30 broker-3/kafka_server_3_logs/test_1-0/00000000000000000000.log
-rw-rw-r-- 1 kafka kafka 105444 Nov 11 05:30 broker-3/kafka_server_3_logs/test_1-0/00000000000000000203.log
-rw-rw-r-- 1 kafka kafka 103878 Nov 11 05:30 broker-3/kafka_server_3_logs/test_1-0/00000000000000000405.log
-rw-rw-r-- 1 kafka kafka 103878 Nov 11 05:30 broker-3/kafka_server_3_logs/test_1-0/00000000000000000604.log
-rw-rw-r-- 1 kafka kafka  87696 Nov 11 05:30 broker-3/kafka_server_3_logs/test_1-0/00000000000000000803.log
. . .

• DumpLogSegment is showing Exception:

kafka@ela4-app0996 [/mnt/u001/kafka_08_replication_system_test] bin/kafka-run-class.sh kafka.tools.DumpLogSegments --files /export/home/kafka/kafka_hudson_test_output_logs/test_1352592075/testcase_0302/logs/broker-2/kafka_server_2_logs/test_1-0/00000000000000000405.log,/export/home/kafka/kafka_hudson_test_output_logs/test_1352592075/testcase_0302/logs/broker-2/kafka_server_2_logs/test_1-0/00000000000000000405.index 
Dumping /export/home/kafka/kafka_hudson_test_output_logs/test_1352592075/testcase_0302/logs/broker-2/kafka_server_2_logs/test_1-0/00000000000000000405.log
Starting offset: 405
offset: 405 position: 0 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 2241529125
offset: 406 position: 522 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 2398559000
offset: 407 position: 1044 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 2470656351
offset: 408 position: 1566 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 3047120790
. . .
offset: 606 position: 102834 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 1253807556
offset: 607 position: 103356 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 1818751757
offset: 608 position: 103878 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 1711972973
offset: 609 position: 104400 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 1980328956
offset: 610 position: 104922 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 1566762723
Dumping /export/home/kafka/kafka_hudson_test_output_logs/test_1352592075/testcase_0302/logs/broker-2/kafka_server_2_logs/test_1-0/00000000000000000405.index
offset: 504 position: 49590
offset: 520 position: 57942
offset: 541 position: 68904
offset: 555 position: 76212
offset: 576 position: 87174
offset: 597 position: 98136
Non-secutive offsets in :00000000000000000405.log
Exception in thread ""main"" java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer
	at scala.runtime.BoxesRunTime.unboxToInt(Unknown Source)
	at scala.Tuple2._1$mcI$sp(Tuple2.scala:23)
	at kafka.tools.DumpLogSegments$$anonfun$main$3$$anonfun$apply$2.apply(DumpLogSegments.scala:85)
	at kafka.tools.DumpLogSegments$$anonfun$main$3$$anonfun$apply$2.apply(DumpLogSegments.scala:84)
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
	at scala.collection.immutable.List.foreach(List.scala:45)
	at kafka.tools.DumpLogSegments$$anonfun$main$3.apply(DumpLogSegments.scala:84)
	at kafka.tools.DumpLogSegments$$anonfun$main$3.apply(DumpLogSegments.scala:81)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
	at kafka.tools.DumpLogSegments$.main(DumpLogSegments.scala:81)
	at kafka.tools.DumpLogSegments.main(DumpLogSegments.scala)
;;;","11/Nov/12 22:15;jfung;log4j messages files for brokers and zookeepers are not uploaded to this JIRA but are available at host 0996 @ /export/home/kafka/kafka_hudson_test_output_logs/test_1352592075/testcase_0302/logs;;;","12/Nov/12 16:29;junrao;There seems to be 2 issues. (1) Since we only do shallow iteration in DumpLogSegments, the offsets may not be consecutive when messages are compressed. (2) There is a bug that we cached the offsets as Int. It should be long.;;;","03/Jul/13 21:43;jfung;Not an issue any more.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add missing metrics in 0.8,KAFKA-604,12615350,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,junrao,junrao,08/Nov/12 16:59,26/Feb/13 20:46,14/Jul/23 05:39,25/Feb/13 17:33,0.8.0,,,0.8.0,,,,,,,core,,,0,p2,,,"It would be good if we add the following metrics:

Producer: droppedMessageRate per topic

ReplicaManager: partition count on the broker

FileMessageSet: logFlushTimer per log (i.e., partition). Also, logFlushTime should probably be moved to LogSegment since the flush now includes index flush time.",,jjkoshy,junrao,swapnilghike,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/13 02:38;swapnilghike;kafka-604-new-v2.patch;https://issues.apache.org/jira/secure/attachment/12570594/kafka-604-new-v2.patch","23/Feb/13 02:04;swapnilghike;kafka-604-new.patch;https://issues.apache.org/jira/secure/attachment/12570588/kafka-604-new.patch","09/Nov/12 00:27;yeyangever;kafka_604_v1.patch;https://issues.apache.org/jira/secure/attachment/12552740/kafka_604_v1.patch","28/Nov/12 22:51;yeyangever;kafka_604_v2.patch;https://issues.apache.org/jira/secure/attachment/12555263/kafka_604_v2.patch","12/Dec/12 02:57;yeyangever;kafka_604_v3.diff;https://issues.apache.org/jira/secure/attachment/12560497/kafka_604_v3.diff",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,255985,,,Mon Feb 25 17:33:57 UTC 2013,,,,,,,,,,"0|i0gp1z:",95489,,,,,,,,,,,,,,,,,,,,"09/Nov/12 00:27;yeyangever;
1. Creating a KafkaTimer.time static function which takes a functor and (variable length) timers as input to support multiple timer.

2. add per-partition log flush timer.

3. make the log flush the time of flushing both the index and  data log;;;","09/Nov/12 01:14;jjkoshy;Looks good overall. Some initial comments:

KafkaTimer:
I see why you needed to add the Seq-based time method: to update a specific and
a global timer at one shot. However, we should still keep the previous (class)
method because: the new method is slightly clunky to use  (in having to wrap
everything around the function call parentheses). The main goal of that method
was convenience - or the user could directly deal with the timer and its
context. So can you restore that code and just use the new object method where
you need it?

With the above, the following can be reverted:
SimpleConsumer
KafkaController
ReplicaStateMachine
SyncProducer
KafkaTimerTest (although we should additionally exercise your new object
method).

Log.scala:
- Can you break line 488?;;;","09/Nov/12 02:02;jjkoshy;Couple other options that we discussed offline for KafkaTimer:

- Add the new multi-timer code to the object level
- Since the most common (only?) use-case is updating a specific and a global timer, have an additional Option[Timer] for the global timer in the constructor that defaults to None
- Currying. e.g.,

  def timeWith[A](timers: Timer*)(f: => A) = {
    val ctxSeq = timers.map(_.time())
    try {
      f
    }
    finally {
      ctxSeq.foreach(_.stop())
    }
  }

  and usage would be KafkaTimer.timeWith(specificTimer, globalTimer) { // block of code to time }

Clearly I'm obsessive compulsive :) but I think it would be good to avoid breaking the clean user-side syntax that the existing API allows.
;;;","28/Nov/12 22:51;yeyangever;rebased patch;;;","10/Dec/12 17:44;junrao;The patch no longer applies. We should probably also add the following metrics in the Producer:
1. Serialization time: time to encoder each event.
2. Handle time: time to handle one or a batch of events, which includes serialization time, request sending time, and retry time.;;;","10/Dec/12 18:13;yeyangever;Sure, I'll do that soon

Best,
-----------------------
Victor Yang Ye
+1(650)283-6547

http://www.linkedin.com/pub/victor-yang-ye/13/ba3/4b8<http://www.linkedin.com/profile/view?id=47740172>
http://www.facebook.com/yeyangever

Founder   Uyan.cc
Software Engineer in Distributed Data System Group, LinkedIn Corporation
Dept. of Computer Science, Graduate School of Art and Scient, Columbia
University
Special Pilot Computer Science Class, Tsinghua University

yeyangever@gmail.com
yye@linkedin.com
yy2314@columbia.edu




;;;","12/Dec/12 02:57;yeyangever;

1. rebased

2. add new timer metrics


unit tests passed and the new timer mBeans are verified. 

This patch had better be checked in earlier as it touches quite a few files whose future changes may need furhur rebase

;;;","12/Dec/12 05:22;swapnilghike;Wow, this is going to be fun. :) Unfortunately patch v3 here has a lot of conflicts with patch v5 at KAFKA-646 where I have done a wholesale reorganization of metrics. The conflicts are mostly in the use of KafkaTimer.

I agree that having a KafkaTimer object might be more efficient at runtime, but the tradeoff is that it changes the syntax of time measurement with some instances where we will need to pass in ({a bunch of statements spread out over multiple lines in a block}, timer).

Victor, can you take a look at patch v5 at the other jira and see which style of time measurement you prefer?

Perhaps Joel can also comment on the syntax changes. ;;;","12/Dec/12 18:06;jjkoshy;The rebased patch does not really comment on/address the concern in the original review - i.e., it breaks a relatively clean
and convenient syntax in the common case (of updating a single timer) to support updates to multiple timers. It's obviously
not a particularly major issue, but a rung higher than say, a debate over whitespace/coding style. My only point is that we
should try and avoid making a change that goes from a nice syntax to a rather inconvenient syntax. For that reason, I'm
more in favor of the change to KafkaTimer in KAFKA-646.

That said, how about the following: both these patches need multiple timer updates to update an aggregate timer as well
as a specific timer - i.e., up to this point we don't really have a use case for updating more than two timers simultaneously.
So we can accomplish this case with the following:

aggregatekafkatimer.time {
  specifickafkatimer.time {
    <code block>
  }
}

and avoid any change to KafkaTimer. Does that seem reasonable to you guys?
;;;","12/Dec/12 18:45;swapnilghike;Yes I like this idea, can follow this in KAFKA-646.;;;","12/Dec/12 19:15;yeyangever;Joel, thanks for the idea, it's good and I will do a new patch

Sent from my iPhone


;;;","18/Feb/13 23:15;junrao;This patch no longer applies. Thinking a bit. We probably just need to get the following stats in jmx. The per topic level flush time is probably overkill.

Producer: droppedMessageRate per topic 

ReplicaManager: partition count on the broker ;;;","19/Feb/13 01:28;junrao;Also, in ProducerSendThread, we should get rid of the thread id part from the jmx bean name.
  newGauge(clientId + ""-ProducerQueueSize-"" + getId,
          new Gauge[Int] {
            def getValue = queue.size
          })
;;;","19/Feb/13 09:45;swapnilghike;I can do this after finishing KAFKA-755.;;;","23/Feb/13 02:04;swapnilghike;Added droppedMessageRate per topic in Producer, partition count on the broker in ReplicaManager. 

Moved droppedMessageRate to per-topic stats, the droppedMessageRate for all topics together is now provided by allTopicsStats in ProducerTopicStats.

Removed the thread id from ProducerQueueSize guage.;;;","23/Feb/13 02:38;swapnilghike;Thanks Jun for observing that ""All.Topics"" does not uniquely identify all topics' stats from stats for a topic called All.Topics.

Made a change so that the stats for all topics will look like ""AllTopicsMessagesPerSec"", and those for a topic called AllTopics will look like ""AllTopics-MessagesPerSec"". Same change for All.Brokers too.;;;","25/Feb/13 17:33;junrao;Thanks for the latest patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System Test Data Validation Failure - Replication Factor less than No. of Broker ,KAFKA-603,12615279,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,,jfung,jfung,08/Nov/12 03:25,09/Nov/12 18:43,14/Jul/23 05:39,09/Nov/12 18:43,,,,,,,,,,,,,,0,,,,,,jfung,jjkoshy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/12 03:28;jfung;kafka-603-reproduce-issue.patch;https://issues.apache.org/jira/secure/attachment/12552610/kafka-603-reproduce-issue.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,255884,,,Fri Nov 09 18:43:17 UTC 2012,,,,,,,,,,"0|i0fvkf:",90712,,,,,,,,,,,,,,,,,,,,"08/Nov/12 03:26;jfung;This test case (testcase_4011) is set up as followings:
1. Set up a 3-broker cluster with 2 topics, 2 partitions, Replication Factor 2
2. Producer to produce messages in Sync mode, Acks = -1, Compression Off
3. The expected results would be to see 3 Simple Consumer message count: one of them is zero, two of them are equal.
4. The results shown below is that all ConsoleConsumer messages are matching. However, simple consumer are showing unexpected messages count


To reproduce the issue:
1. Download the latest 0.8 branch
2. Apply the attached patch and build
3. In <kafka_home>/system_test, execute: python –B system_test_runner.py
 

validation_status  : 
     Unique messages from consumer on [test_1]  :  1000
     Unique messages from consumer on [test_1] at simple_consumer_1.log  :  1000
     Unique messages from consumer on [test_1] at simple_consumer_2.log  :  500
     Unique messages from consumer on [test_1] at simple_consumer_3.log  :  500
     Unique messages from consumer on [test_2]  :  1000
     Unique messages from consumer on [test_2] at simple_consumer_1.log  :  1000
     Unique messages from consumer on [test_2] at simple_consumer_2.log  :  500
     Unique messages from consumer on [test_2] at simple_consumer_3.log  :  505
     Unique messages from producer on [test_1]  :  1000
     Unique messages from producer on [test_2]  :  1000
     Validate for data matched on topic [test_1]  :  PASSED
     Validate for data matched on topic [test_1] across replicas  :  FAILED
     Validate for data matched on topic [test_2]  :  PASSED
     Validate for data matched on topic [test_2] across replicas  :  FAILED



Broker Log Segments:

-rw-r--r-- 1 jfung eng 102834 Nov  7 16:39 kafka_server_1_logs/test_1-0/00000000000000000000.log
-rw-r--r-- 1 jfung eng 102834 Nov  7 16:40 kafka_server_1_logs/test_1-0/00000000000000000197.log
-rw-r--r-- 1 jfung eng  55332 Nov  7 16:40 kafka_server_1_logs/test_1-0/00000000000000000394.log

-rw-r--r-- 1 jfung eng 102834 Nov  7 16:39 kafka_server_1_logs/test_1-1/00000000000000000000.log
-rw-r--r-- 1 jfung eng 102834 Nov  7 16:40 kafka_server_1_logs/test_1-1/00000000000000000197.log
-rw-r--r-- 1 jfung eng  55332 Nov  7 16:40 kafka_server_1_logs/test_1-1/00000000000000000394.log

-rw-r--r-- 1 jfung eng 102834 Nov  7 16:39 kafka_server_1_logs/test_2-0/00000000000000000000.log
-rw-r--r-- 1 jfung eng 102834 Nov  7 16:40 kafka_server_1_logs/test_2-0/00000000000000000197.log
-rw-r--r-- 1 jfung eng  57942 Nov  7 16:40 kafka_server_1_logs/test_2-0/00000000000000000394.log

-rw-r--r-- 1 jfung eng 102834 Nov  7 16:39 kafka_server_1_logs/test_2-1/00000000000000000000.log
-rw-r--r-- 1 jfung eng 102834 Nov  7 16:40 kafka_server_1_logs/test_2-1/00000000000000000197.log
-rw-r--r-- 1 jfung eng  55332 Nov  7 16:40 kafka_server_1_logs/test_2-1/00000000000000000394.log

======================================================

-rw-r--r-- 1 jfung eng 102834 Nov  7 16:39 kafka_server_2_logs/test_1-1/00000000000000000000.log
-rw-r--r-- 1 jfung eng 102834 Nov  7 16:40 kafka_server_2_logs/test_1-1/00000000000000000197.log
-rw-r--r-- 1 jfung eng  55332 Nov  7 16:40 kafka_server_2_logs/test_1-1/00000000000000000394.log

-rw-r--r-- 1 jfung eng 102834 Nov  7 16:39 kafka_server_2_logs/test_2-1/00000000000000000000.log
-rw-r--r-- 1 jfung eng 102834 Nov  7 16:40 kafka_server_2_logs/test_2-1/00000000000000000197.log
-rw-r--r-- 1 jfung eng  55332 Nov  7 16:40 kafka_server_2_logs/test_2-1/00000000000000000394.log

======================================================

-rw-r--r-- 1 jfung eng 102834 Nov  7 16:39 kafka_server_3_logs/test_1-0/00000000000000000000.log
-rw-r--r-- 1 jfung eng 102834 Nov  7 16:40 kafka_server_3_logs/test_1-0/00000000000000000197.log
-rw-r--r-- 1 jfung eng  55332 Nov  7 16:40 kafka_server_3_logs/test_1-0/00000000000000000394.log

-rw-r--r-- 1 jfung eng 102834 Nov  7 16:39 kafka_server_3_logs/test_2-0/00000000000000000000.log
-rw-r--r-- 1 jfung eng 102834 Nov  7 16:40 kafka_server_3_logs/test_2-0/00000000000000000197.log
-rw-r--r-- 1 jfung eng  57942 Nov  7 16:40 kafka_server_3_logs/test_2-0/00000000000000000394.log
;;;","08/Nov/12 04:05;jfung;ConsoleConsumer command:

2012-11-07 20:02:00,410 - DEBUG - executing command: [ssh localhost 'JAVA_HOME=/export/apps/jdk/JDK-1_6_0_27 JMX_PORT=9999 /home/jfung/workspace_kafka/kafka_r1406911_603_reproduce_issue/bin/kafka-run-class.sh kafka.consumer.ConsoleConsumer --zookeeper localhost:2188 --topic test_1 --consumer-timeout-ms 60000 --csv-reporter-enabled --metrics-dir /home/jfung/workspace_kafka/kafka_r1406911_603_reproduce_issue/system_test/log_retention_testsuite/testcase_4011/logs/console_consumer-6/metrics  --from-beginning   >> /home/jfung/workspace_kafka/kafka_r1406911_603_reproduce_issue/system_test/log_retention_testsuite/testcase_4011/logs/console_consumer-6/console_consumer.log  & echo pid:$! > /home/jfung/workspace_kafka/kafka_r1406911_603_reproduce_issue/system_test/log_retention_testsuite/testcase_4011/logs/console_consumer-6/entity_6_pid'] (kafka_system_test_utils)


SimpleConsumerShell command:

2012-11-07 20:04:17,444 - DEBUG - executing command: [ssh localhost 'JAVA_HOME=/export/apps/jdk/JDK-1_6_0_27 /home/jfung/workspace_kafka/kafka_r1406911_603_reproduce_issue/bin/kafka-run-class.sh kafka.tools.SimpleConsumerShell --broker-list localhost:9091,localhost:9092,localhost:9093 --topic test_1 --partition 0 --replica 1 --offset 0 --no-wait-at-logend   >> /home/jfung/workspace_kafka/kafka_r1406911_603_reproduce_issue/system_test/log_retention_testsuite/testcase_4011/logs/console_consumer-6/simple_consumer_1.log  & echo pid:$! > /home/jfung/workspace_kafka/kafka_r1406911_603_reproduce_issue/system_test/log_retention_testsuite/testcase_4011/logs/console_consumer-6/entity_6_pid'] (kafka_system_test_utils);;;","09/Nov/12 02:14;jjkoshy;I think this is a non-issue, since the per-partition breakdown of consumed messages revealed that each partition ends up on exactly two replicas.
;;;","09/Nov/12 18:43;jfung;Thanks Joel for looking into this. The validation approach is modified to break down the messages received per partition as shown below:

_test_case_name  :  testcase_4011
_test_class_name  :  ReplicaBasicTest
arg : bounce_broker  :  true
arg : broker_down_time_in_sec  :  5
arg : broker_type  :  leader
arg : log_retention_test  :  true
arg : message_producing_free_time_sec  :  15
arg : num_iteration  :  1
arg : num_partition  :  2
arg : replica_factor  :  2
arg : sleep_seconds_between_producer_calls  :  1
validation_status  : 
     Leader Election Latency - iter 1 brokerid 2  :  325.00 ms
     Leader Election Latency MAX  :  325.00
     Leader Election Latency MIN  :  325.00
     Unique messages from consumer on [test_1]  :  4000
     Unique messages from consumer on [test_1] at simple_consumer_test_1-0_r1.log  :  2000
     Unique messages from consumer on [test_1] at simple_consumer_test_1-0_r2.log  :  2000
     Unique messages from consumer on [test_1] at simple_consumer_test_1-0_r3.log  :  0
     Unique messages from consumer on [test_1] at simple_consumer_test_1-1_r1.log  :  0
     Unique messages from consumer on [test_1] at simple_consumer_test_1-1_r2.log  :  2000
     Unique messages from consumer on [test_1] at simple_consumer_test_1-1_r3.log  :  2000
     Unique messages from consumer on [test_2]  :  3500
     Unique messages from consumer on [test_2] at simple_consumer_test_2-0_r1.log  :  1755
     Unique messages from consumer on [test_2] at simple_consumer_test_2-0_r2.log  :  1755
     Unique messages from consumer on [test_2] at simple_consumer_test_2-0_r3.log  :  0
     Unique messages from consumer on [test_2] at simple_consumer_test_2-1_r1.log  :  0
     Unique messages from consumer on [test_2] at simple_consumer_test_2-1_r2.log  :  1745
     Unique messages from consumer on [test_2] at simple_consumer_test_2-1_r3.log  :  1745
     Unique messages from producer on [test_1]  :  4000
     Unique messages from producer on [test_2]  :  3500
     Validate for data matched on topic [test_1]  :  PASSED
     Validate for data matched on topic [test_1] across replicas  :  PASSED
     Validate for data matched on topic [test_2]  :  PASSED
     Validate for data matched on topic [test_2] across replicas  :  PASSED
     Validate leader election successful  :  PASSED

This ticket can be closed now;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka should respond gracefully rather than crash when unable to write due to ENOSPC,KAFKA-600,12614599,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,polarbearcold,polarbearcold,02/Nov/12 22:54,14/Dec/12 14:56,14/Jul/23 05:39,14/Dec/12 14:56,,,,,,,,,,,core,,,0,,,,"problem:
user starts kafka with log.dir value set to a small partition and begins writing data to the mq.  when the disk partition is full, kafka crashes.  given that this product is used for both reading and writing operations, crashing seems rather drastic even if the error message is helpful.   something more robust would be appreciated.  perhaps, logging an error and rejecting additional write requests while accepting additional read requests?  perhaps, sending an email alert to Operations?  at least shutdown gracefully so the user is aware that received messages were saved with a helpful message providing some details of the last message received.  when tens or hundreds of thousands of messages can be processed in a second, it isn't helpful to merely log a timestamp and crash.

steps to reproduce:
1) download and install kafka
2) modify server.properties
    # vi /opt/kafka-0.7.2-incubating-src/config/server.properties
    set log.dir=""/var/log/kafka""
3) modify log4j
    # vi /opt/kafka-0.7.2-incubating-src/config/log4j.properties
    set fileAppender.File=/var/log/kafka/kafka-request.log
4) start kafka service
    $ sudo bash
    # ulimit -c unlimited
    # /opt/kafka-0.7.2-incubating-src/bin/kafka-server-start.sh /opt/kafka-0.7.2-incubating-src/config/server.properties &
6) begin writing data to hostname:9092
7) review /var/log/kafka-request.log

results:
$ grep log.dir /opt/kafka-0.7.2-incubating-src/config/server.properties
log.dir=/var/log/kafka
$ df -h /var/log/kafka
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1       4.0G  4.0G     0 100% /
$ tail /var/log/kafka/kafka-request.log
17627442 [ZkClient-EventThread-14-10.0.20.242:2181] INFO  kafka.server.KafkaZooKeeper  - Begin registering broker topic /brokers/topics/raw/0 with 1 partitions
17627444 [ZkClient-EventThread-14-10.0.20.242:2181] INFO  kafka.server.KafkaZooKeeper  - End registering broker topic /brokers/topics/raw/0
17627445 [ZkClient-EventThread-14-10.0.20.242:2181] INFO  kafka.server.KafkaZooKeeper  - done re-registering broker
18337676 [kafka-processor-3] ERROR kafka.network.Processor  - Closing socket for /10.0.20.138 because of error
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:218)
        at sun.nio.ch.IOUtil.read(IOUtil.java:191)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
        at kafka.utils.Utils$.read(Utils.scala:538)
        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)
        at kafka.network.Processor.read(SocketServer.scala:311)
        at kafka.network.Processor.run(SocketServer.scala:214)
        at java.lang.Thread.run(Thread.java:722)
18391974 [kafka-processor-4] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18422004 [kafka-processor-5] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18434563 [kafka-processor-6] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18485005 [kafka-processor-7] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18497083 [kafka-processor-0] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18525720 [kafka-processor-1] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18543843 [kafka-processor-2] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18563230 [kafka-processor-4] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18575613 [kafka-processor-5] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.138.
18677568 [kafka-processor-6] ERROR kafka.network.Processor  - Closing socket for /10.0.20.138 because of error
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:218)
        at sun.nio.ch.IOUtil.read(IOUtil.java:191)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359)
        at kafka.utils.Utils$.read(Utils.scala:538)
        at kafka.network.BoundedByteBufferReceive.readFrom(BoundedByteBufferReceive.scala:54)
        at kafka.network.Processor.read(SocketServer.scala:311)
        at kafka.network.Processor.run(SocketServer.scala:214)
        at java.lang.Thread.run(Thread.java:722)
18828016 [kafka-processor-7] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.248.
18844274 [kafka-processor-0] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.248.
18849691 [kafka-processor-1] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.248.
18896883 [kafka-processor-2] INFO  kafka.network.Processor  - Closing socket connection to /10.0.20.248.
22383195 [kafka-processor-2] FATAL kafka.log.Log  - Halting due to unrecoverable I/O error while handling producer request
java.io.IOException: No space left on device
        at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
        at sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:59)
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:89)
        at sun.nio.ch.IOUtil.write(IOUtil.java:60)
        at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:195)
        at kafka.message.ByteBufferMessageSet.writeTo(ByteBufferMessageSet.scala:76)
        at kafka.message.FileMessageSet.append(FileMessageSet.scala:159)
        at kafka.log.LogSegment.append(Log.scala:105)
        at kafka.log.Log.liftedTree1$1(Log.scala:246)
        at kafka.log.Log.append(Log.scala:242)
        at kafka.server.KafkaRequestHandlers.kafka$server$KafkaRequestHandlers$$handleProducerRequest(KafkaRequestHandlers.scala:69)
        at kafka.server.KafkaRequestHandlers.handleProducerRequest(KafkaRequestHandlers.scala:53)
        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:38)
        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:38)
        at kafka.network.Processor.handle(SocketServer.scala:296)
        at kafka.network.Processor.read(SocketServer.scala:319)
        at kafka.network.Processor.run(SocketServer.scala:214)
        at java.lang.Thread.run(Thread.java:722)
",,jkreps,polarbearcold,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,255015,,,Fri Dec 14 14:56:47 UTC 2012,,,,,,,,,,"0|i0eohz:",83731,,,,,,,,,,,,,,,,,,,,"03/Nov/12 00:06;jkreps;This seems sensible but the problem is this. If you get ENOSPC, you likely accepted a partial write, which means the log itself is corrupt. To fix this we need to run recovery on the log (which checks each message in the last segment and truncates off any invalid partitial writes). This happens automatically when the server is restarted. Theoretically this could be done automatically but since the recovery process can be slow (minutes) and we can't accept writes during that time we felt the best course of action is to have the node shoot itself in the head and let other healthy nodes take over.;;;","14/Dec/12 14:56;jkreps;Since there don't seem to be any objections I am closing out this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor KafkaScheduler,KAFKA-597,12614258,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jkreps,jkreps,jkreps,31/Oct/12 18:46,10/Dec/12 19:31,14/Jul/23 05:39,10/Dec/12 19:31,0.8.1,,,,,,,,,,,,,0,,,,"It would be nice to cleanup KafkaScheduler. Here is what I am thinking

Extract the following interface:

trait Scheduler {
  def startup()
  def schedule(fun: () => Unit, name: String, delayMs: Long = 0, periodMs: Long): Scheduled
  def shutdown(interrupt: Boolean = false)
}

class Scheduled {
  def lastExecution: Long
  def cancel()
}

We would have two implementations, KafkaScheduler and  MockScheduler. KafkaScheduler would be a wrapper for ScheduledThreadPoolExecutor. MockScheduler would only allow manual time advancement rather than using the system clock, we would switch unit tests over to this.

This change would be different from the existing scheduler in a the following ways:
1. Would not return a ScheduledFuture (since this is useless)
2. shutdown() would be a blocking call. The current shutdown calls, don't really do what people want.
3. We would remove the daemon thread flag, as I don't think it works.
4. It returns an object which let's you cancel the job or get the last execution time.

",,jjkoshy,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/12 05:33;jkreps;KAFKA-597-v1.patch;https://issues.apache.org/jira/secure/attachment/12555896/KAFKA-597-v1.patch","04/Dec/12 18:10;jkreps;KAFKA-597-v2.patch;https://issues.apache.org/jira/secure/attachment/12555960/KAFKA-597-v2.patch","06/Dec/12 00:43;jkreps;KAFKA-597-v3.patch;https://issues.apache.org/jira/secure/attachment/12556189/KAFKA-597-v3.patch","07/Dec/12 23:12;jkreps;KAFKA-597-v4.patch;https://issues.apache.org/jira/secure/attachment/12559975/KAFKA-597-v4.patch","08/Dec/12 04:02;jkreps;KAFKA-597-v5.patch;https://issues.apache.org/jira/secure/attachment/12560016/KAFKA-597-v5.patch","10/Dec/12 19:11;jkreps;KAFKA-597-v6.patch;https://issues.apache.org/jira/secure/attachment/12560239/KAFKA-597-v6.patch",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,253491,,,Mon Dec 10 19:22:51 UTC 2012,,,,,,,,,,"0|i0dnun:",77793,,,,,,,,,,,,,,,,,,,,"04/Dec/12 05:33;jkreps;This patch refactors the scheduler as described above except that I didn't implement task cancelation.

It also converts LogManagerTest to use the new MockScheduler.;;;","04/Dec/12 18:10;jkreps;Updated patch, includes misc additional cleanups.;;;","06/Dec/12 00:43;jkreps;Attached v3 patch, same as before but fully rebased to trunk.;;;","06/Dec/12 05:33;jkreps;Ready for review.;;;","07/Dec/12 23:12;jkreps;Patch v4. 
- Rebased
- Makes use of thread factory
- Fixed broken scaladoc;;;","08/Dec/12 01:35;jjkoshy;I haven't fully reviewed, but a couple of initial comments:
- I think the javadoc on KafkaScheduler's daemon param is a bit misleading as it currently suggests that daemon=true would prevent the VM from shutting down.
- The patch inverts the daemon flag on some of the existing usages of KafkaScheduler - i.e., daemon now defaults to true and there are some places where daemon was false. We would need to survey these usages and identify whether it makes sense to keep them non-daemon or not.
- The other question is on shutdownNow: the previous scheduler allowed the relaxed shutdown - i.e., don't interrupt threads that are currently executing. This change forces all shutdowns to use shutdownNow. Question is whether there are existing tasks that need to complete that would not tolerate an interrupt. I'm not sure about that - we'll need to look at existing usages. E.g., KafkaServer's kafkaScheduler used the shutdown() method - now it's effectively shutdownNow.
;;;","08/Dec/12 04:02;jkreps;Thanks, new patch v5 addresses your comments:
- Improved javadoc
- This is actually good. I thought about it a bit and since I am making shutdown block the only time daemon vs non-daemon comes into play is if you don't call shutdown. If that is the case non-daemon threads will prevent garbage collection of the scheduler tasks and eventually block shutdown of the jvm, which seems unnecessary.
- The change to shutdownNow is not good. This will invoke interrupt on all threads, which is too aggressive. Better to let them finish. If we end up needing to schedule long-running tasks we can invent a new notification mechanism. I changed this so that we use normal shutdown instead.;;;","08/Dec/12 06:55;jjkoshy;On daemon vs non-daemon and shutdown vs shutdownNow. I may be misunderstanding the javadoc but I think: since the
default is now daemon=true and you switched to use shutdown, VM shutdown can continue even in the middle of a
scheduled task like checkpointing high watermarks or cleaning up logs. i.e., there may be such scenarios where it makes
sense to make them non-daemon - i.e., set it as a non-daemon, and use shutdown (not shutdownNow - or use
shutdownNow and handle InterruptedException properly in the task) to let them finish gracefully. Otherwise (iiuc)
it seems if we call shutdown on the executor the VM could exit and simply kill (i.e., abruptly terminate) any running
task that was started by the executor in one of the (daemon) threads from its pool.

Minor comments:
- Line 81 of KafkaScheduler: closing brace is mis-aligned.
- The scaladoc on MockScheduler uses a non-existent schedule variant - i.e., I think you intended to add a period < 0
  no?
;;;","08/Dec/12 15:35;jkreps;Nice catch on the brace, will fix before check in.

With respect to the scaladoc, that is a legitimate call, I think. The api take default values so all of the following should work:
  scheduler.schedule(""task"", println(""hello"")) // immediately kick of a one-time background task
  scheduler.schedule(""task"", println(""hello""), delay=50) // kick off a one-time task in 50ms
  scheduler.schedule(""task"", println(""hello""), period = 50) // immediately kick off a repeating task that will repeat every 50 ms
  etc

WRT daemon, you are correct, but I don't think this is necessarily a bad thing. The requirement of the api is that you call startup() before calling schedule() and call shutdown() when done. Shutdown was previously a non-blocking call so it was very important whether the threads were blocking or non-blocking (but this functionality was totally *broken*) because you would likely call shutdown() and then exit the JVM. Now the only possible way to get to JVM shutdown with remaining scheduler threads is if your program has a bug and fails to call shutdown(). What should we do in this case? Hard to say. All tasks must handle unclean shutdown because unclean shutdown is a lot like a crash. Blocking JVM shutdown can really mess up automated deployment, so defaulting to that is not necessarily wise. So I am fine with either default but at least the consumer scheduler really needs to be set to daemon so we don't block people's jvms.;;;","10/Dec/12 19:11;jkreps;Attached patch v6, which fixes Joel's comments--
- Formatting issue
- Scaladoc issue
- Changed shutdownNow to shutdown
- But leaves daemon as the defualt;;;","10/Dec/12 19:22;jjkoshy;+1 - thanks for the patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogSegment.firstAppendTime not reset after truncate to,KAFKA-596,12614203,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,junrao,junrao,31/Oct/12 16:48,05/Nov/12 23:11,14/Jul/23 05:39,05/Nov/12 23:11,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,,,"Currently, we don't reset LogSegment.firstAppendTime after the segment is truncated. What can happen is that we truncate the segment to size 0 and on next append, a new log segment with the same starting offset is rolled because the time-based rolling is triggered.",,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/12 18:32;swapnilghike;kafka-596-v2.patch;https://issues.apache.org/jira/secure/attachment/12551901/kafka-596-v2.patch","02/Nov/12 23:55;swapnilghike;kafka-596-v3.patch;https://issues.apache.org/jira/secure/attachment/12551948/kafka-596-v3.patch","01/Nov/12 21:49;swapnilghike;kafka-596.patch;https://issues.apache.org/jira/secure/attachment/12551773/kafka-596.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,253435,,,Mon Nov 05 23:11:15 UTC 2012,,,,,,,,,,"0|i0dnhz:",77736,,,,,,,,,,,,,,,,,,,,"31/Oct/12 16:52;junrao;The fix is to set LogSegment.firstAppendTime to none if we truncate the segment to size 0. However, this brings up the deeper question of how do we prevent segments with identical starting offset from being created during log roll? Maybe, we should add a check in log.roll to guard this.;;;","01/Nov/12 21:49;swapnilghike;Yes, that's a pretty sharp observation. The current time based roll policy only checks whether the segment was not appended for its lifetime. This patch has three fixes: 

1. Initial assignment of firstAppendTime in Logsegment, because the non-primary constructor could potentially initialize the messageSet and set its size > 0. (I don't know if this fix will affect any other jiras.)

2. In maybeRoll(), a new condition makes sure that roll() happens based on time only if the messageset size > 0, thus different segments cannot have identical starting offsets. It also makes sure that a new segment is not rolled if the last segment is not appended with messages until now.

2. A segment is reborn at three places by setting its firstAppendTime to None if the message set size is 0 - 
i. Log.maybeRoll()
ii. Logsement.truncateTo()
iii. Log.markedDeletedWhile();;;","02/Nov/12 05:04;junrao;Thanks for the patch. A couple of comments:

1.  Log.maybeRoll(): Are the following lines needed since we are not creating a new segment?
      if (segment.messageSet.sizeInBytes == 0)
        segment.firstAppendTime = None

2. Log.markedDeletedWhile(): Is the following line needed since we are not creating a new segment?
          view(numToDelete - 1).firstAppendTime = None

;;;","02/Nov/12 07:01;swapnilghike;Actually the modification in maybeRoll() in the conditions that determine whether to roll a new segment or not, is enough for correctly fixing the issue mentioned above. The fix rolls a new segment only if the size of messageSet is > 0. So if we truncated the segment to size 0, maybeRoll() will not roll a new segment at the same starting offset. 

I kept those lines in Log.maybeRoll(), Logsement.truncateTo() and Log.markedDeletedWhile() for optimization. Setting the firstAppendTime to None whenever the size is found to be 0 will postpone the next time based roll and also will not harm correctness.;;;","02/Nov/12 15:11;junrao;I agree that setting firstAppendTime to None in Logsement.truncateTo() is necessary. However, I don't think this is necessary in Log.maybeRoll() and Log.markedDeletedWhile(). In both cases, we are not changing the log segment. So whoever changed the segment last to make its size 0 (either through truncation or creation) would have set firstAppendTime properly. Note that maybeRoll is called on every log append. We don't want to add unnecessary overhead.;;;","02/Nov/12 18:32;swapnilghike;Yes, I see your point. Attached a new patch.;;;","02/Nov/12 23:55;swapnilghike;After a discussion with Jun, reverted the conditions in maybeRoll() to the trunk version.

Setting firstAppendTime to None in LogSegment.truncateTo() when messageSet size becomes 0 is enough to make sure that Log.maybeRoll() will not roll a new segment at the same starting offset as the last segment.;;;","04/Nov/12 21:42;nehanarkhede;Good catch, Jun !

+1 on v3;;;","05/Nov/12 23:11;junrao;Thanks for patch v3. +1. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Empty log index file created when it shouldn't be empty,KAFKA-593,12613975,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,yeyangever,yeyangever,30/Oct/12 00:07,06/Nov/12 03:44,14/Jul/23 05:39,06/Nov/12 03:44,,,,,,,,,,,,,,0,,,,"We have met empty index file during system test when it shouldn't be empty. In this case, there're around 100 messages in each segment, each of size around 100 bytes, given the ""logIndexIntervalBytes"" 4096, there should be at least 2 log index entries, but we see empty index file. The kafka and zookeeper logs are attached



[yye@yye-ld kafka_server_3_logs]$ cd test_1-2/
[yye@yye-ld test_1-2]$ ls -l
total 84
-rw-r--r-- 1 yye eng        8 Oct 29 15:22 00000000000000000000.index
-rw-r--r-- 1 yye eng    10248 Oct 29 15:22 00000000000000000000.log
-rw-r--r-- 1 yye eng        8 Oct 29 15:22 00000000000000000100.index
-rw-r--r-- 1 yye eng    10296 Oct 29 15:22 00000000000000000100.log
-rw-r--r-- 1 yye eng        0 Oct 29 15:23 00000000000000000200.index
-rw-r--r-- 1 yye eng    10293 Oct 29 15:23 00000000000000000200.log
-rw-r--r-- 1 yye eng        0 Oct 29 15:23 00000000000000000300.index
-rw-r--r-- 1 yye eng    10274 Oct 29 15:23 00000000000000000300.log
-rw-r--r-- 1 yye eng        0 Oct 29 15:23 00000000000000000399.index
-rw-r--r-- 1 yye eng    10276 Oct 29 15:23 00000000000000000399.log
-rw-r--r-- 1 yye eng        0 Oct 29 15:23 00000000000000000498.index
-rw-r--r-- 1 yye eng    10256 Oct 29 15:23 00000000000000000498.log
-rw-r--r-- 1 yye eng 10485760 Oct 29 15:23 00000000000000000596.index
-rw-r--r-- 1 yye eng     3564 Oct 29 15:23 00000000000000000596.log
",,jkreps,junrao,nehanarkhede,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-583,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/12 00:08;yeyangever;kafka_583_zk_kafka_data.tar.gz;https://issues.apache.org/jira/secure/attachment/12551284/kafka_583_zk_kafka_data.tar.gz","02/Nov/12 02:48;yeyangever;kafka_593_v1.diff;https://issues.apache.org/jira/secure/attachment/12551812/kafka_593_v1.diff","02/Nov/12 23:41;yeyangever;kafka_593_v2.diff;https://issues.apache.org/jira/secure/attachment/12551947/kafka_593_v2.diff","03/Nov/12 01:02;yeyangever;kafka_593_v3.diff;https://issues.apache.org/jira/secure/attachment/12551957/kafka_593_v3.diff","05/Nov/12 19:47;yeyangever;kafka_593_v4.diff;https://issues.apache.org/jira/secure/attachment/12552150/kafka_593_v4.diff","06/Nov/12 00:08;yeyangever;kafka_593_v5.diff;https://issues.apache.org/jira/secure/attachment/12552187/kafka_593_v5.diff",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,252903,,,Tue Nov 06 03:44:23 UTC 2012,,,,,,,,,,"0|i0da87:",75586,,,,,,,,,,,,,,,,,,,,"30/Oct/12 03:59;jkreps;I am not sure that this is a bug. The index entries are not placed at exact intervals. Rather, we add a single index entry per append if the bytesSinceLastIndexEntry > indexIntervalBytes. What this means is that you could get a log of 10,296 bytes by appending a single message set of that size or by appending one message set <= 4095 and then another that made up the difference.

What was batch size being used in this test?;;;","30/Oct/12 16:28;junrao;Producer used sync mode. So, there is 1 message per batch and each segment has about 100 messages. I expect at least 2 index entries being added per segment.;;;","31/Oct/12 15:12;junrao;Here is the issue. We rolled a new segment in the follower. The follower in one fetch gets 10k bytes of data and appends to its log. This won't add any index entry since it's the very first append to this segment. After the append, the log rolled since the max segment size is reached. This leaves an empty index.

Technically, the logic is still correct. It does mean that index entries may not be generated as frequently as one expects, depending on the fetch size used in the follower fetcher thread and how far behind a follower is. This may impact consumer performance a bit.;;;","31/Oct/12 15:46;jkreps;Makes sense, clever. I thought a bit about this during the implementation and decided not to try to make the index entries exact just to reduce complexity. It would be possible to be more exact by having LogSegment calculate inter-messageset positions and append multiple entries to the index if necessary. This would not necessarily be a bad change, but there is definitely some complexity, especially in the face of truncation, so this would need pretty thorough testing.;;;","31/Oct/12 15:47;jkreps;Err, that should read ""intra-messageset positions"".;;;","02/Nov/12 02:48;yeyangever;
Basically the problem was during restarting the server and truncation from existing log and index files. 

When after the restart or truncation the existing index file is empty (so it's always full), one of the conditions of maybeRoll() will be true, so a new log segment will be rolled ---- we will end up with two log segments starting from the same offset, one of them is empty.

To fix it, we change the function trimToSize() in OffsetIndex to trimOrReallocate(), it does either trimming or reallocating the offset index file (and memory mapping). At truncation or restart, it will do reallocation, so that enough space for offset index is allocated.

We also check existing log segments at roll() function, and throws exception if some segment exists with the same offset as the target offset. (This should not happen)

;;;","02/Nov/12 04:51;junrao;Thanks for the patch. A couple of comments:

1. Log.rollToOffset(): We just need to verify that the starting offset of the last segment doesn't equal to the new offset.

2. OffsetIndex: When loading the log segment on broker startup, it is possible to have a segment with empty data and empty index. So, we will hit the same issue of rolling a segment with a duplicated name. We probably should extend the index size to max index size in the constructor of OffsetIndex.;;;","02/Nov/12 15:50;jkreps;trimOrReallocate(isReallocate: Boolean)  is a bit of a hacky interface. This supports resizing to two sizes: entries or maxSize, but it would be better to just implement the more general
  resize(numEntries: Int)
numEntries is the mmap.limit/8. It might be nice to leave the helper method trimToSize as a less error prone alias
  def trimToSize() = resize(entries)
If we do this we should be able to use resize to implement OffsetIndex.truncateTo (the difference between truncateTo and resize is that truncateTo is in terms of offset whereas resize is in terms of entries).

We should also add a test case to cover these corner cases.

;;;","02/Nov/12 23:41;yeyangever;
Jay, Jun,

Thanks for the comments.

Jun also pointed out that there may still be cases where at startup, the last log segment has empty index and empty log file ---- and the trimOrReallocate() is not called because it was a clean shutdown before. 

What about an alternative idea, which is, whenever we load a log segment, we always make sure its offset index has enough disk space and memory. In this case, when we truncate back to old segment, its index will not be full, when we start the last log segment with empty log file and index file, its index will also not be full. 

To do this, we just need to change the constructor of OffsetIndex, by always set the index file size and mmap limit to maxIndexSize.
;;;","03/Nov/12 01:02;yeyangever;

Thanks for the discussions, and here's the third patch which is also verified to be working.;;;","05/Nov/12 17:54;junrao;Thanks for patch v3. Looks good. Some minor comments.

30. Log.rollToOffset():  segmentsView.last.index.file.getName.split(""\\."")(0).toLong can just be segmentsView.last.start.

31. Log.loadSegments(): Just to be consistent. Should we use index.maxIndexSize instead of maxIndexSize in the following statement?
      logSegments.get(logSegments.size() - 1).index.resetSizeTo(maxIndexSize)
;;;","05/Nov/12 19:40;yeyangever;
For 30,
It's fixed

For 31,
It seems simpler to keep as it is


Jay, can you help have a look at this patch?;;;","05/Nov/12 19:53;jkreps;Looks good, two minor things:
1. Can we name resetSize to resize?
2. Can we change the argument to be in terms of number of entries rather than number of bytes? It is incorrect to set to a number of bytes that is not a multiple of the entry size and the entry size is kind of an implementation detail of that class so this would be nicer.
3. Can we add a test for this case?;;;","05/Nov/12 21:54;jkreps;Discussed with Victor. (1) and (3) should be doable, but (2) is not very convenient because the usage actually resizes to indexMaxSize which is in terms of bytes. So our solution was to have the API take bytes and just round to a multiple of 8.;;;","06/Nov/12 00:08;yeyangever;
Thanks for the comments, changes in v5:

1. rename resetSize() to resize()
2. add using roundToMultiple() in resize()
3. add comments to resize()
4. add one unit test ""testIndexResizingAtTruncation"" in LogTest for this case;;;","06/Nov/12 00:20;jkreps;+1;;;","06/Nov/12 03:44;nehanarkhede;+1 on v5. Committed this patch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Register metrics beans at kafka server startup ,KAFKA-592,12613969,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,29/Oct/12 23:23,30/Oct/12 01:37,14/Jul/23 05:39,30/Oct/12 01:37,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,,,"jmx beans are not registered until the corresponding part of the code executes. To set alerts on some of the server side beans, they need to be registered at server startup.",,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/12 23:51;swapnilghike;kafka-592-v1.patch;https://issues.apache.org/jira/secure/attachment/12551282/kafka-592-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,252896,,,Tue Oct 30 01:37:50 UTC 2012,,,,,,,,,,"0|i0da53:",75572,,,,,,,,,,,,,,,,,,,,"29/Oct/12 23:51;swapnilghike;Change made to register AllTopicsProduceFailurePerSec, AllTopicsFetchFailurePerSec, OfflinePartitionsPerSec and UncleanLeaderElectionsPerSec in server.startup()

UnderReplicatedPartitionCount is already registered in startup with the creation of replicaManager. 

BrokerTopicStat.getBrokerAllTopicStat() also registers three other beans - AllTopicsMessagesInPerSec, AllTopicsBytesInPerSec and AllTopicBytesOutPerSec. But that's probably ok since these beans would be created anyways in most cases.;;;","30/Oct/12 01:37;junrao;Thanks for the patch. +1. Committed to 0.8 by making registerStats private and adding a comment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add test cases to test log size retention and more,KAFKA-591,12613953,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jfung,jfung,jfung,29/Oct/12 21:05,09/Nov/12 22:58,14/Jul/23 05:39,09/Nov/12 22:58,,,,0.8.0,,,,,,,,,,0,,,,"Add test cases to test the followings:

1. Log Size Retention

2. Replica Factor < no. of brokers in a cluster

3. Multiple instances of Migration Tool

4. Multiple instances of Mirror Maker

5. Set ""log.index.interval.bytes"" to be slightly smaller than message size to force the indexing to be performed for each message",,jfung,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/12 00:32;jfung;kafka-591-v1.patch;https://issues.apache.org/jira/secure/attachment/12552576/kafka-591-v1.patch","09/Nov/12 18:24;jfung;kafka-591-v2.patch;https://issues.apache.org/jira/secure/attachment/12552866/kafka-591-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,252879,,,Fri Nov 09 22:58:54 UTC 2012,,,,,,,,,,"0|i0d9zz:",75549,,,,,,,,,,,,,,,,,,,,"05/Nov/12 18:36;jfung;Uploaded kafka-591-v1.patch with the following changes:

1. Added Log Size Retention (cases 4001 ~ 4008, 4011 ~ 4018)
2. Replica Factor < no. of brokers in a cluster (cases 4011 ~ 4018)
3. Multiple instances of Migration Tool (cases 9003 ~ 9006)
4. Multiple instances of Mirror Maker (cases 5003 ~ 5006)
5. Set ""log.index.interval.bytes"" to be slightly smaller than message size to force the indexing to be performed for each message (cases 0124 ~ 0127);;;","09/Nov/12 07:00;jfung;Uploaded kafka-591-v2.patch with the following changes:

1. Merged log_retention_testsuite into replication_testsuite
2. Fixed the approach to validate the message count by SimpleConsumerShell when the replication factor is less than no. of brokers;;;","09/Nov/12 22:58;junrao;Thanks for patch v2. +1 Committed to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System Test - 4 cases failed due to insufficient no. of retry in ProducerPerformance,KAFKA-590,12613785,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jfung,jfung,27/Oct/12 23:31,31/Oct/12 21:50,14/Jul/23 05:39,31/Oct/12 21:49,,,,,,,,,,,,,,0,,,,"1. Functional Test Area : Replication with Leader Hard Failure (1 Topic, 3 Partitions)

2. Testcases failed : 

0151 (Sync Producer, Acks = -1, No Compression)
0152 (Async Producer, Acks = -1, No Compression)
0155 (Sync Producer, Acks = -1, Compressed)
0156 (Async Producer, Acks = -1, Compressed)

3. Sample test results :

2012-10-25 18:22:20,206 - INFO - ======================================================
2012-10-25 18:22:20,206 - INFO - validating data matched
2012-10-25 18:22:20,206 - INFO - ======================================================
2012-10-25 18:22:20,206 - DEBUG - request-num-acks [-1] (kafka_system_test_utils)
2012-10-25 18:22:20,228 - INFO - no. of unique messages on topic [test_1] sent from publisher  : 900 (kafka_system_test_utils)
2012-10-25 18:22:20,235 - INFO - no. of unique messages on topic [test_1] at simple_consumer_1.log : 853 (kafka_system_test_utils)
2012-10-25 18:22:20,242 - INFO - no. of unique messages on topic [test_1] at simple_consumer_2.log : 853 (kafka_system_test_utils)
2012-10-25 18:22:20,247 - INFO - no. of unique messages on topic [test_1] at simple_consumer_3.log : 853 (kafka_system_test_utils)

4. Investigations :

a. Merge log segment files per partition:
Under test_1351181987/testcase_0151/logs/broker-1/kafka_server_1_logs:
cat test_1-0/00000000000000000000.log >> merged_test_1_0/00000000000000000000.log
cat test_1-0/00000000000000000197.log >> merged_test_1_0/00000000000000000000.log
. . .

b. Retrieve all CRC from merged data log segment:
bin/kafka-run-class.sh kafka.tools.DumpLogSegments merged_test_1_0/00000000000000000000.log | grep crc | sed 's/.* crc: //' | sort -u > test_1_0_crc.log
. . .

c. Merge the CRC files together:
cat test_1_0_crc.log >> all_crc.log
cat test_1_1_crc.log >> all_crc.log
cat test_1_2_crc.log >> all_crc.log

d. Sort the merged CRC file:
cat all_crc.log | sort -u > all_crc_sorted.log

e. Get the no. of 'failed to send' CRC in producer_performance.log (70 in this case):
grep 'failed to send' producer_performance.log | sed 's/.* crc = //' | sed 's/, key = null.*//' | sort -u | wc -l
70

f. Match those 'failed to send' CRC from producer_performance.log to see how many messages eventually got retried to send successfully:

$ for i in `grep 'failed to send' ../../producer_performance-4/producer_performance.log | sed 's/.* crc = //' | sed 's/, key = null.*//' | sort -u`; do echo -n ""$i => ""; grep $i all_crc_sorted.log || echo ""n/a""; done;
. . .
1302684126 => n/a
1456125554 => 1456125554
15299643 => n/a
1653550869 => 1653550869
1741661084 => n/a
1764395211 => 1764395211
. . .
(23 msgs are sent successfully in retry)

g. As a result, (70 messages 'failed to send' in producer_performance.log - 23 messages successfully sent in retry) = 47 messages are lost (which matches the data loss count in the test result)

Therefore, if the no. of retry is increased to a higher value, all the messages could be sent successfully.
",,jfung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,252548,,,Wed Oct 31 21:49:43 UTC 2012,,,,,,,,,,"0|i0cupz:",72921,,,,,,,,,,,,,,,,,,,,"31/Oct/12 21:49;jfung;These 4 test cases are passing after setting ""producer-retry-backoff-ms"" to 2500 which is supported by ProducerPerformance (in KAFKA-267).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean shutdown after startup connection failure,KAFKA-589,12613686,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,ewencp,jbrosenberg,jbrosenberg,26/Oct/12 17:24,26/Sep/14 04:24,14/Jul/23 05:39,26/Sep/14 04:23,0.7.2,0.8.0,,,,,,,,,core,,,0,bugs,newbie,,"Hi,

I'm embedding the kafka server (0.7.2) in an application container.   I've noticed that if I try to start the server without zookeeper being available, by default it gets a zk connection timeout after 6 seconds, and then throws an Exception out of KafkaServer.startup()....E.g., I see this stack trace:

Exception in thread ""main"" org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server within timeout: 6000
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.server.KafkaZooKeeper.startup(KafkaZooKeeper.scala:44)
	at kafka.log.LogManager.<init>(LogManager.scala:93)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:58)
        ....
        ....

So that's ok, I can catch the exception, and then shut everything down gracefully, in this case.  However, when I do this, it seems there is a daemon thread still around, which doesn't quit, and so the server never actually exits the jvm.  Specifically, this thread seems to hang around:

""kafka-logcleaner-0"" prio=5 tid=7fd9b48b1000 nid=0x112c08000 waiting on condition [112c07000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <7f40d4be8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:196)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2025)
	at java.util.concurrent.DelayQueue.take(DelayQueue.java:164)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:609)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:602)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:680)

Looking at the code in kafka.log.LogManager(), it does seem like it starts up the scheduler to clean logs, before then trying to connect to zk (and in this case fail):

  /* Schedule the cleanup task to delete old logs */
  if(scheduler != null) {
    info(""starting log cleaner every "" + logCleanupIntervalMs + "" ms"")    
    scheduler.scheduleWithRate(cleanupLogs, 60 * 1000, logCleanupIntervalMs)
  }

So this scheduler does not appear to be stopped if startup fails.  However, if I catch the above RuntimeException, and then call KafkaServer.shutdown(), then it will stop the scheduler, and all is good.

However, it seems odd that if I get an exception when calling KafkaServer.startup(), that I should still have to do a KafkaServer.shutdown().  Rather, wouldn't it be better to have it internally cleanup after itself if startup() gets an exception?  I'm not sure I can reliably call shutdown() after a failed startup()....",,ewencp,jbrosenberg,jbrosenberg@gmail.com,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/14 23:54;ewencp;KAFKA-589-v1.patch;https://issues.apache.org/jira/secure/attachment/12671345/KAFKA-589-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,251638,,,Fri Sep 26 04:23:45 UTC 2014,,,,,,,,,,"0|i0c2yn:",68424,,,,,,,,,,,,,,,,,,,,"29/Oct/12 02:47;junrao;This problem exists in 0.8 too. What we need to do is to add a try/catch in KafkaServer.start() and call shutdown if we hit any exceptions.;;;","29/Oct/12 21:21;swapnilghike;Hi Jason, 

Are you using the KafkaServer.startup() or KafkaServerStartable.startup()? The latter calls the former and also shuts the server down in case of an exception.;;;","05/Nov/12 20:32;jbrosenberg;I was using KafkaServerStartable.startup(), but switched to KafkaServer.startup(), because I wanted to have a bit more control of things, e.g. I want to be able know if there was a problem within the container, and retry, etc.  In KafkaServerStartable.startup(), if there's an exception, it swallows the exception, and then calls shutdown(), but the caller has no idea if the startup was successful or not.

But I don't think that's relevant here.  I think it's counter intuitive that the KafkaServer.startup() would fail to startup, and throw an exception, and then not cleanup after itself, and require a call to shutdown in the first place.;;;","25/Sep/14 23:52;ewencp;This patch makes KafkaServer clean up after itself, but still rethrow any caught exceptions. This keeps the existing interface the same, should still work if the caller does cleanup themselves by catching exceptions and calling shutdown, but also cleans up if they don't so the leftover thread won't cause a hang. Also adds a test of this behavior.;;;","26/Sep/14 04:23;nehanarkhede;Thanks for fixing a longstanding bug! +1 on the patch.;;;","26/Sep/14 04:23;nehanarkhede;Pushed to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index truncation doesn't seem to remove the last entry properly,KAFKA-588,12613674,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jkreps,junrao,junrao,26/Oct/12 16:00,04/Dec/12 23:45,14/Jul/23 05:39,15/Nov/12 03:56,0.8.0,,,,,,,,,,core,,,0,bugs,,,"[2012-10-26 08:04:13,333] INFO [Kafka Log on Broker 3], Truncated log segment /tmp/kafka_server_3_logs/test_1-0/00000000000000130500.log to target offset 429050 (kafka.log.
Log)
[2012-10-26 08:04:13,333] INFO [ReplicaFetcherManager on broker 3] adding fetcher on topic test_1, partion 0, initOffset 429050 to broker 2 with fetcherId 0 (kafka.server.R
eplicaFetcherManager)
[2012-10-26 08:04:13,335] INFO Replica Manager on Broker 3: Handling leader and isr request LeaderAndIsrRequest(1,,1000,Map((test_1,1) -> PartitionStateInfo({ ""ISR"":""2,3"",""leader"":""2"",""leaderEpoch"":""2"" },3), (test_1,0) -> PartitionStateInfo({ ""ISR"":""2,3"",""leader"":""2"",""leaderEpoch"":""2"" },3))) (kafka.server.ReplicaManager)
[2012-10-26 08:04:13,335] INFO Replica Manager on Broker 3: Starting the follower state transition to follow leader 2 for topic test_1 partition 1 (kafka.server.ReplicaManager)
[2012-10-26 08:04:13,335] INFO Partition [test_1, 1] on broker 3: Current leader epoch [2] is larger or equal to the requested leader epoch [2], discard the become follower request (kafka.cluster.Partition)
[2012-10-26 08:04:13,336] INFO Replica Manager on Broker 3: Starting the follower state transition to follow leader 2 for topic test_1 partition 0 (kafka.server.ReplicaManager)
[2012-10-26 08:04:13,336] INFO Partition [test_1, 0] on broker 3: Current leader epoch [2] is larger or equal to the requested leader epoch [2], discard the become follower request (kafka.cluster.Partition)
[2012-10-26 08:04:13,588] ERROR [ReplicaFetcherThread-2-0-on-broker-3], Error due to  (kafka.server.ReplicaFetcherThread)
java.lang.IllegalArgumentException: Attempt to append an offset (429050) no larger than the last offset appended (429050).
        at kafka.log.OffsetIndex.append(OffsetIndex.scala:180)
        at kafka.log.LogSegment.append(LogSegment.scala:56)
        at kafka.log.Log.append(Log.scala:273)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:51)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:116)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:99)
        at scala.collection.immutable.Map$Map2.foreach(Map.scala:127)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:99)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)
",,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/12 18:35;jkreps;KAFKA-588-v2.patch;https://issues.apache.org/jira/secure/attachment/12551400/KAFKA-588-v2.patch","29/Oct/12 22:58;jkreps;KAFKA-588.patch;https://issues.apache.org/jira/secure/attachment/12551267/KAFKA-588.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,251276,,,Thu Nov 15 03:56:12 UTC 2012,,,,,,,,,,"0|i0bc8v:",64096,,,,,,,,,,,,,,,,,,,,"26/Oct/12 16:03;junrao;From the log, the broker wants to truncate log to offset 429050. The last entry in the log is the following and looks correct.
offset: 429049 position: 155842578 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 3220325748

However, the last index entry is the following, which is off by 1.
offset: 429050 position: 155843100
;;;","29/Oct/12 19:50;jkreps;Jun the OffsetIndex.truncateTo logic looks right to me and we have a number of tests on this so I am wondering if that is the problem. How was this broker shut down (or was it shutdown at all?). I ask because the DumpSegment tool just dumps what is on disk. In the case of the OffsetIndex the end of file pointer is just in memory and truncation just moves that pointer. I wonder if  the truncate wasn't correct but somehow the lastOffset wasn't set correctly and as a result we are giving the error incorrectly. If the broker wen through clean shutdown this should not be possible, but if not it is possible.

One bug I think I see is that we initialize the lastOffset to be the baseOffset of the segment, but this means that if the first message is entered into the index with that same baseOffset (which is legitimate) it would produce an error. Normally I think we don't hit this just because our index interval isn't 1, however after a truncate we don't reset the bytesSinceLastIndexEntry so this becomes possible even with a larger index interval. Do you know how many entries were in that index file that you checked? I am pretty sure this is a bug, but am not sure if it caused this error (it could cause it though).;;;","29/Oct/12 22:58;jkreps;I can't reproduce this issue, but this patch expands the tests and adds a little more logging. Specifically:
1. Expanded OffsetIndex to check we can still append after each truncate
2. Expanded LogSegmentTest to sequentially append 2 messages, truncate one of them off, and repeat this 30 times.
3. Removed obsolete references to ""logical offset"" (we changed terminology--now offsets are always logical and positions are always physical so this makes no sense.

Here is my reasoning: this problem doesn't occur every time so it must be triggered by something non-deterministic. The possibilities I can think of:
1. The OffsetIndex is not properly synchronized
2. The truncate call is not properly setting the lastOffset and so the error is spurious.
3. The truncate call has an off-by-one error.
4. We are shutting down uncleanly and the recovery process is somehow not kicking in or not working correctly.

Here is how I looked into these:
1. It does not look like a synchronization problem since both truncateTo and append are synchronized on the same lock.
2. I suspected a problem in setting the lastOffset, but actually I am handling this case--the check already special cases an empty index. So this is not it.
3. This does not appear to be the case, truncateTo(X) results in an index in which the last entry is <= X-1 which is what we want, we have  a number of tests on this.
4. I don't see this, we are going through the same append path in recovery we go through normally.;;;","30/Oct/12 00:56;junrao;Thanks for the patch. In LogSegmentTest, could we set indexIntervalSize such that we force an index entry to be created for every message?;;;","30/Oct/12 18:18;jkreps;Okay, I fooled around a little more and figured it out.

Here is the issue. In OffsetIndex.truncateTo we have the following code:
      val newEntries = 
        if(slot < 0)
          0
        else if(logical(idx, slot) == offset)
          slot
        else
          slot + 1

This code looks right due to the bad naming. The arithmetic is actually correct, but the comparison is wrong as logical() returns the relative offset which we compare to the global offset. The fixed code looks like this:
      val newEntries = 
        if(slot < 0)
          0
        else if(relativeOffset(idx, slot) == offset - baseOffset)
          slot
        else
          slot + 1
;;;","30/Oct/12 18:35;jkreps;Same as above patch, but actually has a fix for the issue plus tests that reproduce it.;;;","30/Oct/12 19:54;nehanarkhede;+1 on v2, good catch !;;;","15/Nov/12 03:56;jkreps;Fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system test configs are broken,KAFKA-586,12613532,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,jfung,nehanarkhede,nehanarkhede,25/Oct/12 18:22,31/Oct/12 15:26,14/Jul/23 05:39,31/Oct/12 15:26,0.8.0,,,0.8.0,,,,,,,,,,0,replication-testing,,,"system test suite has a set of default config values that are picked up from the testsuite/config directory. One can override the value of a config in the testcase_properties.json file. This is great, but the assumption is that the config property that is being overridden should also present in the testsuite/config/*.properties file. 

Currently, there are a number of properties in KafkaConfig that are not in the testsuite/config/*.properties file. So the tests might intend to override some properties, but that will be ignored. 

Let's either add all the configs in the testsuite/config/*.properties file or remove this depedency and override the property specified in testcase_properties.json.",,jfung,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/12 18:44;jfung;kafka-586-v1.patch;https://issues.apache.org/jira/secure/attachment/12551225/kafka-586-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,251078,,,Wed Oct 31 15:26:31 UTC 2012,,,,,,,,,,"0|i0b40n:",62763,,,,,,,,,,,,,,,,,,,,"26/Oct/12 04:32;jfung;Hi Neha,

In the structure of the testcase_xxxx_properties.json file as shown below, the attributes in each map are mixed with System Test properties and Broker properties:

    {
      ""entity_id"": ""1"",                << System Test property
      ""port"": ""9091"",                  << Broker property
      ""brokerid"": ""1"",                 << System Test property
      ""replica.fetch.min.bytes"": ""1"",
      ""log.file.size"": ""102400"",
      ""log.dir"": ""/tmp/kafka_server_1_logs"",
      ""log_filename"": ""kafka_server_9091.log"",
      ""config_filename"": ""kafka_server_9091.properties""
    },

Since the System Test script cannot tell which property belongs to Broker, it will only match those existing properties from template and update with overridden values to the new broker properties file.

At the mean time, the best solution is to update the template system_test/xxxx_testsuite/config/server.properties with the new properties and the test script will update them properly.;;;","29/Oct/12 18:48;jfung;Uploaded kafka-586-v1.patch which contains the latest properties supported by Kafka broker.;;;","31/Oct/12 15:26;junrao;Thanks for the patch. +1. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove custom metrics jar and replace with latest from metrics HEAD,KAFKA-585,12613421,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jjkoshy,jjkoshy,25/Oct/12 01:25,14/Nov/12 18:58,14/Jul/23 05:39,14/Nov/12 18:58,0.8.0,,,,,,,,,,,,,1,,,,"This is for at least until metrics 3.x is mavenized.

Also:

The KafkaCSVMetricsReporter object may be better named as KafkaMetricsReporter since startCSVMetricsReporter
potentially starts up other (non-CSV) reporters (if any) as well - in which case KafkaMetricsReporter.scala would be a
better place for it. Or, you can just filter out non-CSV reporters.

Also, the top-level/config/server.properties need not enable the csv reporter. I thought the system test replication
suite's server.properties would need to be patched, but it isn't. Should look into whether the test suite picks up the top-level
config as a template.",,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/12 22:29;jjkoshy;KAFKA-585-v1.patch;https://issues.apache.org/jira/secure/attachment/12552350/KAFKA-585-v1.patch","06/Nov/12 22:29;jjkoshy;metrics-annotation-3.0.0-c0c8be71.jar;https://issues.apache.org/jira/secure/attachment/12552351/metrics-annotation-3.0.0-c0c8be71.jar","06/Nov/12 22:29;jjkoshy;metrics-core-3.0.0-c0c8be71.jar;https://issues.apache.org/jira/secure/attachment/12552352/metrics-core-3.0.0-c0c8be71.jar",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,250924,,,Wed Nov 14 18:58:18 UTC 2012,,,,,,,,,,"0|i0b2hz:",62517,,,,,,,,,,,,,,,,,,,,"06/Nov/12 22:31;jjkoshy;The CSV reporter basically does not work if the file already exists - i.e., it doesn't try to open it in append mode. I haven't heard back from metrics-user list on that, but I think it is sufficient for now to just delete the CSV dir when starting up.;;;","12/Nov/12 18:06;junrao;Thanks for patch v1. +1. Just make sure that the basic system tests still work before checking in.;;;","14/Nov/12 18:58;jjkoshy;Checked-in to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
produce/fetch remote time metric not set correctly when num.acks = 1,KAFKA-584,12613415,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,nehanarkhede,nehanarkhede,25/Oct/12 00:43,25/Oct/12 18:16,14/Jul/23 05:39,25/Oct/12 18:16,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,,,"When num.acks = 1, the produce/fetch remote time is set to a very high value (several hours). This is due to a race condition on the apiLocalTime, which is initialized to -1, that makes the (responseTime - apiLocalTime) a very large value.
",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/12 16:27;junrao;kafka-584.patch;https://issues.apache.org/jira/secure/attachment/12550794/kafka-584.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,250918,,,Thu Oct 25 18:16:42 UTC 2012,,,,,,,,,,"0|i0b2gn:",62511,,,,,,,,,,,,,,,,,,,,"25/Oct/12 16:27;junrao;Attach a patch. The problem is that with ack = 1, response could be sent before apilocaltime is updated. Fixed it by setting apilocatime to responsetime, if not set. Also, since those times are updated in different threads, make them volatile so that updates are exposed to other threads. Now, remote times are 0 with ack = 1.;;;","25/Oct/12 16:29;junrao;Also, volatile makes a long value atomic according to the following article, which is what we want.
http://gee.cs.oswego.edu/dl/cpj/jmm.html;;;","25/Oct/12 18:01;nehanarkhede;+1, but should we get rid of the info statement ?;;;","25/Oct/12 18:16;junrao;Thanks for the review. Changed the logging to trace level and committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system test testcase_0122 under replication fails due to large # of data loss,KAFKA-580,12613157,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,junrao,junrao,23/Oct/12 15:21,23/Oct/12 17:26,14/Jul/23 05:39,23/Oct/12 17:26,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,,,"testcase_0122 fails sometimes because a large # of messages is lost with ack = 1. In this case, we expect only a small number of messages to be lost when there are broker failures.",,jfung,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/12 15:24;junrao;kafka-580.patch;https://issues.apache.org/jira/secure/attachment/12550476/kafka-580.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,250588,,,Tue Oct 23 17:26:15 UTC 2012,,,,,,,,,,"0|i0azhj:",62027,,,,,,,,,,,,,,,,,,,,"23/Oct/12 15:24;junrao;Attach a patch. The problem is that if we remove items from a set while iterating it, the behavior is not deterministic.Also fixed kafka-578 in the patch since it's touching the same code.;;;","23/Oct/12 17:14;nehanarkhede;+1;;;","23/Oct/12 17:23;jfung;Thanks Jun for the patch.

The patch is applied to the latest 0.8 branch and testcase_0122 PASSES as shown below. There are data from 2 topics. Topic ""test_1"" are all matching and topic ""test_2"" has minor data loss. Please note that we have a new policy of 1% data loss tolerance for those test cases with leader failure and acks == 1.

_test_case_name  :  testcase_0122
_test_class_name  :  ReplicaBasicTest
arg : bounce_broker  :  true
arg : broker_type  :  leader
arg : message_producing_free_time_sec  :  15
arg : num_iteration  :  3
arg : num_partition  :  3
arg : replica_factor  :  3
arg : sleep_seconds_between_producer_calls  :  1
validation_status  :
     Leader Election Latency - iter 1 brokerid 1  :  375.00 ms
     Leader Election Latency - iter 2 brokerid 2  :  324.00 ms
     Leader Election Latency - iter 3 brokerid 3  :  326.00 ms
     Leader Election Latency MAX  :  375.00
     Leader Election Latency MIN  :  324.00
     Unique messages from consumer on [test_1] at simple_consumer_1.log  :  3900
     Unique messages from consumer on [test_1] at simple_consumer_2.log  :  3900
     Unique messages from consumer on [test_1] at simple_consumer_3.log  :  3900
     Unique messages from consumer on [test_2] at simple_consumer_1.log  :  3893
     Unique messages from consumer on [test_2] at simple_consumer_2.log  :  3893
     Unique messages from consumer on [test_2] at simple_consumer_3.log  :  3893
     Unique messages from producer on [test_1]  :  3900
     Unique messages from producer on [test_2]  :  3900
     Validate for data matched on topic [test_1]  :  PASSED
     Validate for data matched on topic [test_2]  :  PASSED
     Validate for merged log segment checksum in cluster [source]  :  PASSED
     Validate leader election successful  :  PASSED;;;","23/Oct/12 17:26;junrao;Thanks for the review. Committed to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove connection timeout in SyncProducer,KAFKA-579,12612811,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,junrao,junrao,20/Oct/12 01:52,24/Oct/12 17:21,14/Jul/23 05:39,24/Oct/12 16:48,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,newbie,,"Currently, SyncProducer has a few parameters that control how long the client should wait to establish a socket connection and how frequent the connection should be re-established. Those parameters seem to be needed primarily for vip on a load balancer. In 0.8, SyncProducer doesn't deal with VIP any more. So, we probably should get rid of those parameters. One of the issues that I have seen is that when a broker is down, SyncProducer will still wait connectionTimeout time to try to establish a connection. This is unnecessary since the high level producer already has the retry logic and is delaying requests like getMetadataRequest.
",,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/12 01:20;swapnilghike;kafka-579-v1.patch;https://issues.apache.org/jira/secure/attachment/12550573/kafka-579-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,250075,,,Wed Oct 24 17:21:44 UTC 2012,,,,,,,,,,"0|i0ao8f:",60199,,,,,,,,,,,,,,,,,,,,"24/Oct/12 01:20;swapnilghike;1. Modified syncProducer.connect() to not backoff and directly return the exception to the caller.

2. Removed forced reconnect() from syncProducer.doSend() since that was probably useful only in case of load balancing with VIP.

3. Removed connectTimeoutMs, reconnectInterval and reconnectTimeInterval from SyncProducerConfig. 

4. Unrelated: Removed numRetries from ProducerConfig and preferredReplicaWaitTime from KafkaConfig, since they were not being used. ;;;","24/Oct/12 16:48;junrao;Thanks for the patch. +1. Committed to 0.8 after removing an unused variable lastConnectionTime in SyncProducer.;;;","24/Oct/12 16:55;nehanarkhede;I guess it is still useful to have a VIP for the topic metadata request. In that case, what happens if a low throughput producer sends a topic metadata request over an idle VIP connection ?;;;","24/Oct/12 17:05;junrao;Very good point. To address this issue, we have created ClientUtils.fetchTopicMetadata which closes the underlying socket after each getMetadata request is processed. Both producer and consumer use this util to get metadata. Since getMetadata is used infrequently, creating a socket per request is not too much overhead.;;;","24/Oct/12 17:21;nehanarkhede;I see, missed that. Sounds good :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leader finder thread in ConsumerFetcherManager needs to handle exceptions,KAFKA-578,12612447,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,junrao,junrao,18/Oct/12 16:24,23/Oct/12 22:15,14/Jul/23 05:39,23/Oct/12 22:15,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,newbie,,"Saw the leader finder thread due to the following exception. We need to add a try/catch clause to handle the exceptions.

[2012-10-11 17:17:14,192] ERROR [mm_regtest_grp_jrao-ld-1350000983383-47e0e557-leader-finder-thread], Error due to  (kafka.consumer.ConsumerFetcherManager$$anon$1)
kafka.common.KafkaException: fetching topic metadata for topics [Set(test_1)] from broker [ArrayBuffer()] failed
        at kafka.utils.Utils$.getTopicMetadata(Utils.scala:704)
        at kafka.consumer.ConsumerFetcherManager$$anon$1.doWork(ConsumerFetcherManager.scala:55)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)
",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,249593,,,Tue Oct 23 22:15:42 UTC 2012,,,,,,,,,,"0|i0afrr:",58828,,,,,,,,,,,,,,,,,,,,"23/Oct/12 22:15;junrao;Fixed as kafka-580.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
extend DumpLogSegments to verify consistency btw data and index,KAFKA-577,12612446,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,junrao,junrao,18/Oct/12 16:18,05/Nov/12 23:39,14/Jul/23 05:39,05/Nov/12 23:39,0.8.0,,,0.8.0,,,,,,,core,,,0,newbie,tools,,"It would be good to extend DumpLogSegments to do the following verification:
1. The offsets stored in the index match those in the log data.
2. The offsets in the data log is consecutive.",,junrao,nehanarkhede,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/12 04:55;yeyangever;kafka_577_v1.diff;https://issues.apache.org/jira/secure/attachment/12550926/kafka_577_v1.diff","29/Oct/12 21:15;yeyangever;kafka_577_v2.diff;https://issues.apache.org/jira/secure/attachment/12551249/kafka_577_v2.diff","01/Nov/12 09:54;yeyangever;kafka_577_v3.diff;https://issues.apache.org/jira/secure/attachment/12551692/kafka_577_v3.diff","02/Nov/12 02:39;yeyangever;kafka_577_v4.diff;https://issues.apache.org/jira/secure/attachment/12551811/kafka_577_v4.diff","03/Nov/12 00:06;yeyangever;kafka_577_v5.diff;https://issues.apache.org/jira/secure/attachment/12551949/kafka_577_v5.diff","05/Nov/12 21:31;yeyangever;kafka_577_v6.diff;https://issues.apache.org/jira/secure/attachment/12552165/kafka_577_v6.diff",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,249592,,,Mon Nov 05 23:39:12 UTC 2012,,,,,,,,,,"0|i0afrj:",58827,,,,,,,,,,,,,,,,,,,,"27/Oct/12 00:31;junrao;Thanks for the patch. Some comments:

1. Could you add a --verifyOnly option so that we only do the verification but not print out the content?
2. The text in the following statement is too verbose. It doesn't need to print the index file since it's provided in the command line. It can just say ""Index position %d doesn't match log position at offset %d"".
        System.err.println((""The offset in index file [%s] does not match the offset stored in "" +
                ""log file [%s], they're %d and %d separately"").format(entry.offset + index.baseOffset, messageAndOffset.offset))
3. Shouldn't we use %d instead of %l in the following line?
        System.err.println(""The offset in the data log file [%s] is not consecutive, [%l] follows [%l]"".format(file.getName, messageAndOffset.offset, lastOffset))
4. Could you add a shell script for DumpLogSegments in bin/ ?
;;;","29/Oct/12 21:15;yeyangever;
changed according to the comments;;;","30/Oct/12 00:25;junrao;Thanks for patch v2. Instead of exiting on first verification failure, it would be better if we dump the whole log and report all failed verification at the very end.;;;","01/Nov/12 09:54;yeyangever;
cache all verification failure and print out at the end, not verified by real data yet because cannot find proper data for that...., just check the logic of the code..;;;","02/Nov/12 02:39;yeyangever;
1. also ""do it in batch"" for dumpLog() function

2. use scala HashMap instead of Pool

3. Use LinkedList in the right way;;;","02/Nov/12 04:33;junrao;A couple of more comments for patch v4.

40. LinkedList is supposed to be used for manipulating links directly. We should just use List (immutable) instead. Also, instead of the following,

        misMatchesSeq = misMatchesSeq.:+((entry.offset + index.baseOffset, messageAndOffset.offset).asInstanceOf[(Int, Int)])

we should write

        misMatchesSeq :+= ((entry.offset + index.baseOffset, messageAndOffset.offset).asInstanceOf[(Int, Int)])
;;;","03/Nov/12 00:06;yeyangever;
Thanks for the comments, and the LinkedList is replaced with List now ;;;","04/Nov/12 21:55;nehanarkhede;Sorry for coming to this a little late. Patch v5 looks good, just one suggestion - Looks like DumpLogSegments is the only tool that doesn't quite follow the convention for argument parsing. It will be good to standardize on jopt parser since it also helps with a description for each option, a help command and convenient required argument checks. Just my 2c.;;;","05/Nov/12 19:07;yeyangever;@Neha sure, we can do that;;;","05/Nov/12 21:31;yeyangever;
Using jOptSimple for option parsing;;;","05/Nov/12 23:39;junrao;Thanks for patch v6. +1. Committed to 0.8 with a minor change: using ::= on a list instead of .:+=;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleConsumer throws UnsupportedOperationException: empty.head ,KAFKA-576,12612074,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yeyangever,jfung,jfung,16/Oct/12 17:54,24/Oct/12 14:21,14/Jul/23 05:39,24/Oct/12 14:21,,,,0.8.0,,,,,,,,,,0,,,,"* In this case, there are 15 log segment files in broker-1 data dir:

ls -l /tmp/kafka_server_1_logs/test_1-0/
total 240
-rw-r--r-- 1 jfung eng    16 Oct 16 10:41 00000000000000000000.index
-rw-r--r-- 1 jfung eng 10440 Oct 16 10:40 00000000000000000000.log
-rw-r--r-- 1 jfung eng     8 Oct 16 10:41 00000000000000000020.index
-rw-r--r-- 1 jfung eng 10440 Oct 16 10:40 00000000000000000020.log
. . .
-rw-r--r-- 1 jfung eng     8 Oct 16 10:41 00000000000000000280.index
-rw-r--r-- 1 jfung eng 10440 Oct 16 10:41 00000000000000000280.log


* The following are the dump log segment of the first log segment file

bin/kafka-run-class.sh kafka.tools.DumpLogSegments /tmp/kafka_server_1_logs/test_1-0/00000000000000000000.log 
Dumping /tmp/kafka_server_1_logs/test_1-0/00000000000000000000.log
Starting offset: 0
offset: 0 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 1663889063
offset: 1 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 2803454828
offset: 2 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 683347625
. . .
offset: 18 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 1892511043
offset: 19 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 601297044

* Output of SimpleConsumerShell:
. . .
next offset = 16
Topic:test_1:ThreadID:2:MessageID:0000000043:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
next offset = 17
Topic:test_1:ThreadID:3:MessageID:0000000063:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
next offset = 18
Topic:test_1:ThreadID:4:MessageID:0000000083:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
next offset = 19
Topic:test_1:ThreadID:0:MessageID:0000000003:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
next offset = 19
Topic:test_1:ThreadID:0:MessageID:0000000003:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
next offset = 19
Topic:test_1:ThreadID:0:MessageID:0000000003:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
next offset = 19
Topic:test_1:ThreadID:0:MessageID:0000000003:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 
. . .

* It appears that SimpleConsumerShell doesn't advance to the next log segment file

* It should probably block inside the while loop to prevent infinite looping",,jfung,junrao,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-571,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/12 00:38;yeyangever;kafka_576_v1.diff;https://issues.apache.org/jira/secure/attachment/12550565/kafka_576_v1.diff","22/Oct/12 18:57;jfung;kafka_server_9093.log.gz;https://issues.apache.org/jira/secure/attachment/12550331/kafka_server_9093.log.gz",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,249077,,,Wed Oct 24 14:21:35 UTC 2012,,,,,,,,,,"0|i0a4lz:",57020,,,,,,,,,,,,,,,,,,,,"19/Oct/12 19:25;jfung;This is a minor fix which is needed in kafka-571-v1.patch. Therefore, it is included in that patch.;;;","22/Oct/12 18:01;jfung;1. SimpleConsumerShell will receive data from 1 of the broker in a 3-broker cluster with this change:

$ svn diff core/src/main/scala/kafka/tools/SimpleConsumerShell.scala
Index: core/src/main/scala/kafka/tools/SimpleConsumerShell.scala
===================================================================
--- core/src/main/scala/kafka/tools/SimpleConsumerShell.scala	(revision 1400944)
+++ core/src/main/scala/kafka/tools/SimpleConsumerShell.scala	(working copy)
@@ -186,7 +186,7 @@
             var consumed = 0
             for(messageAndOffset <- messageSet) {
               try {
-                offset = messageAndOffset.offset
+                offset = messageAndOffset.nextOffset
                 if(printOffsets)
                   System.out.println(""next offset = "" + offset)
                 formatter.writeTo(messageAndOffset.message, System.out)


2. By printing out the producedOffset in SimpleConsumer and showing -1:

$ svn diff core/src/main/scala/kafka/consumer/SimpleConsumer.scala
Index: core/src/main/scala/kafka/consumer/SimpleConsumer.scala
===================================================================
--- core/src/main/scala/kafka/consumer/SimpleConsumer.scala	(revision 1400944)
+++ core/src/main/scala/kafka/consumer/SimpleConsumer.scala	(working copy)
@@ -50,6 +50,7 @@
       if (simpleConsumer != null)
         simpleConsumer.close()
     }
+    System.out.println(""====> producedOffset : "" + producedOffset)
     producedOffset
   }


3. Exception is thrown from SimpleConsumer.earliestOrLatestOffset( ):

$ bin/kafka-run-class.sh kafka.tools.SimpleConsumerShell --broker-list localhost:9093 --topic test_1 --partition 0 --replica 3

[2012-10-22 10:41:56,525] INFO Getting topic metatdata... (kafka.tools.SimpleConsumerShell$)
[2012-10-22 10:41:56,583] INFO Fetching metadata for topic Set(test_1) (kafka.client.ClientUtils$)
[2012-10-22 10:41:56,589] INFO Connected to localhost:9093 for producing (kafka.producer.SyncProducer)
[2012-10-22 10:41:56,751] INFO Disconnecting from localhost:9093 (kafka.producer.SyncProducer)
[2012-10-22 10:41:56,784] ERROR error in earliestOrLatestOffset()  (kafka.consumer.SimpleConsumer$)
java.lang.UnsupportedOperationException: empty.head
	at scala.collection.immutable.Vector.head(Vector.scala:162)
	at kafka.consumer.SimpleConsumer$.earliestOrLatestOffset(SimpleConsumer.scala:45)
	at kafka.tools.SimpleConsumerShell$.main(SimpleConsumerShell.scala:169)
	at kafka.tools.SimpleConsumerShell.main(SimpleConsumerShell.scala)
====> producedOffset : -1
[2012-10-22 10:41:56,786] INFO Starting simple consumer shell to partition [test_1, 0], replica [3], host and port: [127.0.0.1, 9093], from offset [-1] (kafka.tools.SimpleConsumerShell$)
;;;","22/Oct/12 18:57;jfung;Attached a server log4j messages file;;;","24/Oct/12 00:38;yeyangever;
1. add --no-wait-logend command line option

2. bug found, before in SimpleConsumer line 40:
      val request = if(isFromOrdinaryConsumer)
        OffsetRequest(immutable.Map(topicAndPartition -> PartitionOffsetRequestInfo(earliestOrLatest, 1)))
      else
        OffsetRequest(immutable.Map(topicAndPartition -> PartitionOffsetRequestInfo(earliestOrLatest, 1)), Request.DebuggingConsumerId)

In the else branch, we are intending to use another constructor, but it turned out if we used this way, it's the same constructor. By using ""new OffsetRequest(..."", we uses the constructor we want, as bellow:

      val request = if(isFromOrdinaryConsumer)
        OffsetRequest(immutable.Map(topicAndPartition -> PartitionOffsetRequestInfo(earliestOrLatest, 1)))
      else
        new OffsetRequest(immutable.Map(topicAndPartition -> PartitionOffsetRequestInfo(earliestOrLatest, 1)), Request.DebuggingConsumerId)

;;;","24/Oct/12 14:21;junrao;Thanks for the patch. Committed to 0.8 with the following minor changes:

1. Use nextOffset instead of offset +1 to advance the offset.
2. Removed unused imports and variables.
3. Break long lines into multiple lines.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Partition.makeFollower() reads broker info from ZK,KAFKA-575,12612062,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,junrao,junrao,16/Oct/12 16:18,30/Oct/12 01:29,14/Jul/23 05:39,30/Oct/12 01:29,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,,,"To follow a new leader, Partition.makeFollower() has to obtain the broker info of the new leader. Currently, it reads that info from ZK for every affected partition. This increases the time for a leader to truly available. 
",,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Oct/12 06:13;swapnilghike;kafka-575-v1.patch;https://issues.apache.org/jira/secure/attachment/12551055/kafka-575-v1.patch","29/Oct/12 20:27;swapnilghike;kafka-575-v2.patch;https://issues.apache.org/jira/secure/attachment/12551233/kafka-575-v2.patch","30/Oct/12 01:01;swapnilghike;kafka-575-v3-correct.patch;https://issues.apache.org/jira/secure/attachment/12551292/kafka-575-v3-correct.patch","30/Oct/12 00:45;swapnilghike;kafka-575-v3.patch;https://issues.apache.org/jira/secure/attachment/12551289/kafka-575-v3.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,249028,,,Tue Oct 30 01:29:02 UTC 2012,,,,,,,,,,"0|i0a3lz:",56858,,,,,,,,,,,,,,,,,,,,"16/Oct/12 16:19;junrao;One way to address this issue is to include the broker info in the leaderAndIsr request. This way, Partition.makeFollower() doesn't need to read from ZK any more.;;;","27/Oct/12 06:13;swapnilghike;1. Live broker info included in LeaderIsrRequest. 

ControllerBrokerRequestBatch.sendrequestsToBrokers() passes the live brokers list to LeaderIsrRequest. 

Thus Partition.makeFollower no longer needs to read the broker info from zookeeper. Only the controller reads this info from zookeeper with this patch.

2. Partition.makeLeaderOrFollower feels unnecessary because it reuses only a couple lines of code, and introduces an extra step in understanding the logic. 

Hence, got rid of makeLeaderOrFollower and copied the lock synchronization and discarding the incoming request part to makeLeader and makeFollower separately.;;;","29/Oct/12 15:23;junrao;Patch looks good. Just one comment:

1. ControllerBrokerRequestBatch.sendRequestsToBrokers(): Instead of including all live brokers in the leaderAndIsr request, we probably can just include brokers that are leaders in each LeaderAndIsr request.;;;","29/Oct/12 20:27;swapnilghike;Made the change.;;;","30/Oct/12 00:45;swapnilghike;Minor change + passed sanity test.;;;","30/Oct/12 01:01;swapnilghike;Sorry, please ignore the last patch. This one is the correct one.;;;","30/Oct/12 01:29;junrao;Thanks for patch v3. +1. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaController unnecessarily reads leaderAndIsr info from ZK,KAFKA-574,12612061,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,prashanth.menon,junrao,junrao,16/Oct/12 16:09,15/Nov/12 00:29,14/Jul/23 05:39,15/Nov/12 00:29,0.8.0,,,,,,,,,,core,,,0,bugs,,,"KafkaController calls updateLeaderAndIsrCache() in onBrokerFailure(). This is unnecessary since in onBrokerFailure(), we will make leader and isr change anyway so there is no need to first read that information from ZK. Latency is critical in onBrokerFailure() since it determines how quickly a leader can be made online.

Similarly, updateLeaderAndIsrCache() is called in onBrokerStartup() unnecessarily. In this case, the controller does not change the leader or the isr. It just needs to send the current leader and the isr info to the newly started broker. We already cache leader in the controller. Isr in theory could change any time by the leader. So, reading from ZK doesn't guarantee that we can get the latest isr anyway. Instead, we just need to get the isr last selected by the controller (which can be cached together with the leader in the controller). If the leader epoc in a broker is at or larger than the epoc in the leaderAndIsr request, the broker can just ignore it. Otherwise, the leader and the isr selected by the controller should be used. ",,jfung,junrao,nehanarkhede,prashanth.menon,,,,,,,,,,,,,,,,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/12 15:45;prashanth.menon;KAFKA-574-v1.patch;https://issues.apache.org/jira/secure/attachment/12551119/KAFKA-574-v1.patch","01/Nov/12 02:01;prashanth.menon;KAFKA-574-v2.patch;https://issues.apache.org/jira/secure/attachment/12551661/KAFKA-574-v2.patch","02/Nov/12 00:47;prashanth.menon;KAFKA-574-v3.patch;https://issues.apache.org/jira/secure/attachment/12551800/KAFKA-574-v3.patch","08/Nov/12 01:23;prashanth.menon;KAFKA-574-v4.patch;https://issues.apache.org/jira/secure/attachment/12552585/KAFKA-574-v4.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,249027,,,Thu Nov 15 00:29:47 UTC 2012,,,,,,,,,,"0|i0a3lr:",56857,,,,,,,,,,,,,,,,,,,,"21/Oct/12 20:06;prashanth.menon;Hey Jun, mind if I take this on?  Some big changes have been made to replication/controller and this seems like a compact enough issue to re-familiarize myself with those areas (in preparation for KAFKA-513) while getting back into the swing of things.;;;","21/Oct/12 21:02;junrao;Sure, Prashanth. Do you think that you can provide a patch in the next week or so? We start to looking into performance related issues and this one is on the critical path of leader elections.;;;","21/Oct/12 22:16;prashanth.menon;I should be able to provide a patch in the next week, more specifically towards the end of next week :);;;","26/Oct/12 02:24;prashanth.menon;Just an update on this.  I'm effectively dev complete.  I'll continue testing tomorrow, hoping to get a patch in soon.;;;","28/Oct/12 15:45;prashanth.menon;I've attached a v1 patch for this guy - it's a relatively small change.

KafkaController:
- Removed updateLeaderAndIsrCache from onBrokerStartup because the partition state machine will read ZK when issuing leader and isr requests.  It's also unncessary as there's no guarantee that the leader won't change between issuing the request and all brokers receiving it.  Since each broker's local partition checks leaderEpoch when following/leading, reading ZK in onBrokerStartup isn't necessary.
- Removed updateLeaderAndIsrCache from onBrokerFailure.  After bringing all partitions with dead leaders offline, triggering online partitions change will read ZK for each partition and therefore isn't necessary for all partitions here.  

No tests were added as this was effectively removing duplicate code.  Ensuring tests pass should be good enough.  Otherwise, there is some generic cleanup and small optimizations here and there.  Let me know what you think.  ;;;","29/Oct/12 15:48;junrao;Thanks for the patch. Some comments:

1. ReplicaStateMachine.handleStateChange():  There is one more optimization to consider. When handling the OnlineReplica case, we don't really need to read Isr from ZK and can read it from in-memory cache. To do that, we can extend ControllerContext.allLeaders to store LeaderAndIsr, instead of just the broker Id of the leader. This leaderAndIsr cache will be updated every time the controller makes a leader change.

2. 0.8 has moved since you uploaded the patch. Could you rebase?

;;;","01/Nov/12 02:01;prashanth.menon;I've attached a new patched rebased against 0.8 head which addressed Jun's points.

KafkaController:
- The allLeaders field in the controller context now maps TopicAndPartition to LeaderAndISR objects.

ReplicaSatemachine:
- The OnlineReplica state now reads the leader and ISR from cache.
- The OfflineReplica state also reads the leader and ISR from the controller cache when determining whether to shrink ISR.;;;","01/Nov/12 16:38;junrao;Thanks for patch v2. Just one more comment:

20. ReplicaSatemachine.handleStateChange(): In the OfflineReplica state, after the isr is updated, we need to update the leaderAndIsr cache in controller context. Also, is leaderAndIsrIsEmpty better than leaderAndIsrOpt?

Could you run the basic system tests and make sure that they pass?

<kafka_home>/system_test/ $ python –u –B system_test_runner.py 2>&1 | tee system_test_output_`date +%s`.log;;;","02/Nov/12 00:47;prashanth.menon;Thanks for the review, Jun.  I've attached a new patch.

20. Wow, I'm not sure how this slipped by me.  I've modified KafkaController.removeReplicaFromIsr to refresh the leader cache if it can successfully write back the leader and isr into zookeeper.  As for the second point, I'm not sure I understand?  Would you like to change the name of the variable?

Regarding the system tests, I'm unfortunately running a Mac which isn't supported by the system test suite.  I've got a Ubuntu VM sitting somewhere that I can use to run the system tests, but I'll need a little time to set it up (probably tomorrow).;;;","05/Nov/12 19:08;prashanth.menon;So I ran the system test an a Ubuntu box and two of the test cases fail consistently for me, both with and without the patch:

_test_case_name: test_case_0001
_test_clss_name: ReplicaBasicTest
arg : bounce_broker : false
arg : broker_type : leader
arg : message_producing_free_time_sec : 15
arg : num_iteration : 1
arg : num_messages_to_produce_per_product_call : 50
arg : num_partition : 1
arg : replica_factor : 3
arg: sleep_sseconds_between_producer_calls : 1
validation_status:
  Leader Election latency MAX : None
  Leader Election latency MIN : None
  Validate leader election successful : FAILED

_test_case_name: test_case_1
_test_clss_name: ReplicaBasicTest
arg : bounce_broker : true
arg : broker_type : leader
arg : message_producing_free_time_sec : 15
arg : num_iteration : 2
arg : num_messages_to_produce_per_product_call : 50
arg : num_partition : 2
arg : replica_factor : 3
arg: sleep_sseconds_between_producer_calls : 1
validation_status:
  Validate leader election successful : FAILED

Any idea if this is happening for everyone else?  I'll investigate on my end to see what's causing it.;;;","06/Nov/12 17:39;junrao;Thanks for patch v3. Just one more comment:

30. KafkaController.removeReplicaFromIsr(): Shouldn't we only update newLeaderAndIsr in the cache if updateSucceeded is true?

20. Ignore my comment on leaderAndIsrIsEmpty. It is fine.

I ran system tests with your patch and they seem to pass. Did you build the Kafka jar before running the test?


;;;","06/Nov/12 17:56;nehanarkhede;I ran system tests but they failed and all the kafka server logs were empty. ;;;","08/Nov/12 01:23;prashanth.menon;New patched to address points.

30. Very correct, fixed in this patch.

Yes, I've been updating/building the package before every run, but I'm still consistently getting two failures.  It's the same tests every time.  I can try on another machine too.;;;","08/Nov/12 05:56;junrao;Thanks for patch v4. Patch looks good and system tests pass for me. Can't see the failures that you and Neha are seeing. So, +1 from me.;;;","13/Nov/12 16:15;jfung;The following cases (base case from each functional test group) were executed with patch v4 and all passing:
0001, 0021, 0101, 0111, 0121, 0131, 0151, 0201

So +1 from me.;;;","13/Nov/12 16:22;prashanth.menon;Awesome, thanks John.  I can commit this later today.

Any idea why two of my tests are failling consistently (or why Neha wasn't able to get any of the tests to pass) ?;;;","13/Nov/12 16:43;jfung;Hi Prashanth,

Please kindly check that if there are any previous running instances of brokers / zookeeper in the local host. That may be the reason that the brokers logs are empty.

Thanks
John;;;","15/Nov/12 00:29;nehanarkhede;+1. Looks good and tests pass !;;;","15/Nov/12 00:29;nehanarkhede;Checked it in since it blocks work on KAFKA-532. ;;;","15/Nov/12 00:29;nehanarkhede;Thanks for the patch, Prashanth!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System Test : Leader Failure Log Segment Checksum Mismatched When request-num-acks is 1,KAFKA-573,12611772,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,jfung,jfung,14/Oct/12 22:43,19/Oct/12 00:10,14/Jul/23 05:39,19/Oct/12 00:10,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"• Test Description:

1. Start a 3-broker cluster as source
2. Send messages to source cluster
3. Find leader and terminate it (kill -15)
4. Start the broker again
5. Start a consumer to consume data
6. Compare the MessageID in the data between producer log and consumer log.

• Issue: There will be data loss if request-num-acks is set to 1. 

• To reproduce this issue, please do the followings:

1. Download the latest 0.8 branch
2. Apply the patch attached to this JIRA
3. Build kafka by running ""./sbt update package""
4. Execute the test in directory ""system_test"" : ""python -B system_test_runner.py""
5. This test will execute testcase_2 with the following settings:
    Replica factor : 3
    No. of partitions : 1
    No. of bouncing : 1",,jfung,jjkoshy,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/12 22:51;jfung;acks1_leader_failure_data_loss.tar.gz;https://issues.apache.org/jira/secure/attachment/12549097/acks1_leader_failure_data_loss.tar.gz","14/Oct/12 22:47;jfung;kafka-573-reproduce-issue.patch;https://issues.apache.org/jira/secure/attachment/12549096/kafka-573-reproduce-issue.patch","18/Oct/12 02:02;junrao;kafka-573.patch;https://issues.apache.org/jira/secure/attachment/12549629/kafka-573.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,248589,,,Fri Oct 19 00:10:52 UTC 2012,,,,,,,,,,"0|i09w93:",55666,,,,,,,,,,,,,,,,,,,,"14/Oct/12 22:57;jfung;These are some manual verification of the differences in the log segment files:

• Merging the log segment files for each broker:
  $ for i in `find kafka_server_1_logs/ -name '0*.log' | sort`; do cat $i >> broker-1-merged/00000000000000000000.log; done
  $ for i in `find kafka_server_2_logs/ -name '0*.log' | sort`; do cat $i >> broker-2-merged/00000000000000000000.log; done
  $ for i in `find kafka_server_3_logs/ -name '0*.log' | sort`; do cat $i >> broker-3-merged/00000000000000000000.log; done

• Verify the checksum of each merged log segment and they are different:
  $ cksum broker-1-merged/00000000000000000000.log 
  1742950004 1638036 broker-1-merged/00000000000000000000.log
  $ cksum broker-2-merged/00000000000000000000.log 
  2050258314 1639080 broker-2-merged/00000000000000000000.log
  $ cksum broker-3-merged/00000000000000000000.log 
  1802214049 1639080 broker-3-merged/00000000000000000000.log

• Get the dump log segments of the merged files:
  $ bin/kafka-run-class.sh kafka.tools.DumpLogSegments broker-1-merged/00000000000000000000.log > broker-1-dump-log-segment.log
  $ bin/kafka-run-class.sh kafka.tools.DumpLogSegments broker-2-merged/00000000000000000000.log > broker-2-dump-log-segment.log
  $ bin/kafka-run-class.sh kafka.tools.DumpLogSegments broker-3-merged/00000000000000000000.log > broker-3-dump-log-segment.log

• Diff the dump log segment between broker-1 & broker-2:

$ diff broker-1-dump-log-segment.log broker-2-dump-log-segment.log 

1c1
< Dumping broker-1-merged/00000000000000000000.log
---
> Dumping broker-2-merged/00000000000000000000.log
113a114
> offset: 112 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 2581499653
168a170
> offset: 168 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 3880215630
387a390
> offset: 389 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 3744939326
2734d2736
< offset: 2737 isvalid: true payloadsize: 500 magic: 2 compresscodec: NoCompressionCodec crc: 314900536
;;;","15/Oct/12 05:01;jfung;If ack is set to 1, we actually don't guarantee no data loss. This is because when the client receives an ack, data is only guaranteed to be in the leader, but not necessarily in other replicas. So, if a leader is bounced, some acked data could be lost. 

Nevertheless, merged checksums should still match among all replicas. ;;;","18/Oct/12 02:02;junrao;Attach a patch. There are 2 problems. The first one is the most severe one. We recently changed FileMessageSet to remove the mutable flag. As a result, everytime a new FileMessageSet is created, the constructor sets the file channel's position to the end of the file. What's happening is that while a file channel is being appended for newly produced data, the file position is moved by FileMessageSet created for fetch requests. Since they are not properly synchronized, occasionally, a message in the log is overwritten. The second issue is that in ByteBufferMessageSet.writeTo. We try to reset the buffer position after writing the data in the buffer to the channel. However, since there is no guarantee that the whole buffer will be written to the channel in a single write, resetting the buffer position could cause incorrect bytes being written to the channel.

The patch fixes both issues. The changes are: (1) Added a new flag in the constructor of FileMessageSet to control whether the channel position is set to the end of the file or not. (2) Changed ByteBufferMessageSet.writeTo so that we wait until the whole buffer is written to the channel before resetting the buffer position. (3) Added a few more logging that I found useful while investigating the issues.

The system test passes now.;;;","18/Oct/12 15:54;jfung;Thanks Jun for the patch. The patch is applied together with the reproducing issue patch to the latest 0.8 branch and it is working correctly now.;;;","18/Oct/12 20:27;jfung;A full regression test (35 test cases) has been launched with this patch and the following is the summary of the results:

1. There are all together 5 failures in 5 test cases out of 35 cases.

    * testcase_0103 : 2 out of 2300 msg lost (checksum matched) - failure case with Ack == 1 in Sync mode (1 topic, 1 partition)
    * testcase_0105 : 2 out of 2300 msg lost (checksum matched) - failure case with Ack == 1 in Async mode (1 topic, 1 partition)
    * testcase_0114 : 11 out of 5100 msg lost (checksum matched) - failure case with Ack == 1 in Async mode (1 topic, 3 partitions)
    * testcase_0117 : 44 out of 5100 msg lost (checksum matched) - failure case with Ack == 1 in Sync mode (1 topic, 3 partitions)
    * testcase_0122 : 1524 out of 4600 msg lost (checksum matched) - failure case with Ack == 1 in Sync mode ( 2 topics, 3 partitions)

2. The results suggest that the fix for KAFKA-573 is working well (minor data loss in the first 4 cases are expected in leader failure cases when ack == 1).

3. The result in testcase_0122 could be related to a different issue and a new JIRA will be created to keep track of that.
;;;","18/Oct/12 21:37;jjkoshy;+1

This must have been a pain to track down. Minor comment: In FileMessageSet, since we always open mutable, can you fix the comment (from javadoc) that says it can be opened immutable. Also, the info log reads a bit odd.;;;","18/Oct/12 22:31;nehanarkhede;+1, this one was fun to track down. Reminded me of the FileChannel truncate bug.

Minor comment - Probably best to delete the info statement - 
 info(""After changed to position %d with size %d"".format(channel.position(), channel.size()))
;;;","19/Oct/12 00:10;junrao;Thanks for the review. Removed the unnecessary log statement and committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Merged log segment checksums mismatched in Leader failure System Test case,KAFKA-572,12611665,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jfung,jfung,13/Oct/12 05:12,13/Oct/12 22:22,14/Jul/23 05:39,13/Oct/12 22:22,,,,,,,,,,,,,,0,,,,,,jfung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/12 05:26;jfung;kafka-572-reproduce-issue.patch;https://issues.apache.org/jira/secure/attachment/12549004/kafka-572-reproduce-issue.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,248201,,,Sat Oct 13 22:22:38 UTC 2012,,,,,,,,,,"0|i09phz:",54564,,,,,,,,,,,,,,,,,,,,"13/Oct/12 05:32;jfung;* Test Description:

1. Start a 3-broker cluster as source
2. Send messages to source cluster
3. Find leader and terminate it (kill -15)
4. Start the broker shortly
5. Start a consumer to consume data
6. Compare the MessageID in the data between producer log and consumer log.

* To reproduce this issue, please do the followings:

1. Download the latest 0.8 branch
2. Apply the patch attached to this JIRA
3. Build kafka by running ""./sbt update package""
4. Execute the test in directory ""system_test"" : ""python -B system_test_runner.py""

* Output from the test - No data loss but merged log segment checksums mismatched

2012-10-12 22:27:35,760 - INFO - ======================================================
2012-10-12 22:27:35,760 - INFO - validating data matched
2012-10-12 22:27:35,760 - INFO - ======================================================
2012-10-12 22:27:35,771 - INFO - no. of unique messages on topic [test_1] sent from publisher  : 500 (kafka_system_test_utils)
2012-10-12 22:27:35,771 - INFO - no. of unique messages on topic [test_1] received by consumer : 500 (kafka_system_test_utils)
2012-10-12 22:27:35,771 - INFO - ================================================
2012-10-12 22:27:35,771 - INFO - validating merged broker log segment checksums
2012-10-12 22:27:35,771 - INFO - ================================================
{u'kafka_server_1_logs:test_1-0': 'd70c8d37634b0b08cd407eb042e77ef8',
 u'kafka_server_2_logs:test_1-0': 'd70c8d37634b0b08cd407eb042e77ef8',
 u'kafka_server_3_logs:test_1-0': '924d30d5f0d2a8ba9ef45f7cba88e192'}
2012-10-12 22:27:35,774 - ERROR - merged log segment checksum in test_1-0 mismatched (kafka_system_test_utils)

;;;","13/Oct/12 05:37;jfung;* Data Log Segment files sizes:

system_test/replication_testsuite/testcase_0102/logs $ find broker-* -name '00*.log' -ls

9702225   12 -rw-r--r--   1 jfung    eng         10279 Oct 12 22:27 broker-1/kafka_server_1_logs/test_1-0/00000000000000000301.log
9702226   12 -rw-r--r--   1 jfung    eng         10271 Oct 12 22:27 broker-1/kafka_server_1_logs/test_1-0/00000000000000000000.log
9702227   12 -rw-r--r--   1 jfung    eng         10293 Oct 12 22:27 broker-1/kafka_server_1_logs/test_1-0/00000000000000000201.log
9702228   12 -rw-r--r--   1 jfung    eng         10292 Oct 12 22:27 broker-1/kafka_server_1_logs/test_1-0/00000000000000000101.log
9702230   12 -rw-r--r--   1 jfung    eng         10178 Oct 12 22:27 broker-1/kafka_server_1_logs/test_1-0/00000000000000000401.log

9702239   12 -rw-r--r--   1 jfung    eng         10279 Oct 12 22:27 broker-2/kafka_server_2_logs/test_1-0/00000000000000000301.log
9702240   12 -rw-r--r--   1 jfung    eng         10271 Oct 12 22:27 broker-2/kafka_server_2_logs/test_1-0/00000000000000000000.log
9702241   12 -rw-r--r--   1 jfung    eng         10293 Oct 12 22:27 broker-2/kafka_server_2_logs/test_1-0/00000000000000000201.log
9702242   12 -rw-r--r--   1 jfung    eng         10292 Oct 12 22:27 broker-2/kafka_server_2_logs/test_1-0/00000000000000000101.log
9702244   12 -rw-r--r--   1 jfung    eng         10178 Oct 12 22:27 broker-2/kafka_server_2_logs/test_1-0/00000000000000000401.log

9702252   12 -rw-r--r--   1 jfung    eng         10279 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000361.log
9702255    4 -rw-r--r--   1 jfung    eng          4010 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000461.log
9702256   12 -rw-r--r--   1 jfung    eng         10271 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000000.log
9702257    8 -rw-r--r--   1 jfung    eng          5657 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000101.log
9702259   12 -rw-r--r--   1 jfung    eng         10280 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000261.log
9702262   12 -rw-r--r--   1 jfung    eng         10816 Oct 12 22:27 broker-3/kafka_server_3_logs/test_1-0/00000000000000000156.log
;;;","13/Oct/12 22:22;jfung;This issue is due to a bug in the System Test script (log segment files were not sorted before merging). So mark this Fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka should not need snappy jar at runtime,KAFKA-570,12611625,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,swapnilghike,swapnilghike,12/Oct/12 20:06,07/Jun/16 02:45,14/Jul/23 05:39,07/Jun/16 02:45,0.8.0,,,0.9.0.0,,,,,,,,,,2,bugs,,,"CompressionFactory imports snappy jar in a pattern match. The purpose of importing it this way seems to be avoiding the import unless snappy compression is actually required. However, kafka throws a ClassNotFoundException if snappy jar is removed at runtime from lib_managed. 

This exception can be easily seen by producing some data with the console producer.",,brugidou,donnchadh,guozhang,ijuma,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,248110,,,Tue Jun 07 02:45:00 UTC 2016,,,,,,,,,,"0|i09lin:",53919,,,,,,,,,,,,,,,,,,,,"04/Sep/14 22:25;guozhang;This issue will be automatically fixed when the server code moves to client's common libraries, moving to 0.9 for now.;;;","07/Jun/16 02:45;ijuma;This is fixed in the new clients, closing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup kafka.utils.Utils,KAFKA-569,12611471,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,12/Oct/12 00:29,13/Oct/12 16:36,14/Jul/23 05:39,13/Oct/12 16:36,0.8.0,,,,,,,,,,,,,0,,,,kafka.utils.Utils is a real mess. It is full of odd little pieces of business logic dropped there. We should clean this up.,,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/12 22:44;jkreps;KAFKA-569-v2.patch;https://issues.apache.org/jira/secure/attachment/12548980/KAFKA-569-v2.patch","12/Oct/12 00:39;jkreps;KAFKA-569.patch;https://issues.apache.org/jira/secure/attachment/12548832/KAFKA-569.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,247838,,,Fri Oct 12 23:50:17 UTC 2012,,,,,,,,,,"0|i08q7z:",48847,,,,,,,,,,,,,,,,,,,,"12/Oct/12 00:39;jkreps;This patch has a large number of trivial changes.

Here is a summary:
- Make bad names good
- Create kafka.api.ApiUtils and move all protocol parsing related methods there
- Get rid of the encoding argument in ApiUtils since our protocol requires UTF8, so making this changable is not good.
- Move zookeeper method into ZkUtils, delete random duplicate version of that code elsewhere
- Move SyncJSON which was inside Utils (!) into its own file
- Create ClientUtils and put getTopicMetadata() there
- Create CommandLineUtils and put command line stuff there
- Delete a few silly, unused methods
- Move getTopicPartition into LogManager since it is an implementation detail of LogManager
- Move getAllBrokersFromBrokerList into BrokerPartitionInfo, the only place it is used
- Get rid of Utils.getNextRandomInt. Since random is synchronized it is better to not have a single one.
- Changed DefaultEventHandler to user a counter rather than Utils.randomInt--this will be more evenly distributed since it is true round-robin.
- Delete ~5 custom property parsing methods that were jammed into Utils. Add a generic method VerifiableProperties.getMap that replaces them all.;;;","12/Oct/12 04:19;nehanarkhede;Looks great, thanks for contributing this cleanup patch !
Just one minor suggestion - Can we add a description in ClientUtils similar to the one in Utils and ApiUtils ?
;;;","12/Oct/12 22:44;jkreps;Neha, thanks for the quick review!

New patch:
- Added the comment you requested
- Rebased
- Changed naming of ISR to be consistent (we had, for example, LeaderAndIsrRequest but LeaderAndISRResponse);;;","12/Oct/12 23:22;nehanarkhede;Looks good, just one thing that I missed in the first review - I see you added a counter to DefaultEventHandler for the round-robin load-balancing. For a long running producer, the counter could overflow to a negative value. I think we need to reset it to 0, no ?

+1 otherwise.;;;","12/Oct/12 23:44;jkreps;Oh my, good catch.

BTW, in fixing this I found a rare bug in DefaultPartitioner which does a similar thing. The cause is that we take
  math.abs(key.hashCode) % numPartitions
The problem with this is that since the creators of java hate all living things they made java.lang.Math.abs return a negative number when given Integer.MIN_VALUE (since it has no equivalent positive integer). Sooner or later hashCode() will return a negative value, which would cause this.

I am adding a new method in Utils (how apt)
 def abs(n: Int) = n & 0x7fffffff

That should fix this. Not sure if we ever saw this happen in practice.;;;","12/Oct/12 23:50;nehanarkhede;>> The problem with this is that since the creators of java hate all living things they made java.lang.Math.abs return a negative number when given Integer.MIN_VALUE (since it has no equivalent positive integer). Sooner or later hashCode() will return a negative value, which would cause this.

:O (Stumped and don't know what else to say). Great catch !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replication Data Loss in Mirror Maker Bouncing testcase,KAFKA-567,12611389,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,jfung,jfung,11/Oct/12 17:08,12/Oct/12 20:35,14/Jul/23 05:39,12/Oct/12 20:29,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,,,"* Test Description: 
1. Start a 3-broker cluster as source
2. Start a 3-broker cluster as target
3. Start 1 instance of Mirror Maker to replicate data from source to target
4. While producer is sending data into source cluster, stop Mirror Maker with ""kill -15"". Start Mirror Maker again after 1 second.
5. Start a consumer to consume data from target cluster.
6. Compare the MessageID in the data between producer log and consumer log.

* To reproduce this issue, please do the followings:

1. Download the latest 0.8 branch
2. Apply the patch attached to this JIRA
3. Build kafka by running ""./sbt update package""
4. Execute the test in directory ""system_test"" : ""python -B system_test_runner.py""

* The test result may look like the following:

_test_case_name  :  testcase_5002
_test_class_name  :  MirrorMakerTest
arg : bounce_leader  :  false
arg : bounce_mirror_maker  :  true
arg : message_producing_free_time_sec  :  15
arg : num_iteration  :  1
arg : num_messages_to_produce_per_producer_call  :  50
arg : num_partition  :  1
arg : replica_factor  :  3
arg : sleep_seconds_between_producer_calls  :  1
validation_status  : 
     Log segment checksum matching across all replicas  :  FAILED
     Unique messages from consumer on [test_1]  :  355
     Unique messages from producer on [test_1]  :  400
     Validate for data matched on topic [test_1]  :  FAILED

* Attached a tar file for the system test output log, the brokers' log4j files and data log segment files.

* There are no unusual Exception / Error found in the logs. However, there are consistently data loss in this Mirror Maker bouncing test case. Not sure if this is related to KAFKA-552.",,jfung,jjkoshy,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/12 16:35;jjkoshy;KAFKA-567-mirrormakershutdown-v1.patch;https://issues.apache.org/jira/secure/attachment/12548913/KAFKA-567-mirrormakershutdown-v1.patch","12/Oct/12 04:52;junrao;kafka-567.patch;https://issues.apache.org/jira/secure/attachment/12548853/kafka-567.patch","11/Oct/12 17:08;jfung;kafka-mirror-maker-bouncing-data-loss.patch;https://issues.apache.org/jira/secure/attachment/12548773/kafka-mirror-maker-bouncing-data-loss.patch","11/Oct/12 19:31;jfung;mirror_maker_12.log;https://issues.apache.org/jira/secure/attachment/12548793/mirror_maker_12.log","11/Oct/12 17:09;jfung;system_test_1349971807_testcase_5002.tar;https://issues.apache.org/jira/secure/attachment/12548774/system_test_1349971807_testcase_5002.tar",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,247721,,,Fri Oct 12 20:29:37 UTC 2012,,,,,,,,,,"0|i08pcf:",48705,,,,,,,,,,,,,,,,,,,,"11/Oct/12 18:54;junrao;Took a quick look.

Log in source broker:
system_test_1349971807_testcase_5002 jrao$ ls kafka_server_4_logs/test_1-0/*.log
kafka_server_4_logs/test_1-0/00000000000000000000.log  kafka_server_4_logs/test_1-0/00000000000000000140.log  kafka_server_4_logs/test_1-0/00000000000000000280.log
kafka_server_4_logs/test_1-0/00000000000000000020.log  kafka_server_4_logs/test_1-0/00000000000000000160.log  kafka_server_4_logs/test_1-0/00000000000000000300.log
kafka_server_4_logs/test_1-0/00000000000000000040.log  kafka_server_4_logs/test_1-0/00000000000000000180.log  kafka_server_4_logs/test_1-0/00000000000000000320.log
kafka_server_4_logs/test_1-0/00000000000000000060.log  kafka_server_4_logs/test_1-0/00000000000000000200.log  kafka_server_4_logs/test_1-0/00000000000000000340.log
kafka_server_4_logs/test_1-0/00000000000000000080.log  kafka_server_4_logs/test_1-0/00000000000000000220.log  kafka_server_4_logs/test_1-0/00000000000000000360.log
kafka_server_4_logs/test_1-0/00000000000000000100.log  kafka_server_4_logs/test_1-0/00000000000000000240.log  kafka_server_4_logs/test_1-0/00000000000000000380.log
kafka_server_4_logs/test_1-0/00000000000000000120.log  kafka_server_4_logs/test_1-0/00000000000000000260.log

Log in target broker
system_test_1349971807_testcase_5002 jrao$ ls kafka_server_7_logs/test_1-0/*.log
kafka_server_7_logs/test_1-0/00000000000000000000.log  kafka_server_7_logs/test_1-0/00000000000000000185.log  kafka_server_7_logs/test_1-0/00000000000000000312.log
kafka_server_7_logs/test_1-0/00000000000000000100.log  kafka_server_7_logs/test_1-0/00000000000000000235.log
kafka_server_7_logs/test_1-0/00000000000000000155.log  kafka_server_7_logs/test_1-0/00000000000000000262.log

It seems that the test didn't give mirror maker enough time to catch up data after bouncing. Could you change the test to wait a bit longer after bounce?;;;","11/Oct/12 18:59;nehanarkhede;We should probably increase time outs after knowing why the test is running slower. Increasing timeouts can hide performance regressions.;;;","11/Oct/12 19:36;jfung;Uploaded mirror_maker_12.log for a similar test but with the following changes:

11. After Mirror Maker is bounced and wait 15 seconds, Producer (sending data in the source cluster) is stopped. Then wait for 60 seconds more to make sure that Mirror Maker is able to catch up.

12. kafka.consumer.ConsumerIterator is configured to be at TRACE level in log4j.properties

13. kafka.producer.Producer is also configured to be at TRACE level.

14. The attached mirror maker log file shows that there are 400 messages each showing in the TRACE level messages which is matching the Producer data count but Target consumer is still missing some messages.;;;","12/Oct/12 04:52;junrao;Attach a patch. The problem is that async producer didn't drain the last batch of events on close, a bug introduced during producer refactoring. Now, the test has no data loss.;;;","12/Oct/12 15:51;jfung;Thanks Jun for the patch. The fix is tested with the latest 0.8 branch (r.1397616) by apply both your patch and the patch to reproduce the issue. The following is the test result: (Please ignore the checksum validating failure for the time being)

_test_case_name  :  testcase_5002
_test_class_name  :  MirrorMakerTest
arg : bounce_leader  :  false
arg : bounce_mirror_maker  :  true
arg : message_producing_free_time_sec  :  15
arg : num_iteration  :  1
arg : num_messages_to_produce_per_producer_call  :  50
arg : num_partition  :  1
arg : replica_factor  :  3
arg : sleep_seconds_between_producer_calls  :  1
validation_status  : 
     Log segment checksum matching across all replicas  :  FAILED
     Unique messages from consumer on [test_1]  :  400
     Unique messages from producer on [test_1]  :  400
     Validate for data matched on topic [test_1]  :  PASSED
;;;","12/Oct/12 16:34;jjkoshy;+1 for kafka-567.patch

Also, right now mirror maker does not always shutdown cleanly. E.g., if you want to use consumer.timeout.ms (and avoid having to do an external wait for mirroring to finish). John, I'll attach a patch for that as well if it helps the testing framework.;;;","12/Oct/12 16:57;nehanarkhede;+1 on KAFKA-567.patch, good catch !;;;","12/Oct/12 20:29;junrao;Thanks for the reviews. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaScheduler shutdown in ZookeeperConsumerConnector should check for config.autocommit,KAFKA-563,12611083,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,swapnilghike,swapnilghike,09/Oct/12 22:46,10/Oct/12 16:34,14/Jul/23 05:39,10/Oct/12 16:34,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,,,Kafkascheduler starts up only if ConsumerConfig.autocommit is true. Its shutdown should check for this condition too.,,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,246544,,,Wed Oct 10 16:34:23 UTC 2012,,,,,,,,,,"0|i07r4n:",43161,,,,,,,,,,,,,,,,,,,,"10/Oct/12 16:34;junrao;Committed a trivial fix to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Non-failure System Test Log Segment File Checksums mismatched,KAFKA-562,12611078,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jfung,jfung,09/Oct/12 22:09,22/Nov/12 21:56,14/Jul/23 05:39,22/Nov/12 21:56,,,,0.8.0,,,,,,,,,,0,,,,"To reproduce this issue
1. Download 0.8 branch (reproduced in r1396343)
2. Apply the patch attached
3. Build Kafka under <kafka_home> by running ""./sbt update package""
4. In the directory <kafka_home>/system_test, run ""python -B system_test_runner.py"" and it will run the case ""testcase_0002"" which will reproduce this issue.
5. The log segment files will be located in /tmp",,jfung,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/12 22:10;jfung;kafka-562-reproduce-issue.patch;https://issues.apache.org/jira/secure/attachment/12548475/kafka-562-reproduce-issue.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,246538,,,Thu Nov 22 21:56:51 UTC 2012,,,,,,,,,,"0|i07r2n:",43152,,,,,,,,,,,,,,,,,,,,"09/Oct/12 22:24;jfung;In this testcase, the observations are as follows:

1. By comparing the messages produced and consumed, there is no data loss.

2. There are unequal no. of log segment files

[/tmp] find kafka_server_* -name '*.log' -ls
20189793   12 -rw-r--r--   1 jfung    eng         10322 Oct  9 14:44 kafka_server_1_logs/test_1-0/00000000000000000000.log
20189809    4 -rw-r--r--   1 jfung    eng          1537 Oct  9 14:44 kafka_server_1_logs/test_1-0/00000000000000000261.log

20189797   12 -rw-r--r--   1 jfung    eng         10271 Oct  9 14:44 kafka_server_2_logs/test_1-0/00000000000000000000.log
20189807   12 -rw-r--r--   1 jfung    eng         10189 Oct  9 14:44 kafka_server_2_logs/test_1-0/00000000000000000201.log
20189803   12 -rw-r--r--   1 jfung    eng         10293 Oct  9 14:44 kafka_server_2_logs/test_1-0/00000000000000000101.log

20189798   12 -rw-r--r--   1 jfung    eng         10322 Oct  9 14:44 kafka_server_3_logs/test_1-0/00000000000000000000.log
20189819    4 -rw-r--r--   1 jfung    eng          1537 Oct  9 14:44 kafka_server_3_logs/test_1-0/00000000000000000261.log

3. If the corresponding files for broker 1, broker 2, ... are merged together in sequence, their checksum would not match either.

4. Running the tool DumpLogSegments on each individual file as shown below.

Broker 1
========
bin/kafka-run-class.sh kafka.tools.DumpLogSegments kafka_server_1_logs/test_1-0/00000000000000000000.log

Dumping kafka_server_1_logs/test_1-0/00000000000000000000.log

Starting offset: 0

offset: 4 isvalid: true payloadsize: 167 magic: 2 compresscodec: GZIPCompressionCodec crc: 2048048444
offset: 9 isvalid: true payloadsize: 171 magic: 2 compresscodec: GZIPCompressionCodec crc: 594807606
offset: 14 isvalid: true payloadsize: 172 magic: 2 compresscodec: GZIPCompressionCodec crc: 1696552621
offset: 19 isvalid: true payloadsize: 172 magic: 2 compresscodec: GZIPCompressionCodec crc: 3794535639
offset: 24 isvalid: true payloadsize: 172 magic: 2 compresscodec: GZIPCompressionCodec crc: 4167930995
. . .
offset: 245 isvalid: true payloadsize: 174 magic: 2 compresscodec: GZIPCompressionCodec crc: 3337681338
offset: 250 isvalid: true payloadsize: 174 magic: 2 compresscodec: GZIPCompressionCodec crc: 2655434756
offset: 255 isvalid: true payloadsize: 174 magic: 2 compresscodec: GZIPCompressionCodec crc: 5551624
offset: 260 isvalid: true payloadsize: 171 magic: 2 compresscodec: GZIPCompressionCodec crc: 53200305

bin/kafka-run-class.sh kafka.tools.DumpLogSegments kafka_server_1_logs/test_1-0/00000000000000000261.log 

Dumping kafka_server_1_logs/test_1-0/00000000000000000261.log

Starting offset: 261

offset: 265 isvalid: true payloadsize: 173 magic: 2 compresscodec: GZIPCompressionCodec crc: 1224901688
offset: 270 isvalid: true payloadsize: 173 magic: 2 compresscodec: GZIPCompressionCodec crc: 2027726868
offset: 275 isvalid: true payloadsize: 173 magic: 2 compresscodec: GZIPCompressionCodec crc: 559159044
offset: 280 isvalid: true payloadsize: 173 magic: 2 compresscodec: GZIPCompressionCodec crc: 157990978
offset: 285 isvalid: true payloadsize: 173 magic: 2 compresscodec: GZIPCompressionCodec crc: 2995943272
offset: 290 isvalid: true payloadsize: 173 magic: 2 compresscodec: GZIPCompressionCodec crc: 3964443281
offset: 295 isvalid: true payloadsize: 173 magic: 2 compresscodec: GZIPCompressionCodec crc: 140848011
offset: 299 isvalid: true payloadsize: 150 magic: 2 compresscodec: GZIPCompressionCodec crc: 657039729



Broker 2
=========

bin/kafka-run-class.sh kafka.tools.DumpLogSegments kafka_server_2_logs/test_1-0/00000000000000000000.log 

Dumping kafka_server_2_logs/test_1-0/00000000000000000000.log

Starting offset: 0

offset: 0 isvalid: true payloadsize: 77 magic: 2 compresscodec: GZIPCompressionCodec crc: 2305854709
offset: 1 isvalid: true payloadsize: 79 magic: 2 compresscodec: GZIPCompressionCodec crc: 1768470661
offset: 2 isvalid: true payloadsize: 80 magic: 2 compresscodec: GZIPCompressionCodec crc: 657973900
offset: 3 isvalid: true payloadsize: 80 magic: 2 compresscodec: GZIPCompressionCodec crc: 3672345982
offset: 4 isvalid: true payloadsize: 77 magic: 2 compresscodec: GZIPCompressionCodec crc: 3431890374
. . .
offset: 98 isvalid: true payloadsize: 80 magic: 2 compresscodec: GZIPCompressionCodec crc: 2479186795
offset: 99 isvalid: true payloadsize: 79 magic: 2 compresscodec: GZIPCompressionCodec crc: 2127679297
offset: 100 isvalid: true payloadsize: 81 magic: 2 compresscodec: GZIPCompressionCodec crc: 3367058812


/bin/kafka-run-class.sh kafka.tools.DumpLogSegments kafka_server_2_logs/test_1-0/00000000000000000101.log 

Dumping kafka_server_2_logs/test_1-0/00000000000000000101.log

Starting offset: 101

offset: 101 isvalid: true payloadsize: 81 magic: 2 compresscodec: GZIPCompressionCodec crc: 2061836440
offset: 102 isvalid: true payloadsize: 81 magic: 2 compresscodec: GZIPCompressionCodec crc: 1118186556
offset: 103 isvalid: true payloadsize: 81 magic: 2 compresscodec: GZIPCompressionCodec crc: 374092732
offset: 104 isvalid: true payloadsize: 80 magic: 2 compresscodec: GZIPCompressionCodec crc: 1013512453
. . .
offset: 198 isvalid: true payloadsize: 81 magic: 2 compresscodec: GZIPCompressionCodec crc: 1754585683
offset: 199 isvalid: true payloadsize: 80 magic: 2 compresscodec: GZIPCompressionCodec crc: 3604597143
offset: 200 isvalid: true payloadsize: 81 magic: 2 compresscodec: GZIPCompressionCodec crc: 1508187619


bin/kafka-run-class.sh kafka.tools.DumpLogSegments kafka_server_2_logs/test_1-0/00000000000000000201.log

Dumping kafka_server_2_logs/test_1-0/00000000000000000201.log

Starting offset: 201

offset: 201 isvalid: true payloadsize: 81 magic: 2 compresscodec: GZIPCompressionCodec crc: 782997024
offset: 202 isvalid: true payloadsize: 81 magic: 2 compresscodec: GZIPCompressionCodec crc: 1935012961
offset: 203 isvalid: true payloadsize: 80 magic: 2 compresscodec: GZIPCompressionCodec crc: 554951891
offset: 204 isvalid: true payloadsize: 81 magic: 2 compresscodec: GZIPCompressionCodec crc: 14519573
. . .
offset: 297 isvalid: true payloadsize: 80 magic: 2 compresscodec: GZIPCompressionCodec crc: 1204716196
offset: 298 isvalid: true payloadsize: 81 magic: 2 compresscodec: GZIPCompressionCodec crc: 2682999595
offset: 299 isvalid: true payloadsize: 81 magic: 2 compresscodec: GZIPCompressionCodec crc: 4284106376
;;;","10/Oct/12 18:10;jkreps;Yeah, this is definitely a real problem that is leading to not all messages being replicated. With the new logical offsets the followers should have messages with offset 0, 1, 2, 3, 4... as the leader does. But instead the followers have offsets 4,9,14,19,24... I.e. only every fifth message. Not sure of the cause, looking into it.;;;","10/Oct/12 18:46;jkreps;Okay, this is not a bug exactly, I was mistaken. Here is what is happening:

The leader receives one message at a time, gzip'd. The follower fetches chunks of multiple gzip'd messages.

The current logic is that when appending a message set we check if there are any compressed messages. If there are we need to uncompress all messages and re-compress with new offsets assigned. Because the follower is getting chunks of five messages at a time, it is compressing these together. The reason the follower logs are so much smaller is because they are batch compressed.

Not sure what the best thing to do here is. On one hand it is much nicer if the follower has byte-for-byte identical logs. On the other hand batch compression is a good thing.;;;","11/Oct/12 00:00;jkreps;So the proposed fix for this is to special case appends that come from replication and not do any offset assignment, re-compression, or anything else during these. This had already been proposed as a performance improvement. It also ensures that the offset assignment on the leader and followers will match. A patch is attached to KAFKA-557, and I verified that it fixes this system test issue.;;;","22/Nov/12 21:56;jkreps;This was addressed a while back.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replica fetch thread doesn't need to recompute message id,KAFKA-557,12611020,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jkreps,junrao,junrao,09/Oct/12 16:34,16/Oct/12 04:28,14/Jul/23 05:39,15/Oct/12 17:12,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,,,"With kafka-506, the leader broker computes the logical id for each message produced. This could involve decompressing and recompressing messages, which are expensive. When data is replicated from the leader to the follower, we could avoid recomputing the logical message id since it's the same.",,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/12 02:34;jkreps;KAFKA-557.patch;https://issues.apache.org/jira/secure/attachment/12548689/KAFKA-557.patch","10/Oct/12 23:16;jkreps;KAFKA-557.patch;https://issues.apache.org/jira/secure/attachment/12548661/KAFKA-557.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,246193,,,Thu Oct 11 17:31:24 UTC 2012,,,,,,,,,,"0|i07i1z:",41692,,,,,,,,,,,,,,,,,,,,"10/Oct/12 23:16;jkreps;Attached is a patch that makes offset assignment by the log optional. This is a bit hacky since it adds more casewise logic into Log depending on who is assigning offsets.

But with this patch now
1. It is not possible for the follower to assign different offsets than the leader
2. The follower log should be byte-for-byte identical to the leader

A best effort is made to validate the offsets in the case that we are not assigning them, but this does not extend to actually decompressing the message set.;;;","10/Oct/12 23:58;jkreps;I verified that this fixes KAFKA-562;;;","11/Oct/12 00:16;nehanarkhede;1. Log

1.1 Typos in description of analyzeAndValidateMessageSet
1.2  Wrap long lines in this file
1.3 In the append API, the offsets are first read into an offsets variable and then returned from outside the try catch block. Any reason why the flush and offsets are not inside the try block ? Also, to avoid Scala bugs, it might be better to turn the outer if-else into a case match since case match always evalues to a value. If it doesn't, the code doesn't compile

2. LogTest

2.1 Remove unused import
2.2 It looks like the expected value in the following assertEquals is log.logEndOffset ? If yes, it should be the first argument
      assertEquals(last + 1, log.logEndOffset)
2.3 In the following assertTrue, should we be using sizeInBytes instead ?
      assertTrue(log.read(5, 64*1024).size > 0)
2.4 Does it make sense to check that the offsets of the message set read from the log ?
;;;","11/Oct/12 02:32;jkreps;1.1 Fixed
1.2 The long lines are due to use of String.format() which requires a single string to format. Breaking the lines is going to be uglier as they need to move to +.
1.3 The intention is to move the flush out of the synchronized block. I don't know who put that in there, but that is not good, since that is likely the slowest thing we are doing and has no need for synchronization. You raise a good point which is that the flush might throw an IOException, so it should be in the try catch. I reversed the order of the try/catch to fix this issue. With respect to using match instead of if/else, I disagree with that. if/else reads as english. I see us using match statements for simple if/else switches on boolean variables and I think it is pretty nasty.

2.1 I don't have IDEA so I can't do that.
2.2 No, how I have it is right. I am asserting that the logEndOffset is properly updated. I will add a test description to make that more clear.
2.3 Basically I am checking that we read something. Cleaned this up to make it more readable.
2.4 Unfortunately there isn't a simple test that is easy to implement. One unfortunate thing is that we implemented the shallow/deep iterator logic in ByteBufferMessageSet not in MessageSet so at this layer I ONLY have access to the wrapper message in the compressed case. There isn't a very meaning thing to say about the offset as a result. Instead the test I am doing is just that I can read something at offset 5 (which would not be true if the offsets had been reassigned by the log).;;;","11/Oct/12 04:13;nehanarkhede;1.2 Understand, but that's what we've been following throughout the code. If you want to propose a different line wrapping convention, let's discuss that to keep things consistent.
1.3 Agree that all if/else with simple boolean statements shouldn't be replaced by case-match. I was specifically referring to if-else statements that are meant to evaluate to a certain value. In such scenarios, case-match guarantees evaluation to a value unlike if-else, so it is safer. However, that is just my preference, you can keep it if you disagree.

Latest patch looks good otherwise. ;;;","11/Oct/12 17:31;jkreps;Oh man, you are a tough reviewer:
1.2 The coding guide says ""There is not a maximum line length (certainly not 80 characters, we don't work on punch cards any more), but be reasonable.""  I guess I just think that given the code fits on a 2008 laptop screen that is reasonable :-)
1.3 Yeah, I agree it is a matter of different taste.;;;","11/Oct/12 17:31;nehanarkhede;+1 ;-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change MessageSet.sizeInBytes to Int,KAFKA-556,12611017,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,junrao,junrao,09/Oct/12 16:23,24/Oct/12 16:20,14/Jul/23 05:39,24/Oct/12 16:20,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,,,"With kafka-506, there are various places where we assume that each log segment is less than 2GB.",,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/12 22:38;swapnilghike;kafka-556-v1.patch;https://issues.apache.org/jira/secure/attachment/12550546/kafka-556-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,246189,,,Wed Oct 24 16:20:12 UTC 2012,,,,,,,,,,"0|i07i13:",41688,,,,,,,,,,,,,,,,,,,,"09/Oct/12 16:28;junrao;In this jira, we should also change shallowValidByteCount and sizeInBytes in ByteBufferMessageSet from long to int.;;;","23/Oct/12 22:38;swapnilghike;Changed return/parameter/data types from Long to Int in the foll:

1. MessageSet: sizeInBytes, writeTo()

2. ByteBufferMessageSet: shallowValidByteCount, sizeInBytes, writeTo, validBytes

3. Log: maxLogFileSize

4. FileMessageSet: start, limit, _size, read, writeTo, sizeInBytes, truncateTo

+ Corresponding changes in type conversions ;;;","24/Oct/12 16:20;junrao;+1. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
confusing reference to zk.connect in config/producer.properties,KAFKA-553,12610981,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,,yazgoo,yazgoo,09/Oct/12 12:53,10/Oct/12 15:47,14/Jul/23 05:39,10/Oct/12 15:47,0.8.0,,,0.8.0,,,,,,,config,,,0,producer,zk.connect,,"https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8/config/producer.properties
still has comments relative to zookeeper:
    # need to set either broker.list or zk.connect
    # zk.connect=
    …
But https://issues.apache.org/jira/browse/KAFKA-369 has removed the dependency to
zookeeper.
Those old comments are confusing since they imply that you can still use zk.connect with producers.",any,junrao,yazgoo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/12 08:33;yazgoo;kafka_553_v1.diff;https://issues.apache.org/jira/secure/attachment/12548538/kafka_553_v1.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,246011,,,Wed Oct 10 15:47:32 UTC 2012,,,,,,,,,,"0|i07fwf:",41342,,,,,,,,,,,,,,,,,,,,"09/Oct/12 17:11;junrao;Thanks for finding this issue. Would you like to submit a patch?;;;","10/Oct/12 08:33;yazgoo;In config/producer.properties

1. removing references to zookeeper
2. changing the description of broker.list;;;","10/Oct/12 15:47;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log.truncateTo() may need to trucate immutable log segment,KAFKA-551,12610678,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jkreps,junrao,junrao,06/Oct/12 01:53,09/Oct/12 16:42,14/Jul/23 05:39,09/Oct/12 16:41,0.8.0,,,,,,,,,,core,,,0,bugs,,,"In makeFollower, we need to first truncate the local log to high watermark. It's possible that we need to truncate into segments before the last one. The problem is that all segments except the last one are immutable. So the truncation will fail which prevents the replica fetcher from being started.

One solution is to reopen the segment as mutable during truncation, if it's not mutable already.",,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/12 22:58;jkreps;KAFKA-551.patch;https://issues.apache.org/jira/secure/attachment/12548326/KAFKA-551.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,244154,,,Tue Oct 09 16:41:45 UTC 2012,,,,,,,,,,"0|i05irb:",30136,,,,,,,,,,,,,,,,,,,,"06/Oct/12 02:37;jkreps;I think the simplest solution would just be to open all segments as r/w since that is effectively what they are now.

I can take this if you like since I was just mucking around on that stuff and it is possible we have the same bug in the OffsetIndex.;;;","06/Oct/12 04:05;junrao;Currently, we mark the first n-1 segments as read-only channels on broker startup. It doesn't seem to hurt to open them as r/w. Yes, it would great if you can fix it in your patch to take care of the index part too.;;;","08/Oct/12 22:58;jkreps;Attached patch removes mutable flag from offset index and log file. I think this actually turns out to simplify the code a bit since there was lots of casewise logic.

The downside is that it is now possible to append to the wrong  log segment which would have previously thrown a helpful exception and now would silently succeed.

It would be possible to improve this situation in the future by (1) completing the refactor of log to only reference index and file message set through the LogSegment class, and (2) adding a mutable flag in the log segment and checking that on all mutations.;;;","09/Oct/12 05:06;junrao;Thanks for the patch. Looks good. Just one comment.

1. OffsetIndex.truncateTo(): If there is an index entry that equals to the target offset, shouldn't we delete that index entry too? TruncateTo will make target offset the offset for the next append and there shouldn't be an index entry for offset no available yet.;;;","09/Oct/12 15:24;jkreps;I think (hope) that is what we are doing. Here is the logic in OffsetIndex.truncateTo:
      /* There are 3 cases for choosing the new size
       * 1) if there is no entry in the index <= the offset, delete everything
       * 2) if there is an entry for this exact offset, delete it and everything larger than it
       * 3) if there is no entry for this offset, delete everything larger than the next smallest
       */
      val newEntries = 
        if(slot < 0)
          0
        else if(logical(idx, slot) == offset)
          slot
        else
          slot + 1
The value of newEntries is the number of remaining entries after the truncation. If we truncate to an offset lower than what is contained in the index (say because there is no entry) we truncate to 0 entries. If we are given an offset which is in the index, we delete that entry and all greater entries. Otherwise if we find an entry smaller than the given entry we should not delete that but instead delete all entries larger than it.

For example in OffsetIndexTest.truncate:
    idx.truncateTo(9)
    assertEquals(""Index should truncate off last entry"", OffsetPosition(8, 8), idx.lookup(10))
This is asserting that in an index with an entry for every offset, if we truncate to 9, then the largest entry that remains will be 8.

I think this is right, what was the problem you saw?
;;;","09/Oct/12 15:38;junrao;Yes, you are right. The code looks good. The confusion for me was that slot is an index based off 0 and when using it to set entry size, it actually truncates the entry at slot. +1 on the patch.;;;","09/Oct/12 15:43;nehanarkhede;+1. LGTM;;;","09/Oct/12 16:41;jkreps;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wildcarded consumption is single-threaded,KAFKA-550,12610676,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jjkoshy,jjkoshy,jjkoshy,06/Oct/12 01:38,10/Oct/12 16:30,14/Jul/23 05:39,10/Oct/12 00:07,,,,,,,,,,,,,,0,,,,"It's surprising that we haven't noticed this before, but I was looking at a CPU usage profile on yourkit and It turns out that only one mirror maker thread is actually doing anything. Basically I suspect some bug in fetcher -> queue mapping. Only one queue seems to have any data. I'll look into this probably next week.",,jjkoshy,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/12 23:10;jjkoshy;KAFKA-550-v1.patch;https://issues.apache.org/jira/secure/attachment/12548327/KAFKA-550-v1.patch","09/Oct/12 19:53;jjkoshy;KAFKA-550-v2.patch;https://issues.apache.org/jira/secure/attachment/12548446/KAFKA-550-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,244151,,,Wed Oct 10 16:30:11 UTC 2012,,,,,,,,,,"0|i05iqn:",30133,,,,,,,,,,,,,,,,,,,,"08/Oct/12 23:10;jjkoshy;This was a silly coding bug - the topicThreadIdAndQueues map was getting
updated incorrectly.  This affects both 0.7 and 0.8. The same patch applies to
both trunk and 0.8.

The patch fixes the issue. Also got rid of the implicit def in
reinitializeConsumer.
;;;","09/Oct/12 05:51;junrao;Thanks for the patch. What was the bug? Is it the implicit function? 

1.reinitializeConsumer(): It seems that the computation for threadQueueStreamPairs is slightly different for WildcardTopicCount and StaticTopicCount. However, it seems that we can just use the same computation. ;;;","09/Oct/12 17:35;jjkoshy;The bug was not in the implicit function. I removed the implicit because it
could be easily avoided in this case.
The issue was here: (tt <- topicThreadIds; qs <- queuesAndStreams) yield (tt
-> qs)

If there were topic-threadIds t1, t2, and queues q1, q2, you would get (t1
-> q1) (t1, q2) (t2 -> q1) (t2 -> q2) and they were sequentially added to
the topicThreadIdAndQueues pool. So all the threads would feed into the same
queue.

Re: the computation is different because topicThreadIds in wildcard
consumption may share the same queue so you can't zip them directly. e.g., 
whether there are two or 100 allowed topics, they will all share the same 
num-streams queues, and num-streams != the number of threads, so you can't 
zip them. In the static topic count case, there is one-to-one correspondence
so you can zip them.

That said, I think the code can be made clearer here. I will upload another 
patch in a minute.
;;;","09/Oct/12 19:57;jjkoshy;Another point to consider for future enhancement is that right now all wildcard topics will be assigned
num-threads (i.e., the wildcard stream count) threads regardless of how many partitions
they have.

Ideally, the user should be able to say something like ""numThreads=100"" and messages from all
topic-partitions (that are allowed by the wildcard) should be fed into those queues/streams uniformly.
;;;","09/Oct/12 22:20;nehanarkhede;Good catch, +1;;;","10/Oct/12 00:07;jjkoshy;Committed to 0.8 and trunk.;;;","10/Oct/12 16:30;junrao;Could you open a separate jira for improving partition assignment for wildcard topic? This is probably beyond 0.8 though.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsumerOffsetChecker does not deal with hostnames published in zookeeper,KAFKA-549,12610638,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,bob.cotton@gmail.com,bob.cotton@gmail.com,05/Oct/12 21:17,08/Oct/12 23:19,14/Jul/23 05:39,08/Oct/12 23:19,0.7.1,,,,,,,,,,core,,,0,patch,,,"When configuring the broker using the 'hostname=' property in server.properties to publish hostnames in zookeeper, the ConsumerOffsetChecker can't parse the hostname from the zknode

The attached patch changes the host regexp to match hostnames as well as ip addresses.",,bob.cotton@gmail.com,jjkoshy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/12 21:22;bob.cotton@gmail.com;KAFKA-549-hostnames-in-zookeeper.diff;https://issues.apache.org/jira/secure/attachment/12548048/KAFKA-549-hostnames-in-zookeeper.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,244091,,,Mon Oct 08 23:19:34 UTC 2012,,,,,,,,,,"0|i05h1r:",29859,,,,,,,,,,,,,,,,,,,,"06/Oct/12 01:03;jjkoshy;+1

Thanks for the patch. I'll get this checked-in to both trunk/0.8 and close after that.;;;","08/Oct/12 23:19;jjkoshy;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Retain key in producer and expose it in the consumer,KAFKA-544,12610377,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jkreps,jkreps,jkreps,04/Oct/12 19:23,04/Dec/12 23:43,14/Jul/23 05:39,26/Nov/12 21:21,0.8.0,,,,,,,,,,,,,0,bugs,,,"KAFKA-506 added support for retaining a key in the messages, however this field is not yet set by the producer.

The proposal for doing this is to change the producer api to change ProducerData to allow only a single key/value pair so it has a one-to-one mapping to Message. That is change from
  ProducerData(topic: String, key: K, data: Seq[V])
to
  ProducerData(topic: String, key: K, data: V)

The key itself needs to be encoded. There are several ways this could be handled. A few of the options:
1. Change the Encoder and Decoder to be MessageEncoder and MessageDecoder and have them take both a key and value.
2. Another option is to change the type of the encoder/decoder to not refer to Message so it could be used for both the key and value.

I favor the second option but am open to feedback.

One concern with our current approach to serialization as well as both of these proposals is that they are inefficient. We go from Object=>byte[]=>Message=>MessageSet with a copy at each step. In the case of compression there are a bunch of intermediate steps. We could theoretically clean this up by instead having an interface for the encoder that was something like
   Encoder.writeTo(buffer: ByteBuffer, object: AnyRef)
and
   Decoder.readFrom(buffer:ByteBuffer): AnyRef
However there are two problems with this. The first is that we don't actually know the size of the data until  it is serialized so we can't really allocate the bytebuffer properly and might need to resize it. The second is that in the case of compression there is a whole other path to consider. Originally I thought maybe it would be good to try to fix this, but now I think it should be out-of-scope and we should revisit the efficiency issue in a future release in conjunction with our internal handling of compression.



",,initialcontext,jjkoshy,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/12 05:26;jkreps;KAFKA-544-joel-comments-v2.patch;https://issues.apache.org/jira/secure/attachment/12554634/KAFKA-544-joel-comments-v2.patch","21/Nov/12 18:51;jkreps;KAFKA-544-joel-comments.patch;https://issues.apache.org/jira/secure/attachment/12554550/KAFKA-544-joel-comments.patch","14/Nov/12 05:57;jkreps;KAFKA-544-v1.patch;https://issues.apache.org/jira/secure/attachment/12553457/KAFKA-544-v1.patch","14/Nov/12 21:51;jkreps;KAFKA-544-v2.patch;https://issues.apache.org/jira/secure/attachment/12553569/KAFKA-544-v2.patch","14/Nov/12 22:10;jkreps;KAFKA-544-v3.patch;https://issues.apache.org/jira/secure/attachment/12553574/KAFKA-544-v3.patch","15/Nov/12 01:08;jkreps;KAFKA-544-v4.patch;https://issues.apache.org/jira/secure/attachment/12553600/KAFKA-544-v4.patch","15/Nov/12 03:49;jkreps;KAFKA-544-v5.patch;https://issues.apache.org/jira/secure/attachment/12553614/KAFKA-544-v5.patch","15/Nov/12 21:44;jkreps;KAFKA-544-v6.patch;https://issues.apache.org/jira/secure/attachment/12553699/KAFKA-544-v6.patch",,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,240982,,,Mon Nov 26 19:24:25 UTC 2012,,,,,,,,,,"0|i017jr:",4992,,,,,,,,,,,,,,,,,,,,"05/Oct/12 20:00;jkreps;After looking at the code I think there is a fair amount of work here. I recommend we put off the user-facing API change until 0.9. Instead I propose the following intermediate hack for 0.8:
1. Use the existing ProducerData object to get the key and value. This is slightly unnatural because it allows you to associate a key with many values.
2. Use option (2) above for the encoders

So specifically this means that the two interfaces would now be 
trait Encoder[T] {
  def toBytes(t: T)
}
trait Decoder[T] {
  def fromBytes(b: Array[Byte]
}

There would now be two encoders, one for the key and one for the value. The value would still be configured by the property ""serializer.class"" but we would add a new property ""key.serializer.class"" which would default to use the same value as the value serializer.

The plan would be to hold off on any changes to the consumer for now.;;;","09/Oct/12 16:19;junrao;This plan sounds good to me.;;;","14/Nov/12 05:57;jkreps;This patch does the following:
1. Change Encoder and Decoder to map between object and byte[] rather than between Message and object.
2. Require two encoders for the producer and two decoders for the consumer. This follows the same pattern as before: in the producer there is now serializer.class and key.serializer.class. By default key.serializer takes the same value as serializer.class. If no key is specified then this parameter is essentially ignored. In the consumer ConsumerConnector now requires two decoders, one for the key and one for the value.
3. Message is now no longer exposed in the high level APIs.
4. All tests that used Message with high level apis (i.e. almost all tests) are now converted to use Strings. This is easier to read and debug (since you can print the value) and generally less code.

Overall I have some concern that we are changing the API without deeply thinking it through, but this does expose the key functionality which is needed.

This set of changes is wide but shallow--it touches a lot of classes but there is nothing too tricky.

All unit tests pass, but I haven't yet verified system tests.;;;","14/Nov/12 16:33;jkreps;One other minor thing in this patch:
5. Both the key and the partition are now exposed in MessageAndMetadata;;;","14/Nov/12 21:51;jkreps;Updated patch. This patch fixes on of the uglier things in the producer API. Previously the producer took a ProducerData object which contained a topic, an optional key and one or more messages.

The send method in the producer could take one or more of these. This is a little odd since sending many messages attached to a single ProducerData is the same as sending many ProducerData objects with the same key.

Since we now associate a key per message, I changed this. I added a new class
  case class KeyedMessage(topic: String, key: K, message: V)
this replaces ProducerData. Note that this class takes only a single key-value pair, but since you can send many at once in a single send() call this does not make the API any less general.;;;","14/Nov/12 22:10;jkreps;Okay one more change--added offset to MessageAndMetadata.;;;","15/Nov/12 01:08;jkreps;Final patch fixes a bug that effected the system tests. This patch is ready for review.

To summarize, here are the changes listed in one place
1. Change encoder/decoder to 
     def toBytes(t: T)
     def fromBytes(bytes: Array[Byte]): T
I also took the opportunity to pass properties into the encoders and decoders so encoders and decoders now require a constructor that takes a VerifiableProperties. This allows for things like schema registry url, character encoding, etc.
2. Rename ProducerData to KeyedMessage and make it only contain a single key-value pair.
3. Add a new property for the producer, key.serializer.class to complement the already existing serializer.class. The key serializer defaults to the same value as the value serializer since I think that will be common (e.g. both Avro).
4. ConsumerConnector now requires two decoders, one for the key and one for the value. The type of the resulting stream is now [K,V] rather than just [T].
5. Exposed partition and offset in MessageAndMetadata class
6. Changed unit tests to mostly use strings instead of Message or byte[]

This code is ready for review (por favor).

We also need to make a call whether we want this in 0.8. It is not a very tricky change, but it does touch a lot of files. Since it is a compatibility change it would be nice to do it in 0.8, but it is awfully late in the game for this...;;;","15/Nov/12 01:22;junrao;Thanks for patch v3. Looks good overall. Some minor comments:

30. Encoder: It seems that we require the constructor of  Encoder and Partitioner to take a VerifiableProperty. It would be good if we can add a comment on that in the trait.

31. ConsumerConnector: Can we have a version of create,essageStreamsByFilter without the decoders?

32. ConsumerFetcherManager: no change is needed.

33. BlockingChannel: logger.debug() should be just debug().

34. ChecksumMessageFormatter: We probably can't remove it since it may be used in our tests.
;;;","15/Nov/12 03:49;jkreps;30. Good point, added.
31. No, for some reason scala won't allow that since it causes confusion with the other method (probably why we didn't have the equivalent before). Instead I gave default arguments for the decoders, which accomplishes the same thing.
32. Fixed
33. Fixed
34. Added it back.

New patch contains these changes.;;;","15/Nov/12 04:41;junrao;+1 on patch v5. For 30, could you add the same constructor comment for Partitioner too?
;;;","18/Nov/12 22:35;nehanarkhede;Minor comment - The API docs are broken in Producer.scala, Producer.java and ConsumerConnector.java due to the stale param names;;;","20/Nov/12 02:28;jjkoshy;Overall the patch looks good - I have a few scattered comments (apologies for the late review).

Decoder.scala: What is the point of KeylessMessageDecoder - shouldn't it simply be called MessageDecoder? Phrased
differently, why does it matter to the message decoder whether the message has an associated key or not?

ConsoleConsumer: Does it make sense to provide an option (through formatter args to NewLineMessageFormatter) to also
print the key?

Nulls vs. options:
I'm not sure if we discussed this much on the mailing list as far as coding convention goes, but there are some places
where I personally prefer options over nulls. E.g., KeyedMessage - using null forces the programmer to use hasKey/check
for null. If something is missed, it could result in an NPE @ runtime. Options would catch these at compile time.

One problem is that options are unavailable to Java, but that can be handled by providing a javaapi class that does
use option.

ProducerSendThread: can use ArrayBuffer instead of ListBuffer.

Partitioner.scala
Why is it *required* to have a verifiable props in the constructor? E.g., why should DefaultPartitioner have to take a
props argument?
;;;","21/Nov/12 18:51;jkreps;Hey Joel, those are good points. Here is a follow-up patch that adresses these issues. Specifically:
- KeylessMessageDecoder makes no sense it was a hold over from an older, lower impact approach to defining decoder that would have let us leave the consumer api more or less unchanged. It is irrelevant now. Deleted it.
- This does make sense. I had planned to leave this as a follow-up future item since it is essentially a new feature. However it is not hard. I implemented this on both the producer and consumer side.
- Null vs Options. This is controversial but I am one of the ones who think option is not so good for us. Two reasons: (1) You have to create a new object and if the value is a number, box it, (2) the resulting match statement is a lot less readable than if(x == null) imho. It is true that NullPointerExceptions are a drag but in my experience these are just not that common in our kind of software as a production issue because our testing and load is relatively heavy so these issues are flushed out very quickly.
- ProducerSendThread: good call, changed.
- Yeah I thought about this. I basically just thought it was simpler. The complication is what do you do if there is both a no-arg constructor and one that takes options--the user has to know which we will invoke. Seems simpler to just require the argument even if you don't use it.;;;","21/Nov/12 21:03;jjkoshy;Thanks a lot for addressing the above. Changes look good but for a few typos:

- misspelling (seperat*)
- NewLineMessageFormatter may as well be called DefaultMessageFormatter given the arbitrary message separator.
- ConsoleProducer - valueEncoderClass should use valueEncoderOpt (line 93).

On null vs. options, I agree that it is cumbersome to use and generally try to avoid it, but lately I have begun to think it's not too bad especially for cases like this where it is easy to forget to check for null. E.g., there's if (xOpt.isDefined) which is only slightly more verbose than if (x != null). 
or xOpt.foreach(process(_)) (or map if you need the result) instead of (x != null) process(x) removes the need for the isDefined check/match statement and is not much more verbose.

All that said, you are right that something like this would be caught early on - so either way works.
;;;","22/Nov/12 05:26;jkreps;Nice catches, fixed.;;;","25/Nov/12 07:48;initialcontext;I like it!;;;","26/Nov/12 19:24;jjkoshy;+1 - although can you fix the typo?
** if(props.containsKey(""key.seperator""))
      keySeparator = props.getProperty(""key.separator"").getBytes
** if(props.containsKey(""line.seperator""))
      lineSeparator = props.getProperty(""line.separator"").getBytes;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metadata request from DefaultEventHandler.handle repeats same topic over and over,KAFKA-543,12610179,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jkreps,jkreps,jkreps,04/Oct/12 03:16,16/Oct/12 04:29,14/Jul/23 05:39,10/Oct/12 04:25,0.8.0,,,,,,,,,,,,,0,bugs,,,"It looks like we are calling BrokerPartitionInfo.updateInfo() with a list of the same topic repeated many times:

Here is the line:
Utils.swallowError(brokerPartitionInfo.updateInfo(outstandingProduceRequests.map(_.getTopic)))

The outstandingProduceRequests can (and generally would) have many entries for the same topic.

For example if I use the producer performance test with the default batch size on a topic ""test"" my metadata request will have the topic ""test"" repeated 200 times. On the server side we do several zk reads for each of these repetitions.

This is causing the metadata api to timeout in my perf test periodically.

I think the fix is simply to de-duplicate prior to the call (and perhaps again on the server in case of a misbehaving client).",,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/12 18:04;jkreps;KAFKA-543-v2.patch;https://issues.apache.org/jira/secure/attachment/12548434/KAFKA-543-v2.patch","04/Oct/12 18:35;jkreps;KAFKA-543.patch;https://issues.apache.org/jira/secure/attachment/12547795/KAFKA-543.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,240612,,,Wed Oct 10 04:25:31 UTC 2012,,,,,,,,,,"0|i013wf:",4401,,,,,,,,,,,,,,,,,,,,"04/Oct/12 18:35;jkreps;This patch dedupes in DefaultEventHandler which fixes the issue. It also proactively dedupes in KafkaApis to ensure the server is defensive.

I could semi-reliably reproduce SocketTimeoutExceptions during  perf tests on my local machine always on the get metadata request. This patch seems to fix these (since it was sporadic it is hard to be sure, but it hasn't happened with the change).;;;","04/Oct/12 19:01;junrao;Thanks for the patch. Good catch. +1;;;","09/Oct/12 16:15;nehanarkhede;Good catch. It seems like it is useful that updateInfo takes in a Set instead of a Seq. This will avoid any future accidental duplicate topics bug and also avoids the conversion from Seq -> Set -> Seq.;;;","09/Oct/12 16:25;jkreps;That was what I originally started to do. I ended up going this route because: (1) It was a more surgical change and (2) I feel we should avoid logic like deduplication in the RPC format objects, these should stupidly turn objects into bytes with processing done elsewhere. I thought the double copy was okay since this is functionality off the main data path. Let me know what you think, I can try it the other way if you like and we can compare.;;;","09/Oct/12 16:34;nehanarkhede;Could you explain (1) a little more ?  What I think is that if any correct usage of this API needs the input to be deduped, there are two choices - (1) Have this API do the de-duping (2) Have it input a Set so that callers will definitely dedup. Right now, it still ends up taking a Seq as input and every caller has to remember to dedup the sequence, which is dangerous. ;;;","09/Oct/12 18:04;jkreps;Hey Neha, I actually kind of agree with you. We need to dedupe on the server since we can't trust the client. There are three ways to do the client side dedupe: in the request object, in the call, or in AdminUtils. I don't like changing the request object because I think the list should map directly to the serialized data (i.e same order). I agree that fixing AdminUtils is preferable to doing it at the call site.

Here is a second patch. It does the following:
1. Make AdminUtils.getTopicMetaDataFromZK() and BrokerPartitionInfo.updateInfo take a set instead of a list. Also dedupe on the server side just in case.
2. Cleanup: Rename AdminUtils.getTopicMetaDataFromZK to AdminUtils.fetchTopicMetadataFromZk. This is consistent with our capitalization of TopicMetadata. I also dislike the naming of methods as getX since it sounds like they are a getter when in fact they do remote requests.
3. Cleanup: add a single topic version of the API since the vast majority of uses were fetching only a single topic. Reimplement the mutli-topic api in terms of the single topic api.;;;","09/Oct/12 21:21;nehanarkhede;Looks good. +1;;;","10/Oct/12 04:25;jkreps;Committed v2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade to metrics 3.x,KAFKA-542,12610173,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jjkoshy,jjkoshy,jjkoshy,04/Oct/12 00:52,09/Oct/12 21:29,14/Jul/23 05:39,09/Oct/12 21:29,0.8.0,,,,,,,,,,,,,0,,,,"metrics 3.0.0 fixes some issues with the csv reporter that affect 0.8

E.g., with metrics 2.1.3 we would just have NumDelayedRequests.csv but with 3.0.0 we would have kafka.server.ProducerRequestPurgatory.NumDelayedRequests.csv and kafka.server.FetchRequestPurgatory.NumDelayedRequests.csv
",,jjkoshy,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-541,,,,"04/Oct/12 01:32;jjkoshy;KAFKA-542-v1.patch;https://issues.apache.org/jira/secure/attachment/12547657/KAFKA-542-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,240552,,,Tue Oct 09 21:29:29 UTC 2012,,,,,,,,,,"0|i013av:",4304,,,,,,,,,,,,,,,,,,,,"04/Oct/12 01:32;jjkoshy;Patch overview:

1 - I sent a mail to metrics-user group asking when 3.0.0 will be made
  available in mvn central. For now, I just did a local build off this:
  https://github.com/codahale/metrics.git -
  10ccc80c0f574f104c4745a1ffa24af0ea92efbf
  Added metrics-annotation-3.0.0-10ccc80c0.jar
  metrics-core-3.0.0-10ccc80c0.jar to core/lib (which cannot be included in
  the diff).
2 - Made couple of changes due to slight differences in 3.0.0 API.
3 - Fixed a small bug in the CSV reporter startup.
4 - Provided a sample config in server.properties.
;;;","04/Oct/12 17:29;jjkoshy;I heard back from metrics-user that there is no immediate plan to release 3.0.0, so I think we should move forward with this change.;;;","09/Oct/12 17:35;nehanarkhede;+1. Thanks for the patch !;;;","09/Oct/12 21:29;jjkoshy;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use metrics CSV reporter instead of jmx tool for system tests,KAFKA-541,12610167,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yeyangever,jjkoshy,jjkoshy,04/Oct/12 00:09,25/Oct/12 01:21,14/Jul/23 05:39,23/Oct/12 01:28,0.8.0,,,,,,,,,,,,,0,,,,"The existing system test framework spawns off a bunch of jmxtool processes to collect metrics. This is rather heavy-weight and also, requires advance knowledge of all the beans (many of which are dynamically registered). E.g., per-topic stats pop-up only after the topics are produced to.

Since we are using metrics-core, we can just turn on the CSV reporter to collect these stats. I had originally thought version 2.1.3 had various bugs that rendered it unusable for CSV reporter, but I gave it another try and it seems to be fine. Will post some output.",,jjkoshy,junrao,nehanarkhede,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-542,,"18/Oct/12 07:24;yeyangever;kafka_541_v1.diff;https://issues.apache.org/jira/secure/attachment/12549652/kafka_541_v1.diff","21/Oct/12 07:18;yeyangever;kafka_541_v2.diff;https://issues.apache.org/jira/secure/attachment/12550171/kafka_541_v2.diff","22/Oct/12 23:59;yeyangever;kafka_541_v3.diff;https://issues.apache.org/jira/secure/attachment/12550377/kafka_541_v3.diff","22/Oct/12 23:59;yeyangever;metrics-annotation-3.0.1.jar;https://issues.apache.org/jira/secure/attachment/12550376/metrics-annotation-3.0.1.jar","22/Oct/12 23:59;yeyangever;metrics-core-3.0.1.jar;https://issues.apache.org/jira/secure/attachment/12550375/metrics-core-3.0.1.jar",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,240553,,,Thu Oct 25 01:21:49 UTC 2012,,,,,,,,,,"0|i013b3:",4305,,,,,,,,,,,,,,,,,,,,"04/Oct/12 00:14;jjkoshy;Some sample output:

https://gist.github.com/3830723
;;;","04/Oct/12 00:50;jjkoshy;Actually, now I remember. metrics 3.0.0 fixes this issue: https://github.com/codahale/metrics/pull/225

It affects us because we have different scopes for similar metrics. e.g., NumDelayedRequests in both FetchRequestPurgatory and ProducerRequestPurgatory, so they collide.

E.g., with metrics 2.1.3 we would just have NumDelayedRequests.csv but with 3.0.0 we would have kafka.server.ProducerRequestPurgatory.NumDelayedRequests.csv and kafka.server.FetchRequestPurgatory.NumDelayedRequests.csv

I'll file a separate jira to upgrade to 3.* as this one is for getting our system test framework to consume the generated csv files.
;;;","04/Oct/12 01:29;jjkoshy;Also, FYI, here are the configs to add to a server to get csv reporting enabled on startup. It can also be started up through the JMX operation if not enabled at start-up.

kafka.metrics.polling.interval.secs=5
kafka.metrics.reporters=kafka.metrics.KafkaCSVMetricsReporter
kafka.csv.metrics.dir=kafka_metrics
kafka.csv.metrics.reporter.enabled=true
;;;","04/Oct/12 02:58;nehanarkhede;+1, this will be great to have. Also, even with the upgrade, we still need a way to filter which attributes we want to plot per mbean for system tests and organize the graphs in dashboards per role. I think we can use the existing metrics.json to specify this, unless you are suggesting to change that ?;;;","04/Oct/12 17:28;jjkoshy;That's right - we would just use metrics.json, although instead of a bean it would specify the name of the CSV file, and some of the column names may be different.;;;","18/Oct/12 07:24;yeyangever;
1. Currently, only broker supports csv reporter, we added its support to producer and consumer 

2. In system test, disable the 

3. the output of the csvReporter is not quite the same as JMX metrics collection's, some lines of output file may be empty line (we tried to avoid them), the header of csv files are different, we added some mapping in kafka_system_test_utils.py to discern them.

4. changed the ""generate_overriden_props_files"" to add csv supporting properties to the broker config


NOT DONE YET:
the producer_performance and console_consumer don't support CSV reporter yet.. it's hard to do because they don't support properties file as input, instead they take the configuration directly from the command line. Will think about how to make they support;;;","19/Oct/12 15:53;junrao;Thanks for the patch. Some comments:

1. I see the following error while running run_sanity.sh in system test. 
2012-10-19 07:52:05,210 - ERROR - ERROR while plotting graph /home/jrao/Intellij_workspace/kafka_0.8_203/system_test/replication_testsuite/testcase_1/dashboards/broker/kafka.server:type=BrokerTopicMetrics,name=AllTopicsBytesInPerSec:OneMinuteRate.svg: float division (metrics)

2. kafka_system_test_utils.py: The following line doesn't look right and there are multiple instances of this.
+                    addedCSVConfig[""kfka.metrics.polling.interval.secsafka.csv.metrics.reporter.enabled""] = ""true""

3. We probably shouldn't register the metrics reporter in Producer and ZookeeperConsumerConnector since a single jvm can create multiple instances of each. Instead, we could make a util that registers the metrics reporter and invoke it in standalone tools like ProducerPerformance and ConsoleConsumer.;;;","21/Oct/12 07:18;yeyangever;
Changes since first patch:

1. make the kafka csv metrics reporter a singleton --- only one instance in one JVM

2. adding csv metrics reporter support for producer / consumer /producer-performance / console-consumer

3. fix a few places in the system test to make it actually working --- with producer_performance and console-consumer, and plotting graph correctly

4. there's only one problem left : some of the metrics are of type ""time"", according to the doc, timer reports data as  both a Meter and Histagram. (this is also verified in the JMX console)

But for the exported CSV file, we only see the header like: 
# time,min,max,mean,median,stddev,95%,99%,99.9%

That's to say, we cannot find the Meter / rate related metrics.

For a fiew graphs requiring ""One minute rate"" from a timer metric (and only for them), the data are not reported and graphs are not plotted
;;;","22/Oct/12 23:59;yeyangever;
changes since v2:

1. patch the coda hale metrics project and build new jars, which fixes the csvReporter not reporting meter attributes for timer metrics problem

2. fix the metric.py by not passing non-existent files into inputCsvFiles of plot_graphs() function

3. change the metrics in kafka RequestMetrics class from in unit of nano seconds to milli seconds

4. build a getter for ""props"" attribute in ConsumerConfig, 

5. making the KafkaCSVMetricsReporter a singleton, only one instance can be started within each JVM;;;","23/Oct/12 01:28;nehanarkhede;Committed v4 after making the following changes to v3 -

1. Fixed a logging statement in metrics.py since the newly added statement threw a runtime error
2. Fixed KafkaProject.scala to point to the right metrics package version (3.0.1)
3. Fixed the ConsumerConfig constructor argument to be a val and removed the getter API added in v3;;;","23/Oct/12 17:33;jjkoshy;Delayed review, but nevertheless:

The KafkaCSVMetricsReporter object may be better named as KafkaMetricsReporter since startCSVMetricsReporter
potentially starts up other (non-CSV) reporters (if any) as well - in which case KafkaMetricsReporter.scala would be a
better place for it. Or, you can just filter out non-CSV reporters.

Also, the top-level/config/server.properties need not enable the csv reporter. I thought the system test replication
suite's server.properties would need to be patched, but it isn't. Does this mean the test suite picks up the top-level
config as a template?

I can take care of the above in some other (unrelated) jiras that I have on my plate. However:

Can you please attach the metrics patch to fix the Timer CSV reporting? and ideally, submit a pull-request to the
metrics project? We should get off our patched version of metrics as soon as metrics-core@github is patched. (I had put
the metrics github hash in the previous jar so we know exactly which version of the code we are on.)
;;;","25/Oct/12 01:21;jjkoshy;Looks like your pull request is in. I'll file another jira to do the above.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log.append() should halt on IOException,KAFKA-540,12610152,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,junrao,junrao,03/Oct/12 22:34,04/Oct/12 03:47,14/Jul/23 05:39,04/Oct/12 03:47,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,,,"See the following entry in the broker log in a system test run. We interrupted the ReplicaFetcherThread during shutdown. However, log.append halts the system when we hit the interrupted exception. The fix is not to halt the system in log.append and just pass on the exception. The caller can decide what to do.

[2012-10-03 15:08:53,124] FATAL [Kafka Log on Broker 2], Halting due to unrecoverable I/O error while handling producer request (kafka.log.Log)
java.nio.channels.ClosedByInterruptException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)
        at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:203)
        at kafka.message.ByteBufferMessageSet.writeTo(ByteBufferMessageSet.scala:128)
        at kafka.log.FileMessageSet.append(FileMessageSet.scala:155)
        at kafka.log.LogSegment.append(LogSegment.scala:60)
        at kafka.log.Log.liftedTree1$1(Log.scala:282)
        at kafka.log.Log.append(Log.scala:270)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:42)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:105)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:98)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:98)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)
(",,jkreps,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/12 23:39;junrao;kafka-540.patch;https://issues.apache.org/jira/secure/attachment/12547630/kafka-540.patch","04/Oct/12 01:16;junrao;kafka-540_v2.patch;https://issues.apache.org/jira/secure/attachment/12547653/kafka-540_v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,240606,,,Thu Oct 04 03:47:36 UTC 2012,,,,,,,,,,"0|i013v3:",4395,,,,,,,,,,,,,,,,,,,,"03/Oct/12 23:39;junrao;Attach a patch to fix the issue.;;;","04/Oct/12 00:12;jkreps;This is a good change--calling exit in the lower level code is kind of ugly so it is nice to fix that. Two comments:
1. Can we name the exception something more meaningful. The problem with IOException is that it covers pretty much everything under the sun, so KafkaIOException isn't much better. I think specifically the error we are calling out is some kind of disk access error so how about DiskAccessException?
2. Note that System.exit is not the same as Runtime.halt()--which does scala's exit do? Is there a reason it called halt() before? Using exit() means the shutdown handlers will run. In general this is good, but we need to make sure  may or may not be what we don't write out the clean shutdown file, since that would skip recovery on the restart.;;;","04/Oct/12 01:16;junrao;Attach patch v2.

1. changed to KafkaStorageException.

2. I think we should call halt on real IO exception. This will make sure that kafkaserver.shutdown is not called and will force us to do log recovery on restart, which is the right thing to do. Not sure what scala exit does, but it's being deprecated anyway. 

Also, removed calling exit in handleOffsetRequest. It feels weird to shut down a server on a read only operation.
;;;","04/Oct/12 01:39;jkreps;+1;;;","04/Oct/12 03:47;junrao;Thanks for the review. Committed 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replica.hw should be initialized to the smaller of checkedpointed HW and log end offset,KAFKA-539,12609711,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,junrao,junrao,01/Oct/12 04:05,09/Oct/12 05:18,14/Jul/23 05:39,09/Oct/12 05:18,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,,,"Currently, replica.hw is always initialized to checkedpointed HW. However, on unclean shutdown, log end offset could be less than checkedpointed HW.",,junrao,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/12 21:24;yeyangever;kafka_539_v1.diff;https://issues.apache.org/jira/secure/attachment/12548299/kafka_539_v1.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241602,,,Tue Oct 09 05:18:10 UTC 2012,,,,,,,,,,"0|i0297j:",11093,,,,,,,,,,,,,,,,,,,,"08/Oct/12 21:24;yeyangever;
I only find one place to patch;;;","09/Oct/12 05:18;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
expose clientId and correlationId in ConsumerConfig,KAFKA-537,12609555,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yeyangever,junrao,junrao,28/Sep/12 15:59,16/Oct/12 17:31,14/Jul/23 05:39,16/Oct/12 17:31,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,newbie,,"We need to expose clientId and correlationId in ConsumerConfig and use it properly in AbstractFetcherThread. For follower fetchers, we should set the clientId to a special string, something list ""follower"".",,jkreps,junrao,nehanarkhede,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/12 19:39;yeyangever;kafka_537_v1.diff;https://issues.apache.org/jira/secure/attachment/12548444/kafka_537_v1.diff","12/Oct/12 01:25;yeyangever;kafka_537_v2.diff;https://issues.apache.org/jira/secure/attachment/12548841/kafka_537_v2.diff","15/Oct/12 20:28;yeyangever;kafka_537_v3.diff;https://issues.apache.org/jira/secure/attachment/12549198/kafka_537_v3.diff",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241601,,,Tue Oct 16 17:31:30 UTC 2012,,,,,,,,,,"0|i0297b:",11092,,,,,,,,,,,,,,,,,,,,"01/Oct/12 21:07;junrao;In this jira, we can probably also log the client id when hit unexpected exceptions in the server.;;;","09/Oct/12 19:44;jkreps;The correlation id is meant to be a per-connection, per-client, per-request counter. This lets the client have multiple in-flight requests at the same time on the same connection. So it should not be exposed in the client config as a static value. Instead we should initialize an AtomicInteger to 0 and pass in counter.getAndIncrement() to the request on each call.;;;","11/Oct/12 04:36;nehanarkhede;Agree with Jay here. I assume this is to enable special consumers that can fetch from any replica and to be able to tell external consumers from followers. Wouldn't the clientId suffice ?;;;","11/Oct/12 04:44;yeyangever;To enable special consumer to fetch from any replica, we don't use client id, we use the replicaId in the fetch request. (-1 means from ordinary consumer, -2 from low level consumer (that can fetch from any replica, n>=0 means from broker n)

Client id is currently used for identify each difference service. (client of kafka)

correlation id is currently not used. We can discuss how to use it tmr

;;;","12/Oct/12 01:25;yeyangever;
As suggested by Jay and Neha, I added a ""AtomicInteger"" field in FetchRequestBuilder to indicate and maintain the correlationId. ;;;","12/Oct/12 01:54;nehanarkhede;Forgive me for asking a pretty basic question, what is the purpose of this JIRA and what problem is it trying to solve ?;;;","12/Oct/12 02:26;yeyangever;By exposing clientId to ConsumerConfig, different consumers can specify their own clientId's. So that we will be able to log correct client of kafka (various kind of services)

Also this jira makes the correlationId meaningful  (it's not used before). This is supposed to indicate a request-response correlation between a client and the server;;;","12/Oct/12 02:32;nehanarkhede;Got it, thanks for the explanation, makes it easier to review ! ;;;","12/Oct/12 03:44;nehanarkhede;v2 doesn't apply cleanly, probably due to the KAFKA-432 and KAFKA-510 checkin. I'm afraid this patch needs a rebase. Thanks for your patience !;;;","12/Oct/12 16:19;nehanarkhede;Until you rebase, I reviewed v2 on an older svn review. Here are some review comments -

1. ConsumerConfig

1. Every config value has a comment explaining its purpose. Please can you add this for the new config ""clientid"" ?
2. I wonder if specifying a default value of empty string is useful ? Maybe groupid makes more sense or no default at all.

2. FetchRequest

2.1 According to coding convention, every constant is in camel case with first letter capital. Please can you change replicaFetcherClientId to ReplicaFetcherClientId ?

3. KafkaApis

3.1 Fix the typo - private def appendToLocalLog(producerRquest: ProducerRequest): Iterable[ProduceResult] = {
3.2 Let's fix the requestLogger to print the clientId. This allows us to have a meaningful access log for Kafka where requests can be traced back to client ids.

4. ReplicaFetcherThread

4.1 Every replica's clientId is ""replica fetcher"". I wonder if we can add the replica's id/host/both to the replica's client id. This will allow us to differentiate amongst requests from the various replicas at the leader.

5. FetchRequestBuilder

Right now, we are depending on the user to not re-create the FetchRequestBuilder each time. How about making the builder a singleton so that is not possible ? That way, we can guarantee that it is initialized to 0 only one per client.
;;;","15/Oct/12 20:28;yeyangever;Thanks for the comments, to address then:

1. ConsumerConfig
1. Every config value has a comment explaining its purpose. Please can you add this for the new config ""clientid"" ?

--- added

2. I wonder if specifying a default value of empty string is useful ? Maybe groupid makes more sense or no default at all.

--- Agree, empty string is not meaningful at all. So what about set it default to groupid, also this empty string ""DefaultClientId"" is kept and used as default value of FetchRequest and FetchRequestBuilder. (This default value is only used in testing, when used by real consumer, the clientId will be set to the clientId of the consumer config


2. FetchRequest

2.1 According to coding convention, every constant is in camel case with first letter capital. Please can you change replicaFetcherClientId to ReplicaFetcherClientId ?

--- changed


3. KafkaApis

3.1 Fix the typo - private def appendToLocalLog(producerRquest: ProducerRequest): Iterable[ProduceResult] = {

--- new patch does not affect KafkaApis.scala now, it should be addressed in other patches


3.2 Let's fix the requestLogger to print the clientId. This allows us to have a meaningful access log for Kafka where requests can be traced back to client ids.

--- in requestLogger.trace(""Handling fetch request "" + fetchRequest.toString), the fetchRequest.toString should also contain the clientId, so I think we don't need do it separately 




4. ReplicaFetcherThread

4.1 Every replica's clientId is ""replica fetcher"". I wonder if we can add the replica's id/host/both to the replica's client id. This will allow us to differentiate amongst requests from the various replicas at the leader.

--- added broker host and port in the clientId


5. FetchRequestBuilder

Right now, we are depending on the user to not re-create the FetchRequestBuilder each time. How about making the builder a singleton so that is not possible ? That way, we can guarantee that it is initialized to 0 only one per client.

--- We cannot make it a singleton because the fetchMap within each request builder cannot be shared
;;;","16/Oct/12 17:31;nehanarkhede;Committed the v3 patch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Significant difference in time taken to produce messages between 1, -1 for request-num-acks",KAFKA-535,12609464,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,jfung,jfung,27/Sep/12 22:31,20/Mar/14 21:44,14/Jul/23 05:39,20/Mar/14 21:44,,,,,,,,,,,,,,0,replication-testing,,,"There is a significant difference in time taken for ProducerPerformance to produce messages between 1 & -1 for request-num-acks.

The following are the log4j messages from ProducerPerformance with consequent calls from the system test script.

** Please note the time elapsed in consequent timestamps of calling ProducerPerformance.

The overall test scenarios:
1. This test is set up to have 1 zookeeper, 1 broker cluster of 6 nodes (distributed systems, non-local), replica factor 6, 1 topic, 1 partition
2. The script will wait for ProducerPerformance to complete sending all messages (500 each call in this case) before calling the producer again. 


1. request-num-acks = -1. The rate is about 10 messages per second. The timestamp indicates that it takes 60+ seconds for ProducerPerformance to completely sending 500 messages and exit by itself.

2012-09-26 21:20:56,102 - INFO - #### [producer thread] status of stopBackgroundProducer : [False] => producing [500] messages with starting message id : [0] (kafka_system_test_utils)
2012-09-26 21:20:56,102 - DEBUG - executing command: [ssh host0996.mydomain 'JAVA_HOME=/export/apps/jdk/JDK-1_6_0_21 JMX_PORT=9997 /kafka_pst_wip/bin/kafka-run-class.sh kafka.perf.ProducerPerformance --broker-list host0997.mydomain:9091,host0998.mydomain:9092,host0999.mydomain:9093,host1000.mydomain:9094,host1001.mydomain:9095,host1002.mydomain:9096 --initial-message-id 0 --messages 500 --topic test_1 --threads 5 --compression-codec 0 --message-size 500 --request-num-acks -1   >> /kafka_pst_wip/system_test/replication_testsuite/testcase_0001/logs/producer_performance-7/producer_performance.log  & echo pid:$! > /kafka_pst_wip/system_test/replication_testsuite/testcase_0001/logs/producer_performance-7/entity_7_pid'] (kafka_system_test_utils)
. . .
2012-09-26 21:22:00,162 - INFO - #### [producer thread] status of stopBackgroundProducer : [False] => producing [500] messages with starting message id : [500] (kafka_system_test_utils)
2012-09-26 21:22:00,162 - DEBUG - executing command: [ssh host0996.mydomain 'JAVA_HOME=/export/apps/jdk/JDK-1_6_0_21 JMX_PORT=9997 /kafka_pst_wip/bin/kafka-run-class.sh kafka.perf.ProducerPerformance --broker-list host0997.mydomain:9091,host0998.mydomain:9092,host0999.mydomain:9093,host1000.mydomain:9094,host1001.mydomain:9095,host1002.mydomain:9096 --initial-message-id 500 --messages 500 --topic test_1 --threads 5 --compression-codec 0 --message-size 500 --request-num-acks -1   >> /kafka_pst_wip/system_test/replication_testsuite/testcase_0001/logs/producer_performance-7/producer_performance.log  & echo pid:$! > /kafka_pst_wip/system_test/replication_testsuite/testcase_0001/logs/producer_performance-7/entity_7_pid'] (kafka_system_test_utils)


2. request-num-acks = 1. The rate is about 150 ~ 200 messages per second. The timestamp indicates that it takes < 3 seconds for ProducerPerformance to completely sending 500 messages.

2012-09-26 21:29:23,698 - INFO - #### [producer thread] status of stopBackgroundProducer : [False] => producing [500] messages with starting message id : [500] (kafka_system_test_utils)
2012-09-26 21:29:23,698 - DEBUG - executing command: [ssh host0996.mydomain 'JAVA_HOME=/export/apps/jdk/JDK-1_6_0_21 JMX_PORT=9997 /kafka_pst_wip/bin/kafka-run-class.sh kafka.perf.ProducerPerformance --broker-list host0997.mydomain:9091,host0998.mydomain:9092,host0999.mydomain:9093,host1000.mydomain:9094,host1001.mydomain:9095,host1002.mydomain:9096 --initial-message-id 500 --messages 500 --topic test_1 --threads 5 --compression-codec 0 --message-size 500 --request-num-acks 1   >> /kafka_pst_wip/system_test/replication_testsuite/testcase_0002/logs/producer_performance-7/producer_performance.log  & echo pid:$! > /kafka_pst_wip/system_test/replication_testsuite/testcase_0002/logs/producer_performance-7/entity_7_pid'] (kafka_system_test_utils)
. . .
2012-09-26 21:29:26,576 - INFO - #### [producer thread] status of stopBackgroundProducer : [False] => producing [500] messages with starting message id : [1000] (kafka_system_test_utils)
2012-09-26 21:29:26,577 - DEBUG - executing command: [ssh host0996.mydomain 'JAVA_HOME=/export/apps/jdk/JDK-1_6_0_21 JMX_PORT=9997 /kafka_pst_wip/bin/kafka-run-class.sh kafka.perf.ProducerPerformance --broker-list host0997.mydomain:9091,host0998.mydomain:9092,host0999.mydomain:9093,host1000.mydomain:9094,host1001.mydomain:9095,host1002.mydomain:9096 --initial-message-id 1000 --messages 500 --topic test_1 --threads 5 --compression-codec 0 --message-size 500 --request-num-acks 1   >> /kafka_pst_wip/system_test/replication_testsuite/testcase_0002/logs/producer_performance-7/producer_performance.log  & echo pid:$! > /kafka_pst_wip/system_test/replication_testsuite/testcase_0002/logs/producer_performance-7/entity_7_pid'] (kafka_system_test_utils)
",,jfung,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241636,,,Thu Mar 20 21:44:01 UTC 2014,,,,,,,,,,"0|i029gv:",11135,,,,,,,,,,,,,,,,,,,,"20/Mar/14 21:44;jkreps;I don't think this is a bug and is in any way fixed in the new producer.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple controllers can co-exist during soft failures,KAFKA-532,12609099,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,25/Sep/12 18:34,04/Dec/12 23:44,14/Jul/23 05:39,18/Nov/12 22:49,0.8.0,,,,,,,,,,,,26/Sep/12 00:00,0,bugs,,,"If the current controller experiences an intermittent soft failure (GC pause) in the middle of leader election or partition reassignment, a new controller might get elected and start communicating new state change decisions to the brokers. After recovering from the soft failure, the old controller might continue sending some stale state change decisions to the brokers, resulting in unexpected failures. We need to introduce a controller generation id that increments with controller election. The brokers should reject any state change requests by a controller with an older generation id.",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/12 17:01;nehanarkhede;kafka-532-v1.patch;https://issues.apache.org/jira/secure/attachment/12547011/kafka-532-v1.patch","03/Oct/12 03:36;nehanarkhede;kafka-532-v2.patch;https://issues.apache.org/jira/secure/attachment/12547490/kafka-532-v2.patch","31/Oct/12 22:43;nehanarkhede;kafka-532-v3.patch;https://issues.apache.org/jira/secure/attachment/12551639/kafka-532-v3.patch","02/Nov/12 17:42;nehanarkhede;kafka-532-v4.patch;https://issues.apache.org/jira/secure/attachment/12551891/kafka-532-v4.patch","17/Nov/12 19:57;nehanarkhede;kafka-532-v5.patch;https://issues.apache.org/jira/secure/attachment/12553913/kafka-532-v5.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241599,,,Sun Nov 18 22:49:35 UTC 2012,,,,,,,,,,"0|i0296v:",11090,,,,,,,,,,,,,,,,,,,,"28/Sep/12 17:01;nehanarkhede;Introduced a controller generation/epoch that increments after a successful controller election

Changes include -
1. Guard zookeeper writes by the controller with the controller epoch. This includes initializing a leader/isr path, e
lecting leader for a partition and shrinking the isr for a partition
2. Include controller epoch in the state change requests sent to the broker
3. Include logic to discard state change requests with a stale controller epoch on the brokers

Testing
LeaderElectionTest: Added a unit test to send leader/isr request with a stale controller epoch and check if the bro
ker discards the request and sends back the appropriate error code (StaleControllerEpochCode)
;;;","01/Oct/12 20:58;junrao;Thanks for patch v1. Some comments:
1. PartitionStateMachine.electLeaderForPartition(): We throw an exception if controllerEpoch != controller.epoch. It seems that we should only throw an exception if controllerEpoch > controller.epoch. The controllerEpoch in the leaderAndIsr path in ZK indicates the epoch of the controller when leader was changed last time. It's ok for a leader not to be changed during multiple generations of the controller. What we need to prevent is that if a newer generation of a controller has already changed the leader, the older generation of the controller can't touch the leaderAndIsr path any more. Also, it's not clear to me if we need the var retry. Ditto in ReplicaStateMachine.handleStateChange().

2. ZkUtils.getLeaderAndIsrForPartition(): Instead of returning a tuple, could we return a LeaderAndIsrInfo object?

3. ZookeeperLeaderElector: We keep the controller epoch in the value of the controller path. The problem is that the controller path is ephemeral. During a controller failover, the controller path will be gone and we won't be able to obtain the last controller epoch. We will need to store the controller epoch in a separate persistence path. That will probably complicate things a bit more.

4. LeaderAndIsRRequest,StopReplicaRequest sizeInBytes(): When computing size, could we add a comment for each number which field is it for?

5. Partition.updateISR(): If we want to keep the semantics that the controllerEpoch in the leaderIsrPath is the epoch of the controller when the leader is changed, we need to use the controller epoch in the current leaderIsrPath when updating ISR, not the latest controller epoch this broker has seen.

6. ControllerChannelManager.sendRequestToBrokers: For similar reasons as the above, the controller epoch in the LeaderAndIsrRequest may not always be the epoch of the current controller (e.g., when resending the leadership info stored in ZK during controller failover).

7. ReplicaManager: We will nee to maintain a controller epoc per partition. Do we still need to maintain a global controller epoch?
;;;","03/Oct/12 03:36;nehanarkhede;Thanks for the review, Jun ! 

1, 3 That's a bug, fixed it
2. Changed it to be a case class instead of tuple
4. While adding comments, realized that there was a bug in the way we computed the size of the leader and isr request. The size had an extra 1 byte in the be
ginning, not sure if its required or not. This is probably a bug introduce in the very first version of the controller that we didn't catch during testing.

6. I'm afraid that will not solve the problem. The whole point of the controller generation is to prevent the brokers from following requests sent by a stale controller. It doesn't matter whether the controller is re-publishing the old controller's decision or making its own, once it sends the decision to the brokers, it is effectively certifying that decision to be the right one. Hence, both the leader and isr request as well as the stop replica request needs to contain the epoch of the controller sending the request.With the above semantics, the new controller should re-write the leader and isr path with its epoch after sending the leader and isr request to the brokers. However, re-writing the path during the controller failover will have performance implications on the controller failover latency. An alternative is to do this in the leader and isr response callback. Currently, we rely on asynchronous leader election to work correctly. Ideally, we need to be able to act on the event that the leader and isr response is either negative or lost. When this happens, leader election needs to be triggered again. Since this is asynchronous, we can also update the leader and isr path with the new controller's epoch on receiving a successful leader and isr response. If this sounds good, I can either make the changes in patch v3 or file another JIRA. Let me know what you prefer. Until then, the broker will re-write the zk path with the latest controller epoch, which is theoretically correct, but not semantically.    

5, 7. With the semantics mentioned above, the brokers should just write the isr with the controller epoch that it knows.
;;;","26/Oct/12 16:21;junrao;The purpose of controller epoc is to prevent an older controller from overriding the data already updated by a newer controller. What we can do is that when a controller wants to update leaderAndIsr, it first checks and makes sure that the controller epoc stored in the path is less than or equal to the current controller epoc. Otherwise, the controller won't update the path. This way, the controller epoc associated with leaderAndIsr is only updated when it's truly needed, i.e., when the controller wants to update the leader or the isr. So we don't need to rewrite leaderAndIsr during controller failover. When sending leaderAndIsr requests, we just need to send the controller epoc stored in the leaderAndIsr path.;;;","31/Oct/12 22:43;nehanarkhede;>> So we don't need to rewrite leaderAndIsr during controller failover. When sending leaderAndIsr requests, we just need to send the controller epoc stored in the leaderAndIsr path.

That still doesn't solve the problem. Here is the problem with re-publishing the previous controller's decision with an older epoch-

Let's say that controller with epoch 1 elected leaders for partitions p1 and p2. Over time, controller moves and epoch increments to 2. Let's say this controller reassigns partition p2 and re-elects leader for p2 as part of that. Now, controller with epoch 2 goes into a GC pause and controller moves to epoch 3. This new controller re-publishes leader and isr decision made by controller epoch 1 as well as epoch 2. Now, the same broker will receive leader and isr requests for 2 different epochs. So, it will reject the requests sent by epoch 1 since it already received requests with a higher epoch (2). Ignoring state change requests is dangerous and can lead to a situation where some partitions are offline.

So, the controller should use its own epoch while sending state change requests to the brokers. Precisely, here is how it will work -

1. The leader and isr path has the epoch of the controller that made the new leader/isr decision. The leader and isr path will be conditionally updated. If the conditional update fails, the controller re-reads the controller epoch value. If the epoch has changed, it knows that another controller has taken over and it aborts the state change operation.
2. Whenever a controller sends a state change request to a broker (leaderAndIsr/stopReplica), it tags the request with its own epoch. In other words, it certifies that decision to be current and correct.
3. Each broker maintains the last known highest controller epoch. The broker will reject any state change request that is tagged with a controller epoch value lower than what it knows.

Since multiple controllers during leader election is tricky, lets dive into some details -

When the controller changes leader/isr state, it first
1. conditionally updates the zookeeper path for that partition
2. sends the leader/isr request to the brokers

If the controller goes into soft failure before step #1, a new controller will get elected and it will notice that partition is offline, elect the leader and send the leader/isr request to the broker with the new epoch. When the failed controller falls out of the soft failure, it will try to update the zk path, but will fail and abort the operation.

If the controller goes into soft failure between steps 1 & 2, a new controller will get elected and will just resend failed controller's leader/isr decision to the broker using its own controller epoch. When the failed controller wakes up, it might try to send the leader/isr decision to the broker, but the broker will reject that request since it already knows a higher controller epoch

These changes were covered in patch v2, uploading v3 after rebasing.;;;","01/Nov/12 17:59;junrao;Thanks for patch v3. The overall approach seems to work. Some comments:

30. PartitionStateInfo and LeaderAndIsrRequest: When calculating size in sizeInBytes(), it's more readable if we put each number to be added in a separate line.

31. Partition.updateIsr(): I am thinking about what controllerEpoch the leader should use when updating the leaderAndIsr path. There is probably nothing wrong to use the controllerEpoch in replicaManager. However, it seems to make more sense to use the controllerEpoch in the leaderAndIsr path itself, since this update is actually not made by the controller.

32. ReplicaManager.controllerEpoch: Since this variable can be accessed from different threads, it needs to be a volatile. Also, we only need to update controllerEpoch if the one from the request is larger (but not equal). It probably should be initialized to 0 or -1?

33. LeaderElectionTest.testLeaderElectionWithStaleControllerEpoch(): I wonder if we really need to start a new broker. Can we just send a stale controller epoc using the controllerChannelManager in the current controller?

34. KafkaController: There seems to be a tricky issue with incrementing the controller epoc. We increment epoc in onControllerFailover() after the broker becomes a controller. What could happen is that broker 1 becomes the controller and goes to GC before we increment the epoc. Broker 2 becomes the new controller and increments the epoc. Broker 1 comes back from gc and increments epoc again. Now, broker 1's controller epoc is actually larger. Not sure what's the best way to address this. One thought is that immediately after controller epoc is incremented in onControllerFailover(), we check if this broker is still the controller (by reading the controller path in ZK). If not, we throw an exception. Also, epoc probably should be initialized to 0 if we want the first controller to have epoc 1.

35. We use int to represent both controller and leader epoc. There is the potential issue if the number wraps. We probably don't need to worry about it now.


;;;","02/Nov/12 00:49;nehanarkhede;>> 34. KafkaController: There seems to be a tricky issue with incrementing the controller epoc. We increment epoc in onControllerFailover() after the broker becomes a controller. What could happen is that broker 1 becomes the controller and goes to GC before we increment the epoc. Broker 2 becomes the new controller and increments the epoc. Broker 1 comes back from gc and increments epoc again. Now, broker 1's controller epoc is actually larger. Not sure what's the best way to address this. One thought is that immediately after controller epoc is incremented in onControllerFailover(), we check if this broker is still the controller (by reading the controller path in ZK). If not, we throw an exception. Also, epoc probably should be initialized to 0 if we want the first controller to have epoc 1.

Good point, I missed this earlier. But just raising an exception after writing to the zk path might not be the best solution. This is because it breaks the guarantee that the active controller has the largest epoch in the system. I can't think of an example that would lead to a bug, but it gives me a feeling that this could cause unforeseen-hard-to-debug issues in the future. On the surface, it doesn't seem to be incorrect. However, here is another solution that seems to cover the corner cases -

Every broker registers a watch on the /controllerEpoch persistent path and caches the latest controller epoch and zk version. When a broker becomes controller, it uses this cached zk version to do the conditional write. Now, in the event that another controller takes over when the current controller goes into GC after election, the new controller will use the previous controller's zk version and successfully update the zk path. When the older controller comes back, it will try to use a stale zk version and its zookeeper write will fail. It will not be able to update the new zk version before the write since they are guarded by the same lock.

If this sounds good, I will upload another patch that includes the fix
;;;","02/Nov/12 17:42;nehanarkhede;31. Partition.updateIsr(): I am thinking about what controllerEpoch the leader should use when updating the leaderAndIsr path. There is probably nothing wrong to use the controllerEpoch in replicaManager. However, it seems to make more sense to use the controllerEpoch in the leaderAndIsr path itself, since this update is actually not made by the controller.

You make a good point, I agree that it probably makes more sense to keep the decision maker's controller epoch while changing the isr. Fixed it

32. ReplicaManager.controllerEpoch: Since this variable can be accessed from different threads, it needs to be a volatile. Also, we only need to update controllerEpoch if the one from the request is larger (but not equal). It probably should be initialized to 0 or -1?

Good catch, fixed it.

33. LeaderElectionTest.testLeaderElectionWithStaleControllerEpoch(): I wonder if we really need to start a new broker. Can we just send a stale controller epoc using the controllerChannelManager in the current controller?

I just thought it makes it simpler to understand the logic if there is another broker that acts as the new controller, but you are right. I could've just hijacked the old controller's channel manager

34. KafkaController: There seems to be a tricky issue with incrementing the controller epoc. We increment epoc in onControllerFailover() after the broker becomes a controller. What could happen is that broker 1 becomes the controller and goes to GC before we increment the epoc. Broker 2 becomes the new controller and increments the epoc. Broker 1 comes back from gc and increments epoc again. Now, broker 1's controller epoc is actually larger. Not sure what's the best way to address this. One thought is that immediately after controller epoc is incremented in onControllerFailover(), we check if this broker is still the controller (by reading the controller path in ZK). If not, we throw an exception. Also, epoc probably should be initialized to 0 if we want the first controller to have epoc 1.

Implemented the fix I described earlier for this.;;;","06/Nov/12 16:04;junrao;Thanks for patch v4. A few more comments:

40. PartitionStateInfo: It seems that we need to send the controllerEpoc associated with this partition. Note that this epoc is different from the controllerEpoc in LeaderAndIsrRequest. The former is the epoc of the controller that last changed the leader or isr and will be used when broker updates the isr. The latter is the epoc of the controller that sends the request and will be used in ReplicaManager to decide which controller's decision to follow. We will need to change the controllerEpoc passed to makeLeader and makeFollower in ReplicaManager accordingly.

41. ReplicaManager: In stopReplicas() and becomeLeaderOrFollower(), it would be better to only update controllerEpoch when it's truly necessary, i.e., the new controllerEpoch is larger than the cached one (not equal). This is because updating a volatile variable is a bit expensive than updating a local variable since the update has to be exposed to other threads.

42. KafkaController: The approach in the new patch works. There are a few corner cases that we need to cover.
42.1. incrementControllerEpoch(): If the controllerEpoc path doesn't exist, we create the path using the initial epoc version without using conditional update. It is possible for 2 controllers to execute this logic simultaneously and both get the initial epoc version. One solution is to make sure the controller epoc path exists during context initialization. Then we can always use conditional update here.
42.2. ControllerContext: We need to initialize controllerEpoc by reading from ZK. We also need to make sure that we subscribe to the controllerEpoc path first and then read its value from ZK for initialization.
42.3. ControllerEpochListener: It's safer to set both the epoc and the ZK version using the value from ZkUtils.readData.

43. ControllerMovedException is missing in the patch;;;","17/Nov/12 19:57;nehanarkhede;ew more changes in this patch -

1. Changed leader and isr request to send the controller epoch that made the last change for leader/isr per partition. This is used by the broker to update the leader and isr path with the correct controller epoch. Each Partition object on a Kafka server will maintain the epoch of the controller that made the last leader/isr decision. If/when the broker changes the isr, it uses the correct value for the controller epoch, instead of using the currently active controller's epoch. Functionally, nothing bad will happen even if it uses the currently active controller's epoch (that is sent on every state change request), but semantically it will not quite be right to do so. This can happen when a previous controller has made the leader/isr decisions for partitions, while the newer controllers have merely re-published those decisions upon controller failover.
2. Changed the become controller procedure to resign as the current controller if it runs into any unexpected error/exception while making the state change to become controller. This is to ensure that the currently elected controller is actually serving as the controller.
3. LogRecoveryTest and LogText occasionally fail, but I believe they fail on our nightly build as well. Didn't attempt to fix those tests in this patch. 

Regarding Jun's review -

>> 40. PartitionStateInfo: It seems that we need to send the controllerEpoc associated with this partition. Note that this epoc is different from the controllerEpoc in LeaderAndIsrRequest. The former is the epoc of the controller that last changed the leader or isr and will be used when broker updates the isr. The latter is the epoc of the controller that sends the request and will be used in ReplicaManager to decide which controller's decision to follow. We will need to change the controllerEpoc passed to makeLeader and makeFollower in ReplicaManager accordingly.

You raise a good point here. What I missed is initializing the controller epoch for each partition. There are 2 ways to initialize it 1. zookeeper read on startup 2. Active controller sending the controller epoch of the controller that last made a leader/isr decision for that partition. I'm guessing 2. might be better from a performance perspective.

>> 41. ReplicaManager: In stopReplicas() and becomeLeaderOrFollower(), it would be better to only update controllerEpoch when it's truly necessary, i.e., the new controllerEpoch is larger than the cached one (not equal). This is because updating a volatile variable is a bit expensive than updating a local variable since the update has to be exposed to other threads.

Not sure if this is a performance win. Volatile variables are never cached in memory registers. So a read needs to reload data from memory and write needs to write data back to memory. The if statement would need to access the same volatile variable, requiring it to go to memory anyways. :)

>> 2.1. incrementControllerEpoch(): If the controllerEpoc path doesn't exist, we create the path using the initial epoc version without using conditional update. It is possible for 2 controllers to execute this logic simultaneously and both get the initial epoc version. One solution is to make sure the controller epoc path exists during context initialization. Then we can always use conditional update here.

It is not possible for 2 clients to create the same zookeeper path. This is the simplest guarantee zookeeper provides. One of the writes will fail and that controller will abort its controller startup procedure. The larger problem here is not so much that one of the writes should fail, but we need to ensure that if the failed zk operation happens to be for the latest active controller, then it will abort its controller startup procedure and the old one will lose its zookeeper session anyways

>> 42.2. ControllerContext: We need to initialize controllerEpoc by reading from ZK. We also need to make sure that we subscribe to the controllerEpoc path first and then read its value from ZK for initialization.

The controller constructor is modified to initialize the controller epoch and zk version by reading from zk and then it subscribes to controller epoch's zk path.

>> 42.3. ControllerEpochListener: It's safer to set both the epoc and the ZK version using the value from ZkUtils.readData.

You're right and there is no perfect solution to this. Ideally, the zkclient API should change to expose the version since the underlying zookeeper API exposes it. The problem is that there will always be a window after the listener has fired and before the read returns when the controller's epoch could change. There will be another listener fired, though during each listener invocation, this problem would exist. The right way is to rely on the data the listener returns to the controller. But, with this change, at least the epoch and its version will correspond to the same epoch change, so its still better.
;;;","18/Nov/12 18:06;junrao;Thanks for patch v5. Looks good. Just a couple of minor comments.

50. LeaderAndIsrResponse and StopReplicaResponse: Currently, for all types of response, we have moved to the model that there is no global error code at the response level. Instead, if a request can't be processed for any partition, we just set the same error code for each partition in the response. This achieves the same effect, but makes the handling of the response easier. One just has to deal with the error code per partition.

51. Are the changes in test/resources/log4j.properties intended?
;;;","18/Nov/12 18:52;nehanarkhede;Thanks for reviewing patch v5 quickly !

50. True, thought about it. But in both of these cases, the error code is a request level error code. It is saying that this *request* is coming from an invalid controller. It is true that for other requests, there are just partition level error codes, but I'm guessing there are error codes that make sense at the request level as well. When you get request level errors, the response is empty and the error code is set. The other option of dealing with this is, if you have a request level error code, set it as every partition's error code, and skip the request level error code. But this seemed a little hacky since now the response map will not be empty and will contain each partition with the same error code. So it felt like we are trying to retrofit something to match the currently chosen response format. I can see both sides and probably the user's convenience would help pick one option. If this discussion already happened sometime, I can make the change right away. If not, do you think its worthwhile to quickly check on the user list ? If so, I can commit this patch and make that change, whatever is decided, in a follow up.

51. Not intended, will remove on commit.;;;","18/Nov/12 22:41;junrao;50. My feeling is that request level error code conveys the same meaning that every partition fails with the same error code and my preference is to keep all response format consistent. However, if you prefer, it's ok to check in the patch as it is and revisit it when we finalize the wire format. So, +1 from me on the patch.;;;","18/Nov/12 22:49;nehanarkhede;50. Agree that the request/response formats must be consistent. Let's bring this up while finalizing the format, I will put in the change, if required, immediately.

Thanks a lot for the review, committed v5 !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka.server.ReplicaFetcherThread: java.lang.IndexOutOfBoundsException,KAFKA-529,12608935,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,jfung,jfung,24/Sep/12 22:04,01/Oct/12 22:47,14/Jul/23 05:39,01/Oct/12 22:47,,,,,,,,,,,,,,0,,,,"1. Attached file system_test_1348521165.tar.gz contains all the associated log files for this test session.

2. The system test output log can be found at: system_test_1348521165/system_test_output.log 

3. The following log message can be found at: system_test_1348521165/logs/broker-2/kafka_server_9092.log

[2012-09-24 14:14:19,601] WARN No previously checkpointed highwatermark value found for topic test_1 partition 1. Returning 0 as the highwatermark (ka
fka.server.HighwaterMarkCheckpoint)
[2012-09-24 14:14:19,604] INFO [Kafka Log on Broker 2], Truncated log segment /tmp/kafka_server_2_logs/test_1-1/00000000000000000000.kafka to target o
ffset 0 (kafka.log.Log)
[2012-09-24 14:14:19,611] INFO [ReplicaFetcherThread-1-0-on-broker-2-], Starting  (kafka.server.ReplicaFetcherThread)
[2012-09-24 14:14:19,611] INFO [ReplicaFetcherManager on broker 2, ], adding fetcher on topic test_1, partion 1, initOffset 0 to broker 1 with fetcherId 0 (kafka.server.ReplicaFetcherManager)
[2012-09-24 14:14:19,973] ERROR [ReplicaFetcherThread-1-0-on-broker-2-], Error due to  (kafka.server.ReplicaFetcherThread)
java.lang.IndexOutOfBoundsException
        at java.nio.Buffer.checkIndex(Buffer.java:512)
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:121)
        at kafka.message.Message.magic(Message.scala:119)
        at kafka.message.Message.checksum(Message.scala:132)
        at kafka.message.Message.isValid(Message.scala:144)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNextOuter(ByteBufferMessageSet.scala:118)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:149)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:89)
        at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:61)
        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:53)
        at kafka.message.ByteBufferMessageSet.verifyMessageSize(ByteBufferMessageSet.scala:79)
        at kafka.log.Log.append(Log.scala:250)
        at kafka.server.ReplicaFetcherThread.processPartitionData(ReplicaFetcherThread.scala:42)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:103)
        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:96)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:96)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)
[2012-09-24 14:14:19,975] INFO [ReplicaFetcherThread-1-0-on-broker-2-], Stopped  (kafka.server.ReplicaFetcherThread)
",,jfung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/12 22:05;jfung;system_test_1348521165.tar.gz;https://issues.apache.org/jira/secure/attachment/12546391/system_test_1348521165.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,291938,,,Mon Oct 01 22:47:40 UTC 2012,,,,,,,,,,"0|i0rhpz:",158515,,,,,,,,,,,,,,,,,,,,"01/Oct/12 22:47;jfung;This cannot be reproduced any more with the fix from KAFKA-528. So mark this FIXED.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBoundsException thrown by kafka.consumer.ConsumerFetcherThread,KAFKA-528,12608934,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,jfung,jfung,24/Sep/12 21:49,29/Sep/12 18:19,14/Jul/23 05:39,29/Sep/12 18:19,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,,,"1. Attached file system_test_1348521165.tar.gz contains all the associated log files for this test session.

2. The system test output log can be found at: system_test_1348521165/system_test_output.log

3. The following log message can be found at: system_test_1348521165/logs/console_consumer-5/console_consumer.log

[2012-09-24 14:15:12,016] ERROR [ConsumerFetcherThread-console-consumer-16186_jfung-1348521311426-2c83ced7-0-1], Error due to  (kafka.consumer.ConsumerFetcherThread)

java.lang.IndexOutOfBoundsException

        at java.nio.Buffer.checkIndex(Buffer.java:512)

        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:121)

        at kafka.message.Message.magic(Message.scala:119)

        at kafka.message.Message.checksum(Message.scala:132)

        at kafka.message.Message.isValid(Message.scala:144)

        at kafka.message.ByteBufferMessageSet$$anon$1.makeNextOuter(ByteBufferMessageSet.scala:118)

        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:149)

        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:89)

        at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:61)

        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:53)

        at kafka.message.ByteBufferMessageSet.shallowValidBytes(ByteBufferMessageSet.scala:54)

        at kafka.message.ByteBufferMessageSet.validBytes(ByteBufferMessageSet.scala:49)

        at kafka.consumer.PartitionTopicInfo.enqueue(PartitionTopicInfo.scala:54)

        at kafka.consumer.ConsumerFetcherThread.processPartitionData(ConsumerFetcherThread.scala:42)

        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:103)

        at kafka.server.AbstractFetcherThread$$anonfun$doWork$5.apply(AbstractFetcherThread.scala:96)

        at scala.collection.immutable.Map$Map2.foreach(Map.scala:127)

        at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:96)

        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:50)
",,jfung,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/12 15:42;junrao;kafka-528.patch;https://issues.apache.org/jira/secure/attachment/12546531/kafka-528.patch","29/Sep/12 02:13;junrao;kafka-528_v2.patch;https://issues.apache.org/jira/secure/attachment/12547088/kafka-528_v2.patch","24/Sep/12 21:59;jfung;system_test_1348521165.tar.gz;https://issues.apache.org/jira/secure/attachment/12546390/system_test_1348521165.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,291942,,,Sat Sep 29 18:19:40 UTC 2012,,,,,,,,,,"0|i0rhqv:",158519,,,,,,,,,,,,,,,,,,,,"25/Sep/12 15:42;junrao;Attach a patch. There are a couple of critical issues.

1. FileMessageSet.truncateTo(): There is a java bug in FileChannel.truncate. The problem is that file size is set correctly, but file position is not changed accordingly. Added a fix that manually sets the file position.

2. ZookeeperConsumerConnector: The call to get the initial offset sometimes fails since it's connected to a broker that's not the leader of a partition. Changed it to set the initial offset in ConsumerFetcherThread.

Also fixed a few other issues:
3. Partition: There is unnecessary ISR expansion.

4. SyncProducer: lastConnectionTime should be set in connect().

5. DumpLogSegment: Report if log is corrupted at the end of a file.

6. Added some more tracing code. ;;;","28/Sep/12 19:42;jkreps;+1 Lgtm.

You made the logging for leaderAndIsr request to be info level. If it is info level it would be good to rephrase that in a way folks would be able to understand, otherwise we have a log full of really esoteric things that only make sense to us.;;;","28/Sep/12 23:20;jfung;1. Tested kafka-528.patch with rev. 1389460 and there is no data loss now. The Exception specified in this JIRA is not showing.

2. However, there is still another exception showing in broker log (already filed under KAFKA-529) :

[2012-09-28 23:05:54,669] ERROR [ReplicaFetcherThread-2-0-on-broker-3], Error due to  (kafka.server.ReplicaFetcherThread)
java.lang.IndexOutOfBoundsException
        at java.nio.Buffer.checkIndex(Buffer.java:514)
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:121)
        at kafka.message.Message.magic(Message.scala:119)
        at kafka.message.Message.checksum(Message.scala:132)
        at kafka.message.Message.isValid(Message.scala:144)
;;;","29/Sep/12 02:13;junrao;Attach patch v2. Fixed one additional bug in KafkaController.

The bug is that Map((""a"",0)->0,(""a"",1)->0)).map(_,_1) returns Map(""a"" -> 1) while we expect it to be Seq((""a,0),(""a"",1)). Fixed it by using .keySet instead. 

With patch v2, our system test now passes.;;;","29/Sep/12 17:22;nehanarkhede;+1 on patch v2.

Minor comments on logging -

1. The debug statement in Replica says ""Updated highwatermark to "". It will be good to know the topic and partition as well.
2. The println in DumpLogSegment says ""Log is corrupted from... Is corrupted"". Let's fix that.

;;;","29/Sep/12 18:19;junrao;Thanks for the review. Committed to 0.8 with some minor changes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compression support does numerous byte copies,KAFKA-527,12608923,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,yasuhiro.matsuda,jkreps,jkreps,24/Sep/12 20:38,29/Mar/15 22:53,14/Jul/23 05:39,26/Mar/15 22:51,,,,,,,,,,,compression,,,2,,,,"The data path for compressing or decompressing messages is extremely inefficient. We do something like 7 (?) complete copies of the data, often for simple things like adding a 4 byte size to the front. I am not sure how this went by unnoticed.

This is likely the root cause of the performance issues we saw in doing bulk recompression of data in mirror maker.

The mismatch between the InputStream and OutputStream interfaces and the Message/MessageSet interfaces which are based on byte buffers is the cause of many of these.

",,guozhang,ivan.simonenko,jkreps,junrao,kzadorozhny,lanzaa,nehanarkhede,noslowerdna,ross.black,yasuhiro.matsuda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/13 21:30;guozhang;KAFKA-527.message-copy.history;https://issues.apache.org/jira/secure/attachment/12606449/KAFKA-527.message-copy.history","04/Mar/15 19:43;yasuhiro.matsuda;KAFKA-527.patch;https://issues.apache.org/jira/secure/attachment/12702593/KAFKA-527.patch","16/Mar/15 22:19;yasuhiro.matsuda;KAFKA-527_2015-03-16_15:19:29.patch;https://issues.apache.org/jira/secure/attachment/12704904/KAFKA-527_2015-03-16_15%3A19%3A29.patch","20/Mar/15 04:33;yasuhiro.matsuda;KAFKA-527_2015-03-19_21:32:24.patch;https://issues.apache.org/jira/secure/attachment/12705832/KAFKA-527_2015-03-19_21%3A32%3A24.patch","25/Mar/15 19:09;yasuhiro.matsuda;KAFKA-527_2015-03-25_12:08:00.patch;https://issues.apache.org/jira/secure/attachment/12707290/KAFKA-527_2015-03-25_12%3A08%3A00.patch","25/Mar/15 20:26;guozhang;KAFKA-527_2015-03-25_13:26:36.patch;https://issues.apache.org/jira/secure/attachment/12707304/KAFKA-527_2015-03-25_13%3A26%3A36.patch","03/Aug/13 04:47;jkreps;java.hprof.no-compression.txt;https://issues.apache.org/jira/secure/attachment/12595731/java.hprof.no-compression.txt","03/Aug/13 04:47;jkreps;java.hprof.snappy.text;https://issues.apache.org/jira/secure/attachment/12595732/java.hprof.snappy.text",,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241645,,,Sun Mar 29 22:53:47 UTC 2015,,,,,,,,,,"0|i029iv:",11144,,,,,,,,,,,,,,,,,,,,"03/Aug/13 05:06;jkreps;To test performance checkout trunk and do
./sbt package test:package
./bin/kafka-run-class.sh kafka.TestLinearWriteSpeed --bytes 1007483648 --files 1 --log --message-size 4096 --size 109600 --compression snappy
./bin/kafka-run-class.sh kafka.TestLinearWriteSpeed --bytes 1007483648 --files 1 --log --message-size 4096 --size 109600 --compression none

Performance (on my laptop):
none: 75 MB/sec
snappy: 18 MB/sec

Why is this so slow? After all snappy claims a roundtrip performance of around 200MB/sec...

If you look at the attached hprof traces without compression you see the following:
     1 79.87% 79.87%    2004 300968 sun.nio.ch.FileDispatcher.write0
   2  5.38% 85.25%     135 300978 kafka.utils.Utils$.crc32
   3  5.06% 90.31%     127 301074 sun.nio.ch.FileChannelImpl.force0
   4  1.79% 92.11%      45 301075 java.nio.MappedByteBuffer.force0
I.e. 80% goes to writing, 5 percent goes to computing crcs, and about 7% goes to flushing, then a long tail

If you look at the same trace with compression you expect to see that all the time goes to compressing stuff, right? Wrong:
   1 16.44% 16.44%     807 301044 org.xerial.snappy.SnappyNative.arrayCopy
   2 14.81% 31.24%     727 301073 java.util.Arrays.copyOf
   3  9.61% 40.86%     472 301053 org.xerial.snappy.SnappyNative.rawCompress
   4  7.45% 48.31%     366 301084 org.xerial.snappy.SnappyNative.rawUncompress
   5  5.60% 53.91%     275 301063 java.io.ByteArrayOutputStream.<init>
   6  5.13% 59.04%     252 301090 sun.nio.ch.FileDispatcher.write0
   7  4.32% 63.36%     212 301074 java.nio.HeapByteBuffer.<init>
   8  3.97% 67.33%     195 301049 java.util.Arrays.copyOf
   9  3.83% 71.16%     188 301070 org.xerial.snappy.SnappyNative.arrayCopy
  10  3.22% 74.38%     158 301068 org.xerial.snappy.SnappyNative.rawUncompress
  11  2.10% 76.48%     103 301065 org.xerial.snappy.SnappyNative.arrayCopy
  12  1.91% 78.39%      94 301089 java.nio.HeapByteBuffer.<init>

If you tally this up you see that about 50% of the time is going to array copying and allocation and a mere 22% going to compression with the actual cost of the write knocked down to 5%.

Pretty sad.;;;","03/Aug/13 05:11;jkreps;I should mention that to run the above test you need to apply the patch from KAFKA-615 (v5) to get the compression option and log support in the throughput test.;;;","03/Aug/13 16:35;jkreps;The general idea of the refactoring would be to allow directly appending to a ByteBufferMessageSet. Perhaps we could add a static method Message.write(byteBuffer, key, value) and a ByteBufferMessageSet.append that works in-place.

The compression codec would likely need to change from an OutputStream and InputStream to something that worked directly with byte[].

This is straight-forward for snappy but requires a little more work for gzip to get the header right since I think they only provide array access from the more generic inflate/deflate codec (see Deflater.java and GZIPOutputStream.java in the jdk).;;;","15/Aug/13 18:15;guozhang;One alternative approach would be like this:

Currently in the compression code (ByteBufferMessageSet.create), for each message we write 1) the incrementing logical offset in LONG, 2) the message byte size in INT, and 3) the message payload.

The idea is that since the logical offset is just incrementing, hence with a compressed message, as long as we know the offset of the first message, we would know the offset of the rest messages without even reading the offset field.

So we can ignore reading the offset of each message inside of the compressed message but only the offset of the wrapper message which is the offset of the last message + 1, and then in assignOffsets just modify the offset of the wrapper message. Another change would be at the consumer side, the iterator would need to be smart of interpreting the offsets of messages while deep-iterating the compressed message.

As Jay pointed out, this method would not work with log compaction since it would break the assumption that offsets increments continuously. Two workarounds of this issue:

1) In log compaction, instead of deleting the to-be-deleted-message just setting its payload to null but keep its header and hence keeping its slot in the incrementing offset.
2) During the compression process, instead of writing the absolute value of the logical offset of messages, write the deltas of their offset compared with the offset of the wrapper message. So -1 would mean continuously decrementing from the wrapper message offset, and -2/3/... would be skipping holes in side the compressed message.;;;","02/Oct/13 21:30;guozhang;Attached a file summarizing the copying operations. If we count the shallow iterating over message sets then there are about 8 copy operations throughout the life time of a message. Among them Copy 3/4/5 could be avoided, and we could only apply one shallow iteration on each broker (leader and follower) and consumer.

Given that the offset assignment is currently the main reason for converting between array of Message and ByteBufferMessageSet instead of appending Message to ByteBufferMessageSet, and that trimming invalid bytes on consumer complicates its logic, one proposal to refactoring the compression while still be backward compatible is:

1) On serving fetch request, only returns complete Messages within the request rate so the consumer would not need to trim invalid bytes.
2) Use the first 6 bits of the attribute byte of Message to indicate the number of real messages it wraps. This will limit the number of compressed messages in one wrapper to be 256, which I think is good enough.
3) When doing dedup, only delete the payload of the message but keep its counter so that the offsets are still consecutively incremental. 
4) For offset assignment on the broker, the ByteBufferMessageSet will have multiple wrapper Messages (since we have limit compress size to 256 one ProducerRequest now may have ByteBufferMessageSet of multiple wrapper messages); we need to use the shallowIterator to iterate them and reassign offsets without decompress/recompress.
5) On the consumer side, allow shallow iteration as well as deep iteration; with deep iteration, the consumer iterator needs to assign offset to each decompressed message based on its wrapper messages offset and size. Also add the compression codec to the MessageAndMetadata returned to the consumers, which will then solve KAFKA-1011.

With this we can completely remove the de/re-compression on brokers, and always use the compressed wrapper message unit throughout the pipeline. However cons of this approach is that it complicates logic on producer (256 compression batch limit) and consumer (re-compute message offsets).;;;","02/Oct/13 21:33;guozhang;If we do not want to use the attribute byte and have the limit, then we can just still do decompression to get the size of messages in a wrapper Message. This is the same as my previous proposal which still saves us re-compression.;;;","04/Mar/15 19:43;yasuhiro.matsuda;Created reviewboard https://reviews.apache.org/r/31742/diff/
 against branch origin/trunk;;;","04/Mar/15 20:02;yasuhiro.matsuda;This patch introduces BufferingOutputStream, an alternative for ByteArrayOutputStream. It is backed by a chain of byte arrays, so it does not copy bytes when increasing its capacity. Also, it has a method that writes the content to ByteBuffer directly, so there is no need to create an array instance to transfer the content to ByteBuffer. Lastly, it has a deferred write, which means that you reserve a number of bytes before knowing the value and fill it later. In MessageWriter (a new class), it is used for writing the CRC value and the payload length.

On laptop,I tested the performance using TestLinearWriteSpeed with snappy.

Previously
26.64786026813998 MB per sec

With the patch
35.78401869390889 MB per sec

The improvement is about 34% better throughput.;;;","07/Mar/15 00:08;guozhang;Thanks for the patch, this is very promising.

There are a couple of issues we want to resolve here:

1. ByteArrayOutputStream copies data upon overflowing and resizing.

2. Compressed stream needs one extra copy upon finishing reading / writing.

This patch is mainly aimed at #1 above, and I have uploaded a patch for optimizing decompressed iterator, just as an example for resolving #2. In addition, I think in the end we will deprecate ByeBufferMessageSet and move to o.a.k.c.r.MemoryRecords, which will resolve both points above. We can discuss whether we want to incorporate these patches into ByeBufferMessageSet now or just wait for the migration and improve on o.a.k.c.r.MemoryRecords. 

For example, today MemoryRecords's write pattern is only for appending messages with pre-defined ""records batch size"", and try to close the batch when its size is approached; in ByteBufferMessageSet.create() we are given a set of messages without a predicated batch size, but it is still possible to get the value from the estimated compression ratio as we do in Compressor, such that in the worst case only one or two buffer expansions (i.e. data copies) are needed. Just is just an alternative to the linked-list buffers as proposed in this patch.;;;","07/Mar/15 00:09;guozhang;Created reviewboard https://reviews.apache.org/r/31816/diff/
against branch origin/trunk;;;","08/Mar/15 19:53;nehanarkhede;bq. I think in the end we will deprecate ByeBufferMessageSet and move to o.a.k.c.r.MemoryRecords

Isn't that happening in 0.9 and will be helpful only for folks using the new clients?

Since [~yasuhiro.matsuda]'s patch solves #1 and shows the throughput improvement, it is worth the checkin. If you want to follow up with #2, that's great too.;;;","08/Mar/15 20:10;jkreps;The clients already use MemoryRecords, so 0.8.2 and 0.8.3 will give the speed-up to people uses the clients. I think the question is how best to get the perf improvement to the server which should be largely independent.

Guozhang is correct that moving the server to MemoryRecords should be our long term plan and is the end-state we want. However the Message interface is fairly heavily used inside kafka.log so this would be a very large change to those classes. We haven't had a real discussion about how we would go about this and I don't think there is really a timeline. Several options I see:
1. We could do Yasu and Guozhang's fixes now: they are limited in scope, compression is a painpoint now, and we have lots of things in flight right now.
2. We could do a larger conversion of kafka.log to move it off Message/MessageSet/FileMessageSet/ByteBufferMessageSet as Guozhang proposes. This would be a fairly big refactoring, as there are a number of things tied to the MessageSet interface that would all have to move, and there is a significant amount of test code so this would be a big change. However this is certainly where we want to end up.
3. We could decide that we actually prefer java code, and given that the a significant chunk of the common code has to be in Java we should start moving chunks of the server as well. We had talked about this before but I don't think we should start until we have a real plan to finish. But anyhow if we did that we would say instead of just migrating the server from Message/MessageSet/FileMessageSet/ByteBufferMessageSet we would also just wholesale move the log subpackage to java as the first step in a larger migration. The argument both for and against this would be that instead of doing two rewrites, one to change interfaces, and a second to move scala=>java we could just do both at the same time.;;;","08/Mar/15 20:21;nehanarkhede;Agree with the long term plan. Definitely makes sense, though are very large changes. I see value in checking in Yasuhiro's and Guozhang's changes given that compression is a pain point and the patches clearly show improvement. I'm +1 for moving ahead.;;;","08/Mar/15 21:52;yasuhiro.matsuda;>>This patch is mainly aimed at #1 above

If you read the patch carefully, there are more for the compression part. It avoids copies to an intermediate buffer (byte array) when we do ByteArrayOutputStream to ByteBuffer, also a copy form ByteBuffer to ByteBuffer when we create a MessageSet from a Message at the end of compression.

For the decompression part, your iterator patch looks nice. It seems to make ByteBufferSessageSet.decompress obsolete if you clean up all callers by using your iterator.
;;;","10/Mar/15 16:13;guozhang;Hi Yasuhiro,

I thought for compressed writes, the linked list buffers in BufferingOutputStream still need to be copied to a newly allocated buffer (in line 54/55 of ByteBufferMessageSet) whereas for MemoryRecord, it append messages to the compressed stream in-place and no extra copy is required at the end of the writes, but I may misunderstood Scala's function-parameter syntax and please let me know if I did.

As for the migration plan, I agree that ByteBufferMessageSet replacement would not come in the near future, and we can definitely commit the patches now as compress / de-compress has been a pain for us.;;;","16/Mar/15 22:19;yasuhiro.matsuda;Updated reviewboard https://reviews.apache.org/r/31742/diff/
 against branch origin/trunk;;;","20/Mar/15 04:33;yasuhiro.matsuda;Updated reviewboard https://reviews.apache.org/r/31742/diff/
 against branch origin/trunk;;;","25/Mar/15 19:09;yasuhiro.matsuda;Updated reviewboard https://reviews.apache.org/r/31742/diff/
 against branch origin/trunk;;;","25/Mar/15 20:10;junrao;[~ymatsuda], thanks for the patch. +1 and committed to trunk. Leave this jira open for the other patch from Guozhang.;;;","25/Mar/15 20:26;guozhang;Updated reviewboard https://reviews.apache.org/r/31816/diff/
 against branch origin/trunk;;;","25/Mar/15 20:28;jkreps;Yeah it would be great to get the [~guozhang]'s patch in as well and be able to summarize the improvement from the producer's point of view: that is repeat the perf test Yasuhiro did but using the producer performance test so it is more representative of what the user will actually see. That would be a nice tidbit to have for release notes on the next release.;;;","26/Mar/15 22:50;guozhang;Both patches have been checked in, closing this ticket for now.;;;","26/Mar/15 22:55;jkreps;Do we have any kind of before/after performance assessment from the producer's point of view? It would be nice to be able to say that you guys made the producer X% faster. Even just a simple test over localhost would be good. :-);;;","29/Mar/15 22:53;nehanarkhede;[~guozhang] +1. Will be great to re-run the test after your patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OffsetRequest handler does not handle errors,KAFKA-523,12608661,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,mumrah,mumrah,21/Sep/12 16:18,21/Apr/17 22:55,14/Jul/23 05:39,21/Apr/17 22:55,0.7.1,,,,,,,,,,core,,,0,,,,"There is not error handling in the KafkaRequestHandlers#handleOffsetRequest, as a result invalid requests get no data back since they raise an Exception in the server.

",,guozhang,jjkoshy,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299156,,,Fri Apr 21 22:55:10 UTC 2017,,,,,,,,,,"0|i15zxz:",243123,,,,,,,,,,,,,,,,,,,,"21/Sep/12 16:20;mumrah;E.g., I make an OffsetRequest for an invalid partition I get no response on the client side, and I see the following exception in Kafka server logs:

{code}
    kafka.common.InvalidPartitionException: wrong partition 10
      at kafka.log.LogManager.getOrCreateLog(LogManager.scala:169)
      at kafka.server.KafkaRequestHandlers.handleOffsetRequest(KafkaRequestHandlers.scala:130)
      at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$5.apply(KafkaRequestHandlers.scala:47)
      at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$5.apply(KafkaRequestHandlers.scala:47)
      at kafka.network.Processor.handle(SocketServer.scala:289)
      at kafka.network.Processor.read(SocketServer.scala:312)
      at kafka.network.Processor.run(SocketServer.scala:207)
      at java.lang.Thread.run(Thread.java:680)
{code}
;;;","21/Sep/12 17:13;jjkoshy;This will be addressed as part of KAFKA-501. It requires a wire-format change though so it will be in 0.8.;;;","20/Apr/17 19:21;jozi-k;The specified method does not exist anymore. Does it mean this issue can be closed?;;;","21/Apr/17 22:55;guozhang;I think this issue has already been fixed in {{handleListOffsetRequest}}. Closing for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutOfMemoryError in System Test,KAFKA-522,12608493,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jfung,jfung,20/Sep/12 16:58,20/Sep/12 18:28,14/Jul/23 05:39,20/Sep/12 18:28,,,,,,,,,,,,,,0,,,,"A. This is only reproducible in a distributed environment:

1. Modify system_test/cluster_config.json to have all broker entities running in 3 different remote hosts
2. (Optional) Update system_test/system_test_runner.py to log messages at DEBUG level by uncommenting the following line: 
# namedLogger.setLevel(logging.DEBUG)

3. In <kafka_home>/system_test, run “python -B system_test_runner.py”


B. In this specific test session, the error occurred in Broker-3:

[2012-09-20 16:26:25,777] INFO Closing socket connection to /x.x.x.x. (kafka.network.Processor)

[2012-09-20 16:26:25,778] INFO 3 successfully elected as leader (kafka.server.ZookeeperLeaderElector)

[2012-09-20 16:26:25,779] INFO [Controller 3], Broker 3 starting become controller state transition (kafka.controller.KafkaController)

[2012-09-20 16:26:25,950] INFO [Controller-3-to-broker-2-send-thread], Starting  (kafka.controller.RequestSendThread)

[2012-09-20 16:26:25,950] ERROR Error handling event ZkEvent[Data of /controller changed sent to kafka.server.ZookeeperLeaderElector$LeaderChangeListener@75088a1b] (org.I0Itec.zkclient.ZkEventThread)

java.lang.OutOfMemoryError: unable to create new native thread

        at java.lang.Thread.start0(Native Method)

        at java.lang.Thread.start(Thread.java:597)

        at kafka.controller.ControllerChannelManager.kafka$controller$ControllerChannelManager$$startRequestSendThread(ControllerChannelManager.scala:97)

        at kafka.controller.ControllerChannelManager$$anonfun$startup$1.apply(ControllerChannelManager.scala:40)

        at kafka.controller.ControllerChannelManager$$anonfun$startup$1.apply(ControllerChannelManager.scala:40)

        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)

        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)

        at scala.collection.Iterator$class.foreach(Iterator.scala:631)

        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)

        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)

        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)

        at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)

        at kafka.controller.ControllerChannelManager.startup(ControllerChannelManager.scala:40)

        at kafka.controller.KafkaController.startChannelManager(KafkaController.scala:230)

        at kafka.controller.KafkaController.initializeControllerContext(KafkaController.scala:223)

        at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:72)

        at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:47)

        at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:55)

        at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:94)

        at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)

        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)

[2012-09-20 16:26:31,767] INFO [BrokerChangeListener on Controller 3]: Broker change listener fired for path /brokers/ids with children 3,2,1 (kafka.controller.ReplicaStateMachine$BrokerChangeListener)

[2012-09-20 16:26:31,775] INFO [BrokerChangeListener on Controller 3]: Newly added brokers: 1, deleted brokers: , all brokers: 3,2,1 (kafka.controller.ReplicaStateMachine$BrokerChangeListener)

[2012-09-20 16:26:31,777] ERROR [BrokerChangeListener on Controller 3]: Error while handling broker changes (kafka.controller.ReplicaStateMachine$BrokerChangeListener)

java.lang.OutOfMemoryError: unable to create new native thread

        at java.lang.Thread.start0(Native Method)

        at java.lang.Thread.start(Thread.java:597)

        at kafka.controller.ControllerChannelManager.kafka$controller$ControllerChannelManager$$startRequestSendThread(ControllerChannelManager.scala:97)

        at kafka.controller.ControllerChannelManager.addBroker(ControllerChannelManager.scala:61)

        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1$$anonfun$liftedTree1$1$7.apply(ReplicaStateMachine.scala:212)

        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1$$anonfun$liftedTree1$1$7.apply(ReplicaStateMachine.scala:212)

        at scala.collection.immutable.Set$Set1.foreach(Set.scala:81)

        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1.liftedTree1$1(ReplicaStateMachine.scala:212)

        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1.apply$mcV$sp(ReplicaStateMachine.scala:203)

        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1.apply(ReplicaStateMachine.scala:199)

        at kafka.controller.ReplicaStateMachine$BrokerChangeListener$$anonfun$handleChildChange$1.apply(ReplicaStateMachine.scala:199)

        at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)

        at kafka.controller.ReplicaStateMachine$BrokerChangeListener.handleChildChange(ReplicaStateMachine.scala:199)

        at org.I0Itec.zkclient.ZkClient$7.run(ZkClient.java:568)

        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)

[2012-09-20 16:27:22,821] INFO [BrokerChangeListener on Controller 3]: Broker change listener fired for path /brokers/ids with children 3,2 (kafka.controller.ReplicaStateMachine$BrokerChangeListener)

[2012-09-20 16:27:22,823] INFO [BrokerChangeListener on Controller 3]: Newly added brokers: , deleted brokers: 1, all brokers: 3,2 (kafka.controller.ReplicaStateMachine$BrokerChangeListener)

[2012-09-20 16:27:22,825] INFO [Controller-3-to-broker-1-send-thread], Shutting down (kafka.controller.RequestSendThread)

[2012-09-20 16:27:23,279] INFO Unable to read additional data from server sessionid 0x139e47e2eb00004, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)

[2012-09-20 16:27:23,379] INFO zookeeper state changed (Disconnected) (org.I0Itec.zkclient.ZkClient)



C. At around the same time, the following are the main test output:

2012-09-20 16:26:25,251 - INFO - ======================================================

2012-09-20 16:26:25,252 - INFO - Iteration 2 of 2

2012-09-20 16:26:25,252 - INFO - ======================================================

2012-09-20 16:26:25,252 - INFO - looking up leader... (kafka_system_test_utils)

2012-09-20 16:26:25,252 - DEBUG - executing command [ssh host2 ""grep -i -h 'Completed the leader state transition'  /mnt/u001/kafka_08_replication_system_test/system_test/replication_testsuite/testcase_1/logs/broker-1/kafka_server_9091.log |  sort | tail -1""] (kafka_system_test_utils)

2012-09-20 16:26:25,344 - DEBUG - found the log line : [2012-09-20 16:25:45,195] INFO Replica Manager on Broker 1: Completed the leader state transition for topic test_1 partition 0 (kafka.server.ReplicaManager) (kafka_system_test_utils)

2012-09-20 16:26:25,344 - DEBUG - brokerid: [1] entity_id: [1] (kafka_system_test_utils)

2012-09-20 16:26:25,345 - DEBUG - executing command [ssh host3 ""grep -i -h 'Completed the leader state transition'  /mnt/u001/kafka_08_replication_system_test/system_test/replication_testsuite/testcase_1/logs/broker-2/kafka_server_9092.log |  sort | tail -1""] (kafka_system_test_utils)

2012-09-20 16:26:25,455 - DEBUG - executing command [ssh host3 ""grep -i -h 'Completed the leader state transition'  /mnt/u001/kafka_08_replication_system_test/system_test/replication_testsuite/testcase_1/logs/broker-3/kafka_server_9093.log |  sort | tail -1""] (kafka_system_test_utils)

2012-09-20 16:26:25,540 - INFO - ======================================================

2012-09-20 16:26:25,540 - INFO - validating leader election

2012-09-20 16:26:25,540 - INFO - ======================================================

2012-09-20 16:26:25,540 - INFO - found leader in entity [1] with brokerid [1] for partition [0] (kafka_system_test_utils)

2012-09-20 16:26:25,541 - INFO - ======================================================

2012-09-20 16:26:25,541 - INFO - bounce_leader flag : true

2012-09-20 16:26:25,541 - INFO - ======================================================

2012-09-20 16:26:25,541 - INFO - stopping leader in entity 1 with pid 32679 (kafka_system_test_utils)

2012-09-20 16:26:25,541 - DEBUG - executing command [ssh host2 'pid=32679; prev_pid=""""; echo $pid; while [[ ""x$pid"" != ""x"" ]]; do prev_pid=$pid;   for child in $(ps -o pid,ppid ax | awk ""{ if ( \$2 == $pid ) { print \$1 }}"");     do echo $child; pid=$child;   done;   if [ $prev_pid == $pid ]; then     break;   fi; done' 2> /dev/null (system_test_utils)

2012-09-20 16:26:25,669 - DEBUG - terminating process id: 32679 in host: host2 (kafka_system_test_utils)

2012-09-20 16:26:25,670 - DEBUG - executing command [ssh host2 'kill -15 32681'] (system_test_utils)

2012-09-20 16:26:25,673 - DEBUG - executing command [ssh host2 'kill -15 32679'] (system_test_utils)

2012-09-20 16:26:25,675 - INFO - sleeping for 5s for leader re-election to complete (kafka_system_test_utils)

2012-09-20 16:26:30,681 - INFO - looking up broker shutdown... (kafka_system_test_utils)

2012-09-20 16:26:30,681 - DEBUG - executing command [ssh host2 ""grep -i -h 'shut down completed'  /mnt/u001/kafka_08_replication_system_test/system_test/replication_testsuite/testcase_1/logs/broker-1/kafka_server_9091.log |  sort | tail -1""] (kafka_system_test_utils)

2012-09-20 16:26:30,781 - DEBUG - found the log line : [2012-09-20 16:26:25,776] INFO [Kafka Server 1], shut down completed (kafka.server.KafkaServer) (kafka_system_test_utils)

2012-09-20 16:26:30,781 - DEBUG - brokerid: [1] entity_id: [1] (kafka_system_test_utils)

2012-09-20 16:26:30,781 - DEBUG - unix timestamp of shut down completed: 1348158385.776000 (kafka_system_test_utils)

2012-09-20 16:26:30,781 - DEBUG - looking up new leader (kafka_system_test_utils)
",,jfung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299155,,,Thu Sep 20 18:28:21 UTC 2012,,,,,,,,,,"0|i15zxr:",243122,,,,,,,,,,,,,,,,,,,,"20/Sep/12 18:28;jfung;By increasing the max user processes to 4096, the error doesn't show up any more:

ulimit -u 4096

It seems to be related to system resource configuration and not a Kafka issue. So mark this FIXED.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consider catching all exceptions in ShutdownableThread,KAFKA-516,12607717,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jkreps,jkreps,14/Sep/12 21:12,09/Oct/12 15:40,14/Jul/23 05:39,08/Oct/12 21:24,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,,,I don't think there is any case where we want an uncaught exception to kill the thread. In fact this can be a bit hard to debug if an important background thread disappears. We should consider catching everything.,,jkreps,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241605,,,Mon Oct 08 21:24:31 UTC 2012,,,,,,,,,,"0|i02987:",11096,,,,,,,,,,,,,,,,,,,,"08/Oct/12 21:24;yeyangever;
In ShutdownableThread , all exceptions are actually caught. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log cleanup can close a file channel opnened by Log.read before the transfer completes,KAFKA-515,12607716,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,swapnilghike,swapnilghike,14/Sep/12 21:02,13/Jan/13 03:56,14/Jul/23 05:39,13/Jan/13 03:56,0.8.0,0.8.1,,0.8.1,,,,,,,,,,0,bugs,,,"If consumers are lagging behind, then log cleanup activities can close a file channel opened by Log.read 
1. before the transfer the starts (broker will probably throw an exception in this case) OR
2. during the transfer (possibility of half baked corrupted data being sent to consumer?)

We probably haven't hit this race condition in practice because the consumers consume data well before the logs are cleaned up.

To avoid this issue, we could avoid cleaning up the file until the transfer is complete. Reference counting?",,jkreps,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241607,,,Sun Jan 13 03:56:44 UTC 2013,,,,,,,,,,"0|i0298n:",11098,,,,,,,,,,,,,,,,,,,,"14/Sep/12 22:49;jkreps;A simpler approach then reference counting would be to delete the segment in two phases. First remove it from the segment list. This will prevent future requests from using that segment file. Then wait a bit. Then delete the file. This is essentially a race condition since we can't really guarantee that no requests still reference the file, however if we wait (say) 60 seconds it should be extremely unlikely. This could be done in the LogManager.cleanupLogs() method. In the first iteration the logs to be deleted would be added to a list pending deletion. On a subsequent iteration, after sufficient time had passed, the log would be deleted.;;;","13/Jan/13 03:56;jkreps;This is fixed on trunk by the async delete patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove checksum from ByteBufferMessageSet.iterator,KAFKA-512,12607671,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jkreps,jkreps,jkreps,14/Sep/12 16:24,09/Oct/12 15:44,14/Jul/23 05:39,09/Oct/12 15:43,,,,0.8.0,,,,,,,,,,0,bugs,,,"Messages are explicitly checksumed in Log.append. But there is also a checksum computed and checked automatically in ByteBufferMessageSet.iterator as we iterate. This iterator is used quite a lot and as a result we compute this checksum 39 times on a single message produce. It turns out the default crc32 implementation in java is quite expensive so this is not good.

The proposed fix is to remove the automatic checksum from the iterator and add explicit isValid() checks in the consumer as well as retaining the existing check in Log.append().

If folks are in agreement I will probably include this in the KAFKA-506 patch as that already contains a lot of ByteBufferMessageSet changes.",,jkreps,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241608,,,Tue Oct 09 15:43:55 UTC 2012,,,,,,,,,,"0|i0298v:",11099,,,,,,,,,,,,,,,,,,,,"09/Oct/12 15:43;junrao;Fixed in KAFKA-506.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
offset returned in Producer response may not be correct,KAFKA-511,12607486,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,junrao,junrao,13/Sep/12 17:23,09/Oct/12 16:44,14/Jul/23 05:39,09/Oct/12 15:42,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,,,The problems is that we append messages to the log and then get the offset. Another produce request could have sneaked in between the 2 steps.,,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/12 17:29;junrao;kafka-511_v1.patch;https://issues.apache.org/jira/secure/attachment/12545004/kafka-511_v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241604,,,Tue Oct 09 15:42:50 UTC 2012,,,,,,,,,,"0|i0297z:",11095,,,,,,,,,,,,,,,,,,,,"13/Sep/12 17:29;junrao;Attach patch v1. Fixed it by returning the offset in log.append, which is synchronized. Also, currently, we return the offset of the next message. Changed it to return the offset of the message being added.;;;","18/Sep/12 00:00;jjkoshy;Returning ""next offset"" definitely does not seem to make sense.

+1 on the change - although you will need to rebase. Can you rename the ProducerResponseStatus.nextOffset to just offset?;;;","09/Oct/12 15:42;junrao;Fixed in KAFKA-506;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
broker needs to know the replication factor per partition,KAFKA-510,12607396,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,yeyangever,junrao,junrao,13/Sep/12 04:30,11/Oct/12 23:12,14/Jul/23 05:39,10/Oct/12 16:27,,,,0.8.0,,,,,,,core,,,0,bugs,,,"A broker needs to know the replication factor to report under replicated partitions.
",,jjkoshy,junrao,nehanarkhede,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/12 23:59;yeyangever;kafka_510_v1.diff;https://issues.apache.org/jira/secure/attachment/12547468/kafka_510_v1.diff","04/Oct/12 01:12;yeyangever;kafka_510_v2.diff;https://issues.apache.org/jira/secure/attachment/12547651/kafka_510_v2.diff","05/Oct/12 01:40;yeyangever;kafka_510_v3.diff;https://issues.apache.org/jira/secure/attachment/12547866/kafka_510_v3.diff","06/Oct/12 03:06;yeyangever;kafka_510_v4.diff;https://issues.apache.org/jira/secure/attachment/12548094/kafka_510_v4.diff","08/Oct/12 21:05;yeyangever;kafka_510_v5.diff;https://issues.apache.org/jira/secure/attachment/12548298/kafka_510_v5.diff","09/Oct/12 18:41;yeyangever;kafka_510_v6.diff;https://issues.apache.org/jira/secure/attachment/12548438/kafka_510_v6.diff","10/Oct/12 01:00;yeyangever;kafka_510_v7.diff;https://issues.apache.org/jira/secure/attachment/12548503/kafka_510_v7.diff","11/Oct/12 00:09;yeyangever;kafka_510_v8.diff;https://issues.apache.org/jira/secure/attachment/12548675/kafka_510_v8.diff","11/Oct/12 19:16;yeyangever;kafka_510_v9.diff;https://issues.apache.org/jira/secure/attachment/12548792/kafka_510_v9.diff",,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,240629,,,Thu Oct 11 23:12:18 UTC 2012,,,,,,,,,,"0|i0140v:",4421,,,,,,,,,,,,,,,,,,,,"24/Sep/12 16:56;junrao;Also, if replication factor is 1, we can immediately answer a produce request, without putting it into purgatory first.;;;","02/Oct/12 23:59;yeyangever;

1. Added the filed ""replication factor"" in LeaderAndISR, and all related places

2. directly send leaderAndIsrResponse without the purgatory when all partition in the request has replication factor one. ;;;","03/Oct/12 21:32;jjkoshy;Thanks for patch v1. I have a few comments:

1 - Condition in KafkaApis.handleProducerRequest should be made clear. I think
  this would be clearer to understand:
  (! produceRequest.data.keySet.exists(m => replicaManager.replicationFactorForPartitions.get(m.topic, m.partition) > 1))
2 - Also, can you remove the info log that you added in handleProducerRequest.
3 - Revert the test/resources/log4j change.
4 - Partition.scala.isUnderReplicated should use this change to report
  under-replicated partitions correctly.
;;;","04/Oct/12 01:12;yeyangever;
Joel, 

Thanks for comment, new patch uploaded;;;","05/Oct/12 01:28;jjkoshy;Looks good - one minor change I forgot to mention is to use the TopicAndPartition case class instead of a (String, Int) tuple for the pool you added. We still have many such tuples scattered all over the code, but I think going forward we should avoid doing so.
;;;","05/Oct/12 01:40;yeyangever;
Joel,

Thanks for comment, this part is changes in patch v3, potentially some other places also need be changed...;;;","05/Oct/12 16:35;junrao;Thanks for the patch. A few more comments:

30. KafkaApis: The following code can be made clearer if we use  leaderAndISRRequest.leaderAndISRInfos.foreach{ case
    leaderAndISRRequest.leaderAndISRInfos.foreach(
    {
      m => replicaManager.replicationFactorForPartitions.put(TopicAndPartition(m._1._1, m._1._2), m._2.replicationFactor)
    })

31. ReplicaManager: Instead of putting the replication factor in ReplicaManager, it's more natural to put it in Partition directly. We need to make sure that we can provide the replication factor when a Partition is created. A partition should really only be created when processing a makeLeader or makeFollower request (in both cases we the the replication factor). There are 2 other places that we call getOrCreatePartition. However, both can be changed to a getPartition method instead. The first place is in ReplicaManager.recordFollowerPosition. We can change it so that if a partition is none, we skip the call to updateLeaderHWAndMaybeExpandISR and log a warning. The second place is in DelayedProduce.isSatisfied(). We can also change it so that if a partition is none, we simply set the errorCode to UnknownTopicOrPartition. Finally, we can add a utility method in ReplicaManager to be used in KafkaApi to check the replication factor of one or more partitions.

32. LeaderAndIsr: The value of the leaderAndIsr path in ZK doesn't need to include replication factor. We could add a method toZKString in LeaderAndIsr to get the value that we want to write to ZK.

;;;","06/Oct/12 03:06;yeyangever;
I still use toString() instead of toZkString() in leaderAndISR, because I find we do have code recovering leaderAndISR from zookeeper, including the replication factor:  ZkUtils.getLeaderForPartition() 

Although we can still get rid of it, which means a lot of code, not good encapsulation. I prefer not.


The rest of comments for v3 are addressed.;;;","08/Oct/12 21:05;yeyangever;
Rebase from Jay's patch;;;","09/Oct/12 05:24;junrao;Patch doesn't seem to apply to 0.8.

The text leading up to this was:
--------------------------
|Index: core/src/test/scala/unit/kafka/network/RpcDataSerializationTest.scala
|===================================================================
|--- core/src/test/scala/unit/kafka/network/RpcDataSerializationTest.scala	(revision 1394906)
|+++ core/src/test/scala/unit/kafka/network/RpcDataSerializationTest.scala	(working copy)
--------------------------
File to patch: 
;;;","09/Oct/12 18:41;yeyangever;
Seems some issue with rebase. try this patch please;;;","10/Oct/12 01:00;yeyangever;
Adding the PartitionInfo to wrap the leaderAndIsr and  replication factor;;;","10/Oct/12 16:27;junrao;Thanks for patch v7. Committed to 0.8 with the following minor refactoring.

ControllerBrokerRequestBatch.sendRequestsToBrokers() : leaderAndIsr is renamed to partitionInfo.
;;;","10/Oct/12 16:40;nehanarkhede;Thanks for patch v7, here are few review comments -

1. ControllerChannelManager
Probably better to change partitionInfo to partitionState.

2. KafkaApis
2.1. Wrap the line that computes the partitions with replication factor 1
2.2. Rename allPartitionHasReplicationFactorOne to allPartitionsHaveReplicationFactorOne
2.3. Change the if(partitionOpt.isDefined) to case match.

3. LeaderAndIsrRequest
3.1 Add a space between closing parenthesis and opening bracket for case class definition of LeaderAndIsr
3.2 Rename partitionInfos to partitionState
3.3 Rename PartitionInfo to PartitionState

4. ReplicaManager
4.1 Remove unused import
4.2 Change if(partitionOpt.isDefined) to case match
4.3 Why is the following check required in the absence of a feature that allows us to change the replication factor of a partition -
    if(partition.replicationFactor != replicationFactor)
      partition.replicationFactor = replicationFactor
4.4 Rename partitionInfos to partitionState

5. ReplicaStateMachine
5.1 Wrap the long lines
;;;","10/Oct/12 17:04;nehanarkhede;I can't seem to be able to reopen this JIRA.
Jun, 
can you help re-opening the JIRA until the above comments are addressed or justified ?;;;","10/Oct/12 17:09;junrao;Unfortunately, the jira module that we use is no-reopen on close. We can either just submit new patches on this jira or open a followup jira to track it.;;;","10/Oct/12 17:14;nehanarkhede;This JIRA would have been much easier to work on top of KAFKA-42 since the latter makes some important changes throughout the controller's state machine. Rebasing KAFKA-42 over this is turning out to be a pain;;;","11/Oct/12 00:09;yeyangever;
rebase from kafka 42 and apply Neha's advices;;;","11/Oct/12 00:49;nehanarkhede;1. ControllerChannelManager
Wrap the long line

2. KafkaApis
Wrap the long lines

3. LeaderAndIsrRequest
PartitionInfo is too general, mind changing it to PartitionStateInfo ?

4. PartitionStateMachine
4.1 Let's not change the NOTE back to TODO
4.2 Wrap the long lines

5. ReplicaManager
5.1 Wrap the long lines and please remember to do this carefully in each patch
5.2 Move the comment in getReplicationFactorForPartition to the API description section of that method
5.3 Still have the same question 4.3 from previous review
5.4 Change ""In recording follower position"" to ""While recording the follower position""

6. ReplicaStateMachine
Wrap the long lines
;;;","11/Oct/12 04:59;yeyangever;thanks for the comments. The new one is ready and will be submitted tmr.;;;","11/Oct/12 19:16;yeyangever;
Thanks for comments and they're addressed in patch v9;;;","11/Oct/12 23:12;nehanarkhede;+1. Only 4.1 was not addressed, but I'll take care of it during checkin.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
server should shut down on encountering invalid highwatermark file,KAFKA-509,12607366,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,yeyangever,swapnilghike,swapnilghike,12/Sep/12 23:58,02/Oct/12 15:48,14/Jul/23 05:39,02/Oct/12 15:48,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,,,"1. Somehow I managed to produce the following .highwatermark file (most probably while playing with kafka-create-topic.sh) - 
0
7
abra.kabra 0 0
0 0 0
abrakabra 0 0
\0 0 0
king. 0 0
abra..kabra 0 0
... 0 0

Perhaps the first two lines are not valid. But I am not able to reproduce this issue today. 

2. Since the topic names can contain a whitespace, perhaps the delimiter should change from a space char to / which is not allowed anymore in topic names.

3. With this .highwatermark file, the kafka server produces following - 
[2012-09-12 14:54:49,456] INFO Replica Manager on Broker 0: Becoming Leader for topic [abra.kabra] partition [0] (kafka.server.ReplicaManager)
[2012-09-12 14:54:49,456] INFO [ReplicaFetcherManager on broker 0, ], removing fetcher on topic abra.kabra, partition 0 (kafka.server.ReplicaFetcherManager)
[2012-09-12 14:54:49,457] ERROR Replica Manager on Broker 0: Error processing leaderAndISR request LeaderAndIsrRequest(1,,true,1000,Map((...,0) -> { ""ISR"": ""0"",""leader"": ""0"",""leaderEpoch"": ""0"" }, (\0,0) -> { ""ISR"": ""0"",""leader"": ""0"",""leaderEpoch"": ""0"" }, (abrakabra,0) -> { ""ISR"": ""0"",""leader"": ""0"",""leaderEpoch"": ""0"" }, (0,0) -> { ""ISR"": ""0"",""leader"": ""0"",""leaderEpoch"": ""0"" }, (abra..kabra,0) -> { ""ISR"": ""0"",""leader"": ""0"",""leaderEpoch"": ""0"" }, (king.,0) -> { ""ISR"": ""0"",""leader"": ""0"",""leaderEpoch"": ""0"" }, (abra.kabra,0) -> { ""ISR"": ""0"",""leader"": ""0"",""leaderEpoch"": ""0"" })) (kafka.server.ReplicaManager)
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
	at java.lang.String.substring(String.java:1937)
	at kafka.server.HighwaterMarkCheckpoint$$anonfun$1.apply(HighwaterMarkCheckpoint.scala:103)
	at kafka.server.HighwaterMarkCheckpoint$$anonfun$1.apply(HighwaterMarkCheckpoint.scala:96)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
	at scala.collection.immutable.Range$ByOne$class.foreach(Range.scala:282)
	at scala.collection.immutable.Range$$anon$2.foreach(Range.scala:265)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
	at scala.collection.immutable.Range.map(Range.scala:39)
	at kafka.server.HighwaterMarkCheckpoint.read(HighwaterMarkCheckpoint.scala:96)
        ... (more junk) ..

And then it goes in a while(1) loop to print the following - 
[2012-09-12 14:54:50,458] ERROR Replica Manager on Broker 0: Highwatermark for topic abra.kabra partition 0 doesn't exist during checkpointing (kafka.server.ReplicaManager)
[2012-09-12 14:54:50,459] ERROR Replica Manager on Broker 0: Highwatermark for topic ... partition 0 doesn't exist during checkpointing (kafka.server.ReplicaManager)
[2012-09-12 14:54:50,459] ERROR Replica Manager on Broker 0: Highwatermark for topic \0 partition 0 doesn't exist during checkpointing (kafka.server.ReplicaManager)
[2012-09-12 14:54:50,459] ERROR Replica Manager on Broker 0: Highwatermark for topic 0 partition 0 doesn't exist during checkpointing (kafka.server.ReplicaManager)
[2012-09-12 14:54:50,459] ERROR Replica Manager on Broker 0: Highwatermark for topic king. partition 0 doesn't exist during checkpointing (kafka.server.ReplicaManager)
[2012-09-12 14:54:50,460] ERROR Replica Manager on Broker 0: Highwatermark for topic abra..kabra partition 0 doesn't exist during checkpointing (kafka.server.ReplicaManager)
[2012-09-12 14:54:50,460] ERROR Replica Manager on Broker 0: Highwatermark for topic abrakabra partition 0 doesn't exist during checkpointing (kafka.server.ReplicaManager)

The server should shut down on encountering the error.",,junrao,swapnilghike,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/12 19:49;yeyangever;kafka_509_v1.diff;https://issues.apache.org/jira/secure/attachment/12547034/kafka_509_v1.diff","02/Oct/12 00:41;yeyangever;kafka_509_v2.diff;https://issues.apache.org/jira/secure/attachment/12547318/kafka_509_v2.diff",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,291939,,,Tue Oct 02 15:48:35 UTC 2012,,,,,,,,,,"0|i0rhq7:",158516,,,,,,,,,,,,,,,,,,,,"24/Sep/12 16:50;junrao;The first 2 lines are valid and they indicate version # of # of partitions. The \0 could just be a wrong topic. In any case, we should deal with potential special characters (e.g., space) in the topic name. One way to do this is to use '/' as the delimiter since it's disallowed in topic name. Also, if we hit any exception while reading high watermark, we should log the problematic content.

Finally, I recommend that we take care of the following comments left from kafka-405.
40. HighwaterMarkCheckpoint: 
40.1 If tempHwFile already exists, we can just overwrite it since we know hwFile is always safe. 
40.2 There is no need to delete hwFile first and then rename tempHwFile to it. Rename should do the deletion. Currently, if we fail at the bad time, we could end up without a hwFile.;;;","24/Sep/12 18:13;swapnilghike;The string ""\0"" is a valid topic name, I was just messing around with the topic names for testing.;;;","28/Sep/12 19:49;yeyangever;1 If tempHwFile already exists, we can just overwrite it since we know hwFile is always safe.

solved

2 There is no need to delete hwFile first and then rename tempHwFile to it. Rename should do the deletion. Currently, if we fail at the bad time, we could end up without a hwFile. 

solved


3. delimiter not changed as topic names are now more constrained;;;","01/Oct/12 16:26;junrao;Thanks for patch v1. One comment:

1. If the tmp file exists, we should delete it first (to make sure the content is fully overwritten). Also, before we use FileWriter, we probably should call file.createNewFile().
;;;","01/Oct/12 21:16;junrao;Also, related to HW, currently LogMananger doesn't recognize the high watermark file and log ""Skipping unexplainable file"" during startup. It would be good if we can address this problem in this jira too.;;;","02/Oct/12 00:41;yeyangever;For the 1st comment about FileWriter, I've checked it, it will create new file if it does not existed. Also if it exists, it will truncate the whole file and start from beginning. I've created a test project to do that

http://www.homeandlearn.co.uk/java/write_to_textfile.html


2. That warning is now avoided in LogManager;;;","02/Oct/12 15:48;junrao;Thanks for patch v2. Removed unused import in LogManager and added a comment. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
split out partiondata from fetchresponse and producerrequest,KAFKA-508,12607271,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,charmalloc,charmalloc,charmalloc,12/Sep/12 17:01,05/Oct/12 15:30,14/Jul/23 05:39,05/Oct/12 15:28,,,,0.8.0,,,,,,,,,,0,optimization,,,,,charmalloc,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/12 00:34;charmalloc;KAFKA-508.patch;https://issues.apache.org/jira/secure/attachment/12547643/KAFKA-508.patch","04/Oct/12 20:43;charmalloc;KAFKA-508.v2.patch;https://issues.apache.org/jira/secure/attachment/12547819/KAFKA-508.v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,240639,,,Fri Oct 05 15:30:18 UTC 2012,,,,,,,,,,"0|i0146n:",4447,,,,,,,,,,,,,,,,,,,,"03/Oct/12 20:53;junrao;Hi, Joe,

Do you still plan to work on this jira? Thanks,;;;","03/Oct/12 21:32;charmalloc;yes, sorry been swamped at work.  

will work more on this tonight hopefully get a patch out soon;;;","04/Oct/12 18:08;jjkoshy;Joe, thanks for the patch.  Actually, do you think we can also get rid of
the partition field from the PartitionDataProducerRequest and
PartitionDataFetchResponse? It seems redundant since the map key already
uses (TopicAndPartition). So PartitionDataProducerRequest will become
unnecessary, and PartitionDataFetchResponse can get rid of the partition
member. This shouldn't require further changes to the wire protocol (beyond
your current change), but there are some places in the code that will need to be
fixed. So we can consider doing this in a separate non-blocker jira as
getting the wire protocol change in is more important.

BTW, personally I prefer ProducerRequestPartitionData ""AB"" and
FetchResponsePartitionData ""AB"" would be better than ""BA"". What say?
;;;","04/Oct/12 20:35;charmalloc;<< Actually, do you think we can also get rid of the partition field from the PartitionDataProducerRequest and PartitionDataFetchResponse?

no, i don't think so but maybe I am missing something

<< So we can consider doing this in a separate non-blocker jira as getting the wire protocol change in is more important.

yeah, seperate discussion

<< BTW, personally I prefer ProducerRequestPartitionData ""AB"" and FetchResponsePartitionData ""AB"" would be better than ""BA"". What say?

I originally had it that way and changed it because I thought folks might want it BA, AB is preferable to me patch coming in a sec;;;","04/Oct/12 20:43;charmalloc;BA patch attached for name change to be 

ProducerRequestPartitionData and FetchResponsePartitionData instead of PartitionDataFetchResponse and PartitionDataFetchResponse;;;","04/Oct/12 21:47;jjkoshy;+1 - can you remove the extends from the two objects? or is it there for a reason?

It is definitely possible to do away with the partition member, but we can take care of that later.
;;;","04/Oct/12 23:31;junrao;Joe, thanks for the patch. Looks good overall. I agree with Joel that we should be able to remove partition from ProducerRequestPartitionData and FetchResponsePartitionData. Could you elaborate the problem that you saw?;;;","05/Oct/12 15:30;charmalloc;committed patch removing the extends typo i had left in as Joel caught, thanks!

<< we should be able to remove partition from ProducerRequestPartitionData and FetchResponsePartitionData. Could you elaborate the problem that you saw?

so i see functions getting the partition passed into it without any other way to get the partition except from these 2 objects however, it is possible within those functions it is not needed or is able to get to it some other way.  I really need to spend more time with the 0.8 changes which I will do in the coming week.  opened JIRA for this can work it all out there.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Store logical offset in log,KAFKA-506,12607147,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,11/Sep/12 23:23,16/Nov/17 10:04,14/Jul/23 05:39,08/Oct/12 19:15,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"Currently we only support retention by dropping entire segment files. A more nuanced retention policy would allow dropping individual messages from a segment file by recopying it. This is not currently possible because the lookup structure we use to locate messages is based on the file offset directly.

To fix this we should move to a sequential, logical offset (0,1,2,3,...) which would allow deleting individual messages (e.g. 2) without deleting the entire segment.

It is desirable to make this change in the 0.8 timeframe since we are already doing data format changes.

As part of this we would explicitly store the key field given by the producer for partitioning (right now there is no way for the consumer to find the value used for partitioning).

This combination of features would allow a key-based retention policy that would clean obsolete values either by a user defined key.

The specific use case I am targeting is a commit log for local state maintained by a process doing some kind of near-real-time processing. The process could log out its local state changes and be able to restore from this log in the event of a failure. However I think this is a broadly useful feature.

The following changes would be part of this:
1. The log format would now be
      8 byte offset
      4 byte message_size
      N byte message
2. The offsets would be changed to a sequential, logical number rather than the byte offset (e.g. 0,1,2,3,...)
3. A local memory-mapped lookup structure will be kept for each log segment that contains the mapping from logical to physical offset.

I propose to break this into two patches. The first makes the log format changes, but retains the physical offset. The second adds the lookup structure and moves to logical offset.

Here are a few issues to be considered for the first patch:
1. Currently a MessageSet implements Iterable[MessageAndOffset]. One surprising thing is that the offset is actually the offset of the next message. I think there are actually several uses for the current offset. I would propose making this hold the current message offset since with logical offsets the next offset is always just current_offset+1. Note that since we no longer require messages to be dense, it is not true that if the next offset is N the current offset is N-1 (because N-1 may have been deleted). Thoughts or objections?
2. Currently during iteration over a ByteBufferMessageSet we throw an exception if there are zero messages in the set. This is used to detect fetches that are smaller than a single message size. I think this behavior is misplaced and should be moved up into the consumer.
3. In addition to adding a key in Message, I made two other changes: (1) I moved the CRC to the first field and made it cover the entire message contents (previously it only covered the payload), (2) I dropped support for Magic=0, effectively making the attributes field required, which simplifies the code (since we are breaking compatibility anyway).

",,jjkoshy,jkreps,junrao,nehanarkhede,smeder,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-414,,,,,,,,,KAFKA-634,,,,,,,,,,,,,"10/Oct/12 23:48;jkreps;KAFKA-506-neha-post-review-v2.patch;https://issues.apache.org/jira/secure/attachment/12548670/KAFKA-506-neha-post-review-v2.patch","10/Oct/12 03:59;jkreps;KAFKA-506-neha-post-review.patch;https://issues.apache.org/jira/secure/attachment/12548519/KAFKA-506-neha-post-review.patch","28/Sep/12 01:05;jkreps;KAFKA-506-phase-2-v1.patch;https://issues.apache.org/jira/secure/attachment/12546937/KAFKA-506-phase-2-v1.patch","28/Sep/12 20:20;jkreps;KAFKA-506-phase-2-v2.patch;https://issues.apache.org/jira/secure/attachment/12547038/KAFKA-506-phase-2-v2.patch","02/Oct/12 21:33;jkreps;KAFKA-506-phase-2-v3.patch;https://issues.apache.org/jira/secure/attachment/12547447/KAFKA-506-phase-2-v3.patch","03/Oct/12 20:52;jkreps;KAFKA-506-phase-2-v4.patch;https://issues.apache.org/jira/secure/attachment/12547593/KAFKA-506-phase-2-v4.patch","05/Oct/12 20:18;jkreps;KAFKA-506-phase-2-v5.patch;https://issues.apache.org/jira/secure/attachment/12548036/KAFKA-506-phase-2-v5.patch","04/Oct/12 17:22;jkreps;KAFKA-506-phase-2-v5.patch;https://issues.apache.org/jira/secure/attachment/12547788/KAFKA-506-phase-2-v5.patch","27/Sep/12 05:44;jkreps;KAFKA-506-phase-2.patch;https://issues.apache.org/jira/secure/attachment/12546813/KAFKA-506-phase-2.patch","13/Sep/12 21:27;jkreps;KAFKA-506-v1-draft.patch;https://issues.apache.org/jira/secure/attachment/12545057/KAFKA-506-v1-draft.patch","14/Sep/12 22:41;jkreps;KAFKA-506-v1.patch;https://issues.apache.org/jira/secure/attachment/12545234/KAFKA-506-v1.patch","03/Oct/12 20:52;jkreps;KAFKA-506-v4-changes-since-v3.patch;https://issues.apache.org/jira/secure/attachment/12547594/KAFKA-506-v4-changes-since-v3.patch",,,,,,,,,,,,,,,,12.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,240554,,,Fri Oct 12 00:16:01 UTC 2012,,,,,,,,,,"0|i013bb:",4306,,,,,,,,,,,,,,,,,,,,"13/Sep/12 21:27;jkreps;Add key to message and reorder some fields
Bump up Message magic number to 2
Add offset to MessageSet format
Make MessageAndOffset contain the current offset and add a nextOffset() method to get the next offset
Some misc. cleanups (delete some obsolete files, fix bad formatting)

There are still two problems with this patch:
1. Not handling offsets properly in compressed messages
2. Unit test failures in LogRecoveryTest;;;","14/Sep/12 22:41;jkreps;Updated the patch. This patch fixes the remaining failing tests and correctly handles compressed messages.

This patch is ready for review.;;;","14/Sep/12 22:42;jkreps;I am going to begin phase two of this, implementing the logical offset management in Log.;;;","17/Sep/12 03:47;junrao;Thanks for patch v1. Overall, the log format change is reasonable. Some comments:

1. MessageAndOffset: nextOffset is not correct for compressed messages. Currently, in the high-level consumer, after iterating each message, the consume offset is moved to the offset of the next message. So, if one consumes a message and then commits the offset, the committed offset points to the next message to be consumed. We could probably change the protocol to move the consumer offset to the offset of the current message. Then, the caller will need to commit the offset first and then consumes the message to get the same semantics.

2. Message:
2.1 The comment of the message has a bug. Payload should have (N- K - 10) bytes.
2.2 In constructor, should we assert that offset is btw 0 and bytes.length-1? Also, just to be clear that offset and size are for the payload, should we rename bytes, offset and size to something like payload, payloadOffset and payloadSize?
2.3 computeChecksum(): can use MagicOffset for both starting offset and length
2.4 remove unused import

3. MessageSet: Fix the comment in second line ""A The format"".

4. ByteBufferMessageSet: remove unused comment

5. Log:
5.1 append(): For verifying message size, we need to use the shallow iterator since a compressed message has to be smaller than the configured max message size.
5.2 append(): Compressed messages are forced to be decompressed and then compressed again. This will introduce some CPU overhead. What's the increase in CPU utilization if incoming messages are compressed? Also, for replicaFetchThread, it can just put the data fetched from the leader directly into the log without recomputing the offsets. Could we add a flag in append to bypass regenerating the offsets?
5.3 trimInvalidBytes(): There is a bug in the following statement: messages.size should be messages.sizeInBytes.
    if(messageSetValidBytes == messages.size) {

6. javaapi.ByteBufferMessageSet: Java users shouldn't really be using buffer. So, we don't need the bean property.

7. PartitionData: Do we need to override equal and hash since this is already a case class?

8. ZkUtils.conditionalUpdatePersistenPath(): This method expects exception due to version conflict. So there is no need to log the exception.

9. SyncProducerTest: remove unused imports

10. How do we handle the case that a consumer uses too small a fetch size?;;;","17/Sep/12 15:57;jkreps;Great feedback, thanks.

1. Good point about nextOffset. I think this is slightly tricky to fix. I think I will ignore this problem and work on phase 2 which will fix that issue by making nextOffset=offset+1. This means taking both patches at once which will be a bit big. Sound feasible?
2-4 Good feedback
5.1. Good point.
5.2. I will do a little micro-benchmark on decompression/re-compression. Yes, we can definitely avoid this for the replica fetcher thread. Depending on how much we want to optimize that path there are a lot of options. On the extreme side of total trust I think it might actually possible to do FileChannel.transferTo directly from the socket buffer, though there are complications around metrics and hw mark. I think for now it makes sense to just skip decompression. One question: let's say recompression turns out to be expensive, there are two options: (1) do not set internal offsets (as today), (2) eat the cost and recommend snappy instead of gzip. Personally I prefer (2) since I think we need to fix the correctness bugs, but I am open to implementing either if there is a consensus.
5.3. Good catch
6. OK
7. I am not sure. We had a custom implementation of equals but no hashcode which I think was likely wrong. We can remove both, but I would want to figure out why we added the equals.
8-9. OK
10. Ah, forgot to add that. I think the right thing is just to check (currentDataChunk.messages.size > 0 && currentDataChunk.buffer.size == fetchSize) throw Exception() in the ConsumerIterator. The only thing to consider is that this means there is no check for the simpleconsumer.
;;;","17/Sep/12 21:16;junrao;Actually, there is another thing.

11. We need to change DefaultEventHandler to put the key data into messages sent to the broker. Also, Producer currently can take any type as key, do we want to restrict it to bytes or do we want to define a serializer for key too?;;;","20/Sep/12 21:17;nehanarkhede;Thanks for the patch ! The log format change doesn't interfere with replication as of this patch. A few comments in addition to Jun's -

1. CompressionUtils: How about re-using the ByteBufferMessageSet.writeMessage() API for serializing the compressed message to a byte buffer ?

2. ByteBufferMessageSet.scala, FileMessageSet: Can we use MessageSet.LogOverhead instead of 12 for byte arithmetic ?

3. ConsumerIterator
The nextOffset issue for compressed message sets will get resolved when we actually use the sequential logical offsets. With that, the advantage is that the consumer will be able to fetch a message even if it is inside a compressed message. Today, there is no good way to achieve this unless we have level-2 message offsets for compressed messages. Even if we cannot make that change in time for replication, we can take this change and leave the message set iterator to return the next offset (valid fetch offset), just like we do today. So, either way, we are covered here.;;;","27/Sep/12 05:44;jkreps;This patch is incremental from the previous one. I will rebase and provide an up-to-date patch that covers both phases, but this shows the new work required to support logical offsets.

I think I have addressed most of the comments on the original patch, except:
1. I have put off any performance optimization (avoiding recompression for replicas, memory-mapping the log, etc). I would like to break this into a separate JIRA and write a reasonable standalone Log benchmark that covers these cases and then work against that. I have several other cleanups I would like to do as well: (1) get rid of SegmentList, (2) move more functionality in Log into LogSegment.
2. I am not yet storing the key in the message, this may change the produce api slightly so i think this should be a seperate JIRA too.
3. Neha--I change most of the uses of magical numbers except where the concrete number is more clear.

Here is a description of the new changes.
- Offset now always refers to a logical log offset. I have tried to change any instances where offset meant file offset to instead use the terminology ""position"". References to file positions should only occur in Log.scala and classes internal to that.
- As in the previous patch MessageAndOffset gives three things: (1) the message, (2) the offset of THAT message, and (3) a helper method to calculate the next offset.
- Log.append() is responsible for maintaining the logEndOffset and using it to assign offsets to the messageset before appending to the log.
- Offsets are now assigned to compressed messages too. One nuance is that the offset of the wrapper message is equal to the last offset of the messages it contains. This will be more clear in the discussion of the offset search changes.
- Log.read now accepts a new argument maxOffset, which is the largest (logical) offset that will be returned in addition to the maxSize which limits the size in bytes.
- I have changed Log.read to now support sparse offsets. That is, it is valid to have missing offsets. This sparseness is needed both for the key-retention but also for the correct handling of compressed messages. I will describe the read path in more detail below.
- I moved FileMessageSet to the package kafka.log as already much of its functionality was specific to the log implementation.
- I changed FetchPurgatory back to use a simple counter for accumulated bytes. It was previously re-calculating the available bytes, but because this now is a more expensive operation, and because this calculation is redone for each topic/partition produce (i.e. potentially 200 times per produce request), I think this is better. This is less accurate, but since long poll is a heuristic anyway I think that is okay.
- I changed the default suffix of .kafka files to .log and added a new .index file that contains a sparse index of offset=>file_position to help efficiently resolve logical offsets.
- Entries are added to this index at a configurable frequency, controlled by a new configuration log.index.interval.bytes which defaults to 4096
- I removed numerous instances of byte calculations. I think this is a good thing for code quality.

Here is a description of the new read path.
1. First log tries to find the correct segment to read from using the existing binary search on log segments. I modified this search slightly in two ways. First we had a corner case bug which only occurred if you have two files with successive offsets (unlikely now, impossible before). Second, I now no longer check ranges but instead return the largest segment file less than or equal to the requested offset.
2. Once the segment is found we check the index on that segment. The index returns the largest offset less than or equal to the requested offset and the associated file position in the log file. This position represents a least upper bound on the position in the file, and it is the position from which we begin a linear search checking each message. The index itself is just a sorted sequence of (offset, position) pairs. Complete details are in the header comments on kafka.log.OffsetIndex.scala. It is not required that all messages have an entry in the OffsetIndex, instead there is a confgurable frequency in terms of bytes which is set in LogSegment. So, for example, we might have an entry every 4096 bytes. This frequency is approximate, since a single message may be larger than that.
3. Once we have a greatest lower bound on the location we use FileMessageSet.searchFor to search for the position of the first message with an offset at least as large as the target offset. This search just skips through the file checking the offset only.
;;;","28/Sep/12 01:05;jkreps;Okay attached a fully rebased patch that contains both phase 1 and phase 2 changes.;;;","28/Sep/12 21:54;jkreps;Three preliminary comments from Neha while she does deeper interogations:
- Would be nice if the DumpLogSegment tool also dumped the contents of the index file
- This patch implicitly assumes file segments are limited to 2GB (I use a 4 byte position pointer in the index). Turns out this isn't true. Proposed fix is to limit log segments to 2GB.
- We decided the corner case with sparse messages at the end of a segment isn't really a corner case as it effects compressed messages too. So I will fix that in the scope of this patch.;;;","28/Sep/12 22:43;nehanarkhede;2 additions to the preliminary comments -
- 3 unit tests fail on patch v2 - http://pastebin.com/ECUA2n1f
- It will be nice for maxIndexEntries to be a configurable property on the server;;;","01/Oct/12 03:55;junrao;Thanks for patch v2. Some more comments:

20. Log:
20.1 findRange(): Add to the comment that now this method returns the largest segment file <= the requested offset.
20.2 close(): move the closing } for the for loop to a new line.
20.3 bytesSinceLastIndexEntry is only set but is never read.
20.4 append(): This method returns the offset of the first message to be appended. This is ok for the purpose of returning the offset to the producer. However, when determining whether all replicas have received the appended messages, we need to use the log end offset after the messages are appended. So, what we should do is to have append() return 2 offsets, one before the append and one after the append. We use the former in producer response and use the latter for the replica check. To avoid complicating this patch further, another approach is to, in the jira, have append return the log end offset after the append and use it in both producer response and replica check. We can file a separate jira to have append return 2 offsets.
20.5 read(): The trace statement: last format pattern should be %d instead of %s.
20.6 truncateTo(): The usage of logEndOffset in the following statement is incorrect. It should be the offset of the next segment.
          segments.view.find(segment => targetOffset >= segment.start && targetOffset < logEndOffset)
20.7 There are several places where we need to create a log segment and the code for creating the new data file and the new index file is duplicate. Could we create a utility function createNewSegment to share the code?

21. LogSegment: bytesSinceLastIndexEntry needs to be updated in append().

22. FileMessageSet.searchFor(): The following check seems to be a bit strange. Shouldn't we use position + 12 or just position instead?
    while(position + 8 < size) {

23. OffsetIndex:
23.1 In the comment, ""mutable index can be created to"" seems to have a grammar bug.
23.2 mmap initialization: The following statement seems unnecessary. However, we do need to set the mapped buffer's position to end of file for mutable indexes. 
          idx.position(idx.limit).asInstanceOf[MappedByteBuffer]
23.3 append(): If index entry is full, should we automatically roll the log segment? It's ok if this is tracked in a separate jira.
23.4 makeReadOnly(): should we call flush after raf.setLength()? Also, should we remap the index file to the current length and make it read only?

24. LogManager.shutdown(): log indentation already adds LogManager in the prefix of each log entry.

25. KafkaApis:
25.1 handleFetchRequest: topicDatas is weird since data is the plural form of datum. How about topicDataMap?
25.2 ProducerRequestPurgatory: It seems that it's useful to keep the logIndent since it can distinguish logs from the ProducerRequestPurgatory and FetchRequestPurgatory. Also, it's probably useful to pass in brokerId to RequestPurgatory for debugging unit tests.

26. Partition: There are a few places that the first character of info log is changed to lower case. The current convention is to already use upper case.

27. javaapi.ByteBufferMessageSet: underlying should be private val.

28. DumpLogSegment: Now that each message stores an offset, we should just print the offset in MessageAndOffset. There is no need for var offset now.

29. FetchedDataChunk: No need to use val for parameters in constructor since this is a case class now.

30. PartitionData:
30.1 No need to redefine equals and hashcode since this is already a case class.
30.2 initialOffset is no longer needed.

31. PartitionTopicInfo.enqueue(): It seems that next can be computed using shallow iterator since the offset of a compressed message is always the offset of the last internal message.

32. ByteBufferMessageSet: In create() and decompress(), we probably should close the output and the input stream in a finally clause in case we hit any exception during compression and decompression.

33. remove unused imports.

The following comment from the first round of review is still not addressed.
10. How do we handle the case that a consumer uses too small a fetch size?
;;;","02/Oct/12 21:33;jkreps;New patch with a few new things:

I rebased a few more times to pick up changes.

WRT Neha's comments:
- I made maxIndexEntries configurable by adding the property log.index.max.size. I did this in terms of index file size rather than entries since the user doesn't really know the entry size but may care about the file size.
- For the failing tests: (1) The message set failure is due to scalatest not handling parameterized tests, i had fixed this but somehow it didn't make it into the previous patch. It is in the current one. testHWCheckpointWithFailuresSingleLogSegment is a timing assumption in that test. Fixed it by adding a sleep :-(. The producer test failure I cannot reproduce.
- Wrote a test case using compressed messages to try to produce the corner case at the end of a segment. But actually this turns out not to be possible with compressed messages since the numbering is by the last offset. So effectively our segments are always dense right now. As such I would rather wait until I refactor segment list to fix it since it will be duplicate work otherwise.
- Turns out that log segments are limited to 2GB already, via a restriction in the config. Not actually sure why this is. Given this limitation one cleanup that might be nice to do would be to convert MessageSet.sizeInBytes to an Int, which would remove a lot of casts. Since this is an unrelated cleanup I will not do it in this patch.
- I added support to DumpLogSegment tool to display the index file. I had to revert Jun's change to check that last offset=file size since this is no longer true.

Jun's Comments:
First of all, this is an impressively thorough code review. Thanks!
20.1 Made the Log.findRange comment more reflective of what the method does. I hope to remove this entirely in the next phase.
20.2 Fixed mangled paren in close()
20.3 bytesSinceLastIndexEntry. Yes, good catch. This is screwed up. This was moved into LogSegment, but the read and update are split in two places. Fixed.
20.4 append(): ""We need to have both the begin offset and the end offset returned by Log.append()"". Made Log.append return (Long, Long). I am not wild about this change, but I see the need. I had to refactor KafkaApis slightly since we were constructing an intermediate response object in the produceToLocalLog method (which was kind of weird anyway) so there was only one offset and since this is an API object we can't change it. I think the use of API objects in the business logic is a bit dangerous for this reason.
20.5 Fixed broken log statement to use correct format param.
20.6 truncateTo(): The usage of logEndOffset in the following statement is incorrect. Changed this to use Log.findInRange which I think is the intention.
20.7 ""There are several places where we need to create a log segment and the code for creating the new data file and the new index file is duplicate. Could we create a utility function createNewSegment to share the code?"" Good idea, done. There is still a lot more refactoring that could be done between Log and LogSegment, but I am kind of putting that off.
21. LogSegment: ""bytesSinceLastIndexEntry needs to be updated in append()."" Fixed.
22. FileMessageSet.searchFor() fixed bad byte arithmetic.
23. OffsetIndex:
23.1 Fixed bad english in comment
23.2 mmap initialization: Yes, this doesn't make sense. The correct logic is that the mutable case must be set to index 0, and the read-only case doesn't matter. This was happening implicitly since byte buffers initialize to 0, but I switched it to make it explicit.
23.3 append(): ""If index entry is full, should we automatically roll the log segment?"" This is already handled in Log.maybeRoll(segment) which checks segment.index.isFull
23.4 makeReadOnly(): ""should we call flush after raf.setLength()?"" This is a good point. I think
what you are saying is that the truncate call itself needs the metadata to flush to be considered stable. Calling force on the mmap after the setLength won't do this. Instead I changed the file open to use synchronous mode ""rws"" which should automatically fsync metadata when we call setLength. The existing flush is okay: I verified that flush doesn't cause the sparse file to desparsify or anything like that. ""Also, should we remap the index file to the current length and make it read only?"" Well, this isn't really needed. There is no problem with truncating a file post mmap, but I guess making the mapping read-only could prevent corruption due to any bugs we might have so I made that change.
LogManager
24. ""log indentation already adds LogManager in the prefix of each log entry."" Oops.
25. KafkaApis:
25.1 ""handleFetchRequest: topicDatas is weird since data is the plural form of datum. How about topicDataMap?"" Changed to dataRead (I don't like having the type in the name).
25.2 ""ProducerRequestPurgatory: It seems that it's useful to keep the logIndent since it can distinguish logs from the ProducerRequestPurgatory and FetchRequestPurgatory. Also, it's probably useful to pass in brokerId to RequestPurgatory for debugging unit tests."" Agreed, accidentally removed this; added it back.
26. ""Partition: There are a few places that the first character of info log is changed to lower case. The current convention is to already use upper case."" Made all upper case.
27. ""javaapi.ByteBufferMessageSet: underlying should be private val."" Changed.
28. ""DumpLogSegment: Now that each message stores an offset, we should just print the offset in MessageAndOffset. There is no need for var offset now."" Removed.
29. ""FetchedDataChunk: No need to use val for parameters in constructor since this is a case class now."" Wait is everything a val in a case class? I made this change, but don't know what it means...
30. PartitionData:
30.1 ""No need to redefine equals and hashcode since this is already a case class."" Yeah, this was fixing a bug in the equals/hashcode stuff due to the array that went away when i rebased. Removed it
30.2 ""initialOffset is no longer needed."" I think PartitionData is also used by ProducerRequest. This is a bug, but I think we do need the initial offset for the other case. Until we separate these two I don't think I can remove it.
31. ""PartitionTopicInfo.enqueue(): It seems that next can be computed using shallow iterator."" Ah, very nice. Changed that.
32. ""ByteBufferMessageSet: In create() and decompress(), we probably should close the output and the input stream in a finally clause in case we hit any exception during compression and decompression."" These are not real output streams. I can close them, but they are just arrays so I think it is just noise, no?
33. ""remove unused imports."" Eclipse doesn't identify them, will swing by.
34. ""How do we handle the case that a consumer uses too small a fetch size?"" Added a check and throw for this in ConsumerIterator.
;;;","02/Oct/12 21:33;jkreps;Ran system test, passes:


2012-10-02 14:11:50,376 - INFO - ======================================================
2012-10-02 14:11:50,376 - INFO - stopping all entities
2012-10-02 14:11:50,376 - INFO - ======================================================

2012-10-02 14:12:43,105 - INFO - =================================================
2012-10-02 14:12:43,105 - INFO -                  TEST REPORTS
2012-10-02 14:12:43,105 - INFO - =================================================
2012-10-02 14:12:43,105 - INFO - test_case_name : testcase_1
2012-10-02 14:12:43,105 - INFO - test_class_name : ReplicaBasicTest
2012-10-02 14:12:43,105 - INFO - validation_status : 
2012-10-02 14:12:43,105 - INFO -     Leader Election Latency - iter 2 brokerid 3 : 49636.00 ms
2012-10-02 14:12:43,105 - INFO -     Validate leader election successful : PASSED
2012-10-02 14:12:43,106 - INFO -     Unique messages from consumer : 850
2012-10-02 14:12:43,106 - INFO -     Validate for data matched : PASSED
2012-10-02 14:12:43,106 - INFO -     Unique messages from producer : 850
2012-10-02 14:12:43,106 - INFO -     Leader Election Latency - iter 1 brokerid 2 : 354.00 ms
;;;","03/Oct/12 05:08;junrao;Thanks for patch v3. We are almost there. A few more comments:

40. Log.append: It seems that it's easier if lastOffset returned is just nextOffset instead of nextOffset -1. Then, in KafkaApis, we can just pass end, instead of end+1 to ProducerResponseStatus.

41. OffsetIndex: When initializing mmap, if the index is mutable, shouldn't we move the position to the end of the buffer for append operations?

42. KafkaApis: It's useful to pass in brokerId to RequestPurgatory for debugging unit tests.

43. DumpLogSegments: Currently, the message iterator in FileMessageSet will stop when it hits the first non parsable message. So, we need to check if at the end of the message iteration, location == FileMessageSet.sizeInBytes(). If not, we should report the offset from which data is corrupted.

44. ConsumerIterator: The check for guarding small fetch size doesn't work. This is because in PartitionTopicInfo.enqueue(), we only add ByteBufferMessageSet that has positive valid bytes. We can log an error in PartitionTopicInfo.enqueue() and enqueue a special instance of FetchedDataChunk that indicates an error. In ConsumerIterator, when seeing the special FetchedDataChunk, it can throw an exception.

29. Yes, all parameters in the constructor in a case class are implicitly val.
;;;","03/Oct/12 16:09;junrao;There is another issue:

45. ConsumerIterator: Now that we index each message inside a compressed message, we need to handle the case when a fetch request starting on an offset in the middle of a compressed message. In makeNext(), we need to first skip messages whose offset is less than currentDataChunk.fetchOffset. Otherwise, the consumer would get duplicates. We probably can do this in a followup jira since currently the consumer can get duplicates on compressed messages too.

;;;","03/Oct/12 20:52;jkreps;Here is a new patch that addresses these comments. I also did an incremental diff against the previous patch so you can see the specific changes for the below items (that is KAFKA-506-v4-changes-since-v3.patch)

Also rebased again.

40. I actually disagree. It is more code to add and subtract, but I think it makes more sense. This way we would say the append api returns ""the first and last offset for the messages you appended"" rather than ""the first offset for the messages you appended and the offset of the next message that would be appended"". This is not a huge deal so I can go either way, but I did think about it both ways and that was my rationale.

41. My thinking was that there were only two cases: re-creating a new, mutable index (at position 0) and opening a read-only index. In reality there are three cases: in addition to the previous two you can be re-opening an existing log that went through clean shutdown. I was not handling this properly and in fact was truncating the index on re-open, so the existing entries in the last segment would be unindexed. There are now two cases for mutable indexes. Recall that on clean-shutdown the index is always truncated to the max valid entry. So now when we open an index, if the file exists, I set the position to the end of the file. If the file doesn't exist I allocate it and start at position 0. The recovery process well still re-create the index if it runs, if the shutdown was clean then we will just roll to a new segment on the first append (since the index was truncated, it is now full).

43. I removed that feature since the iterator only has the offset not the file position. However after thinking about it I can add it back by just using MessageSet.entrySize(message) on each entry and use the sum of these to compare to the messageSet.sizeInBytes. Added that.

44. Changed the check to be the messageSet.sizeInBytes. This check was really meant to guard the case where we are at the end of the log and get an empty set. I think it was using validBytes because it needed to calculate the next offset. Now that calculation is gone, so I think it is okay to just use messageSet.sizeInBytes. This would result in a set with 0 valid bytes being enqueued, and then the error getting thrown to the consumer. The fetcher would likely continue to fetch this message set, but that should be bounded by the consumer queue size.

45. The behavior after this patch should be exactly the same as the current behavior, so my hope was to do this as a follow up patch.

Also: Found that I wasn't closing the index when the log was closed, and found a bug in the index re-creation logic in recovery; fixed both, and expanded tests for this.;;;","03/Oct/12 22:01;junrao;Patch v4 looks good overall. A couple of remaining issues:

50. testCompressionSetConsumption seems to fail transiently for me with the following exception. This seems to be related to the change made for #44.
kafka.common.MessageSizeTooLargeException: The broker contains a message larger than the maximum fetch size of this consumer. Increase the fetch size, or decrease the maximum message size the broker will allow.
	at kafka.consumer.ConsumerIterator.makeNext(ConsumerIterator.scala:87)

51. ConsumerIterator: When throwing MessageSizeTooLargeException, could we add the topic/partition/offset to the message string in the exception?
;;;","04/Oct/12 17:22;jkreps;Rebased again and fixed the above issues to make v5

50. I looked into this. It is slightly subtle. The problem was that validBytes is cached in a local variable, and the incremental computation was done on the member variable in ByteBufferMessageSet. The next problem was that AbstractFetcherThread and the ConsumerIterator could both be calling this at the same time, which would lead to setting validBytes to 0 and then iterating over the messages to count the bytes. If the check and the computation occurred at precisely the same time it is possible for validBytes to return essentially any value. The fix is (1) avoid mucking with the MessageSet once it is handed over to ConsumerFetcherThread.processPartitionData, and (2) use a local variable to compute the validbytes, this way even if we do have future threading bugs the worst case is that we recompute the same cached value twice instead of accessing a partial computation (we could also make the variable volatile, but that doesn't really add any additional protection since we don't need precise memory visibility).

51. Done.;;;","04/Oct/12 20:31;junrao;Thanks for patch v5. 

50. There is still a potential issue in that shallowValidByteCount is a long and long value is not guaranteed to be exposed atomically without synchronization in java. So, 1 thread could see a partially updated long value. Thinking about this, since ByteBufferMessageSet is not updatable, is it better to compute validBytes once in the constructor?

51. ConsumerIterator: Could you include currentDataChunk.fetchOffset in the message string in MessageSizeTooLargeException? This will make debugging easier.

Since this is a large patch, it would be good if someone else takes a closer look at it too. At least Neha expressed interests in taking another look at the latest patch.;;;","04/Oct/12 21:25;jkreps;50. It can actually only take Int values, so I don't think this can happen. I will file a follow-up clean-up issue to change sizeInBytes to be an Int (I had mentioned that earlier in the thread) since this anyways leads to innumerable safe-but-annoying casts to int. I think this is better than pre-computing it because in many cases we instantiate a ByteBufferMessageSet without necessarily using validBytes.

51. Yes, I will add this as part of the checkin.;;;","04/Oct/12 22:20;nehanarkhede;I will free up tomorrow after Grace Hopper conference is over. Would like to take another closer look at the follow up patches. If you guys don't mind, please can we hold this at least for this weekend ?;;;","04/Oct/12 22:42;jkreps;It is really hard/error-prone to keep this patch alive and functioning, I basically spend half of each day on rebasing then debugging the new bugs i introduce during rebasing. Could we do it as a post commit review? I am totally down to fix/change things, but the problem is each new change may take a few iterations and meanwhile the whole hunk has to be kept alive. In an ideal world I would have found a way to have done this in smaller pieces, but it is kind of a cross-cutting change so that was hard.;;;","05/Oct/12 01:14;junrao;What we can do is to hold off committing other conflicting patches for now and have this patch more thoroughly reviewed. If there are no major concerns, we can just commit the patch and have follow-up jiras to address minor issues. Neha, do you think that you can finish the review by Saturday?;;;","05/Oct/12 01:19;jjkoshy;Rebasing is painful for sure, especially since 0.8 is moving quite fast. I think the other patches in flight are either small or otherwise straightforward to rebase as they don't have significant overlap. So it seems holding off all check-ins until after this weekend would work for everyone right?
;;;","05/Oct/12 17:42;jkreps;jkreps-mn:kafka-git jkreps$ git pull
remote: Counting objects: 72, done.
remote: Compressing objects: 100% (37/37), done.
remote: Total 42 (delta 26), reused 0 (delta 0)
Unpacking objects: 100% (42/42), done.
From git://git.apache.org/kafka
   0aa1500..65e139c  0.8        -> origin/0.8
Auto-merging core/src/main/scala/kafka/api/FetchResponse.scala
CONFLICT (content): Merge conflict in core/src/main/scala/kafka/api/FetchResponse.scala
Auto-merging core/src/main/scala/kafka/api/ProducerRequest.scala
CONFLICT (content): Merge conflict in core/src/main/scala/kafka/api/ProducerRequest.scala
Auto-merging core/src/main/scala/kafka/consumer/ConsumerFetcherThread.scala
Auto-merging core/src/main/scala/kafka/server/AbstractFetcherThread.scala
CONFLICT (content): Merge conflict in core/src/main/scala/kafka/server/AbstractFetcherThread.scala
Auto-merging core/src/main/scala/kafka/server/KafkaApis.scala
CONFLICT (content): Merge conflict in core/src/main/scala/kafka/server/KafkaApis.scala
Auto-merging core/src/main/scala/kafka/server/ReplicaFetcherThread.scala
Auto-merging core/src/test/scala/unit/kafka/api/RequestResponseSerializationTest.scala
Auto-merging core/src/test/scala/unit/kafka/producer/SyncProducerTest.scala
Auto-merging core/src/test/scala/unit/kafka/utils/TestUtils.scala
Automatic merge failed; fix conflicts and then commit the result.

:-(;;;","05/Oct/12 20:18;jkreps;Rebased patch and improved error message for the MessageSizeTooLargeException.;;;","08/Oct/12 15:55;nehanarkhede;Agree that rebasing is painful. In addition to more reviews, the hope was to check in ~20 more test cases as part of KAFKA-502, so we could test it out thoroughly. But we can check it in and return to fixing issues later as well. ;;;","08/Oct/12 16:00;nehanarkhede;Btw, which svn revision does patch v5 apply correctly on ? ;;;","08/Oct/12 16:06;jkreps;Should apply on HEAD.;;;","08/Oct/12 17:54;junrao;+1 from me.;;;","08/Oct/12 19:15;jkreps;Committed.;;;","09/Oct/12 21:11;nehanarkhede;I have to mention that there is a possibility that some of my comments are not related to this patch directly, but were found while inspecting the new code closely :) Since you know the code better, feel free to file follow up JIRAs

1. Log

1.2 In findRange, the following statements runs the risk of hitting overflow, giving incorrect results from the binary search -
      val mid = ceil((high + low) / 2.0).toInt
Will probably be better to use
     val mid = low + ceil((high - low)/2.0).toInt
1.3 It seems that there are only 2 usages of the findRange API that takes in the array length . We already have an API that covers that use case - findRange[T <: Range](ranges: Array[T], value: Long) and this is used by a majority of API calls.
We can make the findRange method that has the actual binary search logic private and changes the 2 use cases in Log.scala to use the public method that assumes the array length.
1.4 In truncateTo, it is possible that the log file was successfully deleted but the index file was not. In this case, we would end up an unused index file that is never deleted from the kafka log directory.
1.5 In loadSegments, we need to rebuild any missing index files. Or it will error out at a later time. Do we have a follow up JIRA to cover this, it seems like a blocker to me.

2. LogManager
2.1 numPartitions is an unused class variable

3. FileMessageSet
3.1. In searchFor API, fix comment to mention that it searches for the first/least offset that is >= the target offset. Right now it says search for the last offset that is >= target offset
3.2 The searchFor API returns a pair of (offset, position). Right now, it does not always return the offset of the message at the returned position. If the file message set is sparse, it returns the offset of the next message, so the offset and position do not point to the same message in the log. Currently, we are not using the offset returned by the read() API, but in the future if we do, it will be good for it to be consistent.
3.3 In searchFor API, one of the statements uses 12 and the other uses MessageSet.LogOverhead. I think the while condition is better understood if it said MessageSet.LogOverhead.

4. LogSegment
4.1 It is better to make translateOffset return an Option. That way, every usage of this API will be forced to handle the case when the position was not found in the log segment.
4.2 I guess it might make sense to have all the places that uses this segment size to a an Int instead of Long. 

5. ConsumerIterator

Right now, while committing offsets for a compressed message set, the consumer can still get duplicates. However, we could probably fix this by making the ConsumerIterator smarter and discarding messages with offset < fetch offset.

6. ReplicaFetcherThread

When the follower fetches data from the leader, it uses log.append which re-computes the logical message ids. This involves recompression when the data is compressed, which it is in production. This can be avoided by making the data copy from leader -> follower smarter

7. MessageCompressionTest
There are 2 unused imports in this file

8. ByteBufferMessageSet
8.1 There are 3 unused imports in this file
8.2 The return statement in create() API is redundant

9. OffsetIndex
9.1 The last return statement in indexSlotFor is redundant
9.1 The first return statement in indexSlotFor can be safely removed by using case-match or putting the rest of the logic in the else part of if-else block.

10. Performance
Performance test to see the impact on throughput/latency if any due to this patch. What I am curious about is the performance impact due to the following, which are the changes that can impact performance as compared to pre KAFKA-506 -
10.1 Recompression of data during replica reads
10.2 Recompression of data to assign correct offsets inside a compressed message set
10.3 The linear search in the file segment to find the message with a given id. This depends on the index interval and there needs to be a balance between index size and index interval.
10.4 The impact of making the log memory mapped.
10.5 Overhead of using the index to read/write data in Kafka

11. KafkaApis
Unused imports in this file

Just to summarize so that we understand the follow up work and also the JIRAs that got automatically resolved due to this feature. Please correct me if I missed something here -

Follow up JIRAs
1. Retain key in producer (KAFKA-544)
2. Change sizeInBytes() to Int (KAFKA-556)
3. Fix consumer offset commit in ConsumerIterator for compressed message sets (KAFKA-546)
4. Remove the recompression involved while fetching data from follower to leader (KAFKA-557)
5. Rebuild missing index files (KAFKA-561)
6. Add performance test for log subsystem (KAFKA-545)
7. Overall Performance analysis due to the factors listed above

JIRAs resolved due to this feature
1. Fix offsets returned as part of producer response (KAFKA-511)
2. Consumer offset issue during unclean leader election (KAFKA-497)


;;;","10/Oct/12 03:59;jkreps;Hi Neha, here are some comments on your comments and a patch that addresses the comments we are in agreement on.

1. Log
1.2, 1.3 True. This problem exists in both OffsetIndex and Log, though I don't think either are actually possible. In Log this requires one to have 2 billion segment files, though, which is not physically possible; in OffsetIndex one would need to have ~2 billion entries in an index, which isn't possible as the message overhead would fill up the log segment first. I am going to leave it alone in Log since that code I want to delete asap anyway. I fixed it in the OffsetIndex since that code is meant to last.
1.4. This logic is a little odd, I will fix it, but actually this reminds me of a bigger problem. If file.delete() fails on the log file, the presence of that log file will effectively corrupt the log on restart (since we will have a file with the given offset but will also start another log with a parallel offset that we actually append to--on restart the bad file will mask part of the new file). Obviously if file.delete() fails things are pretty fucked and there is nothing we can do in software to recover. So what I would like to do is throw KafkaStorageException and have Partition.makeFollower() shut down the server. What would happen in the leadership transfer if I did that?
1.5 Filed a JIRA for this.

LogManager
2.1 Deleted numPartitions (not related to this patch, I don't think)

FileMessageSet
3.1 Good catch, fixed.
3.2 Right, so I return the offset specifically to be able to differentiate the case where I found the exact location versus the next message. This is important for things like truncate. I always return the offset and corresponding file position of the first offset that meets the >= criteria. So either I am confused, or I think it works the way you are saying it should.
3.3 Well, but the code actually reads and Int and Long out of the resulting buffer, so if MessageSet.LogOverhead != 12 there is a bug, so we aren't abstracting anything just adding a layer of obfuscation. But, yes, it should be consistent, so changed it.

LogSegment
4. LogSegment
4.1 I don't want to allocate an object for each call as this method is internal to LogSegment. I will make it private to emphasize that.
4.2 I agree, though we have had the 2gb limit for a while now so this isn't new. We repurposed KAFKA-556 for this.

5. ConsumerIterator
Agreed. Broke this into a separate issue since current state is no worse than 0.7.x. JIRA is KAFKA-546.

6. ReplicaFetcherThread
Agreed this was discussed above. JIRA is KAFKA-557.

7. Only IDEA detects this, which I don't have. So can't help on this.

8. ByteBufferMessageSet
8.2 Fixed

9. OffsetIndex 
9.1 Fixed
9.2 This is true but I think it would be more convoluted. Simple test and exits make it so you don't have to add another layer of nesting.

10 Agreed, of the various things on my plate I think this is the most important. Any issues here are resolvable, but we need to first get the data.
;;;","10/Oct/12 23:48;jkreps;This patch is identical to the previous Neha related patch except that now in the event that a log segment can't be deleted we throw KafkaStorageException. In KafkaApis.handleLeaderAndISRRequest we catch this exception and shutdown the server.;;;","11/Oct/12 04:04;nehanarkhede;+1. Looks good and thanks for addressing the late review comments. One minor comment -

The following error statement is slightly misleading. The broker could either be in the middle of becoming a leader or a follower, not necessarily the former. 

fatal(""Disk error while becoming leader."");;;","11/Oct/12 16:07;jkreps;Ah, nice catch. Changed it to ""Disk error during leadership change.""

Checked in with the change.;;;","12/Oct/12 00:05;smeder;I think you missed a change to KafkaETLContext. It needs:

diff --git a/contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLContext.java b/contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLContext.java
index bca1757..9498169 100644
--- a/contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLContext.java
+++ b/contrib/hadoop-consumer/src/main/java/kafka/etl/KafkaETLContext.java
@@ -205,7 +205,7 @@ public class KafkaETLContext {
             
             key.set(_index, _offset, messageAndOffset.message().checksum());
             
-            _offset = messageAndOffset.offset();  //increase offset
+            _offset = messageAndOffset.nextOffset();  //increase offset
             _count ++;  //increase count
             
             return true;

or something similar. As it stands it'll run forever...
;;;","12/Oct/12 00:16;jkreps;Ack, nice catch, fixed it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsatisfiedLinkError causes snappy unit tests to fail on hudson server,KAFKA-504,12606971,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,10/Sep/12 23:56,08/Oct/12 19:19,14/Jul/23 05:39,08/Oct/12 19:19,,,,,,,,,,,,,,0,,,,"I am not sure why this happens. It may be that some of the hudson slaves that Apache uses aren't running Linux or x86 or it may be some bug in the library packaging. In any case, we can't assume that native libraries will always load, so I propose just making the test pass if the library is not loadable.",,charmalloc,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/12 00:09;jkreps;KAFKA-504.patch;https://issues.apache.org/jira/secure/attachment/12544565/KAFKA-504.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241656,,,Mon Oct 08 19:19:38 UTC 2012,,,,,,,,,,"0|i029mn:",11161,,,,,,,,,,,,,,,,,,,,"11/Sep/12 00:08;jkreps;Check if snappy is loadable before running the unit test. If snappy can't be loaded, test is a no-op.;;;","11/Sep/12 00:10;jkreps;Note that I can't really test this case very well since I don't have a non-supported platform, but presumably this will fix the failing test on the test server.;;;","05/Oct/12 21:03;charmalloc;patch applies cleanly, tests passed, lets see how it goes through the CI

+1 ;;;","08/Oct/12 19:19;jkreps;Included in KAFKA-506;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getOfffset Api needs to return different latest offset to regular and follower consumers,KAFKA-501,12606803,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jjkoshy,junrao,junrao,10/Sep/12 00:09,01/Oct/12 04:08,14/Jul/23 05:39,27/Sep/12 19:21,,,,0.8.0,,,,,,,,,,0,bugs,,,"For follower consumers, getOffset should return logEndOffset as the latest offset. For regular consumers, getOffset should return highWatermark as the latest offset. ",,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/12 01:57;jjkoshy;KAFKA-501-v1.patch;https://issues.apache.org/jira/secure/attachment/12546633/KAFKA-501-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,291941,,,Mon Oct 01 04:08:56 UTC 2012,,,,,,,,,,"0|i0rhqn:",158518,,,,,,,,,,,,,,,,,,,,"26/Sep/12 01:57;jjkoshy;Overview of changes:
1 - For non-followers, collapse any offsets > hw into a single entry == hw.
2 - I think the returned offsets are reverse sorted, but I did that again
  anyway, for the above collapsing to work correctly. I could just as well
  use Seq.span.
3 - Made the offsetrequest batched, using a map. So this is very similar to
  what was done for KAFKA-391. So I also needed to provide javaapi versions
  of the OffsetRequest and OffsetResponse.
4 - One side-effect of batching was I was forced to clean up the layering
  issue. i.e., I prevent log from taking offsetrequest.
5 - Switched from Array to Seq in the scala version (and got rid of the
  equals).
6 - For consistency, made simple-consumer's getOffsetsBefore take a request
  and return a response.

Other points:
7 - I'm not very sure we should be providing the OffsetRequest directly to
  clients - i.e., maybe we should only have a request builder (like we have
  for FetchRequest). This would make it easier to use from the Java side and
  also we can hide the replicaId option - maybe it shouldn't be exposed to
  clients (especially on the Java-side)
8 - Code in getOffsetsBefore can be cleaned up quite a bit I think, but I left
  it as is for now.
9 - I was thinking of nesting PartitionOffsetRequestInfo inside the
  OffsetRequest object - that would make the code clearer, but only if users
  always fully qualify the reference.
10 - Previously getOffsetsBefore could through an exception on Error. Now, the
  client will need to explicitly check the errorCode. We may need to go
  through all usages and assess whether we want to throw an exception or
  not.
;;;","27/Sep/12 05:19;junrao;Thanks for patch v1. Looks good overall. Some minor comments:

1. javaapi.OffsetRequest: No need to expose replicaId in the constructor since all java clients are non-follower.

2. Log.getOffsetBefore, KafkaApis: So far, offsets are returned in ascending order.

3. removed unused imports: LogManager, LogOffsetTest

4. UpdateOffsetsInZK: Not directly related to this jira, but this class should probably be moved to tools package.

5. ZookeeperConsumerConnector: Do we need to import scala.Some?

Also, could you rebase?;;;","27/Sep/12 14:48;junrao;Actually, for #2, you are right, offsets are returned in descending order. So this is fine.

If the rest of the comments are addressed, the patch can be checked in without another round of review.;;;","27/Sep/12 19:21;jjkoshy;Thanks for the review. Addressed the above comments and committed to 0.8.;;;","01/Oct/12 04:08;junrao;Just realized that we need to patch KafkaETLContext to use getMetaData api to figure out the leader so that it can make the getOffsetBefore call.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
javaapi support for getTopoicMetaData,KAFKA-500,12606799,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,junrao,junrao,09/Sep/12 21:41,25/Sep/12 00:44,14/Jul/23 05:39,19/Sep/12 22:28,,,,0.8.0,,,,,,,core,,,0,bugs,,,TopicMetaRequest and TopicMetaResponse use scala Seq and Option. We need a version so that java applications can use more easily.,,junrao,smeder,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/12 00:24;swapnilghike;kafka-500-v1.patch;https://issues.apache.org/jira/secure/attachment/12545254/kafka-500-v1.patch","18/Sep/12 02:04;swapnilghike;kafka-500-v2.patch;https://issues.apache.org/jira/secure/attachment/12545515/kafka-500-v2.patch","18/Sep/12 19:09;swapnilghike;kafka-500-v3.patch;https://issues.apache.org/jira/secure/attachment/12545605/kafka-500-v3.patch","19/Sep/12 10:12;swapnilghike;kafka-500-v4.patch;https://issues.apache.org/jira/secure/attachment/12545704/kafka-500-v4.patch","19/Sep/12 21:06;swapnilghike;kafka-500-v5.patch;https://issues.apache.org/jira/secure/attachment/12545801/kafka-500-v5.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299150,,,Wed Sep 19 22:28:50 UTC 2012,,,,,,,,,,"0|i15zwn:",243117,,,,,,,,,,,,,,,,,,,,"15/Sep/12 00:24;swapnilghike;1. Created java apis for TopicMetadataRequest and TopicMetadataResponse. 

2. Added a send(topicMetadataRequest) method in javaapi/SyncProducer.

3. Created unit tests to make sure that the underlying object is correctly created for javaapi/TopicMetadataRequest and javaapi/TopicMetadataResponse. (Are they an overkill?)

4. Removed detailedMetadata, timestamp and count from TopicMetadataRequest as those fields were not being used anywhere.

5. Refactored kafka.api.TopicMetaDataResponse to TopicMetadataResponse. Sorry, this patch will touch a couple of more files.;;;","17/Sep/12 16:43;junrao;Thanks for patch v1. Some comments:

1. javaapi.SyncProducer returns a list of the scala version of TopicMetadata. We need to create a java version of TopicMetadata and PartitionMetadata to hide things like Seq and Option. Also, since we only return TopicMetadata to the java client. There is probably no need for a java version of TopicMetadataResponse.

2. In kafka-391, we are getting rid of the java version of SyncProducer since it's no longer a public api (java users should use Producer instead). So, there is no need to put the getMetadata api in javaapi.SyncProducer. However, we should include it in javaapi.SimpleConsumer, which remains a public api.

3. TopicMetadataResponseTest: The test of the java version of the response can probably be made simpler. We just need to make sure that we can convert from a scala response to a java response correctly.

4. Patch doesn't apply cleanly, likely due to renaming class names.;;;","18/Sep/12 02:04;swapnilghike;1. Rebased. javaapi/SyncProducer is gone.

2. Removed the two step creation of underlying objects for objects that won't be created by producer/consumer clients. This will be the case of javaapi/{TopicMetadata, PartitionMetadata, ByteBufferMessageSet, FetchResponse, and the future TopicMetadataResponse that will be added with the completion of KAFKA-473}. The underlying object is now directly passed as a constructor private val. 

3. Added java apis for TopicMetadata and PartitionMetadata given their respective scala objects. I am not sure if we need to expose all the fields to clients via getters, please advise.

4. Removed unit tests from the v1 patch as the new javaapi code merely wraps over the scala code.

5. Cleaned up ByteBufferMessageSetTest, javaapi/Implicits.scala.

6. Removed the logMetadata field from PartitionMetadata as it was not used anywhere and was merely going around in buffers. Removed the classes LogMetadata and LogSegmentMetadata as well.

7.  Removed detailedMetadata, timestamp and count from TopicMetadataRequest as those fields were not being used anywhere. 

8. Refactored kafka.api.TopicMetaDataResponse to TopicMetadataResponse.;;;","18/Sep/12 16:00;junrao;Thanks for patch v2. Can't seem to apply the patch. Got the following error.

patching file core/src/main/scala/kafka/api/TopicMetaDataResponse.scala
can't find file to patch at input line 438
Perhaps you used the wrong -p or --strip option?
The text leading up to this was:
--------------------------
|Index: core/src/main/scala/kafka/api/TopicMetadataResponse.scala
|===================================================================
|--- core/src/main/scala/kafka/api/TopicMetadataResponse.scala	(working copy)
|+++ core/src/main/scala/kafka/api/TopicMetadataResponse.scala	(working copy)
--------------------------
File to patch: ^C

A few minor comments:
20. ByteBufferMessageSet: The way that we write equal, we don't really need canEqual.

21. TopicMetadataRequest: remove unused imports


;;;","18/Sep/12 19:09;swapnilghike;1. Probably svn generated a wrong diff due to a simultaneous rename and modify. As advised, deleted the old file and simply added the new one.

2. Removed canEqual.

3. Removed unused imports one last time. :);;;","19/Sep/12 01:13;junrao;Thanks for patch v3. Looks good. A few more things:

30. javaapi.PartitionMetadata, javaapi.TopicMetadata: no need to expose method writeTo().

31. Could you expose the getMetadata api in SimpleConsumer (both scala and java)?

32. Cloud you take care of kafka-505 in this patch too since it's small and is related to TopicMetadata?;;;","19/Sep/12 10:12;swapnilghike;1. Removed writeTo() from javaapi.{PartitionMetadata, TopicMetadata}.

2. Exposed send(TopicMetadataRequest) in javaapi/Simpleconsumer and kafka/api/SimpleConsumer. 

3. KAFKA-505 : Removed errorCode from TopicMetadataResponse, accordingly modified server.handleTopicMetadataRequest.

4. I tried to use generic parameters in javaapi/Implicits.scala but that's probably not going to work because we have different return types for the implicit defs. So in order to keep the javaapi code clean, I created two implicit defs to convert Seq[api.*] to java.util.List[*] and one implicit def to convert Option[*] to *.;;;","19/Sep/12 16:05;junrao;Thanks for patch v4. Thinking about this a bit more. To be consistent with other requests, it's probably better to return TopicMetadataResponse in both SyncProducer and SimpleConsumer, instead of Seq[TopicMetadata]. So we will also need a java version of TopicMetadataResponse. The implicits will be a bit easier now since we just need to define a conversion at TopicMetadataResponse level.;;;","19/Sep/12 21:06;swapnilghike;1. Yes, {SyncProducer, SimpleConsumer}.send() now returns TopicMetadataResponse. 

2. Added the javaapi for TopicMetadataResponse.

3. Rearranged implicits as discussed.;;;","19/Sep/12 22:28;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor controller state machine ,KAFKA-499,12606438,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,06/Sep/12 16:57,18/Sep/12 20:35,14/Jul/23 05:39,18/Sep/12 17:30,0.8.0,,,0.8.0,,,,,,,,,,0,optimization,,,"Currently, the controller logic is very procedural and is similar to KafkaZookeeper. Controller should have a well defined state machine with states and transitions. This will make it easier to understand and maintain the controller code. ",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,345600,345600,,0%,345600,345600,,,,,KAFKA-43,KAFKA-42,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/12 17:01;nehanarkhede;kafka-499-v1.patch;https://issues.apache.org/jira/secure/attachment/12545171/kafka-499-v1.patch","14/Sep/12 21:06;nehanarkhede;kafka-499-v2.patch;https://issues.apache.org/jira/secure/attachment/12545214/kafka-499-v2.patch","17/Sep/12 21:42;nehanarkhede;kafka-499-v3.patch;https://issues.apache.org/jira/secure/attachment/12545481/kafka-499-v3.patch","18/Sep/12 00:58;nehanarkhede;kafka-499-v4.patch;https://issues.apache.org/jira/secure/attachment/12545508/kafka-499-v4.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299149,,,Tue Sep 18 17:30:52 UTC 2012,,,,,,,,,,"0|i15zwf:",243116,,,,,,,,,,,,,,,,,,,,"14/Sep/12 16:59;nehanarkhede;This patch refactors (almost rewrites) the controller. The controller has the following components -
1. A partition state machine.
2. A replica state machine
3. Controller channel manager for communication with the brokers.

1. Partition state machine
 This represents the state machine for partitions. It defines the states that a partition can be in, and
 transitions to move the partition to some legal state. The different states that a partition can be in are -
 1. NonExistentPartition: This state indicates that the partition was either never created or was created and then
deleted. Valid previous state, if one exists, is OfflinePartition
 2. NewPartition        : After creation, the partition is in the NewPartition state. In this state, the partition should have replicas assigned to it, but no leader/isr yet. Valid previous states are NonExistentPartition
 3. OnlinePartition     : Once a leader is elected for a partition, it is in the OnlinePartition state. Valid previous states are NewPartition/OfflinePartition
 4. OfflinePartition    : If, after successful leader election, the leader for partition dies, then the partition moves to the OfflinePartition state. Valid previous states are NewPartition/OnlinePartition

2. Replica state machine
  This class represents the state machine for replicas. It defines the states that a replica can be in, and
  transitions to move the replica to another legal state. The different states that a replica can be in are -
  1. OnlineReplica     : Once a replica is started, it is in this state. Valid previous state are OnlineReplica or OfflineReplica
  2. OfflineReplica    : If a replica dies, it moves to this state. Valid previous state is OnlineReplica. 
There might be some changes to this state machine after KAFKA-42 is checked in. For now, these are the relevant states that a replica can be in.

3. Controller channel manager

  Everything related to communication between controller and broker is moved here. This includes the request channels to the brokers, the request send thread, and the request batching mechanism. Since the request batching mechanism is new, I'll explain it here -
ControllerBrokerRequestBatch
3.1. All the state changes initiated by the controller requires batching of requests per broker. Previously, the code to achieve this was copy pasted in several state changes APIs. I moved it into one class that allows the controller to -
3.1.1. newBatch() - start a new request batch when a set of state changes need to be initiated
3.1.2  sendRequestsToBrokers() - collate the state changes per broker and send them

4. Testing

 While I was testing the controller code, I found that most of our existing unit tests already cover most of the corner cases. For example, LogRecoveryTest and LeaderElectionTest. However, I guess the controller needs more system testing where we have multiple topics and partitions and it -
4.1 Introduces soft (GC) as well as hard failures to trigger controller failover. Right now, our tests only cover controller failures (clean shutdown)
4.2 Introduce state changes for multiple partitions in quick succession.
I feel the above test could be cleanly done as a system test. Since this patch blocks KAFKA-42, KAFKA-43 and some other controller related JIRAs, I thought I can push this to a separate patch or under KAFKA-42. If people feel it is important to add it as part of this patch, let me know, I can get it in.

5. I wanted to include a state change diagram to make it easier to understand both the partitions and replica state machines. I think I will add that to the Internal Design wiki soon.
;;;","14/Sep/12 21:06;nehanarkhede;Rebased and fixed a minor JMX bean issue;;;","16/Sep/12 19:33;junrao;Thanks for patch v2. It's a big change. However, it's changing things in the right direction. It makes the logic cleaner and covers some of the corner cases that were missed before. Excellent job! Some comments:

20. ReplicaStateMachine:
20.1 handleStateChanges(): There is no need to read the partition assignment from ZK. We can just use the cache version in controller context.
20.2 handleStateChange(): We can use leaderAndIsr in the last parameter of the following method directly.
                  brokerRequestBatch.addRequestForBrokers(List(replicaId), topic, partition,
                    controllerContext.allLeaders((topic, partition)))
20.3 handleStateChange(): For the OfflineReplica case, we should send new LeaderISRRequests to the leader broker. We should also increment the leader epoc. In theory, we just need to generate new LeaderISRRequests for partitions that lost a follower, not the leader (those partitions are handled in partitionStateChange already). For the latter partitions, we will be sending the same LeaderISRRequests twice, once through partitionStateTransition, and another through replicaStateTransition. However, we don't really need to optimize this right now since the makeLeader process on a broker is very cheap.
20.4 handleStateChange(): The OnlineReplica case is a bit special in that valid previous states include OnlineReplica itself. No other state transition allows transition from one state to itself. This seems to be due to that in initializeReplicaState(), we already initialized some replicas to online state already. However, in general, should we allow self transition for all states? This seems to be safer and will guard the case when we somehow redid the same transition again.
20.5 Before this patch, we will send LeaderAndISR requests with the INIT flag in 2 cases: one during controller failover and another during broker startup. The only purpose for the INIT flad is really to ensure that a broker will clean up deleted topics properly. Thinking about this more, using INIT is really a hacky way to handle topic deletion when we do have a clean state machine. Now that we have one, a better way is probably to use a separate topic deletion path and only delete it once every broker has received the stop replica request. On both controller failover and replica startup, the new controller can find out all topics that still need to be deleted and resend the stop replica requests. This seems to be a better way of handling deletion. So, in this patch, maybe we can get rid of the INIT flag and not to worry about deletion. We can add the proper delete state transition when we get to topic deletion.

21. PartitionStateMachine:
21.1 isShuttingDown needs to be set to false in startup, not just in constructor since startup can be called multiple times. Ditto for ReplicaStateMachine.
21.2 initializeLeaderAndIsrForPartition(): Do we really need to read leaderAndISR from ZK before updating it? In this case, we expect that path not to exist. We can just create the path with the initial leaderAndISR. If the path already exists, we will get a ZK exception and we can log a state change failure.
21.3 electLeaderForPartition(): we should only update controllerContext.allLeaders if zookeeperPathUpdateSucceeded is true.
21.4 handleStateChange(): We should update controllerContext.allLeaders in both the OfflinePartition and the NonExistentPartition case.
21.5 remove unused imports

22. RequestPurgatoryTest: Are the changes om testRequestExpiry() needed?

23. ControllerBrokerRequestBatch.addRequestForBrokers(): The following statement seems to be unnecessarily creating a new HashMap on each call (except for the first one, which is necessary).
      brokerRequestMap.getOrElseUpdate(brokerId, new mutable.HashMap[(String, Int), LeaderAndIsr])

24. KafkaController:
24.1 can startChannelManager() be part of initializeControllerContext?
24.2 In ControllerContext.allLeaders, we just need to cache the leader of each partition, not the ISR. This is because ISR can be changed by the leader at any time, which will invalidate the cache in the controller. Every time that the controller wants to update leaderAndISR in ZK, it always has to read the latest ISR from ZK anyway. Also, there is no need to call updateLeaderAndIsrCache in onBrokerFailure and onBrokerStartup. The leader cache is always updated when there is a leader change. ISR always has to be read from ZK before each update.
24.3 We need to hold a controller lock whenever we call onControllerFailover()  since it registers listeners and therefore allows the watcher handler to update cached maps in controller context concurrently. Currently, ZookeeperLeaderElector.startup can call onControllerFailover() without the controller lock.
24.4 It seems that all ZK listeners like BrokerChangeListener, TopicChangeListener and PartitionChangeListener should be in controller, not inside ReplicaStateMachine and PartitionStateMachine. Both new-topic and broker failure watchers are defined in controller and state change machines just need to act on state changes.

25. ZookeeperLeaderElector:
25.1 Currently, it seems that we can call onBecomingLeader() twice, once in startup and another in LeaderChangeListener.handleDataChange. This may not cause any harm now, but it would be better if we can avoid it. One possible way is to call onBecomingLeader in elect if the election wins. Then we can get rid of the code in LeaderChangeListener.handleDataChange and the call to onBecomingLeader() in startup.
25.2 The name ZookeeperLeaderElector seems very generic. Should we name it to ControllerElector? Ditto for LeaderChangeListener?
25.3 The subscription of leaderChangeListener needs to be done in elect() after the ephemeral path is created successfully, not in startup. This is because we need to set the watcher each time the controller path is created and only the subscription sets the watcher.
;;;","17/Sep/12 21:42;nehanarkhede;Appreciate your patience in reviewing the controller code (again). Thanks a lot for the detailed review !
0. ReplicaStateMachine:
0.1 handleStateChanges(): Right. Changed it to use the cached version in controller context.
0.2 handleStateChange(): Changed it.
0.3 handleStateChange(): Good catch! I had forgotten to include the actual request send. Also, in addition to sending the LeaderAndIsr request to the leader replica, we also should update the controller's allLeaders cache. And I agree that we shouldn't try to optimize the non-follower case, at least just yet.
0.4 handleStateChange(): You raise a good point here. I think the OnlineReplica state description is a little fuzzy right now. I hope that after KAFKA-42, it will be more clear. You are right that it is special from the others and this is due to the hacky way we use the INIT flag to resend the list of partitions to all the replicas. I actually don't think we should be handling deletes the way we do today. I think you've agreed on this as part of 20.5 below. However, until we have that, OnlineReplica probably might stay special. Also, I'm not sure the other states can be subjected to self transitions without fully knowing the triggers that can cause such self transitions. I can revisit all states and see if we can safely allow self transitions for those as well.
0.5 Awesome, I remember discussing the hackiness of this approach during the design discussions. Great that you agree with me here. Let's push this work to KAFKA-330. Until then, I've removed the INIT flag support.

1. PartitionStateMachine:
1.1 Right, this probably happened since the startup() API was an after-thought. Fixed it.
1.2 initializeLeaderAndIsrForPartition(): Good point. Fixed it.
1.3 electLeaderForPartition(): True, fixed it
1.4 handleStateChange(): Actually, changing the state of the partition has no real effect on the allLeaders cache, unless a new leader is elected or ISR has changed. We remove the partition from the allLeaders cache only when it is deleted. We add a partition to this cache, when it enters the OnlinePartition state.

2. RequestPurgatoryTest: This was accidental, probably during the rebase. Reverted it.

3. ControllerBrokerRequestBatch.addRequestForBrokers(): It will create a new hashmap only if it executes that part of the code, which is when the brokerId doesn't exist.

4. KafkaController:
4.1 Moved startChannelManager to initializeControllerContext.
4.2 Very good point. I agree that we shouldn't really be caching the ISR. I had initially changed it to cover the Online/OfflineReplica state change user case to send the LeaderAndIsr request to the affected brokers, without realizing that caching ISR after that is not useful/safe.
4.3 Good catch ! Fixed it.
4.4 They were in controller initially, but I moved it to the respective state machine on purpose. The reason is that I thought it will be good for each state machine to also have the appropriate zookeeper listeners that trigger the state changes wrapped in that state machine. The reason that the controller has the callbacks is because the controller needs to notify multiple state machines about the trigger in a certain order. So you can look at the controller as a bunch of callbacks required to handle the replication state machine transitions correctly. That is, controller offers a high level view of the all the possible meta state changes and triggers that can happen as part of replication. If one needs to dig deeper, they can look at the individual state machines for the details.

5. ZookeeperLeaderElector:
5.1 Actually, we don't need to call onBecomingLeader in the startup(), that is redundant. It needs to be called when the leader successfully writes the new ephemeral node in zookeeper. So, this can happen only once either in the LeaderChangeListener or inside elect(). I moved it to elect()
5.2 It is generic for a purpose. When we include the consumer co-ordinator change, it will require to use the same ZookeeperLeaderElector component for the election of the consumer co-ordinator. Since it is written to be generic, the name is generic too.
5.3 Actually, the point is that you just need to register a watcher on startup and then on new session creation. Since it is a data change listener, it also includes an exists listener. The problem was that on session expiration, it wasn't re-registering the leader change listener. This is a bug. I fixed it to move the registration as the first line in elect() API.

6. Removed all code that referred to the INIT flag in the LeaderAndIsrRequest
;;;","18/Sep/12 00:58;nehanarkhede;Rebased after KAFKA-391's checkin. ;;;","18/Sep/12 04:37;junrao;Thanks for patch v4. Have a few minor comments. Once those are addressed, the patch can be checked in without another review.

40. ControllerBrokerRequestBatch.sendRequestsToBrokers(): no need for  isInit in constructor.

41. ReplicaStateMachine: The purpose of isShuttingDown is probably to guard that we don't startup (shutdown) again if it's already started (shutdown). If so, we should do that. Otherwise, there is really no need for isShuttingDown. Ditto for PartitionStateMachine.

42. PartitionStateMachine: In all places that we mark a partition offline, we should remove the partition from controllerContext.allLeaders.

43. ZookeeperLeaderElector: We can remove all the code in LeaderChangeListener.handleDataChange() now.
;;;","18/Sep/12 17:14;nehanarkhede;ReplicaStateMachine: The purpose of isShuttingDown is to avoid triggering state changes from the BrokerChangeListener when the state machine is in the middle of a shutdown. Ditto for PartitionStateMachine.
PartitionStateMachine: The allLeaders cache stores the controller's leader decision for all existing partitions in zookeeper. A partition is deleted from this list only when it is deleted.

Rest of the comments are addressed.;;;","18/Sep/12 17:26;junrao;That sounds good. +1 on the patch.;;;","18/Sep/12 17:30;nehanarkhede;Thanks for the review ! Just committed it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller code has race conditions and synchronization bugs,KAFKA-498,12606436,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,06/Sep/12 16:53,12/Sep/12 04:14,14/Jul/23 05:39,11/Sep/12 20:42,0.8.0,,,,,,,,,,,,,0,bugs,,,The controller maintains some internal data structures that are updated by state changes triggered by zookeeper listeners. There are race conditions in the controller channel manager and the controller state machine.,,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,KAFKA-42,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/12 18:09;nehanarkhede;kafka-498-v1.patch;https://issues.apache.org/jira/secure/attachment/12544084/kafka-498-v1.patch","07/Sep/12 18:08;nehanarkhede;kafka-498-v2.patch;https://issues.apache.org/jira/secure/attachment/12544250/kafka-498-v2.patch","06/Sep/12 22:59;nehanarkhede;kafka-498-v2.patch;https://issues.apache.org/jira/secure/attachment/12544129/kafka-498-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299148,,,Fri Sep 07 22:34:22 UTC 2012,,,,,,,,,,"0|i15zw7:",243115,,,,,,,,,,,,,,,,,,,,"06/Sep/12 18:09;nehanarkhede;1. Refactoring of current procedural controller code to functional style
1.1. Using switch case statements instead of if-else
1.2. Avoid explicit return statements from try-catch blocks
1.3. Use Option instead of null. This is very important, there were several places that handled only the non-null case, basically making way for NPE's in error cases.
1.4. Rename variables in all capital letters to camel case
1.5. Fixed logging since a lot of log statements were at info and were somewhat unclear
1.6. Fixed indentation, some places uses 4 spaces, others used 2 spaces.
1.7. ZkUtils.scala
1.7.1 Refactored readDataMaybeNull to return an Option instead of null. This allows all usages of that API that query an ephemeral path to handle the case when the value in zookeeper does not exist anymore
1.7.2. Refactored getBrokerInfoFromIds to handle only a single broker id. The reason is that most usages of that API used it to query for a single broker id.
1.8 Many places in the code did not wrap the lines correctly. Fixed this as much as I could.

2. KafkaController.scala
2.1/ Renamed deliverLeaderAndISRFromZookeeper to readLeaderAndIsrFromZookeeper
2.2 There is a race condition in KafkaController where it doesn't synchronize access to the controller's data structures while creating a new session. Basically, controllerRegisterOrFailover is a private API that modifies almost all the internal controller data structures that require synchronization. Since this API is not synchronizing on the controller lock, all usages of this API need to do this correctly. Fixed handleNewSession to synchronize the controllerRegisterOrFailover API, since that can be concurrently executed with the startup procedure.
2.3 In onBrokerChange, defaulting to empty list instead of null. This lets us avoid null checks
2.4 Refactored onBrokerChange() API and moved the leader election logic to a separate API. After starting on this path, I figured that this code is going to need a major refactoring that I'd like to fix in a separate patch. Filed KAFKA-499 to cover that. For now, we can keep this although it will look complete only after KAFKA-499 is in.
2.5 onBrokerChange() and initLeaders() APIs are very similar and duplicate quite a lot of code. I refactored onBrokerChange() but realized later that initLeaders would have to refactored too. To keep the changes in this patch small enough, I will fix this as part of KAFKA-499.

3. ControllerChannelManager & KafkaController
3.1 There are 3 types of information that the controller maintains per broker - request thread, message queue and socket channel. Currently, they are maintained in 3 separate variables and we need to ensure all 3 are synchronized correctly. To simplify this, I created a case class to wrap this state in one object.
3.2 There is a lock object whose purpose is to synchronize access to the broker cache. Currently, the lock doesn't seem to protect all access to these caches, which looks like a synchronization bug. Fixed this to have startup API synchronize access to the broker cache.
3.3. The addBroker logic was duplicated in 2 places. Refactored the constructors to add an auxilary constructor to call the addBroker API that handles creating controller state info for a new broker and the appropriate synchronization. Currently, the constructor duplicates code to add a new broker, probably since it is not right to invoke an API from inside the primary constructor
3.4 There are 2 ways to remove a broker from the controller's broker cache - during shutdown of the channel manager or when a broker change listener fires. Since removeBroker is a public API, it is synchronized using the brokerLock. The shutdown API calls removeBroker internally and ends up acquiring and releasing the lock multiple times. Ideally, it is sufficient to acquire the brokerLock just once for the entire shutdown API. Refactored to move the common removeBroker logic to a private API that doesn't synchronize on the brokerLock. Changed removeBroker to acquire lock and call removeExistingBroker, changed shutdown to acquire lock once and call removeExistingBroker.

4. Renamed LeaderAndISR to LeaderAndIsr

5. Renamed BrokerNotExistException to BrokerNotAvailableException to remain consistent with other exceptions of the same type (LeaderNotAvailableException, ReplicaNotAvailableException)


NOTE: Apache svn seems to hang at the time of uploading this patch. It will apply cleanly on revision 1380945;;;","06/Sep/12 22:59;nehanarkhede;Deleted an unused file, fixed minor typo;;;","07/Sep/12 17:26;junrao;Thanks for the patch. Overall, a good cleanup patch. Some minor comments:

1. ControllerBrokerStateInfo: Should we include Broker in this class too?

2. ControllerChannelManager:
2.1  brokers.foreach(broker => brokerStateInfo(broker._1).requestSendThread.start()) can be done as 
    brokers.foreach{ case(brokerId, _)  => brokerStateInfo(brokerId).requestSendThread.start() }
2.2 startup: Should call startRequestSendThread()

3. readLeaderAndIsrFromZookeeper:
3.1 This method also sends leaderAndISR requests to brokers, in addition to reading from ZK. Maybe we can call it readAndSendLeaderAndIsrFromZookeeper?
3.2 If no replicas are assigned to a partition, is it necessary to log the assignment for all partitions? Ditto for onBrokerChange;;;","07/Sep/12 18:08;nehanarkhede;Thanks for the review !

1. Good point. Included Broker in ControllerBrokerStateInfo
2.1 brokerStateInfo.foreach(brokerState => startRequestSendThread(brokerState._1)) looks simpler
2.2 Al though it will be slightly inefficient to do that, it probably is easier to read
3.1 This API is pretty badly named and designed. It will be refactored in KAFKA-499, but changed the name to what you
suggested for now.
3.2 Not sure it is very useful in the context of the controller's state machine. But right now, it is very unclear as
to what the state machine is and which transitions cause what state changes. I think I will have a better handle on this in KAFKA-499.
4. Synchronized the addBroker public API on the brokerLock
5. Synchronized the sendRequest public API on the brokerLock
;;;","07/Sep/12 22:34;junrao;+1 on the latest patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
recover consumer during unclean leadership change,KAFKA-497,12606434,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,,junrao,junrao,06/Sep/12 16:51,09/Oct/12 15:48,14/Jul/23 05:39,09/Oct/12 15:48,,,,0.8.0,,,,,,,core,,,0,bugs,,,"When we do an unclean leadership change (i.e., no live broker in ISR), offsets already exposed to consumers may no longer be valid. As a result, a consumer could either receive an OffsetOutOfRange exception or hit an InvalidMessageSizeException while iterating a fetched message set. In either case, we need to be able to recover the consumption (probably from the current latest offset from the leader). We need to recover both regular and follower consumers.",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241609,,,Tue Oct 09 15:48:29 UTC 2012,,,,,,,,,,"0|i02993:",11100,,,,,,,,,,,,,,,,,,,,"09/Oct/12 15:48;junrao;With KAFKA-506, this is no longer an issue. Since offsets are logical, the consumer can never see a partial message. During unclean leader election, a small number of messages may not match among replicas. So, it's possible for a consumer to see a different message on the same offset at different point of time. However, this is ok since during unclean leader election, there is no guarantee that all data is protected.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
high level producer send should return a response,KAFKA-496,12606425,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jkreps,junrao,junrao,06/Sep/12 16:10,11/Apr/16 23:18,14/Jul/23 05:39,09/Feb/14 23:58,,,,0.9.0.0,,,,,,,core,,,1,features,,,"Currently, Producer.send() doesn't return any value. In 0.8, since each produce request will be acked, we should pass the response back. What we can do is that if the producer is in sync mode, we can return a map of (topic,partitionId) -> (errorcode, offset). If the producer is in async mode, we can just return a null.",,adenysenko,clehene,craigwblake,jkreps,junrao,matan,nehanarkhede,noslowerdna,steamshon,,,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,KAFKA-1227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241618,,,Sat Feb 01 19:39:31 UTC 2014,,,,,,,,,,"0|i029br:",11112,,,,,,,,,,,,,,,,,,,,"06/Sep/12 17:13;jkreps;An alternative would be give a Future back which would be immediately satisfied in the case of a synchronous call and would eventually become satisfied in the event of an async call. This is probably slightly harder to implement.;;;","06/Sep/12 17:32;jkreps;A more concrete description would be that we return a ProduceResponse which has an offset and error. Calls to 
  response.offset() or response.error()
would block until the request completed in the case of an async request. We would probably also need 
  response.onComplete(fun)
to register a callback that would be run when the response was done.

One question, though, is whether an error should result in an exception or in an error code when you try to get either field.

The advantage of this is that the semantics of produce would remain the same for both sync and async. Code written to work with sync could be changed to async with only a config change.

It would be worth thinking through if there is a use case for this because it is likely a bit more complicated.;;;","18/Sep/12 17:55;junrao;The following is a straw-man proposal:

Define the following class and trait.

class ProducerCallbackResponse (val response: ProducerResponse) {
}

trait ProducerCallback[V] {
  def onSerializationError(event: V, e: Throwable)
  def onSendResponse(response: ProducerResponse)

  def getCallbackResponse(response: ProducerResponse): Option[ProducerCallbackResponse]
}

1. In Producer, add a new api registerCallBack(callback: ProducerCallback[V]). Change the send api to:
   def send(producerData: ProducerData[K,V]*) : Option[ProducerCallbackResponse]

2. For sync Producer, define the following default callback. Send() will either get an exception or a ProducerCallbackResponse.

class DefaultSyncProducerCallback[V] extends ProducerCallback[V] {
  var response: Option[ProducerCallbackResponse] = none
  def oonSerializationError(event: V, e: Throwable) {
       throw e   
   }
   def onSendResponse(response: ProducerResponse) {
       // instantiate response with a DefaultSyncProducerResponse
       response = Some(new ProducerCallbackResponse(response))
   }

   def getCallbackResponse(): ProducerCallbackResponse = {
      return response
   }
}   

3. For async Producer, define the following default callback that simply ignores the callback.

class DefaultAsyncProducerCallback[V] extends ProducerCallback[V] {
  def onSerializationError(event: V, e: Throwable) {
       // let it go  
   }
   def onSendResponse(response: ProducerResponse) {
       // let it go
   }

   def getCallbackResponse(): ProducerCallbackResponse = {
      return none
   }
}   

4. A user can also define and register it's own customized ProducerCallback.
;;;","18/Sep/12 18:33;jkreps;I would propose we work back from what the user code would look like.

One point I would like to bring up is that the current producer only allows a single request at a time. This is a huge hit on throughput since a single producer can only utilize only one partition on one broker at any given time. For some uses where the # producers is huge compared to the number of brokers this is fine, but this is not universally true. The fix for this is to make the send() call non-blocking and support multiplexing requests over the connection. This is not hard to do--we have the correlation id in the requests so the only change is in the network layer to avoid reordering requests from the same connection. If we made this change that would make the producer request ALWAYS be a sort of future.

The state of futures in java and scala seems to be a little complex. Java has a future but it doesn't allow registering a callback. Fineagle and Akka both have custom versions of Future. There is a proposal to unify all these, though I don't know the status (http://docs.scala-lang.org/sips/pending/futures-promises.html).

For our purpose I recommend we just make our own. It supports two methods
trait Future[T] {
  /* returns true if the result is ready */
  def complete: Boolean
  /* add a function to be called when the result is ready. The function takes the result of the execution--either an exception or a object of type T. Note you can call this more than once to register multiple actions. */
  def onComplete((Either[T, Exception]) => Unit): Future[T]
  /* await completion and return the result or throw the exception */
  def result: T
}

So the function prototype would be
   def send(data: ProduceData*) => Future[ProduceResponse]

In the current code the future would immediately be satisfied for the sync producer. When we have fully implemented the non-blocking client it wouldn't. But this change would be transparent to the user.

I think there are a couple of use cases
1. You don't really care what the result it (basically ""fire and forget""), in which case you use this api as you do today:
     send(...)
2. You want to make sure the send succeeded or do some follow up action but you don't mind blocking the current thread:
    val response = send(...).result
3. You want to do something more complicated. This could be sending out lots of requests without blocking then handling responses or asynchronously handling failures or whatever. In this case you would use
    send(...).onComplete { result: Either[T, Exception] =>
        result match {
          case result: T => .. do something
          case e: Exception => handle exception
     }
      


  ;;;","18/Sep/12 19:13;junrao;Actually, Producer already allows a client to send a list of ProducerData.;;;","04/Dec/12 23:54;junrao;Moving this to 0.8.1.;;;","20/Dec/12 18:22;jkreps;I found this helpful in thinking about futures: http://docs.scala-lang.org/overviews/core/futures.html;;;","05/Mar/13 19:03;matan;I spent some time trying to first-iteration-refactor the producer's sending activity, as a basis for playing around with adding some persistence to it.... I'll probably wait for this item here (KAFKA-496) to be implemented first. 

I'm writing here just to add that it would potentially be nice if the producer's wrappers around sending messages would become simplified in the code through some behavior-maintaining refactoring. The producer internals around managing the queue and around sync/async flows can probably be made much simpler in terms of class and method relationships (or the relationships between producer and producer.async), as part of modifications implied on the previous posts above. I'm writing this as it may seem that KAFKA-496 here may take care of refactoring in this area anyway. It seems this can help future modifications... ;;;","01/Feb/14 19:39;nehanarkhede;Will be fixed as part of the new producer release (0.9);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Handle topic names with ""/"" on Kafka server",KAFKA-495,12605833,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,nehanarkhede,nehanarkhede,01/Sep/12 01:25,01/Oct/12 15:51,14/Jul/23 05:39,12/Sep/12 02:16,0.7,0.8.0,,0.7.2,0.8.0,,,,,,,,,0,bugs,,,"If a producer publishes data to topic ""foo/foo"", the Kafka server ends up creating an invalid directory structure on the server. This corrupts the zookeeper data structure for the topic - /brokers/topics/foo/foo. This leads to rebalancing failures on the consumer as well as errors on the zookeeper based producer. 

We need to harden the invalid topic handling on the Kafka server side to avoid this.",,jjkoshy,jkreps,junrao,nehanarkhede,rmelick,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/12 02:52;swapnilghike;kafka-495-v0.8-v2.patch;https://issues.apache.org/jira/secure/attachment/12544581/kafka-495-v0.8-v2.patch","11/Sep/12 21:53;swapnilghike;kafka-495-v0.8-v3.patch;https://issues.apache.org/jira/secure/attachment/12544721/kafka-495-v0.8-v3.patch","08/Sep/12 01:12;swapnilghike;kafka-495-v0.8.patch;https://issues.apache.org/jira/secure/attachment/12544338/kafka-495-v0.8.patch","04/Sep/12 22:55;swapnilghike;kafka-495-v1.patch;https://issues.apache.org/jira/secure/attachment/12543764/kafka-495-v1.patch","05/Sep/12 23:55;swapnilghike;kafka-495-v2.patch;https://issues.apache.org/jira/secure/attachment/12543939/kafka-495-v2.patch","06/Sep/12 00:35;swapnilghike;kafka-495-v3.patch;https://issues.apache.org/jira/secure/attachment/12543949/kafka-495-v3.patch","06/Sep/12 17:53;swapnilghike;kafka-495-v4.patch;https://issues.apache.org/jira/secure/attachment/12544081/kafka-495-v4.patch","26/Sep/12 00:12;swapnilghike;kafka-v0.8-v4.patch;https://issues.apache.org/jira/secure/attachment/12546619/kafka-v0.8-v4.patch",,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299147,,,Thu Sep 27 01:34:56 UTC 2012,,,,,,,,,,"0|i15zvz:",243114,,,,,,,,,,,,,,,,,,,,"04/Sep/12 22:55;swapnilghike;Created two server config properties - 

1. Black list of characters that are not allowed in the topic names. Currently it excludes alphanumeric characters, _, -, (, ), [, ], { and }. Whitespaces are blacklisted.

2. Maximum length of a topic name.;;;","04/Sep/12 22:57;swapnilghike;Some relevant discussion is at KAFKA-196.;;;","04/Sep/12 22:58;jkreps;It would be good to first work out the regular expression for valid topic names. It might also be nice to have a helper class TopicName that we use when the string being passed is meant as a topic name which applies this regex so we can be sure we are always checking validity.;;;","04/Sep/12 23:14;jkreps;A couple suggestions:
1. This is a fairly conservative character list. Is there a rationale for being more conservative?
2. I don't think this list should be configurable, we should be able to put in our documentation what is legal and have that always be true and not worry about what happens if someone changes the definition of legal characters.
3. One corner case we need to consider is the interaction between regular expressions and topic names.
4. This patch does ~30 passes over the topic name, it would probably be faster to use a pre-compiled regular expression;;;","05/Sep/12 01:16;swapnilghike;So I am planning to insert the following regex check.

Unix constaints - 
1. Ban / and \0 (both not allowed in unix, / is used as a path name component separator and \0 terminates file names in unix.)

Zookeeper constaints:- 
1. The null character (\u0000) cannot be part of a path name. (This causes problems with the C binding.)
2. The following characters can't be used because they don't display well, or render in confusing ways: \u0001 - \u0019 and \u007F - \u009F.
3. The following characters are not allowed: \ud800 -uF8FFF, \uFFF0-uFFFF, \uXFFFE - \uXFFFF (where X is a digit 1 - E), \uF0000 - \uFFFFF.
4. . and .. Are not allowed as filenames. Zookeeper supports only absolute filepaths.
5. The token ""zookeeper"" is reserved.

In addition, I suggest the following two - 
1. Ban leading and trailing whitespaces (because they confuse GUI users) - also takes care forbidding filenames composed of only whitespaces.
2. Ban leading . And .. Because unix hides those files.

There are other cases where unfortunate errors may happen in ways not related to kafka or zookeeper, and programmers would need to take care about them.;;;","05/Sep/12 16:19;jkreps;That sounds reasonable. I would actually argue for allowing anything unix+zk will allow. I agree that leading or trailing whitespace can be confusing and dots are odd, but I think forbidding or allowing that should be done at a higher level than kafka, we should allow anything we can tolerate. LiKafka wrapper can support a more conservative set of restrictions and naming conventions based on what we think names should look like.;;;","05/Sep/12 16:31;junrao;Does it make sense to make the regex configurable that defaults to what Swapnil suggests?;;;","05/Sep/12 16:35;jkreps;I don't think it does. It is like a filesystem--we should be able to document what works and what doesn't. I don't see a lot of value to users to making it configurable, and even if you try to configure '/' to work it won't. I think it would be better to say in our documentation ""the following unicode characters are valid in a topic name"" and just have that be definitive...;;;","05/Sep/12 20:50;jjkoshy;I agree with not having a configurable topic name spec. I think that would be applicable if we decide to have an extremely restrictive spec by default (e.g., only alpha-numeric) in which case many users would want to override that. However the spec that Swapnil has sounds reasonably broad.
;;;","05/Sep/12 23:55;swapnilghike;1. Actually the zookeeper code does not implement all the checks that I mentioned earlier. So I have implemented:
* unix constraints (don't allow / and \0 chars, and don't allow . and .. filenames)
* zookeeper constraints (dont allow chars in range \u0001-\u001F, \u007F-\u009F, \uD800-\uF8FF, \uFFF0-\uFFFF adnd \0)

2. The list of illegal chars is not configurable.

3. Regex used to check for illegal chars, . and .. filenames. It will be good if someone could take a look at the regular expression.

4. Topic name length is configurable.

5. Created a class TopicNameValidator in Utils. ;;;","06/Sep/12 00:17;junrao;Thanks for patch v2. Looks good overall. A couple of minor comments:

20. LogManagerTest: Instead of duplicating code, could we create a list of invalid topics and run through logManager.getOrCreate?

21. TopicNameValidator: Could we name the scala file TopicNameValidator.scala? Also, the message in InvalidTopicException should probably be something like ""topic is not allowed"".;;;","06/Sep/12 00:35;swapnilghike;Thanks, made the changes. ;;;","06/Sep/12 17:53;swapnilghike;Changed the property string in KafkaConfig for maxTopicNameLength.;;;","06/Sep/12 19:56;jkreps;+1;;;","07/Sep/12 03:50;junrao;Thanks for the patch. Committed to trunk. Could you port it to 0.8 too?;;;","08/Sep/12 01:12;swapnilghike;Attached the 0.8 patch. It was taken with git --no-prefix, hope it works.;;;","10/Sep/12 15:29;junrao;Thanks for the patch for 0.8. Overall, it looks good. Some minor comments.

80. LogManager:
80.1. We don't need to verify the topic name in getLog() since logs are not created there. Verifying it in createLog is enough.
80.2. Get rid of unused imports.

81. TopicNameValidator: get rid of ""/**/""
;;;","11/Sep/12 02:52;swapnilghike;- After discussing with Jun, inserted topic validation in createTopic() instead of LogManager in 0.8.

- Created a singleton Topic in Utils/Topic.scala, topicNameValidators in KafkaConfig and CreateTopicCommand default to the maxNameLen in this singleton. The value at both these locations can be overridden.

- The check in LogManager.createLog is not necessary anymore because the topic would be validated by then, removed this check.

- Added a new unit test in test/utils for topic validation.

- Tested this change with bin/kafka-create-topic.sh for fun.

- Opened KAKFA-505 to deal with the TopicMetaDataResponse errorcode.;;;","11/Sep/12 16:31;junrao;Thanks for 0.8 patch v2. Some additional comments:

1. KafkaApis: Instead of adding maxTopicNameLength in the constructor, we can get that value from replicaManager.config.

2. CreateTopicCommand.createTopic(): Could we put topicNameValidator as the last parameter that defaults to an instance of topicNameValidator with the default max length. This way, most existing unit tests don't have to create a separate topicNameValidator.

3. TopicTest: Is there a particular reason that the following loop is not from 1 to 5 instead?
for (i <- 3 to 8);;;","11/Sep/12 21:53;swapnilghike;1. Done.

2. Thanks, should have thought of that. :\

3. Oh, that was because topicName.length == pow(2, i). The default max length is 255, and this loop would have created a name of length 256. Changed it to 1 to 6 since the former does not seem to be obvious in first glance.

4. Slight changes in two unit tests because of ReplicaManager EasyMock.;;;","12/Sep/12 02:16;junrao;Thanks for patch v3 for 0.8. Committed to 0.8.;;;","25/Sep/12 21:53;swapnilghike;Reopening this issue because of additional constraints imposed due to jmx mbean Object naming spec and our own highwatermark handling. 

JMX Mbean naming spec is tedious to implement. The full rules are at http://docs.oracle.com/javase/7/docs/api/javax/management/ObjectName.html. The easy way to make sure that we don't break any rule is to ban the characters colon (:), asterisk (*), question mark (?), comma (,), equals (=), quote (""), backslash (\), newline ('\n'). There does not seem to be any restrictions on single quote ('), period (.) and the rest.

For Highwatermark handling, we need to ban whitespaces [ \t\r\n\f].;;;","25/Sep/12 22:43;swapnilghike;After a bit of discussion, decided to ALLOW only ASCII alphanumeric characters and underscore and dash. ;;;","26/Sep/12 00:12;swapnilghike;Uploading kafka-v0.8-v4.patch. 

This patch allows only ASCII alphanumeric chars, underscore and dash in the topic names. Also since we are arguing that the hard constraints set by us are not going to be challenged in likely usages, I have also hardcoded topic name length to have a max value of 255 since it's very difficult to exceed this length as well. 
;;;","27/Sep/12 01:34;junrao;Thanks for the 0.8 v4 patch. +1 Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaRequestHandler needs to handle exceptions,KAFKA-491,12605322,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,junrao,junrao,29/Aug/12 01:02,20/Nov/12 00:05,14/Jul/23 05:39,02/Nov/12 02:07,,,,0.8.0,,,,,,,core,,,0,bugs,,,"Currently, if apis.handle() throws an exception (e.g., if the broker receives an invalid request), KafkaRequestHandler will die. We need to handle exceptions properly.",,jkreps,junrao,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Oct/12 00:45;yeyangever;kafka-491-v1.diff;https://issues.apache.org/jira/secure/attachment/12550568/kafka-491-v1.diff","01/Nov/12 09:42;yeyangever;kafka-491-v2.diff;https://issues.apache.org/jira/secure/attachment/12551690/kafka-491-v2.diff",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241610,,,Tue Nov 20 00:05:50 UTC 2012,,,,,,,,,,"0|i0299b:",11101,,,,,,,,,,,,,,,,,,,,"24/Oct/12 00:45;yeyangever;
rebase and adding the exception handling code;;;","24/Oct/12 16:35;junrao;Thanks for patch v1. Some comments:

1. KafkaApis:
1.1 For StopReplicaRequest, we should send StopReplicaResponse, not LeaderAndIsrResponse.
1.2 Instead of using ErrorMapping.UnknownCode, we should use ErrorMapping.codeFor().
1.3 The logging of the error can be done in one place, not in each type of requests.
1.4 To be consistent with other requests, we probably should remove the global error code in LeaderAndIsrResponse and StopReplicaResponse. We already have an error code for each partition.

2. KafkaRequestHandler: Indent the code in the try/catch clause.;;;","01/Nov/12 09:42;yeyangever;
Fixed comments 1.1, 1.2, 1.4, 2

For 1.3,
It seems not easy to do that, if we have some common variable outside, then the type of this variable has only to be vague, then when we log it, we cannot get the detailed info inside it. ;;;","02/Nov/12 02:07;junrao;Thanks for patch v2. +1. Committed to 0.8.;;;","20/Nov/12 00:05;jkreps;Jun and Victor--is there really no better way to do this than adding a massive cut-and-paste case statement to handle each request type?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check max message size on server instead of producer,KAFKA-490,12605302,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,swapnilghike,swapnilghike,28/Aug/12 21:44,12/Sep/12 01:31,14/Jul/23 05:39,12/Sep/12 01:31,0.8.0,,,0.8.0,,,,,,,,,,0,bug,,,"Message size is checked currently only in SyncProducer and not at the server. Therefore, non-java clients can push bigger messages to the server. Need a message size check at the server. Can remove the check from producer side since server can send acks in 0.8.",,jjkoshy,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/12 01:06;swapnilghike;kafka-490-v1.patch;https://issues.apache.org/jira/secure/attachment/12543215/kafka-490-v1.patch","06/Sep/12 02:09;swapnilghike;kafka-490-v2.patch;https://issues.apache.org/jira/secure/attachment/12543970/kafka-490-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299146,,,Wed Sep 12 01:31:04 UTC 2012,,,,,,,,,,"0|i15zvr:",243113,,,,,,,,,,,,,,,,,,,,"31/Aug/12 01:06;swapnilghike;1. The changes in Log are mostly similar to those in Kafka-469. 

2. In addition to that, I have removed the message size check in producer. 

3. Now, the broker returns an error code for MessageSizeTooLargeException, this is logged as an error in the broker just like the other exception cases. 

4. The producer on receiving the MessageSizeTooLargeCode logs a warning producerRetries times, and eventually logs a ""failed to send request"" error. 

5. Since there is no point in re-sending a message that was previously declined by the broker due to its size, we could return the errorcode along with every (topic, partition) pair. Any pair with MessageSizeTooLargeCode error could be excluded from further attempts of re-transmission. However, this feels like a hack only for the sake of preventing large messages from being re-transmitted. If we are to do anything along this line, then we are better off preventing them from transmitting in the first place. 

6. A unit test in LogTest to check maxMessageSize in Log.append.

7. A unit test in SyncProducer to check for MessageSizeTooLargeCode in response.;;;","04/Sep/12 15:30;junrao;Thanks for patch v1. Some comments:

1. DefaultEventHandler.send(): We should log a warning if we get any error in the response, not just for MessageSizeTooLarge.

2. KafkaApis: 
2.1 Is the change in the file necessary?
2.2 handleProducerRequest(): If produceToLocalLog() returns a response with an error code on every partition, we can send the response immediately without putting the request into the purgatory. 

3. SyncProducer.send(): We can get rid of the 2 for loops here. 
;;;","06/Sep/12 02:09;swapnilghike;attached the modified patch. I dont know if it will need a rebase, svn is still down.;;;","07/Sep/12 17:02;jjkoshy;+1

Looks good to me. The git mirror should have the latest svn revision, and v2 applies cleanly on that.;;;","12/Sep/12 01:31;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add metrics collection and graphs to the system test framework,KAFKA-489,12605259,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,28/Aug/12 17:32,06/Sep/12 22:28,14/Jul/23 05:39,06/Sep/12 22:28,0.8.0,,,,,,,,,,,,,0,replication-testing,,,"We have a new system test framework that allows defining a test cluster, starting kafka processes in the cluster, running tests and collecting logs. In addition to this, it will be great to have the ability to do the following for each test case run -

1. collect metrics as exposed by mbeans
2. collect various system metrics exposed by sar/vmstat/jvm
3. graph the metrics

The expected output of this work should be the ability to output a link to all the graphs for each test case.",,jfung,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/12 21:29;nehanarkhede;kafka-489-broker-metrics.pdf;https://issues.apache.org/jira/secure/attachment/12542831/kafka-489-broker-metrics.pdf","28/Aug/12 21:29;nehanarkhede;kafka-489-consumer-metrics.pdf;https://issues.apache.org/jira/secure/attachment/12542832/kafka-489-consumer-metrics.pdf","28/Aug/12 21:29;nehanarkhede;kafka-489-producer-metrics.pdf;https://issues.apache.org/jira/secure/attachment/12542833/kafka-489-producer-metrics.pdf","28/Aug/12 19:50;nehanarkhede;kafka-489-v1.patch;https://issues.apache.org/jira/secure/attachment/12542814/kafka-489-v1.patch","29/Aug/12 00:59;nehanarkhede;kafka-489-v2.patch;https://issues.apache.org/jira/secure/attachment/12542874/kafka-489-v2.patch","28/Aug/12 21:29;nehanarkhede;kafka-metrics-home.pdf;https://issues.apache.org/jira/secure/attachment/12542834/kafka-metrics-home.pdf",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241663,,,Wed Aug 29 17:37:39 UTC 2012,,,,,,,,,,"0|i029p3:",11172,,,,,,,,,,,,,,,,,,,,"28/Aug/12 19:49;nehanarkhede;
1. Added a new metrics definition file defined in json format. The purpose of this file is to define the graphs that we want to plot at the end of each test case run. The file is organized as a list of dashboards, one per role (zookeeper/broker/producer/consumer). Each dashboard has all the graphs associated for that role. Each graph is associated with a mbean and has -
1. Graph title
2. Mbean name
3. Attributes and respective y axis labels.
	A separate graph is plotted for each attribute specified for a mbean. 

2. Metrics are collected when an entity is started. The metrics definition json file is read and the JmxTool is used to collect all the attributes for a particular mbean in a separate csv file. So, we have one csv file per mbean definied in metrics.json

3. At the end of each test case run, the metrics scripts go through the metrics.json file, find the csv files for all entities associated with a mbean, and plot one graph per attribute. These graphs are placed under testcase/dashboards/[role]/. 

4. Once the graphs are plotted, another script goes through metrics.json and creates html dashboards arranging the graphs in html files, one per role. So we have 4 dashboards, one for zookeeper, broker, producer and consumer. To view the graphs, open testcase/dashboards/metrics.html

5. Here are the new packages used to build this framework -
5.1. The graphs are plotted using matplotlib. So matplotlib needs to be installed in order to get these graphs
5.2. I included a very lightweight python package “pyh” to create the html pages. This avoids writing boiler-plate code to create simple html pages.

6. Code changes -
6.1. metrics.py includes APIs to collect metrics, plot graphs and create dashboards
6.2. Currently, we use SnapshotStats to collect metrics. The problem is that it only supports collecting metrics at fixed time windows which is not configurable. It was hard coded to be 1 minute, but for tests, this turns out to be too large to get any meaningful metrics.  Another issue is that we use static objects to register mbeans in most places. I couldn't find a better way to pass the monitoring duration config parameter. For now, I've hardcoded that to 1s, I realize that this is not ideal. But since we will be scraping SnapshotStats as part of KAFKA-203, we can punt on this for now. 
6.3. The new codahale metrics for socket server and request purgatory attached the broker id to the mbean name. This is inconvenient for most monitoring systems since now they need to pick up the broker id from the application config in order to create the right mbean name for metrics collection. Also, since each broker starts in a separate JVM, there isn't much value in identifying the mbeans by the broker id. So, I removed the broker id from all mbeans exposed by the codahale metrics package. 
6.4. Right now, there is one csv file output per mbean. The other alternative is one csv file for all possible mbeans exposed by the kafka process. There are 2  concerns with including all metrics in one big csv file -
1. This file might become too large for long running tests. 
2. If some column gets wedged for some mbean, that can affect the graphs for all other mbeans
6.5. Arguably, we don't even need this metrics.json file if we decide to collect all mbeans exposed by a kafka process. The reason I have one is that each graph needs to have a meaningful x/y axes label and a meaningful title. But, if we think the fully qualified mbean name is not too bad for a graph title, and the mbean attribute name is workable as the yaxes label, we could remove it. 
6.6. Removed the single_host_multiple_brokers_test

To see the graphs -

1. Run the test
cd system_test
python -B system_test_runner.py

2. Open the system_test/replication_testsuite/testcase_1/dashboards/metrics.html in a browser;;;","28/Aug/12 21:29;nehanarkhede;Attaching some screenshots of the metrics dashboard for a test case run to give people an approximate idea of how the output looks like. 
;;;","28/Aug/12 23:26;jfung;Hi Neha,

The patch looks good in the system test framework part.

1. However, the following exception is seen when the test is trying to plot graph. Is this something we can ignore for now ?

2012-08-28 16:18:20,470 - ERROR - ERROR while plotting graph /home/jfung/workspace_kafka/kafka_r1378357_489v1_sanity/system_test/replication_testsuite/testcase_1/dashboards/broker/kafka.server:type=ProducerRequestPurgatory,name=SatisfactionRate:MeanRate.svg: [Errno 2] No such file or directory: u'/home/jfung/workspace_kafka/kafka_r1378357_489v1_sanity/system_test/replication_testsuite/testcase_1/dashboards/broker/kafka.server:type=ProducerRequestPurgatory,name=SatisfactionRate:MeanRate.svg' (metrics)
Traceback (most recent call last):
  File ""/home/jfung/workspace_kafka/kafka_r1378357_489v1_sanity/system_test/utils/metrics.py"", line 176, in draw_graph_for_role
    ""time"", labelAndAttribute[0], labelAndAttribute[1], outputGraphFile)
  File ""/home/jfung/workspace_kafka/kafka_r1378357_489v1_sanity/system_test/utils/metrics.py"", line 138, in plot_graphs
    plt.savefig(outputGraphFile)

2. This is the pathname of the file in the exception message:

/home/jfung/workspace_kafka/kafka_r1378357_489v1_sanity/system_test/replication_testsuite/testcase_1/dashboards/broker/kafka.server:type=ProducerRequestPurgatory,name=ExpirationRate:MeanRate.svg;;;","29/Aug/12 00:11;junrao;+1 on the patch. One minor issue: We should remove the info level logging in SyncProducerStats.recordProduceRequest().;;;","29/Aug/12 00:59;nehanarkhede;John, 

That was a bug in the patch that didn't pre-create the required dashboards directory. Hence the graph plotting script failed while writing to the svg file at that location. I've fixed that in patch v2. 

Fixed the info statement in SyncProducerStats.;;;","29/Aug/12 17:37;jfung;+1 on patch v2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Require values in Utils.getTopic* methods to be positive,KAFKA-481,12604688,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,swapnilghike,swapnilghike,23/Aug/12 21:57,25/Aug/12 05:30,14/Jul/23 05:39,25/Aug/12 05:30,0.7.1,,,0.7.2,0.8.0,,,,,,,,,0,bugs,,,"KafkaConfig can currently accept negative values for topic specific properties, need to prevent this. ",,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/12 01:16;swapnilghike;kafka-481-0.8.patch;https://issues.apache.org/jira/secure/attachment/12542385/kafka-481-0.8.patch","24/Aug/12 21:46;swapnilghike;kafka-481.patch;https://issues.apache.org/jira/secure/attachment/12542346/kafka-481.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,292178,,,Sat Aug 25 05:30:04 UTC 2012,,,,,,,,,,"0|i0rs9z:",160225,,,,,,,,,,,,,,,,,,,,"24/Aug/12 21:46;swapnilghike;Inserted require() checks.;;;","25/Aug/12 01:16;swapnilghike;Attached the patch for 0.8.;;;","25/Aug/12 05:30;junrao;Thanks for the patch. Committed to both trunk and 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
change Pool to use Option ,KAFKA-476,12604252,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,junrao,junrao,21/Aug/12 17:20,07/Feb/15 23:38,14/Jul/23 05:39,07/Feb/15 23:38,0.8.0,,,,,,,,,,,,,0,optimization,,,It would be good if we change the get API in Pool to use Option.,,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241627,,,2012-08-21 17:20:51.0,,,,,,,,,,"0|i029dz:",11122,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
support changing host/port of a broker,KAFKA-474,12604078,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,mumrah,junrao,junrao,20/Aug/12 16:14,09/Oct/12 17:09,14/Jul/23 05:39,09/Oct/12 17:09,,,,0.8.0,,,,,,,core,,,0,bugs,newbie,,"Currently, the consumer client caches the host/port of a broker and never refreshes the cache. This means that if a broker changes to a different host, the consumer client won't be able to connect to the new host without a restart. One possibility is to change AbstractFetcherManager to maintain a map of <Broker, fetcher>, instead of <broker id, fetcher>.",,junrao,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/12 13:37;mumrah;KAFKA-474.patch;https://issues.apache.org/jira/secure/attachment/12547975/KAFKA-474.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,239988,,,Tue Oct 09 17:09:34 UTC 2012,,,,,,,,,,"0|i00v9z:",2999,,,,,,,,,,,,,,,,,,,,"04/Oct/12 12:41;mumrah;I took a look at AbstractFetcherManager and the broker -> thread map. I don't see where/how this map is used to actually get the fetcher thread for a particular broker. I see the fetcherSourceBroker method, but cannot find anywhere where it is invoked.

In fact, the only usage of this map I can see is when fetcher threads are created and removed. 

Am I overlooking something?
;;;","04/Oct/12 17:18;junrao;Yes, fetcherSourceBroker is actually no longer needed. It was used before when a follower switches leaders. The follower uses that method to see if it's already fetching from the new leader. We removed this when we cache the leader epoch in Partition, which avoids executing duplicated leader change requests.

The real problem here is that in addFetcher(), we use the broker id as the key. Of course, the host/port info could have changed. I think a simple solution may be just key fetcherThreadMap on Broker, instead of broker id.;;;","05/Oct/12 13:37;mumrah;Something as simple as this?;;;","09/Oct/12 17:09;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use getMetadata Api in ZookeeperConsumerConnector,KAFKA-473,12604077,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,yeyangever,junrao,junrao,20/Aug/12 16:05,27/Sep/12 01:20,14/Jul/23 05:39,27/Sep/12 01:20,,,,0.8.0,,,,,,,core,,,0,newbie,optimization,,"Currently, ZookeeperConsumerConnector gets topic metadata from ZK directly. It's better to use the getMetadata Api since it's batched. This is especially helpful if the consumer client is in a different data center.",,junrao,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/12 01:07;yeyangever;kafka_473_v1.diff;https://issues.apache.org/jira/secure/attachment/12546143/kafka_473_v1.diff","25/Sep/12 01:01;yeyangever;kafka_473_v2.diff;https://issues.apache.org/jira/secure/attachment/12546420/kafka_473_v2.diff","26/Sep/12 00:01;yeyangever;kafka_473_v3.diff;https://issues.apache.org/jira/secure/attachment/12546617/kafka_473_v3.diff","26/Sep/12 23:44;yeyangever;kafka_473_v4.diff;https://issues.apache.org/jira/secure/attachment/12546779/kafka_473_v4.diff",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,291940,,,Thu Sep 27 01:20:30 UTC 2012,,,,,,,,,,"0|i0rhqf:",158517,,,,,,,,,,,,,,,,,,,,"22/Sep/12 01:07;yeyangever;
1. create a utility function ""getTopicMetadata"" in Utils.scala to get topic metadata, and let the BrokerPartitionInfo and leaderFinderThread use it

2. a function getConsumerTopicMaps() in zkUtils() is never used, remove it.

3. potentially still use the function in consumer rebalancing;;;","24/Sep/12 16:29;junrao;Thanks for patch v1. Looks good overall. Some comments:

1. ProducerPool.createSyncProducer(): It's probably better to use the input parameter config an Option than allowing it to be null.

2. Utils.getBrokerPartitionInfo(): We should probably rename this to something like getTopicMetadata() and change log messages with ""broker partition metadata"" to ""topic metadata"".

3. It's probably worthwhile to use the getTopicMetadata utility to replace getPartitionsForTopics in ZookeeperConsumerConnector.rebalance(). This saves the reading of all partitions. We still need to do 1 delete and 1 write to ZK for each partition that a consumer owns. However, that's only a subset of the partitions.;;;","25/Sep/12 01:01;yeyangever;
1. Use the api call at producer side 
2. use ""Option[ProducerConfig]"" as parameter to createSyncProducer method, handle case where no producerConfig or target broker id is provided;;;","25/Sep/12 17:14;junrao;Thanks for patch v2. A couple of more comments:

20. What does the following line in ZookeeperConsumerConnector.rebalance() do?
        (topic, partitions)

21. ProducerPool.createSyncProducer(configOpt: Option[ProducerConfig]): This one always expects a ProducerConfig. So, we shouldn't use Option. Also, this method seems not to be used anywhere. We can just remove it.;;;","26/Sep/12 00:01;yeyangever;
Thank you for pointing out the issues. This patch should look good;;;","26/Sep/12 23:44;yeyangever;
v3 plus replacing leader id look up from zookeeper with reading from the cache;;;","27/Sep/12 01:20;junrao;+1 on patch v4. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update metadata in batches in Producer,KAFKA-472,12604068,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,junrao,junrao,20/Aug/12 15:40,03/Apr/14 14:26,14/Jul/23 05:39,03/Apr/14 14:26,,,,0.8.2.0,,,,,,,core,,,0,optimization,,,"Currently, the producer obtains the metadata of topics being produced one at a time. This means that tools like mirror maker will make many getMetadata requests to the broker. Ideally, we should make BrokerPartition.getBrokerPartitionInfo() a batch api that takes a list of topics.",,guozhang,junrao,nehanarkhede,pkwarren,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241622,,,Thu Apr 03 14:26:38 UTC 2014,,,,,,,,,,"0|i029c7:",11114,,,,,,,,,,,,,,,,,,,,"02/Apr/14 18:53;guozhang;Is this still an issue with the new producer?;;;","03/Apr/14 01:46;nehanarkhede;Seems like this is a non-issue with the new producer. I think we can close this.;;;","03/Apr/14 14:26;junrao;The new producer already does batching. So, this can be closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transient failure in ProducerTest,KAFKA-471,12603970,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,nehanarkhede,nehanarkhede,19/Aug/12 04:07,07/Sep/12 03:59,14/Jul/23 05:39,07/Sep/12 03:59,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,,,"[0m[[31merror[0m] [0mTest Failed: testSendToNewTopic(kafka.producer.ProducerTest)[0m
java.lang.AssertionError: Message set should not have any more messages
	at org.junit.Assert.fail(Assert.java:69)
	at org.junit.Assert.assertTrue(Assert.java:32)
	at org.junit.Assert.assertFalse(Assert.java:51)
	at kafka.producer.ProducerTest.testSendToNewTopic(ProducerTest.scala:183)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:228)
	at junit.framework.TestSuite.run(TestSuite.java:223)
	at junit.framework.TestSuite.runTest(TestSuite.java:228)
	at junit.framework.TestSuite.run(TestSuite.java:223)
	at org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)
	at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
	at sbt.TestRunner.run(TestFramework.scala:53)
	at sbt.TestRunner.runTest$1(TestFramework.scala:67)
	at sbt.TestRunner.run(TestFramework.scala:76)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.NamedTestTask.run(TestFramework.scala:92)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
	at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
	at sbt.impl.RunTask.runTask(RunTask.scala:85)
	at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Control$.trapUnit(Control.scala:19)
	at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/12 16:58;junrao;kafka-471_v1.patch;https://issues.apache.org/jira/secure/attachment/12542624/kafka-471_v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299140,,,Fri Sep 07 03:59:08 UTC 2012,,,,,,,,,,"0|i15zuf:",243107,,,,,,,,,,,,,,,,,,,,"27/Aug/12 16:58;junrao;Not sure how this can happen and can't reproduce it either. Changed the test a bit to produce a more meaningful error message should the test fails again.;;;","06/Sep/12 22:27;nehanarkhede;+1;;;","07/Sep/12 03:59;junrao;Thanks for the review. Committed to 0.8. Closing the jira for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
transient unit test failure in RequestPurgatoryTest,KAFKA-470,12603944,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,18/Aug/12 05:11,07/Sep/12 03:55,14/Jul/23 05:39,07/Sep/12 03:54,,,,0.8.0,,,,,,,core,,,0,bugs,,,"[error] Test Failed: testRequestExpirey(kafka.server.RequestPurgatoryTest)
junit.framework.AssertionFailedError: Time for expiration was about 20ms
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.assertTrue(Assert.java:20)
	at kafka.server.RequestPurgatoryTest.testRequestExpirey(RequestPurgatoryTest.scala:75)
",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/12 16:19;junrao;kafka-470_v1.patch;https://issues.apache.org/jira/secure/attachment/12542620/kafka-470_v1.patch","27/Aug/12 16:38;junrao;kafka-470_v2.patch;https://issues.apache.org/jira/secure/attachment/12542621/kafka-470_v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299139,,,Fri Sep 07 03:54:55 UTC 2012,,,,,,,,,,"0|i15zu7:",243106,,,,,,,,,,,,,,,,,,,,"27/Aug/12 16:19;junrao;Attach patch v1. ;;;","27/Aug/12 16:38;junrao;Attach patch v2 to fix a busy wait issue in TestUtils.;;;","06/Sep/12 22:28;nehanarkhede;+1;;;","07/Sep/12 03:54;junrao;Thanks for the review. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Message size is not checked at the server,KAFKA-469,12603932,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,swapnilghike,swapnilghike,18/Aug/12 00:45,29/Aug/12 14:54,14/Jul/23 05:39,24/Aug/12 14:33,0.7.1,,,0.7.2,,,,,,,,,,0,bug,,,"Message size is checked currently only in SyncProducer and not at the server. Therefore, non-java clients can push bigger messages to the server. Need a message size check at the server as well.",,junrao,nehanarkhede,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/12 00:56;swapnilghike;kafka-469-v1.patch;https://issues.apache.org/jira/secure/attachment/12542067/kafka-469-v1.patch","23/Aug/12 23:36;swapnilghike;kafka-469-v2.patch;https://issues.apache.org/jira/secure/attachment/12542207/kafka-469-v2.patch","29/Aug/12 02:24;swapnilghike;kafka-469-v3.patch;https://issues.apache.org/jira/secure/attachment/12542885/kafka-469-v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,258977,,,Wed Aug 29 14:54:53 UTC 2012,,,,,,,,,,"0|i0l6nb:",121727,,,,,,,,,,,,,,,,,,,,"23/Aug/12 00:56;swapnilghike;Implemented message size check in Log.append(). The default maxMessageSize in server config is set to the same value as maxMessageSize in producer config.

Will need to rebase after kafka-475 patch is checked in. ;;;","23/Aug/12 15:02;junrao;Thanks for the patch. Looks good overall. One minor comment:

Instead of passing in maxMessageSize in each log.append, is it better to pass it in the constructor of Log?;;;","23/Aug/12 23:36;swapnilghike;Yes. Attached the change.;;;","24/Aug/12 01:27;nehanarkhede;+1 on v2. Thanks for the patch !;;;","24/Aug/12 14:33;junrao;Thanks for patch v2. Committed to trunk.;;;","29/Aug/12 02:24;swapnilghike;Reopening this issue to remove the stack trace logged in case of receiving a message larger than the permissible limit set by the server config.;;;","29/Aug/12 14:54;junrao;Thanks for patch v3. We need to log the topic/partition in handleProducerRequest() when we hit a MessageSizeTooLargeException. Committed to trunk with the fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
String#getBytes is platform dependent,KAFKA-468,12603911,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,,mumrah,mumrah,17/Aug/12 20:42,22/Nov/12 22:04,14/Jul/23 05:39,22/Nov/12 22:04,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"Just noticed while looking at the source that some calls to java.lang.String#getBytes do not include the encoding. They should probably specify ""UTF-8"" for platform-independence.
",,initialcontext,jkreps,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-367,,,,,,,,,,,,,,,,,,,"17/Aug/12 23:39;mumrah;KAFKA-468.diff;https://issues.apache.org/jira/secure/attachment/12541434/KAFKA-468.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,248160,,,Thu Nov 22 22:04:14 UTC 2012,,,,,,,,,,"0|i09lwf:",53981,,,,,,,,,,,,,,,,,,,,"17/Aug/12 23:38;mumrah;Specifying ""UTF-8"" in StringEncoder/Decoder;;;","24/Aug/12 12:33;mumrah;Didn't see that one when I create this ticket.;;;","30/Aug/12 00:47;initialcontext;Hey David, if you're happy with your fix let me know and I can resolve mine (I was working on KAFKA-367.) The other end of mine is fact that in KAFKA-367 they want the UTF-8 to be configurable rather than set in stone, but I agree having some agreed-upon default is better than not setting one at all so your patch works for me if it works for you.

I am having trouble viewing your diff file, it comes up on Safari as a blank page URL -- could you check it?

Anyway let me know, and I can cancel KAFKA-367...or you're welcome to help complete it I think I was basically done and am just having a small problem with StringEncoder subclass. Will try to fix it tonight or tomorrow AM but not all that sure I will succeed as I am new to Scala.

Thanks again!
;;;","30/Aug/12 15:43;jkreps;I think it does make sense to make the encoding configurable.;;;","22/Nov/12 22:04;jkreps;I integrated the changes in KAFKA-367 into the patch for KAFKA-544, which has been committed on the 0.8 branch. I wonder if anyone would be willing to take a look at the change and validate I didn't muck anything up?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller based leader election failed ERROR messages in LazyInitProducerTest,KAFKA-467,12603879,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yeyangever,nehanarkhede,nehanarkhede,17/Aug/12 15:49,18/Aug/12 05:30,14/Jul/23 05:39,18/Aug/12 05:30,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,replication-testing,,"[info] Test Starting: testMultiProduce(kafka.integration.LazyInitProducerTest)
[2012-08-17 08:46:45,165] ERROR Timing out after 500 ms since leader is not elected for topic test1 partition 0 (kafka.utils.TestUtils$:93)
[2012-08-17 08:46:45,668] ERROR Timing out after 500 ms since leader is not elected for topic test2 partition 0 (kafka.utils.TestUtils$:93)
[2012-08-17 08:46:46,171] ERROR Timing out after 500 ms since leader is not elected for topic test3 partition 0 (kafka.utils.TestUtils$:93)
[info] Test Passed: testMultiProduce(kafka.integration.LazyInitProducerTest)
[info] Test Starting: testMultiProduceResend(kafka.integration.LazyInitProducerTest)
[2012-08-17 08:46:49,028] ERROR Timing out after 1500 ms since leader is not elected for topic test1 partition 0 (kafka.utils.TestUtils$:93)
[2012-08-17 08:46:50,531] ERROR Timing out after 1500 ms since leader is not elected for topic test2 partition 0 (kafka.utils.TestUtils$:93)
[2012-08-17 08:46:52,034] ERROR Timing out after 1500 ms since leader is not elected for topic test3 partition 0 (kafka.utils.TestUtils$:93)
[info] Test Passed: testMultiProduceResend(kafka.integration.LazyInitProducerTest)

Leader election should not time out",,junrao,nehanarkhede,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/12 20:41;yeyangever;kafka_467.diff;https://issues.apache.org/jira/secure/attachment/12541421/kafka_467.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299138,,,Sat Aug 18 05:30:01 UTC 2012,,,,,,,,,,"0|i15ztz:",243105,,,,,,,,,,,,,,,,,,,,"17/Aug/12 20:41;yeyangever;
Neha,

This exception:

[info] Test Starting: testMultiProduce(kafka.integration.LazyInitProducerTest)
[2012-08-17 08:46:45,165] ERROR Timing out after 500 ms since leader is not elected for topic test1 partition 0 (kafka.utils.TestUtils$:93)


is due to the line of code TestUtils.waitUntilLeaderIsElectedOrChanged()....

But this line of code is not needed here, because the topics are not created initially, they're created on the fly when the server receives produce requests to these topics. So they will definitely hit the timeout exception.

I've checked out svn version 1367821, which is before the controller code is checked in, the same exceptions are there. 

The way to fix it is just to remove the waitUntil() lines. 

;;;","18/Aug/12 05:30;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance test scripts - refactoring leftovers from tools to perf package,KAFKA-465,12603614,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,queinnec,queinnec,15/Aug/12 17:02,11/Jul/13 22:22,14/Jul/23 05:39,11/Jul/13 22:22,0.8.0,,,,,,,,,,,,,0,patch,,,"The performance test shell scripts seem like they weren't updated to the new package hierarchy, and still reference kafka.tools.ProducerPerformance for example. Patch attached.",,akitada,jkreps,queinnec,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/12 17:03;queinnec;KAFKA-465.diff;https://issues.apache.org/jira/secure/attachment/12541088/KAFKA-465.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241644,,,Thu Jul 11 22:22:56 UTC 2013,,,,,,,,,,"0|i029in:",11143,,,,,,,,,,,,,,,,,,,,"16/Aug/12 12:27;akitada;I think this is a duplicate of KAFKA-425;;;","11/Jul/13 22:22;jkreps;As mentioned I think this was fixed in KAFKA-425.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaController NPE in SessionExpireListener,KAFKA-464,12603613,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yeyangever,junrao,junrao,15/Aug/12 16:53,17/Aug/12 23:10,14/Jul/23 05:39,17/Aug/12 23:10,,,,0.8.0,,,,,,,core,,,0,bugs,,,"Sometime see the following in LogRecoverTest.

[2012-08-15 09:06:01,845] ERROR Error handling event ZkEvent[New session event sent to kafka.server.KafkaController$SessionExpireListener@e8ae59a] (org.I0Itec.zkclient.ZkEventThread)
java.lang.NullPointerException
	at kafka.server.KafkaController$SessionExpireListener.handleNewSession(KafkaController.scala:284)
	at org.I0Itec.zkclient.ZkClient$4.run(ZkClient.java:472)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
",,junrao,nehanarkhede,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/12 23:28;yeyangever;kafka_464.diff;https://issues.apache.org/jira/secure/attachment/12541152/kafka_464.diff","17/Aug/12 01:14;yeyangever;kafka_464_v2.diff;https://issues.apache.org/jira/secure/attachment/12541315/kafka_464_v2.diff","17/Aug/12 05:22;yeyangever;kafka_464_v3.diff;https://issues.apache.org/jira/secure/attachment/12541328/kafka_464_v3.diff","17/Aug/12 21:36;yeyangever;kafka_464_v4.diff;https://issues.apache.org/jira/secure/attachment/12541424/kafka_464_v4.diff","17/Aug/12 22:00;yeyangever;kafka_464_v5.diff;https://issues.apache.org/jira/secure/attachment/12541425/kafka_464_v5.diff",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299136,,,Fri Aug 17 23:10:09 UTC 2012,,,,,,,,,,"0|i15ztj:",243103,,,,,,,,,,,,,,,,,,,,"15/Aug/12 23:28;yeyangever;Fixed it;;;","16/Aug/12 16:57;junrao;Thanks for the patch. 

I think you always need to call controllerRegisterOrFailover when handling a new session, whether controllerChannelManager is null or not.;;;","16/Aug/12 16:59;junrao;Also, instead of testing controllerChannelManager is null, can we use isActive()?;;;","17/Aug/12 01:14;yeyangever;
Added a new flag of isRunning in controller and always call controllerRegisterOrFailOver() on session expiration;;;","17/Aug/12 04:20;junrao;Well, unfortunately, v2 doesn't address the original problem that controllerChannelManager can be null when calling controllerChannelManager.shutdown in handleNewSession().;;;","17/Aug/12 05:22;yeyangever;

Sorry for missing it...;;;","17/Aug/12 15:28;junrao;A couple of more comments:

1. The following check in handleNewSession() should probably be done inside controllerRegisterOrFailover() since other callers of controllerRegisterOrFailover() should check isRunning too.
        if(isRunning)
          controllerRegisterOrFailover()

2. This is not really related to this jira, but it's in KafkaController. So, maybe we can fix it in this jira too. It seems that we can simplify the following code in controllerRegisterOrFailover() using filter() on a map.

      // If there are some partition with leader not initialized, init the leader for them
      val partitionReplicaAssignment = allPartitionReplicaAssignment.clone()
      for((topicPartition, replicas) <- partitionReplicaAssignment){
        if (allLeaders.contains(topicPartition)){
          partitionReplicaAssignment.remove(topicPartition)
        }
      }
;;;","17/Aug/12 21:36;yeyangever;

V4 fixes as suggested;;;","17/Aug/12 21:40;nehanarkhede;Minor comments -

1.1 Rename shutDown to shutdown()
1.2 Rename isLeaderController to isController or isCurrentController
;;;","17/Aug/12 22:00;yeyangever;
So I find a few other places in the code using ""startUp"" or ""shutDown"", let me fix them all together.;;;","17/Aug/12 23:10;junrao;Thanks for patch v5. Committed to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
log.truncateTo needs to handle targetOffset smaller than the lowest offset in the log,KAFKA-463,12603598,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,swapnilghike,junrao,junrao,15/Aug/12 15:15,17/Sep/12 14:55,14/Jul/23 05:39,17/Sep/12 14:55,,,,0.8.0,,,,,,,core,,,0,bugs,,,"When this happens, we should truncate all existing log segments.",,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/12 21:34;swapnilghike;kafka-463-v1.patch;https://issues.apache.org/jira/secure/attachment/12544874/kafka-463-v1.patch","14/Sep/12 06:17;swapnilghike;kafka-463-v2.patch;https://issues.apache.org/jira/secure/attachment/12545103/kafka-463-v2.patch","14/Sep/12 21:20;swapnilghike;kafka-463-v3.patch;https://issues.apache.org/jira/secure/attachment/12545219/kafka-463-v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299135,,,Mon Sep 17 14:55:16 UTC 2012,,,,,,,,,,"0|i15ztb:",243102,,,,,,,,,,,,,,,,,,,,"12/Sep/12 21:34;swapnilghike;Truncates all existing log segments if targetOffset is smaller than the lowest offset in the log, creates and appends a new log segment at targetOffset.;;;","13/Sep/12 15:20;junrao;Thanks for the patch. A few comments:

1. Log.truncateTo(): 
1.1 Should synchronize on the lock since we are touching the segment files.
1.2 It doesn't seem that we need to call segments.truncLast(truncatedSegmentIndex) since we are deleting those segments whose start is larger than targetOffset later. If so, we can get rid of of SegmentList.truncLast and the associated unit test. Also, it seems that the current logic will fail to delete some segment files (only the in memory segment list is updated) if there the startOffset is in the middle of a list of segments. 
1.3 The following piece of code is duplicated in 3 places: creating the first segment with offset 0, maybe roll and runcateTo(). Can we create a separate function (something like rollSegement(startOffset)) to share the code?
      val newFile = new File(dir, nameFromOffset(targetOffset))
      if (newFile.exists) {
        warn(""newly rolled logsegment "" + newFile.getName + "" already exists; deleting it first"")
        newFile.delete()
      }
      debug(""Rolling log '"" + name + ""' to "" + newFile.getName())
      segments.append(new LogSegment(newFile, new FileMessageSet(newFile, true), targetOffset, time))
1.4 Can we add a unit test to verify that if targetOffset is smaller that the smallest offset in the log, after tuncateTo is called, log size is 0 and logEndOffset is targetOffset?
1.5 The following message should say this is failure during truncateTo.
      error(""Failed to delete some segments during log recovery"")

2. FileMessageSet.truncateTo() : Condition     if(targetSize >= sizeInBytes())  should be changed to     if(targetSize > sizeInBytes())
;;;","14/Sep/12 06:17;swapnilghike;1. Made the changes 1.1, 1.3, 1.5, 2 as recommended. 

2. Made an appropriate change for 1.2. If all segments are deleted then truncate entire segments.view and roll a new segment from that targetOffset, otherwise check for appropriate segment to truncate and change the last segment of segments.view or throw error if targetOffset is out of range. 

3. A unit test that exercises all cases for truncateTo.

Perhaps not super-crucial, but here it goes anyways:

i. Added a check for contents.get().size > 0 in while loop in  trunc and truncLast, otherwise it can throw illegalArgumentException in Array.copy for certain cases even when the passed argument is ok. Eg in trunc when newStart == 0 and curr.length == 0. There is most probably no danger of changing the value of contents between the while condition check and curr = contents.get(), because any calls to trunc and truncLast are synchronized with the same lock, thus there should be no other thread executing between while() and curr = contents.get().

ii. truncLast does not throw exception for argument==-1, but throws exception for anything less. Probably it should behave uniformly for all negative integers. Made a change so that truncLast(-ve no) will wipe out the entire segments.view. This is also similar to what trunc does when the argument is greater than (segments.view.length-1).

iii. Changed the condition for throwing exception in truncLast to > because an index which is equal to (segments.view.length-1) should not throw exception.

iv. Added max/min condition to avoid IllegalArgumentException in truncLast.

v. The unit test for truncLast did not check for all cases properly, fixed it.

vi. Combined the two different test cases for trunc by imitating the structure of unit test for truncLast.

A question:

Since all calls to SegmentList.trunc, truncLast and append (which are the only ways to modify contents) seem to be locked, so only one thread will be able to modify SegmentList.contents at any time. Do we really need a while(compareAndSet) for contents? I could be wrong, pls correct me whenever you have free time.;;;","14/Sep/12 17:02;junrao;Thanks for patch v2. Some more comments:

20. Log:
20.1 rollSegment can be used for rolling the very first segment with offset 0 in the constructor too. Also, add a new line after rollSegment()
20.2 The following log message is bit confusing since Log shouldn't understand hw. Let's changed it to something more generic.
              error(""Last checkpointed hw %d cannot be greater than the latest message offset %d in the log %s"".
                format(targetOffset, segments.view.last.absoluteEndOffset, segments.view.last.file.getAbsolutePath))

21. SegmentList: 
21.1 The comment of truncLast is wrong. It's truncating everything from newOffset+1 to the end.
21.2 truncLast(): It's better to make sure that newOffset is >=0. Then we don't need the code max(newEnd + 1, 0).
21.3 It is true that callers of trunc and truncLast are already sync-ed on log.lock. All we need is to create a memory boundary so that the updated reference of  contents are exposed to other threads. AtomicReference guarantees this. However, we could just use a volatile var. We can probably leave AtomicReference for now since the overhead shouldn't be too much. However, we probably don't need to use compareAndSet in trunc() and truncLast(). We can just use set and get rid of the loop.


 ;;;","14/Sep/12 21:20;swapnilghike;20.1. Letting the constructor be as it is as discussed. 
20.2. Made the changes.

21.1 Oops, had forgotten to update it, thanks.
21.2 Changed.
21.3 Removed compareAndSet loops. AtomicReference is unchanged.;;;","17/Sep/12 14:55;junrao;Thanks for patch v3. Committed to 0.8 with a minor fix: use contents.get().length instead of contents.get().size in SegmetList.truncLast to be consistent with the rest of the code.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
remove support for format for magic byte 0 in 0.8,KAFKA-461,12603484,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,swapnilghike,junrao,junrao,14/Aug/12 17:18,21/Aug/12 04:56,14/Jul/23 05:39,21/Aug/12 04:56,0.8.0,,,0.8.0,,,,,,,core,,,0,backward-incompatible,bugs,,"Since 0.8 is a non-backward compatible release, should we remove the support for magic byte 0 in Message and support only magic byte 1?",,jkreps,junrao,swapnilghike,,,,,,,,,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/12 18:26;swapnilghike;kafka-461-v2.patch;https://issues.apache.org/jira/secure/attachment/12541622/kafka-461-v2.patch","17/Aug/12 18:21;swapnilghike;kafka-461.patch;https://issues.apache.org/jira/secure/attachment/12541401/kafka-461.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299133,,,Tue Aug 21 04:56:23 UTC 2012,,,,,,,,,,"0|i15zsv:",243100,,,,,,,,,,,,,,,,,,,,"14/Aug/12 22:33;jkreps;I think the message format hasn't changed. Technically what we should do is change the request ids, but I think we decided not just to kind of ""start fresh"". Basically I don't think this would help since an old client can't make requests it won't be able to get any messages anyway.;;;","15/Aug/12 14:25;junrao;No, the message format hasn't changed. It's just that if we drop the support for magic byte 0, the code in 0.8 will be simpler since we can get rid of all the switch statements.;;;","15/Aug/12 15:55;jkreps;OIC, those were already there. Yes, that makes sense.;;;","17/Aug/12 17:06;swapnilghike;Can test the change once the backward compatibility tests are removed from 0.8.;;;","17/Aug/12 17:16;junrao;You can remove the backward compatibility tests as part of the patch.;;;","17/Aug/12 18:21;swapnilghike;Done.;;;","18/Aug/12 05:12;junrao;Thanks for the patch. A couple of comments:

1. Message: Let's remove the following comment:

 * If magic byte is 1

2. We need to remove the test resource core/src/test/resources/test-kafka-logs;;;","20/Aug/12 18:26;swapnilghike;Made the above changes and rebased.;;;","21/Aug/12 04:56;junrao;Thanks for patch v2. Committed to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ControllerChannelManager needs synchronization btw shutdown and add/remove broker,KAFKA-460,12603483,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yeyangever,junrao,junrao,14/Aug/12 17:11,15/Aug/12 15:09,14/Jul/23 05:39,15/Aug/12 15:09,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,,,,,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299132,,,Wed Aug 15 15:09:13 UTC 2012,,,,,,,,,,"0|i15zsn:",243099,,,,,,,,,,,,,,,,,,,,"15/Aug/12 15:09;junrao;Fixed as part of kafka-459.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaController.RequestSendThread can throw exception on broker socket,KAFKA-459,12603479,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yeyangever,junrao,junrao,14/Aug/12 16:51,15/Aug/12 15:08,14/Jul/23 05:39,15/Aug/12 15:08,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,,,It can hit NullPointerException at line 74 in KafkaController,,junrao,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/12 02:06;yeyangever;kafka_459.diff;https://issues.apache.org/jira/secure/attachment/12540996/kafka_459.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299131,,,Wed Aug 15 15:08:40 UTC 2012,,,,,,,,,,"0|i15zsf:",243098,,,,,,,,,,,,,,,,,,,,"15/Aug/12 02:06;yeyangever;
1. Fix this problem 

2. Fix the unnecessary error level logging of ""Log.scala"" in truncateTo() function

3. Fix Jira Kafka-460 too

4. Jira 415, 416 no longer happens after over 10 times of testing, maybe they can also be closed;;;","15/Aug/12 15:08;junrao;Thanks for the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leader Re-election is broken in rev. 1368092,KAFKA-457,12603286,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,yeyangever,jfung,jfung,13/Aug/12 17:26,15/Aug/12 23:26,14/Jul/23 05:39,15/Aug/12 23:26,,,,,,,,,,,,,,0,,,,"Steps to reproduce:

1. Check out rev. 1368092 and execute the command: $ ./sbt update package

2. Replace <kafka_home>/system_test/single_host_multi_brokers/bin/run-test.sh with the attached script because the logging message related to ""shut down completed"" and ""leader state transition"" has been modified in this revision.

3. Execute the command bin/run-test.sh

4. The test seems to be hung.

5. The reason is that there is no leader re-election happening after the first leader is terminated. By checking the logs, there may be only 1 ""completed the leader state transition"" log message found in the server logs:

$ grep ""completed the leader state transition"" *
kafka_server_3.log:[2012-08-13 10:07:58,085] INFO Replica Manager on Broker 3, completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager)

6. The correct behavior as in rev 1367821, re-election log messages are found after leader termination as follows:
$ grep ""completed the leader state transition"" *
kafka_server_1.log:[2012-08-13 09:40:23,542] INFO Broker 1 completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager)
kafka_server_1.log:[2012-08-13 09:42:17,881] INFO Broker 1 completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager)
kafka_server_2.log:[2012-08-13 09:40:29,082] INFO Broker 2 completed the leader state transition for topic mytest partition 0 (kafka.server.ReplicaManager)
kafka_server_3.log:[2012-08-13 09:44:06,695] INFO Broker 3 completed the leader state transition for topic mytest part

",,jfung,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/12 17:39;jfung;run-test.sh;https://issues.apache.org/jira/secure/attachment/12540710/run-test.sh",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299129,,,Wed Aug 15 23:26:22 UTC 2012,,,,,,,,,,"0|i15zrz:",243096,,,,,,,,,,,,,,,,,,,,"13/Aug/12 17:39;jfung;This script ""run-test.sh"" contains the following changes
1. KAFKA-380-v5.patch
2. Updated log message patterns for looking up leader election in revision 1368092;;;","15/Aug/12 01:02;yeyangever;The behaviour of the code is changed after kafka-369 is checked in, and the system tests all work well. So this jira should be marked as resolved.;;;","15/Aug/12 23:26;jfung;Hi Victor,

Thank you for your fix. I have tested rev. 1373633 for system_test/single_host_multi_brokers/run-test.sh and it is working correctly.

Regards,
John;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProducerSendThread calls ListBuffer.size a whole bunch. That is a O(n) operation,KAFKA-456,12602686,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,mumrah,rathboma,rathboma,09/Aug/12 22:42,19/Jun/14 05:11,14/Jul/23 05:39,04/Oct/12 17:32,0.8.0,,,0.8.0,,,,,,,core,,,0,newbie,,,"Hi all,

So there are various statements throughout the async code that call 'events.size', mostly for debugging purposes.
Problem is that this call is O(n), so it could add up if the batch size is high. (it's a ListBuffer)

I see this in at least ProducerSendThread (x4), likely more. Will factor this out myself soon when I start hacking on the project, just wanted to put this somewhere.
",NA,jkreps,junrao,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/12 12:59;mumrah;KAFKA-456.patch;https://issues.apache.org/jira/secure/attachment/12547723/KAFKA-456.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,239984,,,Thu Oct 04 18:02:32 UTC 2012,,,,,,,,,,"0|i00v93:",2995,,,,,,,,,,,,,,,,,,,,"04/Oct/12 12:59;mumrah;I looked around in other parts of the code for similar situations (lots of calls to ListBuffer#size), didn't see any that were unnecessary.

In the worst case, this saves two calls to size(); in the common case, it saves one call.;;;","04/Oct/12 13:02;mumrah;Only compatible with 0.8;;;","04/Oct/12 17:30;junrao;Thanks for the patch. Committed to 0.8.

This performance issue seems to have been fixed in scala 2.9.1.
https://issues.scala-lang.org/browse/SI-4933;;;","04/Oct/12 18:02;jkreps;Why not just make this an ArrayBuffer so that there are 0 O(n) operations? I suspect the ListBuffer thing was just a misunderstanding of scala collections underlying implementation (i.e. ListBuffer != ArrayList).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multifetch response size overflow,KAFKA-436,12601314,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,kamaradclimber,kamaradclimber,03/Aug/12 07:11,07/Feb/15 23:39,14/Jul/23 05:39,07/Feb/15 23:39,0.7.1,0.8.0,,,,,,,,,core,,,0,,,,"Fetchsize parameter is per partition in fetch requests. 
This makes the size of response unpredicatable and thus might overflow.

(original email : http://mail-archives.apache.org/mod_mbox/incubator-kafka-users/201208.mbox/%3C416A89FBBC95114F8B2D2E91781541E9638B201E%40SRVEX02.criteois.lan%3E   )

Three propositions : 
- fetch size is only a hint, broker will ensure that it won't overflow (maybe not sending too much data)
- broker respect fetchsize as a global limit and not by partition
- broker may not send data for all topics to avoid the overflow.

Changing size to int64 could also decrease probablity of such event.

",,jkreps,kamaradclimber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,248174,,,Sat Feb 07 23:39:42 UTC 2015,,,,,,,,,,"0|i09m07:",53998,,,,,,,,,,,,,,,,,,,,"07/Feb/15 23:39;jkreps;This should be fixed as part of the response size issues being done for the new consumer.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
allow consumer to read from followers,KAFKA-432,12601089,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,yeyangever,junrao,junrao,01/Aug/12 16:19,12/Oct/12 02:57,14/Jul/23 05:39,12/Oct/12 02:57,0.8.0,,,,,,,,,,core,,,0,optimization,,,"For debugging purpose, it would be convenient if we allow a consumer to consume from a follower replica.",,junrao,nehanarkhede,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/12 23:19;yeyangever;kafka_432_v1.diff;https://issues.apache.org/jira/secure/attachment/12547304/kafka_432_v1.diff","02/Oct/12 21:48;yeyangever;kafka_432_v2.diff;https://issues.apache.org/jira/secure/attachment/12547451/kafka_432_v2.diff","04/Oct/12 00:48;yeyangever;kafka_432_v3.diff;https://issues.apache.org/jira/secure/attachment/12547646/kafka_432_v3.diff","12/Oct/12 00:27;yeyangever;kafka_432_v4.diff;https://issues.apache.org/jira/secure/attachment/12548830/kafka_432_v4.diff","12/Oct/12 02:13;yeyangever;kafka_432_v5.diff;https://issues.apache.org/jira/secure/attachment/12548848/kafka_432_v5.diff",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,240632,,,Fri Oct 12 02:57:15 UTC 2012,,,,,,,,,,"0|i0142v:",4430,,,,,,,,,,,,,,,,,,,,"01/Oct/12 23:19;yeyangever;
1. removing the ""ConsumerShell.scala"" file

2. Use ""replicaId"" field in ""FetchRequest"" and ""OffsetRequest"" to indicate where the fetch is from, -1 means ""OrdinaryConsumerId"", -2 means ""LowLevelConsumerId"", instead of using ""clientId"" to distinguish different request 

3. create a function ""getLocalReplica()"" in ReplicaManager, which returns the local replica even if it's not the leader. This function is used in KafkaApis, to work with requests from ""LowLevelConsumerId""

4. In kafkaApis, when handling ""offsetRequest"" and ""fetchRequest"", treat differently depending on the value of the ""replicaId"" field. If it's from ""lowLevelConsumer"", use ""getLocalReplica"" instead of ""getLocalReplicaIfLeader""

5. Move the ""earliestOrLatestOffset()"" function from ""zookeeperConsumerConnector"" to ""SimpleConsumer"" because it's now used by both the high level and low level consumer. (And create two overloaded functions because the client can get it from either zkConnect or from brokerList)

6. Move the ""MessageFormmater"" inner class out of ""ConsoleConsumer"", because it's now used by both the console consumer and the simple consumer shell

7. Update the logic of simpleConsumerShell so that it supports fetching from any specified replica now. 

8. Removing some unused import and fixed missing arguments in some log statement;;;","02/Oct/12 17:07;junrao;Thanks for patch v1. Some comments:

1. ReplicaManager.getLocalReplica(): We shouldn't be creating a new replica in this method. Also, this function is almost the same as getReplica() except that it wants to throw an exception if the replica is not found. How about creating a new method getReplicaOrException that calls getReplica and throws an exception if replica is none?

2. object MessageFormatter: remove the extra empty lines above.

3. RequestOrResponse: Could we change LowLevelConsumerId to DebuggingConsumerId so that people is aware that it's only for debugging purpose?

4. remove unused imports

5. SimpleConsumerShell: 
5.1 The comment for the --offset is inaccurate. -2 is from beginning.
5.2 The --server option should be changed to --broker-list and the format should be changed accordingly (to take in  a list).
5.3 Can we add a command line option to print out the offset for each message too?
5.4 Could we add a command line option to specify the maxWaitMs for each fetch request and defaults it to 1000ms? Also, in FetchRequestBuilder, could we set the default maxWait and minBytes to that in ConsumerConfig object, instead of FetchRequest?

Could you also rebase?
;;;","02/Oct/12 21:48;yeyangever;
All comments for patch v1 addressed;;;","04/Oct/12 00:36;junrao;Thanks for patch v2. A few other minor comments:

10. FetchRequestBuilder: could we set the default maxWait and minBytes to that in ConsumerConfig object, instead of FetchRequest?

11. ReplicaManager.getLocalReplica(): Could we use getReplica?

12. Let's remove the consumer shell script in bin/;;;","04/Oct/12 00:48;yeyangever;
addressed all comments for v2 patch;;;","09/Oct/12 04:38;junrao;Patch no longer applies. Could you rebase?;;;","12/Oct/12 00:27;yeyangever;
rebase from kafka 510;;;","12/Oct/12 01:46;nehanarkhede;Thanks for the patch! Overall, looks pretty good. A few more comments -

1. FetchRequest
1.1 isFromOrdinaryConsumer and isFromLowLevelConsumer methods are unused. How about adding them when we have a use for them ?
1.2 If you still want to keep them, rename isFromLowLevelConsumer to isFromDebuggingConsumer to be consistent

2. FetchResponse

Revert this file since you changed the spacing between the ( and { at the end of a function name and that does not conform with our coding convention

3. KafkaApis

The following statements are not easy to understand and should not be at info level

    info(""bytes readable: "" + bytesReadable)
    info(""data read: "" + dataRead)

For example, if you read them in a busy Kafka log, would you be able to make sense of it ?

3. OffsetRequest

Rename isFromLowLevelClient to isFromDebuggingClient

4. SimpleConsumer

In earliestOrLatestOffset API, the response variable is never used

5. SimpleConsumerShell

1. You don't have a space between ) and { at the start of few blocks. We have been following the convention of including a space between those, please can you conform to convention in all your patches ?

2. There is one very-long-statement in the file. Please can you break it ?
;;;","12/Oct/12 02:13;yeyangever;
Thanks for the review! And they're addressed in v5 patch.;;;","12/Oct/12 02:36;nehanarkhede;+1. v5 looks good. ;;;","12/Oct/12 02:57;nehanarkhede;Committed v5.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogCorruptionTest.testMessageSizeTooLarge fails occasionally,KAFKA-431,12600935,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,junrao,junrao,31/Jul/12 23:57,23/Aug/12 17:17,14/Jul/23 05:39,23/Aug/12 17:17,0.8.0,,,0.8.0,,,,,,,core,,,0,replication-testing,,,"It fails with the following exception:

[0m[[0minfo[0m] [0mTest Starting: testMessageSizeTooLarge(kafka.log.LogCorruptionTest)[0m
[2012-07-31 15:54:57,525] ERROR KafkaApi on Broker 0, Error while retrieving topic metadata (kafka.server.KafkaApis:99)
java.lang.NullPointerException
	at scala.util.parsing.combinator.Parsers$NoSuccess.<init>(Parsers.scala:131)
	at scala.util.parsing.combinator.Parsers$Failure.<init>(Parsers.scala:158)
	at scala.util.parsing.combinator.Parsers$$anonfun$acceptIf$1.apply(Parsers.scala:489)
	at scala.util.parsing.combinator.Parsers$$anonfun$acceptIf$1.apply(Parsers.scala:487)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Success.flatMapWithNext(Parsers.scala:113)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:162)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Success.flatMapWithNext(Parsers.scala:113)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Success.flatMapWithNext(Parsers.scala:113)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Success.flatMapWithNext(Parsers.scala:113)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:200)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:203)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:208)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:182)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:742)
	at scala.util.parsing.json.JSON$.parseRaw(JSON.scala:71)
	at scala.util.parsing.json.JSON$.parseFull(JSON.scala:85)
	at kafka.utils.ZkUtils$$anonfun$getPartitionAssignmentForTopics$1.apply(ZkUtils.scala:461)
	at kafka.utils.ZkUtils$$anonfun$getPartitionAssignmentForTopics$1.apply(ZkUtils.scala:456)
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)
	at scala.collection.LinearSeqLike$$anon$1.foreach(LinearSeqLike.scala:52)
	at kafka.utils.ZkUtils$.getPartitionAssignmentForTopics(ZkUtils.scala:456)
	at kafka.admin.AdminUtils$$anonfun$getTopicMetaDataFromZK$1.apply(AdminUtils.scala:93)
	at kafka.admin.AdminUtils$$anonfun$getTopicMetaDataFromZK$1.apply(AdminUtils.scala:91)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
	at scala.collection.immutable.List.foreach(List.scala:45)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
	at scala.collection.immutable.List.map(List.scala:45)
	at kafka.admin.AdminUtils$.getTopicMetaDataFromZK(AdminUtils.scala:91)
	at kafka.server.KafkaApis$$anonfun$handleTopicMetadataRequest$1.apply(KafkaApis.scala:433)
	at kafka.server.KafkaApis$$anonfun$handleTopicMetadataRequest$1.apply(KafkaApis.scala:423)
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
	at scala.collection.immutable.List.foreach(List.scala:45)
	at kafka.server.KafkaApis.handleTopicMetadataRequest(KafkaApis.scala:422)
	at kafka.server.KafkaApis.handle(KafkaApis.scala:61)
	at kafka.server.KafkaRequestHandler.run(KafkaRequestHandler.scala:38)
	at java.lang.Thread.run(Thread.java:662)
",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299124,,,Thu Aug 23 17:17:20 UTC 2012,,,,,,,,,,"0|i15zqv:",243091,,,,,,,,,,,,,,,,,,,,"01/Aug/12 00:01;junrao;Not sure if this is due to that JSON.parseFull is not thread safe.;;;","23/Aug/12 17:17;junrao;Should be fixed as part of kafka-379.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
need to update leaderAndISR path in ZK conditionally in ReplicaManager,KAFKA-428,12600874,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yeyangever,junrao,junrao,31/Jul/12 15:44,25/Aug/12 00:07,14/Jul/23 05:39,25/Aug/12 00:07,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,,,"When the leader tries to update the leaderAndISR path in ZK, the path may have been updated by the controller. When this happens, the leader should abort the update and log it. The controller should send new requests to the leader later on.",,junrao,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/12 00:50;yeyangever;kafka_428_v1.diff;https://issues.apache.org/jira/secure/attachment/12542219/kafka_428_v1.diff","24/Aug/12 21:34;yeyangever;kafka_428_v2.diff;https://issues.apache.org/jira/secure/attachment/12542344/kafka_428_v2.diff","24/Aug/12 23:24;yeyangever;kafka_428_v3.diff;https://issues.apache.org/jira/secure/attachment/12542363/kafka_428_v3.diff",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299123,,,Sat Aug 25 00:07:26 UTC 2012,,,,,,,,,,"0|i15zqn:",243090,,,,,,,,,,,,,,,,,,,,"24/Aug/12 00:50;yeyangever;
Adding conditional update ;;;","24/Aug/12 16:19;junrao;Thanks for patch v1. Some comments:

1. Since there can by more than 1 leaderAndISR requests being served at a broker  concurrently, we should use AtomicInt for leaderEpoch so that we can conditionally check and update leaderEpoch. Also, it's probably better o add a becomeLeaderOrFollower method in Partition, which ReplicaManager calls directly. That way, the atomic check and update of leaderEpoch only needs to be done once.

2. updateISR(): There is no need to read leaderAndISR from ZK since they are already cached in Partition.;;;","24/Aug/12 21:34;yeyangever;
Fixes as suggested;;;","24/Aug/12 22:01;junrao;Comments for patch v2:

20. Partition.updateISR(): No need to call leaderReplicaIfLocal() since the caller guarantees that local replica is still the leader.

21. There is no need to compute and check shouldBecomeLeader in makeLeader(). If the leaderEpoch in the request is higher, this broker can just become the leader. Similarly, there is no need to compute shouldBecomeFollwer in makeFollwer().;;;","25/Aug/12 00:07;junrao;thanks for patch v3. Committed to 0.8 with minor format change.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong class name in performance test scripts,KAFKA-425,12600586,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,akitada,akitada,29/Jul/12 07:00,14/Aug/12 19:57,14/Jul/23 05:39,14/Aug/12 19:57,,,,0.8.0,,,,,,,,,,0,,,,perf tools were moved to perf by KAFKA-176 but scripts used to run those tools weren't updated accordingly.,,akitada,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/12 07:09;akitada;KAFKA-425.patch;https://issues.apache.org/jira/secure/attachment/12538270/KAFKA-425.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,248163,,,Tue Aug 14 19:57:24 UTC 2012,,,,,,,,,,"0|i09lxb:",53985,,,,,,,,,,,,,,,,,,,,"29/Jul/12 07:09;akitada;This patch fixes class names;;;","14/Aug/12 19:57;jkreps;Applied to 0.8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove invalid mirroring arguments from kafka-server-start.sh,KAFKA-424,12600439,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,jkreps,tommie,tommie,27/Jul/12 08:44,14/Aug/12 19:59,14/Jul/23 05:39,14/Aug/12 19:59,0.7.1,,,0.8.0,,,,,,,core,,,0,,,,"Since r1310645, mirroring is in MirrorMaker, and kafka.Kafka.main()
only supports a single argument.
",,tommie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/12 08:44;tommie;0001-Remove-invalid-mirroring-arguments-from-kafka-server.patch;https://issues.apache.org/jira/secure/attachment/12538140/0001-Remove-invalid-mirroring-arguments-from-kafka-server.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,248162,,,Fri Jul 27 08:47:21 UTC 2012,,,,,,,,,,"0|i09lx3:",53984,,,,,,,,,,,,,,,,,,,,"27/Jul/12 08:47;tommie;This old invocation is shown at https://cwiki.apache.org/confluence/display/KAFKA/Kafka+mirroring, so maybe that page should be updated/get a deprecation mark as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogRecoveryTest has a transient test failure,KAFKA-421,12600384,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yeyangever,nehanarkhede,nehanarkhede,26/Jul/12 20:59,29/Aug/12 00:00,14/Jul/23 05:39,21/Aug/12 20:34,0.8.0,,,,,,,,,,,,,0,replication-testing,,," [0m[ [31merror [0m]  [0mTest Failed: testHWCheckpointWithFailuresMultipleLogSegments(kafka.server.LogRecoveryTest) [0m
java.lang.AssertionError: expected:<120> but was:<150>
        at org.junit.Assert.fail(Assert.java:69)
        at org.junit.Assert.failNotEquals(Assert.java:314)
        at org.junit.Assert.assertEquals(Assert.java:94)
        at org.junit.Assert.assertEquals(Assert.java:104)
        at kafka.server.LogRecoveryTest.testHWCheckpointWithFailuresMultipleLogSegments(LogRecoveryTest.scala:239)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at junit.framework.TestCase.runTest(TestCase.java:164)
        at junit.framework.TestCase.runBare(TestCase.java:130)
        at junit.framework.TestResult$1.protect(TestResult.java:110)
        at junit.framework.TestResult.runProtected(TestResult.java:128)
        at junit.framework.TestResult.run(TestResult.java:113)
        at junit.framework.TestCase.run(TestCase.java:120)
        at junit.framework.TestSuite.runTest(TestSuite.java:228)
        at junit.framework.TestSuite.run(TestSuite.java:223)
        at junit.framework.TestSuite.runTest(TestSuite.java:228)
        at junit.framework.TestSuite.run(TestSuite.java:223)
        at org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)
        at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
        at sbt.TestRunner.run(TestFramework.scala:53)
        at sbt.TestRunner.runTest$1(TestFramework.scala:67)
        at sbt.TestRunner.run(TestFramework.scala:76)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.NamedTestTask.run(TestFramework.scala:92)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
        at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
        at sbt.impl.RunTask.runTask(RunTask.scala:85)
        at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Control$.trapUnit(Control.scala:19)
        at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)
 [0m[ [0minfo [0m]  [34m== core-kafka / kafka.server.LogRecoveryTest == [0m",,nehanarkhede,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-427,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299119,,,Tue Aug 28 23:59:56 UTC 2012,,,,,,,,,,"0|i15zpr:",243086,,,,,,,,,,,,,,,,,,,,"16/Aug/12 00:37;yeyangever;

I've run the test a few times and it seems has stabilized and the failure never happens;;;","28/Aug/12 23:59;nehanarkhede;Happens in Jenkins build #33 

[0m[[31merror[0m] [0mTest Failed: testHWCheckpointNoFailuresSingleLogSegment(kafka.server.LogRecoveryTest)[0m
java.lang.AssertionError: expected:<60> but was:<90>
	at org.junit.Assert.fail(Assert.java:69)
	at org.junit.Assert.failNotEquals(Assert.java:314)
	at org.junit.Assert.assertEquals(Assert.java:94)
	at org.junit.Assert.assertEquals(Assert.java:104)
	at kafka.server.LogRecoveryTest.testHWCheckpointNoFailuresSingleLogSegment(LogRecoveryTest.scala:72)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:228)
	at junit.framework.TestSuite.run(TestSuite.java:223)
	at junit.framework.TestSuite.runTest(TestSuite.java:228)
	at junit.framework.TestSuite.run(TestSuite.java:223)
	at org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)
	at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
	at sbt.TestRunner.run(TestFramework.scala:53)
	at sbt.TestRunner.runTest$1(TestFramework.scala:67)
	at sbt.TestRunner.run(TestFramework.scala:76)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.NamedTestTask.run(TestFramework.scala:92)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
	at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
	at sbt.impl.RunTask.runTask(RunTask.scala:85)
	at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Control$.trapUnit(Control.scala:19)
	at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
maintain HW correctly with only 1 replica in ISR,KAFKA-420,12600349,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,26/Jul/12 17:12,25/Aug/12 00:47,14/Jul/23 05:39,25/Aug/12 00:47,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,,,"Currently, the HW maintenance logic is only triggered when handling fetch requests from the follower. As a result, if the ISR has only 1 replica, the HW won't be incremented since there is no request from the follower to trigger the maintenance logic.",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/12 03:41;junrao;kafka-420_v1.patch;https://issues.apache.org/jira/secure/attachment/12541860/kafka-420_v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299118,,,Sat Aug 25 00:47:16 UTC 2012,,,,,,,,,,"0|i15zpj:",243085,,,,,,,,,,,,,,,,,,,,"26/Jul/12 17:18;junrao;First of all, we need to think through what message commits mean when there is only 1 replica in ISR. Does it mean message is in memory in the only replica or do we have to wait for the message to be flushed to disk. It seems to me that the latter is probably more meaningful and is compatible with what we have in 0.7. If the latter is what we want to implement, we will need to add a call to ReplicaManager.maybeIncrementLeaderHW from the log flushing logic.;;;","22/Aug/12 03:41;junrao;Attach patch v1.

This is a simpler fix. Instead of waiting for data flushed to disk, this patch just advances the HW after the data is in memory. If a user cares about durability, he can always increase the replication factor. 

Patched an existing unit test to expose the problem. Patched 3 places where we may need to increment the HW: (1) new produce requests coming to the leader; (2) ISR shrinks; (3) a replica becomes the leader.;;;","24/Aug/12 02:07;nehanarkhede;Looks good. +1;;;","25/Aug/12 00:47;junrao;Thanks for the review. Rebased and committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in ConsumerFetcherManager,KAFKA-418,12600205,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,nehanarkhede,nehanarkhede,25/Jul/12 18:52,27/Jul/12 00:46,14/Jul/23 05:39,26/Jul/12 23:58,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"[info] Test Starting: testConsumerDecoder(kafka.consumer.ZookeeperConsumerConnectorTest)
[2012-07-25 11:50:23,324] ERROR ConsumerFetcherThread-group1_consumer1-0-1 error in fetching (kafka.consumer.ConsumerFetcherThread:99)
java.lang.NullPointerException
        at kafka.consumer.ConsumerFetcherManager.getPartitionTopicInfo(ConsumerFetcherManager.scala:124)
        at kafka.consumer.ConsumerFetcherThread.processPartitionData(ConsumerFetcherThread.scala:36)
        at kafka.server.AbstractFetcherThread$$anonfun$run$5$$anonfun$apply$1.apply(AbstractFetcherThread.scala:97)
        at kafka.server.AbstractFetcherThread$$anonfun$run$5$$anonfun$apply$1.apply(AbstractFetcherThread.scala:89)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
        at kafka.server.AbstractFetcherThread$$anonfun$run$5.apply(AbstractFetcherThread.scala:89)
        at kafka.server.AbstractFetcherThread$$anonfun$run$5.apply(AbstractFetcherThread.scala:88)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
        at kafka.server.AbstractFetcherThread.run(AbstractFetcherThread.scala:88)
[2012-07-25 11:50:23,328] ERROR Closing socket for /127.0.0.1 because of error (kafka.network.Processor:99)
",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/12 02:02;junrao;kafka-418_v1.patch;https://issues.apache.org/jira/secure/attachment/12537946/kafka-418_v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299117,,,Thu Jul 26 23:58:23 UTC 2012,,,,,,,,,,"0|i15zpb:",243084,,,,,,,,,,,,,,,,,,,,"26/Jul/12 02:02;junrao;Attach patch v1. The problem is that ConsumerFetcherManager cleared the partitionMap first, before closing the fetchers. ;;;","26/Jul/12 16:05;nehanarkhede;+1;;;","26/Jul/12 23:58;junrao;Thanks for reviewing this. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller tests throw several zookeeper errors,KAFKA-416,12600198,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yeyangever,nehanarkhede,nehanarkhede,25/Jul/12 18:17,25/Aug/12 00:28,14/Jul/23 05:39,25/Aug/12 00:28,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,replication-testing,,"[info] == kafka.controller.ControllerBasicTest ==
[info] Test Starting: testControllerFailOver(kafka.controller.ControllerBasicTest)
[2012-07-25 11:16:06,911] WARN Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running (org.apache.zookeeper.server.NIOServerCnxn:639)
[info] Test Passed: testControllerFailOver(kafka.controller.ControllerBasicTest)
[info] Test Starting: testControllerCommandSend(kafka.controller.ControllerBasicTest)
[2012-07-25 11:16:13,802] WARN Session 0x138bf5a6e12000c for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:13,814] WARN Session 0x138bf5a6e12000a for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:13,822] WARN Session 0x138bf5a6e120008 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:13,866] WARN Session 0x138bf5a6e12000b for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:14,153] WARN Session 0x138bf5a6e120006 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:15,347] WARN Session 0x138bf5a6e12000c for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:15,712] WARN Session 0x138bf5a6e12000b for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:15,810] WARN Session 0x138bf5a6e12000a for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[2012-07-25 11:16:15,848] WARN Session 0x138bf5a6e120008 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
[info] Test Passed: testControllerCommandSend(kafka.controller.ControllerBasicTest)

These error messages suggest that the controller is not closing zookeeper sessions correctly. It could also mean that the unit test doesn't close zookeeper sessions correctly.",,junrao,nehanarkhede,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/12 01:48;yeyangever;kafka_416_v1.diff;https://issues.apache.org/jira/secure/attachment/12542226/kafka_416_v1.diff","24/Aug/12 21:55;yeyangever;kafka_416_v2.diff;https://issues.apache.org/jira/secure/attachment/12542349/kafka_416_v2.diff","25/Aug/12 00:11;yeyangever;kafka_416_v3.diff;https://issues.apache.org/jira/secure/attachment/12542377/kafka_416_v3.diff",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299116,,,Sat Aug 25 00:28:16 UTC 2012,,,,,,,,,,"0|i15zp3:",243083,,,,,,,,,,,,,,,,,,,,"24/Aug/12 01:48;yeyangever;
removing the controllerBasicTest and added a RpcDataSerializationTest;;;","24/Aug/12 16:44;junrao;Thanks for patch v1. Some comments:
1. StopReplicaRequest: Why do we need to add the encoding explicitly in readShortString() since it already defaults to utf-8?

2. RpcDataSerializationTestUtils is only used by RpcDataSerializationTest. So, it probably should be moved to RpcDataSerializationTest.

3. Can we rename RpcDataSerializationTest to RequestSerializationTest?;;;","24/Aug/12 21:55;yeyangever;
fixed as suggested;;;","25/Aug/12 00:28;junrao;Thanks for patch v3. Reverted the change in StopReplicaRequest in this patch. We probably need a separate jira to consistently change all usage of readShortString(). Committed the rest of the patch to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Controller throws NoSuchElementException while marking a broker failed,KAFKA-415,12600197,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yeyangever,nehanarkhede,nehanarkhede,25/Jul/12 18:15,23/Aug/12 18:36,14/Jul/23 05:39,23/Aug/12 18:35,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,,,"[2012-07-25 11:13:50,078] ERROR Error while removing broker by the controller (kafka.server.ControllerChannelManager:99)
java.util.NoSuchElementException: key not found: 0
        at scala.collection.MapLike$class.default(MapLike.scala:223)
        at scala.collection.mutable.HashMap.default(HashMap.scala:39)
        at scala.collection.MapLike$class.apply(MapLike.scala:134)
        at scala.collection.mutable.HashMap.apply(HashMap.scala:39)
        at kafka.server.ControllerChannelManager.removeBroker(KafkaController.scala:138)
        at kafka.server.ControllerChannelManager$$anonfun$shutDown$3.apply(KafkaController.scala:111)
        at kafka.server.ControllerChannelManager$$anonfun$shutDown$3.apply(KafkaController.scala:110)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
        at kafka.server.ControllerChannelManager.shutDown(KafkaController.scala:110)
        at kafka.server.KafkaController.shutDown(KafkaController.scala:197)
        at kafka.server.KafkaServer.shutdown(KafkaServer.scala:126)
        at kafka.server.LogRecoveryTest$$anonfun$testHWCheckpointWithFailuresMultipleLogSegments$5.apply(LogRecoveryTest.scala:237)
        at kafka.server.LogRecoveryTest$$anonfun$testHWCheckpointWithFailuresMultipleLogSegments$5.apply(LogRecoveryTest.scala:237)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
        at scala.collection.immutable.List.foreach(List.scala:45)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.immutable.List.map(List.scala:45)
        at kafka.server.LogRecoveryTest.testHWCheckpointWithFailuresMultipleLogSegments(LogRecoveryTest.scala:237)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at junit.framework.TestCase.runTest(TestCase.java:164)
        at junit.framework.TestCase.runBare(TestCase.java:130)
        at junit.framework.TestResult$1.protect(TestResult.java:110)
        at junit.framework.TestResult.runProtected(TestResult.java:128)
        at junit.framework.TestResult.run(TestResult.java:113)
        at junit.framework.TestCase.run(TestCase.java:120)
        at junit.framework.TestSuite.runTest(TestSuite.java:228)
        at junit.framework.TestSuite.run(TestSuite.java:223)
        at junit.framework.TestSuite.runTest(TestSuite.java:228)
        at junit.framework.TestSuite.run(TestSuite.java:223)
        at org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)
        at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
        at sbt.TestRunner.run(TestFramework.scala:53)
        at sbt.TestRunner.runTest$1(TestFramework.scala:67)
        at sbt.TestRunner.run(TestFramework.scala:76)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.NamedTestTask.run(TestFramework.scala:92)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
        at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
        at sbt.impl.RunTask.runTask(RunTask.scala:85)
        at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Control$.trapUnit(Control.scala:19)
        at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)
[info] Test Passed: testHWCheckpointWithFailuresMultipleLogSegments(kafka.server.LogRecoveryTest)
",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299115,,,Thu Aug 23 18:35:58 UTC 2012,,,,,,,,,,"0|i15zov:",243082,,,,,,,,,,,,,,,,,,,,"25/Jul/12 18:56;nehanarkhede;Also thrown by kafka.producer.ProducerTest -

[2012-07-25 11:55:07,684] ERROR Error while removing broker by the controller (kafka.server.ControllerChannelManager:99)
java.util.NoSuchElementException: key not found: 0
        at scala.collection.MapLike$class.default(MapLike.scala:223)
        at scala.collection.mutable.HashMap.default(HashMap.scala:39)
        at scala.collection.MapLike$class.apply(MapLike.scala:134)
        at scala.collection.mutable.HashMap.apply(HashMap.scala:39)
        at kafka.server.ControllerChannelManager.removeBroker(KafkaController.scala:138)
        at kafka.server.KafkaController$BrokerChangeListener$$anonfun$handleChildChange$1.apply$mcVI$sp(KafkaController.scala:266)
        at kafka.server.KafkaController$BrokerChangeListener$$anonfun$handleChildChange$1.apply(KafkaController.scala:265)
        at kafka.server.KafkaController$BrokerChangeListener$$anonfun$handleChildChange$1.apply(KafkaController.scala:265)
        at scala.collection.immutable.Set$Set1.foreach(Set.scala:81)
        at kafka.server.KafkaController$BrokerChangeListener.handleChildChange(KafkaController.scala:265)
        at org.I0Itec.zkclient.ZkClient$7.run(ZkClient.java:568)
        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[info] Test Passed: testZKSendWithDeadBroker(kafka.producer.ProducerTest)
;;;","23/Aug/12 18:35;junrao;This is already fixed. The controller now sync on map access.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
single_host_multi_brokers system test fails on laptop,KAFKA-413,12600044,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jfung,junrao,junrao,24/Jul/12 18:01,26/Jul/12 16:59,14/Jul/23 05:39,26/Jul/12 16:59,0.8.0,,,0.8.0,,,,,,,core,,,0,,,," I got the following exception when running system_test/single_host_multi_brokers/bin/run-test.sh. This seems to only happen on laptop, not desktop.

2012-07-24 00:22:51 cleaning up kafka server log/data dir 
2012-07-24 00:22:53 starting zookeeper 
2012-07-24 00:22:55 starting cluster 
2012-07-24 00:22:55 starting kafka server 
2012-07-24 00:22:55 -> kafka_pids[1]: 75282 
2012-07-24 00:22:55 starting kafka server 
2012-07-24 00:22:55 -> kafka_pids[2]: 75286 
2012-07-24 00:22:55 starting kafka server 
2012-07-24 00:22:55 -> kafka_pids[3]: 75291 
2012-07-24 00:22:57 creating topic [mytest] on [localhost:2181] 
creation failed because of org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids 
org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids 
at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47) 
at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685) 
at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:413) 
at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:409) 
at kafka.utils.ZkUtils$.getChildren(ZkUtils.scala:363) 
at kafka.utils.ZkUtils$.getSortedBrokerList(ZkUtils.scala:80) 
at kafka.admin.CreateTopicCommand$.createTopic(CreateTopicCommand.scala:86) 
at kafka.admin.CreateTopicCommand$.main(CreateTopicCommand.scala:73) 
at kafka.admin.CreateTopicCommand.main(CreateTopicCommand.scala) 
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids 
at org.apache.zookeeper.KeeperException.create(KeeperException.java:102) 
at org.apache.zookeeper.KeeperException.create(KeeperException.java:42) 
at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1249) 
at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1277) 
at org.I0Itec.zkclient.ZkConnection.getChildren(ZkConnection.java:99) 
at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:416) 
at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:413) 
at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675) 
... 7 more ",,jfung,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/12 16:16;jfung;kafka-413-v1.patch;https://issues.apache.org/jira/secure/attachment/12537849/kafka-413-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,248181,,,Thu Jul 26 16:59:53 UTC 2012,,,,,,,,,,"0|i09m27:",54007,,,,,,,,,,,,,,,,,,,,"25/Jul/12 16:16;jfung;Hi Jun,

The issue is due to the different shell command line options in Linux and MacOS. This is now fixed by using generic shell commands in kafka-413-v1.patch.

Thanks,
John;;;","26/Jul/12 16:59;junrao;Thanks for the patch. Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
deal with empty TopicData list in producer and fetch request,KAFKA-412,12600031,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,24/Jul/12 15:52,27/Jul/12 00:17,14/Jul/23 05:39,27/Jul/12 00:17,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"Both producer and fetch request can pass in an empty list of TopicData. Instead of handling those requests through RequestPurgatory, we should just send a response with an empty list immediately.",,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/12 16:49;junrao;kafka-412_v1.patch;https://issues.apache.org/jira/secure/attachment/12538023/kafka-412_v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,248179,,,Fri Jul 27 00:17:03 UTC 2012,,,,,,,,,,"0|i09m1r:",54005,,,,,,,,,,,,,,,,,,,,"26/Jul/12 16:49;junrao;Attach patch v1.;;;","26/Jul/12 16:56;nehanarkhede;Can we change nRequestedPartitions to something like numPartitions ? 

Other than that, it looks good. 

+1;;;","27/Jul/12 00:17;junrao;Changed the naming. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor DefaultEventHandler ,KAFKA-409,12599672,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,nehanarkhede,nehanarkhede,20/Jul/12 20:33,17/May/16 14:09,14/Jul/23 05:39,10/Feb/14 00:00,0.8.0,,,0.8.2.0,,,,,,,,,,0,optimization,,,"The code in DefaultEventHandler has evolved to be a big blob of complex procedural logic. This is difficult to understand and read. Particularly the partitionAndCollate() API returns a nested complex data structure Option[Map[Int, Map[(String, Int), Seq[ProducerData[K,Message]]]]]. This class would definitely benefit from a refactor",,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-1227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241624,,,Mon Feb 10 00:00:11 UTC 2014,,,,,,,,,,"0|i029cv:",11117,,,,,,,,,,,,,,,,,,,,"10/Feb/14 00:00;jkreps;Instead we rewrote it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the high water mark maintenance to store high watermarks for all partitions in a single file on disk,KAFKA-405,12599038,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,16/Jul/12 21:10,01/Aug/12 18:10,14/Jul/23 05:39,25/Jul/12 23:42,0.8.0,,,,,,,,,,,,,0,,,,KAFKA-46 introduced per partition leader high watermarks. But it stores those in one file per partition. A more performant solution would be to store all high watermarks in a single file on disk,,fxbing,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-355,,,,,,,,,,,,,,,,,,,,,"18/Jul/12 06:20;nehanarkhede;kafka-405-v1.patch;https://issues.apache.org/jira/secure/attachment/12536958/kafka-405-v1.patch","21/Jul/12 00:38;nehanarkhede;kafka-405-v2.patch;https://issues.apache.org/jira/secure/attachment/12537431/kafka-405-v2.patch","25/Jul/12 20:09;nehanarkhede;kafka-405-v3.patch;https://issues.apache.org/jira/secure/attachment/12537887/kafka-405-v3.patch","25/Jul/12 23:29;nehanarkhede;kafka-405-v4.patch;https://issues.apache.org/jira/secure/attachment/12537921/kafka-405-v4.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,248175,,,Wed Aug 01 18:10:55 UTC 2012,,,,,,,,,,"0|i09m0n:",54000,,,,,,,,,,,,,,,,,,,,"18/Jul/12 06:20;nehanarkhede;This patch improves the high watermark persistence strategy to store the high watermark values for all partitions in a single file.

The changes include -

1. Log.scala
1.1 Moved the highwatermark file out of the Log. Since highwatermark is related to replication state and not log persistence, it makes sense for it to not be part of kafka.log
1.2 Since the log recovery logic requires to modify log segments, recoverUptoLastCheckpointedHighWatermark() API stays in Log.scala. It is passed in the last checkpointed high watermark from ReplicaManager during make follower state change

2. ReplicaManager
2.1 Added a startup API to ReplicaManager to be consistent with all other components.
2.2 Added a scheduler that will checkpoint high watermarks at defaultFlushIntervalMs rate. I didn't think it was useful to introduce another config to control the rate at which high watermarks are flushed to disk, so I reused the one we have for flushing log segments
2.3 Added a checkpointHighwaterMark() API that will iterate through all the local replicas for each partition and write the high watermark file in the following format
number of entries  (4 bytes)
topic              (UTF)
partition          (4 bytes)
highwatermark      (8 bytes)
2.4 Added a readCheckpointedHighWatermark() API that reads the high watermark file to get the latest high watermark for a particular topic/partition. This method is called once per partition on startup, and during every make follower state change.

3. HighWatermarkPersistenceTest
Added a couple of unit tests to verify that the new high watermark persistence code is working.
This will further get tested during system testing, once KAFKA-350 is checked in
;;;","19/Jul/12 00:49;jkreps;Some of these comments are not directly related to this change but this is just the first time I have looked at this code in detail

- I think Log.recoverUptoLastCheckpointedHighWatermark(lastKnownHW: Long) is misnamed. I don't think the log should know anything about hw marks, and it also isn't doing recovery (e.g. checking the validity of the log), I think it is just blindly truncating the log. Can we change it to Log.truncateTo
- Is it possible that we need to truncate more than one segment? I.e. couldn't the segment to be truncated not be the last segment (unlikely with 1gb segments, but still a problem)
- Can we change the api in MessageSet.truncateUpto should be truncateUpTo or truncateTo
- Can you make a wrapper for the RandomAccessFile called HighwaterMarkCheckpoint and put the logic related to that there. Intuitively the logic for serializing a checkpoint shouldn't be mixed into ReplicaManager. Can you also document the file format? Is there a reason this can't be plain text? Also I think a better approach to the updates would be to create a new file, write to it, and then move it over the old one; this will make the update atomic. Not sure if that is needed...
- It would be good to have a test that covered the HighwaterMarkCheckpoint file read and write. Just basic reading/writing, nothing fancy.
- Do we need to version the hw mark checkpoint file format? I.e. maybe the first line of the file is version=x or something... Not sure if that is needed but I am paranoid about persistent formats after blowing that and being stuck. This would let format changes be handled automatically.
- Can we fix the setters/getters in Replica.scala and make changes to the leo update the leoUpdateTime. Currently I think it is encumbant on the caller to do these two things together which is odd...
- I think the use of KafkaScheduler is not quite right. This is a thread pool meant to execute many tasks. There should really only be one for all of kafka, not one per background thread. You should probably pass in a central instance as an argument rather than making two new ones.
- Also I notice that ReplicaManager.getReplica returns an Option. But then everyone who calls it needs to check the option and return an exception if it is not found. Can we just have getReplica either return the Replica or throw the exception?
- I think abbreviations should be in the form updateLeo not updateLEO and updateIsr not updateISR. Let's standardize that.;;;","20/Jul/12 02:39;nehanarkhede;1. Good point. Changed the name to Log.truncateTo(targetOffset)
2. It is possible to truncate multiple segments. The truncateTo API handles that. It deletes segments that have start offset > targetOffset. Only one segment will ever need to be truncated. Rest will have to be deleted.
3. Changed FileMessageSet.truncateUpto to truncateTo to make it consistent with Log.truncateTo
4. Created wrapper HighwatermarkCheckpoint. Documented the file format here. I had thought about atomic updates but there is only one thread that serializes the checkpoints, so didn't think swapping the old file with the new one would be required, no ?
5. As for unit testing, I've added a new test HighwatermarkPersistenceTest that tests the writing/reading high watermark values for single as well as multiple partitions.
6. I think it is a good idea to version the high watermark file, just in case we didn't cover something that we need to in the future
7. We have a JIRA open for fixing all getters/setters, so I'll defer that change. The logEndOffset logic is a little tricky. It seems correct to not expose a separate API to set the logEndOffsetUpdateTime and just let the logEndOffset setter API do it. But, here is the problem. The leader needs to record its own log end offset update time while appending messages to local log. However, since the Log doesn't know anything about logEndOffsetUpdateTime, its append API cannot set the udpate time. Also, the leader cannot use the logEndOffset setter API since its log end offset is recorded by its local Log object. The logEndOffset setter API is meant only to record the follower's log end offset. But since it makes sense for the update time to be updated while setting the logEndOffset, I've fixed it. Basically, the logEndOffset() setter API updates only the logEndOffset time when a local log exists for the replica. For all other cases, it updates both the logEndOffset as well as the logEndOffsetUpdateTime 
8. Yeah, there are several callers that use the getReplica() API and don't always re-throw an exception. Some are re-throwing an error while others are using the Option to return some default value for some state of the Replica (highwatermark). And case match in Scala is good for that since it always evaluates to a value, a try catch block doesn't. But if all the callers throw an exception, then it makes sense to have getReplica throw it instead. 
9. You have a valid point about KafkaScheduler usage. However, we name the thread appropriately with every instance of the scheduler. Ideally, if there was a way to override the base thread name independently with the same scheduler, it would be possible to use a single scheduler.
10. Good point about abbreviations. Fixed that. Let's standardize on this.

Also, changed the name to updateLEO to updateLeo
;;;","23/Jul/12 16:02;jkreps;4. The concern I have is that fs writes are not atomic unless they are < 1 block. What happens if the broker fails in the middle of a write? Also is there a reason we can't just have a plain text file? That will be a little bulkier, but the good thing is you can cat it and see what is there. I think that will be a lot nicer operationally then another binary format...
5. Can we do that without the Thread.sleeps? For example it would seem this would be accomplished by not using a time based flush interval. Also
9. Can you add that facility then to KafkaScheduler? Use Thread.setName() with a wrapper runnable. I think making lots of single-threaded thread pools is too hacky.

Also
- FileChannel.truncateTo sets the size to whatever is given and calls truncate, it would be good to be a little more defensive. If the offset given is larger than size we should handle that gracefully (throw illegalargumentexception or something). Currently it would call truncate() on the filechannel which would have no effect but it would set the size to the new size which would not match the size of the file, which might cause odd things to happen.
- HighwaterMarkCheckpoint.scala: new RandomAccessFile(path + ""/"" + HighwaterMarkCheckpoint.highWatermarkFileName. Should use new File(path, filename) for portability.
- Can you mark any method not in the public interface for ReplicaManager as private? It is currently really hard to tell what the capabilities it provides...
;;;","23/Jul/12 16:09;jkreps;Also, ReplicaManager:

This class has a very odd public interface. Instead of managing replicas it has a bunch of passive calls--addLocalReplica(), addRemoteReplica(), etc. Who calls these? KafkaServer seems to have its own wrapper for these. Then it passes these methods on as arguments to KafkaZookeeper. Does this make sense? I think it would be good if ReplicaManager handled replica management, even if that means it depends on zookeeper.

;;;","24/Jul/12 01:30;junrao;Thanks for patch v2. Some comments:
20. Log.truncateTo(): The following code seems to be used just for getting the first segment. Can we just use segmentToBeTruncated(0)?
      segmentToBeTruncated match {
        case Some(segment) =>
          val truncatedSegmentIndex = segments.view.indexOf(segment)
          segments.truncLast(truncatedSegmentIndex)
        case None =>
      }

21. FileMessageSet: Do we need setHighWaterMark? It seems it's always the same as setSize.

22. ReplicaManager:
22.1 recordLeaderLogUpdate(): Could we rename it to recordLeaderLogEndOffset()?
22.2 close(): Could we rename it to shutdown to map startup()?
22.3 readCheckpointedHighWatermark(): We should just read the HW from memory. The on-disk version is only useful on broker startup when we populate the in-memory HW using the on disk version.

23. HighwaterMarkCheckpoint: Is it better to name the file "".highwaterMark"" so that it's hidden?
;;;","25/Jul/12 20:09;nehanarkhede;Jay's comments

4. Changed the write operation for highwatermark file to be atomic.
5. The sleep is in place to allow the follower to send another fetch request to the leader to allow the leader to tell it the latest leader high watermark. It cannot be fixed by flushing more frequently

9. Added the ability to set the name of a thread in KafkaScheduler. Also, saw that the KafkaScheduler took in a isDaemon variable, but didn't really use it. Refactored KafkaScheduler to create daemon/non-daemon threads with different names.

10. There was an assertion that protects against this in the only API that called FileMessageSet.truncateTo. Moved that to FileMessageSet instead and changed it to throw KafkaException. Also, handled all exceptions in the become follower/become leader state change API to log an error stating that the state change failed. This will make debugging easier.

11. Have marked methods not in the public interface for ReplicaManager as private? Agree that there is some room for refactoring. Added your suggestion to KAFKA-351 that we have filed to cover the refactoring of ReplicaManager and KafkaZookeeper. Currently, with the controller patch, KafkaZookeeper is going to look very different. So I'd rather wait until controller patch is in.

Jun's comments

20. There will only be one segment that will fit this criteria => segment.start >= hw && segment.endOffset < hw. That code truncates the one and only segment that matches this criteria

21. The setHighwatermark variable and its references are deleted as part of KAFKA-350

22. 
1. Changed to recordLeaderLogEndOffset()
2. Changed close() to shutdown() for LogManager, ReplicaManager and KafkaZookeeper
3. Good point. Fixed it to read from the file only on startup

23. Yes, it might be ok to have the file hidden.

Other fixes -

Log.scala
1. truncateTo() had bugs in that it used the size() API on the FileMessageSet of the segment to get the absolute end offset. Fixed it to use the absoluteEndOffset() API of LogSegment instead.
;;;","25/Jul/12 23:10;jkreps;+1 ;;;","25/Jul/12 23:29;nehanarkhede;Removed the sleeps in LogRecoveryTest. Will probably checkin this version of the patch;;;","25/Jul/12 23:42;nehanarkhede;Thanks a lot for the timely reviews ! Checked in v4 ;;;","01/Aug/12 18:10;junrao;Just had a look of v4. A couple of minor comments:

40. HighwaterMarkCheckpoint:
40.1 If tempHwFile already exists, we can just overwrite it since we know hwFile is always safe.
40.2 There is no need to delete hwFile first and then rename tempHwFile to it. Rename should do the deletion. Currently, if we fail at the bad time, we could end up without a hwFile.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0.7.1 release notes should point to origin like 0.7.0 does,KAFKA-403,12598739,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,charmalloc,charmalloc,charmalloc,13/Jul/12 18:18,16/Jul/12 18:05,14/Jul/23 05:39,16/Jul/12 18:05,,,,0.7.1,,,,,,,,,,0,,,,,,charmalloc,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/12 17:41;charmalloc;KAFKA-403.patch;https://issues.apache.org/jira/secure/attachment/12536669/KAFKA-403.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299112,,,Mon Jul 16 18:05:58 UTC 2012,,,,,,,,,,"0|i15zo7:",243079,,,,,,,,,,,,,,,,,,,,"16/Jul/12 17:48;nehanarkhede;+1. Thanks for getting this in, Joe !;;;","16/Jul/12 18:05;charmalloc;commited

ran svn up on /www/incubator.apache.org/content/kafka;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mirroring system test fails on 0.8,KAFKA-396,12598106,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jjkoshy,jjkoshy,jjkoshy,09/Jul/12 22:58,14/Jun/13 03:59,14/Jul/23 05:39,14/Jun/13 03:59,0.8.0,,,0.8.0,,,,,,,core,,,0,replication-testing,,,Just making a note of this - will look into this later.,,jjkoshy,junrao,yeyangever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241668,,,Fri Jun 14 03:59:47 UTC 2013,,,,,,,,,,"0|i029qn:",11179,,,,,,,,,,,,,,,,,,,,"16/Aug/12 00:32;yeyangever;I think this no longer happens, may be closed;;;","16/Aug/12 00:55;jjkoshy;Did you check? It may be fixed but needs to be verified - the script is obsolete due to changes in the arguments to tools such as producer performance.;;;","14/Jun/13 03:59;junrao;This is fixed in 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
update site with steps and notes for doing a release under developer,KAFKA-394,12597819,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,charmalloc,charmalloc,charmalloc,06/Jul/12 19:42,25/Nov/13 18:35,14/Jul/23 05:39,25/Nov/13 18:35,,,,,,,,,,,,,,0,,,,steps in release process including updating the dist directory,,charmalloc,joestein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241667,,,Mon Nov 25 18:35:53 UTC 2013,,,,,,,,,,"0|i029q7:",11177,,,,,,,,,,,,,,,,,,,,"25/Nov/13 18:35;joestein;https://cwiki.apache.org/confluence/display/KAFKA/Release+Process;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Producer request and response classes should use maps,KAFKA-391,12596693,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jjkoshy,jjkoshy,jjkoshy,02/Jul/12 18:32,07/Nov/14 06:57,14/Jul/23 05:39,17/Sep/12 20:21,,,,0.8.0,,,,,,,,,,0,optimization,,,"Producer response contains two arrays of error codes and offsets - the ordering in these arrays correspond to the flattened ordering of the request arrays.

It would be better to switch to maps in the request and response as this would make the code clearer and more efficient (right now, linear scans are used in handling producer acks).

We can probably do the same in the fetch request/response.",,jjkoshy,junrao,waldenchen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/12 22:26;jjkoshy;KAFKA-391-draft-r1374069.patch;https://issues.apache.org/jira/secure/attachment/12543757/KAFKA-391-draft-r1374069.patch","12/Sep/12 01:33;jjkoshy;KAFKA-391-v2.patch;https://issues.apache.org/jira/secure/attachment/12544753/KAFKA-391-v2.patch","14/Sep/12 00:14;jjkoshy;KAFKA-391-v3.patch;https://issues.apache.org/jira/secure/attachment/12545086/KAFKA-391-v3.patch","15/Sep/12 00:12;jjkoshy;KAFKA-391-v4.patch;https://issues.apache.org/jira/secure/attachment/12545251/KAFKA-391-v4.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299107,,,Fri Nov 07 06:57:57 UTC 2014,,,,,,,,,,"0|i15zn3:",243074,,,,,,,,,,,,,,,,,,,,"04/Sep/12 22:26;jjkoshy;Attached a draft patch. I was out of action for the last two weeks and 0.8
has moved along quite a bit, so this will need a significant rebase. (This
patch should cleanly apply on r1374069.) Before I spend time rebasing I was
hoping to get some high-level feedback on this change. I think it makes the
code clearer and likely more efficient (although an extensive perf test is
pending).

It is pretty straightforward and should be quick to review at least for this
round. The main changes are as follows:

- Switched to maps for the data/status in the producer request/response
  classes.
- I'm using SortedMap as that helps with the serializing to the wire format.
  The maps of course are only in memory. The wire format is mostly the same
  although I modified the response format to include topic names (similar to
  the request format). This is not strictly required but I think it is
  clearer this way.
- Made some of the related code simpler/clearer (e.g., produceToLocalLog,
  DelayedProduce.respond, etc.)
- Minor fixes in the tests due to the above changes.
;;;","04/Sep/12 23:20;jjkoshy;One more comment I forgot to add above: we can/should probably get rid of the global
error code in ProducerResponse as it is unused and does not seem to make sense
anyway in the absence of a ""generic error"" code.;;;","05/Sep/12 17:40;junrao;Thanks for the patch. Some comments:

1. We probably shouldn't use scala sortedMap in kafka.javaapi.ProducerRequest. On that thought, why can't ProducerRequest take a regular map in the constructor? If we want some ordering on the serialized data, we can sort the map before serialization. SortedMap seems to reveal an implementation detail that clients don't (and shouldn't) really care. Ditto for ProducerResponse.

2. To be consistent, should we change FetchResponse (and maybe FetchRequest) to use map, instead of array too?

3. ProducerRequest.writeTo() can use foreach like the following:
   groupedData.foreach{ case(topic, TopciAndPartitionData) => ... }
;;;","05/Sep/12 20:45;jjkoshy;(1) Yes that's a good point. I should have thought through it more carefully. We only need the sorted property once (serialization).
(2) I had considered this, but decided to punt to see how the producer-side came out. I'll take a stab at the fetch side as well as part of this jira.
(3) Nice.

So I'll rebase now and incorporate the above.
;;;","05/Sep/12 20:51;jjkoshy;(Accidentally deleted the description.);;;","05/Sep/12 23:46;junrao;Also, I agree that we can remove the global errorcode from both the fetch and the produce responses.;;;","12/Sep/12 01:33;jjkoshy;2.1 - Rebased after the first review, but I will need to rebase again. Unit
  tests and system tests pass. This patch applies cleanly to svn revision
  1381858 - it would be great if this can be reviewed against that revision.   I can provide an incremental patch if that helps after I rebase.

2.2 - Removed all the equals methods in classes that were using Arrays earlier.

2.3 - Switched fetch request/response to maps. It makes the code a little
  cleaner, but I think the improvement was greater with the refactoring on
  the producer side. Anyway, this makes the APIs more consistent.  One small
  observation: previously, fetch requests would throw a
  FetchRequestFormatException if the TopicData array for a fetch request
  contained the same topic multiple times. Right now we don't do any checks
  since we use a map. We can add it to the FetchRequestBuilder, but not sure
  if it is required/worth it. Also, I think it previously would allow
  fetches from different offsets in the same partition in the same fetch
  request. That is no longer allowed, although I don't know why anyone would
  need that.

2.4 - Removed the global error code.

Another minor detail: I wonder if it would help to have a case class for
TopicAndPartition. We use (topic, partition) and the associated tuple
addressing all over the place. That would make the Map declarations slightly
clearer, although now we're accustomed to understanding that (String, Int)
must mean (topic, partitionId).
;;;","12/Sep/12 16:38;jjkoshy;BTW, I forgot to remove OffsetDetail from FetchRequest - we don't need that anymore.;;;","12/Sep/12 17:17;junrao;Thanks for patch v2. Some comments.

20. It's a good idea to create a case class of TopicPartition. This helps 2 things: (1) Tuple doesn't exist in java and javaapi.ProducerRequest currently uses tuple. (2) This avoids things like x._1._1, which is harder to understand. Similarly, should we create a case class of ProducerResponsStatus that wraps (errrocode, offset)?

21. javaapi.ProduceRequest should use java map, instead of scala map. javaapi.SyncProducer should return a java version of ProducerResponse. Thinking about it. Should we even provide a javaapi for SyncProducer since everyone should really be using the high level Producer api? In other words, SyncProducer probably is not our public api for clients. For consumers, we likely still need to provide a java version of SimpleConsumer since there may be applications that want to control the fetch offset. 

23. It doesn't look like that we have a java version of FetchRequest. We should add that  and use it in the java version of SimpleConsumer.

24. FetchResponse: We probably should add a helper method errorCode(topic, partition)?

25. AbstractFetchThread: can use the pattern response.data.foreach{ case(key, partitionData) => }

26. KafkaApis.handleFetchRequest(): can use the pattern in #25 too.

;;;","13/Sep/12 17:25;jjkoshy;Marking as blocker since I ended up changing the wire format.;;;","14/Sep/12 00:14;jjkoshy;Overview of changes in v3:

(3.1) - (20) - This actually caused bulk of the changes in v3. I did this 
for just the topic-partition pairs in producer/consumer request handling;
same for producer response status. There are a lot more places where we
could move from tuples to case classes. I think (as mentioned on the mailing
list) it would be good to do this as part of cleanup but we should defer
that for later since such changes cut across a lot of files. Going forward I
think this brings out a convention that we might want to follow. The ""scala
book"" has a reasonable guideline. I'll send out an email to kafka-dev for
discussion and add it to our coding convention depending on how that pans
out.

(3.2) (21)-(23) - Thanks for catching the javaapi issue. Couple of changes
here: 
a - Added javaapi for FetchRequest. (I needed to provide both java/non-java
  fetchrequest to the simpleconsumer since FetchBuilder returns a scala
  FetchRequest.) 
b - Java map for all the Fetch/Produce request/response in the javaapi.
c - Removed SyncProducer: agreed that is unnecessary since Producer supports
  both sync and async; made the hadoop producer code use the high level
  producer. I think that's safe - i.e., I don't see a good reason why anyone
  would ""depend"" on the data going to a single broker.
d - Got rid of the unreferenced ProducerConsumerTestHarness in javaapi.
e - Fixed the equals method in javaapi ProducerRequest; added one to
FetchResponse - actually we can abstract this out into a trait, but that is
a minor improvement that I punted on.
f - Made the ProducerRequest use Java map in javaapi.
g - (I did not add Java versions of ProducerResponse since the SyncProducer
  has been removed.)

(3.3) (24) - added the helper method, although I don't think I'm using it
anywhere.

(3.4) - Got rid of OffsetDetail.

For (25)(26) - I tried to use the pattern wherever I could, but may have 
missed a few.

I did not rebase this (i.e., it still applies on r1381858). I'll rebase 
after we are done with reviews.
;;;","14/Sep/12 04:27;junrao;V3 looks good overall. Some additional comments:

30. remove javaapi.ProducerRequest
31. We probably should call TopicPartition TopicAndPartition.
32. javaapi.SimpleConsumer: It's bit confusing to the scala version of send here. Let's at least explain in the comment how to use it.
;;;","14/Sep/12 16:49;jjkoshy;30 - Why should it be removed?;;;","14/Sep/12 16:50;jjkoshy;Ok nm - I see it's because we removed SyncProducer and only need ProducerData. Ok - so I'll rebase now and upload the final patch in a bit.;;;","15/Sep/12 00:12;jjkoshy;Here is the rebased patch. I also had to include a small edit to ReplicaFetcherThread to address the issue in KAFKA-517 which affects our system tests.;;;","16/Sep/12 23:35;junrao;Thanks for patch v4. +1 Could you fix the following minor issues before checking in?

41. scala version of FetchResponse: We throw an exception in errorCode if the map key doesn't exist. To be consistent, we should do the same for messageSet and highWatermark.

42. PartitionStatus: Since this is a case class, there is no need to define requiredOffset as val.

43. DefaultEventHandler.serialize(): This is not introduced in this patch, but could you change  error(""Error serializing message "" + t) to error(""Error serializing message "", t)




;;;","17/Sep/12 20:21;jjkoshy;Thanks for the review. Checked-in to 0.8 after addressing the minor issues.
;;;","23/Oct/14 09:35;waldenchen;Why this fix add this line code?
Sometimes got 2 responses for one request,   Why fall into this situation, will there be duplicate data in Kafka?

https://git-wip-us.apache.org/repos/asf?p=kafka.git;a=commitdiff;h=b688c3ba045df340bc32caa40ba1909eddbcbec5 
+        if (response.status.size != producerRequest.data.size)+          throw new KafkaException(""Incomplete response (%s) for producer request (%s)""+                                           .format(response, producerRequest))

;;;","23/Oct/14 21:58;junrao;The client should always receive one response per request. Are you see otherwise?;;;","23/Oct/14 22:44;waldenchen;Yes,  after add more information to the error message ,  sometimes see two response for one request.;;;","23/Oct/14 23:49;junrao;Which version of Kafka are you using? Is that easily reproducible?;;;","24/Oct/14 00:23;jjkoshy;You mean two responses to the caller of send? I don't see why those two lines would cause two responses. Can you explain further and provide steps to reproduce if there really is an issue? Do you see errors/warns in the logs?;;;","24/Oct/14 00:45;waldenchen;Not that two line cause two responses.
We just hit that two line while run.
And after change that line to output the value of response.status.size and  producerRequest.data.size ,   we found that sometimes  response.status.size=2 and producerRequest.data.size =1.

Want to ask initially why add this line?   when will response.status.size != producerRequest.data.size happen?;;;","24/Oct/14 01:08;jjkoshy;Yeah I see your point. That's interesting - I actually don't remember why that was added but it appears there must have been a legitimate reason (since you ran into it:)  ).

Since you are able to reproduce it can you actually print the full original request and response itself? It should be in the exception that is thrown.

Also, what is your broker Kafka version? Also, what is the version of the producer? Is it the same?
;;;","04/Nov/14 06:11;waldenchen;This situation happen under below scenario:
one broker is leader for several partitions, for example 3,   when send one messageset which has message for all of the 3 partitions of this broker ,      the response.status.size is 3 and  the producerRequest.data.size is 1.    then it hit this exception.   Any idea for fix?  Do we need compare response.status.size  with messagesPerTopic.Count instead of producerRequest.data.size ?


  private def send(brokerId: Int, messagesPerTopic: collection.mutable.Map[TopicAndPartition, ByteBufferMessageSet]) = {
    if(brokerId < 0) {
      warn(""Failed to send data since partitions %s don't have a leader"".format(messagesPerTopic.map(_._1).mkString("","")))
      messagesPerTopic.keys.toSeq
    } else if(messagesPerTopic.size > 0) {
      val currentCorrelationId = correlationId.getAndIncrement
      val producerRequest = new ProducerRequest(currentCorrelationId, config.clientId, config.requestRequiredAcks,
        config.requestTimeoutMs, messagesPerTopic)
      var failedTopicPartitions = Seq.empty[TopicAndPartition]
      try {
        val syncProducer = producerPool.getProducer(brokerId)
        debug(""Producer sending messages with correlation id %d for topics %s to broker %d on %s:%d""
          .format(currentCorrelationId, messagesPerTopic.keySet.mkString("",""), brokerId, syncProducer.config.host, syncProducer.config.port))
        val response = syncProducer.send(producerRequest)
        debug(""Producer sent messages with correlation id %d for topics %s to broker %d on %s:%d""
          .format(currentCorrelationId, messagesPerTopic.keySet.mkString("",""), brokerId, syncProducer.config.host, syncProducer.config.port))
        if(response != null) {
          if (response.status.size != producerRequest.data.size)
            throw new KafkaException(""Incomplete response (%s) for producer request (%s)"".format(response, producerRequest))

;;;","05/Nov/14 17:29;jjkoshy;If there are three partitions, then there will be three message-sets. i.e., producerRequest.data.size will be three, not one. Can you give example _application_ code that reproduces the issue that you are seeing?;;;","07/Nov/14 06:57;waldenchen;Many thanks for you help.
After debugging and testing, seemly I can't hit that exception.
Actually we're using one c# version client which is inherit from https://github.com/precog/kafka/tree/master/clients/csharp/src/Kafka/Kafka.Client , and after debug and compare it's code with java version, finally prove that it's the bug of the C# code.
In java version, when create ProducerRequest, it set produceRequest.data as messagesPerTopic,  and do group by topic just before send binary.
But in our c# version,  it group it first and set the produceRequest.data as dictionary of <Topic,Data>, so we hit this exception wrongly, we fixed it.

Many thanks for your time.
But anyway, can't find our related open source version from internet, our version has DefaultCallbackHandler.cs, but the version on https://github.com/precog/kafka/tree/master/clients/csharp/src/Kafka/Kafka.Client has no, so can't provide the link here.

The java link:
https://git-wip-us.apache.org/repos/asf?p=kafka.git;a=blob;f=core/src/main/scala/kafka/api/ProducerRequest.scala;h=570b2da1d865086f9830aa919a49063abbbe574d;hb=HEAD
https://git-wip-us.apache.org/repos/asf?p=kafka.git;a=blob;f=core/src/main/scala/kafka/producer/async/DefaultEventHandler.scala;h=821901e4f434dfd9eec6eceabfc2e1e65507a57c;hb=HEAD#l260
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in accessing ISR,KAFKA-386,12596468,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jjkoshy,jjkoshy,29/Jun/12 22:45,23/Aug/12 16:51,14/Jul/23 05:39,23/Aug/12 16:50,0.8.0,,,0.8.0,,,,,,,,,,0,bugs,,,"Also brought up in KAFKA-353 - Partition's inSyncReplicas is used by both KafkaApis and ReplicaManager; and is subject to concurrent writes. Should be able to just switch it to an AtomicReference, but need to look at the code more carefully to determine if that is sufficient.
",,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299105,,,Thu Aug 23 16:50:56 UTC 2012,,,,,,,,,,"0|i15zmn:",243072,,,,,,,,,,,,,,,,,,,,"23/Aug/12 16:50;junrao;This should be fixed as part of kafka-351. All accesses to ISR are now synchronized.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"RequestPurgatory enhancements - expire/checkSatisfy issue; add jmx beans",KAFKA-385,12596467,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jjkoshy,jjkoshy,jjkoshy,29/Jun/12 22:40,16/Aug/12 23:35,14/Jul/23 05:39,16/Aug/12 21:28,,,,0.8.0,,,,,,,,,,0,bugs,,,"As discussed in KAFKA-353:
1 - There is potential for a client-side race condition in the implementations of expire and checkSatisfied. We can just synchronize on the DelayedItem.
2 - Would be good to add jmx beans to facilitate monitoring RequestPurgatory stats.
",,jjkoshy,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Aug/12 23:48;jjkoshy;KAFKA-385-v1.patch;https://issues.apache.org/jira/secure/attachment/12539746/KAFKA-385-v1.patch","13/Aug/12 17:30;jjkoshy;KAFKA-385-v2.patch;https://issues.apache.org/jira/secure/attachment/12540704/KAFKA-385-v2.patch","16/Aug/12 01:44;jjkoshy;KAFKA-385-v3-with-lazy-fix.patch;https://issues.apache.org/jira/secure/attachment/12541168/KAFKA-385-v3-with-lazy-fix.patch","15/Aug/12 19:14;jjkoshy;KAFKA-385-v3.patch;https://issues.apache.org/jira/secure/attachment/12541114/KAFKA-385-v3.patch","15/Aug/12 19:14;jjkoshy;KAFKA-385-v3.patch;https://issues.apache.org/jira/secure/attachment/12541113/KAFKA-385-v3.patch","16/Aug/12 18:05;jjkoshy;KAFKA-385-v4.patch;https://issues.apache.org/jira/secure/attachment/12541255/KAFKA-385-v4.patch","07/Aug/12 23:48;jjkoshy;example_dashboard.jpg;https://issues.apache.org/jira/secure/attachment/12539744/example_dashboard.jpg","07/Aug/12 23:48;jjkoshy;graphite_explorer.jpg;https://issues.apache.org/jira/secure/attachment/12539745/graphite_explorer.jpg",,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299104,,,Thu Aug 16 21:28:56 UTC 2012,,,,,,,,,,"0|i15zmf:",243071,,,,,,,,,,,,,,,,,,,,"07/Aug/12 23:48;jjkoshy;Summary of changes and notes:

1 - Fixed the synchronization issue (raised in KAFKA-353) between
  checkSatisfied and expire by synchronizing on the DelayedItem.

2 - Added request purgatory metrics using the metrics-core library. Also
  added support for csv/ganglia/graphite reporters which I think is useful -
  e.g., I attached a graphite dashboard that was pretty easy to whip up. It
  should be a breeze to use metrics-core for other stats in Kafka.

3 - This brings in dependencies on metrics and slf4j, both with Apache
  compatible licenses. I don't know of any specific best-practices in using
  metrics-core as I have not used it before, so it would be great if people
  with experience using it glance over this patch.

4 - It's a bit hard to tell right now which metrics are useful and which are
  pointless/redundant.  We can iron that out over time.

5 - Some metrics are only global and both global and per-key (which I think
  is useful to have, e.g., to get a quick view of which partitions are
  slower).  E.g., it helped to see (in the attached screen shots) that fetch
  requests were all expiring - and it turned out to be a bug in how
  DelayedFetch requests from followers are checked for satisfaction.  The
  issue is that maybeUnblockDelayedFetch is only called if required acks is
  0/1. We need to call it always - in the FetchRequestPurgatory
  checkSatisfied method, if it is a follower request then we need to use
  logendoffset to determine the available bytes to the fetch request, and HW
  if it is a non-follower request. I fixed it to always check
  availableFetchBytes, but it can be made a little more efficient by having
  the DelayedFetch request keep track of currently available bytes in each
  topic-partition key.

6 - I realized that both the watchersForKey and per-key metrics pools keep
  growing.  It may be useful to have a simple garbage collector in the Pool
  class that garbage collects entries that are stale (e.g., due to a
  leader-change), but this is non-critical.

7 - I needed to maintain DelayedRequest metrics outside the purgatory:
  because the purgatory itself is abstract and does not have internal
  knowledge of delayed requests and their keys. Note that these metrics are
  for delayed requests - i.e., these metrics are not updated for those
  requests that are satisfied immediately without going through the
  purgatory.

8 - There is one subtlety with producer throughput: I wanted to keep per-key
  throughput, so the metric is updated on individual key satisfaction. This
  does not mean that the DelayedProduce itself will be satisfied - i.e,.
  what the metric reports is an upper-bound since some DelayedProduce
  requests may have expired.

9 - I think it is better to wait for Kafka-376 to go in first. In this
  patch, I hacked a simpler version of that patch - i.e., in
  availableFetchBytes, I check the logEndOffset instead of the
  high-watermark. Otherwise, follower fetch requests would see zero
  available bytes. Of course, this hack now breaks non-follower fetch
  requests.

10 - KafkaApis is getting pretty big - I can try and move DelayedMetrics out
  if that helps although I prefer having it inside since all the
  DelayedRequests and purgatories are in there.

11 - There may be some temporary edits to start scripts/log4j that I will
  revert in the final patch.

What's left to do:

a - This was a rather painful rebase, so I need to review in case I missed
  something.

b - Optimization described above: DelayedFetch should keep track of
  bytesAvailable for each key and FetchRequestPurgatory's checkSatisfied
  should take a topic, partition and compute availableBytes for just that
  key.

c - The JMX operations to start and stop the reporters are not working
  properly. I think I understand the issue, but will fix later.;;;","08/Aug/12 22:43;junrao;Thanks for the patch. Overall, it seems that metrics-core is easy to use. Some comments:

1. KafkaMetricsGroup.metricName: add a comment on the following statement and explain what it does
          actualPkg.replaceFirst(""""""\."""""", "".%s."".format(ident))

2. Unused imports: KafkaApis, KafkaConfig

3. Pool.getAndMaybePut(): This seems to force the caller to create a new value object on each call and in most cases, the new object is not needed.

4. FetchRequestKey and ProduceRequestKey: can they be shared?

5. Is it better to put all metrics classes in one package metrics?

6. It's useful to have topic level stats. I am wondering if we can just keep stats at the global and the topic level, but not at topic/partition level. 

7. DelayedProducerRequestMetrics,DelayedFetchRequestMetrics : Can we name the stats consistently? Something like FetchSatisfiedTime and ProduceStatisfiedTime,  FetchSatisfiedRequest and ProduceStatisfiedRequest. Also, there is some overlap with the stats in BrokerTopicStat.

8. ProducerPerformance: Why forcing producer acks to be 2? Shouldn't that come from the command line?

9. The changes and the new files in config/: Are they needed?;;;","09/Aug/12 00:57;jjkoshy;Thanks for the review.

1. Will do.

2. Do you remember which class? I can run an optimize imports everywhere, so
   there may be some noise in the final patch.

3. Oops - yes that's true. I'll fix this to accept a factory method that
   creates the object if absent.

4. Yes - I'll combine them.

5. I had considered this, but I think it is better to keep only generic    stuff in the metrics package and keep concrete metrics close to where    they are used. I think we can decide this as part of KAFKA-203 since that
   may result in a more elaborate wrapper around metrics-core than I have
   now in the kafka.metrics package.

6/7. The issue is that produce/fetch request stats are asymmetric. E.g.,   per-key expiration stats do not make sense for DelayedFetch requests, but   they do make sense for DelayedProduce requests. Also, the caught-up fetch
   request rps and duration stats for the producer are updated on a per-key
   basis so that's why they are named as such - i.e., it does not exactly   equal the satisfaction time. I can add an additional stat that does track
   satisfaction time for completely satisfied produce requests. There is
   *some* overlap with BrokerTopicStat but as I mentioned in (7) in my first
   comment these stats are only account for DelayedRequests. As for topic
   level stats, throughput stats would help, but are satisfaction/expiration
   stats (especially with multi-produce) useful since those events occur at
   the key(partition)-level?

8. Yes - will revert. This was done before Neha added that option in
   KAFKA-350.

9. Not required - I used it for testing only. Will revert.
;;;","09/Aug/12 01:00;jjkoshy;Also, re: my comment concerning the dependency on KAFKA-376: this is not strictly required - KAFKA-350 actually made a similar temporary fix to use the log end offset instead of hw.;;;","13/Aug/12 17:30;jjkoshy;
The jira system was down around the time of my last comment and it was not
sent to the mailing list. That further explains some of these updates.

1 - Added produce satisfied stats.
2 - Common request key for fetch/produce.
3 - Factory method for getAndMaybePut.
4 - Added doc describing fudged name for KafkaMetricsGroup.
5 - Fixed the issues with the JMX operations I had earlier.
6 - Reverted the ProducerPerformance and temporary config changes.

I decided against doing the optimization for the FetchRequestPurgatory
checkSatisfied as I don't think it makes a significant difference, as the
available bytes computation is pretty much in memory.

One more comment on the usage of metrics: it is possible to combine
rate/histogram metrics into a metrics Timer object. I chose not to do this
because the code turns out a little cleaner by using a meter/histogram, but
will revisit later.
;;;","14/Aug/12 16:54;jjkoshy;Just want to add a comment on metrics overheads of using histograms, meters
and gauges.

Meters use an exponential weighted moving average and don't need to maintain
past data.  Histograms are implemented using reservoir sampling and need to
maintain a sampling array. There is only one per-key histogram - the
""follower catch-up time"" histogram in DelayedProducerRequestMetrics. There
are four more global histograms.

The sampling array is 1028 atomic longs by default. So if we have say, 500
topics with four partitions each, four brokers, assume that leadership is
evenly spread out, a *lower* bound (if we ignore object overheads and the
global histograms) on memory would be ~ 4MB per broker.

Also, after looking at the metrics code a little more, I think we should use
the exponentially decaying sampling option. By default, the histogram uses a
uniform sample - i.e., it effectively takes a uniform sample from all the
seen data points. Exponential decaying sampling gives more weight to the
past five minutes of data - but it would use at least double the memory of
uniform sampling which pushes memory usage to over 8MB.
;;;","14/Aug/12 17:59;nehanarkhede;Patch v2 doesn't apply cleanly on a fresh checkout of 0.8 -

nnarkhed-ld:kafka-385 nnarkhed$ patch -p0 -i ~/Projects/kafka-patches/KAFKA-385-v2.patch 
patching file core/src/main/scala/kafka/Kafka.scala
patching file core/src/main/scala/kafka/api/FetchResponse.scala
patching file core/src/main/scala/kafka/metrics/KafkaMetrics.scala
patching file core/src/main/scala/kafka/metrics/KafkaMetricsConfigShared.scala
patching file core/src/main/scala/kafka/metrics/KafkaMetricsGroup.scala
patching file core/src/main/scala/kafka/server/KafkaApis.scala
Hunk #4 FAILED at 160.
Hunk #12 FAILED at 360.
2 out of 24 hunks FAILED -- saving rejects to file core/src/main/scala/kafka/server/KafkaApis.scala.rej
patching file core/src/main/scala/kafka/server/KafkaConfig.scala
Hunk #1 succeeded at 24 with fuzz 2 (offset 2 lines).
Hunk #2 succeeded at 36 with fuzz 1 (offset 2 lines).
Hunk #3 succeeded at 139 (offset 2 lines).
patching file core/src/main/scala/kafka/server/RequestPurgatory.scala
patching file core/src/main/scala/kafka/utils/Pool.scala
patching file core/src/main/scala/kafka/utils/Utils.scala
Hunk #1 succeeded at 502 (offset 1 line).
patching file core/src/test/scala/unit/kafka/integration/LogCorruptionTest.scala
patching file core/src/test/scala/unit/kafka/integration/TopicMetadataTest.scala
patching file core/src/test/scala/unit/kafka/server/RequestPurgatoryTest.scala
patching file project/build/KafkaProject.scala
;;;","14/Aug/12 18:05;jjkoshy;Yes - looks like KAFKA-369 went in yesterday. I will need to rebase now.;;;","14/Aug/12 22:46;jkreps;1. Can we make the reporters pluggable? We shouldn't hard code those, you should just give something like
  metrics.reporters=com.xyz.MyReporter, com.xyz.YourReporter
2. Please remove the reference to scala class names from the logging (e.g. DelayedProduce)
3. How are you measuring the performance impact of the change you made to synchronization?
4. It would be good to cut-and-paste the scala timer class they provide in the scala wrapper. That is really nice.
5. Size.
I think the overhead is the following:
8 byte pointer to the value
12 byte object header
8 byte value
Total: 28 bytes

This is too much memory for something that should just be monitoring. I think we should not do per-key histograms.

;;;","14/Aug/12 23:52;jjkoshy;I'm about to upload a rebased patch so I'd like to incorporate as much of this as I can.

> 1. Can we make the reporters pluggable? We shouldn't hard code those, you should just give something like 
  metrics.reporters=com.xyz.MyReporter, com.xyz.YourReporter 

These (and ConsoleReporter) are the only reporters currently available. It would be good to make it pluggable, but their constructors are different. i.e., I don't think it is possible to do this without using a spring-like framework.

> 2. Please remove the reference to scala class names from the logging (e.g. DelayedProduce)

I don't follow this comment - can you clarify? Which file are you referring to?

> 3. How are you measuring the performance impact of the change you made to synchronization?

I did not - I can measure the satisfaction time with and without the synchronization but I doubt it would add much overhead. Also, we have to add synchronization one way or the other - either inside the purgatory or outside (i.e,. burden the client's usage).

> 4. It would be good to cut-and-paste the scala timer class they provide in the scala wrapper. That is really nice. 

They == ? Not clear on this - can you clarify?

> 5. Size. 
I think the overhead is the following: 
8 byte pointer to the value 
12 byte object header 
8 byte value 
Total: 28 bytes 

> This is too much memory for something that should just be monitoring. I think we should not do per-key histograms. 

This is certainly a concern. So with the scenario I have in my previous comment, this would be > 13MB per broker and > double that if I use exponentially decaying sampling. That said, there is only one per-key histogram (which is the follower catch up time). OTOH since the main use I can think of is to see which followers are slower we can achieve that with grep's in the log. So I guess the visual benefit comes at a prohibitive memory cost.
;;;","15/Aug/12 19:14;jjkoshy;Changes over v2:
- Rebased (twice!)
- For the remaining (global) histograms switched to biased histograms.
- Addressed Jay's comments:
  - 1: actually there's a workaround - basically pass through the properties
    to the custom reporter. (I provided an example
    (KafkaCSVMetricsReporter). If JMX operations need to be exposed the
    custom reporter will need to implement an mbean trait that extends from
    KafkaMetricsReporterMBean. I did this to avoid having to implement the
    DynamicMBean interface. Since we now have pluggable reporters I removed
    the KafkaMetrics class and the dependency on metrics-ganglia and
    metrics-graphite.
  - 2: changed the logging statements in KafkaApis to just say producer
    requests/fetch requests.
  - 3: I did a quick test as described above, but couldn't see any
    measurable impact.
  - 4: Added KafkaTimer and a unit test (which I'm thinking of removing as
    it is pretty dumb other than showing how to use it).
  - 5: Got rid of the per-key follower catch up time histogram from
    DelayedProduceMetrics.  Furthemore, meters are inexpensive and the
    per-key caught up follower request meter should be sufficient.
;;;","15/Aug/12 20:01;jkreps;+1;;;","15/Aug/12 20:04;jjkoshy;I forgot to update the running flag in the example reporter - will fix that before check-in.;;;","16/Aug/12 01:44;jjkoshy;Jun had brought up one more problem - the factory method to getAndMaybePut doesn't really
fix the problem of avoiding instantiation of objects if they are already present in the Pool since the
anonymous function that I use needs to instantiate the object. I tweaked the code to use lazy vals
and used logging to verify that each object in the Pool is instantiated only once.

From what I understand, it seems lazy val's implementation effectively uses a synchronized bitmap
to keep track of whether a particular val has been created or not. However, I'm not so sure how it works
if the val involves a parameter. e.g., lazy val factory = new MyClass(param) as opposed to
lazy val factory = new MyClass The concern is that scala may need to create some internal wrapper
classes (at runtime). I tried disassembling the bytecode but did not want to spend too much time on
it - so I thought I'd ask if anyone know details of how lazy vals work when the actual instance is only
known at runtime?
;;;","16/Aug/12 01:44;jjkoshy;Attached an incremental patch over v3 to illustrate.;;;","16/Aug/12 04:18;junrao;For the issue with getAndMaybePut(), can we add an optional createValue() method to the constructor of Pool? If an object doesn't exist for a key, Pool can call createValue() to create a new value object from key.

A few other comments:
30. KafkaConfig: brokerid should be a required property. So we shouldn't put a default value.

31. satisfiedRequestMeter: Currently, it doesn't include the time for expired requests. I prefer to have a stat that gives the time for all requests, whether expired or not. 

32. I am not sure how useful the metric CaughtUpFollowerFetchRequestsPerSecond is.;;;","16/Aug/12 06:18;jjkoshy;That's a good suggestion, and should work as well. I'll make that change tomorrow. It would be good to understand the lazy val implementation as well - will try and dig into some toy examples.

30. Correct - I had reverted that in the last attachment. There was a (temporary I think) reason I needed that default but I don't remember.

For 31 and 32 I think we should defer this to a later discussion. I actually had expiration time before but removed it. I'm not sure it makes a lot of sense. It would perhaps be useful to detect producers that are setting a very low expiration period, but even so it is driven by producer configs and would be a mash-up of values from different producers with expiring requests. Stats can be asymmetric between delayed-produce/delayed-fetch and also between expired/satisfied.
;;;","16/Aug/12 18:05;jjkoshy;Moved the valueFactory to Pool's constructor.
Unit tests/system tests pass.;;;","16/Aug/12 18:20;junrao;+1 on patch v4.;;;","16/Aug/12 21:28;jjkoshy;Thanks for the reviews. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix intermittent test failures and remove unnecessary sleeps,KAFKA-384,12596466,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,jjkoshy,jjkoshy,29/Jun/12 22:29,31/Jul/12 23:28,14/Jul/23 05:39,31/Jul/12 23:28,0.8.0,,,,,,,,,,,,,0,,,,"Seeing intermittent failures in 0.8 unit tests. Also, many sleeps can be removed (with producer acks in place) and I think MockTime isn't used in some places where it should.
",,jjkoshy,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/12 19:24;nehanarkhede;kafka-384-v1.patch;https://issues.apache.org/jira/secure/attachment/12538287/kafka-384-v1.patch","30/Jul/12 20:39;nehanarkhede;kafka-384-v2.patch;https://issues.apache.org/jira/secure/attachment/12538408/kafka-384-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,248177,,,Tue Jul 31 23:28:30 UTC 2012,,,,,,,,,,"0|i09m1b:",54003,,,,,,,,,,,,,,,,,,,,"29/Jul/12 19:24;nehanarkhede;This patch removes sleep statements from the unit tests. Changes include -

1. All sleep statements except those related to a scheduler are removed. So 2-3 sleep statements that exercise the producer side queue expiration logic have not been removed
2. For the Log tests, passed in a Time parameter to the Log. Kafka server already takes in an optional Time parameter that defaults to SystemTime. However, it wasn't passed around in LogManager. Fixed it so KafkaServer passes its Time
 variable to LogManager which passes it to Log.
This is useful in removing all sleep statements from unit tests for the log manager and logs;;;","30/Jul/12 20:12;junrao;Thanks for the patch. It looks good. Just 1 comment:

1. AsyncProducerTest.testQueueTimeExpired(): This is an existing issue, but probably can be fixed in this patch too. It seems that we should do     producerSendThread.shutdown after EasyMock.verify(mockHandler). Shutdown always sends all remaining messages in the buffer. If we call shutdown before verify, it's not clear if the send was triggered by timeout or shutdown.;;;","30/Jul/12 20:39;nehanarkhede;Thanks for the review, Jun. That is a good point. I fixed that and also removed the reference to mockTime since that is not useful here.;;;","30/Jul/12 20:44;jjkoshy;+1 for v2.;;;","30/Jul/12 21:42;jkreps;You are my hero...;;;","30/Jul/12 21:57;junrao;+1 on v2 too.;;;","31/Jul/12 18:19;nehanarkhede;KAFKA-343 checkin broke some unit tests and cause others to hang. I think I might have to hold off on the checkin until that is either fixed or reverted. ;;;","31/Jul/12 23:28;nehanarkhede;Thanks all for the review! Committed the v2 patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TopicCount.constructTopicCount isn't thread-safe,KAFKA-379,12596325,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,baroquebobcat,baroquebobcat,28/Jun/12 21:01,18/Sep/12 05:06,14/Jul/23 05:39,18/Sep/12 05:06,0.7,0.8.0,,0.7,0.8.0,,,,,,clients,,,1,bugs,,,"TopicCount uses scala.util.parsing.json.JSON, which isn't thread-safe https://issues.scala-lang.org/browse/SI-4929

If you have multiple consumers within the same JVM, and they all rebalance at the same time, you can get errors like the following:

[...] kafka.consumer.TopicCount$.constructTopicCount:39] ERROR: error parsing consumer json string [...]
java.lang.NullPointerException
        at scala.util.parsing.combinator.Parsers$NoSuccess.<init>(Parsers.scala:131)
        at scala.util.parsing.combinator.Parsers$Failure.<init>(Parsers.scala:158)
        at scala.util.parsing.combinator.Parsers$$anonfun$acceptIf$1.apply(Parsers.scala:489)
        ...
        at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:742)
        at scala.util.parsing.json.JSON$.parseRaw(JSON.scala:71)
        at scala.util.parsing.json.JSON$.parseFull(JSON.scala:85)
        at kafka.consumer.TopicCount$.constructTopicCount(TopicCount.scala:32)
        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$getTopicCount(ZookeeperConsumerConnector.scala:422)
        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.kafka$consumer$ZookeeperConsumerConnector$ZKRebalancerListener$$rebalance(ZookeeperConsumerConnector.scala:460)
        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener$$anonfun$syncedRebalance$1.apply$mcVI$sp(ZookeeperConsumerConnector.scala:437)
        at scala.collection.immutable.Range$ByOne$class.foreach$mVc$sp(Range.scala:282)
        at scala.collection.immutable.Range$$anon$2.foreach$mVc$sp(Range.scala:265)
        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.syncedRebalance(ZookeeperConsumerConnector.scala:433)
        at kafka.consumer.ZookeeperConsumerConnector$ZKRebalancerListener.handleChildChange(ZookeeperConsumerConnector.scala:375)
        at org.I0Itec.zkclient.ZkClient$7.run(ZkClient.java:568)
        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)

I ran into this on 0.7.0, but the code in trunk appears to be vulnerable to the same issue.",,baroquebobcat,jjkoshy,junrao,nehanarkhede,velvia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/12 21:26;baroquebobcat;TopicCount.scala.diff;https://issues.apache.org/jira/secure/attachment/12533891/TopicCount.scala.diff","01/Aug/12 17:38;junrao;kafka-379_0.8_v1.patch;https://issues.apache.org/jira/secure/attachment/12538804/kafka-379_0.8_v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,292177,,,Tue Sep 18 05:06:08 UTC 2012,,,,,,,,,,"0|i0rs9r:",160224,,,,,,,,,,,,,,,,,,,,"28/Jun/12 21:26;baroquebobcat;I patched our fork like this.

The patch takes the parsing code from scala.util.parsing.json.JSON and puts it in a class instead of an object, so it's instantiable.;;;","29/Jun/12 00:45;jjkoshy;Thanks for reporting this - and the SI reference.

This likely affects 0.8 in other places as well:
git grep parseFull
core/src/main/scala/kafka/consumer/TopicCount.scala:        JSON.parseFull(topicCountString) match {
core/src/main/scala/kafka/server/StateChangeCommand.scala:      JSON.parseFull(requestJson) match {
core/src/main/scala/kafka/utils/ZkUtils.scala:        JSON.parseFull(jsonPartitionMap) match {;;;","05/Jul/12 16:29;junrao;Thanks for the patch. In 0.7, the only usage of JSON is in TopicCount and all usages are called from ZookeeperConsumerConnector.rebalance, which is synchronized. So, it seems that we just need to create a new JSONParser instance per ZookeeperConsumerConnctor instance. Creating 1 JSONParser instance each time a parser is needed may be too expensive.;;;","01/Aug/12 17:38;junrao;Attach patch v1 in 0.8. Just created a synchronized singleton that wraps scala JSON. Since json parsing is used rarely in Kafka, this is likely not a performance concern.;;;","08/Aug/12 19:09;jjkoshy;+1;;;","08/Aug/12 19:14;nehanarkhede;+1. Looks good.;;;","08/Aug/12 19:49;jjkoshy;Forgot to add - can we get this patched on trunk as well?;;;","23/Aug/12 17:16;junrao;Thanks for the review. Rebased and committed to 0.8. Will port to 0.7.;;;","06/Sep/12 20:52;velvia;Do you guys want a patch for 0.6?   I just created one and can attach it.   I know, we should really upgrade to 0.7.;;;","06/Sep/12 22:45;junrao;Evan,

The 0.6 release is pre Apache. So, we won't be able to patch it. Sorry.;;;","18/Sep/12 05:06;junrao;Patched and committed to trunk too.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mirrors but must never be used for verification,KAFKA-378,12596255,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,charmalloc,charmalloc,28/Jun/12 12:49,28/Jun/12 16:14,14/Jul/23 05:39,28/Jun/12 16:14,,,,0.7.1,,,,,,,,,,0,,,,"http://incubator.apache.org/guides/releasemanagement.html#understanding-distribution

Mirrored copies of checksums, KEYS and signature files (.asc and .md5 files) will be present on the mirrors but must never be used for verification. So, all links from the podling website to signatures, sums and KEYS need to refer to the original documents on www.apache.org

",,charmalloc,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/12 13:11;charmalloc;KAFKA-378.patch;https://issues.apache.org/jira/secure/attachment/12533827/KAFKA-378.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299101,,,Thu Jun 28 16:14:20 UTC 2012,,,,,,,,,,"0|i15zlr:",243068,,,,,,,,,,,,,,,,,,,,"28/Jun/12 13:11;charmalloc;patch pointing to the original asc key and md5 for verification;;;","28/Jun/12 14:48;junrao;+1;;;","28/Jun/12 16:14;charmalloc;committed and updated svn on site box;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
some of the dist mirrors are giving a 403,KAFKA-377,12596209,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,charmalloc,charmalloc,charmalloc,28/Jun/12 04:30,19/Sep/12 16:40,14/Jul/23 05:39,19/Sep/12 16:39,0.7.1,,,0.7.2,,,,,,,,,,0,,,,"https://issues.apache.org/jira/browse/INFRA-4975

will make patch to change the links to by pass this",,cburroughs,charmalloc,jkreps,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/12 04:31;charmalloc;KAFKA-377.patch;https://issues.apache.org/jira/secure/attachment/12533776/KAFKA-377.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299100,,,Tue Aug 14 20:26:16 UTC 2012,,,,,,,,,,"0|i15zlj:",243067,,,,,,,,,,,,,,,,,,,,"28/Jun/12 14:38;junrao;+1;;;","28/Jun/12 20:07;charmalloc;have an update here, don't think we need to patch so holding off the commit

""Somehow new incubator directories get bad permissions on the rsync servers eos, aurora. 
The perms on mino are ok ; some stupid bug we can't track down. The perms were corrected 
on eos & aurora. 

Some mirrors rsync with bad options ; missing '-p' (preserve permissions), so the initial bad 
permissions aren't corrected on the mirror. ""

I mailed the mirror in the example (reverse.net). ""

https://issues.apache.org/jira/browse/INFRA-4975?focusedCommentId=13403334&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13403334;;;","18/Jul/12 19:59;cburroughs;INFRA-4975 is closed now, we good here?;;;","19/Jul/12 01:18;charmalloc;nope, i found a few mirrors still bad will re-open the ticket in INFRA

http://newverhost.com/pub/incubator/kafka 
http://mirror.symnds.com/software/Apache/incubator/kafka 
http://apache.osuosl.org/incubator/kafka
http://ftp.wayne.edu/apache/incubator/kafka 
http://mirror.atlanticmetro.net/apache/incubator/kafka
http://mirror.nyi.net/apache/incubator/kafka
http://apache.mesi.com.ar/incubator/kafka 
http://apache.deathculture.net/incubator/kafka
http://mirror.candidhosting.com/pub/apache/incubator/kafka 
http://apache.ziply.com/incubator/kafka
http://mirrors.axint.net/apache/incubator/kafka
http://apache.mirrors.tds.net/incubator/kafka;;;","04/Aug/12 02:29;cburroughs;I clicked through everything at https://www.apache.org/dyn/closer.cgi/incubator/kafka/kafka-0.7.1-incubating/, got one (presumably transient) timeout and no 403s.  I think we are good here.;;;","14/Aug/12 20:26;jkreps;Can we close this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
expose different data to fetch requests from the follower replicas and consumer clients,KAFKA-376,12596003,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,prashanth.menon,junrao,junrao,26/Jun/12 23:40,10/Sep/12 15:29,14/Jul/23 05:39,10/Sep/12 13:29,0.8.0,,,0.8.0,,,,,,,core,,,0,bugs,,,"Currently, the broker always uses highwatermark to calculate the available bytes to a fetch request, no matter where the request is from. Instead, we should use highwatermark for requests coming from real consumer clients and use logendoffset for requests coming from follower replicas.",,jjkoshy,jkreps,junrao,prashanth.menon,,,,,,,,,,,,,,,,,,,,,,,,,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/12 01:59;prashanth.menon;KAFKA-376-DRAFT.patch;https://issues.apache.org/jira/secure/attachment/12538294/KAFKA-376-DRAFT.patch","12/Aug/12 22:16;prashanth.menon;KAFKA-376-v1.patch;https://issues.apache.org/jira/secure/attachment/12540588/KAFKA-376-v1.patch","19/Aug/12 21:22;prashanth.menon;KAFKA-376-v2.patch;https://issues.apache.org/jira/secure/attachment/12541535/KAFKA-376-v2.patch","27/Aug/12 02:47;prashanth.menon;KAFKA-376-v3.patch;https://issues.apache.org/jira/secure/attachment/12542568/KAFKA-376-v3.patch","30/Aug/12 02:35;prashanth.menon;KAFKA-376-v4.patch;https://issues.apache.org/jira/secure/attachment/12543026/KAFKA-376-v4.patch","02/Sep/12 17:59;prashanth.menon;KAFKA-376-v5.patch;https://issues.apache.org/jira/secure/attachment/12543494/KAFKA-376-v5.patch","09/Sep/12 20:33;prashanth.menon;KAFKA-376-v6.patch;https://issues.apache.org/jira/secure/attachment/12544416/KAFKA-376-v6.patch","10/Sep/12 01:45;prashanth.menon;KAFKA-376-v7.patch;https://issues.apache.org/jira/secure/attachment/12544424/KAFKA-376-v7.patch",,,,,,,,,,,,,,,,,,,,8.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299099,,,Mon Sep 10 13:29:58 UTC 2012,,,,,,,,,,"0|i15zlb:",243066,,,,,,,,,,,,,,,,,,,,"07/Jul/12 12:57;prashanth.menon;I can take a look at this, probably this weekend.;;;","18/Jul/12 23:48;jkreps;Hey Prashanth, how goes it? Let me know if you don't have time and I can pick this up.;;;","19/Jul/12 14:10;prashanth.menon;I actually didn't get a chance to look at this, though I'd really like to.  Tell you what, I'll try to start this weekend and if I'm unable to, you can take it off my hands :);;;","19/Jul/12 14:56;jkreps;Awesome, sounds great! No pressure, just didn't want it to fall through the cracks. :-);;;","23/Jul/12 17:24;prashanth.menon;Minor update: Started some of the work involved, didn't get a chance to put as much time to it as I'd like.  Hoping to get a patch out by mid-end of this week.;;;","30/Jul/12 01:59;prashanth.menon;Hi all, I feel like I should add a draft patch because the code has grown much more complex since the last time I touched it.  The patch itself is minimal but a little nasty, so I'd like to clean it up a little but I figured I'd submit a draft so more eyes can get on it.  Here goes ... an outline:

1. Modified KafkaApis.availableFetchBytes to check the leader replicas highwaterMark if the request is coming from a follower, otherwise use the logEndOffset.  
2. Modified readMessageSet to not read beyond the highwaterMark of the replica's log (which is local because it's the leader) if the request is coming from a regular, non-follower, consumer.

One thing I'm considering doing is putting the log read logic in the replica itself since it is aware of hw and leo.  There's also a ReplicaManager.getLeaderReplica that throws an exception if the leader replica doesn't exist but returns an Option[Replica] - can this just return a Replica?  

As for the test, I couldn't find a decent place to put it because it includes both replication and simple consumer tests.  Rather than split the tests into two separate classes, I thought it'd be better to group them together.

Looking forward to the suggestions.;;;","30/Jul/12 17:22;junrao;Prashanth,

Thanks for the patch. Some comments:

1. In KafkaApis, currently we allow fetch requests on follower replicas, in addition to the leader replica. This is potentially useful for debugging purpose and it would be good to keep this. For normal consumer clients, the fetch requests will still be routed to the leader. However, for tools like ConsoleConsumer, we can potentially consume from any replica.

2. SimpleFetchTest:
2.1 Could you add the Apache header and remove the author comment?
2.2 Since you mocked ReplicaManager, could we do the test with just 1 replica?
2.3 Not sure if bigFetch tests anything more than goodFetch in testNonReplicaSeesHwWhenFetching(). In addition to goodFetch, we should add a fetchRequest with an offset at HW and expect an empty MessageSet in the response.
2.4 In testNonReplicaSeesHwWhenFetching(), the second comment of "" // there should be no response"" is incorrect.
2.5 For verification, it's probably simpler if we just verify that all log.read requests made are correct.

Finally, we probably should have kafka-434 committed first since it's a big patch and has been rebased quite a few times. It's almost there and should be committed this week.;;;","01/Aug/12 00:41;prashanth.menon;Thanks for the comments, Jun.

1. Do we currently support this?  The fetch requests always check that the leader is on the broker handling the request before actually reading from the log, so I'm not sure how a read on a follower succeeds?

2.1, 2.2, 2.3, 2.4, 2.5 All valid.  Will upload new patch later this week.

I assume you mean, KAFKA-343?  I've been following it, and yes, it's grown quite large/complex hahaha.  I'll definitely wait for that to get ironed out and committed.;;;","01/Aug/12 15:24;junrao;For 1, yes, you are right. We actually ensure that fetch requests can only be made on the leader right now. So, we can relax it in a separate jira.

Yes, I meant 343.;;;","12/Aug/12 22:16;prashanth.menon;Sigh, I took way too long to take a look at this again.  Regardless, I've attached a new patch that addresses the points.  I'm still a little unhappy with the logic to determine offsets - though I believe it's functionality correct, something about it seems off.  I'll mull it over while comments come in, hopefully someone points something out.

;;;","14/Aug/12 01:31;jjkoshy;Hi Prashanth, the patch didn't apply cleanly for me. Can you svn up? ;;;","14/Aug/12 15:19;junrao;Thanks for patch v1. It looks good overall. Some comments:

20. KafkaApis.availableFetchBytes: getting the leo of a replica can be done as leader.logEndOffset.

21. Could you rebase and make sure that single_host_multi_brokers under system_test passes?;;;","19/Aug/12 21:22;prashanth.menon;Hi all,

Find a second patch that addresses 20 and 21 (and some line spacing cleanup).  It passes all the tests except for BackwardsCompatibilityTest; that test uses a pre-constructed log-data file that's loaded up when the Kafka server starts.  The test then issues an offset request and pulls all the data back and expected 100 messages total.  There are three issues here:

1) It looks like we need to handle the case where a topic/partition has a replication factor of one.  In such cases, updating the LEO of the log should also update the HW to the same value.  I took care of this in the patch, but I'd like some feedback on if it's the correct approach.
2) What should the server do on startup if it is a leader of a specific topic/partition and finds local data available for it.  Presumable, it should truncate it's log to the value in the checkpoint file, but it doesn't seem to be done now.  Is this thinking correct? 
3) KafkaApis should use the leader's HW to perform a bounds check when responding to an offset request.  It currently checks for leader presence, adding a check in should be relatively simple.  I can attach another patch with that (and any other issues that come up) mid-week.  ;;;","20/Aug/12 15:31;junrao;Prashanth, thanks for the updates. 

1) We have a separate jira kafka-420 to track the issue of maintaining HW with just 1 replica. It probably needs a bit more work than what's in your patch. First of all, we should increase HW as long as ISR (not assigned replica) has only 1 replica. Second, we need to check if we can increase HW in at least two other places (a) when a replica becomes the leader, (b) when ISR shrinks to size 1.

2) A new leader should never truncate its log. Only a follower truncates its log. This is because the new leader is guaranteed to have all committed data in its log and truncating any data from its log may risk losing a committed message.

3) Yes, we should fix the getOffset api. We will need to distinguish requests from a follower and a regular client, so that we can return either LEO or HW accordingly. We can add a replicaId to the getOffsetRequest like FetchRequest.

4) There are a couple other jiras that complicate things a bit. (a) In kafka-461, we are trying to get rid of the support of magic byte 0 in Message, which will end up remove BackwardsCompatibilityTest. However, if BackwardsCompatibilityTest is the only test that fails for this jira, that means we don't have enough unit test coverage for this jira and kafka-420. So, we will need to add a new unit test. (b) kafka-351 is doing some refactoring of ReplicaManager, which will change a bit how KafkaApis interacts with ReplicaManager.

Ideally, we probably should fix things in the following order. (a) get kafka-351 (I hope to check in mid this week) in first so that we can use the cleaner api in ReplicaManager; (b) fix kafka-420 and add a new unit test; (c) fix this jira and patch OffsetRequest. Does this make sense to you? We can probably have someone else work on kafka-420 if you don't have the time. ;;;","25/Aug/12 00:50;junrao;Prashanth,

kafka-461, kafka-351 and kafka-420 have all been committed to 0.8. This jira should be unblocked now. Could you rebase your patch?;;;","27/Aug/12 02:47;prashanth.menon;Here we go, I've attached v3 of the patch.  It's effectively the same, but with some adjustments to compile against latest trunk.  Let me know what you guys think.

Minor edit, thanks for taking care of the other JIRA's.  The last few weeks have been fairly busy, but the good news is that I see a break which I hope to use to get back into the swing of things :)  As for the offsets API, can we take care of that in a separate JIRA?;;;","27/Aug/12 15:22;junrao;Thanks for patch v3. +1 on the patch. The following are some minor comments. Once they are addressed, the patch can be checked in without another review.

30. KafkaApis:
30.1 availableFetchBytes(): Not all exceptions are no leader exceptions. So, we should log the exception and change the error message to be more general.
30.2 readMessageSets(fetchRequest): The comment about replica id value of -1 can be removed. There is no need to pass in brokerId for the first replicaManager.getReplica call since it defaults to local replica. There is no need to make the second replicaManager.getReplica call for the remote replica since topic and partitionId are available locally. 
30.3 readMessageSet(): Leader's log should always to be available. If not, we should log an error in addition to returning an empty set.

31. SimpleFetchTest: There is no need to mock KafkaZooKeeper any more.

Also, could you create another jira to fix the getOffset api?;;;","29/Aug/12 14:01;prashanth.menon;Thanks for the review Jun.  30 and 31 are all silly mistakes by me.  I'll attach a patch later today to address them.;;;","30/Aug/12 02:31;prashanth.menon;Hi all,

So I've attached a new patch that addresses all the points except the second point in 30.2 .  It looks like using EasyMock to mock methods with optional arguments is tricky; the call to ReplicaManager.getReplica is causing issues because the brokerId is optional.  I suspect the Scala compiler produces several versions of the method that are chained together at runtime, but which EasyMock can't resolve for some reason.  I've tried some trickery to no avail.

It's a bad idea to modify code to accomadate faulty test/tools, but I figured I'd attach a patch for review while I check out other options.  Let me know what you think,;;;","30/Aug/12 15:25;junrao;Thanks for patch v4. The EasyMock issue is interesting. Maybe you have to mock getReplica(topic, partition) directly. 

Another thought. The only reason that we call getReplica in KafkaApis.readMessageSets(fetchRequest) is to get the highWatermark. We can change readMessageSet(topic: String, partition: Int, offset: Long, maxSize: Int, fromFollower: Boolean) to return Either[Short, (MessageSet, highWatermark]) instead. This way, we can avoid calling getReplica in the first readMessageSets(). In general, the fewer times that we call getReplica the better since there are few places for error handling.;;;","30/Aug/12 15:37;junrao;Also, in availableFetchBytes(), we should probably distinguish UnknownTopicOrPartitionException from other exceptions. For the former, we just need to add an info level log. For the latter, we need to add an error level log with the stacktrace.;;;","02/Sep/12 17:59;prashanth.menon;Thanks for taking a look, Jun.  I've attached a new patch that ddresses the following items:
- readMessageSet now returns Either[Short, (MessageSet, Long)] where, on success, we return both the messages and the highwatermark of the leader.
- availableFetchBytes now logs info when it gets an UnknownTopicOrPartitionException exception, and logs an error for any other exceptions.

I tried to fiddle with EasyMock to get around the optional argument issue, but had no luck with it.  Seems like Mockito get's around it by mocking out all possible invocations permuting the parameters which is a little nasty.

Let me know what you think.;;;","04/Sep/12 14:55;junrao;Thanks for patch v5, Prashanth. +1 on the patch.;;;","04/Sep/12 23:36;junrao;Actually, there is one other issue:

In readMessageSet(topic: String, partition: Int, offset: Long, maxSize: Int, fromFollower: Boolean), could we make sure that actualSize is always >=0 ?;;;","09/Sep/12 20:32;prashanth.menon;Good catch, Jun.  V6 patch attached; if all is good, I'll go ahead and commit to the 0.8 branch.;;;","10/Sep/12 00:04;junrao;Thanks for patch v6. We probably should do the length check inside log.read() , instead of readMessageSet. This way, we make sure that the caller's offset is checked by Log.findRange and an OffsetOutOfRangeException can be thrown if needed. In log.read(), after Log.findRange, we can check if length is <=0  and if so, immediately return an empty set.;;;","10/Sep/12 01:38;prashanth.menon;Argh, should have run the test suite before submitting the patch.  I've attached a new one that moves the logic into Log.read().;;;","10/Sep/12 04:07;junrao;Thanks for patch v7. +1 Please commit to 0.8.;;;","10/Sep/12 13:29;prashanth.menon;Thanks Jun.  Comitted to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer doesn't receive all data if there are multiple segment files,KAFKA-372,12595466,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jfung,jfung,21/Jun/12 18:30,26/Jun/12 16:24,14/Jul/23 05:39,26/Jun/12 15:47,0.8.0,,,,,,,,,,core,,,0,,,,"This issue happens inconsistently but could be reproduced by following the steps below (repeat step 4 a few times to reproduce it):

1. Check out 0.8 branch (currently reproducible with rev. 1352634)

2. Apply kafka-306-v4.patch

3. Please note that the log.file.size is set to 10000000 in system_test/broker_failure/config/server_*.properties (small enough to trigger multi segment files)

4. Under the directory <kafka home>/system_test/broker_failure, execute command:
$ bin/run-test.sh 20 0

5. After the test is completed, the result will probably look like the following:

========================================================
no. of messages published            : 14000
producer unique msg rec'd            : 14000
source consumer msg rec'd            : 7271
source consumer unique msg rec'd     : 7271
mirror consumer msg rec'd            : 6960
mirror consumer unique msg rec'd     : 6960
total source/mirror duplicate msg    : 0
source/mirror uniq msg count diff    : 311
========================================================

6. By checking the kafka log files, the sum of the sizes of the source cluster segments files are equal to those in the target cluster.

[/tmp] $  find kafka* -name *.kafka -ls

18620155 9860 -rw-r--r--   1 jfung    eng      10096535 Jun 21 11:09 kafka-source3-logs/test01-0/00000000000000000000.kafka
18620161 9772 -rw-r--r--   1 jfung    eng      10004418 Jun 21 11:11 kafka-source3-logs/test01-0/00000000000020105286.kafka
18620160 9776 -rw-r--r--   1 jfung    eng      10008751 Jun 21 11:10 kafka-source3-logs/test01-0/00000000000010096535.kafka
18620162 4708 -rw-r--r--   1 jfung    eng       4819067 Jun 21 11:11 kafka-source3-logs/test01-0/00000000000030109704.kafka
19406431 9920 -rw-r--r--   1 jfung    eng      10157685 Jun 21 11:10 kafka-target2-logs/test01-0/00000000000010335039.kafka
19406429 10096 -rw-r--r--   1 jfung    eng      10335039 Jun 21 11:09 kafka-target2-logs/test01-0/00000000000000000000.kafka
19406432 10300 -rw-r--r--   1 jfung    eng      10544850 Jun 21 11:11 kafka-target2-logs/test01-0/00000000000020492724.kafka
19406433 3800 -rw-r--r--   1 jfung    eng       3891197 Jun 21 11:12 kafka-target2-logs/test01-0/00000000000031037574.kafka

7. If the log.file.size in target cluster is configured to a very large value such that there is only 1 data file, the result would look like this:

========================================================
no. of messages published            : 14000
producer unique msg rec'd            : 14000
source consumer msg rec'd            : 7302
source consumer unique msg rec'd     : 7302
mirror consumer msg rec'd            : 13750
mirror consumer unique msg rec'd     : 13750
total source/mirror duplicate msg    : 0
source/mirror uniq msg count diff    : -6448
========================================================

8. The log files are like these:

[/tmp] $ find kafka* -name *.kafka -ls

18620160 9840 -rw-r--r--   1 jfung    eng      10075058 Jun 21 11:24 kafka-source2-logs/test01-0/00000000000010083679.kafka
18620155 9848 -rw-r--r--   1 jfung    eng      10083679 Jun 21 11:23 kafka-source2-logs/test01-0/00000000000000000000.kafka
18620162 4484 -rw-r--r--   1 jfung    eng       4589474 Jun 21 11:26 kafka-source2-logs/test01-0/00000000000030269045.kafka
18620161 9876 -rw-r--r--   1 jfung    eng      10110308 Jun 21 11:25 kafka-source2-logs/test01-0/00000000000020158737.kafka
19406429 34048 -rw-r--r--   1 jfung    eng      34858519 Jun 21 11:26 kafka-target3-logs/test01-0/00000000000000000000.kafka
",,jfung,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/12 00:56;junrao;kafka-372_v1.patch;https://issues.apache.org/jira/secure/attachment/12533416/kafka-372_v1.patch","25/Jun/12 04:11;jfung;multi_seg_files_data_loss_debug.patch;https://issues.apache.org/jira/secure/attachment/12533252/multi_seg_files_data_loss_debug.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,248186,,,Tue Jun 26 16:24:14 UTC 2012,,,,,,,,,,"0|i09m4f:",54017,,,,,,,,,,,,,,,,,,,,"25/Jun/12 04:11;jfung;** Uploaded a patch with a simplified scenario to reproduce the data loss in multi segment files. 

** This patch provides a script ""run-test-debug.sh"" to do the following:
1. Start 1 broker
2. Start a modified version of Producer to send 300 messages with user specified message string length (500 chars will reproduce the issue while 50 chars will not). This producer produces messages with sequence ID and send the messages in sequence starting from 1, 2, 3, … Etc.
3. Start ConsoleConsumer to receive data

** To reproduce the issue, under <kafka home>/system_test/broker_failure, execute the following command:

$ bin/run-test-debug.sh 500 (which means each message string is 500 chars long)

The consumer only receives the first 120 messages. (This is verified by checking kafka.tools.DumpLogSegments.
========================================================
no. of messages published            : 300
producer unique msg rec'd            : 300
source consumer msg rec'd            : 120
source consumer unique msg rec'd     : 120
========================================================

The number of segment files are 

$ ls -l /tmp/kafka-source1-logs/test01-0/
-rw-r--r--   1 jfung  wheel  10431 Jun 24 20:59:21 2012 00000000000000000000.kafka
-rw-r--r--   1 jfung  wheel  10440 Jun 24 20:59:22 2012 00000000000000010431.kafka
-rw-r--r--   1 jfung  wheel  10440 Jun 24 20:59:23 2012 00000000000000020871.kafka
-rw-r--r--   1 jfung  wheel  10440 Jun 24 20:59:24 2012 00000000000000031311.kafka
-rw-r--r--   1 jfung  wheel  10441 Jun 24 20:59:26 2012 00000000000000041751.kafka
-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:27 2012 00000000000000052192.kafka
-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:28 2012 00000000000000062652.kafka
-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:29 2012 00000000000000073112.kafka
-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:31 2012 00000000000000083572.kafka
-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:32 2012 00000000000000094032.kafka
-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:33 2012 00000000000000104492.kafka
-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:34 2012 00000000000000114952.kafka
-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:35 2012 00000000000000125412.kafka
-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:37 2012 00000000000000135872.kafka
-rw-r--r--   1 jfung  wheel  10460 Jun 24 20:59:38 2012 00000000000000146332.kafka
-rw-r--r--   1 jfung  wheel      0 Jun 24 20:59:38 2012 00000000000000156792.kafka
-rw-r--r--   1 jfung  wheel      8 Jun 24 21:00:08 2012 highwatermark


** However, if the length of each message string is changed to a lower value 50, the issue won't be showing:

$ bin/run-test-debug.sh 50

The consumer receives all data:
========================================================
no. of messages published            : 300
producer unique msg rec'd            : 300
source consumer msg rec'd            : 300
source consumer unique msg rec'd     : 300
========================================================

The number of segment files are

$  ls -l /tmp/kafka-source1-logs/test01-0
total 64
-rw-r--r--  1 jfung  wheel  10039 Jun 24 20:29:26 2012 00000000000000000000.kafka
-rw-r--r--  1 jfung  wheel  10001 Jun 24 20:29:34 2012 00000000000000010039.kafka
-rw-r--r--  1 jfung  wheel   1752 Jun 24 20:29:36 2012 00000000000000020040.kafka
-rw-r--r--  1 jfung  wheel      8 Jun 24 20:30:06 2012 highwatermark
;;;","26/Jun/12 00:56;junrao;There were several issues that caused the problem.

1. Log.nextAppendOffset() calls flush each time. Since this method is called for every produce request, we force a disk flush for every produce request independent of the flush interval in the broker config. This makes producers very slow.

2. The default value for MaxFetchWaitMs in consumer config is 3 secs, which is too long.

3. The script runs console consumer in background and only waits for 20 secs, which is too short. What we should do is to run console consumer in foreground and wait until it finishes (since it has consumer timeout).

Attach patch v1 that fixes items 1 and 2. The test now passes. However, we should address item 3 in the script too.;;;","26/Jun/12 15:47;jfung;Thanks Jun. It is working correctly after applying kafka-372-v1.patch.;;;","26/Jun/12 16:24;junrao;Thanks John for reviewing the patch. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Creating topic of empty string puts broker in a bad state,KAFKA-371,12595368,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,martinkl,martinkl,21/Jun/12 01:09,14/Aug/12 17:23,14/Jul/23 05:39,14/Aug/12 17:23,0.6,0.7,,0.8.0,,,,,,,,,,0,,,,"Using the Java client library, I accidentally published a message where the topic name was the empty string. This put the broker in a bad state where publishing became impossible, and the following exception was logged 10-20 times per second:

2012-06-21 00:41:30,324 [kafka-processor-3] ERROR kafka.network.Processor  - Closing socket for /127.0.0.1 because of er
ror
kafka.common.InvalidTopicException: topic name can't be empty
        at kafka.log.LogManager.getOrCreateLog(LogManager.scala:165)
        at kafka.server.KafkaRequestHandlers.kafka$server$KafkaRequestHandlers$$handleProducerRequest(KafkaRequestHandle
rs.scala:75)
        at kafka.server.KafkaRequestHandlers.handleProducerRequest(KafkaRequestHandlers.scala:58)
        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:43)
        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$1.apply(KafkaRequestHandlers.scala:43)
        at kafka.network.Processor.handle(SocketServer.scala:289)
        at kafka.network.Processor.read(SocketServer.scala:312)
        at kafka.network.Processor.run(SocketServer.scala:207)
        at java.lang.Thread.run(Thread.java:679)

Restarting Kafka did not help. I had to manually clear out the bad state in Zookeeper to resolve the problem.

The broker should not accept a message that would put it in such a bad state.",,jcreasy,jkreps,martinkl,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/12 01:27;jcreasy;KAFKA-371-0.7.1.patch;https://issues.apache.org/jira/secure/attachment/12536938/KAFKA-371-0.7.1.patch","18/Jul/12 21:30;jkreps;KAFKA-371-0.8-v2.patch;https://issues.apache.org/jira/secure/attachment/12537072/KAFKA-371-0.8-v2.patch","18/Jul/12 01:20;jcreasy;KAFKA-371-0.8.patch;https://issues.apache.org/jira/secure/attachment/12536937/KAFKA-371-0.8.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,248166,,,Thu Jul 19 22:44:41 UTC 2012,,,,,,,,,,"0|i09lxz:",53988,,,,,,,,,,,,,,,,,,,,"18/Jul/12 01:25;jcreasy;Modifed createLog() to fail if it has been asked to create a log that would fail on the getLogPool() function.

I am not sure if the partition check is correct here, it appears that the partition could be any >0 partition and be OK since the file would get created. If there is no where else that partition would need to exist that would be OK, I think that this may not be the case though as it would need to be in Zk if my current understanding is correct. 
;;;","18/Jul/12 01:27;jcreasy;Probably should set fix version to 0.7.1;;;","18/Jul/12 17:17;jkreps;Committed the 0.8 patch. Thanks.

At the moment I don't know if we are doing a 0.7.2 so I am going to hold off on the 0.7.1 patch.;;;","18/Jul/12 17:34;nehanarkhede;I think in the 0.8 patch, the changes to createLog are unnecessary. This is because the same check happens inside getLogPool, which is always called before createLog in getOrCreateLog, no ?;;;","18/Jul/12 19:10;jcreasy;Well, I didn't see any way for the bug to have occurred unless createLog was being called outside of where getLogPool gets called. So, it does appear to be unnecessary, perhaps these checks were added and the ticket was never updated? 

;;;","18/Jul/12 20:26;jkreps;Hmm, good point. This is a little odd, though, no? Why would getting a log pool check the validity of a partition? Not sure how that came about. I recommend we delete that and keep the check in create--if you can't create a bad log you don't need to check on every access. Objections?

Actually I have a bunch of clean-ups I would like to do in LogManager. Let me post a patch with all these while I am in there.;;;","18/Jul/12 20:32;nehanarkhede;Yes, that will work too. I guess the API was added so that every access to the log was guarded with the check. But if we ensure that you can't create one in the first place, we can get rid of the check in getLogPool and keep the one in createLog. ;;;","18/Jul/12 20:40;jkreps;Reopening for cleanups of LogManager;;;","18/Jul/12 21:30;jkreps;Neha can you sanity check this? Here are the changes:

- Remove some unneeded getter getServerConfig
- Rename getTopicIterator to topics()
- Simplify getLogIterator() and rename to allLogs()
- Remove awaitStartup latch and awaitStartup latch acquisition. The comment on startup() says it registers in zookeeper, but this doesn't seem to be true
- Remove getLogPool. createLog checks correctness of creation, and fails if it fails. getLog returns null if the log doesn't exist, as per the contract. getOrCreateLog will fail when createLog fails. I think this is more sensible, and elimantes the getter
- Remove the word ""Map"" from things of type Map
- MS should be Ms by our usual conventions
- Remove helper getLogRetentionMSMap

I think the latch was there because log manager was somehow doing zk registration (according to comments). But I don't see that code there at all now, and logManager shouldn't be talking to zk, so i think it got cleaned up. So now theoretically we shouldn't need that and the weird ordering where we take requests before log manager is initiatialized but then block should not be needed. 

Also, I removed the EasyMock verification on logmanager (I think all it does is check that the config was fetched), which I don't think is useful? But EasyMock is kind of a blackbox to me.;;;","19/Jul/12 19:11;nehanarkhede;+1. 

Minor comment -

Change logCleanupThresholdMS to logCleanupThresholdMs while you're in there;;;","19/Jul/12 20:15;jcreasy;Does it make sense to have a function to ""validate a log"" and call that any time you want to validate that the log you are reading/writing is valid? Potentially some of the validation would be redundant and called at times it didn't need to be, reducing capacity. 

So the balance is between consistent checking and efficient execution.;;;","19/Jul/12 22:34;jkreps;Got it, committed.;;;","19/Jul/12 22:44;jkreps;Hey Jonathan, I don't think so, but I may have misunderstood. Basically a log has two states
 OPEN
 CLOSED

Every time we create a log object we run recovery on it which validates any messages since the last know flush point and truncates any partial writes to put the log in a known state. This is done as part of log construction so there is no way to have a reference to a log until it is known to be valid and ready for writes.

Our policy is that we actually kill the instance of the broker if we see an IOException while writing to disk (STONITH, if you will), so there is effectively no invalid state. The reason for this is that an IO error is effectively the same as a broker failure in that it indicates a potentially partial or corrupt write. You cannot append to the log in this state, so continuing to accept traffic only makes thing worse, the best policy is to die and let the other brokers cover things.

Attempt to read or write to a closed log would be a programming error in the broker, and should just give an exception about the file being closed, so I don't know if we need additional checks there.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Exception ""java.util.NoSuchElementException: None.get"" appears inconsistently in Mirror Maker log.",KAFKA-370,12595316,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,jfung,jfung,20/Jun/12 18:28,22/Jun/12 20:16,14/Jul/23 05:39,22/Jun/12 17:32,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"Exception in Mirror Maker log:
=========================
[2012-06-20 10:56:04,364] DEBUG Getting broker partition info for topic test01 (kafka.producer.BrokerPartitionInfo)
[2012-06-20 10:56:04,365] INFO Fetching metadata for topic test01 (kafka.producer.BrokerPartitionInfo)
[2012-06-20 10:56:04,366] ERROR Error in handling batch of 200 events (kafka.producer.async.ProducerSendThread)
java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:185)
        at scala.None$.get(Option.scala:183)
        at kafka.producer.ProducerPool.getAnyProducer(ProducerPool.scala:76)
        at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:73)
        at kafka.producer.BrokerPartitionInfo.getBrokerPartitionInfo(BrokerPartitionInfo.scala:45)
        at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$getPartitionListForTopic(DefaultEventHandler.scala:129)
        at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:95)
        at kafka.producer.async.DefaultEventHandler$$anonfun$partitionAndCollate$1.apply(DefaultEventHandler.scala:94)
        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
        at scala.collection.immutable.List.foreach(List.scala:45)
        at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:44)
        at scala.collection.mutable.ListBuffer.foreach(ListBuffer.scala:42)
        at kafka.producer.async.DefaultEventHandler.partitionAndCollate(DefaultEventHandler.scala:94)
        at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:65)
        at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:49)
        at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:96)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:82)
        at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:60)
        at scala.collection.immutable.Stream.foreach(Stream.scala:254)
        at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:59)
        at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:37)

Steps to reproduce
=================
It cannot be reproduced consistently. However, running the following script 2 or 3 times (step 2) will show the error:

1. Apply kafka-306-v2.patch to 0.8 branch (revision 1352192 is used to reproduce this)

2. Under the directory <kafka home>/system_test/broker_failure, execute the following command:
=> $ bin/run-test.sh 5 0

3. Check the log under the directory <kafka home>/system_test/broker_failure:
=> $ grep Exception `ls kafka_mirror_maker*.log`
=>    kafka_mirror_maker2.log:java.util.NoSuchElementException: None.get

4. Also the kafka log sizes between source and target will not match:

[/tmp]  $ find kafka* -name *.kafka -ls
19400444 6104 -rw-r--r--   1 jfung    eng       6246655 Jun 20 10:56 kafka-source4-logs/test01-0/00000000000000000000.kafka
19400819 5356 -rw-r--r--   1 jfung    eng       5483627 Jun 20 10:56 kafka-target3-logs/test01-0/00000000000000000000.kafka

Notes about the patch kafka-306-v2.patch
===============================
This patch fix the broker_failure test suite to do the followings:

a. Start 4 kafka brokers as source cluster
b. Start 3 kafka brokers as target cluster
c. Start 3 mirror maker to enable mirroring
d. Send n messages to source cluster
e. No bouncing is performed in this test for simplicity
f. After the producer is stopped, validate the data count is matched between source & target
",,jfung,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/12 16:57;junrao;kafka-370_v1.patch;https://issues.apache.org/jira/secure/attachment/12533071/kafka-370_v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299097,,,Fri Jun 22 20:16:01 UTC 2012,,,,,,,,,,"0|i15zkv:",243064,,,,,,,,,,,,,,,,,,,,"22/Jun/12 16:57;junrao;Attach patch v1. The issue is that the code relies on the broker ids in the hashmap to be always between 0 and size - 1, which is not always true, especially when there are failures.;;;","22/Jun/12 17:17;jjkoshy;+1;;;","22/Jun/12 17:32;junrao;Thanks for the review. Committed to 0.8;;;","22/Jun/12 20:16;jfung;Downloaded rev. 1353005 and tried the test a few times. The issue is fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StringEncoder/StringDecoder use platform default character set,KAFKA-367,12595050,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,initialcontext,jkreps,jkreps,19/Jun/12 05:13,19/Jun/14 05:16,14/Jul/23 05:39,22/Nov/12 22:03,0.8.0,,,0.8.0,,,,,,,,,,0,newbie,,,"StringEncoder and StringDecoder take the platform default character set. This is bad since the messages they produce are sent off that machine. We should
-- add a new required argument to these that adds the character set and default to UTF-8 rather than the machine setting
-- add a commandline parameter for the console-* tools to let you specify the correct encoding.",,initialcontext,jghoman,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-468,,,,,,,,,,,,,,,,,,,,,"21/Aug/12 23:32;initialcontext;KAFKA-367-1.patch;https://issues.apache.org/jira/secure/attachment/12541834/KAFKA-367-1.patch","22/Aug/12 02:22;initialcontext;KAFKA-367-2.patch;https://issues.apache.org/jira/secure/attachment/12541856/KAFKA-367-2.patch","05/Sep/12 04:04;initialcontext;KAFKA-367-3.patch;https://issues.apache.org/jira/secure/attachment/12543805/KAFKA-367-3.patch","05/Sep/12 04:01;initialcontext;KAFKA-367-3.patch;https://issues.apache.org/jira/secure/attachment/12543804/KAFKA-367-3.patch","11/Sep/12 00:53;initialcontext;KAFKA-367-4.patch;https://issues.apache.org/jira/secure/attachment/12544568/KAFKA-367-4.patch","17/Sep/12 18:35;initialcontext;KAFKA-367-5.patch;https://issues.apache.org/jira/secure/attachment/12545451/KAFKA-367-5.patch",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,241633,,,Thu Nov 22 22:03:22 UTC 2012,,,,,,,,,,"0|i029fr:",11130,,,,,,,,,,,,,,,,,,,,"21/Aug/12 23:32;initialcontext;First attempt here. Defaulting StringEncoder/Decoder to ""UTF-8"" is done, should be able to get user-defined alternate charsets from CLI and use from Consumer side, not so sure about Producer side.

Let me know if there's stuff to fix here, thanks again!
;;;","21/Aug/12 23:46;initialcontext;Producer side is messed up, working on patch #2 here...
;;;","22/Aug/12 02:22;initialcontext;Another attempt. Also not working quite yet. I'm new to Scala and not quite sure which syntax trick allows you to call a default constructor with and without parens from client code, and to have an override with a string in the constructor without upsetting Scala or the test code. This is happening in my modifications to StringEncoder. I would ideally like to make the character encoding setting a ""val"" as well. But all this will be solved soon I hope...

Feel free to take a look and advise a wayward Java refugee...;;;","30/Aug/12 15:55;jkreps;This looks good. The only issue is that the encoder and decoder are pluggable (you just give the class name). This makes hard coding a property as a variable in the config object for a particular serializer a little awkward. I think this is actually a general need for any kind of serializer not just strings, the serializer might need access to various URLs, or to a schema string or all kinds of other things. Since we want people to be able to make their own serializers without modifying any kafka code, this is not very good.

Currently we require a no-arg constructor for the serializer. What if, instead, we required a constructor that took a single argument, the java.util.Properties instance? This way you could write a custom serializer and pass any properties you liked to it without needing to modify any kafka code. Perhaps ideally we could look for either the Properties only constructor or the no-arg constructor preferring the properties one, allowing backwards compatibility with any encoders people have already. ;;;","31/Aug/12 05:03;initialcontext;Love it. I'll get right on this, thanks for the advice!
;;;","05/Sep/12 04:01;initialcontext;Is this more like it? This seems right and the decoder related tests seem to be passing, but when I run the suite in SBT on my local machine I get several errors in tests regarding ZooKeeper. I am unaware whether these are due to local test config or genuine trouble. They seem to culminate at the Kafka end in Utils.getObject() at line 675.
;;;","05/Sep/12 04:04;initialcontext;Is this more like it? I am still getting an error on several tests on SBT but they are ZooKeeper tests and I'm not certain if they relate directly to this patch or my local setup is not right for running the tests? They culminate on the Kafka side of the code in 2 ZK tests at Utils.getObject() line 675. Does this ring any bells?

Thanks!

;;;","05/Sep/12 18:58;jkreps;This looks good, but I don't see where we make use of the new constructor. I think we need to pass in the properties in the producer and consumer, right?;;;","06/Sep/12 18:01;initialcontext;Sorry, that makes sense. Wasn't sure if that was ""to be added later when someone needs it"" or if that should be wired up now. Will do. Kind of felt like the constructor should have been in the trait rather than the classes too but the compiler didn't agree. Might try another swipe at getting that to happen too. ;;;","06/Sep/12 19:43;jkreps;Yeah I think the contract now becomes ""your class needs to implement the interface and provide a constructor that takes Properties, so it does make sense to have this in the class rather than the trait since we won't know what to do with the properties except in the class.

I think we do need to use the new constructor to make this bug fixed since otherwise we are still going to pick up the default char set.;;;","09/Sep/12 04:09;initialcontext;Thanks, sounds good. Will do.
;;;","09/Sep/12 05:19;initialcontext;I still need to add console opts for encoding and write these new constructors into the producer and consumer stuff, but the way I have the patch now I believe the properties calls have hardcoded defaults to ""UTF-8"" so acutally I do think even the no-arg constructors would be safe with this in the case where no character encoding option is chosen. Am I wrong about that? 

Anyway there's more to do here regardless, will get to it and post something ASAP
;;;","11/Sep/12 00:53;initialcontext;This seems to be working/passing all the tests. If this is looking like what you had in mind, I can add a command line option to set the encoding manually and maybe we're good to go.

;;;","17/Sep/12 18:35;initialcontext;This includes the Encoder/Decoder improvement as in the 367-4 patch, and the Consumer and Producer console options to set character encoding manually and place it into the relevant Properties objects for consumption inside Kafka. Passes SBT tests etc. should be good to go?;;;","22/Nov/12 22:03;jkreps;Eli, I was also working on the same area for KAFKA-544 so I attempted to integrate this patch into that one. I have committed that to the 0.8 branch. If you get a chance take a look at the current 0.8 branch HEAD and let me know if I got it right.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace numerical compression codes in config with something human readable,KAFKA-363,12559886,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jkreps,jkreps,jkreps,08/Jun/12 19:13,03/Oct/12 19:51,14/Jul/23 05:39,03/Oct/12 19:51,0.8.0,,,,,,,,,,config,,,0,newbie,,,"Currently we have compression codes like 1 or 2, which is pretty unintuitive. What does 1 mean?

We should replace these with human-readable codes like ""snappy"", ""gzip"", or ""none"" and change the documentation. We can continue to support the existing integer codes in addition for backwards compatibility with existing configurations.",,jjkoshy,jkreps,mumrah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/12 02:51;mumrah;KAFKA-363.patch;https://issues.apache.org/jira/secure/attachment/12547486/KAFKA-363.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299095,,,Wed Oct 03 19:51:31 UTC 2012,,,,,,,,,,"0|i15zkf:",243062,,,,,,,,,,,,,,,,,,,,"25/Sep/12 14:20;mumrah;Here's a go at a simple fix for this. I added a ""name"" attribute to the CompressionCodec case objects and added getCompressionCodec(String). ;;;","02/Oct/12 22:44;jjkoshy;Thanks for the patch, David.

1. Can we make the name match case insensitive? i.e., just name.toLowerCase.match
2. Any particular reason to change the config name to compression.name? I think compression.codec looks better.
3. Can you make it backward compatible? i.e., allow both int and string in ProducerConfig. (Also, there are other usages of compression.codec - e.g., in system tests and contrib/hadoop* that still use the int).
;;;","03/Oct/12 00:21;mumrah;Joel, thanks for the review.

I've changed the config property back to ""compression.codec"". It accepts the short names (none, gzip, snappy) as well as the old int values. I also made the name match case insensitive like you suggested. New patch is attached;;;","03/Oct/12 01:45;jjkoshy;Thanks for making that change. ProducerConfig: There's a compilation error due to getString default being given an int. Also, the second call to getCompressionCodec should have the default as well (or some unit tests will fail). Looks good otherwise.;;;","03/Oct/12 02:51;mumrah;Compilation error - embarrassing. Maybe I should be more careful since I'm still pretty new to Scala.

Fixed those two issues. `sbt compile test` is happy;;;","03/Oct/12 19:51;jjkoshy;+1
(I don't think we need to apply this to trunk.) Committed this to 0.8 with a very minor edit.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw exception to client if it makes a produce/consume request to a Kafka broker for a topic that hasn't been created,KAFKA-352,12558525,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,30/May/12 02:04,19/Jul/12 19:51,14/Jul/23 05:39,19/Jul/12 19:51,0.8.0,,,0.8.0,,,,,,,,,,0,,,,It will be good to inform the client if it makes a produce/consume request to a Kafka broker for a topic that hasn't been created. The exception should be something like UnknownTopicException that is descriptive.,,jkreps,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-50,,,,,,,,,,,,,,,,,"18/Jul/12 20:17;nehanarkhede;kafka-352-v1.patch;https://issues.apache.org/jira/secure/attachment/12537058/kafka-352-v1.patch","19/Jul/12 02:32;nehanarkhede;kafka-352-v2.patch;https://issues.apache.org/jira/secure/attachment/12537124/kafka-352-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,248183,,,Thu Jul 19 19:51:30 UTC 2012,,,,,,,,,,"0|i09m2n:",54009,,,,,,,,,,,,,,,,,,,,"18/Jul/12 20:17;nehanarkhede;1. Improved error handling for produce, fetch, get offsets request to handle unknown topic error code

2. Topic metadata request error handling is fixed as part of KAFKA-350.

3. I understand that we have KAFKA-402 filed to go over all other error handling. This patch only handled unknown topics and unit tested it for all types of requests.;;;","19/Jul/12 00:16;jkreps;+1

A few minor things:
1. Remove the println in KafkaApis.scala
2. In LogOffsetTest, I think there is no need to define val part = 0 since it is only used once
3. What does the Thread.sleep(500) in SyncProducerTest.testProduceCorrectlyReceivesResponse wait for?


;;;","19/Jul/12 02:32;nehanarkhede;Thanks for the review, Jay ! Uploading another patch with the review comments addressed -
1. Deleted the println
2. Removed the definition of part variable
3. Removed the sleep. It seems like one of those unnecessary sleep() calls we have in our unit tests.
;;;","19/Jul/12 04:26;jkreps;+1;;;","19/Jul/12 19:51;nehanarkhede;Thanks for the review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor some new components introduced for replication ,KAFKA-351,12558227,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,nehanarkhede,nehanarkhede,26/May/12 02:26,30/Oct/17 12:18,14/Jul/23 05:39,21/Aug/12 17:25,0.8.0,,,0.8.0,,,,,,,,,,0,optimization,,,Jay had some good refactoring suggestions as part of the review for KAFKA-46. I'd like to file this umbrella JIRA with individual sub tasks to cover those suggestions,,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,259200,259200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/12 17:07;junrao;kafka-351_v1.patch;https://issues.apache.org/jira/secure/attachment/12540699/kafka-351_v1.patch","15/Aug/12 17:26;junrao;kafka-351_v2.patch;https://issues.apache.org/jira/secure/attachment/12541091/kafka-351_v2.patch","17/Aug/12 06:07;junrao;kafka-351_v3.patch;https://issues.apache.org/jira/secure/attachment/12541330/kafka-351_v3.patch","18/Aug/12 01:29;junrao;kafka-351_v4.patch;https://issues.apache.org/jira/secure/attachment/12541457/kafka-351_v4.patch","18/Aug/12 01:38;junrao;kafka-351_v5.patch;https://issues.apache.org/jira/secure/attachment/12541459/kafka-351_v5.patch","21/Aug/12 05:01;junrao;kafka-351_v6.patch;https://issues.apache.org/jira/secure/attachment/12541713/kafka-351_v6.patch",,,,,,,,,,,,,,,,,,,,,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,299091,,,Tue Aug 21 17:25:50 UTC 2012,,,,,,,,,,"0|i15zjj:",243058,,,,,,,,,,,,,,,,,,,,"25/Jul/12 20:07;nehanarkhede;From KAFKA-405 -

ReplicaManager:

This class has a very odd public interface. Instead of managing replicas it has a bunch of passive calls--addLocalReplica(), addRemoteReplica(), etc. Who calls these? KafkaServer seems to have its own wrapper for these. Then it passes these methods on as arguments to KafkaZookeeper. Does this make sense? I think it would be good if ReplicaManager handled replica management, even if that means it depends on zookeeper.
;;;","13/Aug/12 17:07;junrao;Attach patch v1. An overview of the patch.
A. Use synchronized instead of lock for synchronization. The latter has more functionality, but more overhead. See
http://blog.rapleaf.com/dev/2011/06/16/java-performance-synchronized-vs-lock/

B. Partition: Consolidated all reads/writes to leader/ISR in Partition and all accessed are synchronized. This makes sure that leader/ISR values don't change while doing the following operations.
- makeLeader
- makerFollower
- maybeShrinkISR
- updateLeaderHWAndMaybeExpandISR (for maintaining remote replicas)
- checkEnoughReplicasReachAnOffset (for checking if a produce request is satisfied)
This means that Partition has to access ReplicaManager. In some sense, Partition probably should be a nested class under ReplicaManager since it needs to access several members of ReplicaManager. However, this will make ReplicaManager too big.

C. RepicaManager:
- Moved most per partition operations to Partition.
- Cleaned up public methods and added a few helper methods for getting partition and replica.
- Use ConcurrentHashMap for managing all partitions.

D. KafkaApis: Removed callbacks in the constructer. Instead, call methods in ReplicaManager directly.

E. Replica:
- Changed to the new getter/setter style for logEndOffset and highWatermark.
- Local replica doesn't need to set logEndOffset anymore since the logEndOffsetUpdateTime for local replica is never used.

F. BrokerPartitionInfo: Partition is now a complex structure and is supposed to be used only on the server side. Create a simpler PartitionAndLeader class for client usage.

G. KafkaZookeeper: Removed ensurePartitionLeaderOnThisBroker(). The checking of the existence of a leader is now done by replicaManager.leaderReplicaIfLocalOrException(), which is cheaper since it doesn't access ZK.

H. ISRExpirationTest: Remove the test testISRExpirationForMultiplePartitions. It doesn't seem to add additional value since ISR expiration is always done on a per partition basis.

I. SyncProducerTest: Remove the test testProducRequestForUnknowTopic since the logic is always covered by #1 in testProduceCorrectlyReceivesResponse.

J. TestUtils: Added a helper method leaderLocalOnBroker() that more reliably ensures that a leader exists on a broker.

K. TopicCounTest: removed since it's not doing any useful test.

L. ZookeeperConsumerConnectorTest.testCompressionSetConsumption(): removed the part that tests consumer timeout since it's covered in testBasic already.
;;;","13/Aug/12 17:13;junrao;I still see some transient failures in unit tests, most of which seem to be due to leader not ready. I will probably wait until kafka-369 is committed since it fixes some of those issues.;;;","13/Aug/12 18:24;junrao;Another change in the patch:
M. Currently, on leaderAndISR request, the broker gets and creates all assigned replicas. In this patch, the broker only creates replicas in ISR (since they are required in the logic for shrinking ISR). Other remote replicas are created on demand during the handling for follower fetch requests. This will make implementing kafka-42 a bit easier since newly bootstrapped replicas can be added on demand.;;;","14/Aug/12 17:40;nehanarkhede;Jun, The patch didn't apply cleanly on a fresh checkout of the 0.8 branch. Do you mind uploading another patch after rebasing ?;;;","15/Aug/12 17:26;junrao;Attach patch v2 after rebase. Made one more change:
N. Make logEndOffset and highWaterMark in replica atomic long. This is because java doesn't guarantee consistency of long value if not synchronized. ;;;","17/Aug/12 06:07;junrao;Attach patch v3. Just a rebase.;;;","17/Aug/12 20:20;nehanarkhede;Thanks for the patch! Overall, this refactoring is a good change. Here are a few review comments -

1. TestUtils. How about renaming leaderLocalOnBroker to isLeaderLocalOnBroker ?

2. LogOffsetTest:
2.1. The change in testGetOffsetsForUnknownTopic doesn't look right. Since the topic ""foo"" doesn't exist, the client should get back UnknownTopicException. The partition is not invalid, it doesn't even exist.

3. SyncProducerTest
3.1 Same here, client should get back UnknownTopicException, not InvalidPartitionException

4. ISRExpirationTest
4.1 getLogWithLogEndOffset expected 6 calls for log.logEndOffset since the test exercised that API 6 times during correct operation. If you change it to anyTimes, it will hide problems with either the test or the code. Was it changed to
get rid of some transient test failure ?
4.2 Minor formatting: For consistency, you might want to change to first letter caps for error messages. So far, I don't think everyone quite followed this. So some log statements have first letter caps, others don't. I personally prefer
 first letter caps for all log statements.

5. Replica
5.1 Is there a reason why logEndOffsetUpdateTimeMs is not AtomicLong ? It's access is not protected by a lock.
5.2 What is the difference between private[this] var and private var ?
5.3 It's great that you changed logEndOffset to use the new getter/setter API convention. I think there is only one drawback to using that. I don't know a way to search the code to list all places that use the setter. Do you ?

6. Partition
6.1 Rename addReplicaIfNotExist to addReplicaIfNotExists.
6.2 In getOrCreateLog, it is better to use case match, since in Scala case match always evaluates to some value. Since this API needs to return the Replica object, using case match will protect against code bugs. Instead of if-else that checks isDefined, case-match handles Options naturally, since it forces you to handle all the cases. Same for makeLeader, makeFollower, checkEnoughReplicasReachAnOffset since they also return some value.
6.3 Looks like assignedReplicaMap is meant to be a map of replica_id->Replica. It might be a good idea to change Pool to handle Options. Options are much easier to use than handling null values. For example, getReplica can reduce to just returning assignedReplicaMap.get(replicaId), instead of the if-else checking for nulls.
6.5 Minor formatting comment same as 4.2
6.6. maybeIncrementLeaderHW: Since you are trying to access inSyncReplicas here, this method should be synchronized on the leaderAndIsrUpdateLock
6.7 getOutOfSyncReplicas, updateISR: Same as 6.6
6.8 checkEnoughReplicasReachAnOffset: 
6.8.1 We should probably rename this to checkEnoughReplicasReachOffset. 

7. ReplicaManager
7.1 I think leaderReplicas was a poorly chosen name by me in the past. It should be renamed to leaderPartitions since it is the set of partitions with their leader hosted on the local broker. Also, that would mean we should rename leaderReplicasLock to leaderPartitionsLock
7.2 Same as 6.3 for allPartitions. This will greatly simplify getOrCreatePartition
7.3 Same as 4.2 for some of the APIs
7.4 Fix typo: shuttedd down 
7.5 Fix identation and parenthesis style for checkpointHighWatermarks. 
7.6 Same as 6.2 for becomeLeaderOrFollower 
7.7 I wonder if it is better to rename leaderReplicaIfLocalOrException to getLeaderReplicaIfLocal ?
;;;","18/Aug/12 01:29;junrao;Thanks for the review. 

2,3. The difficulty is that a broker currently doesn't cache all topic/partitions (only controller does that). It only knows about topic/partitions assigned to itself. So, it's hard for a broker distinguish between a missing topic and a missing partition. We could cache all topic/partitions in all brokers, but we need to add additional ZK logic in each broker. So, in this patch, just combined UnknownTopicException and InvailidPartitionException into a more general UnknowTopicPartitionException. It's not ideal, but probably not too painful for the user to understand.

5.1 That's a good point. Moved the update (which updates logEndOffsetUpdateTimeMs) of logEndOffset into Partition.updateLeaderHWAndMaybeExpandISR(). This way, both the reader and the writer of logEndOffsetUpdateTimeMs is synchronized by leaderISRUpdateLock. So, there is no need to make it an AtomicLong.

6.3 Just not to make this patch too large. I will create a separate jira for changing the Pool api to use Option.

6.6 and 6.7 Both methods are only called in Partition and the caller already synchronizes on leaderISRUpdateLock.

The rest of the comments have been addressed.;;;","18/Aug/12 01:38;junrao;Upload patch v5 to fix an svn rename issue.;;;","20/Aug/12 16:40;nehanarkhede;Thanks for incorporating review suggestions!

2.3 Agreed that it is a bit of work to get meaningful error codes in the client. Often this is ignored, but client contract should be very well thought out and easy to understand. It is best if we give the most descriptive error code, but if we feel it takes significant amount of work, we can start with a simple solution. We went through a pretty detailed review of the new request formats, but not error codes. It will be good to go through this before the release.

5.1 That change is correct. However, in Replica.scala, highWatermarkValue and logEndOffsetValue are synchronized via AtomicLong, but not logEndOffsetUpdateTime. Right now, like you said, there is only one API that updates/reads logEndOffsetUpdateTime and it synchronizes those accesses. But since these are Replica APIs, I'm pretty sure there will be more places in the code that will either update or read the logEndOffset/logEndOffsetUpdateTime and each of those APIs would have to synchronize those accesses. For what it's worth, changing it to AtomicLong actually protects us from future synchronization errors and is not much of a performance hit as well. 

8. HighwatermarkPersistenceTest. Fix fail error message to say KafkaException instead of IllegalStateException. I forgot to do this in my patch when I added this test, it will be great if you can include this minor change.

9. Minor comment - Probably better to rename UnknownTopicPartition to UnknownTopicOrPartitionException.
;;;","21/Aug/12 05:01;junrao;Thanks for the review. Attach patch v6.

5.1 Changed logEndOffsetUpdateTime to AtomicLong.

8,9: Fixed.

Optimized Replica.getOutOfSyncReplicas() a bit to avoid the unnecessary check for leader replica.

Rebased.;;;","21/Aug/12 16:56;nehanarkhede;+1 

Minor comments before checking it in - 

1. Fix comment in UnknownTopicOrPartitionException.scala. Right now it describes InvalidPartitionException
2. Replica - Fix error message ""shouldn't set logEndOffset for replica %d topic %s partition %d since it's local"" to first letter capital
;;;","21/Aug/12 17:25;junrao;Thanks for the review. Addressed the issues in the last review and committed to 0.8.

Create kafka-476 to track using Option in Pool.

For priviate[this] var, it restricts the usage of a member field to only this instance of the class. This way, one is always forced to use the public api to access the member field in other instances of the class. 

Yes, Intellij seems to have an issue finding references of x_=(), which is inconvenient. Not sure if it has been addressed in a new version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable message replication in the presence of controlled failures,KAFKA-350,12558226,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,26/May/12 02:22,24/Jul/12 18:13,14/Jul/23 05:39,24/Jul/12 18:13,0.8.0,,,,,,,,,,,,,0,,,,KAFKA-46 introduced message replication feature in the absence of server failures. This JIRA will improve the log recovery logic and fix other bugs to enable message replication to happen in the presence of controlled server failures,,guozhang,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/12 23:33;nehanarkhede;kafka-350-v1.patch;https://issues.apache.org/jira/secure/attachment/12536746/kafka-350-v1.patch","21/Jul/12 00:26;nehanarkhede;kafka-350-v2.patch;https://issues.apache.org/jira/secure/attachment/12537430/kafka-350-v2.patch","24/Jul/12 02:29;nehanarkhede;kafka-350-v3.patch;https://issues.apache.org/jira/secure/attachment/12537640/kafka-350-v3.patch","24/Jul/12 16:31;nehanarkhede;kafka-350-v4.patch;https://issues.apache.org/jira/secure/attachment/12537706/kafka-350-v4.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,248182,,,Tue Jul 24 18:12:37 UTC 2012,,,,,,,,,,"0|i09m2f:",54008,,,,,,,,,,,,,,,,,,,,"31/May/12 22:28;nehanarkhede;Handle the logic of deleting segments that have start offset > highwatermark during log recovery. ;;;","16/Jul/12 21:16;nehanarkhede;This patch contains fixes to bugs in various components to make message replication work in the presence of controlled server failures. The system test under system_test/single_host_multiple_brokers passes with server failures enabled. 

This patch contains the following changes -

1. Topic metadata request bug

1.1. While responding to a topic metadata request bug, the server uses the AdminUtils to query ZK for the host-port info for the leader and other replicas. However, it can happen that one of the brokers in the replica is offline and hence its broker registration in ZK is unavailable. Since the system test simulates exactly this scenario, the server's request handler thread was exiting due to a NoNodeException while reading the /brokers/ids/[id] path. The general problem seems to be handling error codes correctly in the request handlers and sending them to the client. I think once KAFKA-402 is resolved, all error codes will be handled cleanly by the server. For now, I've modified topic metadata response to have an error code per topic as well as per partition. So, if the leader is not available for some partitions, it will set the LeaderNotAvailable error code at the partition level. If some other replica is not available, it will set the ReplicaNotAvailable error code in the response instead of throwing an exception and exiting.
1.2. On the producer side, it fails the produce request retry if the leader for that partition is not available. It logs all other kinds of errors and ignores them. 
1.3. Changed some unit tests in AdminTest to elect a leader before making topic metadata requests, so that the test passes.
1.4. Fixed another bug in the deserialization of the topic metadata response that read the versionId and errorCode in incorrect order.
1.5. In DefaultEventHandler, during a retry, it relied on the topic cache inside BrokerPartitionInfo for refreshing topic metadata. However, if the topic cache is empty (when no previous send request has succeeded), it doesn't end up refreshing the metadata for the required topics in the produce request. Fixed this by passing the list of topics explicitly in the call to updateInfo()
1.6. In general, it seems like a good idea to have a global error code for the entire response (to handle global errors like illegal request format), then a per topic error code (to handle error codes like UnknownTopic), a per-partition error code (to handle partition-level error codes like LeaderNotAvailable) and also a per-replica error code (to handle ReplicaNotAvailable). Jun had a good suggestion about the format of TopicMetadata response. I would like to file another JIRA to improve the format of topic metadata request.

2. Bug in TestUtils waitUntilLeaderIsElected was signalling a condition object without acquiring the corresponding lock. This error message was probably getting masked since we had turned off ERROR log4j level for unit tests. Fixed this.

3. Log recovery
3.1. Removed the highWatermark() API in Log.scala, since we used “highwatermark” to indicate the offset of the latest message flushed to disk. This was causing the server to prevent the replicas from fetching unflushed data on the leader. With replication, we use highwatermark to denote the offset of the latest committed message. I removed all references to the older API (highWatermark). To remain consistent with setHW, I added an API called getHW. As I understand it, these APIs will be refactored/renamed to match conventions, when KAFKA354 is resolved
3.2. To handle truncating a log segment, I added a truncateUpto API that takes in the checkpointed highwatermark value for that partition,  computes the correct end offset for that log segment and truncates data after the computed end offset
3.3. Improved log recovery to delete segments that have start offset > highwatermark
3.4. Fixed logEndOffset to return the absolute offset of the last message in the log for that partition.
3.5. To limit the changes in this patch, it does not move the highwatermarks for all partitions to a single file. This will be done as part of KAFKA-405
3.6. Added a LogRecoveryTest to test recovery of log segments with and without failures

4. Config options
4.1. We might want to revisit the defaults for all config options. For example, the isr.keep.in.sync.time.ms defaulted to 30s which seems way too optimistic.  While running the system tests, most messages timed out since the isr.keep.in.sync.time.ms and the frequency of bouncing the replicas was also roughly 30s. The socket timeout for the producer also defaults to 30s which seems very long to block a producer for.
4.2.  The socket.timeout.ms should probably be set to producer.request.ack.timeout.ms, if it is a non-negative value. If producer.request.ack.timeout.ms  = -1, socket.timeout.ms should probably default to a meaningful value. While running the system test, I observed that the socket timeout was 30s and the producer.request.ack.timeout.ms was -1, which means that the producer would always block for 30s if the server failed to send back an ACK. Since the frequency of bouncing the brokers was also 30s, most produce requests were timing out.

5. Socket server bugs
5.1. A bug in processNewResponses() causes the SocketServer to process responses inspite of it going through a shutdown. It probably makes sense to let outstanding requests timeout and shutdown immediately
5.2. Another bug in processNewResponses() causes it to go in an infinite loop when the selection key processing throws an exception. It failed to move to the next key in this case. I fixed it by moving the next key processing in the finally block.

6. Moved the zookeeper client connection from startup() API in KafkaZookeeper to startup() API  in KafkaServer.scala. This is because the ReplicaManager is instantiated right after KafkaZookeeper and was passed in the zkclient object created by KafkaZookeeper. Since KafkaZookeeper started the zkclient only in startup() API, ReplicaManager's API's got NPE while trying to access the passed-in zkclient. With the fix, we can create the zookeeper client connection once in the KafkaServer startup, pass it around and tear it down in the shutdown API of KafkaServer. 

7. System testing
7.1. Hacked ProducerPerformance to turn off forceful use of the async producer. I guess ProducerPerformance has grown over time into a complex blob of if-else statements. Will file another JIRA to refactor it and fix it so that all config options work well with both sync and async producer.
7.2. Added producer ACK config options to ProducerPerformance. Right now, they are hardcoded. I'm hoping this can be fixed with the above JIRA. 
7.3. The single_host_multiple_brokers system test needs to be changed after KAFKA-353. Basically, it needs to count the successfully sent messages correctly. Right now, there is no good way to do this in the script. One way is to have the system test script grep the logs to find the successfully ACKed producer requests. To get it working for now, hacked it to use sync producer. Hence, before these issues are fixed it will be pretty slow.
7.4. Improved the system test to start the console consumer at the very end of the test for verification

8. Another thing that can be fixed in KAFKA-402 is the following -
8.1. When a broker gets a request for a partition for which it is not the leader, it should sent back a response with an error code immediately. Right now, I see a produce request timing out in LogRecoveryTest since the producer never gets a response. It doesn't break the test since the producer retries, but it is adding unnecessary delay.
8.2. In quite a few places, we log an error and then retry, or log an error which is actually meant to be a warning. Due to this, it is hard to spot real errors during testing. We can probably fix this too as part of KAFKA-402. 

8. Found a NPE while running ReplicaFetchTest. Would like to file a bug for it

9. Controller bugs 
9.1. Unit tests fail intermittently due to NoSuchElementException thrown by testZKSendWithDeadBroker() in KafkaController. Due to this, the zookeeper server doesn't shut down and rest of the unit tests fail. The root cause is absence of a broker id as a key in the java map. I think Scala maps should be used as it forces the user to handle invalid values cleanly through Option variables.
9.2. ControllerBasicTest tests throw tons of zookeeper warnings that complain a client not cleanly closing a zookeeper session. The sessions are forcefully closed only when the zookeeper server is shutdown. These warnings should be paid attention to and fixed. LogTest is also throwing similar zk warnings.
9.3. Since controller bugs were not in the critical path to getting replication-with-failures to work, I'd like to file another JIRA to fix it.

If the above future work sounds good, I will go ahead and file the JIRAs;;;","19/Jul/12 05:03;jkreps;This is a pretty hard to review due to the large number of changes and also because I don't know some of this code well.

A lot of things like bad logging/naming that I think you could probably catch just perusing the diff.

Log:
- Log should not know about hw right? We seem to be adding more hw stuff there?
- This adds a getHW() that just returns a private val, why not make the val public? Please fix these. Regardless of cleanup being done get/set methods have been against the style guide for a long time, lets not add more. Ditto getEndOffset() which in addition to being a getter is inconsistent with Log.logEndOffset
- There is debug statement in a for loop in Log.scala that needs to be removed
- I don't understand the difference between nextAppendOffset and logEndOffset. Can you make it clear in the javadoc and explain on why we need both of these. Our public interface to Log is getting incredibly complex, which is really sad so I think we should really think through deeply what is added here and why.
- The javadoc on line 138 of Log.scala doesn't match the style of javadoc for the preceeding 5 variables.

- Does making isr.keep.in.sync.time.ms more stringent actually make sense? 10 seconds is pretty tight. I think what you are saying is that every server bounce will introduce 30 seconds of latency. But I think that is kind of a weakness of our design. If we lower that timeout we may just get spurious dropped replicas, no?
- Can we change the name of isr.keep.in.sync.time.ms to replica.max.lag.time.ms?
- Good point about the socket timeouts. We can't set socket timeout equal to request timeout, though, as there may be a large network latency. I recommend we just default the socket timeout to something large (like 10x the request timeout), and throw an exception if it is less than the request timeout (since that is certainly wrong). I don't think we should be using the socket timeout except as an emergency escape for a totally hung broker now that we have the nice per-request timeout.
- Can we change producer.request.ack.timeout.ms to producer.request.timeout.ms so it is more intuitive? I don't think the word ""ack"" is going to be self-explanatory to users.

SocketServer.scala
- Please remove: info(""Shut down acceptor completed"")
- Is there a reason to add the port into the thread name? That seems extremely odd...is it to simplify testing where there are multiple servers on a machine?
- Why is it a bug for processNewResponses() to happen while a shutdown is occuring. I don't think that is a bug. That is called in the event loop. It is the loop that should stop, no? Is there any ill effect of this?
- Good catch on the infinite loop

System testing
- I think we should fix the performance test hacks. The performance tool is critical. I have watched this play out before. No one ever budgets time for making the performance test stuff usable and then it just gets re-written umpteen times and never does what is needed. Most of these are just a matter of some basic cleanup and adding options. Let's work clean.

AdminUtils
- I don't understand the change in getTopicMetaDataFromZK

Replica.scala
- Can you remove trace(""Returning leader replica for leader "" + leaderReplicaId) unless you think it is of value going forward

ErrorMapping.scala
- getMaxErrorCodeValue - seems to be recomputed for each TopicMetadata. Let's get rid of this, I don't think we need it. We already have an unknown entry in the mapping, we should use that and get rid of the Utils.getShortInRange
- If we do want to keep it, fix the name
- We should really give a base class KafkaException to all exceptions so the client can handle them more easily
- Instead of having the client get an IllegalArgumentException we should just throw new KafkaException(""Unknown server error (error code "" + code + "")"")
- The file NoLeaderForPartitionException seems to be empty now, I think you meant to delete it

ConsoleConsumer
- What does moving those things into a finally block do? We just caught all exceptions...

FileMessageSet
- You added more log spam. Please format it so it is intelligible to someone not working on the code or remove: debug(""flush size:"" + sizeInBytes())
- Ditto info(""recover upto size:"" + sizeInBytes())

BrokerPartitionInfos is a really weird class

DefaultEventHandler
- This seems to have grown into a big glump of proceedural logic.
- Inconsistent spacing of parens should be cleaned up
- partitionAndCollate is extemely complex
- Option[Map[Int, Map[(String, Int), Seq[ProducerData[K,Message]]]]]

ZkUtils
- LeaderExists class needs a name that is a noun
- Also we have a class with the same name in TopicMetadata

I really like TestUtils.waitUntilLeaderIsElected. God bless you for not adding sleeps. We should consider repeating this pattern for other cases like this.

ZooKeeperTestHarness.scala
- Can we replace the thread.sleep with a waitUntilZkIsUp call?

;;;","20/Jul/12 15:51;guozhang;Would it be better to have a general read/write formatting for boolean, like writeBoolean/readBoolean just like writeShortString/readShortString? Then we do not need to specify, for example, in TopicMetadata:

sealed trait LeaderRequest { def requestId: Byte }
case object LeaderExists extends LeaderRequest { val requestId: Byte = 1 }
case object LeaderDoesNotExist extends LeaderRequest { val requestId: Byte = 0 }


which would need to be done for every request/response that has a boolean.;;;","20/Jul/12 20:34;nehanarkhede;Thanks for the review, Jay ! Here is another patch fixing almost all review comments -

1. Log
1. Refactoring of the hw logic is part of KAFKA-405. It is moved to a new file HighwaterMarkCheckpoint and is controlled by the ReplicaManager. The Log does not and should not know about high watermarks.
2. Getter/setter for hw is removed as part of KAFKA-405 anyways.
3. Agree with you on the weak public interface, especially Log needs a cleanup. I think you attempted that as part of KAFKA-371. I've cleaned up quite a few things as part of KAFKA-405 and this patch. Nevertheless, fixed the nextAppendOffset as part of this patch. It is not required when we have logEndOffset. Also, removed the getEndOffset from FileMessageSet. Added endOffset() API to the LogSegment in addition to a size() API. This is useful during truncation of the log based on high watermark.
4. Fixed the javadoc and removed the debug statement.

2. Config options
1. The isr.keep.in.sync.time.ms set to 10 seconds is also very lax. A healthy follower should be able to catch up in 100s of milliseconds even with a lot of topics, with a worst case of maybe 4-5 seconds. We will know the latency better when we run some large scale tests. But yeah, the issue with the system test is independent of what the right value should be. I was just explaining how I discovered this issue. :)
2. Good point about renaming it to replica.max.lag.time.ms. Also changed isr.keep.in.sync.bytes to replica.max.lag.bytes.
3. Since the producer does a blocking read on the socket, the socket timeout cannot be greater than the request timeout. If it is, then the request timeout guarantee would be violated, no ?

3. SocketServer.scala
1. Removed info log statement
2. Yes, wanted to simplify testing/debugging (thread dumps) when there are multiple servers on one machine. Not sure if this is the best way to do that.
3. processNewResponses() doesn't have to process outstanding requests during shutdown. It can shutdown by ignoring them and those requests will timeout anyways. But yes, good point about the event loop doing it instead. Fixed it.

4. System testing
1. Fixed the hacky change. I still need ProducerPerformance needs a complete redo. Filed bug KAFKA-408 to do that.

5. AdminUtils
1. Need this change to return appropriate error codes for topic metadata request. Without this, all produce requests are timing out while fetching metadata, since in the system test, at any point of time, one replica is always down.

6. Partition.scala
1. Removed the trace statement.

7. ErrorMapping
1. Removed getMaxErrorCode and its usage from ErrorMapping
2. Good point. Introduced a new class KafkaException and converted IllegalStateException and IllegalArgumentException to it.
3. NoLeaderForPartitionException is in fact marked as deleted in the patch

8. ConsoleConsumer
1. Good point. The finally block there didn't really make any sense.

9. FileMessageSet
1. I haven't added the log statements in this patch, we always had it. 
2. The purpose of adding that was help with debugging producer side queue full exceptions. We turned on DEBUG to see what the server side flush sizes and latencies were. I think these statements were added when we didn't have an ability to monitor these. However, whoever added the log flush monitoring maybe forgot to remove these statements. I removed it in this patch.
3. Removed the “recover upto” log statement too

10. DefaultEventHandler
1. Agree with the unwieldly procedural code. Filed bug KAFKA-409 to clean this up. 

11. ZkUtils
1. Renamed LeaderExists -> LeaderExistsListener

12. ZookeeperTestHarness
1. The sleep is actually not required. The EmbeddedZookeeper constructor returns only after the zk server is completely started.;;;","23/Jul/12 16:03;junrao;Thanks for patch v2. Some comments:

20. PartitionMetadata: Could we make getLeaderRequest() private?

21. DefaultEventHandler.partitionAndCollate(): Is it necessary for this method to return an Option? It seems that if this method hits any exception, it's simpler to just pass on the exception and let the caller deal with it. 

22. LogSegment: Ideally this class should only deal with offsets local to the segment. Global offsets are only used at the Log level. So it's probably better to use local offset in the input of truncateUpto(). Similarly, we probably don't need endOffset since it returns global offset. If we do want to keep it, it probably should be named globalEndOffset.

23. Log: Since we removed the HW check in FileMessageSet.read. We will need to add a guard in Log.read() so that we only expose messages up to HW to the consumer.

24. LogManager: remove unused import

25. ControllerChannelManager.removeBroker: the error message is not correct since this method is called in places other than shutdown too.

26. system_test/single_host_multi_brokers/bin/run-test.sh: We should remove comments saying ""If KAFKA-350 is fixed"".

27. Unit tests pass on my desktop but fail on my laptop at the following test consistently. Without the patch, unit tests pass on my laptop too. This seems to be due to the change in ZooKeeperTestHarness.
[info] Test Starting: testFetcher(kafka.consumer.FetcherTest)
[error] Test Failed: testFetcher(kafka.consumer.FetcherTest)
org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server within timeout: 2000
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.zk.ZooKeeperTestHarness$class.setUp(ZooKeeperTestHarness.scala:31)
	at kafka.consumer.FetcherTest.kafka$integration$KafkaServerTestHarness$$super$setUp(FetcherTest.scala:35)
	at kafka.integration.KafkaServerTestHarness$class.setUp(KafkaServerTestHarness.scala:35)
	at kafka.consumer.FetcherTest.setUp(FetcherTest.scala:57)
;;;","23/Jul/12 16:31;jkreps;1.1, 1.2, 2.1, 2.2 Awesome!

2.3 Hmm, well so but that doesn't make sense because we would always pop the socket timeout and fully close out the connection, which is not good. Basically the socket timeout would ALWAYS go off before the server-side request processing timeout due to the network latency both ways. I think that timeout should just be for ""emergencies"". I agree it is a bit of a muddle now that we have two.

7.2 Hmm, but that changed ALL IllegalStateExceptions to KafkaException. IllegalStateException is used to say ""this should not be possible"". For example in SocketServer those checks are in the form of assertions. I like the idea of KafkaException, and I think we should make all the other exceptions extend it to ease error handling on the client, but I don't think we should get rid of IllegalStateException or IllegalArgumentException, they are informative.
;;;","24/Jul/12 02:29;nehanarkhede;Thanks for the review !

Jun's comments -

20. Made getLeaderRequest and getLogSegmentMetadataRequest private
21. partitionAndCollate() is used by dispatchSerializedData() API. The contract of dispatchSerializedData() is to return the list of messages that were not sent successfully so that they can be used on the next retry. Now, if partitionAndCollate() fails, we need dispatchSerializedData to return the entire input list of messages. If something else fails after partitionAndCollate, we need to return a filtered list of messages back. To handle this, partitionAndCollate returns an Option. I agree that this class has very complex code that needs a cleanup. Have filed KAFKA-409 for that.
22. We need to know the absolute end offset of the segment for truncation during make follower state change. Have changed the name of the API to absoluteEndOffset. 
23. We have KAFKA-376 filed to ensure that the consumer sees data only after the HW. This patch exposes data upto log end offset to replicas as well as consumers. So FileMessageSet.read() exposes all data upto log end offset.
24. Removed unused import.
25. Hmm, fixed the error message
26. Removed the TODOs related to KAFKA-350
27. Introduced the connection timeout and session timeout variables in the test harness and set it to 6s. 

Jay's comments -

2.3 So we talked about this offline, stating it here so that others can follow. There are 2 choices here -
2.3.1. One is to keep request timeout = socket timeout. This would make us fully dependent on the socket for the timeout. So, any requests that took longer than that on the server for some reason (GC, bugs etc), would throw SocketTimeoutException on the producer client. The only downside to this is that the producer client wouldn't know why it failed or whether it got written to 0 or 1 or n replicas. Al though, this should be very rare and in the normal case, the server process the request and be able to send the response within the timeout. This is assuming the timeout value was set counting the network delay as well. So if the request is travelling cross-colo, it is expected that you set the timeout keeping in mind the cross-colo network latency. 
2.3.2. The second choice is to set the socket timeout >> request timeout. This would ensure that in some rare failure cases on the server, the producer client would still be able to get back a response (most of the times) with a descriptive error message explaining the cause of the failure. However, it would also mean that under some failures like (server side kernel bug crashing the server etc), the timeout would actually be the socket timeout, which is usually set to a much higher value. This can confuse users who might expect the timeout to be the request timeout. Also, having two different timeouts also seems to complicate the guarantees provided by Kafka 
2.3.3. I personally think option 1 is simpler and provides no worse guarantees than option 2. This patch just sets the socket timeout to be the request timeout. 

2.4 Yeah, I think differentiating between “this should not be possible” and “this should not be possible in Kafka” is a little tricky. On one hand, it seems nicer to know that any exception thrown by Kafka will either be KafkaException or some exception that extends KafkaException. On the other hand, some states are just impossible and must be treated like assertions, for example, a socketchannel key that is not readable or writable or even valid. And in such cases, it might be slightly more convenient to have IllegalStateException. And I'm not sure I know the right answer here. I took a pass over all the IllegalStateException usages and converted the ones I think should be KafkaException, but I might not have done it in the best way possible.
;;;","24/Jul/12 07:27;junrao;Thanks for patch v3. 

For 21, my point is that the exceptions that can be thrown in partitionAndCollate() are non-recoverable and therefore retries won't help. partitionAndCollate() won't throw NoBrokersForPartitionException since only BrokerPartitionInfo.updateInfo can throw such an exception and updateInfo is not called here. partitionAndCollate() throws InvalidPartitionException that indicates a wrong partition of a topic. If we just retry, we will hit the same exception and fail again. It's simpler to just throw an exception and treat it as a failed case. Ditto for other exceptions that partitionAndCollate() may throw.

A few new comments:
30. SyncProducerConfig.requestTimeoutMs: We should make sure that the value is a positive integer and change the comment accordingly.

31. IteraterTemplate: The two KafkaExceptions are better reverted to IllegalStateException.

32. ProducerPerformance: We should remove socketTimeoutMsOpt.

33. SocketServer: no need to import illegalStateException

34. I got the following exception when running system_test/single_host_multi_brokers/bin/run-test.sh

2012-07-24 00:22:51 cleaning up kafka server log/data dir
2012-07-24 00:22:53 starting zookeeper
2012-07-24 00:22:55 starting cluster
2012-07-24 00:22:55 starting kafka server
2012-07-24 00:22:55   -> kafka_pids[1]: 75282
2012-07-24 00:22:55 starting kafka server
2012-07-24 00:22:55   -> kafka_pids[2]: 75286
2012-07-24 00:22:55 starting kafka server
2012-07-24 00:22:55   -> kafka_pids[3]: 75291
2012-07-24 00:22:57 creating topic [mytest] on [localhost:2181]
creation failed because of org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids
org.I0Itec.zkclient.exception.ZkNoNodeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids
	at org.I0Itec.zkclient.exception.ZkException.create(ZkException.java:47)
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:685)
	at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:413)
	at org.I0Itec.zkclient.ZkClient.getChildren(ZkClient.java:409)
	at kafka.utils.ZkUtils$.getChildren(ZkUtils.scala:363)
	at kafka.utils.ZkUtils$.getSortedBrokerList(ZkUtils.scala:80)
	at kafka.admin.CreateTopicCommand$.createTopic(CreateTopicCommand.scala:86)
	at kafka.admin.CreateTopicCommand$.main(CreateTopicCommand.scala:73)
	at kafka.admin.CreateTopicCommand.main(CreateTopicCommand.scala)
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:102)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1249)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1277)
	at org.I0Itec.zkclient.ZkConnection.getChildren(ZkConnection.java:99)
	at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:416)
	at org.I0Itec.zkclient.ZkClient$2.call(ZkClient.java:413)
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)
	... 7 more
;;;","24/Jul/12 16:31;nehanarkhede;
21. Not really. updateInfo() is not the only API that throws NoBrokersForPartitionException. partitionAndCollate can encounter that exception while calling the getPartitionListForTopic() API.  But you raised a good point here. What partitionAndCollate() should do (given its current design) is to log a retry warning for recoverable exceptions and return None, so that they are included in the retry. For non-recoverable exceptions, it should just throw the exception to the handle API and mark it failed.

30. Added an error message for non-positive values for request timeout. Al though a value of zero means infinite timeout
31. I guess so. Changed it back to IllegalStateException
32. Removed socket timeout related stuff from ProducerPerformance
33. Removed the import.
34. Hmm, that can happen if you try to create a topic when the brokers haven't yet registered in zookeeper. The system tests waits for 2 seconds, which should be enough for couple of brokers to startup in the normal case. I haven't been able to reproduce this. Its possible that this is something that can be fixed in the system test. File a bug if you see it.;;;","24/Jul/12 17:59;junrao;Thanks for patch v4. We are almost there.

21. In partitionAndCollate(), UnknownTopicException doesn't seem to recoverable.

30. It doesn't look like the broker handles requestTimeoutMs of 0 through RequestPurgatory. It's probably simpler if we just require timeout to be positive. If someone wants infinite timeout, MAX_INT can be used instead.

If those issues are addressed, the patch can be committed without another round of review.

34. This seems to only happen on my laptop. Will file another jira. ;;;","24/Jul/12 18:12;nehanarkhede;21. It is recoverable when there is auto create topic enabled on the server. Until we get rid of auto create, I think we can keep this. 
30. Changed it to be non-negative and non-zero

Committing it with these changes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Write unit test for kafka server startup and shutdown API ,KAFKA-328,12549926,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,balaji.seshadri@dish.com,nehanarkhede,nehanarkhede,06/Apr/12 19:53,02/Dec/14 23:18,14/Jul/23 05:39,23/Sep/14 04:04,,,,0.9.0.0,,,,,,,,,,0,newbie,,,"Background discussion in KAFKA-320

People often try to embed KafkaServer in an application that ends up calling startup() and shutdown() repeatedly and sometimes in odd ways. To ensure this works correctly we have to be very careful about cleaning up resources. This is a good practice for making unit tests reliable anyway.

A good first step would be to add some unit tests on startup and shutdown to cover various cases:
1. A Kafka server can startup if it is not already starting up, if it is not currently being shutdown, or if it hasn't been already started
2. A Kafka server can shutdown if it is not already shutting down, if it is not currently starting up, or if it hasn't been already shutdown. ",,balaji.seshadri,balaji.seshadri@dish.com,nehanarkhede,sriharsha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/14 05:48;balaji.seshadri@dish.com;KAFKA-328-FORMATTED.patch;https://issues.apache.org/jira/secure/attachment/12672978/KAFKA-328-FORMATTED.patch","29/Nov/14 07:10;balaji.seshadri@dish.com;KAFKA-328-REVIEW-11-29.patch;https://issues.apache.org/jira/secure/attachment/12684233/KAFKA-328-REVIEW-11-29.patch","31/Oct/14 22:35;balaji.seshadri@dish.com;KAFKA-328-REVIEW-COMMENTS.patch;https://issues.apache.org/jira/secure/attachment/12678591/KAFKA-328-REVIEW-COMMENTS.patch","10/Nov/14 17:52;balaji.seshadri@dish.com;KAFKA-328.patch;https://issues.apache.org/jira/secure/attachment/12680612/KAFKA-328.patch","23/Sep/14 04:05;balaji.seshadri@dish.com;KAFKA-328.patch;https://issues.apache.org/jira/secure/attachment/12670599/KAFKA-328.patch","10/Nov/14 18:05;balaji.seshadri@dish.com;KAFKA-328_2014-11-10_11:05:58.patch;https://issues.apache.org/jira/secure/attachment/12680613/KAFKA-328_2014-11-10_11%3A05%3A58.patch","29/Nov/14 07:07;balaji.seshadri@dish.com;KAFKA-328_2014-11-29_00:08:05.patch;https://issues.apache.org/jira/secure/attachment/12684231/KAFKA-328_2014-11-29_00%3A08%3A05.patch",,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,234917,,,Tue Dec 02 23:18:08 UTC 2014,,,,,,,,,,"0|i029vb:",11200,,nehanarkhede,,,,,,,,,,,,,,,,,,"17/Jul/14 02:11;balaji.seshadri@dish.com;Please assign to me i will work on the fix.;;;","18/Jul/14 03:07;nehanarkhede;Hey [~balajisn], it will be good to see if we already have a unit test for this. If not, write one and see if this is still an issue or not. ;;;","18/Jul/14 15:08;balaji.seshadri@dish.com;Sure will do. Can you guys please verify my patch for KAFKA-1476.

Balaji

;;;","18/Jul/14 23:07;balaji.seshadri@dish.com;Is this the kind of test you are expecting.

 @Test
  def testServerStartupConsecutively(){
    var server = new KafkaServer(config)
    server.startup()
    Thread.sleep(100)
    try{
    	server.startup()
    }
    catch{
      case ex => {
        assertTrue(ex.getMessage().contains(""This scheduler has already been started!""))
      }
    }
    
    server.shutdown()
  };;;","24/Jul/14 17:15;balaji.seshadri;[~nehanarkhede] can you please advise on my comments on this unit test ?.;;;","24/Jul/14 17:22;balaji.seshadri;[~nehanarkhede] i didnot see any such unit tests i have mentioned in my comment in current codebase.;;;","04/Sep/14 20:22;balaji.seshadri@dish.com;[~nehanarkhede] can u please suggest on my previous comment.;;;","14/Sep/14 17:04;nehanarkhede;[~balaji.seshadri@dish.com] Thanks for looking into this. The tests mentioned in the description of the JIRA can be added to the ServerShutdownTest test suite. However, I would avoid adding any sleeps as it defies the point of the test. I'm guessing the fix for the right behavior already exists, but your tests would verify it. I will also take a look at KAFKA-1476 today.;;;","20/Sep/14 19:42;nehanarkhede;Ping [~balaji.seshadri@dish.com];;;","23/Sep/14 04:05;balaji.seshadri@dish.com;[~nehanarkhede] please find patch attached.;;;","25/Sep/14 05:07;nehanarkhede;Thanks for the patch, [~balaji.seshadri@dish.com]. The patch has a lot of unrelated indentation and whitespace changes that distracts from the actual changes and makes the review process harder. Could you please fix your env so all your patches don't suffer from the whitespace/indentation issues?;;;","25/Sep/14 14:46;balaji.seshadri@dish.com;[~nehanarkhede] Can you please send me code style you are using ?.

Are you are using IntelliJ or Eclipse,please give me the formmater xml,i will import it to my environment.;;;","05/Oct/14 01:45;nehanarkhede;[~balaji.seshadri@dish.com] I use Intellij 13 community edition and whatever formatting that comes in that by default. ;;;","05/Oct/14 05:47;balaji.seshadri@dish.com;[~nehanarkhede] Please find patch for Startup Test.;;;","16/Oct/14 18:33;nehanarkhede;Thanks for the patch. Would you mind using our patch review tool going forward? It will make it easier to review.
1. Better to use intercept[IllegalStateException] in the test. 
2. We should add all relevant test cases mentioned in the description like repeated shutdown;;;","31/Oct/14 22:35;balaji.seshadri@dish.com;[~nehanarkhede] Please review;;;","10/Nov/14 04:17;nehanarkhede;Few review comments-
1. Can you please use intercept[IllegalStateException] in testServerStartupConsecutively? 
2. Can you move server.shutdown() to the finally block in testServerStartupConsecutively?
3. Can you add a similar try catch finally block in testConsecutiveShutdown as well?;;;","10/Nov/14 17:52;balaji.seshadri@dish.com;Created reviewboard https://reviews.apache.org/r/27818/diff/
 against branch origin/trunk;;;","10/Nov/14 18:05;balaji.seshadri@dish.com;Updated reviewboard https://reviews.apache.org/r/27818/diff/
 against branch origin/trunk;;;","10/Nov/14 18:07;balaji.seshadri@dish.com;[~nehanarkhede] Please find changes in review board.

https://reviews.apache.org/r/27818/;;;","14/Nov/14 19:32;nehanarkhede;[~balaji.seshadri@dish.com] Reviewed. Left a suggestion on the rb;;;","29/Nov/14 07:07;balaji.seshadri@dish.com;Updated reviewboard https://reviews.apache.org/r/27818/diff/
 against branch origin/trunk;;;","29/Nov/14 07:09;balaji.seshadri@dish.com;[~nehanarkhede] Please find updated review board.;;;","02/Dec/14 23:18;nehanarkhede;Thanks for your patch. Pushed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CallbackHandler.afterDequeuingExistingData is not called during event queue timeout,KAFKA-326,12549896,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,06/Apr/12 15:23,06/Apr/12 17:01,14/Jul/23 05:39,06/Apr/12 17:01,0.7,,,0.7.1,,,,,,,core,,,0,,,,"CallbackHandler.afterDequeuingExistingData is only called when new events are coming and dequeued. It should be called when no new events are coming, but a queue timeout is reached.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/12 15:26;junrao;kafka-326.patch;https://issues.apache.org/jira/secure/attachment/12521683/kafka-326.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,234887,,,Fri Apr 06 17:01:26 UTC 2012,,,,,,,,,,"0|i0rsan:",160228,,,,,,,,,,,,,,,,,,,,"06/Apr/12 15:26;junrao;Attached a patch.;;;","06/Apr/12 16:00;nehanarkhede;+1. Looks good;;;","06/Apr/12 17:01;junrao;Thanks for the review.committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove dead brokers from ProducerPool,KAFKA-321,12548212,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,prashanth.menon,prashanth.menon,27/Mar/12 00:33,07/Feb/15 23:50,14/Jul/23 05:39,07/Feb/15 23:50,0.8.0,,,,,,,,,,core,,,0,replication,,,"Currently, the ProducerPool does not remove producers that are tied to dead brokers.  Keeping such producers around can adversely effect normal producer operation by handing them out and failling - one such scenario is when updating cached topic metadata.  It's best if we remove any producers that are tied to dead brokers to avoid such situations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,233309,,,2012-03-27 00:33:00.0,,,,,,,,,,"0|i029rz:",11185,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testZKSendWithDeadBroker fails intermittently due to ZKNodeExistsException,KAFKA-320,12548191,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,26/Mar/12 21:27,08/Apr/12 02:00,14/Jul/23 05:39,08/Apr/12 02:00,0.7,0.8.0,,,,,,,,,core,,,0,,,,"The testZKSendWithDeadBroker inside ProducerTest fails intermittently with the following exception -

[error] Test Failed: testZKSendWithDeadBroker(kafka.producer.ProducerTest)
java.lang.RuntimeException: A broker is already registered on the path /brokers/ids/0. This probably indicates that you either have configured a brokerid that is already in use, or else you have shutdown this broker and restarted it faster than the zookeeper timeout so it appears to be re-registering.
        at kafka.utils.ZkUtils$.registerBrokerInZk(ZkUtils.scala:109)
        at kafka.server.KafkaZooKeeper.kafka$server$KafkaZooKeeper$$registerBrokerInZk(KafkaZooKeeper.scala:60)
        at kafka.server.KafkaZooKeeper.startup(KafkaZooKeeper.scala:52)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:84)
        at kafka.producer.ProducerTest.testZKSendWithDeadBroker(ProducerTest.scala:174)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at junit.framework.TestCase.runTest(TestCase.java:164)
        at junit.framework.TestCase.runBare(TestCase.java:130)
        at junit.framework.TestResult$1.protect(TestResult.java:110)
        at junit.framework.TestResult.runProtected(TestResult.java:128)
        at junit.framework.TestResult.run(TestResult.java:113)
        at junit.framework.TestCase.run(TestCase.java:120)
        at junit.framework.TestSuite.runTest(TestSuite.java:228)
        at junit.framework.TestSuite.run(TestSuite.java:223)
        at junit.framework.TestSuite.runTest(TestSuite.java:228)
        at junit.framework.TestSuite.run(TestSuite.java:223)
        at org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)
        at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
        at sbt.TestRunner.run(TestFramework.scala:53)
        at sbt.TestRunner.runTest$1(TestFramework.scala:67)
        at sbt.TestRunner.run(TestFramework.scala:76)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.NamedTestTask.run(TestFramework.scala:92)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
        at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
        at sbt.impl.RunTask.runTask(RunTask.scala:85)
        at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Control$.trapUnit(Control.scala:19)
        at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)

The test basically restarts a server and fails with this exception during the restart

This is unexpected, since server1, after shutting down, should trigger the deletion of its registration of the broker id from ZK. But, here is the Kafka bug causing this problem -

In the test during server1.shutdown(), we do close the zkClient associated with the broker and it successfully deletes the broker's registration info from Zookeeper. After this, server1 can be succesfully started. Then the test completes and in the teardown(), we call server1.shutdown(). During this, the server doesn't really shutdown, since it is protected with the isShuttingDown variable, which was never set to false in the startup() API. Now, this leads to an open zkclient connection for the current test run. 

If you try to re-run ProducerTest without exiting sbt, it will first bring up the zookeeper server. Then, since the kafka server during the previous run is still running, it can succesfully renew its session with zookeeper, and retain the /brokers/ids/0 ephemeral node. If it does this before server1.startup() is called in the test, the test will fail.

The fix is to set the shutdown related variables correctly in the startup API of KafkaServer. Also, during debugging this, I found that we don't close zkclient in the Producer as well. Due to this, unit tests throw a whole bunch of WARN that look like - 

[2012-03-26 14:14:27,703] INFO Opening socket connection to server nnarkhed-ld /127.0.0.1:2182 (org.apache.zookeeper.ClientCnxn:1061)
[2012-03-26 14:14:27,703] WARN Session 0x13650dbf8dd0005 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn:1188)
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1146)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/12 21:43;nehanarkhede;kafka-320-v2.patch;https://issues.apache.org/jira/secure/attachment/12520518/kafka-320-v2.patch","06/Apr/12 22:04;junrao;kafka-320-v3-delta.patch;https://issues.apache.org/jira/secure/attachment/12521772/kafka-320-v3-delta.patch","06/Apr/12 19:54;nehanarkhede;kafka-320-v3.patch;https://issues.apache.org/jira/secure/attachment/12521721/kafka-320-v3.patch","26/Mar/12 21:36;nehanarkhede;kafka-320.patch;https://issues.apache.org/jira/secure/attachment/12520016/kafka-320.patch",,,,,,,,,,,,,,,,,,,,,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,233288,,,Sun Apr 08 02:00:49 UTC 2012,,,,,,,,,,"0|i09m8v:",54037,,,,,,,,,,,,,,,,,,,,"26/Mar/12 21:36;nehanarkhede;This patch includes the following -

1. Fixes the kafka server restart bug by resetting the shutdown state variables in the startup() API of the KafkaServer. 

2. Shuts down the zkclient in the Producer

3. ZkClient and Zookeeper can be set to WARN since we fixed the real issue, causing several warnings during the unit tests.
;;;","29/Mar/12 16:53;nehanarkhede;Can someone review this ?;;;","29/Mar/12 17:14;junrao;That's a good finding. We should probably patch it in both trunk and 0.8. Just one comment.

We should only allow a KafkaServer to startup if it has been shutdown.;;;","29/Mar/12 17:29;prashanth.menon;Great catch!  +1, looks good and works on my machine.;;;","29/Mar/12 21:43;nehanarkhede;OK, I made some more changes -

1. Cleaned up zkClient instance creations in unit tests. Now it is wrapped up inside ZookeeperTestHarness, so we ensure that it gets cleanup at an appropriate time. 

2. Changed Kafka server startup and shutdown behavior. Possibly made it more complex. Basically, 

2.1 A Kafka server can startup if it is not already starting up, if it is not currently being shutdown, or if it hasn't been already started

2.2 A Kafka server can shutdown if it is not already shutting down, if it is not currently starting up, or if it hasn't been already shutdown. ;;;","02/Apr/12 18:11;junrao;The ZookeeperTestHarness change looks nice. a couple more comments:

4. KafkaServer: It does look  a bit more complex now and some of the testing is not done atomically. How about the following? 
4.1 add an AtomticBoolean isServerStartable and initialize to true;
4.2 in startup(), if we can atomically set isServerStartable from true to false, proceed with startup; otherwise throw an exception.
4.3 in shutdown(), if isServerStartable is false, proceed with shutdown, at the very end, set isServerStartable to true. 
Startup() and shutdown() are expected to be called from the same thread. So we can expect a shutdown won't be called until a startup completes.

5. SyncProducerTest: unused imports
;;;","02/Apr/12 18:37;nehanarkhede;Thanks for the review!

4. Regarding this, what do people think about the conditions under which a Kafka server should be allowed to startup and shutdown (listed under 2.1 and 2.2 above) ?
5. Will fix this before checkin.
6. Also, looks like improving the kafka server startup and shutdown is orthogonal to this bug fix. Can this be fixed (cleanly) through another JIRA ? I'd like to include just the fix for this issue as part of the checkin. ;;;","03/Apr/12 04:30;junrao;Yes, we can use another jira to see how we can improve kafka server startup and shutdown. For this jira, we can just make minimal changes in kafka server.;;;","06/Apr/12 19:54;nehanarkhede;Filed KAFKA-328 for improving startup and shutdown API of Kafka server.

Kept everything in v2 minus the complexity of changes in Kafka server.;;;","06/Apr/12 22:04;junrao;Overall, patch v3 looks good. I made a minor tweak of KafkaServer on top of v3. How does that look? You will need to:
1. apply patch v3
2. svn revert core/src/main/scala/kafka/server/KafkaServer.scala
3. apply patch kafka-320-v3-delta.patch;;;","07/Apr/12 00:39;nehanarkhede;After applying the v3-delta patch, I see that some zkclient connections are not closed cleanly - 
[2012-04-06 17:34:12,805] WARN Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running (org.apache.zookeeper
.server.NIOServerCnxn:639)
[2012-04-06 17:34:12,809] WARN Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running (org.apache.zookeeper
.server.NIOServerCnxn:639)
[2012-04-06 17:34:13,201] WARN EndOfStreamException: Unable to read additional data from client sessionid 0x1368a38c37a0005, likely client has clos
ed socket (org.apache.zookeeper.server.NIOServerCnxn:634)
[2012-04-06 17:34:13,264] WARN EndOfStreamException: Unable to read additional data from client sessionid 0x1368a38a86a0080, likely client has clos
ed socket (org.apache.zookeeper.server.NIOServerCnxn:634)

Also, after you set the isShutdown variable and before you check canStart, there can be interleaving between startup and shutdown, that can lead to open zookeeper client connection.;;;","07/Apr/12 02:05;junrao;Ok, we can take v3 for now and track the broker startup/shutdown in kafka-328.;;;","08/Apr/12 02:00;nehanarkhede;Checked into trunk and 0.8 branch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incomplete message set validation checks in kafka.log.Log's append API can corrupt on disk log,KAFKA-310,12547095,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,19/Mar/12 21:33,22/Mar/12 16:04,14/Jul/23 05:39,22/Mar/12 16:04,0.7,,,,,,,,,,,,,0,,,,"The behavior of the ByteBufferMessageSet's iterator is to ignore and return false if some trailing bytes are found that cannot be de serialized into a Kafka message. The append API in Log, iterates through a ByteBufferMessageSet and validates the checksum of each message. Though, while appending data to the log, it just uses the underlying ByteBuffer that forms the ByteBufferMessageSet. Now, due to some bug, if the ByteBuffer has some trailing data, that will get appended to the on-disk log too. This can cause corruption of the log.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-308,,,,,,,"22/Mar/12 15:42;nehanarkhede;kafka-310-v2.patch;https://issues.apache.org/jira/secure/attachment/12519452/kafka-310-v2.patch","21/Mar/12 23:23;nehanarkhede;kafka-310.patch;https://issues.apache.org/jira/secure/attachment/12519362/kafka-310.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,232253,,,Thu Mar 22 16:04:26 UTC 2012,,,,,,,,,,"0|i0rsb3:",160230,,,,,,,,,,,,,,,,,,,,"19/Mar/12 21:34;nehanarkhede;This can potentially cause the log corruption described in KAFKA-308;;;","21/Mar/12 23:23;nehanarkhede;In this patch, Log's append API truncates the ByteBufferMessageSet to validBytes before appending its backing byte buffer to the FileChannel.;;;","22/Mar/12 15:26;junrao;ByteBufferMessageSet.validBytes currently makes a deep iteration of all messages, which means that we need to decompress messages. To avoid this overhead, we should change ByteBufferMessageSet.validBytes to use a shallow iterator.;;;","22/Mar/12 15:42;nehanarkhede;That's true. This was probably overlooked KAFKA-277. Fixed it and uploaded v2.;;;","22/Mar/12 16:00;junrao;+1 on v2.;;;","22/Mar/12 16:04;nehanarkhede;Thanks for the review. Committed this to trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in FileMessageSet's append API can corrupt on disk log,KAFKA-309,12546933,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,18/Mar/12 01:14,22/Mar/12 16:59,14/Jul/23 05:39,22/Mar/12 16:59,0.7,,,,,,,,,,core,,,0,,,,"In FileMessageSet's append API, we write a ByteBufferMessageSet to a log in the following manner -

    while(written < messages.sizeInBytes)
      written += messages.writeTo(channel, 0, messages.sizeInBytes)

In ByteBufferMessageSet, the writeTo API uses buffer.duplicate() to append to a channel -

  def writeTo(channel: GatheringByteChannel, offset: Long, size: Long): Long =
    channel.write(buffer.duplicate)

If the channel doesn't write the ByteBuffer in one call, then we call it again until sizeInBytes bytes are written. But the next call will use buffer.duplicate() to write to the FileChannel, which will write the entire ByteBufferMessageSet again to the file. 

Effectively, we have a corrupted set of messages on disk. 

Thinking about it, FileChannel is a blocking channel, so ideally, the entire ByteBuffer should be written to the FileChannel in one call. I wrote a test (attached here) and saw that it does. But I'm not aware if there are some corner cases when it doesn't do so. In those cases, Kafka will end up corrupting on disk log segment.
",,kzadorozhny,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-308,,,,,,,"18/Mar/12 01:18;nehanarkhede;kafka-309-test.patch;https://issues.apache.org/jira/secure/attachment/12518815/kafka-309-test.patch","22/Mar/12 00:03;nehanarkhede;kafka-309.patch;https://issues.apache.org/jira/secure/attachment/12519384/kafka-309.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,232091,,,Thu Mar 22 16:08:31 UTC 2012,,,,,,,,,,"0|i0rsav:",160229,,,,,,,,,,,,,,,,,,,,"18/Mar/12 01:18;nehanarkhede;The test includes a FileChannelTest that writes byte buffer of varying lengths to a file channel in a single call and checks if the buffer was completely written.;;;","19/Mar/12 21:34;nehanarkhede;This can potentially cause the log corruption described in KAFKA-308;;;","22/Mar/12 00:03;nehanarkhede;This patch changes the writeTo API of the ByteBufferMessageSet to use the message set's buffer to write to the FileChannel. The writeTo API does *not* change the underlying buffer's position marker. 

The right fix might be to not call ByteBufferMessageSet's writeTo in a loop in FileMessageSet's append API, since the guarantee of a blocking channel would not allow it to return without writing the entire message set or throwing an error. But that fix is arguably higher risk, so punting it for now, until we fully understand the guarantees of FileChannel;;;","22/Mar/12 16:08;junrao;+1 on the patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor server code to remove interdependencies between LogManager and KafkaZooKeeper,KAFKA-307,12546701,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,16/Mar/12 01:14,23/Mar/12 20:20,14/Jul/23 05:39,23/Mar/12 20:20,0.7,0.8.0,,,,,,,,,,,,0,,,,"Currently, LogManager wraps KafkaZooKeeper which is meant for all zookeeper interaction of a Kafka server. With replication, KafkaZookeeper will handle leader election, various state change listeners and then start replicas. Due to interdependency between LogManager and KafkaZookeeper, starting replicas is not possible until LogManager starts up completely. Due to this, we have to separate the broker startup procedures required for replication to get around this problem.

It will be good to refactor and clean up the server code, before diving deeper into replication.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-45,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/12 23:05;nehanarkhede;kafka-307-draft.patch;https://issues.apache.org/jira/secure/attachment/12519342/kafka-307-draft.patch","22/Mar/12 19:26;nehanarkhede;kafka-307-v2.patch;https://issues.apache.org/jira/secure/attachment/12519491/kafka-307-v2.patch","23/Mar/12 00:09;nehanarkhede;kafka-307-v3.patch;https://issues.apache.org/jira/secure/attachment/12519551/kafka-307-v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,231859,,,Fri Mar 23 20:20:08 UTC 2012,,,,,,,,,,"0|i09m9j:",54040,,,,,,,,,,,,,,,,,,,,"21/Mar/12 23:06;nehanarkhede;Attaching a patch for this refactoring. Broadly, it includes the following changes -

1. LogManager and KafkaZooKeeper are decoupled and don't depend on each other

2. KafkaServer maintains replicas, instead of LogManager
;;;","22/Mar/12 16:49;junrao;Overall looks good. Some suggestions:

1. KafkaServer:
Replicas are important data structures on each broker. Could we create a separate class Replicas that manage all replicas needed on a broker? Also, I see 2 different apis for adding a replica data structure. For replicas physically assigned to a broker, they always need a local Log. So, when adding those replicas, we need an api that creates a new log, if it's not there already (e.g. for newly created topics). The rest of replica data structures needed on a broker are not physically assigned to this broker and they are used to track the progress of the replicas in the followers. When adding those replicas, we need another api that doesn't force the creation of a local log.

2. log4j.properties: Do we really want to turn off logging for all kafka during unit tests? How about keeping it at ERROR level. We can probably turn off logging for ZK.;;;","22/Mar/12 19:26;nehanarkhede;Thanks for the review !

1. I like your suggestion. I wasn't quite satisfied with the way replicas were being managed. Incorporated the suggested changes. Looks much better now.

2. Didn't mean to check it in. But yes, I think zookeeper should be on ERROR and so should Kafka.;;;","22/Mar/12 23:30;junrao;Thanks for patch v2. Looks cleaner. A couple of other comments:

3. Is it better for KafkaZooKeeper to call ReplicaManager.addLocalReplica directly, instead of going through KafkaServer? For all replicas added in KafkaZookeeper, we know they are all local and should have a log. We could call LogManager.getOrCreateLog to get the log and pass it to ReplicaManager.

4. There are unused imports in KafkaServer.;;;","22/Mar/12 23:34;nehanarkhede;3. Yeah, I thought about this, but then that requires LogManager to be referenced inside KafkaZooKeeper again. So, thats why it goes through KafkaServer. The point was to have only KafkaServer have access to LogManager, ReplicaManager and KafkaZookeeper, but not have those 3 components have interdependencies. What do you think ?

4. Will clean it up before committing the patch.;;;","23/Mar/12 00:09;nehanarkhede;3. Fixed addReplica logic to use getOrCreateLog instead of getLog
4. Cleaned up the imports;;;","23/Mar/12 16:20;junrao;+1 on patch v3.;;;","23/Mar/12 20:20;nehanarkhede;Thanks for the review ! Committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
broker failure system test broken on replication branch,KAFKA-306,12546609,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jfung,nehanarkhede,nehanarkhede,15/Mar/12 16:33,10/Jul/12 18:06,14/Jul/23 05:39,10/Jul/12 18:06,0.8.0,,,0.8.0,,,,,,,,,,0,replication,,,"The system test in system_test/broker_failure is broken on the replication branch. This test is a pretty useful failure injection test that exercises the consumer rebalancing feature, various replication features like leader election. It will be good to have this test fixed as well as run on every checkin to the replication branch",,jfung,jjkoshy,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-45,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/12 16:15;jfung;kafka-306-v1.patch;https://issues.apache.org/jira/secure/attachment/12530225/kafka-306-v1.patch","20/Jun/12 16:18;jfung;kafka-306-v2.patch;https://issues.apache.org/jira/secure/attachment/12532716/kafka-306-v2.patch","20/Jun/12 20:41;jfung;kafka-306-v3.patch;https://issues.apache.org/jira/secure/attachment/12532759/kafka-306-v3.patch","21/Jun/12 17:53;jfung;kafka-306-v4.patch;https://issues.apache.org/jira/secure/attachment/12532912/kafka-306-v4.patch","27/Jun/12 21:28;jfung;kafka-306-v5.patch;https://issues.apache.org/jira/secure/attachment/12533709/kafka-306-v5.patch","03/Jul/12 21:31;jfung;kafka-306-v6.patch;https://issues.apache.org/jira/secure/attachment/12534985/kafka-306-v6.patch","05/Jul/12 23:00;jfung;kafka-306-v7.patch;https://issues.apache.org/jira/secure/attachment/12535280/kafka-306-v7.patch","06/Jul/12 19:56;jfung;kafka-306-v8.patch;https://issues.apache.org/jira/secure/attachment/12535410/kafka-306-v8.patch","10/Jul/12 16:54;jfung;kafka-306-v9.patch;https://issues.apache.org/jira/secure/attachment/12535871/kafka-306-v9.patch",,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,231767,,,Tue Jul 10 18:06:45 UTC 2012,,,,,,,,,,"0|i029n3:",11163,,,,,,,,,,,,,,,,,,,,"16/Mar/12 17:23;nehanarkhede;KAFKA-45 marks the start of server side replication related code changes. I think this test is a pretty good sanity check, if not a complete system testing suite. I would prefer having this fixed before accepting more patches on 0.8 branch. ;;;","30/May/12 16:15;jfung;Broker Failure Test is broken in Kafka 0.8 branch. This patch is fixing the issues and contains the following changes:
1. All server_*.properties are updated such that the first brokerid is starting from '0'
2. All mirror_producer*.properties are updated to use zk.connect (and not broker.list)
3. After the source brokers cluster is started, call kafka.admin.CreateTopicCommand to create topic.

Currently this patch is working with branch 0.8 (rev. 1342841 patched with KAFKA-46) with the following workarounds:

1. Before starting the target brokers cluster, start and stop one target broker to eliminate the following error:
       org.I0Itec.zkclient.exception.ZkNoNodeException: 
       org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /brokers/ids

2. The argument ""--consumer-timeout-ms"" doesn't seem to work properly. The consumer processes will be terminated manually

3. Consumer Lag info is not available from Zookeeper. Therefore, extra sleep time is added to the test to wait for the complete consumption of messages

The above issues are being investigated.;;;","12/Jun/12 16:37;junrao;Sorry, just got to review this. Trunk has moved, could you rebase? 

For 1, do you know the cause of this? Is this a bug? If so, please create a jira.

For 2,3, we just merged some changes from trunk to 0.8. Could you retry and see if this works now?
;;;","20/Jun/12 16:18;jfung;Uploaded kafka-306-v2.patch for branch 0.8 with the following changes:

1. Removed the worked around code and comments for NoNodeException (which is not reproducible with the latest 0.8 code).
2. The script can take a command line argument to bounce any combination of source broker, target broker and mirror maker in a round-robin fashion.
3. Use ""info"", ""kill_child_processes"" methods from a common script ""system_test/common/util.sh"".
4. Updated README.;;;","20/Jun/12 17:55;junrao;Thanks for patch v2. Some comments:

21. run_test.sh
21.1 Does the following check in start_test() need to be repeated for source, mirror and target, or can we just use 1 check for all 3 cases?
            if [[ $num_iterations -ge $iter && $svr_idx -gt 0 ]]; then
                echo
                info ""==========================================""
                info ""Iteration $iter of ${num_iterations}""
                info ""==========================================""
21.2 Do we need to sleep for 30s at the end start_test()? Isn't calling wait_for_zero_consumer_lags enough? Also, the comment says sleep for 10s.
21.3 In the header, we should add that mirror make can be terminated too.
21.4 If the test fails, could we generate a list of missing messages in a file? Ideally, messages can just be strings with sequential numbers in them.

22. The following test seems to fail sometimes.
bin/run-test.sh 2 23

23. README: We should add that one needs to do ./sbt package at the root level first.


;;;","20/Jun/12 20:41;jfung;Uploaded kafka-306-v3.patch with the following changes:

1. Set the server_source*.properties - log file size to approx 10MB:
log.file.size=10000000

2. Set the server_target*.properties - log file size to approx 10MB:
log.file.size=10000000;;;","21/Jun/12 17:53;jfung;Hi Jun,

Thanks for reviewing kafka-306-v2.patch.

kafka-306-v4.patch is uploaded with the following changes suggested by you:

21.1 The following check is required for each of the source, target and mirror maker. It is because the following 2 lines are needed for:
    Line 1: find out if the $bounce_source_id is a char in the string $svr_to_bounce
    Line 2: check to see if $num_iterations is already reached and if $svr_idx > 0 (meaning this server needs to be bounced)

    Line 1:        svr_idx=`expr index $svr_to_bounce $bounce_source_id`
    Line 2:        if [[ $num_iterations -ge $iter && $svr_idx -gt 0 ]]; then

21.2 ConsumerOffsetChecker needs to be enhanced for 0.8 and it depends on KAFKA-313. ""sleep"" is temporarily used for kafka to catch up with the offset lags.

21.3 The header is now updated to ""#### Starting Kafka Broker / Mirror Maker Failure Test ####""

21.4 There is a file ""checksum.log"" generated at the end of the test which will give the checksums found in producer, source consumer, target consumer logs

22. You may see inconsistent failure in this test due to the issue specified in KAFKA-370

23. README is updated with the steps for ./sbt package

Thanks,
John
;;;","22/Jun/12 16:58;junrao;Thanks for patch v4. A few more comments:

21.2 KAFA-313 adds 2 more options, which option does this jira depends on?

21.3 I meant that we should add mirror maker in the following line in the header:
# 5. One of the Kafka SOURCE or TARGET brokers in the cluster will
#    be randomly terminated and waiting for the consumer to catch up.

21.4 Instead of using checksum, can we use the message string itself? This makes it a bit easier to figure out the missing messages, if any.

22. Just attached a patch to kafka-370. Could you give it a try?
;;;","27/Jun/12 21:28;jfung;** In replying to Jun's question about KAFKA-313: in this script, the function ""wait_for_zero_consumer_lag"" is calling ConsumerOffsetChecker to get the Consumer lag value. However, the topic-partition info is changed in 0.8 and it's not returned correctly in ConsumerOffsetChecker. Please refer to this comment: https://issues.apache.org/jira/browse/KAFKA-313?focusedCommentId=13397990&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13397990

** Uploaded kafka-306-v5.patch. Changes made in kafka-306-v5.patch:

1. In ""initialize"" function, added code to find the location of the zk & kafka log4j log files.

2. In ""cleanup"" function, added code to remove the zk & kafka log4j log files

3. The header of the script is now removed and the description are in README

4. Use getopt to process command line arguments

5. Consolidated the following functions:

    * start_console_consumer_for_source_producer
    * start_console_consumer_for_mirror_producer
    * wait_for_zero_source_console_consumer_lags
    * wait_for_zero_mirror_console_consumer_lags

6. The file to notify producer to stop:
The producer is sent to the background to run in a while-loop. If a file is used to notify the producer process in the background, the producer will exit properly inside the while loop.

7. The following check is required for each of the source, target and mirror maker. It is because the following 2 lines are needed for:

    * Line 1: find out if the $bounce_source_id is a char in the string $svr_to_bounce
    * Line 2: check to see if $num_iterations is already reached and if $svr_idx > 0 (meaning this server needs to be bounced)

    * Line 1: svr_idx=`expr index $svr_to_bounce $bounce_source_id`
    * Line 2: if [[ $num_iterations -ge $iter && $svr_idx -gt 0 ]]; then;;;","03/Jul/12 21:31;jfung;Uploaded kafka-306-v6.patch and made further changes in ProducerPerformance and ConsoleConsumer to support producing sequential message IDs such that it would be easier to troubleshoot data loss.

ProducerPerformance.scala

    * Added command line option ""--seq-id-starting-from"". This option enable ""seqIdMode"" with the following changes:

    * Every message will be tagged with a sequential message ID such that all IDs are unique
    * Every message will be sent by its own producer thread sequentially
    * Each producer thread will use a unique range of numbers to give sequential message IDs
    * All message IDs are leftpadded with 0s for easier troubleshooting
    * Extra characters are added to the message to make up the required message size

ConsoleConsumer.scala

    * Added DecodedMessageFormatter class to display message contents

run-test.sh

    * Modified to use the enhanced ProducerPerformance and ConsoleConsumer
    * Validate ""MessageID"" instead of ""checksum"" for data matching between source and target consumers

;;;","04/Jul/12 01:45;jjkoshy;John, thanks for the patch. The test script itself looks good - as we
discussed on the other jira we can do further cleanup separately. Here are
some comments on the new changes:

ProducerPerformance:
- seqIdStartFromopt -> startId or initialId would be more
  convenient/intuitive.
- May be better not to describe the message format in detail in the help
  message. I think the template: ""Message:000..1:xxx..."" is good enough.
- On line 136, 137 I think you mean if (options.has) and not
  if(!options.has) - something odd there. Can you double-check?
- Try to avoid using vars if possible. vals are generally clearer and safer
  - for example,
  val isFixSize = options.has(seqIdStartFromOpt) || !options.has(varyMessageSizeOpt)
  val numThreads = if (options.has(seqIdStartFromOpt)) 1 else options.valueOf(numThreadsOpt).intValue()
  etc.
- For user-specified options that you override can you log a warning?
- Instead of the complicated padding logic I think you can get it for free
  with Java format strings - i.e., specify the width/justification of each
  column in the format string. That would be much easier I think.
- numThreads override to 1 -> did it work to prefix the id with thread-id
  and allow > 1 thread?

Server property files:
- send/receive.buffer.size don't seem to be valid config options - may be
  deprecated by the socket buffer size settings, but not sure.

Util functions:

- Small suggestion: would be better to echo the result than return. So you
  can have: idx=$(get_random_range ...) which is clearer than
  get_random_range; idx=$? . Also, non-zero bash returns typically indicate
  an error.
;;;","05/Jul/12 23:00;jfung;Hi Joel,

Thanks for reviewing. I just uploaded kafka-306-v7.patch with the changes you suggested:

ProducerPerformance
===================
* seqIdStartFromopt -> startId or initialId would be more convenient/intuitive.
- Changed

* May be better not to describe the message format in detail in the help message. I think the template: ""Message:000..1:xxx..."" is good enough.
- Changed

* On line 136, 137 I think you mean if (options.has) and not if(!options.has) - something odd there. Can you double-check?
- In ""seqIdMode"", if ""numThreadsOpt"" is not specified, numThreads default to 1. Otherwise, it will take the user specified value

* Try to avoid using vars if possible. vals are generally clearer and safer, for example,
  val isFixSize = options.has(seqIdStartFromOpt) || !options.has(varyMessageSizeOpt)
  val numThreads = if (options.has(seqIdStartFromOpt)) 1 else options.valueOf(numThreadsOpt).intValue()
- This is because the values may be overridden later by user specified values. Therefore, some of the val is changed to var

* For user-specified options that you override can you log a warning?
- Changed

* Instead of the complicated padding logic I think you can get it for free with Java format strings - i.e., specify the width/justification of each column in the format string. That would be much easier I think.
- Changed

* numThreads override to 1 -> did it work to prefix the id with thread-id and allow > 1 thread?
- numThreads will be overridden if ""--threads"" is specified in command line arg


Server property files
=====================
* send/receive.buffer.size don't seem to be valid config options - may be deprecated by the socket buffer size settings, but not sure.
- Changed

Util functions
==============
* Small suggestion: would be better to echo the result than return.
So you can have: idx=$(get_random_range ...) which is clearer than get_random_range; idx=$? .
Also, non-zero bash returns typically indicate an error.
- Changed
;;;","06/Jul/12 19:56;jfung;Uploaded kafka-306-v8.patch.

The changes made in the previous patch (kafka-306-v7.patch) will break single_host_multi_brokers/bin/run-test.sh due to the fact that ProducerPerformance will no longer print the message checksum.

The changes made in this patch supports single_host_multi_brokers/bin/run-test.sh to make use of the sequential message ID for test results validation.;;;","06/Jul/12 20:08;jjkoshy;Thanks for making the changes - looks better.

> - This is because the values may be overridden later by user specified
> values. Therefore, some of the val is changed to var 

I meant even with overrides I don't think you need these vars and they can 
be handled better with vals. However, it's a minor issue and looking at
ProducerPerformance it seems it needs an overhaul - the main loop is pretty hard to 
read. We should probably do that in a separate jira as it isn't directly
related to this one.

BTW, it seems bytesSent is not updated in seqIdMode.
;;;","10/Jul/12 00:43;junrao;Patch v8 looks good overall. Some minor comments on ProducerPerformance:

81. Could we default numThreadsOpt to 1? Then we can get rid of the following override.

      if (!options.has(numThreadsOpt)) { 
        numThreads = 1 
        warn(""seqIdMode - numThreads is overridden to: "" + numThreads)
      }

82. Could we replace the following code
            if (config.seqIdMode) {
              producer.send(new ProducerData[Message,Message](config.topic, null, message))
            }
            else if(!config.isFixSize) {
     with
            if(!config.isFixSize || !config.seqIdMode) {;;;","10/Jul/12 16:54;jfung;Thanks Jun for reviewing. Your suggestion are made in kafka-306-v9.patch.

The changes are:
91. numThreadsOpt is defaulted to 1 and the 'if' block is removed

92. The following block is actually not necessary and it's now removed:
          if (config.seqIdMode) { 
              producer.send(new ProducerData[Message,Message](config.topic, null, message)) 
            } ;;;","10/Jul/12 18:06;junrao;John, thanks for patch v9. Removed the commented out code in ProducerPerformance and committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SyncProducer does not correctly timeout,KAFKA-305,12546581,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,prashanth.menon,prashanth.menon,prashanth.menon,15/Mar/12 13:11,02/Apr/12 01:48,14/Jul/23 05:39,26/Mar/12 17:19,0.7,0.8.0,,0.8.0,,,,,,,core,,,0,,,,So it turns out that using the channel in SyncProducer like we are to perform blocking reads will not trigger socket timeouts (though we set it) and will block forever which is bad.  This bug identifies the issue: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4614802 and this article presents a potential work-around: http://stackoverflow.com/questions/2866557/timeout-for-socketchannel for workaround. The work-around is a simple solution that involves creating a separate ReadableByteChannel instance for timeout-enabled reads.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-49,,KAFKA-295,,,,,,,,,,,,,,,"26/Mar/12 01:45;prashanth.menon;BlockingChannel2.scala;https://issues.apache.org/jira/secure/attachment/12519889/BlockingChannel2.scala","20/Mar/12 02:04;prashanth.menon;KAFKA-305-v1.patch;https://issues.apache.org/jira/secure/attachment/12519005/KAFKA-305-v1.patch","23/Mar/12 13:57;prashanth.menon;KAFKA-305-v2.patch;https://issues.apache.org/jira/secure/attachment/12519623/KAFKA-305-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,231739,,,Mon Apr 02 01:48:37 UTC 2012,,,,,,,,,,"0|i09m9b:",54039,,,,,,,,,,,,,,,,,,,,"16/Mar/12 17:16;nehanarkhede;Produce ACK should not be blocking. It probably makes sense to fix this before closing KAFKA-49.;;;","19/Mar/12 02:15;prashanth.menon;I, unfortunately, didn't get a chance to work on this over the weekend.  From my point of view, creating a new ReadableByteChannel that wraps the socket channel InputStream seems like the simplest solution.  Then the SyncProudcer will have a writeChannel (the SocketChannel) and a readChannel (the wrapped version).  All writes and reads go through the respective channels with the additional of timeout functionality.  

Another step we can take it is to move all that logic into some class BlockingChannel which can be reused on the consumer side in SimpleConsumer.  Such a class would have, perhaps, four methods: connect, disconnect, send and receive.  Connect and disconnect would be synchronized, send would take a Request object and receive would return a Tuple2[Receive, Int] like usual.  Send and receive will need to be synchronized externally, meaning the class can be effectively treated like a regular Channel otherwise ...

Thoughts?;;;","20/Mar/12 02:00;prashanth.menon;Hi all, I've attached a patch.  Some notes:

- New class called BlockingChannel that has timeouts enabled.
- SyncProducer uses BlockingChannel instead of creating its own SocketChannel
- Re-introducsed testZKSendWithDeadBroker which passes now.

I'd like to get feedback on this.  It's simple and may be reused on the consumer side.  When I think about it, it would be nice to combine SimpleConsumer and SyncProducer into one generic ""SimpleClient"" since the functionality is effectively the same.

I'd also like to benchmark this against a pure NIO implementation where we can use selectors to enabled timeout functionality.  It'll be more complex and will require minor adjustment to BoundedByteBuffer and BoundedByteBufferSend but it may be worth it.;;;","21/Mar/12 00:12;junrao;Prashanth,

Thanks for the patch. This is very useful. Some comments:

1. I think it makes sense for SimpleConsumer to use BlockingChannel as well. Could you change that in this patch too?
2. ProducerTest.testZKSendWithDeadBroker: This test doesn't really test the timeout on getting a response. We probably need to create a mock kafkaserver (that don't send a response) to test this out.
3. BlockingChannel: 
3.1 We probably should rename timeoutMs to readTimeoutMs since only reads are subject to the timeout.
3.2 We should pass in a socketSendBufferSize and a socketReceiveBufferSize.
3.3 Should host and port be part of the constructor? It seems to me it's cleaner if each instance of BlockingChannel is tied to 1 host and 1 port.

I'd also be interested in your findings on the comparison with NIO with selectors.
;;;","22/Mar/12 02:18;prashanth.menon;Thanks for review, Jun.

1. Will do.
2. So that test actually exposed the issue to begin with - the initial send would fail and then hang forever when attempting to refresh the topic metadata.  Regardless, I'll create a separate more direct test for timeouts.  On my local machine, this test seems to be unpredictable around 30% of the time.  In these cases, it seems like the ephemeral broker nodes aren't removed from ZK and bringing back up a broker after shutdown throws a ""Broker already exists"" exception.  Is anyone else experiencing it or just me?  Increasing the wait time after shutdown helps but not 100%.
3. 1,2,3 Sounds fair.

I should be able to get a patch in for this by Friday.  Then continue on KAFKA-49 over the weekend and get it in on Saturday or Sunday should the review go okay.  Apologies for the delays :(;;;","22/Mar/12 15:01;junrao;Prashant,

2. If you want to make sure that a broker is shut down, you need to call kafkaServer.awaitShutdown after calling kafkaServer.shutdown. Overall, I don't quite understand how the new test works. It only brought down 1 broker and yet the comment says all brokers are down. If it is indeed that all brokers are down, any RPC call to the broker should get a broken pipe or socket closed exception immediately, not a sockettimeout exception. So, to really test that the timeout works, we need to keep the broker alive and somehow delay the response from the server. This can probably be done with a mock request handler.;;;","22/Mar/12 15:32;nehanarkhede;Prashanth,

Thanks for the patch. A couple of suggestions -

1. Since you are adding a new abstraction, BlockingChannel, would it make sense to change SimpleConsumer to use it ? Its your call if you'd rather fix it in another JIRA.
2. In BlockingChannel, since you are synchronizing on a lock, any reason the connected boolean be a volatile ? Also, you can avoid resetting the read and write channels to null values in disconnect.
3. Lets add some more tests for this, since it is unclear if the workaround of wrapping input stream in a channel actually works or not. I like Jun's suggestion of mocking out the request handler to achieve this. Tests would include SyncProducer as well as async producer (DefaultEventHandler);;;","23/Mar/12 02:54;prashanth.menon;I've uploaded a new patch with the suggestions, but it's not ready for commit, just another review.  A few notes:

1. BlockingChannel modified to meet suggestions.
2. SimpleConsumer uses BlockingChannel.
3. To test the BlockingChannel (in SyncProducer and async producer), I bring up a regular server but shutdown the requesthandler.  So the socket remains open, accepts requests and queues them in the request channel, but there are no handlers processing requests.
4. The original testZKSendWithDeadBroker wasn't commented entirely correctly.  I've modified to actually test what the name suggests.
5. Though I wait for the broker to do down, testZKSendWithDeadBroker still unpredictably throws the ""Broker already registered"" exception.  Are you experiencing this locally?

I think there might be an issue with the BrokerPartitionInfo and ProduerPool classes.  ProducerPool never removes producers even if one is connected to a downed broker, so calls to getAnyProducer (used by BrokerPartitioninfo.updateInfo to update cached topic metadata information) could return the same ""bad"" producer on consecutive calls when attempting to refresh the cache.  This could potentially cause an entire send to fail though there may exist a broker that is able to service the topic metadata request.  We need to somehow, remove ""bad"" producers, or refresh the ProducerPool when brokers go down, or have BrokerPartitionInfo retry its updateInfo call a certain number of times.  Thoughts?;;;","23/Mar/12 16:57;junrao;Prashanth,

v2 patch looks good. 

As for 5, I do see transient failures of testZKSendWithDeadBroker. This is a bit weird. During broker shutdown, we close the ZK client, which should cause all ephemeral nodes to be deleted in ZK. Could you verify if this is indeed the behavior of ZK?

As for BrokerPartitionInfo and ProducerPool, we should clean up dead brokers. Could you open a separate jira to track that?;;;","23/Mar/12 18:58;nehanarkhede;v2 looks good. 

Regarding the test failure, I debugged it and see a probable bug with either Zookeeper or ZkClient. See below - 

[info] Test Starting: testZKSendWithDeadBroker(kafka.producer.ProducerTest)
Shutting down broker 0
[2012-03-23 11:50:36,870] DEBUG Deleting ephemeral node /brokers/ids/0 for session 0x13640e55f240013 (org.apache.zookeeper.server.DataTree:831)
[2012-03-23 11:50:36,873] DEBUG Deleting ephemeral node /brokers/topics/new-topic/partitions/3/leader for session 0x13640e55f240013 (org.apache.zookeeper.server.DataTree:831)
[2012-03-23 11:50:36,873] DEBUG Deleting ephemeral node /brokers/topics/new-topic/partitions/1/leader for session 0x13640e55f240013 (org.apache.zookeeper.server.DataTree:831)
[2012-03-23 11:50:36,873] DEBUG Deleting ephemeral node /brokers/topics/new-topic/partitions/2/leader for session 0x13640e55f240013 (org.apache.zookeeper.server.DataTree:831)
[2012-03-23 11:50:36,873] DEBUG Deleting ephemeral node /brokers/topics/new-topic/partitions/0/leader for session 0x13640e55f240013 (org.apache.zookeeper.server.DataTree:831)
Shut down broker 0
Restarting broker 0
[2012-03-23 11:50:45,194] DEBUG Deleting ephemeral node /brokers/ids/1 for session 0x13640e55f24001b (org.apache.zookeeper.server.DataTree:831)
[error] Test Failed: testZKSendWithDeadBroker(kafka.producer.ProducerTest)
java.lang.RuntimeException: A broker is already registered on the path /brokers/ids/0. This probably indicates that you either have configured a brokerid that is already in use, or else you have shutdown this broker and restarted it faster than the zookeeper timeout so it appears to be re-registering.
	at kafka.utils.ZkUtils$.registerBrokerInZk(ZkUtils.scala:109)
	at kafka.server.KafkaZooKeeper.kafka$server$KafkaZooKeeper$$registerBrokerInZk(KafkaZooKeeper.scala:60)
	at kafka.server.KafkaZooKeeper.startup(KafkaZooKeeper.scala:52)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:84)
	at kafka.producer.ProducerTest.testZKSendWithDeadBroker(ProducerTest.scala:173)

Notice that after shutting down broker 0, the ephemeral node was deleted from its in memory data tree. That happens part of the close session workflow. Still, when we try to create the ephemeral node again, it complains that it already exists. 

I'll come back to this zookeeper bug later. I'd say lets checkin this test since it helps reproduce this zk bug. 

I think your patch looks good. ;;;","23/Mar/12 19:32;prashanth.menon;Thanks for the input everyone.  Regarding the ZK failure, that is effectively the trace I'm seeing on my end as well - the log makes it clear that the ephemeral nodes get deleted but the test still fails when creating them afterwards.  

I would like to delay commiting this patch, atleast for the weekend, as I'd like to perform a little benchmark against a pure NIO implementation.  The benefits there would be having timeouts for both read and write operations and a potential performance boost.;;;","23/Mar/12 20:22;junrao;If this is indeed a ZK issue, we can probably check/wait that the ephemeral node is gone before restarting the broker.;;;","26/Mar/12 01:45;prashanth.menon;I've attached another non-blocking implementation that uses selectors, but I'm not seeing any significant performance boost on my machine.  I tested it on the producer side using the ProducerPerformance class by varying the number of messages, the message sizes and the number of threads.  Each test scenario was run four times and the average result was used.  Find the results here: https://gist.github.com/2202142.  

For what it's worth, I think we should go ahead with the simple solution attached in the v2 path - if everyone is okay with it, please commit.  Regarding the test error, it could potentially be a valid ZK or ZKClient bug.  I can investigate a little by digging into ZKClient and asking around the mailing list and channels.  Keeping the test in breaks the test unpredictably.  Thought I'm not entirely okay with it keeping the bug in, waiting for the node to go down doesn't seem to be the right solution either.  ;;;","26/Mar/12 17:19;junrao;Prashanth,

Thanks for the patch. I agree that v2 is less risky than the selector approach. So, we can revisit the selector approach later. Thanks for the patch though and it will probably be useful in the future. Committed v2 patch to 0.8 branch with the following minor changes in DefaultEventHandler:
* log all unsent messages
* maintain outstandingRequests properly on both successful and unsuccessful sends.

Could you file 2 jiras, one for taking out dead brokers in ProducerPool and another for transient failures due to ZK ephemeral node not deleted in time?;;;","26/Mar/12 21:36;nehanarkhede;I found the zookeeper related problem, filed KAFKA-320 and also included a patch.;;;","27/Mar/12 00:22;prashanth.menon;Awesome!;;;","02/Apr/12 01:48;prashanth.menon;KAFKA-300 and KAFKA-305 ticket together resolve KAFKA-295.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change broker request and response to use Seqs rather than Array,KAFKA-299,12545763,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,prashanth.menon,prashanth.menon,prashanth.menon,09/Mar/12 02:56,07/Feb/15 23:51,14/Jul/23 05:39,07/Feb/15 23:51,0.8.0,,,,,,,,,,core,,,0,0.8,replication,wireprotocol,"The new Produce and Fetch request and response classes use primitive Arrays, but becaue they are case classes and Java's array hashCode/equals functionality is broken, the case class equality contract is broken as well.  We should change the models to use Seqs to resolve the issue along with gaining all the functional benefits that goes along with it.  This change will require appropriate Java versions to convert between Array's and Seqs for Java clients.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-240,,,,KAFKA-240,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,230946,,,Fri Apr 06 17:29:53 UTC 2012,,,,,,,,,,"0|i029rj:",11183,,,,,,,,,,,,,,,,,,,,"06/Apr/12 17:29;nehanarkhede;Assigning to you Prashanth, since you mentioned you want to resolve this;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in async producer DefaultEventHandler retry logic,KAFKA-295,12545382,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,prashanth.menon,prashanth.menon,prashanth.menon,06/Mar/12 18:00,02/Apr/12 01:50,14/Jul/23 05:39,02/Apr/12 01:49,0.8.0,,,0.8.0,,,,,,,core,,,0,,,,"In the DefaultEventHandler's retry loop, the logic should not return after a successful retry.  Rather, it should set a boolean flag indicating that the retry was successful and exit or break the while loop.  In the end it should throw an exception only the flag is false.  Otherwise, it should continue the outer for loop and send remaining data to remaning brokers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-305,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,230568,,,Mon Apr 02 01:49:15 UTC 2012,,,,,,,,,,"0|i09m93:",54038,,,,,,,,,,,,,,,,,,,,"13/Mar/12 01:04;prashanth.menon;This is a small but important bug.  I can drop a patch since I've been touching that code anyways.  It will probably come in after KAFKA-49.;;;","02/Apr/12 01:49;prashanth.menon;Incorporated as part of KAFKA-300 and KAFKA-305.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Path length must be > 0"" error during startup",KAFKA-294,12545274,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,tomdz,tomdz,06/Mar/12 03:47,20/Jul/15 14:41,14/Jul/23 05:39,01/Oct/14 22:12,,,,0.8.2.0,,,,,,,,,,0,,,,"When starting Kafka 0.7.0 using zkclient-0.1.jar, I get this error:

INFO 2012-03-06 02:39:04,072  main kafka.server.KafkaZooKeeper Registering broker /brokers/ids/1
FATAL 2012-03-06 02:39:04,111  main kafka.server.KafkaServer Fatal error during startup.
java.lang.IllegalArgumentException: Path length must be > 0
        at org.apache.zookeeper.common.PathUtils.validatePath(PathUtils.java:48)
        at org.apache.zookeeper.common.PathUtils.validatePath(PathUtils.java:35)
        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:620)
        at org.I0Itec.zkclient.ZkConnection.create(ZkConnection.java:87)
        at org.I0Itec.zkclient.ZkClient$1.call(ZkClient.java:308)
        at org.I0Itec.zkclient.ZkClient$1.call(ZkClient.java:304)
        at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)
        at org.I0Itec.zkclient.ZkClient.create(ZkClient.java:304)
        at org.I0Itec.zkclient.ZkClient.createPersistent(ZkClient.java:213)
        at org.I0Itec.zkclient.ZkClient.createPersistent(ZkClient.java:223)
        at org.I0Itec.zkclient.ZkClient.createPersistent(ZkClient.java:223)
        at kafka.utils.ZkUtils$.createParentPath(ZkUtils.scala:48)
        at kafka.utils.ZkUtils$.createEphemeralPath(ZkUtils.scala:60)
        at kafka.utils.ZkUtils$.createEphemeralPathExpectConflict(ZkUtils.scala:72)
        at kafka.server.KafkaZooKeeper.registerBrokerInZk(KafkaZooKeeper.scala:57)
        at kafka.log.LogManager.startup(LogManager.scala:124)
        at kafka.server.KafkaServer.startup(KafkaServer.scala:80)
        at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:47)
        at kafka.Kafka$.main(Kafka.scala:60)
        at kafka.Kafka.main(Kafka.scala)

The problem seems to be this code in ZkClient's createPersistent method:

String parentDir = path.substring(0, path.lastIndexOf('/'));
createPersistent(parentDir, createParents);
createPersistent(path, createParents);

which doesn't check for whether parentDir is an empty string, which it will become for /brokers/ids/1 after two recursions.
",,anandnalya,githubbot,gwenshap,jbrosenberg,jbrosenberg@gmail.com,jkreps,junrao,lanzaa,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,230460,,,Mon Jul 20 14:41:27 UTC 2015,,,,,,,,,,"0|i029uf:",11196,,,,,,,,,,,,,,,,,,,,"06/Mar/12 15:33;junrao;Are you using namespace in ZK connection string? If so, the typical problem is that the namespace is not present. You have to manually create the namespace in ZK.;;;","06/Mar/12 16:55;tomdz;Ah I see, this is very non-descriptive error then. Maybe you could add this to the documentation/FAQ (or make the error more descriptive) ?;;;","08/Jun/12 19:33;jkreps;I think we can fix this so it gives a more intuitive error message that explains the problem. No one will be able to figure this out otherwise.;;;","09/May/13 05:05;jbrosenberg;This issue happens also in 0.8.0
It would be even better, if the chroot is not present in zk, that it be automatically created, thus avoiding this issue altogether.;;;","09/May/13 16:29;junrao;We can auto-create it, but then we can't prevent config mistakes. We can probably start by just providing a more meaningful error. One way is to just catch IllegalArgumentException with that message and covert it to a more meaningful exception (and message). Jason, you want to give this a shot?;;;","01/Oct/14 22:12;gwenshap;With KAFKA-404 committed, this is resolved too.;;;","20/Jul/15 14:41;githubbot;Github user fsaintjacques closed the pull request at:

    https://github.com/apache/kafka/pull/2
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
broker deletes all file segments when cleaning up an empty log segment,KAFKA-292,12545024,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,03/Mar/12 01:46,18/Mar/12 18:35,14/Jul/23 05:39,18/Mar/12 18:35,,,,0.7.1,,,,,,,core,,,0,,,,"Suppose that a log has only one log segment left and it's empty. If that segment expires and is deleted, we roll out a new segment of the same name. However, the deletion happens after the log is rolled. This will make the log directory empty, which should never happen.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/12 01:47;junrao;kafka-292.patch;https://issues.apache.org/jira/secure/attachment/12516922/kafka-292.patch","05/Mar/12 23:25;junrao;kafka-292_v2.patch;https://issues.apache.org/jira/secure/attachment/12517154/kafka-292_v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,230210,,,Sun Mar 18 18:35:44 UTC 2012,,,,,,,,,,"0|i0l3p3:",121249,,,,,,,,,,,,,,,,,,,,"03/Mar/12 01:48;junrao;patch attached.;;;","05/Mar/12 22:30;nehanarkhede;It will be very useful to have a unit test for this. Will make it easier to catch this in the future.;;;","05/Mar/12 23:25;junrao;Attach patch v2. There is already a unit test for this. Just need to add the necessary checking.;;;","06/Mar/12 01:21;nehanarkhede;+1 on v2. Good catch !;;;","18/Mar/12 18:35;junrao;Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running a consumer client using scala 2.8 fails,KAFKA-287,12544314,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,elben,elben,27/Feb/12 21:24,07/Feb/15 23:52,14/Jul/23 05:39,07/Feb/15 23:52,,,,,,,,,,,,,,0,,,,"Built the kafka library using the instructions found in the README. My client uses scala 2.9.1, sbt 0.11. My consumer client has this snippet of code: https://gist.github.com/a35006cc25e39ba386e2

The client compiles, but running it produces this stacktrace: https://gist.github.com/efeb85f50402b477d6e0

I think this may be because of a bug found in scala 2.9.0 (though I'm not sure if it was present in scala 2.8.0): https://issues.scala-lang.org/browse/SI-4575

To get around this, I built the kafka library using scala 2.9.1 (by changing build.properties).","Java 1.6, OS X",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,229551,,,Tue Feb 28 20:20:20 UTC 2012,,,,,,,,,,"0|i029wf:",11205,,,,,,,,,,,,,,,,,,,,"28/Feb/12 20:20;charmalloc;I run Kafka in production using Scala 2.9.1 changing build.properties and creating a new build

I run sbt 11.2 for all of my code but use the existing sbt in the Kafka project for building kafka.

KAFKA-134 tries to address an upgrade to 0.10.1 which I tried once but ran into an issue.  we should look at KAFKA-134 and making it be 11.2 (should not be much more than what was already in 0.10.1 changes)

do you want to take a look into KAFKA-134 ? I can review when you are done..   I think this ticket though is covered in KAFKA-134 changes.

the workaround is exactly what you did, change build.properties to 2.9.1 and rebuild.  works great just some warnings otherwise have no issue in production;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consumer sometimes don't release partition ownership properly in ZK during rebalance,KAFKA-286,12544143,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,25/Feb/12 22:59,27/Feb/12 19:52,14/Jul/23 05:39,27/Feb/12 19:52,,,,0.7.1,,,,,,,core,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/12 23:01;junrao;kafka-286.patch;https://issues.apache.org/jira/secure/attachment/12516069/kafka-286.patch","27/Feb/12 15:57;junrao;kafka-286_v2.patch;https://issues.apache.org/jira/secure/attachment/12516172/kafka-286_v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,229380,,,Mon Feb 27 19:52:35 UTC 2012,,,,,,,,,,"0|i0l41b:",121304,,,,,,,,,,,,,,,,,,,,"25/Feb/12 23:01;junrao;Patch attached.;;;","27/Feb/12 05:40;nehanarkhede;Wondering if the patch can get the consumer into the following state -

Say, there are 2 consumers in a group, c1 and c2. Both are consuming topic1 with partitions 0-0, 0-1 and 1-0. Say c1 owns 0-0 and 0-1 and c2 owns 1-0. 

1. Broker 1 goes down. This triggers rebalancing attempt in c1 and c2. 
2. c1 releases partition ownership, but fails to rebalance. 
3. Meanwhile, c2 completes rebalancing successfully, and owns partition 0-1 and starts consuming data. 
4. c1 starts next rebalancing attempt and it releases partition 0-1 (since 0-1 is still part of topicRegistry). It owns partition 0-0 again, and starts consuming data. 
5. Effectively, rebalancing has completed successfully, but there is no owner for partition 0-1 registered in Zookeeper. 

I think using the topicRegistry cache is dangerous, since it has to be in sync with the ownership information in zookeeper. How about reading the ownership information from ZK along with the other data and only release that ?
;;;","27/Feb/12 15:57;junrao;That's a good point. What can happen is that we may delete ZK paths that c1 didn't successfully own in step 2, if rebalance fails. Added patch v2 that deletes all temporarily owned ZK paths in reflectPartitionOwnershipDecision, if we can't own everything. I think this fix addresses this issue.;;;","27/Feb/12 18:29;nehanarkhede;That's a good change and will handle the majority of failure cases. There is another failure case, I think still needs to be fixed in the rebalancing -

Say, for the above mentioned scenario, c1 fails to rebalance due to some error/exception that exercises this code path -

          try {
            done = rebalance(cluster)
          }
          catch {
            case e =>
              /** occasionally, we may hit a ZK exception because the ZK state is changing while we are iterating.
               * For example, a ZK node can disappear between the time we get all children and the time we try to get
               * the value of a child. Just let this go since another rebalance will be triggered.
               **/
              info(""exception during rebalance "", e)
          }

After this, c1 only closes its fetcher queues and backs off (0-0 and 0-1 are already released), while c2 owns 0-1.
Then during step 4 above, c1 releases things from its topic registry again which contains 0-0 and 0-1. So it releases 0-1, which it does not own anymore

;;;","27/Feb/12 18:44;junrao;Will this happen? It doesn't seem possible to me. In step 2, when we release 0-0 and 0-1 during rebalance, we clear topicRegistry. Since this rebalance fails, topicRegistry will not be populated. So, in step 4, there is nothing to release for c1.;;;","27/Feb/12 19:02;nehanarkhede;Yes, missed the fact that after releasing the partitions, it also gets deleted from the passed in topic registry. Looks good now, we will have to be careful about keeping this topic registry cache in sync at all times though. I like the idea of refreshing the cache from ZK during each rebalance attempt, but we can look into that later. 

+1 on v2.;;;","27/Feb/12 19:52;junrao;Thanks for the review. Just committed this to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C++ client does not compile,KAFKA-284,12543957,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Trivial,Fixed,,nieksand,nieksand,24/Feb/12 06:14,01/Mar/12 22:24,14/Jul/23 05:39,01/Mar/12 22:24,0.7,,,,,,,,,,clients,,,0,,,,"clients/cpp/src/encoder.hpp has an unclosed comment on line 19.


/*
 * encoder.hpp
 *

Needs to be

/*
 * encoder.hpp
 */
",g++ 4.4.5 on linux,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/12 02:45;charmalloc;KAFKA-284.patch;https://issues.apache.org/jira/secure/attachment/12516638/KAFKA-284.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,229195,,,Thu Mar 01 02:47:22 UTC 2012,,,,,,,,,,"0|i0rsbb:",160231,,,,,,,,,,,,,,,,,,,,"28/Feb/12 20:39;charmalloc;Niek do you want to upload a patch for this?;;;","28/Feb/12 20:47;charmalloc;I noticed another thing also

autoconf.sh

does not have execute on it;;;","01/Mar/12 02:37;nieksand;Index: clients/cpp/src/encoder.hpp
===================================================================
--- clients/cpp/src/encoder.hpp	(revision 1295383)
+++ clients/cpp/src/encoder.hpp	(working copy)
@@ -16,7 +16,7 @@
 */
 /*
  * encoder.hpp
- *
+ */
 
 #ifndef KAFKA_ENCODER_HPP_
 #define KAFKA_ENCODER_HPP_;;;","01/Mar/12 02:45;charmalloc;+1

uploaded the patch as file for convenience;;;","01/Mar/12 02:47;charmalloc;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Currently iterated chunk is not cleared during consumer shutdown,KAFKA-282,12543765,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jjkoshy,jjkoshy,jjkoshy,22/Feb/12 23:30,25/Jun/15 07:05,14/Jul/23 05:39,07/Feb/15 23:53,,,,,,,,,,,core,,,0,,,,"During consumer connector shutdown, fetch queues are cleared, but the currently iterated chunk is not cleared.",,msprunck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-2302,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,229004,,,Wed Jun 24 22:55:54 UTC 2015,,,,,,,,,,"0|i029wn:",11206,,,,,,,,,,,,,,,,,,,,"24/Jun/15 22:55;msprunck;I still have this issue with the API version 0.8.1.1. During the connector shutdown and after offsets have been committed, the ConsumerIterator give me more messages.

What was the resolution to this issue ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka-console-producer does not take in customized values of --batch-size or --timeout,KAFKA-279,12543270,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,junrao,milindparikh,milindparikh,19/Feb/12 19:04,21/Mar/12 00:25,14/Jul/23 05:39,21/Mar/12 00:25,0.7,,,0.7.1,,,,,,,contrib,,,0,,,,"1. While the default console-producer, console-consumer paradigm works great, when I try modiying the batch size

bin/kafka-console-producer.sh --batch-size 300   --zookeeper localhost:2181 --topic test1

it gives me a

Exception in thread ""main"" java.lang.NumberFormatException: null
    at java.lang.Integer.parseInt(Integer.java:443)
    at java.lang.Integer.parseInt(Integer.java:514)
    at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:207)
    at scala.collection.immutable.StringOps.toInt(StringOps.scala:31)
    at kafka.utils.Utils$.getIntInRange(Utils.scala:189)
    at kafka.utils.Utils$.getInt(Utils.scala:174)
    at kafka.producer.async.AsyncProducerConfigShared$class.$init$(AsyncProducerConfig.scala:45)
    at kafka.producer.ProducerConfig.<init>(ProducerConfig.scala:25)
    at kafka.producer.ConsoleProducer$.main(ConsoleProducer.scala:108)
    at kafka.producer.ConsoleProducer.main(ConsoleProducer.scala)

I have looked at the code and can't figure out what's wrong

2. When I do bin/kafka-console-producer.sh --timeout 30000   --zookeeper localhost:2181 --topic test1

I would think that console-producer would wait for 30s if the batch size (default 200) is not full. It doesn't. It takes the same time without the timeout parameter (default 1000) and dumps whatever the batch size.


Resolution from Jun

1. The code does the following to set batch size
     props.put(""batch.size"", batchSize)
Instead, it should do
     props.put(""batch.size"", batchSize.toString)

2. It sets the wrong property name for timeout. Instead of doing
   props.put(""queue.enqueueTimeout.ms"", sendTimeout.toString)
it should do
   props.put(""queue.time"", sendTimeout.toString)
","Ubuntu 10.04, openjdk1.6 with default installation of 0.7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/12 22:33;junrao;kafka-279.patch;https://issues.apache.org/jira/secure/attachment/12515291/kafka-279.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,228516,,,Wed Mar 21 00:25:38 UTC 2012,,,,,,,,,,"0|i0l37j:",121170,,,,,,,,,,,,,,,,,,,,"20/Feb/12 22:33;junrao;Milind, 

Attached is a patch to trunk. Could you try it out and see if it fixes your problem?;;;","27/Feb/12 16:48;archs;I tried the patch and issuing the same command get the following stack trace part-way through an import of a 100000 line file (--batchsize 300):
[2012-02-27 16:44:37,911] ERROR Event queue is full of unsent messages, could not send event: 7300043|103|60|1329080400|en|1987973|269118099490000000000000103153898086 (kafka.producer.async.AsyncProducer)
Exception in thread ""main"" kafka.producer.async.QueueFullException: Event queue is full of unsent messages, could not send event: 7300043|103|60|1329080400|en|1987973|269118099490000000000000103153898086
        at kafka.producer.async.AsyncProducer.send(AsyncProducer.scala:121)
        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$2.apply(ProducerPool.scala:131)
        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$2.apply(ProducerPool.scala:131)
        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
        at scala.collection.immutable.List.foreach(List.scala:45)
        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1.apply(ProducerPool.scala:131)
        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1.apply(ProducerPool.scala:130)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:130)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:102)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:102)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:102)
        at kafka.producer.Producer.zkSend(Producer.scala:143)
        at kafka.producer.Producer.send(Producer.scala:105)
        at kafka.producer.ConsoleProducer$.main(ConsoleProducer.scala:120)
        at kafka.producer.ConsoleProducer.main(ConsoleProducer.scala)
;;;","27/Feb/12 17:51;junrao;That typically means that you are sending data at a rate faster than the broker can persist. Try increasing flush.interval to sth like 10000 on the broker to increase server throughput.;;;","02/Mar/12 23:31;nehanarkhede;Milind, does the attached patch fix your problem ?;;;","19/Mar/12 17:07;ers81239;I tested this and verified that this fixes the error for --batch-size.  ;;;","21/Mar/12 00:25;junrao;Edward,

Thanks for the review. Just committed the patch to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issues partitioning a new topic,KAFKA-278,12543113,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,mabateman,mabateman,17/Feb/12 22:57,11/Jul/13 22:43,14/Jul/23 05:39,11/Jul/13 22:43,0.7,,,,,,,,,,core,,,0,,,,"There are two cases where correct partitioning fails for a new topic.

Case 1: Topic exists on current Kafka cluster. A new broker is added to the cluster. The new broker will never host partitions for the existing topic.

To reproduce:
1) Create a cluster of brokers along with a ZooKeeper ensemble.
2) Send messages for a topic to the cluster.
3) Add a new broker to the cluster.
4) New broker will never see the existing topic.

Case 2: Topic does not exist on current Kafka cluster. Producer sends messages to a new topic that did not previously exist in the cluster. If, during the producer session, one or more partitions are not created on a broker, the broker will never host those partitions.

To reproduce:
1) Create a cluster of brokers along with a ZooKeeper ensemble.
2) Send messages to a new topic.
3) Shut down the producer before the topic is created on at least one broker.
4) The broker that did not allocate the topic will never host the topic.

My guess(!) here is that when a new producer is created, it gets a list of topics and partitions based on the current state of the brokers in the cluster. Since some brokers are missing the topic, the producer will never send messages to that broker and partitions will never be created.


Work around:
Manually create the topic/partition directories in the kafka logs directory and reboot kafka. It will register the topic/partitions in ZooKeeper.",,baroquebobcat,ianfriedman,jkreps,junrao,rangadi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/12 00:27;baroquebobcat;bootstrap_new_brokers.patch;https://issues.apache.org/jira/secure/attachment/12538463/bootstrap_new_brokers.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,228399,,,Thu Jul 11 22:43:33 UTC 2013,,,,,,,,,,"0|i15zgf:",243044,,,,,,,,,,,,,,,,,,,,"18/Feb/12 00:50;mabateman;I changed the description and title to reflect that there are 2 ways that this condition can arise.;;;","16/Mar/12 00:51;mabateman;Refined the steps to reproduce.;;;","16/Mar/12 01:02;mabateman;Added a work around.;;;","31/Jul/12 00:27;baroquebobcat;Here's a patch we applied to our fork to deal with this issue.

What it does is bootstrap new brokers in the same way existing brokers are bootstrapped for new topics.

It includes a test.;;;","18/Jun/13 19:30;ianfriedman;Hi, I didn't realize it was expected for multiple brokers to split partitions of a single topic in 0.7? Can anyone point me to where that behavior is documented?;;;","20/Jun/13 04:22;junrao;Basically, in 0.7, a partition is local to a broker. So, if you send data to a broker, a partition for that topic will be automatically created in that broker.;;;","11/Jul/13 22:43;jkreps;This should be fixed in 0.8 where we handle partitioning properly.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add a shallow iterator to the ByteBufferMessageSet, which is only used in SynchProducer.verifyMessageSize() function",KAFKA-277,12543112,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,yeyangever,yeyangever,yeyangever,17/Feb/12 22:41,23/Feb/12 22:59,14/Jul/23 05:39,23/Feb/12 22:59,,,,0.7.1,,,,,,,,,,0,,,,"Shallow iterator just traverse the first level messages of a ByteBufferMessageSet, compressed messages won't be decompressed and treated individually ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/12 21:59;yeyangever;internal_iterator_with_unit_test.patch;https://issues.apache.org/jira/secure/attachment/12515826/internal_iterator_with_unit_test.patch","17/Feb/12 22:45;yeyangever;shallow_iterator.patch;https://issues.apache.org/jira/secure/attachment/12515040/shallow_iterator.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,228398,,,Thu Feb 23 22:59:50 UTC 2012,,,,,,,,,,"0|i0rsbz:",160234,,,,,,,,,,,,,,,,,,,,"17/Feb/12 22:45;yeyangever;The lastConnectionTime adjustment is also in this patch.

Also the following file is also affected. Curious, in the repository copy, the variable ""event"" has no context (an variable not defined), only when I change it to events the make process can succeed. 

--- core/src/main/scala/kafka/producer/async/DefaultEventHandler.scala  (revision 1245727)
+++ core/src/main/scala/kafka/producer/async/DefaultEventHandler.scala  (working copy)
@@ -38,8 +38,8 @@
       processedEvents = cbkHandler.beforeSendingData(events)

     if(logger.isTraceEnabled)
-      processedEvents.foreach(event => trace(""Handling event for Topic: %s, Partition: %d""
-        .format(event.getTopic, event.getPartition)))
+      processedEvents.foreach(events => trace(""Handling event for Topic: %s, Partition: %d""
+        .format(events.getTopic, events.getPartition)))

     send(serialize(collate(processedEvents), serializer), syncProducer)
   }
;;;","20/Feb/12 22:08;junrao;Some comments:
1. The variable event just binds to every item in the sequence by the foreach method. There is no need to rename it since each item in processedEvents is supposed to be a single event.
2. In ByteBufferMessageSet, instead of duplicating code in shallowIterator, could we rename deepIterator to internalIterator and add a flag to control whether we want to do shallow iteration or deep iteration? In general, we don't want to expose the shallow iterator externally. So, it's better if we just add a verifyMessageSize method in ByteBufferMessageSet that uses shallow iterator.
 ;;;","21/Feb/12 17:04;junrao;Also, please add a unit test for this. Use a max_message size larger than each individual uncompressed message, but smaller than the compressed message.;;;","23/Feb/12 21:59;yeyangever;in ByteBufferMessageSet, internal_iterator() is built with one flag to control the deep or shallow behavior. verifyMessageSize() function is moved as an member function. 

Unit test is built as a separate function in SynchProducerTest.scala ;;;","23/Feb/12 22:59;junrao;Thanks for the patch. It looks good. Just committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
max.message.size is not enforced for compressed messages,KAFKA-275,12542981,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,nehanarkhede,nehanarkhede,17/Feb/12 01:56,17/Aug/17 12:02,14/Jul/23 05:39,17/Aug/17 12:02,0.7,,,,,,,,,,core,,,1,,,,"The max.message.size check is not performed for compressed messages, but only for each message that forms a compressed message. Due to this, even if the max.message.size is set to 1MB, the producer can technically send n 1MB messages as one compressed message. This can cause memory issues on the server as well as deserialization issues on the consumer. The consumer's fetch size has to be > max.message.size in order to be able to read data. If one message is larger than the fetch.size, the consumer will throw an exception and cannot proceed until the fetch.size is increased. 

Due to this bug, even if the fetch.size > max.message.size, the consumer can still get stuck on a message that is larger than max.message.size.",,omkreddy,sslavic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-273,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,228267,,,Thu Aug 17 12:02:41 UTC 2017,,,,,,,,,,"0|i15zfz:",243042,,,,,,,,,,,,,,,,,,,,"17/Feb/12 02:22;nehanarkhede;In corner cases, this can cause KAFKA-273;;;","17/Aug/17 12:02;omkreddy;This issue is fixed in latest versions.  Please reopen if the issue still exists. 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Occassional GZIP errors on the server while writing compressed data to disk,KAFKA-273,12542954,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,16/Feb/12 22:22,17/Aug/17 12:00,14/Jul/23 05:39,17/Aug/17 12:00,0.7,,,,,,,,,,core,,,1,,,,"Occasionally, we see the following errors on the Kafka server -

2012/02/08 14:58:21.832 ERROR [KafkaRequestHandlers] [kafka-processor-6] [kafka] Error processing MultiProducerRequest on NusImpressionSetEvent:0
java.io.EOFException: Unexpected end of ZLIB input stream
        at java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:223)
        at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:141)
        at java.util.zip.GZIPInputStream.read(GZIPInputStream.java:92)
        at java.io.FilterInputStream.read(FilterInputStream.java:90)
        at kafka.message.GZIPCompression.read(CompressionUtils.scala:52)
        at kafka.message.CompressionUtils$$anonfun$decompress$1.apply$mcI$sp(CompressionUtils.scala:143)
        at kafka.message.CompressionUtils$$anonfun$decompress$1.apply(CompressionUtils.scala:143)
        at kafka.message.CompressionUtils$$anonfun$decompress$1.apply(CompressionUtils.scala:143)
        at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:598)
        at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:598)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:555)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:549)
        at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:394)
        at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:394)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:555)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:549)
        at scala.collection.immutable.Stream.foreach(Stream.scala:255)
        at kafka.message.CompressionUtils$.decompress(CompressionUtils.scala:143)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNextOuter(ByteBufferMessageSet.scala:119)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:132)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:81)
        at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:59)
        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:51)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:30)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
        at kafka.message.MessageSet.foreach(MessageSet.scala:87)
        at kafka.log.Log.append(Log.scala:204)
        at kafka.server.KafkaRequestHandlers.kafka$server$KafkaRequestHandlers$$handleProducerRequest(KafkaRequestHandlers.scala:70)
        at kafka.server.KafkaRequestHandlers$$anonfun$handleMultiProducerRequest$1.apply(KafkaRequestHandlers.scala:63)
        at kafka.server.KafkaRequestHandlers$$anonfun$handleMultiProducerRequest$1.apply(KafkaRequestHandlers.scala:63)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.mutable.ArrayOps.map(ArrayOps.scala:34)
        at kafka.server.KafkaRequestHandlers.handleMultiProducerRequest(KafkaRequestHandlers.scala:63)
        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$4.apply(KafkaRequestHandlers.scala:42)
        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$4.apply(KafkaRequestHandlers.scala:42)
        at kafka.network.Processor.handle(SocketServer.scala:297)
        at kafka.network.Processor.read(SocketServer.scala:320)
        at kafka.network.Processor.run(SocketServer.scala:215)
        at java.lang.Thread.run(Thread.java:619)
",,jkreps,omkreddy,sslavic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-274,KAFKA-275,,,,,,,,,,,,,,"06/Mar/12 19:09;nehanarkhede;kafka-273.patch;https://issues.apache.org/jira/secure/attachment/12517289/kafka-273.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,228240,,,Thu Aug 17 12:00:09 UTC 2017,,,,,,,,,,"0|i15zfr:",243041,,,,,,,,,,,,,,,,,,,,"06/Mar/12 17:56;nehanarkhede;We've seen this error very occasionally, but in our deployments the Deflator uses jdk-1.6.0.21 and zlib-1.2.3 on Linux and jdk-1.6.0_16 and zlib-1.2.3 on Solaris. And the Inflator uses jdk-1.6.0.21 and zlib-1.2.3 on Linux.

According to this Java bug - http://bugs.sun.com/bugdatabase/view_bug.do;jsessionid=e8f7802ea035813254fc6aba9bf0?bug_id=6519463, the bug is fixed by using a combination of zlib1.23 and jdk7-b72

Here is the code snippet from InflatorInputStream.java - 
  157                   if (inf.needsInput()) {
  158                       fill();
  159                   }

This bug occurs when the native Inflator on the platform indicates there are more bytes to decompress, when there aren't any. So, the InflatorInputStream.read() calls fill() based on that, where it throws EOFException(). 

The workaround seems to be catching the EOFException in CompressionUtils.decompress and do nothing.
;;;","06/Mar/12 18:39;junrao;We have also seem the following gzip issue. Is that the same issue since it's not triggered by EOF?

 ERROR [CompressionUtils$] [kafka-processor-0] [kafka] Error while reading from the GZIP input stream
java.io.IOException: Corrupt GZIP trailer
        at java.util.zip.GZIPInputStream.readTrailer(GZIPInputStream.java:182)
        at java.util.zip.GZIPInputStream.read(GZIPInputStream.java:94)
        at java.io.FilterInputStream.read(FilterInputStream.java:90)
        at kafka.message.GZIPCompression.read(CompressionUtils.scala:52)
        at kafka.message.CompressionUtils$$anonfun$decompress$1.apply$mcI$sp(CompressionUtils.scala:143)
        at kafka.message.CompressionUtils$$anonfun$decompress$1.apply(CompressionUtils.scala:143)
        at kafka.message.CompressionUtils$$anonfun$decompress$1.apply(CompressionUtils.scala:143)
        at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:598)
        at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:598)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:555)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:549)
        at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:394)
        at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:394)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:555)
        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:549)
        at scala.collection.immutable.Stream.foreach(Stream.scala:255)
        at kafka.message.CompressionUtils$.decompress(CompressionUtils.scala:143)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNextOuter(ByteBufferMessageSet.scala:119)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:132)
        at kafka.message.ByteBufferMessageSet$$anon$1.makeNext(ByteBufferMessageSet.scala:81)
        at kafka.utils.IteratorTemplate.maybeComputeNext(IteratorTemplate.scala:59)
        at kafka.utils.IteratorTemplate.hasNext(IteratorTemplate.scala:51)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:30)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
        at kafka.message.MessageSet.foreach(MessageSet.scala:87)
        at kafka.log.Log.append(Log.scala:204)
        at kafka.server.KafkaRequestHandlers.kafka$server$KafkaRequestHandlers$$handleProducerRequest(KafkaRequestHandlers.scala:70)
        at kafka.server.KafkaRequestHandlers$$anonfun$handleMultiProducerRequest$1.apply(KafkaRequestHandlers.scala:63)
        at kafka.server.KafkaRequestHandlers$$anonfun$handleMultiProducerRequest$1.apply(KafkaRequestHandlers.scala:63)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.ArrayOps.foreach(ArrayOps.scala:34)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.mutable.ArrayOps.map(ArrayOps.scala:34)
        at kafka.server.KafkaRequestHandlers.handleMultiProducerRequest(KafkaRequestHandlers.scala:63)
        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$4.apply(KafkaRequestHandlers.scala:42)
        at kafka.server.KafkaRequestHandlers$$anonfun$handlerFor$4.apply(KafkaRequestHandlers.scala:42)
        at kafka.network.Processor.handle(SocketServer.scala:297)
        at kafka.network.Processor.read(SocketServer.scala:320)
        at kafka.network.Processor.run(SocketServer.scala:215)
        at java.lang.Thread.run(Thread.java:619)
;;;","06/Mar/12 19:09;nehanarkhede;Changed CompressionUtils.decompress to handle EOFException and return -1 from the read API;;;","06/Mar/12 23:33;nehanarkhede;Jun, I am not sure its the same issue. See this - http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4262583

The Java bugs mention that it happens due to very large compressed or uncompressed data (larger than 2GB). Not sure how Kafka server can get into that situation, since the request size is 100 MB. ;;;","07/Mar/12 17:42;junrao;The patch for EOF looks fine. We probably need to do some system test to make sure this doesn't introduce new problems, especially when the compressed size is relatively large. Once that test is done. We can commit the patch.;;;","12/Mar/12 20:07;nehanarkhede;Have run the system test with message size = 100K, batch size = 200 and compression turned on. It passed.;;;","11/Jul/13 22:43;jkreps;Is this still happening?;;;","17/Aug/17 12:00;omkreddy; We have seen this issues recently. Pl reopen if you think the issue still exists 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in the consumer rebalancing logic causes one consumer to release partitions that it does not own,KAFKA-262,12540955,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,02/Feb/12 17:36,20/Oct/14 14:08,14/Jul/23 05:39,09/Feb/12 22:05,0.7,,,0.7.1,,,,,,,core,,,0,,,,"The consumer maintains a cache of topics and partitions it owns along with the fetcher queues corresponding to those. But while releasing partition ownership, this cache is not cleared. This leads the consumer to release a partition that it does not own any more. This can also lead the consumer to commit offsets for partitions that it no longer consumes from. 

The rebalance operation goes through following steps -

1. close fetchers
2. commit offsets
3. release partition ownership. 
4. rebalance, add topic, partition and fetcher queues to the topic registry, for all topics that the consumer process currently wants to own. 
5. If the consumer runs into conflict for one topic or partition, the rebalancing attempt fails, and it goes to step 1.

Say, there are 2 consumers in a group, c1 and c2. Both are consuming topic1 with partitions 0-0, 0-1 and 1-0. Say c1 owns 0-0 and 0-1 and c2 owns 1-0.

1. Broker 1 goes down. This triggers rebalancing attempt in c1 and c2.
2. c1's release partition ownership and during step 4 (above), fails to rebalance.
3. Meanwhile, c2 completes rebalancing successfully, and owns partition 0-1 and starts consuming data.
4. c1 starts next rebalancing attempt and during step 3 (above), it releases partition 0-1. During step 4, it owns partition 0-0 again, and starts consuming data.
5. Effectively, rebalancing has completed successfully, but there is no owner for partition 0-1 registered in Zookeeper.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/12 02:14;nehanarkhede;kafka-262-v3.patch;https://issues.apache.org/jira/secure/attachment/12513896/kafka-262-v3.patch","06/Feb/12 23:20;nehanarkhede;kafka-262.patch;https://issues.apache.org/jira/secure/attachment/12513523/kafka-262.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,226308,,,Thu Feb 09 02:31:34 UTC 2012,,,,,,,,,,"0|i0l4sf:",121426,,,,,,,,,,,,,,,,,,,,"06/Feb/12 21:49;nehanarkhede;This patch removes the cache used by each consumer to decide whether or not it should trigger a rebalance operation. The reason being that it is very tricky to keep the cache updated in each participating consumer, leading to incorrect partition ownership decisions. 

This patch also changes the code to use the topicRegistry correctly. It is used to keep track of the fetcher queues for every topic and partition the consumer owns. Hence, whenever partition ownership is released, the relevant data needs to be deleted from the topic registry.;;;","06/Feb/12 23:20;nehanarkhede;Includes some cleanup. Removing oldPartitionsPerTopicMap and oldConsumersPerTopicMap.;;;","08/Feb/12 17:06;junrao;Some comments:

1. In ZookeeperConsumerConnector.reflectPartitionOwnershipDecision, the local variable success is not intuitive. It should be named to something like hasFailure.

2. In ZookeeperConsumerConnector.releasePartitionOwnership. It's not clear to me why this method has to take an input parameter. Wouldn't it be simpler to always release partition ownership according to topicRegistry?

;;;","08/Feb/12 18:27;nehanarkhede;1. Will change that before committing the patch
2. Maybe. This sounds like a doable optimization. Al though, the code is very complex, and each optimization needs to be thought through deeply and comes with extensive testing. I'd like to commit this patch, since it holds off KAFKA-253 and the release and fixes the bug. I can open a JIRA to address that optimization, or can put it as part of KAFKA-265. ;;;","09/Feb/12 00:12;junrao;2. If releasePartitionOwnership always just checks topicRegistry, the code and the logic will be a bit simpler. Could we run the system test and see if there is any issue with the simplification?;;;","09/Feb/12 01:53;nehanarkhede;1. Changed releasePartitionOwnership to not take in a map
2. Changed the name of the variable success to hasPartitionOwnershipFailed.;;;","09/Feb/12 02:31;junrao;In releasePartitionOwnership(), topicAndPartitionsToBeReleased is no longer used and should be removed. Otherwise, the patch looks good.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Corrupted request shuts down the broker,KAFKA-261,12540838,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,01/Feb/12 23:40,02/Feb/12 17:34,14/Jul/23 05:39,02/Feb/12 17:34,0.7,,,0.7.1,,,,,,,,,,0,,,,"Currently, a corrupted produce request brings down the broker. Instead, we should just log it and let it go.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/12 23:41;junrao;kafka-261.patch;https://issues.apache.org/jira/secure/attachment/12512869/kafka-261.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,226192,,,Thu Feb 02 17:34:20 UTC 2012,,,,,,,,,,"0|i0l37r:",121171,,,,,,,,,,,,,,,,,,,,"01/Feb/12 23:41;junrao;patch attached.;;;","02/Feb/12 03:04;jkreps;+1;;;","02/Feb/12 06:52;nehanarkhede;+1;;;","02/Feb/12 17:34;junrao;Just committed this to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Give better error message when trying to run shell scripts without having built/downloaded the jars yet,KAFKA-259,12540545,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,rossc,rossc,31/Jan/12 12:40,30/May/13 03:27,14/Jul/23 05:39,30/May/13 03:27,0.8.0,,,0.8.0,,,,,,,,,,0,newbie,,,"Hi there, I've cloned from the kafka github repo and tried to run the start server script:

 ./bin/kafka-server-start.sh config/server.properties 

Which results in:

Exception in thread ""main"" java.lang.NoClassDefFoundError: kafka/Kafka
Caused by: java.lang.ClassNotFoundException: kafka.Kafka
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)

It seems that Im missing a build step? what have I forgotten to do?

Thanks in advance and I look forward to using kafka.

regards
rcdh",Mac OSX Lion,ashwanthfernando@gmail.com,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/13 22:44;ashwanthfernando@gmail.com;KAFKA-259-v1.patch;https://issues.apache.org/jira/secure/attachment/12583908/KAFKA-259-v1.patch","29/May/13 23:10;ashwanthfernando@gmail.com;KAFKA-259-v2.patch;https://issues.apache.org/jira/secure/attachment/12585326/KAFKA-259-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,225958,,,Thu May 30 03:27:37 UTC 2013,,,,,,,,,,"0|i15zev:",243037,,,,,,,,,,,,,,,,,,,,"31/Jan/12 14:23;charmalloc;cd kafka
./sbt update
./sbt package

this will make the jars for you to be able to run the server as you are attempting to-do;;;","31/Jan/12 22:47;jkreps;This is covered in the README and the releases come with packaged jars. The only thing I think we could do better is error out if there are no jars in dist, let's change this bug to be about doing that.;;;","01/Feb/12 07:11;rossc;as a noob to java/scala I can honestly say that it may be covered in the readme but there is allot of forest in there and not many trees to be seen ;) its just the basics (and probably only form my point of view) :) thanks for your time and effort guys and keep up the great work! ;;;","01/Feb/12 17:23;jkreps;Yeah, I didn't mean that in a snotty way, just that if we version control the jars the java people get all sulky and complain that we aren't using maven to download them, but if we do that then the non-maven people are unhappy because nothing works.;;;","01/Feb/12 20:05;rossc;@jay, absolutely no problem mate, sorry if I sounded snooty ;) was not meant as such. As far as I can see maven +- sbt are really good tools. But for a rank noob with java its a case of figuring out the nomenclature and processes that is the java world lol ;) ;;;","19/May/13 07:42;ashwanthfernando@gmail.com;Hi, I have a patch for this, I am going through legal to get it approved. Will upload it here asap.;;;","20/May/13 22:45;ashwanthfernando@gmail.com;I have submitted a patch for this. Basically the patch checks whether the java process returns with an exit code of 1 (abnormal), and if it does checks the output of the java process to see whether there are NoClassDefFoundError or ""Could not find or load main class"" messages and then if it does, displays this message:

""Please build the project using sbt. Documentation is available at http://kafka.apache.org/""

Please let me know if you have any concerns with this approach.;;;","29/May/13 16:49;junrao;Thanks for the patch. It doesn't apply to 0.8 though. Could you provide another patch?

git apply ~/Downloads/KAFKA-259-v1.patch 
/Users/jrao/Downloads/KAFKA-259-v1.patch:21: trailing whitespace.
if [ $exitval -eq ""1"" ] ; then 
/Users/jrao/Downloads/KAFKA-259-v1.patch:27: trailing whitespace.
	if [[ -n ""$match"" ]]; then 
error: patch failed: bin/kafka-run-class.sh:81
error: bin/kafka-run-class.sh: patch does not apply
error: patch failed: bin/kafka-run-class.sh:93
error: bin/kafka-run-class.sh: patch does not apply
;;;","29/May/13 23:10;ashwanthfernando@gmail.com;[~junrao] - I executed the simple contributor workflow in this page (https://cwiki.apache.org/confluence/display/KAFKA/Git+Workflow) again. Attached the patch. Can you please try again?;;;","30/May/13 03:27;junrao;Thanks for patch v2. +1. Committed to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in the consumer rebalancing logic leads to the consumer not pulling data from some partitions,KAFKA-256,12539814,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,25/Jan/12 19:32,03/Feb/12 01:13,14/Jul/23 05:39,02/Feb/12 19:08,0.7,,,0.7.1,,,,,,,core,,,0,,,,"There is a bug in the consumer rebalancing logic that makes a consumer not pull data from some partitions for a topic. It recovers only after the consumer group is restarted and doesn't hit this bug again.

Here is the observed behavior of the consumer when it hits the bug -

1. Consumer is consuming 2 topics with 1 partition each on 2 brokers
2. Broker 2 is bounced
3. Rebalancing operation triggers for topic_2, where the consumer decides to now consume data only from Broker 1 for topic_2
4. During the rebalancing operation, ZK has not yet deleted the /brokers/topics/topic_1/broker_2, so the consumer still decides to consumer from both brokers for topic_1
5. While restarting the fetchers, it tries to restart fetcher for broker 2 and throws a RuntimeException. Before this, it has successfully started fetcher for broker 1 and is consuming data from broker_1
6. This exception trickles all the way upto syncedRebalance API and the oldPartitionsPerTopicMap does not get updated to reflect that for topic_2, the consumer has now seen only broker_1. It still points to topic_2 -> broker_1, broker_2
7. Next rebalancing attempt gets triggered
8. By now, broker 2 is restarted and registered in zookeeper
9. For topic_2, the consumer tries to see if rebalancing needs to be done. Since it doesn't see a change in the cached topic partition map, it decides there is no need to rebalance.
10. It continues fetching only from broker_1
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/12 01:03;nehanarkhede;kafka-256-v2.patch;https://issues.apache.org/jira/secure/attachment/12512061/kafka-256-v2.patch","28/Jan/12 01:27;nehanarkhede;kafka-256-v3.patch;https://issues.apache.org/jira/secure/attachment/12512267/kafka-256-v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,225317,,,Fri Feb 03 01:13:36 UTC 2012,,,,,,,,,,"0|i0l5pz:",121577,,,,,,,,,,,,,,,,,,,,"26/Jan/12 22:26;nehanarkhede;Changes include -

1. The bug was caused due to a stale cache problem

2. This patch fixes the bug by clearing the cache on every single unsuccessful rebalancing attempt. This includes exception during rebalancing OR failure to own one or more partitions 

3. A tool to verify if a consumer group successfully completed a rebalancing operation. 

4. To facilitate such a tool, partition ownership is split into 3 steps -

4.1 the consumer records its decision to own partitions that it has selected
4.2 the fetchers are started to start pulling data from the selected partitions
4.3 the above partition ownership decision is written to zookeeper

5. Cleanup to remove unrequired imports;;;","27/Jan/12 01:03;nehanarkhede;This patch applies cleanly to trunk;;;","28/Jan/12 01:27;nehanarkhede;A slight change here -

The fetchers are updated only after the partition ownership is reflected in zookeeper. This will reduce the possibility of duplicate data;;;","28/Jan/12 02:42;jjkoshy;+1 for v3. 

I like the idea of having a tool to check if a consumer is correctly balanced.
A more general comment/question on the kafka.tools package: I thought the tools
package is meant for stand-alone tools that people can run on the command-line,
whose output can be piped for further processing if desired.  If so, it would
be better not to use logging for the tool's output and simply println. 
;;;","29/Jan/12 18:33;nehanarkhede;You have a good point about the tools package. This tool is meant for use in running system tests (KAFKA-227) in verifying the correctness of Kafka. This means at the very least the output about whether the rebalancing attempt was successful or not, can be println and maybe the other info useful for debugging can be log4j ? If that makes sense, I'll make that change before committing this patch.

Thanks for reviewing this patch and catching the possible duplication issue in v2. ;;;","02/Feb/12 19:08;nehanarkhede;Committed this patch to trunk. Will fix KAFKA-262 separately.;;;","03/Feb/12 00:32;nehanarkhede;I found a bug in the v3 patch. The reflectPartitionOwnershipDecision API has a bug that doesn't set the value of partitionOwnershipSuccessful correctly. Will fix this as part of KAFKA-262.;;;","03/Feb/12 01:05;junrao;Some comments:
1. ZookeeperConsumerConnector.reflectPartitionOwnershipDecision, the following code seems incorrect.
      val success = partitionOwnershipSuccessful.foldLeft(0)((sum, decision) => if(decision) 0 else 1)
  The function in foldLeft should check both sum and decision. Also, the local variable success should be named to something like hasFailure.
2. ZookeeperConsumerConnector.syncedRebalance
  done = false in the catch clause is not necessary. If we hit an exception, done will be left with the initial value, which is false.
  The else after the following
         if (done) {
            return
          }
  is not necessary.
  Also, it seems there is no need to call commitOffset before closeFetchersFprQuues since the latter commits offsets already.
3. It seems that we don't need to check ownership registry in ZK in processPartition. The same check will be done later in reflectPartitionOwnershipDecision.
;;;","03/Feb/12 01:13;nehanarkhede;1. That is the bug I was referring to in my previous comment. 

2. done =false in the catch clause is to prevent a bug, in case the code elsewhere in the rebalance API changes in the future. These bugs are very hard to spot and time consuming to debug. This one-liner seems harmless since it could potentially save a lot of time.
Though, commitOffsets() can be skipped before closing the fetchers. 

3. This is also a good point, and seems like an over optimization. Will get rid of it as part of KAFKA-262.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
max.message.size and fetch.size defaults should be consistent,KAFKA-247,12538348,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,pyritschard,bmatheny,bmatheny,13/Jan/12 18:21,19/Jun/14 05:16,14/Jul/23 05:39,17/Jan/12 17:32,0.7,,,0.7.1,,,,,,,,,,0,newbie,,,"The default max.message.size for a producer is ~976kB. The default fetch.size for a consumer is 300kB. Having the default fetch.size less than the default max.message.size causes new users with messages larger than fetch.size to run into the InvalidMessageSizeException issue.

Making the default max.message.size less than or equal to the default fetch.size would eliminate that problem for most new setups.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/12 10:12;pyritschard;0001-Fix-KAFKA-247-by-bumping-fetch.size.patch;https://issues.apache.org/jira/secure/attachment/12510570/0001-Fix-KAFKA-247-by-bumping-fetch.size.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,223854,,,Tue Jan 17 17:32:17 UTC 2012,,,,,,,,,,"0|i0lo3r:",124555,,,,,,,,,,,,,,,,,,,,"14/Jan/12 10:12;pyritschard;The rationale here is to bump fetch.size a bit beyond the
default produce size.

Coincidentally, it seems that the MaxFetchSize property is used
nowhere throughout the code.;;;","17/Jan/12 17:32;junrao;Pierre-Yves, thanks for the patch. Took MaxFetchSize out and committed the patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsumerIterator throws a IllegalStateException after a ConsumerTimeout occurs,KAFKA-241,12537450,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,patricioe,patricioe,06/Jan/12 00:55,19/Jun/14 05:16,14/Jul/23 05:39,19/Jan/12 17:54,0.7,,,0.7.1,,,,,,,,,,0,newbie,,,"Please find the test case attached.

After a timeout occurs (property consumer.timeout.ms > 0 ) the consumerIterator throws an IllegalStateException.

The work around seems to be to recreate the MessageStream an issue a new Iterator.",,patricioe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/12 00:55;patricioe;consumerIteratorTestCase.java;https://issues.apache.org/jira/secure/attachment/12509640/consumerIteratorTestCase.java","19/Jan/12 00:48;junrao;kafka-241.patch;https://issues.apache.org/jira/secure/attachment/12511072/kafka-241.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,222958,,,Thu Jan 19 17:54:29 UTC 2012,,,,,,,,,,"0|i0rscf:",160236,,,,,,,,,,,,,,,,,,,,"19/Jan/12 00:48;junrao;Patch attached.;;;","19/Jan/12 00:49;junrao;Added a unit test that exposes the bug and a fix.;;;","19/Jan/12 01:33;jjkoshy;+1
;;;","19/Jan/12 17:54;junrao;Thanks for the review. Just committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The producer's load balancing logic can send requests to dead brokers, when using the async producer option",KAFKA-233,12537090,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,nehanarkhede,nehanarkhede,03/Jan/12 19:08,19/Jun/14 05:16,14/Jul/23 05:39,01/Jul/13 03:53,0.7,,,0.8.0,,,,,,,core,,,0,newbie,,,"The ZK producer, when used with the async producer option does the following 

1. Create a pool of async producers, one each for a broker registered under /broker/ids
2. On each send request, apply the Partitioner, to decide the broker and partition to send the data
3. Use the Async producer's send API to enqueue that data into the async producer's queue
4. When the data is dequeued by the ProducerSendThread, use the underlying sync producer to send it to the broker

The load balancing decision is taken in step 2, before entering the queue. This leaves a window of error, equal to the queue length, when a broker can go down. When this happens, potentially, a queue worth of data can fail to reach a broker, and will be dropped by the EventHandler. 

To correct this, the Producer, with the async option, needs to be refactored to allow only a single queue to hold all requests. And the application of the Partitioner should be moved to the end of the queue, in the EventHandler.",,ashwanthfernando@gmail.com,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,1209600,1209600,,0%,1209600,1209600,,,,,,,,,,,,,,,KAFKA-253,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,222600,,,Mon Jul 01 03:53:36 UTC 2013,,,,,,,,,,"0|i15zdz:",243033,,,,,,,,,,,,,,,,,,,,"03/Jan/12 19:19;nehanarkhede;Al though this JIRA is marked with the newbie tag, this is amongst the harder of all the newbie JIRAs. Anyone who is interested in knowing the Producer logic inside out, can give this a try. It will be a very good enhancement to the producer logic;;;","29/Jun/13 22:04;ashwanthfernando@gmail.com;Hi, I think I am seeing this already implemented in 0.8. Can we close this if that is the case?;;;","01/Jul/13 03:53;junrao;Fixed in 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleConsumer is not logging exceptions correctly so detailed stack trace is not coming in the logs,KAFKA-229,12536586,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,charmalloc,charmalloc,29/Dec/11 15:57,02/Jan/12 20:55,14/Jul/23 05:37,02/Jan/12 20:55,,,,0.8.0,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/11 15:59;charmalloc;KAFKA-229.patch;https://issues.apache.org/jira/secure/attachment/12508850/KAFKA-229.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,222269,,,Mon Jan 02 20:54:54 UTC 2012,,,,,,,,,,"0|i09mav:",54046,,,,,,,,,,,,,,,,,,,,"02/Jan/12 20:54;jkreps;+1 committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SyncProducer connect may return failed connection on reconnect,KAFKA-226,12535401,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,16/Dec/11 02:07,16/Dec/11 02:21,14/Jul/23 05:39,16/Dec/11 02:21,0.7,,,0.7.1,,,,,,,core,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/11 02:09;junrao;KAFKA-226.patch;https://issues.apache.org/jira/secure/attachment/12507640/KAFKA-226.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,221085,,,Fri Dec 16 02:21:15 UTC 2011,,,,,,,,,,"0|i0l3pr:",121252,,,,,,,,,,,,,,,,,,,,"16/Dec/11 02:09;junrao;Patch attached.;;;","16/Dec/11 02:09;nehanarkhede;+1;;;","16/Dec/11 02:21;junrao;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in mirroring code causes mirroring to halt,KAFKA-225,12534563,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,nehanarkhede,nehanarkhede,10/Dec/11 00:09,14/Dec/11 00:09,14/Jul/23 05:39,14/Dec/11 00:09,0.7,,,0.7.1,,,,,,,core,,,0,,,,"The mirroring code has an API that restarts the consumer connector when a new topic watcher fires. This triggers a rebalancing operation in the consumer connector. But if this rebalancing operation fails, the mirroring code simply throws an exception and never recovers. Ideally, if the rebalancing operation fails due to n retries, we should shut down the mirror",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/11 17:49;junrao;KAFKA-225.patch;https://issues.apache.org/jira/secure/attachment/12507211/KAFKA-225.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,220285,,,Wed Dec 14 00:09:28 UTC 2011,,,,,,,,,,"0|i15zdj:",243031,,,,,,,,,,,,,,,,,,,,"13/Dec/11 17:49;junrao;Patch attached.;;;","13/Dec/11 18:36;nehanarkhede;+1;;;","14/Dec/11 00:09;junrao;just committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LICENSE and NOTICE problems in Kafka 0.7,KAFKA-221,12533519,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jghoman,kevan,kevan,01/Dec/11 21:54,15/Dec/11 18:35,14/Jul/23 05:39,15/Dec/11 18:35,,,,0.7,,,,,,,,,,0,,,,"The source LICENSE file for Kafka is incomplete. The LICENSE file needs to accurately reflect the Kafka source and included artifacts.

Similarly, the NOTICE file is likely to be missing information. I'll attach a file with some information that I created. It's incomplete and will need additional work...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-222,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/11 21:39;jghoman;KAFKA-221-2.patch;https://issues.apache.org/jira/secure/attachment/12506795/KAFKA-221-2.patch","09/Dec/11 21:44;jghoman;KAFKA-221-3.patch;https://issues.apache.org/jira/secure/attachment/12506797/KAFKA-221-3.patch","05/Dec/11 21:50;jghoman;KAFKA-221.patch;https://issues.apache.org/jira/secure/attachment/12506171/KAFKA-221.patch","15/Dec/11 18:28;nehanarkhede;kafka-221-4.patch;https://issues.apache.org/jira/secure/attachment/12507562/kafka-221-4.patch","01/Dec/11 22:00;kevan;kafka-license-info.txt;https://issues.apache.org/jira/secure/attachment/12505817/kafka-license-info.txt",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,219247,,,Thu Dec 15 18:35:02 UTC 2011,,,,,,,,,,"0|i15zcv:",243028,,,,,,,,,,,,,,,,,,,,"01/Dec/11 22:00;kevan;This is a partial analysis of Kafka LICENSE/NOTICE requirements. I use emacs to look inside jars for license/notice files. if you don't find one, then you need to search for a license for the artifact.

Once you have this information pulled together, it's not too hard to pull it all into a LICENSE/NOTICE file.

Unfortunately, there's just not a good automated way to generate this information. If it's any comfort, Geronimo has *way* more embedded jar files than Kafka... ;-);;;","05/Dec/11 21:50;jghoman;Patch that re-does the LICENSE and NOTICE file assuming KAFKA-222 goes in.  NOTICE now just has entries for LinkedIn's contribution, zkclient and sbt.  The other remaining jars are from Apache projects and so don't need to be included here (per my understanding).  The License file has Scala, sbt's license (copied from its LICENSE file in the release we use). zkclient is Apache licensed and as far as I can tell, it therefore doesn't need to be included here.  Is this correct?;;;","06/Dec/11 09:41;aelder;I've tried to have a look but i don't have git and can't work out what SVN revision that patch is against so its hard for me to tell what the resultant files will look like, would you be able to attach the complete license and notice files here?

What is the reasoning behind keeping the NOTICE file entries for LinkedIn, zkclient and sbt? For example, looking at the sbt license at https://github.com/harrah/xsbt/blob/0.11/LICENSE if you include that complete license text in the Kafka LICENSE file then I don't think there is a need to mention sbt in the Kafka NOTICE.

;;;","09/Dec/11 21:39;jghoman;OK, updated based on comments from Ant and the other thread.  NOTICE (new version: http://dl.dropbox.com/u/565949/NOTICE ) only contains the Apache notice based on:
* as Ant mentioned, the sbt license is included in full in LICENSE so not necessary in NOTICE
* zkclient is Apache 2.0 licensed (category A), so no need to mention it in LICENSE
New LICENSE (new version: http://dl.dropbox.com/u/565949/LICENSE ):
* Removed Scala license, since we're not distributing the Scala runtime (it's pulled in via sbt)
* Has sbt license since it is being included and requires inclusion (although I still don't understand why including it in its jar isn't enough to satisfy this condition)
* Doesn't include anything for zkclient since it's ASL2.0
* Doesn't include anything for the pig stuff since they're sister ASF projects.

I think this is enough for a source release.
;;;","09/Dec/11 21:40;jghoman;re-submitting patch.;;;","09/Dec/11 21:44;jghoman;here's a new version without the final line of dashes in LICENSE.;;;","10/Dec/11 00:14;nehanarkhede;+1 on the latest patch.;;;","15/Dec/11 18:28;nehanarkhede;I think we missed the nunit entry in the LICENSE and the NOTICE files. We have the nunit.dll checked into the source repository under clients/csharp;;;","15/Dec/11 18:35;nehanarkhede;Thanks a bunch for helping out on this patch, Jakob ! 
Just committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LogManager test fails on linux,KAFKA-220,12533392,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,junrao,nehanarkhede,nehanarkhede,01/Dec/11 01:45,19/Jun/14 05:15,14/Jul/23 05:39,08/Mar/12 15:19,0.7,,,0.7.1,,,,,,,core,,,0,newbie,,,"On Linux, LogManagerTest fails on each and every run
[info] Test Starting: testCleanupExpiredSegments
[error] Test Failed: testCleanupExpiredSegments
junit.framework.AssertionFailedError: Now there should only be only one segment. expected:<1> but was:<12>
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.failNotEquals(Assert.java:277)
        at junit.framework.Assert.assertEquals(Assert.java:64)
        at junit.framework.Assert.assertEquals(Assert.java:195)
        at kafka.log.LogManagerTest.testCleanupExpiredSegments(LogManagerTest.scala:87)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.internal.runners.TestMethodRunner.executeMethodBody(TestMethodRunner.java:99)
        at org.junit.internal.runners.TestMethodRunner.runUnprotected(TestMethodRunner.java:81)
        at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
        at org.junit.internal.runners.TestMethodRunner.runMethod(TestMethodRunner.java:75)
        at org.junit.internal.runners.TestMethodRunner.run(TestMethodRunner.java:45)
        at org.junit.internal.runners.TestClassMethodsRunner.invokeTestMethod(TestClassMethodsRunner.java:71)
        at org.junit.internal.runners.TestClassMethodsRunner.run(TestClassMethodsRunner.java:35)
        at org.junit.internal.runners.TestClassRunner$1.runUnprotected(TestClassRunner.java:42)
        at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
        at org.junit.internal.runners.TestClassRunner.run(TestClassRunner.java:52)
        at org.junit.internal.runners.CompositeRunner.run(CompositeRunner.java:29)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:121)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:100)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:91)
        at org.scalatest.junit.JUnitSuite$class.run(JUnitSuite.scala:261)
        at kafka.log.LogManagerTest.run(LogManagerTest.scala:28)
        at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
        at sbt.TestRunner.run(TestFramework.scala:53)
        at sbt.TestRunner.runTest$1(TestFramework.scala:67)
        at sbt.TestRunner.run(TestFramework.scala:76)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.NamedTestTask.run(TestFramework.scala:92)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
        at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
        at sbt.impl.RunTask.runTask(RunTask.scala:85)
        at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Control$.trapUnit(Control.scala:19)
        at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)
[info] Test Starting: testCleanupSegmentsToMaintainSize

","nnarkhed-ld:~ nnarkhed$ uname -r
2.6.32-131.4.1.el6.x86_64
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/12 02:48;junrao;kafka-220.patch;https://issues.apache.org/jira/secure/attachment/12517189/kafka-220.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,219120,,,Thu Mar 08 15:19:16 UTC 2012,,,,,,,,,,"0|i0l3sn:",121265,,,,,,,,,,,,,,,,,,,,"06/Mar/12 02:49;junrao;Attach a patch by setting lastmodifiedtime manually. Also, tuned flush interval to speed up the test.;;;","08/Mar/12 05:59;nehanarkhede;+1;;;","08/Mar/12 15:19;junrao;committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ZOOKEEPER-961 is nasty, upgrade to zk 3.3.4",KAFKA-218,12533188,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,pyritschard,cburroughs,cburroughs,29/Nov/11 20:40,19/Jun/14 05:15,14/Jul/23 05:39,10/Jan/12 22:54,,,,0.7.1,,,,,,,,,,0,newbie,,,"3.3.4 is out with ZOOKEEPER-961, which I think is our most reported issue.

http://www.cloudera.com/blog/2011/11/apache-zookeeper-3-3-4-has-been-released/

Should be a one char changes, but the jar hasn't hit the maven repos yet.",,jdanbrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/12 21:43;pyritschard;0002-KAFKA-202-upgrade-zookeeper-to-3.3.4.patch;https://issues.apache.org/jira/secure/attachment/12510107/0002-KAFKA-202-upgrade-zookeeper-to-3.3.4.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,218916,,,Tue Jan 10 22:54:42 UTC 2012,,,,,,,,,,"0|i0l35r:",121162,,,,,,,,,,,,,,,,,,,,"10/Jan/12 21:43;pyritschard;it has now, here's the patch;;;","10/Jan/12 22:46;nehanarkhede;+1. Thanks for the patch !;;;","10/Jan/12 22:54;nehanarkhede;Committed this patch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add nunit license to the NOTICE file,KAFKA-216,12533056,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,jghoman,nehanarkhede,nehanarkhede,29/Nov/11 03:20,30/Nov/11 23:43,14/Jul/23 05:39,30/Nov/11 23:43,,,,0.7,,,,,,,packaging,,,0,,,,"According to yet some more feedback from general@, we need to add NUnit (http://www.nunit.org/) to the NOTICE file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Nov/11 01:34;jghoman;KAFKA-216-2.patch;https://issues.apache.org/jira/secure/attachment/12505565/KAFKA-216-2.patch","29/Nov/11 08:18;jghoman;KAFKA-216.patch;https://issues.apache.org/jira/secure/attachment/12505463/KAFKA-216.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,218784,,,Wed Nov 30 23:43:53 UTC 2011,,,,,,,,,,"0|i15zc7:",243025,,,,,,,,,,,,,,,,,,,,"29/Nov/11 08:18;jghoman;Patch adds nunit verbage to NOTICE and text of zlib/libpng to LICENSE.;;;","29/Nov/11 20:06;cburroughs;Could you add something like ""For Nunit used in clients/foo/bar"" to the LICENSE text so this is easier to make sense of?  Otherwise looks good to me.;;;","30/Nov/11 00:36;nehanarkhede;Yeah, I think it will be useful to add ""For Nunit used in clients/csharp"" to the LICENSE file.;;;","30/Nov/11 01:34;jghoman;Updated with new nunit verbiage.;;;","30/Nov/11 01:51;nehanarkhede;+1. ;;;","30/Nov/11 01:57;nehanarkhede;This should be double committed to the 0.7 branch, in addition to trunk;;;","30/Nov/11 23:43;jghoman;Committed to trunk and 0.7.  Resolving as fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalThreadStateException in topic watcher for Kafka mirroring,KAFKA-212,12532524,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,23/Nov/11 19:43,01/Dec/11 01:04,14/Jul/23 05:39,01/Dec/11 01:04,,,,0.7.1,,,,,,,,,,0,,,,"If the kafka mirroring embedded consumer receives a new topic watcher notification, it runs into the following exception 

[2011-11-23 02:49:15,612] FATAL java.lang.IllegalThreadStateException (kafka.consumer.ZookeeperTopicEventWatcher)
[2011-11-23 02:49:15,612] FATAL java.lang.IllegalThreadStateException
        at java.lang.Thread.start(Thread.java:595)
        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$3.apply(KafkaServerStartable.scala:142)
        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$3.apply(KafkaServerStartable.scala:142)
        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
        at scala.collection.immutable.List.foreach(List.scala:45)
        at kafka.server.EmbeddedConsumer.startNewConsumerThreads(KafkaServerStartable.scala:142)
        at kafka.server.EmbeddedConsumer.handleTopicEvent(KafkaServerStartable.scala:109)
        at kafka.consumer.ZookeeperTopicEventWatcher$ZkTopicEventListener.liftedTree2$1(ZookeeperTopicEventWatcher.scala:83)
        at kafka.consumer.ZookeeperTopicEventWatcher$ZkTopicEventListener.handleChildChange(ZookeeperTopicEventWatcher.scala:78)
        at org.I0Itec.zkclient.ZkClient$7.run(ZkClient.java:568)
        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
 (kafka.consumer.ZookeeperTopicEventWatcher)

This happens since it tries to start a thread which has finished executing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Nov/11 20:47;nehanarkhede;KAFKA-212.patch;https://issues.apache.org/jira/secure/attachment/12504924/KAFKA-212.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,218257,,,Thu Dec 01 01:04:02 UTC 2011,,,,,,,,,,"0|i0l40v:",121302,,,,,,,,,,,,,,,,,,,,"23/Nov/11 20:47;nehanarkhede;This patch clears the threadList that holds the older thread references, before adding newer threads to it. This avoids trying to start an already finished thread, thus avoiding IllegalThreadStateException ;;;","23/Nov/11 21:00;jkreps;Any thread that doesn't shut down cleanly will leak, is that a problem? Can that happen?;;;","23/Nov/11 21:04;nehanarkhede;the thread shutdown is guarded by a countdown latch. It will finish shutdown only after the thread exists the run() method. The problem here is that we keep older shutdown thread references in that list and end up calling start on those, which leads to the exception;;;","28/Nov/11 19:04;nehanarkhede;Can somebody help review this ?;;;","28/Nov/11 20:01;jkreps;I don't think that answers my question, though, which is how do we know if we are leaking threads? I guess the patch doesn't make it better or worse, since we definitely don't want to keep them in the list, but can you assess what happens if shutdown fails? Can that happen? Do we log it? Or is there a guarantee that the thread must shutdown in some bounded period of time?;;;","28/Nov/11 21:38;nehanarkhede;>> which is how do we know if we are leaking threads? 

Good question. From what I see, the entire thread run() method is guarded by a try-catch-finally block. In the finally block, we count down the latch. So if the thread itself runs into some exception/error, it will exit the run() method and shut itself down. The other case of thread shutdown is when the mirroring thread itself calls the shutdown API. Here, we wait until the current producer send operation succeeds to count down the latch. In both cases, I don't see how we can leak threads

>> I guess the patch doesn't make it better or worse, since we definitely don't want to keep them in the list, but can you assess what happens if shutdown fails?

Not true. The patch fixes the bug filed here. Your concerns are about the shutdown logic of the thread, which if you suspect is a bug, can go in a separate JIRA.

>> Do we log it? 

Yes, in any case of shutdown, it gets logged as a FATAL error.

>> Or is there a guarantee that the thread must shutdown in some bounded period of time? 

Maybe. If the producer send operation hangs indefinitely, which is a serious bug in the producer send logic.
;;;","01/Dec/11 00:11;junrao;We shouldn't be leaking threads. If we can get to the code that creates new MirrorThreads, the old threads should have finished since shutdown is blocking. If the shutdown blocks forever, we won't be able to create new threads. Again, there is no thread leak. Although the latter would suggest another serious bug somewhere else.

+1 on the patch.;;;","01/Dec/11 01:04;nehanarkhede;Fix is committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncProducerStats is not a singleton,KAFKA-207,12531689,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,16/Nov/11 20:52,22/Nov/11 00:58,14/Jul/23 05:39,22/Nov/11 00:58,0.7,,,0.7,,,,,,,core,,,0,,,,"AsyncProducerStats is not a singleton. This means that if a client instantiates multiple producers, the stat is collected for only 1 instance, instead of all instances.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/11 23:20;junrao;KAFKA-207.patch;https://issues.apache.org/jira/secure/attachment/12504451/KAFKA-207.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,217425,,,Mon Nov 21 22:31:34 UTC 2011,,,,,,,,,,"0|i15zb3:",243020,,,,,,,,,,,,,,,,,,,,"20/Nov/11 23:20;junrao;Make AsyncProducerStats a singleton that tracks dropped events from all AsyncProducer instances. Add a separate jmx bean that reports the queue size for each AsyncProducer instance.;;;","21/Nov/11 22:31;nehanarkhede;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"no DISCLAIMER, NOTICE needs cleanup",KAFKA-206,12531622,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,cburroughs,cburroughs,cburroughs,16/Nov/11 14:18,01/Dec/11 23:36,14/Jul/23 05:39,01/Dec/11 23:36,0.7,,,,,,,,,,,,,0,,,,"Followup from the incubator vote.  We need a DISCLAIMER and to clean up the notice.

http://mail-archives.apache.org/mod_mbox/incubator-general/201111.mbox/%3CCAOGo0VbZd23mxtVFMCuMkHN9fVhiekBUg1x7mxtH7oAzqgp9mQ%40mail.gmail.com%3E",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/11 00:14;nehanarkhede;KAFKA-206-v2.patch;https://issues.apache.org/jira/secure/attachment/12504147/KAFKA-206-v2.patch","16/Nov/11 14:46;cburroughs;k206-v1.txt;https://issues.apache.org/jira/secure/attachment/12503891/k206-v1.txt",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,217358,,,Fri Nov 18 00:28:54 UTC 2011,,,,,,,,,,"0|i15zav:",243019,,,,,,,,,,,,,,,,,,,,"16/Nov/11 14:46;cburroughs;Still not sure which things actually need to be mentioned in the NOTICE.  Other projects don't seem to be entirely consistent, or at least the pattern is lost on me.;;;","18/Nov/11 00:14;nehanarkhede;Thanks Chris for the original patch. I modified the NOTICE file to include sbt;;;","18/Nov/11 00:18;junrao;+1;;;","18/Nov/11 00:28;nehanarkhede;Thanks for the patch Chris ! Just committed this to publish a new RC ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BoundedByteBufferReceive hides OutOfMemoryError,KAFKA-204,12531486,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Critical,Fixed,cburroughs,cburroughs,cburroughs,15/Nov/11 16:31,23/Nov/11 23:46,14/Jul/23 05:39,23/Nov/11 23:46,0.7,,,0.7.1,,,,,,,,,,0,,,,"  private def byteBufferAllocate(size: Int): ByteBuffer = {
    var buffer: ByteBuffer = null
    try {
      buffer = ByteBuffer.allocate(size)
    }
    catch {
      case e: OutOfMemoryError =>
        throw new RuntimeException(""OOME with size "" + size, e)
      case e2 =>
        throw e2
    }
    buffer
  }

This hides the fact that an Error occurred, and will likely result in some log handler printing a message, instead of exiting with non-zero status.  Knowing how large the allocation was that caused an OOM is really nice, so I'd suggest logging in byteBufferAllocate and then re-throwing OutOfMemoryError",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/11 14:12;cburroughs;k204-v1.txt;https://issues.apache.org/jira/secure/attachment/12503888/k204-v1.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,217222,,,Wed Nov 23 23:46:49 UTC 2011,,,,,,,,,,"0|i0l4g7:",121371,,,,,,,,,,,,,,,,,,,,"15/Nov/11 16:37;jkreps;Uh...does anyone know what the motivation of this was originally? Catching OutOfMemoryError is a bit unorthodox...;;;","15/Nov/11 17:13;tgautier;Agree with Jay - if you get an OOME all bets are off.  Best to just exit.;;;","16/Nov/11 14:16;cburroughs;Tiny patch for this one case, created KAFKA-205 to follow up.;;;","16/Nov/11 16:22;junrao;The main reason this was added is to show the requested size and the caller that triggered such a request. It would be nice if both pieces are logged together. With the new patch, those two pieces are logged separately (although should be close) and someone has to link them together manually.;;;","16/Nov/11 16:24;junrao;Another possibility is to rethrow a new RuntimeException with OOME wrapped as the cause.;;;","19/Nov/11 13:23;erikvanoosten;If you rethrow, then rethrow a new OOME with the original OOME wrapped.;;;","21/Nov/11 00:16;junrao;Hmm, it doesn't look like OutOfMemoryError allow one to specify a cause during initialization.;;;","23/Nov/11 07:51;jkreps;I understand the intent, but the important thing here is not to swallow the OOM exception, right? I mean once you hit that all bets are off, you need to restart your process...basically I think we shouldn't be messing with that.;;;","23/Nov/11 07:52;jkreps;We have request limits in the server and consumer that should provide protection against this so i think that is the appropriate way to handle it. If it does happen I think we should just break everything and then the person running things should set the configs correctly to limit the max request size the server will accept and the max fetch size for the client.;;;","23/Nov/11 13:18;cburroughs;> I mean once you hit that all bets are off, you need to restart your process...basically I think we shouldn't be messing with that. 

Yeah, the most important thing to do is get out of the way and let the process exit with a non-zero status code.

So the options as I see it are:
 (1) Do something ugly (like pass the original fetch request to byteBufferAllocate) for the purposes of a valiant but possible futile logging attempt (there is no guarantee we will be able to allocate the logging Strings we are already asking for, everything we ad makes that less likely).
 (2)  Just rethrow e after a logging attempt in byteBufferAllocate.

My preference is (2), but if someone prefers (1) that's a reasonable trade off.;;;","23/Nov/11 18:47;junrao;Ok, I am fine with the patch then. Any objection to commit it?;;;","23/Nov/11 18:50;nehanarkhede;+1;;;","23/Nov/11 23:46;junrao;Just committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Embedded consumer doesn't shut down if the server can't start,KAFKA-197,12530895,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,09/Nov/11 16:45,13/Dec/11 03:04,14/Jul/23 05:39,13/Dec/11 03:04,0.7,,,0.7.1,,,,,,,,,,0,,,,"If a broker embeds a consumer and the broker itself doesn't start (e.g., conflicting broker id in ZK), the embedded consumer is still running. In this case, we should probably shut down the embedded consumer too.

To do this, we need to either throw an exception or return an error in KafkaServer.startup and act accordingly in KafkaServerStartable.startup.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/11 23:15;junrao;KAFKA-197.patch;https://issues.apache.org/jira/secure/attachment/12505942/KAFKA-197.patch","05/Dec/11 18:55;junrao;KAFKA-197_v2.patch;https://issues.apache.org/jira/secure/attachment/12506138/KAFKA-197_v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,216633,,,Tue Dec 13 03:04:19 UTC 2011,,,,,,,,,,"0|i0l3sf:",121264,,,,,,,,,,,,,,,,,,,,"02/Dec/11 23:15;junrao;Patch attached.;;;","05/Dec/11 03:55;jkreps;I don't think we want to call halt(), that is like kill -9 the process. I think we want the logs to flush and shutdown gracefully. Can't we just do a graceful shutdown on both the server and the embedded consumer?;;;","05/Dec/11 04:39;nehanarkhede;Would it be reasonable to have KafkaServerStartable register a callback with KafkaServer, and have the shutdown API of KafkaServer invoke that callback ? That way, we can ensure that KafkaServerStartable can cleanly shutdown the embedded consumer when the server is shutdown for some reason.;;;","05/Dec/11 05:37;jkreps;A less invasive way would just be to have the embedded consumer register a shutdown hook and use System.exit.

I am a little concerned about this whole embedded consumer thing, though. The original approach where we wrote to the local log in process was pretty fool proof. I think sending to a remote broker is actually riddled with issues. The producer send buffer is vulnerable to quite a large loss on any unclean shutdown or indeed any shutdown bugs. And also any condition that leads to a broker being unable to take requests but still registered in zk will lead to unbounded data loss. I wonder if this issue isn't just a special case of many many bad things that could happen.

With the current approach I actually don't see any benefits at all to bundling the replication process with the kafka broker. It would actually be better to have that run independently it seems to me.;;;","05/Dec/11 18:25;junrao;The main reason that we moved away from writing to local log is to pick up the compression support in the high level producer. Decoupling the embedded consumer from the broker may not be a bad idea. There is one more service/process that one has to manage. However, it's probably more flexible (to support things like consuming from multiple sources and plugging in logic for consumer-side auditing) and is less intrusive to the core Kafka code.;;;","05/Dec/11 18:55;junrao;Attach patch v2. Consolidate all error handling in KafkaServerStarble.;;;","13/Dec/11 02:15;nehanarkhede;+1 on v2;;;","13/Dec/11 03:04;junrao;Just committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Topic creation fails on large values,KAFKA-196,12530541,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,pyritschard,pyritschard,07/Nov/11 09:08,17/Aug/17 11:38,14/Jul/23 05:39,17/Aug/17 11:38,,,,,,,,,,,core,,,0,,,,"Since topic logs are stored in a directory holding the topic's name, creation of the directory might fail for large strings.
This is not a problem per-se but the exception thrown is rather cryptic and hard to figure out for operations.

I propose fixing this temporarily with a hard limit of 200 chars for topic names, it would also be possible to hash the topic name.

Another concern is that the exception raised stops the broker, effectively creating  a simple DoS vector, I'm concerned about how tests or wrong client library usage can take down the whole broker.",,omkreddy,promiseu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/11 09:09;pyritschard;0001-Set-a-hard-limit-on-topic-width-this-fixes-KAFKA-196.patch;https://issues.apache.org/jira/secure/attachment/12502738/0001-Set-a-hard-limit-on-topic-width-this-fixes-KAFKA-196.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,216279,,,Thu Aug 17 11:38:50 UTC 2017,,,,,,,,,,"0|i029zr:",11220,,,,,,,,,,,,,,,,,,,,"07/Nov/11 14:23;tgautier;There are a lot of things that do this. In particular lots of different characters can cause problems but especially filesystem ones like '/' and ' ' etc.;;;","07/Nov/11 14:36;pyritschard;Yep, the thing is, I think the message makes it hard to figure out what just happened, at lest I thought so.;;;","07/Nov/11 16:11;junrao;Thanks for the patch. Where is the limit 200 coming from? Is that the file name limit in most file systems?;;;","07/Nov/11 16:16;pyritschard;I tried coming up with a sensible and not limitative number which will
not clash with MAXPATHLEN when prefixing with the log directory. Some
old systems are rumored to have it as low as 256, but I don't see many
use cases where very large topic strings are relevant. It could safely
be bumped to 1000 (which would leave 23 chars for the prefix path).

Java has no way of accessing MAXPATHLEN unfortunately, since it is OS-specific

On Mon, Nov 7, 2011 at 5:12 PM, Jun Rao (Commented) (JIRA)
;;;","07/Nov/11 16:28;junrao;Do you think we can make it configurable and default it to 200? Also, can we create a method like verifyTopicName and put all the checkings there?;;;","07/Nov/11 16:34;pyritschard;You're taking me away from one-liner territory, but i'll take a look
at it. as for configurable, a property in server.properties is enough
?

On Mon, Nov 7, 2011 at 5:28 PM, Jun Rao (Commented) (JIRA)
;;;","07/Nov/11 16:37;junrao;Yes, a new server property should work.;;;","07/Nov/11 16:39;tgautier;Not sure if you will try to address my comment, but verifyTopicName wouldn't suffice - topic names should either be encoded to protect against special characters causing problems, or hashed as Ritschard suggests.;;;","07/Nov/11 16:49;junrao;Yes, we can use verifyTopicName to capture all constraints on topic names. We probably don't want to make it too complicated. How big is the character list that we should disallow?;;;","07/Nov/11 17:15;junrao;Not sure why the server would hang when it couldn't create a log directory. Our socket server processors capture all throwable. So this shouldn't kill any of the processors. Could you take a thread dump and see why the server hangs?;;;","07/Nov/11 23:42;jkreps;I think it would be good just to fully think through escaping and validating topic names. Currently we do essentially nothing, and we could potentially file infinite number of bugs against all the individual corner cases. If that is out of scope for what you are trying to do Pierre-Yves we can open a separate ticket, but I think we need to define the acceptable set of strings in a topic name and check that. For example, should we allow spaces? slashes? semicolons? etc. We need to do this escaping against both the unix fs and zookeeper. We are super permissive now, which leads to all kinds of corner cases. The larger solution might just be to make a KafkaTopic class that has the validation logic in the constructor and includes an escapedForFs() and escapedForZk() methods. We probably won't use this everywhere up front, but at least it begins to centralize the logic and makes it easy to reason if a name has already been escaped or not.;;;","10/Nov/11 21:26;cburroughs;'/' is particularly complicated since we want to eventually have support for  hierarchical topics, in which case '/' (or whatever we choose) will have special meaning to us, ZK, and the local filesystem.  I'd also prefer to have one way to represent topics as strings and not have separate ZK and local fs escaping schemes.

That said, unless Pierre-Yves feels like biting off a big patch lets keep this one for a configurable max topic length so that the problem users are running into now is fixed.;;;","17/Aug/17 11:38;omkreddy;Topic MAX_NAME_LENGTH is set to 249 is newer Kafka verions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompressionUtilTest does not run and fails when it does,KAFKA-192,12530476,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,joestein,joestein,joestein,06/Nov/11 06:51,14/Nov/11 21:55,14/Jul/23 05:39,12/Nov/11 23:05,,,,0.8.0,,,,,,,,,,0,,,,"CompressionUtilTest does not run the functions inside of it during ./sbt test

if you change CompressionUtilTest to extend JUnitSuite then the existing functions run (once you adorne them with @Test) but then fail ...

I suspect the TestUtils.checkEquals(messages.iterator, decompressedMessages.iterator) is failing in testSimpleCompressDecompress because all of the messages are serialized into byte arrays and the entire set of messages compressed and that new compressed messages is what is returned as one message instead of the List[Message] and therefor are not interpreted within TestUtil.checkEquals to see this nuance.

e.g.

[error] Test Failed: testSimpleCompressDecompress
junit.framework.AssertionFailedError: expected:<message(magic = 1, attributes = 0, crc = 3819140844, payload = java.nio.HeapByteBuffer[pos=0 lim=8 cap=8])> but was:<MessageAndOffset(message(magic = 1, attributes = 0, crc = 3819140844, payload = java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]),18)>

and

[error] Test Failed: testComplexCompressDecompress
junit.framework.AssertionFailedError: expected:<2> but was:<3>
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-187,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/11 18:58;joestein;kafka-192.patch;https://issues.apache.org/jira/secure/attachment/12502681/kafka-192.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,216214,,,Sun Nov 06 19:07:58 UTC 2011,,,,,,,,,,"0|i09mbr:",54050,,,,,,,,,,,,,,,,,,,,"06/Nov/11 18:58;joestein;made the tests run and fixed them so they succeed (as they should have to compare messages to messages);;;","06/Nov/11 19:07;nehanarkhede;+1. Excellent ! thanks for the patch. Just committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
no clean way to getCompressionCodec from Java-the-language,KAFKA-186,12530354,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,cburroughs,cburroughs,04/Nov/11 17:26,17/Aug/17 11:41,14/Jul/23 05:39,17/Aug/17 11:41,0.7,,,,,,,,,,,,,0,,,,"The obvious thing fails:

CompressionCodec.getCompressionCodec(1) results in cannot find symbol
symbol  : method getCompressionCodec(int)
location: interface kafka.message.CompressionCodec

Writing a switch statement with  kafka.message.NoCompressionCodec$.MODULE$ and duplicating the logic in CompressionCodec.getCompressionCodec is no fun, nor is creating a Hashtable just to call Utils.getCompressionCodec.  I'm not sure if there is a magic keyword to make it easy for javac to understand which CompressionCodec I'm referring to.

",,omkreddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,216092,,,Thu Aug 17 11:41:22 UTC 2017,,,,,,,,,,"0|i15z9r:",243014,,,,,,,,,,,,,,,,,,,,"04/Nov/11 17:36;junrao;We could just use kafka.message.NoCompression(). It's a static method.

Also, it doesn't look like there is a constant defined for GzipCompressionCode in the message class.;;;","04/Nov/11 17:41;junrao;Actually, we should use kafka.message.NoCompressionCodec.codec() and  kafka.message.GZIPCompressionCodec.codec(). Both are static methods.;;;","04/Nov/11 18:58;cburroughs;That requires everyone to write their own switch statement though.;;;","05/Nov/11 02:16;junrao;You only need do deal with the codec directly if you use SyncProducer. We recommend most people use Producer, in which compress level can be configured in the producer property.;;;","05/Nov/11 06:28;jkreps;Yes, agreed, I think we can definitely change this but the SyncProducer should be considered an internal class now. We should remove it from the docs with the 0.7 release as I think the Producer api can be either sync or async.;;;","03/Jan/12 19:17;nehanarkhede;Chris,

Based on the discussion above, do you feel like the code needs improvement ? If not, can we close this JIRA ?
;;;","17/Aug/17 11:41;omkreddy;CompressionType Java class added in newer Kafka version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log retention size and file size should be a long,KAFKA-184,12529809,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,jjkoshy,jjkoshy,02/Nov/11 05:37,03/Jul/13 22:08,14/Jul/23 05:39,03/Jul/13 22:08,0.7,,,0.8.1,,,,,,,,,,0,,,,"Realized this in a local set up: the log.retention.size config option should be a long, or we're limited to 2GB. Also, the name can be improved to log.retention.size.bytes or Mbytes as appropriate. Same comments for log.file.size. If we rename the configs, it would be better to resolve KAFKA-181 first.
",,jcreasy,jkreps,sriramsub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/12 00:19;jcreasy;KAFKA-184-0.8.patch;https://issues.apache.org/jira/secure/attachment/12536926/KAFKA-184-0.8.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,215668,,,Wed Jul 03 22:07:27 UTC 2013,,,,,,,,,,"0|i09m2v:",54010,,,,,,,,,,,,,,,,,,,,"18/Jul/12 00:14;jcreasy;The first part of this appears to be changed in the 0.8 branch. 

 /* the maximum size of the log before deleting it */
  val logRetentionSize = Utils.getLong(props, ""log.retention.size"", -1);;;","18/Jul/12 00:19;jcreasy;Added patch to change log.retention.size to log.retention.size.bytes, and updated included server.properties;;;","18/Jul/12 17:32;jkreps;Thanks! I am going to hold off on this. As you mention, changing the name of the properties is kind of undesirable as people would likely not notice and silently set the wrong thing. Fixing KAFKA-181 would make this less painful. In particular it would be nice to have aliases so we can fix config names but still conveniently allow the old name. Will add a comment to that ticket.;;;","03/Jul/13 22:07;sriramsub;We have already fixed all the config naming and types in 0.8. We can resolve this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set a TCP connection timeout for the SimpleConsumer,KAFKA-182,12529732,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jkreps,jkreps,01/Nov/11 20:19,07/Feb/15 23:46,14/Jul/23 05:39,07/Feb/15 23:46,,,,,,,,,,,,,,0,,,,"Currently we use SocketChannel.open which I *think* can block for a long time. We should make this configurable, and we may have to create the socket in a different way to enable this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,215591,,,2011-11-01 20:19:39.0,,,,,,,,,,"0|i02a07:",11222,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up shell scripts,KAFKA-180,12529619,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,01/Nov/11 04:08,09/Feb/14 23:51,14/Jul/23 05:39,09/Feb/14 23:51,,,,,,,,,,,,,,0,,,,"Currently it is a bit of a mess:
jkreps-mn:kafka-git jkreps$ ls bin
kafka-console-consumer-log4j.properties	kafka-producer-perf-test.sh		kafka-server-stop.sh			zookeeper-server-stop.sh
kafka-console-consumer.sh		kafka-producer-shell.sh			kafka-simple-consumer-perf-test.sh	zookeeper-shell.sh
kafka-console-producer.sh		kafka-replay-log-producer.sh		kafka-simple-consumer-shell.sh
kafka-consumer-perf-test.sh		kafka-run-class.sh			run-rat.sh
kafka-consumer-shell.sh			kafka-server-start.sh			zookeeper-server-start.sh

I think all the *-shell.sh scripts and all the *-simple-perf-test.sh scripts should die. If anyone has a use for these test classes we can keep them around and use the via kafka-run-class, but they are clearly not made for normal people to use. The *-shell.sh scripts are obsolete now that we have the *-console-*.sh scripts, since these do everything the old scripts did and more. I recommend we also delete the code for these.

I would like to change each tool so that it produces a usage line explaining what it does when run without arguments. Currently I actually had to go read the code to figure out what some of these are.

I would like to clean up places where the arguments are non-standard. Argument names should be the same across all the tools.

I would also like to rename kafka-replay-log-producer.sh to kafka-copy-topic.sh. I think this tool should also accept two zookeeper urls, the url of the input cluster and the url of the output cluster so this tool can be used to copy between clusters. I think we can have a --zookeeper a --input-zookeeper and a --output-zookeeper where --zookeeper is equivalent to setting both the input and the output zookeeper. Also confused why the options for this list --brokerinfo which can be either a zk url or brokerlist AND also --zookeeper which must be a zk url.

Any objections to all this? Any other gripes people have while I am in there?",,jkreps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,215478,,,Sun Feb 09 23:51:30 UTC 2014,,,,,,,,,,"0|i02a0f:",11223,,,,,,,,,,,,,,,,,,,,"01/Nov/11 15:50;junrao;SimpleConsumeShell is still useful for debugging purpose. I'd like to keep the code. The script can go.;;;","09/Feb/14 23:51;jkreps;We mostly did this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka producer should do a single write to send message sets,KAFKA-171,12528928,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,26/Oct/11 17:52,23/Nov/11 07:30,14/Jul/23 05:39,23/Nov/11 07:30,0.7,0.8.0,,0.8.0,,,,,,,core,,,0,,,,"From email thread: 
http://mail-archives.apache.org/mod_mbox/incubator-kafka-dev/201110.mbox/%3cCAFbh0Q1PYUj32thBaYQ29E6J4wT_mrG5SuUsfdeGWj6rmEx9Gw@mail.gmail.com%3e
> Before sending an actual message, kafka producer do send a (control) message of 4 bytes to the server. Kafka producer always does this action before send some message to the server.

I think this is because in BoundedByteBufferSend.scala we do essentially
 channel.write(sizeBuffer)
 channel.write(dataBuffer)

The correct solution is to use vector I/O and instead do
 channel.write(Array(sizeBuffer, dataBuffer))",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/11 17:57;jkreps;KAFKA-171-draft.patch;https://issues.apache.org/jira/secure/attachment/12500925/KAFKA-171-draft.patch","01/Nov/11 04:33;jkreps;KAFKA-171-v2.patch;https://issues.apache.org/jira/secure/attachment/12501735/KAFKA-171-v2.patch","31/Oct/11 05:55;jkreps;KAFKA-171.patch;https://issues.apache.org/jira/secure/attachment/12501570/KAFKA-171.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,214788,,,Wed Nov 23 07:30:02 UTC 2011,,,,,,,,,,"0|i09mbb:",54048,,,,,,,,,,,,,,,,,,,,"26/Oct/11 18:25;jkreps;Attached is a draft patch which turns the request into a single write. This is just a draft if this actually improves performance we should change Receive to use ScatteringByteChannel for consistency and also clean up a few more files with the same trick.

On my mac laptop I do see a change in tcpdump which seems to eliminate the 4 byte send. However I don't see any positive result in performance for synchronous single-threaded sends of 10 byte messages (which should be the worst case for this). I think this may just be because I am testing over localhost.

Here are the details on the results I have:

TRUNK:
jkreps-mn:kafka-git jkreps$ sudo tcpdump -i lo0 port 9093 
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on lo0, link-type NULL (BSD loopback), capture size 96 bytes
10:32:30.128938 IP jkreps-mn.linkedin.biz.56953 > jkreps-mn.linkedin.biz.9093: S 323648854:323648854(0) win 65535 <mss 16344,nop,wscale 3,nop,nop,timestamp 377871870 0,sackOK,eol>
10:32:30.129004 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56953: S 526915069:526915069(0) ack 323648855 win 65535 <mss 16344,nop,wscale 3,nop,nop,timestamp 377871870 377871870,sackOK,eol>
10:32:30.129013 IP jkreps-mn.linkedin.biz.56953 > jkreps-mn.linkedin.biz.9093: . ack 1 win 65535 <nop,nop,timestamp 377871870 377871870>
10:32:30.129022 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56953: . ack 1 win 65535 <nop,nop,timestamp 377871870 377871870>
10:32:30.129306 IP jkreps-mn.linkedin.biz.56953 > jkreps-mn.linkedin.biz.9093: P 1:5(4) ack 1 win 65535 <nop,nop,timestamp 377871870 377871870>
10:32:30.129319 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56953: . ack 5 win 65535 <nop,nop,timestamp 377871870 377871870>
10:32:30.129339 IP jkreps-mn.linkedin.biz.56953 > jkreps-mn.linkedin.biz.9093: P 5:41(36) ack 1 win 65535 <nop,nop,timestamp 377871870 377871870>
10:32:30.129350 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56953: . ack 41 win 65535 <nop,nop,timestamp 377871870 377871870>
10:32:30.151892 IP jkreps-mn.linkedin.biz.56953 > jkreps-mn.linkedin.biz.9093: F 41:41(0) ack 1 win 65535 <nop,nop,timestamp 377871870 377871870>
10:32:30.151938 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56953: . ack 42 win 65535 <nop,nop,timestamp 377871870 377871870>
10:32:30.151946 IP jkreps-mn.linkedin.biz.56953 > jkreps-mn.linkedin.biz.9093: . ack 1 win 65535 <nop,nop,timestamp 377871870 377871870>
10:32:30.152554 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56953: F 1:1(0) ack 42 win 65535 <nop,nop,timestamp 377871870 377871870>
10:32:30.152571 IP jkreps-mn.linkedin.biz.56953 > jkreps-mn.linkedin.biz.9093: . ack 2 win 65535 <nop,nop,timestamp 377871870 377871870>

PATCHED:
jkreps-mn:kafka-git jkreps$ sudo tcpdump -i lo0 port 9093 
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on lo0, link-type NULL (BSD loopback), capture size 96 bytes
10:35:40.637220 IP jkreps-mn.linkedin.biz.56993 > jkreps-mn.linkedin.biz.9093: S 1456363353:1456363353(0) win 65535 <mss 16344,nop,wscale 3,nop,nop,timestamp 377873772 0,sackOK,eol>
10:35:40.637287 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56993: S 1260172914:1260172914(0) ack 1456363354 win 65535 <mss 16344,nop,wscale 3,nop,nop,timestamp 377873772 377873772,sackOK,eol>
10:35:40.637296 IP jkreps-mn.linkedin.biz.56993 > jkreps-mn.linkedin.biz.9093: . ack 1 win 65535 <nop,nop,timestamp 377873772 377873772>
10:35:40.637306 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56993: . ack 1 win 65535 <nop,nop,timestamp 377873772 377873772>
10:35:40.657848 IP jkreps-mn.linkedin.biz.56993 > jkreps-mn.linkedin.biz.9093: P 1:41(40) ack 1 win 65535 <nop,nop,timestamp 377873773 377873772>
10:35:40.657886 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56993: . ack 41 win 65535 <nop,nop,timestamp 377873773 377873773>
10:35:40.711399 IP jkreps-mn.linkedin.biz.56993 > jkreps-mn.linkedin.biz.9093: F 41:41(0) ack 1 win 65535 <nop,nop,timestamp 377873773 377873773>
10:35:40.711430 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56993: . ack 42 win 65535 <nop,nop,timestamp 377873773 377873773>
10:35:40.711437 IP jkreps-mn.linkedin.biz.56993 > jkreps-mn.linkedin.biz.9093: . ack 1 win 65535 <nop,nop,timestamp 377873773 377873773>
10:35:40.762640 IP jkreps-mn.linkedin.biz.9093 > jkreps-mn.linkedin.biz.56993: F 1:1(0) ack 42 win 65535 <nop,nop,timestamp 377873774 377873773>
10:35:40.762678 IP jkreps-mn.linkedin.biz.56993 > jkreps-mn.linkedin.biz.9093: . ack 2 win 65535 <nop,nop,timestamp 377873774 377873774>

TRUNK:
bin/kafka-producer-perf-test.sh --topic test --brokerinfo zk.connect=localhost:2181 --messages 300000 --message-size 10 --batch-size 1 --threads 1
...
[2011-10-26 10:33:58,458] INFO Total Num Messages: 300000 bytes: 3000000 in 13.636 secs (kafka.tools.ProducerPerformance$)
[2011-10-26 10:33:58,459] INFO Messages/sec: 22000.5867 (kafka.tools.ProducerPerformance$)
[2011-10-26 10:33:58,459] INFO MB/sec: 0.2098 (kafka.tools.ProducerPerformance$)

PATCHED:
jkreps-mn:kafka-git jkreps$ bin/kafka-producer-perf-test.sh --topic test --brokerinfo zk.connect=localhost:2181 --messages 300000 --message-size 10 --batch-size 1 --threads 1
...
[2011-10-26 10:38:03,965] INFO Total Num Messages: 300000 bytes: 3000000 in 13.254 secs (kafka.tools.ProducerPerformance$)
[2011-10-26 10:38:03,965] INFO Messages/sec: 22634.6763 (kafka.tools.ProducerPerformance$)
[2011-10-26 10:38:03,966] INFO MB/sec: 0.2159 (kafka.tools.ProducerPerformance$)

;;;","26/Oct/11 18:30;nehanarkhede;This is a good change to make. A couple of comments -

1. Since we are changing WritableByteChannel to GatheringByteChannel, it is better to change the return type of writeTo and writeCompletely to return long, instead of int. This will avoid the coercion to Int in BoundedByteBufferSend.scala.

2. There are a couple of other places, where we do these double writes, e.g. OffsetArraySend, MessageSetSend etc.  We might as well fix those ? ;;;","26/Oct/11 18:59;jkreps;Moving off localhost between my mac laptop and dev workstation (linux) I see similar results:

TRUNK:
jkreps-mn:kafka-git jkreps$ bin/kafka-producer-perf-test.sh --topic test --brokerinfo zk.connect=jkreps-ld:2181 --messages 500000 --message-size 10 --batch-size 1 --threads 1
[2011-10-26 11:59:51,795] INFO Total Num Messages: 500000 bytes: 5000000 in 13.046 secs (kafka.tools.ProducerPerformance$)
[2011-10-26 11:59:51,795] INFO Messages/sec: 38325.9237 (kafka.tools.ProducerPerformance$)
[2011-10-26 11:59:51,795] INFO MB/sec: 0.3655 (kafka.tools.ProducerPerformance$)

PATCHED:
jkreps-mn:kafka-git jkreps$ bin/kafka-producer-perf-test.sh --topic test --brokerinfo zk.connect=jkreps-ld:2181 --messages 500000 --message-size 10 --batch-size 1 --threads 1
[2011-10-26 11:58:42,335] INFO Total Num Messages: 500000 bytes: 5000000 in 13.125 secs (kafka.tools.ProducerPerformance$)
[2011-10-26 11:58:42,335] INFO Messages/sec: 38095.2381 (kafka.tools.ProducerPerformance$)
[2011-10-26 11:58:42,335] INFO MB/sec: 0.3633 (kafka.tools.ProducerPerformance$);;;","27/Oct/11 00:07;cburroughs;Even if this doesn't measurably improve node to node performance (and I'm not sure we should expect it to since we don't have to wait for an ACK to send the next packet), isn't it definitely making life better for network engineer?;;;","27/Oct/11 01:07;jkreps;Yes, I think we should do it. My concern was just that I might be misunderstanding tcpdump or something since I find this a little counter-intuitive..;;;","31/Oct/11 05:55;jkreps;Okay this patch completes the conversion to GatheringByteChannel. I recommend we take this even though there doesn't seem to be real perf difference just because the network profile is better.;;;","01/Nov/11 02:20;nehanarkhede;Since we are changing WritableByteChannel to GatheringByteChannel, would it be better to change the return type of writeTo and writeCompletely to return long, instead of int. This will avoid the coercion to Int in BoundedByteBufferSend.scala.

;;;","01/Nov/11 04:33;jkreps;I actually don't think we should make the writeTo method return a long since we use 4 byte ints in the protocol to delimit size of request so we really can't take a buffer > Int.MaxValue. I added a check on this to avoid an overflow if the size is > Int.MaxValue - 4, which is unlikely but possible.;;;","01/Nov/11 05:10;nehanarkhede;nnarkhed-mn:kafka-171 nnarkhed$ find . -name ""*scala"" -exec grep -Hi ""asInstanceOf\[Int\]"" {} \;
./core/src/main/scala/kafka/api/OffsetRequest.scala:  header.putInt(size.asInstanceOf[Int] + 2)
./core/src/main/scala/kafka/api/ProducerRequest.scala:  def sizeInBytes(): Int = 2 + topic.length + 4 + 4 + messages.sizeInBytes.asInstanceOf[Int]
./core/src/main/scala/kafka/network/BoundedByteBufferSend.scala:    written.asInstanceOf[Int]
./core/src/main/scala/kafka/producer/SyncProducer.scala:    val setSize = messages.sizeInBytes.asInstanceOf[Int]
./core/src/main/scala/kafka/server/MessageSetSend.scala:  header.putInt(size.asInstanceOf[Int] + 2)
./core/src/main/scala/kafka/server/MessageSetSend.scala:      written += fileBytesSent.asInstanceOf[Int]
./core/src/main/scala/kafka/server/MessageSetSend.scala:  def sendSize: Int = size.asInstanceOf[Int] + header.capacity
./core/src/main/scala/kafka/utils/Utils.scala:    buffer.putInt((value & 0xffffffffL).asInstanceOf[Int])
./core/src/main/scala/kafka/utils/Utils.scala:    buffer.putInt(index, (value & 0xffffffffL).asInstanceOf[Int])

Its not great that we have so many places where we need to worry about coercion, but we can clean this up the next time we change the on wire protocol.

+1 on the latest patch;;;","01/Nov/11 16:10;junrao;MessageSet has a couple of unused imports. Other than that, the patch looks good. ;;;","01/Nov/11 16:27;jkreps;Cool, will clean up imports before checking in. I am going to hold off on this until after 0.7 goes out.;;;","01/Nov/11 17:37;nehanarkhede;You can check it into trunk. 0.7 is going off its own branch;;;","23/Nov/11 07:30;jkreps;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Producer using broker list does not load balance requests across multiple partitions on a broker,KAFKA-161,12527665,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,18/Oct/11 21:58,25/Oct/11 04:59,14/Jul/23 05:39,25/Oct/11 04:59,0.7,,,0.7,,,,,,,,,,0,,,,"https://issues.apache.org/jira/browse/KAFKA-129 introduced a bug in the load balancing logic of the Producer using broker.list.
Since the broker.list doesn't specify the number of partitions in total, it should ideally pick a broker randomly, and then send the produce request with partition id -1, so that the EventHandler routes the request to a random partition.
Instead of that, it defaults to 1 partition on each broker and ends up using the Partitioner to pick a partitions amongst the available ones.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/11 19:41;nehanarkhede;KAFKA-161.patch;https://issues.apache.org/jira/secure/attachment/12499914/KAFKA-161.patch","20/Oct/11 17:34;nehanarkhede;KAFKA-161.patch;https://issues.apache.org/jira/secure/attachment/12499897/KAFKA-161.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,88925,,,Thu Oct 20 23:51:23 UTC 2011,,,,,,,,,,"0|i15z8f:",243008,,,,,,,,,,,,,,,,,,,,"20/Oct/11 17:34;nehanarkhede;Fixing the load balancing strategy for the broker.list option on the Producer. With this change, when broker.list is used, the producer will pick a random broker and send request to partition -1 on that broker.;;;","20/Oct/11 18:53;junrao;1. We should guard logger.debug with the isDebugEnabled check.
2. The method name getNumPartitionsForTopic is misleading since the return value is a list of partitions, instead of number of partitions. Can we rename it to sth like getPartitionListForTopic? Ditto for the name of the variable being assigned to.
;;;","20/Oct/11 19:41;nehanarkhede;1. Guarded the logger.debug
2. Changed the names of variables and functions to make more sense;;;","20/Oct/11 23:51;junrao;+1 on the new patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZK consumer gets into infinite loop if a message is larger than fetch size,KAFKA-160,12527622,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,junrao,junrao,junrao,18/Oct/11 17:11,19/Oct/11 23:53,14/Jul/23 05:39,19/Oct/11 23:53,0.7,,,0.7,,,,,,,core,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/11 17:30;junrao;KAFKA-160.patch;https://issues.apache.org/jira/secure/attachment/12499567/KAFKA-160.patch","19/Oct/11 16:04;junrao;KAFKA-160_v2.patch;https://issues.apache.org/jira/secure/attachment/12499702/KAFKA-160_v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,88873,,,Wed Oct 19 23:53:34 UTC 2011,,,,,,,,,,"0|i15z87:",243007,,,,,,,,,,,,,,,,,,,,"18/Oct/11 17:39;junrao;There were a couple of problems.
1. We used to throw an exception while iterating messages in a ByteBufferMessageSet, if we can't iterate a single message out of the buffer. This indicates that either the fetch size is too small or if there is corruption in the data. Throwing exception is a good way to notify the consumer that something is wrong. After the compression patch, such an exception is no longer thrown.

2. The constructor of ByetBufferMessageSet recently added a new parameter in the middle and some of the callers didn't get changed accordingly. So, some input parameters are misaligned.

Attach a patch that fixes both problems.;;;","18/Oct/11 21:44;jkreps;Hey did I break this?;;;","18/Oct/11 21:50;junrao;I think this was introduced when compression was added. I reviewed the patch, but didn't catch this.;;;","19/Oct/11 16:04;junrao;Submitted patch v2, adding a unit test that uncovers the problem.;;;","19/Oct/11 22:57;nehanarkhede;+1 on patch v2;;;","19/Oct/11 23:53;junrao;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Php Client support for compression attribute,KAFKA-159,12527543,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,araddon,araddon,18/Oct/11 04:44,18/Oct/11 17:57,14/Jul/23 05:39,18/Oct/11 17:57,,,,0.7,,,,,,,clients,,,0,,,,"The php client didn't support the new compression attribute 

https://cwiki.apache.org/confluence/display/KAFKA/Compression",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/11 04:48;araddon;phpclient_newmessagecompression.patch;https://issues.apache.org/jira/secure/attachment/12499506/phpclient_newmessagecompression.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,88753,,,Tue Oct 18 17:57:58 UTC 2011,,,,,,,,,,"0|i15z7z:",243006,,,,,,,,,,,,,,,,,,,,"18/Oct/11 04:48;araddon;Not entirely sure about this but it allowed the producer/consumer examples to work with other clients better.  ;;;","18/Oct/11 17:57;junrao;Thanks for the patch Aaron. Just committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
go consumer & producer to support compression,KAFKA-158,12527450,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,jdamick,jdamick,17/Oct/11 13:52,27/Oct/11 14:23,14/Jul/23 05:39,27/Oct/11 14:23,0.7,,,,,,,,,,clients,,,0,go-client,,,"As related to KAFKA-79, the go consumer and producer needs to support the compression attribute per https://cwiki.apache.org/confluence/display/KAFKA/Compression.

Can someone assign this to me, i'll add support and create a patch.

thanks ",,jdamick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/11 16:46;araddon;KAFKA-158-try2.patch;https://issues.apache.org/jira/secure/attachment/12499710/KAFKA-158-try2.patch","19/Oct/11 01:35;araddon;KAFKA-158.patch;https://issues.apache.org/jira/secure/attachment/12499621/KAFKA-158.patch","20/Oct/11 16:00;jdamick;kafka_158_go_compress.patch;https://issues.apache.org/jira/secure/attachment/12499881/kafka_158_go_compress.patch","23/Oct/11 15:22;jdamick;kafka_158_go_compress_working.patch;https://issues.apache.org/jira/secure/attachment/12500368/kafka_158_go_compress_working.patch","27/Oct/11 14:14;jdamick;kafka_158_go_compress_working_2.patch;https://issues.apache.org/jira/secure/attachment/12501095/kafka_158_go_compress_working_2.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,88155,,,Thu Oct 27 14:23:33 UTC 2011,,,,,,,,,,"0|i15z7r:",243005,,,,,,,,,,,,,,,,,,,,"17/Oct/11 16:11;nehanarkhede;Jeffrey, I tried assigning this to you, but your name doesn't show up in the JIRA list for Kafka. ;;;","19/Oct/11 01:35;araddon;tested against the kafka/bin/producershell and consumer.  ;;;","19/Oct/11 03:52;jdamick;ok, but the tests weren't updated and no new ones added, it doesnt support either compressed or uncompressed messages, and it doesnt transparently decompress the messages or have a way to plugin different compression codecs.. Let me just finish up this patch...;;;","19/Oct/11 16:46;araddon;Take 2 on the patch, with updated tests ;;;","20/Oct/11 16:00;jdamick;patch to add ability to compressed messages

Seeing strange behavior from kafka when sending the compressed messages:

go client sends this: 

00 00 00 36 00 00 00 04 74 65 73 74 00 00 00 00 00 00 00 26 00 00 00 22 01 01 F0 17 43 A5 1F 8B 08 00 00 00 00 00 04 FF 4A CE CF 2D 00 04 00 00 FF FF 3A 6F 0A CB 04 00 00 00

and servers gives:

[2011-10-20 11:51:50,106] DEBUG Listening to new connection from /127.0.0.1:59861 (kafka.network.Processor)
[2011-10-20 11:51:50,107] TRACE 54 bytes read from /127.0.0.1:59861 (kafka.network.Processor)
[2011-10-20 11:51:50,115] TRACE Handling produce request from /127.0.0.1:59861 (kafka.request.logger)
[2011-10-20 11:51:50,119] TRACE Producer request ProducerRequest(test,0,38) (kafka.request.logger)
[2011-10-20 11:51:50,119] DEBUG makeNext() in deepIterator: innerDone = true (kafka.message.ByteBufferMessageSet)
[2011-10-20 11:51:50,119] TRACE Remaining bytes in iterator = 34 (kafka.message.ByteBufferMessageSet)
[2011-10-20 11:51:50,119] TRACE size of data = 34 (kafka.message.ByteBufferMessageSet)
[2011-10-20 11:51:50,120] DEBUG Message is compressed. Valid byte count = 0 (kafka.message.ByteBufferMessageSet)
[2011-10-20 11:51:50,133] DEBUG makeNext() in deepIterator: innerDone = true (kafka.message.ByteBufferMessageSet)
[2011-10-20 11:51:50,133] TRACE Remaining bytes in iterator = 0 (kafka.message.ByteBufferMessageSet)
[2011-10-20 11:51:50,133] TRACE size of data = 1668246896 (kafka.message.ByteBufferMessageSet)
[2011-10-20 11:51:50,134] ERROR Error processing ProduceRequest on test:0 (kafka.server.KafkaRequestHandlers)
kafka.common.InvalidMessageSizeException: invalid message size: 1668246896 only received bytes: 0 at 0( possible causes (1) a single message larger than the fetch size; (2) log corruption )

;;;","20/Oct/11 16:21;junrao;It seems that you are sending a message marked as compressed. However, after the payload is decompressed, the content is not well-formatted. Likely causes include: (1) the message is actually not compressed; (2) the compression codec is not gzip. ;;;","20/Oct/11 18:27;jdamick;the curious part is that the scala producer writes this:

0000003c0000000474657374000000000000002c000000280101891b70861f8b0800000000000000636060e0626438cd956f959c9f5b0000dce317a70e000000

And after breaking it down, the message part is:

1f 8b 08 00 00 00 00 00 00 00 63 60 60 e0 62 64 38 cd 95 6f 95 9c 9f 5b 00 00 dc e3 17 a7 0e 00 00 00

after ungzip'ing it becomes (gzip cmd line to hexdump):

00 00 00 0a 01 00 cb 0a  6f 3a 63 6f 6d 70        |........o:comp|

Did something else in the message format change when it's compressed?

I see the same result when i decompress it in go in my consumer.. 

;;;","20/Oct/11 18:27;jdamick;i should add the text i send from the producer was:  'comp';;;","20/Oct/11 18:33;junrao;The go producer and the scala producer should send the same bytes for the same message, right?;;;","20/Oct/11 18:44;jdamick;not necessarily, the gzip implementation is different. But I'm wondering why when I use gzip (cmd line, not from go) to ungzip that message it has this on the front: (as seen above) 00 00 00 0a 01 00 cb 0a 6f 3a

This happens to match what i see in go (the extra bytes on the front of 'comp') .. so i must be parsing it wrong somehow...
;;;","20/Oct/11 19:02;junrao;In the broker, we need to unzip compressed messages to verify the crc. Can unzip decoder messages encoded by compress?;;;","20/Oct/11 19:20;jdamick;Are you saying you don't use gzip to decompress the messages in the broker?  i don't understand the question?;;;","21/Oct/11 15:28;jdamick;I think I figured out my mistake, inside the gzip is yet another message... i misunderstood and thought it was only the payload that compressed.  So it's a compressed message with an uncompressed message inside it, is that really what it's supposed to be? 

;;;","21/Oct/11 15:37;junrao;That's right. We take one or more uncompressed messages and compress them using gzip and store the compressed bytes in the payload of a single message. This way, we can recursively iterate into a compressed message. See CompressionUtils.compress for details.;;;","21/Oct/11 15:51;jdamick;thanks, i'll make the appropriate updates.  But it seems like this compression flag would be a better fit on the 'message set' and then use that to encapsulate all messages.. ;;;","21/Oct/11 16:06;junrao;There is a compression flag on ByteBufferMessageSet, with the following signature:

  def this(compressionCodec: CompressionCodec, messages: Message*)
;;;","21/Oct/11 18:03;jdamick;nevermind, i see.;;;","23/Oct/11 15:22;jdamick;Working patch for dealing with compressed & uncompressed messages.  Updated tests & added a pluggable interface for future other payload codecs (compression or other);;;","24/Oct/11 19:46;nehanarkhede;Jeffrey, thanks for the patch. It looks good, though I wasn't able to build the go code and run the unit tests. The instructions in the README seem to be outdated ? ;;;","24/Oct/11 20:38;jjkoshy;I get the following - are these files missing from your patch?

Thanks,

Joel

clients/go]$ GOROOT=. make install
Makefile:1: src/Make.inc: No such file or directory
Makefile:14: src/Make.pkg: No such file or directory
make: *** No rule to make target `src/Make.pkg'.  Stop.;;;","25/Oct/11 00:36;jdamick;joel: just a guess but it looks like you goroot isn't set right.  It needs to point to the location where you installed go, mine points to /opt/go for example.  May want to double check: http://golang.org/doc/install.html#install

Neha: i'm glad to help, what error did you get?;;;","25/Oct/11 00:48;nehanarkhede;I saw the same error that Joel mentioned. Let me follow the installation for go and see if that helps.;;;","25/Oct/11 04:43;jjkoshy;Yes - that was the issue. It is probably obvious to go users. Otherwise, it would be good to mention this in the readme.

+1

;;;","25/Oct/11 04:58;nehanarkhede;+1 on updating the README. Thanks for the patch !;;;","27/Oct/11 13:58;jdamick;who commits the patch then?  i would if i could, can i have access to that part of the tree?;;;","27/Oct/11 14:03;nehanarkhede;One of the committers can accept the patch. Please can you update the README and upload an updated patch ?;;;","27/Oct/11 14:14;jdamick;updated the README, including links to the incubator website & go installation.;;;","27/Oct/11 14:23;nehanarkhede;Thanks for being responsive. Just committed this !;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Message may be delivered to incorrect partition incase of semantic partitioning,KAFKA-157,12527296,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,sharadag,sharadag,15/Oct/11 14:38,07/Feb/15 23:49,14/Jul/23 05:39,07/Feb/15 23:49,,,,,,,,,,,,,,0,,,,"Incase the broker hosting the partition is down, messages are currently repartitioned with the number of available brokers. This may lead to void the partitioning contract.

http://bit.ly/oEz2fT",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,86701,,,Mon Oct 17 22:44:29 UTC 2011,,,,,,,,,,"0|i02a0v:",11225,,,,,,,,,,,,,,,,,,,,"17/Oct/11 22:44;jkreps;I think the only fix for this is the replication work that makes the partitions highly available. I think with the current non-HA partitions you have only two choices: non-availablility or sloppy partitioning.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZK consumer may lose a chunk worth of message during rebalance in some rare cases,KAFKA-154,12526693,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,11/Oct/11 17:34,15/Oct/11 07:33,14/Jul/23 05:39,11/Oct/11 19:12,0.7,,,0.7,,,,,,,core,,,0,,,,"Occasionally, we have see errors with the following message in the consumer log after a rebalance happens.
   consumed offset: xxx doesn't match fetch offset: yyy for topicz

The consumer offset xxx should always match the fetch offset yyy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/11 17:46;junrao;KAFKA-154.patch;https://issues.apache.org/jira/secure/attachment/12498631/KAFKA-154.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,59338,,,Sat Oct 15 07:33:38 UTC 2011,,,,,,,,,,"0|i15z7j:",243004,,,,,,,,,,,,,,,,,,,,"11/Oct/11 17:44;junrao;This problem is caused by a very subtle bug. When a fetcher calls PartitionTopicInfo.enqueue, we first advance the fetch offset and then enqueue the fetched chunk into a blocking queue. When the fetcher thread is interrupted (because we are shutting down the fetcher after a rebalance), it can happen that we just advanced the fetch offset to xxx, but got interrupted while trying to add the fetched chunk into the queue (so the chunk is not added to the queue). Then a new fetcher gets created to start fetching from xxx. This causes a chunk worth of data just before xxx to be lost to the consumer.  ;;;","11/Oct/11 17:48;junrao;Patch is ready for review.

We just need to make sure that we enqueue first and then advance the fetch offset. This way, if the enqueue operation gets interrupted, the fetch offset is not advanced. If the enqueue operation succeeds, the offset is guaranteed to be advanced since it can't be interrupted.;;;","11/Oct/11 19:06;nehanarkhede;+1. Good catch !;;;","11/Oct/11 19:12;junrao;Thanks for the review. Committed the patch.;;;","15/Oct/11 07:27;jjkoshy;While looking at this area of the code, I was wondering about this patch: wouldn't it permit the mirror issue of enqueue and not advancing the offset, since the interrupt could occur just before the fetch offset update? So the new fetcher may fetch the same offset again. It seems to me that the interrupt and PartitionTopicInfo's enqueue method itself should be mutually exclusive - or perhaps provide suitable handling for chunks with the same fetch offset in the consumer iterator. Or I must be missing something obvious :);;;","15/Oct/11 07:33;jjkoshy;Ok nm - it's late.. I see that the queues are cleared out.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Current perf directory has buggy perf tests,KAFKA-149,12526092,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,nehanarkhede,nehanarkhede,06/Oct/11 18:25,09/Oct/11 21:53,14/Jul/23 05:39,06/Oct/11 19:48,,,,0.7,,,,,,,,,,0,,,,"The scripts in the current perf directory are buggy and not useful to run any reliable Kafka performance tests. The performance tools that work correctly are -

ProducerPerformance.scala
SimpleConsumerPerformance.scala
ConsumerPerformance.scala

Currently, the above are in the tools directory. Ideally, a Kafka performance suite should repackage these tools with some sample performance load and output data in csv format that can be graphed. 

I suggest deleting the perf directory and redoing this cleanly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,49665,,,Sun Oct 09 21:53:23 UTC 2011,,,,,,,,,,"0|i15z6f:",242999,,,,,,,,,,,,,,,,,,,,"06/Oct/11 19:02;junrao;+1;;;","07/Oct/11 01:43;cburroughs;I'm not following what was wrong with the perf/ tools, or what changed to make them unreliable.;;;","09/Oct/11 20:16;nehanarkhede;The perf directory had quite a few problems 

1. Our perf tools which are tested well were not used.
2. No clear separation between perf tools and definition of performance load
3. A lot of custom code written to plot some custom graphs, that cannot be easily understood by existing and new developers
4. The custom code was in Java, and was buggy (the perf numbers that it gave wasn't exactly indicative of real Kafka performance)

Ideally, our thought is to improve and bundle up the perf tools that we have and write some scripts that clearly define the performance load used and purpose of the perf test.

;;;","09/Oct/11 21:53;cburroughs;Thanks, that makes sense.  Is there a ticket yet for the New and Improved perf scripts?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
kafka integration tests fail on a fresh checkout,KAFKA-147,12525755,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,05/Oct/11 01:28,06/Oct/11 01:59,14/Jul/23 05:39,06/Oct/11 01:59,0.7,,,0.7,,,,,,,,,,0,,,,"On a fresh checkout and with an empty .ivy2 and .m2 cache, if you execute ./sbt update test, the integration tests will fail with this error - 

java.lang.NoSuchMethodError: junit.framework.TestSuite.<init>([Ljava/lang/Class;)V
	at org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)
	at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
	at sbt.TestRunner.run(TestFramework.scala:53)
	at sbt.TestRunner.runTest$1(TestFramework.scala:67)
	at sbt.TestRunner.run(TestFramework.scala:76)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.NamedTestTask.run(TestFramework.scala:92)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
	at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
	at sbt.impl.RunTask.runTask(RunTask.scala:85)
	at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Control$.trapUnit(Control.scala:19)
	at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)

The reason being 2 versions of the junit jar on the test classpath that SBT uses to run the ""test"" command. The KafkaProject.scala file corrects defines one of the test dependencies to be junit-4.1, since it uses a JUnit api in some of the tests. The problem is that there is another junit jar (v3.8.1) which gets downloaded as a transitive dependency on Scala 2.8.0. The cause of the above error is an incorrect test classpath, that includes both v4.1 as well as v3.8.1.

One of the possible fixes is to override the ""testClasspath"" variable in SBT to explicitly exclude junit from directories other than core/lib_managed/test
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-92,,,,,,,,"06/Oct/11 01:44;nehanarkhede;KAFKA-147.patch;https://issues.apache.org/jira/secure/attachment/12497936/KAFKA-147.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,46280,,,Thu Oct 06 01:59:39 UTC 2011,,,,,,,,,,"0|i15z5z:",242997,,,,,,,,,,,,,,,,,,,,"06/Oct/11 01:44;nehanarkhede;Reverting the checkin (r1178669) for KAFKA-92 to resolve this bug.;;;","06/Oct/11 01:46;junrao;+1. We can upgrade to sbt 10.0 later.;;;","06/Oct/11 01:59;nehanarkhede;Thanks. Committed the patch;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testUnreachableServer sporadically fails,KAFKA-146,12525615,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,nehanarkhede,cburroughs,cburroughs,04/Oct/11 02:09,05/Oct/11 20:29,14/Jul/23 05:39,05/Oct/11 20:29,,,,0.7,,,,,,,,,,0,,,,"(If anyone can tell me how to convince Jira to do verbatim output,  I would be grateful)

This seems to fail about 50% of the time on builds.apache.org, also reported by Bao Thai Ngo on the -dev list.  I have not had success reproducing it locally on my Ubuntu laptop.

[0m[[0minfo[0m] [34m[0m
[0m[[0minfo[0m] [34m== core-kafka / kafka.javaapi.producer.SyncProducerTest ==[0m
[0m[[0minfo[0m] [0mTest Starting: testUnreachableServer[0m
First message send retries took 365 ms
[0m[[31merror[0m] [0mTest Failed: testUnreachableServer[0m
junit.framework.AssertionFailedError: null
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.assertTrue(Assert.java:20)
	at junit.framework.Assert.assertTrue(Assert.java:27)
	at kafka.javaapi.producer.SyncProducerTest.testUnreachableServer(SyncProducerTest.scala:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.internal.runners.TestMethodRunner.executeMethodBody(TestMethodRunner.java:99)
	at org.junit.internal.runners.TestMethodRunner.runUnprotected(TestMethodRunner.java:81)
	at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
	at org.junit.internal.runners.TestMethodRunner.runMethod(TestMethodRunner.java:75)
	at org.junit.internal.runners.TestMethodRunner.run(TestMethodRunner.java:45)
	at org.junit.internal.runners.TestClassMethodsRunner.invokeTestMethod(TestClassMethodsRunner.java:71)
	at org.junit.internal.runners.TestClassMethodsRunner.run(TestClassMethodsRunner.java:35)
	at org.junit.internal.runners.TestClassRunner$1.runUnprotected(TestClassRunner.java:42)
	at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
	at org.junit.internal.runners.TestClassRunner.run(TestClassRunner.java:52)
	at org.junit.internal.runners.CompositeRunner.run(CompositeRunner.java:29)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:121)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:100)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:91)
	at org.scalatest.junit.JUnitSuite$class.run(JUnitSuite.scala:261)
	at kafka.javaapi.producer.SyncProducerTest.run(SyncProducerTest.scala:33)
	at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
	at sbt.TestRunner.run(TestFramework.scala:53)
	at sbt.TestRunner.runTest$1(TestFramework.scala:67)
	at sbt.TestRunner.run(TestFramework.scala:76)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.NamedTestTask.run(TestFramework.scala:92)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
	at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
	at sbt.impl.RunTask.runTask(RunTask.scala:85)
	at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Control$.trapUnit(Control.scala:19)
	at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/11 01:39;nehanarkhede;KAFKA-146.patch;https://issues.apache.org/jira/secure/attachment/12497739/KAFKA-146.patch","04/Oct/11 22:50;nehanarkhede;KAFKA-146.patch;https://issues.apache.org/jira/secure/attachment/12497722/KAFKA-146.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,43994,,,Wed Oct 05 20:29:38 UTC 2011,,,,,,,,,,"0|i15z5r:",242996,,,,,,,,,,,,,,,,,,,,"04/Oct/11 22:50;nehanarkhede;This test keeps failing on and off on Mac boxes as well. Basically, it is written to test the producer reconnect timeout, so it is timing dependent. I think we should clean all unit tests that have a timing dependency. 

For this ticket, I've attached a patch that deletes these tests from the unit test suite. ;;;","05/Oct/11 01:12;junrao;It seems that the problem is caused by this assertion: Assert.assertTrue((firstEnd-firstStart) < 300). However, I don't see too much value of these 2 tests and would agree that they can be removed. We should also remove the same tests in javaapi too.
.;;;","05/Oct/11 01:39;nehanarkhede;Deleted those tests from kafka.producer.SyncProducerTest as well as kafka.javaapi.producer.SyncProducerTest;;;","05/Oct/11 01:53;junrao;+1;;;","05/Oct/11 20:29;nehanarkhede;Committed this patch. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka server mirror shutdown bug,KAFKA-145,12525599,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,nehanarkhede,nehanarkhede,03/Oct/11 23:25,05/Oct/11 23:03,14/Jul/23 05:39,05/Oct/11 23:03,,,,0.7,,,,,,,core,,,0,,,,"When a machine that is mirroring data off of another Kafka broker is shutdown, it runs into the following exception, effectively dropping data. The shutdown API needs to be fixed to first shutdown the consumer threads, drain all the data to the producer, and only then shutdown the producer. 

FATAL kafka.server.EmbeddedConsumer  - kafka.producer.async.QueueClosedException: Attempt to add event to a closed queue.kafka.producer.async.QueueClosedException: Attempt to add event to a closed queue.
        at kafka.producer.async.AsyncProducer.send(AsyncProducer.scala:87)
        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$2.apply(ProducerPool.scala:131)
        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$2.apply(ProducerPool.scala:131)
        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
        at scala.collection.immutable.List.foreach(List.scala:45)
        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1.apply(ProducerPool.scala:131)
        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1.apply(ProducerPool.scala:130)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:130)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:102)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:102)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:102)
        at kafka.producer.Producer.zkSend(Producer.scala:144)
        at kafka.producer.Producer.send(Producer.scala:106)
        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$1$$anonfun$apply$1$$anon$1$$anonfun$run$1.apply(KafkaServerStartable.scala:136)
        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$1$$anonfun$apply$1$$anon$1$$anonfun$run$1.apply(KafkaServerStartable.scala:134)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:30)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Oct/11 22:27;nehanarkhede;KAFKA-145.patch;https://issues.apache.org/jira/secure/attachment/12497912/KAFKA-145.patch","05/Oct/11 21:09;nehanarkhede;KAFKA-145.patch;https://issues.apache.org/jira/secure/attachment/12497895/KAFKA-145.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,43962,,,Wed Oct 05 23:03:16 UTC 2011,,,,,,,,,,"0|i15z5j:",242995,,,,,,,,,,,,,,,,,,,,"05/Oct/11 21:09;nehanarkhede;This patch corrects the shutdown behavior of Kafka mirroring, i.e. EmbeddedConsumer. It first shuts down the new topic watcher, then the zookeeper consumer connector. After this, it stops the mirroring threads. At this point, all mirroring threads have finished mirroring all data that they've ever consumed. Only then, it shuts down the producer. 

This ensures that the producer is not shutdown, before the mirroring threads have finished their work, thereby avoiding data loss caused due to QueueClosedException.;;;","05/Oct/11 22:14;junrao;In MirroringThread.run, it's probably better to do the countdown even when we hit an exception.;;;","05/Oct/11 22:27;nehanarkhede;Uploading an updated patch, in which the shutdown latch is decremented in a finally block, to make sure the mirroring threads will shutdown even when they run into an error/exception.;;;","05/Oct/11 22:40;junrao;+1 on the new patch.;;;","05/Oct/11 23:03;nehanarkhede;Thanks. Just committed the patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in the queue timeout logic of the async producer,KAFKA-138,12524582,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,nehanarkhede,nehanarkhede,25/Sep/11 22:59,28/Sep/11 00:52,14/Jul/23 05:39,28/Sep/11 00:52,0.7,,,0.7,,,,,,,core,,,0,,,,There is a bug in the queue timeout logic of the async producer. This bug shows up when the producer is very low throughput. The behavior observed by such very low throughput producers is delayed dispatching of the events. There is no observed data loss though.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/11 23:34;nehanarkhede;KAFKA-138.patch;https://issues.apache.org/jira/secure/attachment/12496420/KAFKA-138.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,3227,,,Wed Sep 28 00:52:18 UTC 2011,,,,,,,,,,"0|i15z47:",242989,,,,,,,,,,,,,,,,,,,,"25/Sep/11 23:33;nehanarkhede;This patch corrects the queue.poll logic to respect the queue timeout. Before this, under very low traffic, the producer ended up waiting for a timeout proportional to number of events added to the queue, the upper bound being the batch size. ;;;","26/Sep/11 16:55;junrao;+1;;;","28/Sep/11 00:52;junrao;Thanks, Neha. Just committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the ruby kafka gem is not functional,KAFKA-135,12523754,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,pyritschard,pyritschard,pyritschard,20/Sep/11 17:10,04/Mar/13 17:02,14/Jul/23 05:39,26/Sep/11 17:41,0.6,0.7,,0.7,,,,,,,clients,,,0,,,,"The gem spec is missing a file declaration, the resulting gem is thus unusable",,pyritschard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-123,,,,,,,,,,,,,,,,,,,"20/Sep/11 17:10;pyritschard;0001-Fix-gem-building.patch;https://issues.apache.org/jira/secure/attachment/12495243/0001-Fix-gem-building.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,3199,,,Mon Sep 26 17:41:41 UTC 2011,,,,,,,,,,"0|i15z3j:",242986,,,,,,,,,,,,,,,,,,,,"26/Sep/11 17:41;junrao;Thanks for the patch, Pierre-Yves. Just committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop Consumer goes into an infinite loop when  kafka.request.limit is set to -1,KAFKA-131,12521223,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,sampd,sampd,02/Sep/11 19:20,29/Sep/11 18:39,14/Jul/23 05:39,13/Sep/11 01:20,0.7,,,0.7,,,,,,,contrib,,,0,patch,,,There is a bug in  KafkaETLContext.java  where in a new Iterator instance is being created every time. This causes endless loops.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/11 17:34;sampd;KAFKA-131.patch;https://issues.apache.org/jira/secure/attachment/12493181/KAFKA-131.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,40345,,,Thu Sep 29 18:39:11 UTC 2011,,,,,,,,,,"0|i15z33:",242984,,,,,,,,,,,,,,,,,,,,"02/Sep/11 20:38;sampd;Adding an instance variable _respIterator , so that response.iterator() isn't called multiple times;;;","02/Sep/11 21:38;sampd;Fix for the infinite loop bug in KafkaETLContext.java;;;","02/Sep/11 23:36;rbpark;Just one thing's missing.
On line 201 (of original file):
_offset += msgAndOffset.offset();

That's incorrect. The msgAndOffset returns the offset, not the offset increment. So it should be:
_offset = msgAndOffset.offset();
;;;","06/Sep/11 17:34;sampd;Adding the fix for offsets (Line 201);;;","06/Sep/11 17:41;rbpark;Great. I'm good with this patch.;;;","07/Sep/11 21:48;junrao;Thanks Sam and Richard. I just committed this.;;;","29/Sep/11 18:08;felixgv;I just wanted to point out that this bug seems to happen whether kafka.request.limit is set to -1 or not.

The current 0.6 release that is available on the site is not very usable because of this bug...

The current trunk does fix this problem though, which is great. Thanks :) !;;;","29/Sep/11 18:23;bmatheny;We have run into this as well Felix. I'd like to backport whatever change fixed this in trunk into our 0.6.1 branch. Any idea where I should look?;;;","29/Sep/11 18:30;sampd;Blake,
   You could apply the attached patch  (https://issues.apache.org/jira/secure/attachment/12493181/KAFKA-131.patch)  to the file KafkaETLContext.java;;;","29/Sep/11 18:39;bmatheny;Sorry I should have been more clear. We actually run into this issue not using the KafkaETL. We occasionally see regular consumers go into a loop (continue to fetch the same offset), I thought the comment from Felix was referring to that specifically.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZK-based producer can throw an unexpceted exception when sending a message,KAFKA-129,12521006,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,junrao,junrao,01/Sep/11 01:36,04/Oct/11 23:05,14/Jul/23 05:39,04/Oct/11 23:05,0.7,,,0.7,,,,,,,,,,0,,,,"Here is a log trace when that happens.

2011/08/26 11:25:20.104 FATAL [EmbeddedConsumer] [kafka-embedded-consumer-firehoseActivity-0] [kafka] java.util.NoSuchElementException: None.getjava.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:185)
        at scala.None$.get(Option.scala:183)
        at kafka.producer.Producer$$anonfun$3.apply(Producer.scala:115)
        at kafka.producer.Producer$$anonfun$3.apply(Producer.scala:101)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.mutable.WrappedArray.map(WrappedArray.scala:32)
        at kafka.producer.Producer.send(Producer.scala:101)
        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$1$$anonfun$apply$1$$anon$1$$anonfun$run$1.apply(KafkaServerStartable.scala:136)
        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$1$$anonfun$apply$1$$anon$1$$anonfun$run$1.apply(KafkaServerStartable.scala:134)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:30)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
        at kafka.consumer.KafkaMessageStream.foreach(KafkaMessageStream.scala:29)
        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$1$$anonfun$apply$1$$anon$1.run(KafkaServerStartable.scala:134)
        at java.lang.Thread.run(Thread.java:619)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/11 21:55;nehanarkhede;KAFKA-129.patch;https://issues.apache.org/jira/secure/attachment/12497714/KAFKA-129.patch","04/Oct/11 18:05;nehanarkhede;KAFKA-129.patch;https://issues.apache.org/jira/secure/attachment/12497673/KAFKA-129.patch","03/Oct/11 23:08;nehanarkhede;KAFKA-129.patch;https://issues.apache.org/jira/secure/attachment/12497565/KAFKA-129.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,43959,,,Tue Oct 04 21:59:32 UTC 2011,,,,,,,,,,"0|i15z2n:",242982,,,,,,,,,,,,,,,,,,,,"03/Oct/11 23:08;nehanarkhede;The producer using the zookeeper software load balancer maintains a ZK cache that gets updated by the zookeeper watcher listeners. During some events like a broker bounce, the producer ZK cache can get into an inconsistent state, for a small time period. In this time period, it could end up picking a broker partition that is unavailable, to complete the send operation. 

When this happens, the ZK cache needs to be updated, and the process of picking a broker partition for the current event should be repeated. This is repeated for a configurable number of retries, defaulting to 3. Arguably, if it takes more than 1-2 retries to get consistent data from zookeeper, something is really wrong, and we should throw an exception back to the user and fail the send operation.

This patch also adds a check around the debug and trace level logging in the producer;;;","04/Oct/11 17:35;junrao;1. import collection.SortedSet is not used in Producer.

2. In ZKBrokerPatitionInfo, it seem that we need to synchronize on zkWatcherLock in getBrokerInfo, to prevent seeing inconsistent info between allBrokers and the syncProducer list in ProducerPool.

3. In ProducerTest.testSendSingleMessage, the comment says that we want to send the request to a random partition, why is the partition number changed from -1 to 0? ;;;","04/Oct/11 18:05;nehanarkhede;2. Good catch. Updated the patch to include this.
3. While I was making this change, I found a bug in the partitioning approach of the producer when broker.list option is used. Previously, it choose a random broker, and then created a produce request with -1 as the partition. This is not, however, we intend to do partitioning. We let the default partitioner pick the right broker partition from amongst all available. So we never end up with a request with -1 as the partition. That test was also written with this buggy logic. It is using the StaticPartitioner, so the broker partition is deterministically selected. I fixed the bug as well as the tests to expose the right behavior.;;;","04/Oct/11 18:43;junrao;OK, for item 3, could you change the comment accordingly?;;;","04/Oct/11 19:58;nehanarkhede;Yeah, the comment in the test didn't quite match the new behavior. Fixed it;;;","04/Oct/11 21:55;nehanarkhede;This patch fixes the comments that didn't match the expected behavior in the code;;;","04/Oct/11 21:59;junrao;+1. Thanks for the patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DumpLogSegments outputs wrong offsets,KAFKA-128,12521003,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,junrao,01/Sep/11 01:24,01/Sep/11 07:50,14/Jul/23 05:39,01/Sep/11 07:50,,,,0.7,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/11 01:25;junrao;kafka-128.patch;https://issues.apache.org/jira/secure/attachment/12492563/kafka-128.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,62962,,,Thu Sep 01 01:57:55 UTC 2011,,,,,,,,,,"0|i15z2f:",242981,,,,,,,,,,,,,,,,,,,,"01/Sep/11 01:25;junrao;Patch attached.;;;","01/Sep/11 01:57;jjkoshy;
The start offset is printed incorrectly before the inner loop. Looks good otherwise.

+1

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Log flush should complete upon broker shutdown,KAFKA-126,12520992,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,jjkoshy,jjkoshy,31/Aug/11 23:58,05/Oct/11 23:05,14/Jul/23 05:39,26/Sep/11 17:35,,,,0.7,,,,,,,,,,0,,,,"Broker shutdown currently forces the flush scheduler to shutdown(Now). This leads to an unclean shutdown. cleanupLogs may be affected by a similar scenario.

2011/08/31 09:45:34.833 ERROR [LogManager] [kafka-logflusher-0] [kafka] Error flushing topic MyTopic
java.nio.channels.ClosedByInterruptException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)
        at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:362)
        at kafka.message.FileMessageSet.flush(FileMessageSet.scala:174)
        at kafka.log.Log.flush(Log.scala:306)
        at kafka.log.LogManager$$anonfun$kafka$log$LogManager$$flushAllLogs$1.apply(LogManager.scala:274)
        at kafka.log.LogManager$$anonfun$kafka$log$LogManager$$flushAllLogs$1.apply(LogManager.scala:263)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:30)
        at kafka.log.LogManager.kafka$log$LogManager$$flushAllLogs(LogManager.scala:263)
        at kafka.log.LogManager$$anonfun$startup$1.apply$mcV$sp(LogManager.scala:129)
        at kafka.utils.Utils$$anon$2.run(Utils.scala:58)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

A possible fix this would be to use shutdown() instead of shutdownNow() in the scheduler.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-148,,,,,,,,,,,,,,,,,,,,,"15/Sep/11 19:08;jjkoshy;KAFKA-126_v1.patch;https://issues.apache.org/jira/secure/attachment/12494678/KAFKA-126_v1.patch","21/Sep/11 01:08;jjkoshy;KAFKA-126_v2.patch;https://issues.apache.org/jira/secure/attachment/12495301/KAFKA-126_v2.patch","21/Sep/11 01:10;jjkoshy;KAFKA-126_v3.patch;https://issues.apache.org/jira/secure/attachment/12495302/KAFKA-126_v3.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,3201,,,Mon Sep 26 17:35:11 UTC 2011,,,,,,,,,,"0|i15z1z:",242979,,,,,,,,,,,,,,,,,,,,"15/Sep/11 19:08;jjkoshy;This is pretty much the simple fix that I suggested in the description. i.e., change the shutdown behavior of KafkaScheduler to use shutdown() instead of shutdownNow(). I also added a shutdownNow() method to KafkaScheduler for access to the immediate shutdown behavior.

For testing: I changed the flush settings to:
log.flush.interval=10000
log.default.flush.interval.ms=0
log.default.flush.scheduler.interval.ms=20
ran ProducerPerformance and shut down the broker a couple of times. These settings consistently reproduce the exception without this fix.

The other usages of shutdown (now called shutdownNow) are by the zookeeper offset committer, and the LogManager's log cleanup. I think these schedulers should also switch to using shutdown() and that is accomplished by this patch.

Finally, I'm piggy backing an small unrelated warning message in the producer config - i.e. when both zk.connect and broker.list are specified, zk.connect takes precedence. (Let me know if you prefer this to be removed from this patch.)
;;;","16/Sep/11 01:26;nehanarkhede;Using the executor shutdown API, the process would block until the currently executing thread finishes execution. I guess that works fine unless for some odd reason, the thread blocks on one action forever, in which case the process would have to be killed manually. I can't imagine of a concrete example of when it could block forever though. 

On the other hand, I guess that if/when that kind of blocking happens, it is a serious problem/bug that needs attention anyways. So using the shutdown() API looks like a good approach. 

+1;;;","20/Sep/11 15:46;junrao;Thanks Joel for the patch. The zookeeper offset committer should use shutdownNow. If there is anything wrong with ZK server, we still want to be able to shut down a consumer immedidately.;;;","21/Sep/11 01:08;jjkoshy;That makes sense. Here is the updated patch.;;;","21/Sep/11 01:10;jjkoshy;Oops - forgot to rebase.;;;","26/Sep/11 17:35;junrao;Thanks, Joel. Just committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
tooBigRequestIsRejected fails with unexpected Exceptoin,KAFKA-125,12520945,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jkreps,cburroughs,cburroughs,31/Aug/11 17:04,03/Oct/11 20:12,14/Jul/23 05:39,03/Oct/11 20:12,0.7,,,0.7,,,,,,,,,,0,,,,"Commit: http://svn.apache.org/viewvc?view=revision&revision=1159837

[info] Test Starting: tooBigRequestIsRejected
[error] Test Failed: tooBigRequestIsRejected
java.lang.Exception: Unexpected exception, expected<java.io.EOFException> but was<java.net.SocketException>
	at org.junit.internal.runners.TestMethodRunner.runUnprotected(TestMethodRunner.java:91)
	at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
	at org.junit.internal.runners.TestMethodRunner.runMethod(TestMethodRunner.java:75)
	at org.junit.internal.runners.TestMethodRunner.run(TestMethodRunner.java:45)
	at org.junit.internal.runners.TestClassMethodsRunner.invokeTestMethod(TestClassMethodsRunner.java:71)
	at org.junit.internal.runners.TestClassMethodsRunner.run(TestClassMethodsRunner.java:35)
	at org.junit.internal.runners.TestClassRunner$1.runUnprotected(TestClassRunner.java:42)
	at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
	at org.junit.internal.runners.TestClassRunner.run(TestClassRunner.java:52)
	at org.junit.internal.runners.CompositeRunner.run(CompositeRunner.java:29)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:121)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:100)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:91)
	at org.scalatest.junit.JUnitSuite$class.run(JUnitSuite.scala:261)
	at kafka.network.SocketServerTest.run(SocketServerTest.scala:32)
	at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
	at sbt.TestRunner.run(TestFramework.scala:53)
	at sbt.TestRunner.runTest$1(TestFramework.scala:67)
	at sbt.TestRunner.run(TestFramework.scala:76)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.NamedTestTask.run(TestFramework.scala:92)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
	at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
	at sbt.impl.RunTask.runTask(RunTask.scala:85)
	at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Control$.trapUnit(Control.scala:19)
	at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.net.SocketInputStream.read(SocketInputStream.java:182)
	at java.io.DataInputStream.readInt(DataInputStream.java:370)
	at kafka.network.SocketServerTest.sendRequest(SocketServerTest.scala:56)
	at kafka.network.SocketServerTest.tooBigRequestIsRejected(SocketServerTest.scala:78)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.internal.runners.TestMethodRunner.executeMethodBody(TestMethodRunner.java:99)
	at org.junit.internal.runners.TestMethodRunner.runUnprotected(TestMethodRunner.java:81)
	at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
	at org.junit.internal.runners.TestMethodRunner.runMethod(TestMethodRunner.java:75)
	at org.junit.internal.runners.TestMethodRunner.run(TestMethodRunner.java:45)
	at org.junit.internal.runners.TestClassMethodsRunner.invokeTestMethod(TestClassMethodsRunner.java:71)
	at org.junit.internal.runners.TestClassMethodsRunner.run(TestClassMethodsRunner.java:35)
	at org.junit.internal.runners.TestClassRunner$1.runUnprotected(TestClassRunner.java:42)
	at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
	at org.junit.internal.runners.TestClassRunner.run(TestClassRunner.java:52)
	at org.junit.internal.runners.CompositeRunner.run(CompositeRunner.java:29)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:121)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:100)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:91)
	at org.scalatest.junit.JUnitSuite$class.run(JUnitSuite.scala:261)
	at kafka.network.SocketServerTest.run(SocketServerTest.scala:32)
	at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
	at sbt.TestRunner.run(TestFramework.scala:53)
	at sbt.TestRunner.runTest$1(TestFramework.scala:67)
	at sbt.TestRunner.run(TestFramework.scala:76)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.NamedTestTask.run(TestFramework.scala:92)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
	at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
	at sbt.impl.RunTask.runTask(RunTask.scala:85)
	at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Control$.trapUnit(Control.scala:19)
	at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/11 04:34;jkreps;kafka-125.patch;https://issues.apache.org/jira/secure/attachment/12497423/kafka-125.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,41678,,,Mon Oct 03 04:51:29 UTC 2011,,,,,,,,,,"0|i15z1r:",242978,,,,,,,,,,,,,,,,,,,,"30/Sep/11 22:31;nehanarkhede;Jay,

We are getting ready to cut the first RC early next week. It would be good to fix this before the RC is published. Do you think that can happen ?

;;;","03/Oct/11 04:51;junrao;+1 for the patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Console consumer does not exit if consuming process dies,KAFKA-124,12520315,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,26/Aug/11 22:40,01/Sep/11 05:48,14/Jul/23 05:39,01/Sep/11 05:48,0.6,0.7,,0.7,,,,,,,,,,0,,,,Running the kafka console consumer it should be the case that if the consuming subprocess dies the java process dies as well. Instead it continues consuming messages even though there is no one to give them to.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/11 22:41;jkreps;kafka-console-consumer.diff;https://issues.apache.org/jira/secure/attachment/12491847/kafka-console-consumer.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,62966,,,Thu Sep 01 03:46:09 UTC 2011,,,,,,,,,,"0|i15z1j:",242977,,,,,,,,,,,,,,,,,,,,"30/Aug/11 01:53;jkreps;Can a brother get a +1?;;;","30/Aug/11 12:00;bmatheny;Appreciate the comment cleanup. The System.out.flush() line looks like it has a different indentation level than the other lines in the same block. Everything else looks good. Not sure if a +1 is required from Kafka devs or not but it has my vote.;;;","01/Sep/11 01:32;junrao; the patch looks good to me. +1 please fix the indentation before checking in.;;;","01/Sep/11 03:46;jjkoshy;+1

PrintStream javadoc suggests that System.out.flush() is redundant, since checkError also does a flush. Per our coding conventions since checkError has that side-effect, optional parentheses would be appropriate for that call.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The FetcherRunnable busy waits on empty fetch requests ,KAFKA-117,12519882,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,nehanarkhede,nehanarkhede,23/Aug/11 21:42,13/Sep/11 01:30,14/Jul/23 05:39,13/Sep/11 01:30,0.7,,,0.7,,,,,,,,,,0,,,,The FetcherRunnable busy waits on empty fetch requests by skipping the backoff logic. This can fill up the disk space due to the public access log being filled up. Also the CPU usage shoots up to 100%. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/11 00:21;nehanarkhede;KAFKA-117-v2.patch;https://issues.apache.org/jira/secure/attachment/12491421/KAFKA-117-v2.patch","24/Aug/11 00:59;nehanarkhede;KAFKA-117-v3.patch;https://issues.apache.org/jira/secure/attachment/12491427/KAFKA-117-v3.patch","23/Aug/11 21:49;nehanarkhede;KAFKA-117.patch;https://issues.apache.org/jira/secure/attachment/12491393/KAFKA-117.patch",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,60814,,,Wed Aug 24 01:05:45 UTC 2011,,,,,,,,,,"0|i15z0f:",242972,,,,,,,,,,,,,,,,,,,,"23/Aug/11 21:49;nehanarkhede;The bug was in the shallowValidBytes logic where it returns a negative value when dealing with an empty fetch request.

Corrected the logic of shallowValidBytes to return 0 when there is no data in the ByteBufferMessageSet created from an empty fetch request.;;;","23/Aug/11 23:21;junrao;We probably should just have a method validBytes that returns the valid bytes in the original message (ie, if the message is compressed, the bytes refer to the compressed bytes). There is no real need for knowing the total valid bytes on a compressed message set.;;;","24/Aug/11 00:18;nehanarkhede;I think your suggestion makes sense. The deepValidBytes method is useless and never used, even by the unit tests. It is better to have just a single API for calculating valid bytes.;;;","24/Aug/11 00:21;nehanarkhede;Deleted the deepValidBytes API. Made the shallowValidBytes API private and had the validBytes API point to shallowValidBytes.

validBytes() API will always give you the number of compressed bytes in the ByteBufferMessageSet, if it is compressed. ;;;","24/Aug/11 00:59;nehanarkhede;Includes all changes from v2 patch + a unit test to cover the valid bytes for empty message sets;;;","24/Aug/11 01:05;junrao;+1 on v3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AsyncProducer shutdown logic causes data loss,KAFKA-116,12519801,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,nehanarkhede,nehanarkhede,23/Aug/11 07:55,13/Sep/11 01:30,14/Jul/23 05:39,13/Sep/11 01:30,0.7,,,0.7,,,,,,,,,,0,,,,"The current shutdown logic of the AsyncProducer allows adding events to the queue, after adding the shutdown command to it. The ProducerSendThread drains all the data in the queue until it hits the shutdown command. Hence, all data added after the shutdown command is lost.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/11 08:16;nehanarkhede;KAFKA-116.patch;https://issues.apache.org/jira/secure/attachment/12491322/KAFKA-116.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,60812,,,Wed Aug 24 21:10:20 UTC 2011,,,,,,,,,,"0|i15z07:",242971,,,,,,,,,,,,,,,,,,,,"23/Aug/11 08:16;nehanarkhede;1. Fixed the shutdown bug by setting the closed queue flag before adding the shutdown command to the async producer queue
2. Added more logging at the trace level 
3. Refactored the blocking async producer logic to use case match instead of nested if else;;;","24/Aug/11 00:18;junrao;+1;;;","24/Aug/11 02:48;cburroughs;IllegalQueueStateException was missed in the commit.;;;","24/Aug/11 21:10;nehanarkhede;thanks for pointing that out. I checked it in now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka server access log does not log request details coming from a MultiProduceRequest,KAFKA-115,12519771,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,nehanarkhede,nehanarkhede,23/Aug/11 01:51,13/Sep/11 01:30,14/Jul/23 05:39,13/Sep/11 01:30,0.7,,,0.7,,,,,,,,,,0,,,,"the access logger logic on the kafka server has a bug, that doesn't log the individual produce request that are part of a MultiProduceRequest. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/11 02:24;nehanarkhede;KAFKA-115.patch;https://issues.apache.org/jira/secure/attachment/12491301/KAFKA-115.patch","23/Aug/11 01:55;nehanarkhede;KAFKA-115.patch;https://issues.apache.org/jira/secure/attachment/12491297/KAFKA-115.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,60813,,,Tue Aug 23 02:27:54 UTC 2011,,,,,,,,,,"0|i15yzz:",242970,,,,,,,,,,,,,,,,,,,,"23/Aug/11 02:24;nehanarkhede;Please ignore the previous patch. That could lead to redundant logging.;;;","23/Aug/11 02:27;junrao;V2 looks good. +1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A bug in the iterator of the ByteBufferMessageSet returns incorrect offsets when it encounters a compressed empty message set,KAFKA-111,12519279,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,nehanarkhede,nehanarkhede,18/Aug/11 10:56,13/Sep/11 01:28,14/Jul/23 05:39,13/Sep/11 01:28,0.7,,,0.7,,,,,,,,,,0,,,,"The deep iterator logic in the ByteBufferMessageSet returns incorrect offsets when it encounters empty compressed data. Ideally, it should be able to decompress the data, figure out that it is somehow empty, skip it and proceed to decoding rest of the data. To make this possible, the manner in which we update the offset to be returned by the iterator, needs to be tweaked.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/11 11:04;nehanarkhede;KAFKA-111.patch;https://issues.apache.org/jira/secure/attachment/12490784/KAFKA-111.patch","18/Aug/11 18:11;junrao;kafka-111.patch.v2;https://issues.apache.org/jira/secure/attachment/12490832/kafka-111.patch.v2","18/Aug/11 19:51;junrao;kafka-111.patch.v3;https://issues.apache.org/jira/secure/attachment/12490843/kafka-111.patch.v3",,,,,,,,,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,60816,,,Thu Aug 18 19:51:46 UTC 2011,,,,,,,,,,"0|i15yz3:",242966,,,,,,,,,,,,,,,,,,,,"18/Aug/11 11:04;nehanarkhede;This patch corrects the offset management in the deep iterator of the ByteBufferMessageSet, to handle the case of compressed empty message sets.;;;","18/Aug/11 16:01;junrao;1. Please remove unreferenced imports before checking in.
2. The following unit test now hangs.
Test Starting: testCompression(kafka.consumer.ZookeeperConsumerConnectorTest);;;","18/Aug/11 16:29;junrao;The patch in kafka-108 also identifies this problem. The problem is that we need to set the new offset in 2 cases: (1) if the compressed unit is empty; (2) otherwise, the last message of the compressed unit is being iterated. This patches covers (1), but not (2). One solution is probably to use the code that we had before, but treat case (1) specially, i.e., we won't even create an inner iterator and just advance the offset.;;;","18/Aug/11 18:11;junrao;How about patch v2? It currently breaks PrimitiveApiTest. However, the test probably should be fixed since it includes an empty message set.;;;","18/Aug/11 19:24;nehanarkhede;Yes, looks like patch v2 is a better way of handling the above 2 cases. We should check what breaks with PrimitiveApiTest though, maybe just a test bug.;;;","18/Aug/11 19:51;junrao;Patch v3. Fix test in kafka.javaapi.message.ByteBufferMessageSetTest.

PrimitiveApiTest for javaapi still fails. However, it should pass after kafka-109 is fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in the collate logic of the DefaultEventHandler dispatches empty list of messages using the producer,KAFKA-110,12519275,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,nehanarkhede,nehanarkhede,18/Aug/11 10:39,13/Sep/11 01:27,14/Jul/23 05:39,13/Sep/11 01:27,0.7,,,0.7,,,,,,,,,,0,,,,"The collate logic in the DefaultEventHandler is designed to batch together requests for a single topic and partition in order to send it to the server in a single request. In this collate logic, the use of the partition API might give back an empty sequence of data for a particular topic,partition pair. It is useless to add it to the list of data to be sent, and it should avoid making network requests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/11 10:50;nehanarkhede;KAFKA-110.patch;https://issues.apache.org/jira/secure/attachment/12490783/KAFKA-110.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,60817,,,Thu Aug 18 16:04:06 UTC 2011,,,,,,,,,,"0|i15yyv:",242965,,,,,,,,,,,,,,,,,,,,"18/Aug/11 10:50;nehanarkhede;This patch corrects the behavior of the collate API to avoid adding empty sequences of data to the list of events to be sent using the producer.

Also, it modifies the ProducerSendThread so that it never dispatches an empty list of events to the event handler.;;;","18/Aug/11 16:04;junrao;+1 (please remove unreferenced imports before checking in);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompressionUtils introduces a GZIP header while compressing empty message sets,KAFKA-109,12519264,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,nehanarkhede,nehanarkhede,18/Aug/11 09:10,13/Sep/11 01:27,14/Jul/23 05:39,13/Sep/11 01:27,0.7,,,0.7,,,,,,,,,,0,,,,"The CompressionUtils helper class takes in a sequence of messages and compresses those, using the appropriate codec. But even if it receives an empty sequence, it still ends up adding a GZIP compression header to the data, efffectively ""adding"" data to the resulting ByteBuffer. This doesn't match with the behavior for uncompressed empty message sets. CompressionUtils should be fixed by removing this side-effect.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/11 19:55;nehanarkhede;KAFKA-109.patch;https://issues.apache.org/jira/secure/attachment/12490844/KAFKA-109.patch","18/Aug/11 10:35;nehanarkhede;KAFKA-109.patch;https://issues.apache.org/jira/secure/attachment/12490782/KAFKA-109.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,60818,,,Thu Aug 18 21:43:47 UTC 2011,,,,,,,,,,"0|i15yyn:",242964,,,,,,,,,,,,,,,,,,,,"18/Aug/11 10:34;nehanarkhede;This patch handles the behavior of ByteBufferMessageSet for compression of empty list of messages. This modifies the ByteBufferMessageSet to create an empty byte buffer, in this case, instead of attaching a GZIP header to it. There are a couple of reasons to do this -

1. To maintain consistent behavior between an empty uncompressed message set and an empty compressed message set
2. To avoid attaching extraneous header information to non-existing data, effectively occupying space on disk;;;","18/Aug/11 16:03;junrao;+1. The patch looks good.;;;","18/Aug/11 17:03;junrao;Actually, this doesn't cover javaapi.ByteBufferMessageSet. It seems that javaapi.ByteBufferMessageSet duplicates some of the constructor code in ByteBufferMessageSet. We should avoid doing that.;;;","18/Aug/11 19:55;nehanarkhede;This is a revised patch that refactors the constructors of both java and scala ByteBufferMessageSet into a common API in MessageSet. This ensures that the bug fix exists both in the Java API as well as the Scala API;;;","18/Aug/11 21:43;junrao;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in serialize and collate logic in the DefaultEventHandler,KAFKA-107,12519065,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,nehanarkhede,nehanarkhede,17/Aug/11 03:02,13/Sep/11 01:27,14/Jul/23 05:39,13/Sep/11 01:27,0.7,,,0.7,,,,,,,,,,0,,,,"There is a bug in the serialize and collate in the DefaultEventHandler, that uses the map() API on a hashmap to convert a sequence of messages to a ByteBufferMessageSet, based on the compression configs. The usage of the zip() API after the map() API on a hashmap, has the side effect of reordering the mapping between the keys and the values. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/11 19:46;nehanarkhede;KAFKA-107-unit-test.patch;https://issues.apache.org/jira/secure/attachment/12490842/KAFKA-107-unit-test.patch","17/Aug/11 03:05;nehanarkhede;KAFKA-107.patch;https://issues.apache.org/jira/secure/attachment/12490597/KAFKA-107.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,60819,,,Thu Aug 18 21:40:06 UTC 2011,,,,,,,,,,"0|i15yy7:",242962,,,,,,,,,,,,,,,,,,,,"17/Aug/11 03:04;nehanarkhede;This patch refactors the code in the DefaultEventHandler serialize API to fix the reordering bug;;;","17/Aug/11 16:05;junrao;Ok, so the problem was that the code was trying to zip a map (topicsAndPartitions) and a list (messages). Even though they are derived from the same map, there is no guarantee that the map is going to be iterated in the same order as the list. So, in the future, we need to be careful about using zip. We need to make sure that the two collections to be zipped have well defined ordering (e.g., list) for iteration and the ordering is what we want. Could you double check other usage of zip introduced in the compression patch?

The patch looks fine. However, could you also add a unit test (probably just at the serialize() level) that exposes this problem?;;;","17/Aug/11 18:45;junrao;Another thing, please remove unreferenced package imports.;;;","17/Aug/11 19:08;nehanarkhede;Jun, the code is not  zipping a map with a list. That won't even compile. The code is just zipping a sequence of tuples with a sequence of ByteBufferMessageSet. The problem is that one sequence is generated using the map API on the original HashMap and another sequence is generated using the map API on another HashMap. So ordering of data across those sequences is not guaranteed. Hence, a zip on those sequences will not associate the keys with the correct values.

I will add a unit test to cover this, and remove the unreferenced package imports.;;;","18/Aug/11 19:46;nehanarkhede;Adding a unit test to the test suite for AsyncProducer that catches the reordering bug in serialize() API of DefaultEventHandler;;;","18/Aug/11 21:40;junrao;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Shutting down Kafka should be FATAL, not ERROR",KAFKA-102,12518896,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,bmatheny,bmatheny,15/Aug/11 16:45,15/Aug/11 17:32,14/Jul/23 05:39,15/Aug/11 17:32,,,,,,,,,,,core,,,0,,,,"When Kafka encounters an unrecoverable error it generates an error level log record and then calls Runtime.getRuntime.halt(1). This should really be fatal, not an error.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/11 17:14;bmatheny;kafka-log-2.patch;https://issues.apache.org/jira/secure/attachment/12490450/kafka-log-2.patch","15/Aug/11 16:47;bmatheny;kafka-log.patch;https://issues.apache.org/jira/secure/attachment/12490445/kafka-log.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,64995,,,Mon Aug 15 17:32:04 UTC 2011,,,,,,,,,,"0|i15yxr:",242960,,,,,,,,,,,,,,,,,,,,"15/Aug/11 16:56;jkreps;Thanks for catching this! While we are cleaning up we should fix the error messages. The capitalization is wrong and they reference internal method names, which makes them sysadmin unfriendly.;;;","15/Aug/11 17:14;bmatheny;Updated the patch to at least cleanup the error/fatal logging in those two methods according to the kafka style guide.;;;","15/Aug/11 17:21;jkreps;+1;;;","15/Aug/11 17:32;jkreps;Applied.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SocketServer.scala does not enforce a maximum request size,KAFKA-99,12518811,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,jkreps,jkreps,14/Aug/11 07:30,20/Aug/11 04:08,14/Jul/23 05:39,20/Aug/11 04:08,,,,,,,,,,,,,,0,,,,"The socket server should enforce a max request size to avoid running out of memory if a large request is sent. I see code in BoundedByteBufferReceive and in KafkaConfig to specify this, but it doesn't seem to be getting used. I added this in and added a stand-alone test for the socket server to start handling some of this stuff.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/11 21:29;jkreps;kafka-socket-server-max-request-v2.diff;https://issues.apache.org/jira/secure/attachment/12490472/kafka-socket-server-max-request-v2.diff","14/Aug/11 07:31;jkreps;kafka-socket-server-max-request.diff;https://issues.apache.org/jira/secure/attachment/12490375/kafka-socket-server-max-request.diff",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,64371,,,Fri Aug 19 22:55:05 UTC 2011,,,,,,,,,,"0|i15yxb:",242958,,,,,,,,,,,,,,,,,,,,"15/Aug/11 20:23;junrao;Shouldn't we instantiate SocketServer with the max size from KafkaConfig? Other than that, the patch looks good.;;;","15/Aug/11 21:28;jkreps;Wups, somehow that got dropped from my patch, but should have been there. Here is the full patch.;;;","15/Aug/11 21:29;jkreps;This includes the stuff i dropped from my original patch.;;;","19/Aug/11 22:55;junrao;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests hard code ports,KAFKA-98,12518808,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jkreps,jkreps,jkreps,14/Aug/11 05:36,15/Aug/11 17:34,14/Jul/23 05:39,15/Aug/11 17:34,,,,,,,,,,,,,,0,,,,This doesn't work on a test server or on your own box if you have kafka running. Patch removes this.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/11 05:37;jkreps;kafka-fix-hardcoded-ports.diff;https://issues.apache.org/jira/secure/attachment/12490372/kafka-fix-hardcoded-ports.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,64994,,,Mon Aug 15 17:34:35 UTC 2011,,,,,,,,,,"0|i15yx3:",242957,,,,,,,,,,,,,,,,,,,,"14/Aug/11 05:37;jkreps;Also check out the awesome assertion I fixed:

assert(x - x < 300)

We had better hope that is true or we have bigger problems.;;;","15/Aug/11 17:02;junrao;+1. The patch looks good to me.;;;","15/Aug/11 17:34;jkreps;Applied.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaServer can throw a NullPointerException during startup if zookeeper is down,KAFKA-94,12518663,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,jjkoshy,jjkoshy,12/Aug/11 01:17,18/Jul/12 00:00,14/Jul/23 05:39,20/Aug/11 03:57,,,,,,,,,,,core,,,0,,,,"Starting up KafkaServer when zookeeper is down can lead to a NullPointerException as shown below. The LogManager throws a ZkTimeoutException from startup if zookeeper is down. This is caught and we call shutdown. socketServer is uninitialized at this point, and hence the NPE. Will upload patch in a bit.

2011/08/12 00:22:36.194 FATAL [KafkaServer] [pool-2-thread-1] [kafka] java.lang.NullPointerException
	at kafka.server.KafkaServer.shutdown(KafkaServer.scala:98)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:84)
	at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:40)
	at com.linkedin.kafka.KafkaStartable.start(KafkaStartable.java:49)
	at com.linkedin.spring.servlet.ComponentsContextLoaderListener.setServletContextAttributes(ComponentsContextLoaderListener.java:113)
	at com.linkedin.spring.servlet.ComponentsContextLoaderListener.contextInitialized(ComponentsContextLoaderListener.java:50)
	at org.mortbay.jetty.handler.ContextHandler.startContext(ContextHandler.java:549)
	at org.mortbay.jetty.servlet.Context.startContext(Context.java:136)
	at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)
	at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)
	at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at com.linkedin.emweb.ContextBasedHandlerImpl.doStart(ContextBasedHandlerImpl.java:123)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at com.linkedin.emweb.WebappDeployerImpl.start(WebappDeployerImpl.java:324)
	at com.linkedin.emweb.WebappDeployerImpl.deploy(WebappDeployerImpl.java:178)
	at com.linkedin.emweb.StateKeeperWebappDeployer.deploy(StateKeeperWebappDeployer.java:73)
	at com.linkedin.emweb.mbeans.WebappDeployerAdmin.deploy(WebappDeployerAdmin.java:86)
	at com.linkedin.emweb.mbeans.WebappDeployerAdmin$1.call(WebappDeployerAdmin.java:130)
	at com.linkedin.emweb.mbeans.WebappDeployerAdmin$1.call(WebappDeployerAdmin.java:127)
	at com.linkedin.emweb.mbeans.WebappDeployerAdmin$3.call(WebappDeployerAdmin.java:171)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/11 01:26;jjkoshy;kafka-94.patch.v1;https://issues.apache.org/jira/secure/attachment/12490204/kafka-94.patch.v1",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,64372,,,Sat Aug 20 03:57:29 UTC 2011,,,,,,,,,,"0|i15ywn:",242955,,,,,,,,,,,,,,,,,,,,"12/Aug/11 01:21;jjkoshy;Since the logmanager is also uninitialized, we should check for that as well.

2011/08/12 00:22:36.191 FATAL [KafkaServer] [pool-2-thread-1] [kafka] org.I0Itec.zkclient.exception.ZkTimeoutException: Unable to connect to zookeeper server within timeout: 300000
	at org.I0Itec.zkclient.ZkClient.connect(ZkClient.java:876)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:98)
	at org.I0Itec.zkclient.ZkClient.<init>(ZkClient.java:84)
	at kafka.server.KafkaZooKeeper.startup(KafkaZooKeeper.scala:45)
	at kafka.log.LogManager.<init>(LogManager.scala:86)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:59)
	at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:40)
	at com.linkedin.kafka.KafkaStartable.start(KafkaStartable.java:49)
	at com.linkedin.spring.servlet.ComponentsContextLoaderListener.setServletContextAttributes(ComponentsContextLoaderListener.java:113)
	at com.linkedin.spring.servlet.ComponentsContextLoaderListener.contextInitialized(ComponentsContextLoaderListener.java:50)
	at org.mortbay.jetty.handler.ContextHandler.startContext(ContextHandler.java:549)
	at org.mortbay.jetty.servlet.Context.startContext(Context.java:136)
	at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)
	at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)
	at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at com.linkedin.emweb.ContextBasedHandlerImpl.doStart(ContextBasedHandlerImpl.java:123)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at com.linkedin.emweb.WebappDeployerImpl.start(WebappDeployerImpl.java:324)
	at com.linkedin.emweb.WebappDeployerImpl.deploy(WebappDeployerImpl.java:178)
	at com.linkedin.emweb.StateKeeperWebappDeployer.deploy(StateKeeperWebappDeployer.java:73)
	at com.linkedin.emweb.mbeans.WebappDeployerAdmin.deploy(WebappDeployerAdmin.java:86)
	at com.linkedin.emweb.mbeans.WebappDeployerAdmin$1.call(WebappDeployerAdmin.java:130)
	at com.linkedin.emweb.mbeans.WebappDeployerAdmin$1.call(WebappDeployerAdmin.java:127)
	at com.linkedin.emweb.mbeans.WebappDeployerAdmin$3.call(WebappDeployerAdmin.java:171)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619);;;","12/Aug/11 01:37;nehanarkhede;+1. Thanks for the patch.;;;","12/Aug/11 02:10;junrao;Thanks, Joel. Just committed this.;;;","20/Aug/11 03:51;jkreps;Should this be closed?;;;","20/Aug/11 03:57;jjkoshy;Yes - this has been committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
zkclient does not show up in pom,KAFKA-91,12518343,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,cburroughs,cburroughs,cburroughs,09/Aug/11 18:27,30/Oct/11 21:42,14/Jul/23 05:39,30/Oct/11 21:42,,,,0.7,,,,,,,packaging,,,0,,,,"The pom from created by `make-pom`. Does not include zkclient, which is  of course a key dependency.  Not sure yet how to pull in zkclient while excluding sbt itself.

$ cat core/target/scala_2.8.0/kafka-0.7.pom  | grep -i zkclient | wc -l
0
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/11 00:37;cburroughs;k91-v1.txt;https://issues.apache.org/jira/secure/attachment/12498495/k91-v1.txt","26/Oct/11 23:55;cburroughs;k91-v2.txt;https://issues.apache.org/jira/secure/attachment/12500992/k91-v2.txt",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,49662,,,Sun Oct 30 21:40:21 UTC 2011,,,,,,,,,,"0|i15yw7:",242953,,,,,,,,,,,,,,,,,,,,"09/Aug/11 18:36;nehanarkhede;One way to do this would be to override the make-pom command in KafkaProject.scala and customize the output pom ;;;","09/Aug/11 18:59;cburroughs;Your right that's probably the best way instead of trying to do something clever and automatic with the contents of lib.  Some examples I found: http://vasilrem.com/blog/software-development/from-sbt-to-maven-in-one-move/ , https://gist.github.com/878462;;;","06/Oct/11 17:58;nehanarkhede;Moving it to 0.8;;;","11/Oct/11 00:37;cburroughs;This magic incantation appears to work.;;;","24/Oct/11 19:05;cburroughs;Could I get a review?;;;","24/Oct/11 19:15;nehanarkhede;Have we verified what other dependent jars this zkClient pulls in ? We've hit the multiple jar versions problem before.;;;","24/Oct/11 19:19;cburroughs;Our custom build of zkclient isn't in any repo and does not have a pom, so it can not pull in any dependencies.;;;","24/Oct/11 19:41;nehanarkhede;Oh, I missed that. I tried to apply the patch, but it doesn't apply on a clean checkout of the repository. Could you rebase and upload it again ?;;;","26/Oct/11 23:55;cburroughs;Sorry missed your reply, rebased as of 76957e5e3a748b59525e5e7934f93721eb8f4c38;;;","30/Oct/11 21:40;nehanarkhede;+1. Just committed this, but realized you could've done it yourself.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Producer perf test fails against localhost with > 10 threads,KAFKA-88,12518225,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jkreps,jkreps,jkreps,08/Aug/11 17:36,08/Aug/11 19:09,14/Jul/23 05:39,08/Aug/11 18:44,0.6,,,0.7,,,,,,,config,,,0,,,,"The perf test starts producing errors when it is run with --threads 11 (or higher). The cause is that we create a zookeeper connection per thread, and zookeeper recently added a feature which limits the number of connections per ip in ZOOKEEPER-336. This setting is set to 10 by default. I recommend we bump this up in our packaged zk config, since it is hard to figure this out and makes it look like the client itself is having issues.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/11 17:37;jkreps;zk-config.diff;https://issues.apache.org/jira/secure/attachment/12489715/zk-config.diff",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,65727,,,Mon Aug 08 19:09:58 UTC 2011,,,,,,,,,,"0|i15yvj:",242950,,,,,,,,,,,,,,,,,,,,"08/Aug/11 17:37;jkreps;Change zk config to allow unlimited connections.;;;","08/Aug/11 18:28;junrao;Thanks, Jay. Looks good. Just committed this.;;;","08/Aug/11 19:06;cburroughs;Is this a zk conn per thead only for perf code,  Or do the producers/consumers do this?;;;","08/Aug/11 19:09;jkreps;There is effectively one producer per test thread, this is good as it is more realistic i think.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"the ""design"" link on the home page 404's",KAFKA-86,12518124,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,jkreps,rberger,rberger,06/Aug/11 21:32,07/Aug/11 01:37,14/Jul/23 05:39,07/Aug/11 01:37,,,,,,,,,,,website,,,0,,,,"The link for ""design"" in See our design page for more details. is broken (http://incubator.apache.org/kafka/design.php)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,65842,,,Sun Aug 07 01:37:07 UTC 2011,,,,,,,,,,"0|i15yv3:",242948,,,,,,,,,,,,,,,,,,,,"07/Aug/11 01:37;jkreps;Nice catch! I fixed both links. Should be fixed on the site in a few hours as soon as the weird delayed apache sync thing happens.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Options in SyncProducerConfig and AsyncProducerConfig can leak,KAFKA-83,12517962,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,jjkoshy,jjkoshy,04/Aug/11 20:31,12/Nov/11 21:13,14/Jul/23 05:39,12/Nov/11 21:13,0.6,0.7,,,,,,,,,config,,,0,,,,"There is the high-level producer api, and then there is the sync producer and then there is the async producer. Some config options are shared across these which leads to a small degree of confusion. I have a diagram lying around that I can attach to this jira. Anyway, due to this sharing the ProducerPool code copies around properties which is unsafe, because a developer who adds a new option may forget to copy it over. The fix is simple and should make the preceding summary a bit more clear.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/11 20:55;jjkoshy;kafka-83.patch;https://issues.apache.org/jira/secure/attachment/12489398/kafka-83.patch","04/Aug/11 20:54;jjkoshy;kafkaproducerconfig.pdf;https://issues.apache.org/jira/secure/attachment/12489397/kafkaproducerconfig.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,65810,,,Sat Nov 12 21:13:24 UTC 2011,,,,,,,,,,"0|i15yuf:",242945,,,,,,,,,,,,,,,,,,,,"04/Aug/11 20:54;jjkoshy;Config hierarchy.;;;","04/Aug/11 20:55;jjkoshy;Patch for KAFKA-83;;;","05/Aug/11 00:25;nehanarkhede;Looks great. 
+1;;;","07/Aug/11 23:56;junrao;Thanks, Joel. Just committed this.;;;","12/Nov/11 21:13;jkreps;Closing as I believe this has been applied.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong path in bin/kafka-run-class.sh ,KAFKA-81,12517818,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,cburroughs,cburroughs,03/Aug/11 18:07,14/Jun/13 03:56,14/Jul/23 05:39,14/Jun/13 03:56,0.6,,,0.8.0,,,,,,,packaging,,,0,,,,"https://github.com/kafka-dev/kafka/issues/28

{{monospace}}
I just downloaded the official 0.6 archive:
https://github.com/downloads/kafka-dev/kafka/kafka-0.6.zip

and tried starting zookeeper / kafka.

The above archive will extract the deps into a dir called ""libs"", but in bin/kafka-run-class.sh there's a loop to add the jars in ""lib"" to the classpath:

for file in $base_dir/lib/*.jar;
do
  CLASSPATH=$CLASSPATH:$file
done

It's a little more complicated then that. The tarball also places kafka-0.6.jar in the root of the directory, where no scripts look. config does not seem to have the log4j properties files, which makes zookeeper sad.
{{monospace}}
",,junrao,quipo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,43999,,,Fri Jun 14 03:56:39 UTC 2013,,,,,,,,,,"0|i09mdr:",54059,,,,,,,,,,,,,,,,,,,,"05/Aug/11 00:21;nehanarkhede;It will be great to improve the release-zip target in sbt. The following should be the contents of the zip file, created by the ./sbt release-zip file. 

kafka
|_bin            (all scripts that are required to get kafka up and running, also includes various scripts hidden in contrib, java-examples and perf)
|_config       (all related config files )
|_lib             (all dependent libraries required by various scripts in bin)
|_kafka-x.x.jar
|_kafka-java-examples-x.x.jar
|_kafka-perf-x.x.jar
|_hadoop-consumer_x.x.jar
|_hadoop-producer-x.x.jar
;;;","04/Oct/11 02:31;cburroughs;I have not had a change to work on this.  I still intended to, but anyone else should feel free to grab it since we can't release with this.;;;","06/Oct/11 18:02;nehanarkhede;Since, we haven't received a patch yet, we will hand create the release zip for 0.7. Moving this to 0.8;;;","14/Jun/13 03:56;junrao;This is fixed in 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleProducer lose messages when socket gets an io exception,KAFKA-61,12514698,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,,,,0.6,,,,,,,,,,0,,,,"Currently, if we get any io exception while sending a message, SimpleProducer reestablishes the socket connection without resending the message. Thus the message is lost. 

One way to fix this is to only reset socket channel to null when there is io exception during send and throw the exception back to the caller. The caller can capture the exception and resend the message.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67469,,,2011-07-19 21:32:25.0,,,,,,,,,,"0|i15yqv:",242929,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If the producer sends an invalid MessageSet the broker will append it, corrupting the log",KAFKA-60,12514697,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,,,,0.6,,,,,,,,,,0,,,,"It appears our producer request handling is actually a little buggy. We allow messagesets to be ragged on the right (i.e. contain a trailing partial message), but when we append() we ultimately do 
messages.writeTo(channel, 0, messages.sizeInBytes) 
which i believe would append not just the valid messages, but also the partial message, thus corrupting the log. 

We need to set limit() on the ByteBuffer to truncate off invalid trailing messages before writing, or something like that.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67471,,,2011-07-19 21:32:25.0,,,,,,,,,,"0|i15yqn:",242928,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ByteBufferMessageSet logs error about fetch size,KAFKA-59,12514696,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,,,,0.6,,,,,,,,,,0,,,,"Not sure how this happened, but someone added an error message about fetch size being too small in ByteBufferMessageSet. This obviously makes no sense since this class is used in the producer and broker as well as in the consumer, neither of which have a fetch size. This error needs to be properly handled (say by throwing an error), and each user needs to be modified to handle it appropriately.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67470,,,2011-07-19 21:32:25.0,,,,,,,,,,"0|i15yqf:",242927,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
shutdown kafka when there is any disk IO error,KAFKA-55,12514692,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,,,,0.6,,,,,,,,,,0,,,,"Currently, if we encounter any IO error while writing to a kafka log, we simply log the error and continue. However, this kind of errors could leave the log in a corrupted state (e.g., only part of a message is added to the log). When this happens, we should stop accepting new requests and force kafka to shutdown. Once kafka is restarted, log recovery can clean up any corrupted log.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67473,,,2011-07-19 21:32:24.0,,,,,,,,,,"0|i15ypz:",242925,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Propagate server all exceptions to consumer,KAFKA-54,12514691,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,,,,0.6,,,,,,,,,,0,,,,"Currently, we only propagate a few known exceptions to the consumer. We should propagate all exceptions to the consumer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67474,,,2011-07-19 21:32:24.0,,,,,,,,,,"0|i15ypr:",242924,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
close() in SimpleConsumer should be synchronized,KAFKA-53,12514690,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,,,,0.6,,,,,,,,,,0,,,,"Similar to KAFKA-12, the close method in SimpleConsumer doesn't hold the lock while closing the channel and setting it to null, potentially creating a race condition if a message is being received.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67472,,,2011-07-19 21:32:24.0,,,,,,,,,,"0|i15ypj:",242923,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer Code documentation,KAFKA-52,12514689,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,,,,0.6,,,,,,,,,,0,,,,"The example code for the ""Consumer Code"" section on http://sna-projects.com/kafka/quickstart.php seems to contain a couple of errors. 

Here's the working code: 

{code} 
// specify some consumer properties 
Properties props = new Properties(); 
props.put(""zk.connect"", ""localhost:2181""); 
props.put(""zk.connectiontimeout.ms"", ""1000000""); 
props.put(""groupid"", ""test_group""); 

// Create the connection to the cluster 
ConsumerConfig consumerConfig = new ConsumerConfig(props); 
ConsumerConnector consumerConnector = Consumer.create(consumerConfig); 

// create 4 partitions of the stream for topic ""test"", to allow 4 threads to consume 
Map<String, List<KafkaMessageStream>> topicMessageStreams = 
consumerConnector.createMessageStreams(ImmutableMap.of(""test"", 4)); 
// create list of 4 threads to consume from each of the partitions 
List<KafkaMessageStream> streams = topicMessageStreams.get(""test""); 
ExecutorService executor = Executors.newFixedThreadPool(4); 

// consume the messages in the threads 
for (final KafkaMessageStream stream : streams) { 
executor.submit(new Runnable() { 
//final KafkaMessageStream stream = topicStream.getValue(); 
public void run() { 
for (Message message : stream) { 
// process message 
} 
} 
}); 
} 
{code} 

It might also be worth specifying the imports: 

{code} 
import kafka.consumer.*; 
import kafka.message.Message; 

import java.util.Properties; 
import java.util.Map; 
import java.util.List; 
import java.util.concurrent.ExecutorService; 
import java.util.concurrent.Executors; 

import com.google.common.collect.ImmutableMap; 
{code}",web,,,,,,,,,,,,,,,,,,,,,,,,,,,,,300,300,,0%,300,300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67477,,,2011-07-19 21:32:23.0,,,,,,,,,,"0|i15ypb:",242922,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getOffsetsBefore() returns wrong offset when given a specific timestamp,KAFKA-51,12514688,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,,,,0.6,,,,,,,,,,0,,,,"When a specific timestamp is specified, getOffsetsBefore() always returns the current HW, which is incorrect.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67476,,,2011-07-19 21:32:23.0,,,,,,,,,,"0|i15yp3:",242921,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add acknowledgement to the produce request.,KAFKA-49,12514686,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,prashanth.menon,junrao,,19/Jul/11 21:32,02/May/13 02:29,14/Jul/23 05:39,28/May/12 18:02,0.8.0,,,0.8.0,,,,,,,,,,0,,,,"Currently, the produce request doesn't get acknowledged. We need to have a broker send a response to the producer and have the producer wait for the response before sending the next request.",,craigwblake,felixgv,heavydawson,prashanth.menon,sharadag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-50,,,,,,,,KAFKA-57,,,,KAFKA-305,,,,,,,,,,,KAFKA-73,,KAFKA-240,,"05/Apr/12 00:50;prashanth.menon;KAFKA-49-continued-v2.patch;https://issues.apache.org/jira/secure/attachment/12521435/KAFKA-49-continued-v2.patch","29/Mar/12 17:54;prashanth.menon;KAFKA-49-continued.patch;https://issues.apache.org/jira/secure/attachment/12520461/KAFKA-49-continued.patch","07/Mar/12 03:35;prashanth.menon;KAFKA-49-v1.patch;https://issues.apache.org/jira/secure/attachment/12517358/KAFKA-49-v1.patch","13/Mar/12 00:57;prashanth.menon;KAFKA-49-v2.patch;https://issues.apache.org/jira/secure/attachment/12518130/KAFKA-49-v2.patch","14/Mar/12 01:17;prashanth.menon;KAFKA-49-v3.patch;https://issues.apache.org/jira/secure/attachment/12518276/KAFKA-49-v3.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67068,,,Mon May 28 18:02:36 UTC 2012,,,,,,,,,,"0|i09m33:",54011,,,,,,,,,,,,,,,,,,,,"16/Nov/11 06:03;jkreps;If there are any other producer-related changes coming we should try to batch these together at the same time to avoid having to update clients too often.;;;","22/Nov/11 14:19;prashanth.menon;So I've started a little work on this.  Looks to me like the ProducerRequest is going to need an additional ""acknowledge"" boolean field (default false) which we send along with the rest of the fields.  On the producer side, there are a couple of options:

1. We can leave the producer API and have producers (both sync and async) acknowledge all or non of their messages.  This behaviour will be driven by a configuration field or a class parameter.
2. We add the acknowledgement parameter (default false) to the producer send API for and leave the remaining behaviour the same.  The producer then handles waiting or not waiting for acks on a per-message case.
 
I would prefer the second option as it's simple to do and gives the option to clients.  Waiting on the producing end shouldn't be an issue.  On the broker side, it becomes easy as we just send a boolean response after handling the request (ditto for the multi-produce request).  

Did anyone else have any thoughts on this?;;;","22/Nov/11 16:36;junrao;Prshanth, thanks for getting started on this. I agree with your second approach. Basically, add a new parameter in SyncProducer.send/multisend to indicate whether an ack is needed or not. The high level producer can then set that parameter based on ProducerConfig (probably true for sync mode and false for async mode). 

Another question is what kind of ack does the broker send back. A simple approach is to send back a boolean. Another possibility is to return for each partition in the produce request, the latest offset after the request is served. Some clients could potentially make use of the returned offset.;;;","22/Nov/11 17:05;tgautier;I second returning the new offset.;;;","22/Nov/11 17:36;jkreps;+1 for returning the offset

If we are changing the request format it would also be good to think it through in some detail to get it right since these kinds of API changes are harder to rollout then to make. Some questions:
1. Are we trying to maintain compatibility for this change? If so we should bump up the request id number and ignore the new fields for the old request id. This is not too hard, but requires a little extra work.
2. Currently we have ProduceRequest and MultiProducerRequest and FetchRequest and MultiFetchRequest. I recommend we rename MultiProducerRequest to ProduceRequest and delete the existing ProduceRequest. The current ProduceRequest has no advantages over MultiProducerRequest and having both means each change we make has to be done for both. The two variations are just there for historical reasons--originally there was no multi-* version of the requests and we added that later. I recommend we do the same for the FetchRequest/MultiFetchRequest. This will make our lives simpler going forward.
3. Both of the MultiProducerRequest has the format [(topic, partition, messages), (topic, partition, messages), ...]. This is because it is just a bunch of repeated ProducerRequests. This is really inefficient, though, as a common case is that we are producing a bunch of messages for different partitions under the same topic (i.e. if we are doing the key-based partitioning). It would be better for the format to be [(topic, [(partition, messages), ...], topic, [(partition, messages), ...], ...]. This would mean that each topic is only given once.

I would like to get a quick consensus on the desired format of the produce and fetch requests up front, then we can break this into appropriate sub tasks so we don't expand the scope of Prasanth's work too much.;;;","22/Nov/11 18:04;junrao;Both 2 and 3 make sense. 

If we do this in a bug fix release in 0.7, we probably need to maintain backward compatibility. If we do this as part of the replication work, we probably can make a non-backward compatible change. My preference is the latter.;;;","22/Nov/11 18:12;tgautier;Generally I think it's a good idea to have a version embedded into the protocol.  This allows for backwards and forwards compatibility.  In a sense, the request id works as such, so in some sense it's a matter of semantics, but the only way to identify that there are multiple versions of the same request is to have some kind of external mapping that says id 2 and id 8 are really the same request, just different versions.

OTOH, if you use a version, you can then have:

id 2 version 1
id 2 veriosn 2

etc. and this is imho easier to manage.

Usually, the version should in fact be the first value in the protocol, so that you never have formatting issues that lie outside the realm of the versioned data.

Currently, all requests are preceded by a header, which contains the length of the data.  This is where I would start, we should either strive for:

version: 2 bytes
length: 4 bytes

or 

length: 4 bytes
version: 2 bytes

Note that the message request already has a way to represent versions, using the magic field, but honestly I find it a little bit non explicit for my taste.

I would also include a 64-bit ""flags"" area that will allow for future flags to be set to indicate various things.

So, if I were to suggest a standard header for requests and responses it would look like:

length: 2 bytes
version: 2 bytes
reuest id: 2 bytes
flags: 4 bytes
payload: n bytes
;;;","22/Nov/11 18:14;tgautier;It could be possible to split the current request id - 2 bytes - into a version and an id field one byte long each.   Assuming there's not much need for a vocabulary greater than 256 verbs, and versions > 256, this would probably work within the current binary protocol and give backwards compatibility to 0.6 and 0.7 clients...;;;","22/Nov/11 18:38;jkreps;Hi Taylor, as you say the request id was meant to be the version. However in retrospect I do think I like the idea of separating the request and the version of the request. I agree it would be nice to split this.

I think the open question here is whether we should try to maintain backwards compatibility for the next major release. It would probably be very convenient for us at code-writing time not to, but is more painful at rollout time.;;;","22/Nov/11 19:01;tgautier;Well, how many versions do you think you want?  Maybe we could split the request field up into say the first 5 or 6 bits instead of 8 for the versions.;;;","23/Nov/11 05:09;jkreps;So guys, my thought is the best plan of attack would be fully think through the protocol changes for all the use cases we currently know about and do those all at once (even if the features the new fields support aren't yet available). This will avoid doing this in lots of little patches that all conflict, and it will make us think things through holistically.

I created a wiki with some of the outstanding ideas, I am going to move this discussion to the main dev and user lists to get broader feedback. It would be good if people could give their thoughts there so we can try to get these changes right.;;;","05/Jan/12 19:09;nehanarkhede;KAFKA-240 implements the new wire format for producer and consumer. Since this JIRA requires the new format, it depends on KAFKA-240;;;","01/Mar/12 22:33;nehanarkhede;KAFKA-239 is complete and KAFKA-240 is almost there. 

Prashanth, in your comment above, you've mentioned you've started work on this. If so, mind assigning this JIRA to yourself ? This looks like the next JIRA to work on after KAFKA-240 is in. (https://issues.apache.org/jira/browse/KAFKA-50?focusedCommentId=13180712&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13180712);;;","01/Mar/12 22:53;prashanth.menon;Sure, but I actually can't change assignments.  Could be a privileges issue?;;;","01/Mar/12 23:56;nehanarkhede;Looks like you didn't exist in the Apache Kafka JIRA list. Added you there, also assigned this JIRA to you;;;","05/Mar/12 03:51;prashanth.menon;Hey all.  Getting the acknowledgements done was fairly straightforward - I've ""todo-ed"" actually waiting for replica acknowledgements as part of kafka-44 when it introduces true replicas and partitions.  Having the sync producer wait for a response when it requires acknowledgements was trivial.  My question is what to do with the async producer.  Was the intent to perform some logic in the default event handler, or to expect clients to write custom event handlers that deal errors?  Should we bubble up the response to the producer level?;;;","05/Mar/12 22:43;junrao;Prashanth,

That's a good question. Initially, I was just thinking that for async producers, if we get an error during send (after retries), we will just log the error without telling the client. Currently, the event handler is not really extensible. I can image that we add some kind of callback to return those errors. The question is what will the client do on those errors. Will it resend? If so, we will need to pass the failed requests through callback too. I am curious about how other messaging systems like activeMQ do in the async mode.;;;","05/Mar/12 23:20;prashanth.menon;If I remember right, HornetQ allows  you to implement a callback interface to be notified of message acknowledgments.  We could do something similar, passing back the request and response for the erroneous parts.  To your second point, I suppose it depends on the error.  Some may be logged and skipped, some will require a refresh of topic metadata .  The default behaviour should cover the base cases.

Curious to hear other thoughts;;;","05/Mar/12 23:33;junrao;The current defaulteventhandler already refreshes topic metadata on retries. So, if we return any failed request to the client, there is probably not much the client can do, except for logging it. In any case, we should use a separate jira to track if we need any aysnc callback on the producer side.;;;","06/Mar/12 03:39;prashanth.menon;Sounds good to me.  Doing some additional work on DefaultEventHandler, I noticed something off in the retry logic that I'd like to get confirmed.  Consider the case where I've got data destined for more than one broker, say three.  
- Enter handleSerializedData()
  - Partioning and collating makes a map with three key/value pairs (broker -> topic partition data and messages).  
  - Enter for loop
    - Assume the first send works on the first try for broker #1.  
    - Next iteration, the second send to broker #2 fails on the first try, we fall into the retry loop and recursive into handleSerializedData with requiredRetries = 0.
      - In handleSerializedData
      - This time, the partitioned data will one one key/value pair for the single broker (broker #2) we're attempting to resend data to.  
      - Enter for loop
        - Attempt to send data to broker #2, the send succeeds
      - We exhaust the map entries and the for loop condition.
    - We return to the retry loop for retry=1 on broker #2 in the catch block.  
    - The previous send succeeded on first try and now there's the ""return"" statement.  This exists the function, but we have one more broker (broker #3) to send data to.

Does the flow sound about right?  I think what needs to happen is to set a flag and break the retry while loop after a successful retry.  Then we check the flag after the loop and either throw the exception or continue the outer for loop.

Am I crazy?  Am I missing something in my sleep-deprived state here?;;;","06/Mar/12 16:45;junrao;Prashanth,

Yes, that's actually a real bug. Instead of returning in retry, we should just set a boolean to indicate that a send has succeeded. At the end, we will throw FailedToSendMessageException if the boolean is not set. Otherwise, we will continue with the for loop. Could you file a separate jira to fix that? Thanks for catching the bug.;;;","06/Mar/12 20:12;prashanth.menon;Done, created KAFKA-295.  Expect patch for this jira later tonight.;;;","07/Mar/12 03:42;prashanth.menon;I've attached a patch for this.  A few comments:

- Modified ProducerResponse
- Broker does not actually wait for replica ACKS.  This will be done with KAFKA-44.
- Sync producer has been modified to wait for response from broker.  Async producer isn't aware of request level errors, this will require a separate ticket.
- Some general cleanup on producer request, async producer and removal of MultiProduce request key.

One oddity is since we use Arrays in the request and response, it breaks the case class equality/hashcode logic since Java's arrays are broken.  We should probably make them Seq's (separate JIRA?) or WrappedArrays.
;;;","08/Mar/12 17:25;junrao;Prashanth,

Thanks for the patch. Overall, it looks pretty good. Some comments:
1. KafkaApis: Even when the produce request requires no ack, we will still need to send error code back. So we should always send a ProduceResponse. When no ack is specified, we probably don't need to send offsets back.
2. ProduceRequest.getNumMessages: rename it to something like getNumTopicPartitions
3. AsyncProducerTeest.testDefaultHanlderRetryLogic: doesn't really test retry
4. SyncProducerTest.testProduceBlocksWhenRequired: Use createTopic instead of creating ZK path directly.
5. I agree that it's probably better to use Seq in our requests/response, instead of Array. Then we need a java version to convert Seq to java array and vice versa. Please open a separate jira to track this.
;;;","09/Mar/12 02:50;prashanth.menon;Thanks for the pointers!  

1.  Hmmm, do you propose returning an empty offsets array back to the client when no ack is required?  That seems perfectly reasonable since the broker can't make guarantees as to the offsets; but it  does feel somehow incongrous since one would expect the errors and offsets array sizes to be equal.  I'm fine with the idea as long as it's agreed in the wire format.  If I've completely missed the point, forgive me!
2.  Done.
3.  Done.  Wow, that wasn't supposed to be included.  It was part of my sanity check for the incorrect retry logic I mentioned earlier.
4.  Done.
5.  Done.;;;","09/Mar/12 05:03;junrao;1. Or we can just treat noacks the same as act=1 for now.;;;","12/Mar/12 00:18;prashanth.menon;A few more concerns popped up as a result of making the send in syncproducer blocking.  

1. Edit: So it turns out that using the channel in SyncProducer like we are to perform reads won't trigger socket timeouts though we set it and will block forever which is bad news(check http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4614802 and http://stackoverflow.com/questions/2866557/timeout-for-socketchannel for workaround).  The latter post has a simple solution that involves creating a separate ReadableByteChannel instance for timeout-enabled reads.  The other option being using non-blocking sockets with selectors which is more complex.
2.  It is conceivable that a broker listed in the replica set /brokers/topics/[topic]/partitions/[partition]/replicas is offline or shutdown which means their ephemeral entries are missing in ZK.  A problem then arises when an active broker attempts to pull metadata and node information for topics these brokers host since AdminUtils assumes any broker in AR or ISR must have paths/info in ZK /brokers/ids/[brokerId], but since they don't an NoNodeException is thrown.  A corner case for sure, but something that should probably be fixed.;;;","13/Mar/12 01:01;prashanth.menon;I've attached an updated patch.  

1. ProduceRequest always receives a ProducerResponse.  If acks=0, offsets in the response are treated as if acks=1, meaning only the leader has acked.
2. SyncProducer blocks waiting for a response from a ProducerRequest.
3. I've commented out ProducerTest.testZKSendWithDeadBroker since it relies on SyncProducer logic that will need to change.  It also will need to be rewritten with the changes coming in as part of KAFKA-45 leader election logic.

Let me know if I've missed anything!
;;;","14/Mar/12 00:33;junrao;Prashanth,

I am ready to commit your patch. A couple things:

1. Your patch added a new class ResponseHandler, but it's not used. Should that be removed?
2. Your concern #1 is valid. Could you create another jira to track this?
3. For your concern #2, it's ok for getMetadataApi to return empty leader in a transition state. The client will simply backoff a bit and call getMetadataApi again.;;;","14/Mar/12 01:26;prashanth.menon;Hi Jun, I've attached a new patch.

1. Yes, I've removed it.
2. Done.
3. You are correct in the case of leaders, but I believe the problem stands when pulling topic metadata with atleast one offline broker listed in the assigned replicas.

;;;","14/Mar/12 01:38;junrao;Thanks for the patch Prashanth. Just committed to 0.7 branch.

For 3, if a broker is offline, then eventually it will not be in ISR. In the transition state, we could have an ISR broker without matching host and port.;;;","15/Mar/12 05:24;nehanarkhede;Sorry for visiting this late. I have a few questions about producer ACK. 

1. In DefaultEventHandler, the producer ACK is not used. Shouldn't it be used to figure out if the send operation needs to be retried ? 
2. In DefaultEventHandler, should the producer wait for ACK and timeout if it doesn't receive one ?
3. In KafkaApis, why doesn't the broker send an error back to the producer if it received a producer request for a partition that is not hosted on that broker ?;;;","15/Mar/12 13:21;prashanth.menon;Better late than never.  A second review is always a plus!  To your points:

1. Absolutely, this was overlooked.  Expect patch later tonight or tomorrow.
2. The DefaultEventHandler does wait for the ack by waiting for the response.  Unfortunately, the current SyncProduer doesn't timeout correctly for which KAFKA-305 was created.
3. KafkaApis does not explicitly do the check, instead relying on LogManager which currently does.  It makes sense to move that piece of logic along with the TODO from LogManager into KafkaApis for clarity and to separate ZK from LogManager.

What do you think?;;;","15/Mar/12 21:57;nehanarkhede;1. I'm refactoring some part of DefaultEventHandler as part of KAFKA-300. I'll upload a patch tonight. You can either choose to work off of the changed code or not. Your call.
2. Sounds good
3. In handleProduceRequest, the logManager.append() throws InvalidPartitionException when it receives a request for a partition that it does not own. Does it make sense to send an ACK to the producer with an error code like  NotLeaderForPartitionException ?

;;;","16/Mar/12 17:16;nehanarkhede;Reopening this issue to address some review suggestions and to fix KAFKA-305 as part of this JIRA;;;","28/Mar/12 16:22;nehanarkhede;Prashanth, thanks for resolving KAFKA-305 ! Would you be up for finishing up the remaining work on this ? It seems like a good idea to complete earlier JIRAs, before moving to the later ones.;;;","29/Mar/12 17:54;prashanth.menon;Okay, I've attached a patch that should take care of the outstanding items.  A couple of points:

1. KafkaApis not checks whether a partition has a leader on the broker.  It uses KafkaZooKeeper to check this, for now, but should probably rely on ReplicaManager and the Replica itself to determine this.  KAFKA-46 should take care of this.
2. I've removed the random partition check on the server-side since the partitioning is done in default event handler.  Producers should know which broker a partition belongs to.
3. I've added a new NotLeaderForPartitionException and added it to ErrorMapping so clients can receive it.
4. DefaultEventHandler.send now returns a list of topic/partition tuples that represents those messages that need to be resent due to an error.
5. Due to the changes in KafkaApis produce check, some of the tests have been modified to ensure topics are in ZK and to wait for leadership.

I think that covers all, please point out any issues!;;;","29/Mar/12 23:03;nehanarkhede;+1. Thanks for incorporating the review suggestions !;;;","03/Apr/12 18:25;junrao;Prashanth,

Patch looks good: Just one minor thing.

6. Unused imports: KafkaZookeeper

While looking at the patch, I realized that there are a couple of other things that we will need to follow up.

a. In DefaultEventHandler, it seems that we rely on the fact that broker.id is a non-negative integer. However, we don't enforce that in broker startup.
b. With the create topic ddl, some of the broker configs like topic.partition.count.map probably don't make sense anymore.

I will create a jira for each item to follow up.;;;","05/Apr/12 00:50;prashanth.menon;Thanks for the review!  I've attached newest patch for #6 resolved.;;;","05/Apr/12 18:17;junrao;Prashanth,

Thanks for the patch. It seems that kafka-48 is almost ready. Since that's a relatively large patch, I will commit your patch after kafka-48 is committed.;;;","07/May/12 21:45;junrao;Prashanth,

Now that kafka-48 is committed to 0.8, we can commit your patch. Since you are a committer now, could you commit this yourself?;;;","24/May/12 02:20;prashanth.menon;Sorry for the delay everyone.  I'm planning to block off some time this weekend to commit this patch, and hoping I don't run into any access/permissions issues :);;;","28/May/12 18:02;prashanth.menon;Excellent, committed this to 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Implement optional ""long poll"" support in fetch request",KAFKA-48,12514685,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,jkreps,junrao,,19/Jul/11 21:32,30/Apr/12 21:42,14/Jul/23 05:39,30/Apr/12 21:42,,,,,,,,,,,,,,0,,,,"Currently, the fetch request is non-blocking. If there is nothing on the broker for the consumer to retrieve, the broker simply returns an empty set to the consumer. This can be inefficient, if you want to ensure low-latency because you keep polling over and over. We should make a blocking version of the fetch request so that the fetch request is not returned until the broker has at least one message for the fetcher or some timeout passes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-202,,KAFKA-50,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/12 18:29;jkreps;KAFKA-48-v2.patch;https://issues.apache.org/jira/secure/attachment/12521353/KAFKA-48-v2.patch","04/Apr/12 18:39;jkreps;KAFKA-48-v3.patch;https://issues.apache.org/jira/secure/attachment/12521358/KAFKA-48-v3.patch","09/Apr/12 20:35;jkreps;KAFKA-48-v4.patch;https://issues.apache.org/jira/secure/attachment/12522016/KAFKA-48-v4.patch","03/Feb/12 22:49;jkreps;KAFKA-48.patch;https://issues.apache.org/jira/secure/attachment/12513197/KAFKA-48.patch","09/Apr/12 20:37;jkreps;kafka-48-v3-to-v4-changes.diff;https://issues.apache.org/jira/secure/attachment/12522017/kafka-48-v3-to-v4-changes.diff",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,63278,,,Mon Apr 30 21:42:45 UTC 2012,,,,,,,,,,"0|i0l46f:",121327,,,,,,,,,,,,,,,,,,,,"14/Aug/11 22:01;jkreps;This latency issue is important for replication because the latency for the producer will now depend on the replication (fetching) on the followers. This means our current polling mechanism is not going to be good, because we have to either back off for a period of time to avoid busy waiting on the server. In addition with replication we need a similar ability to process requests asynchronously--we do not want to block any threads while waiting for acks from followers. This also breaks our simple request/response model.

This is also important for the streaming use cases as they involve a large number of stacked topics, and hence the end-to-end latency is a multiple of the single-hop consumer latency.

Fixing this is slightly tricky.

The first thing I think we would need to do is move the execution of request handling out of the socket server threads. This would generally be a good thing to do anyway, as I/O currently blocks request handling for all sockets sharing a thread. This can add unnecessary latency.

The design for this could be a request BlockingQueue that the SocketServer submits all requests to, and N response BlockingQueues, one for each socket thread. The request processing would happen in a separate threadpool that would feed off the request queue and send responses back to the response queue. For asynchronous requests, no response need be enqueued.

The request handling would now be an ExecutorService with a fixed number of processes. Each process would poll the request queue, process any request it gets, and send back responses.

Long poll requests from fetchers would either be handled immediately, or, if there is no available data, would add themselves to a list of watchers on the topic. When a request comes in on that topic, it would and responses for all watchers. The request would specify a max timeout after which the request would return empty, this could be implemented with a DelayQueue that was checked periodically for expired requests. A generalization of this would be to have the fetch request provide not only a max_wait_time but also a min_data_size, which would make the request block until the given number of bytes of data have accumulated. This would actually enable the opposite of simple long poll--instead of trying to minimize latency the fetcher would be able to ensure they got a good size chunk of data on each request to ensure good throughput and avoid lots of little requests each fetching only a few small messages.

A similar mechanism would be possible for acknowledgements coming from followers. When a produce request occurs with a min_ack_count > 1, the request would go into a list of waiting requests for that topic/partition. When the ack request comes in from followers, we would check the waiting producers and add responses to the response queue for any newly unblocked request.

I would like to do a round of refactoring on the SocketServer anyway, so let me know if anyone has comments on this before i go do anything too crazy. For example, I want someone else to validate the interaction with the replication design.
;;;","15/Aug/11 16:43;junrao;Jay, thanks for carefully thinking ahead. I agree that we will need to decouple the socket processor thread from the handler thread, to make it easy for producers to wait for acks from followers, and for the consumers to block until new data is produced. We probably need 1 request queue and 1 response queue per socket processor thread. That way, we can ensure that the response is always handled by the socket thread that has registered the needed socket key for the response.;;;","31/Oct/11 05:17;jkreps;This is a draft patch that refactors the socket server to make requests and responses asynchronous. No need for a detailed review, it still needs a lot of cleanup, but I wanted to show people the idea in more detail.;;;","31/Oct/11 15:38;tgautier;Hi - please keep in mind the use case where a consumer is interested in more than one topic.

This feature if implemented only for one topic will not be useful for this use case - assuming it's infeasible to open multiple tcp connections.

The first proposal I have is to allow the request to contain a list of topics.  However, upon consideration, this would require the response to also be adjusted such that it would contain the name of the topic, otherwise it would be next to impossible to ascertain which topic the response corresponds to - well it could be done such that the response is returned in the same way as the request was requested, and for topics with no messages, an empty response is given, but this seems pretty bad from a network bandwidth standpoint.

So my final proposal would be to introduce an epoll like request/response.  The consumer would submit a request with a list of interested topics, and the response would be a topic and # of messages available on that topic when the topic(s) have messages.

The advantage to this solution is that it would be entirely backward compatible, since you would simply introduce a new request/response pair and it would also allow the consumer to decide which topics to poll (or pull) from first, so that it could prioritize, if it wanted.  

Finally, I like the idea of allowing the consumer to specify a min # of messages required to trigger the poll, you might want to copy the pattern you already setup for log flushing, e.g. max time and/or min # of messages.  So the request might look like:

list-of :  topic-name:min msgs:max time

and the response might be:

list-of : topic-name:# msgs available


;;;","31/Oct/11 18:31;jkreps;Yes, these are all good points. The work I have done so far just splits request processing into a separate thread pool and enables asynchronous handling. This is a fairly general thing we need for a few different use cases. Perhaps this should be broken into a separate JIRA.

I have thought a little bit about how to do long poll, though. Logically what I want to do is make it possible to give a minimum byte size for the response and a maximum delay in ms; then have the server delay the response until we have at least min_bytes messages in the response OR we hit the maximum delay time. The goal is both to improve latency (by avoiding waiting in between poll requests), to reduce load on the server (by not polling), and to make it possible to improve throughput. If you set min_bytes = 0 or max_delay_ms = 0 you effectively get the current behavior. The throughput improvement comes if you set the min_bytes > 1; this would give a way to artificially increase the response size for requests to the topic (i.e. avoid fetching only a few messages at a time) while still giving hard latency guarantees. We have seen, the request size is one of the important things for network throughput.

As you say, the only case to really consider is the multi-fetch case. The single topic fetch can just be seen as a special case of this. I think your first proposal is closer to what I had in mind. Having the response contain an empty message set for the topics that have no data has very little overhead since it is just positionally indexed, so it is like 4 bytes or something. I don't like doing a poll() style interface that just returns ready topics doesn't seem very useful to me because the only logical thing you can do is then initiate a fetch on those topics, right? So might as well just send back the data and have a single request type to worry about?

One of the tricky questions for multifetch is what does the minimum byte size pertain to? A straight-forward implementation in the current system would be to add the min_bytes and timeout to the fetch request which would effectively bundle it up N times in the multi-fetch (currently multi-fetch is just N fetches glued together). This doesn't really make sense, though. Which of these minimum sizes would cause the single response to be sent? Would it be when all conditions were satisfied or when one was satisfied? I think the only thing that makes sense is to set these things at the request level. Ideally what I would like to do is remove the fetch request entirely because it is redundant and fix multi-fetch to have the following:
   [(topic1, partitions1), (topic2, partitions2),...], max_total_size, max_wait_ms
This also fixes the weird thing in multifetch now where you have to specify the topic with each partition, so a request for 10 partitions on the same topic repeats the topic name 10 times. This is an invasive change, though, since it means request format changes.

I am also not 100% sure how to implement the min_bytes parameter efficiently for multi-fetch. For the single fetch case it is pretty easy, the implementation would be to keep a sort of hybrid priority queue by timeout time (e.g. the unix timestamp at which we owe a response). When a fetch request came in we would try to service it immediately, and if we could meet its requirements we would immediately send a response. If we can't meet its min_bytes requirement then we would calculate the offset for that topic/partition at which the request would be unblocked (e.g. if the current offset is X and the min_bytes is M then the target size is X+M). We would insert new requests into this watchers list maintaining a sort by increasing target size. Each time a produce request is handled we would respond to all the watching requests whose target size is < then new offset, this would just require walking the list until we see a request with a target size greater than the current offset. All the newly unblocked requests would be added to the response queue. So this means the only work added to a produce request is the work of transferring newly unblocked requests to the response queue and at most we only need to examine one blocked request.

The timeout could be implemented by keeping a priority queue of requests based on the unix timestamp of the latest allowable response (i.e. the ts the request came in, plus the max_wait_ms). We could add a background thread to remove items from this as their timeout occurs, and add them to the response queue with an empty response.
 
For the multifetch case, things are harder to do efficiently. The timeouts can still work the same way. However the min_bytes is now over all the topics the request covers. The only way I can see to implement this is to keep a counter associated with each watcher, and have the watcher watch all the requested topics. But now on each produce request we need to increment ALL the watchers on the topic produced to.

Dunno, maybe for practical numbers of blocked requests (a few hundred? a thousand?) this doesn't matter. Or maybe there is a more clever approach. Ideas welcome.
;;;","31/Oct/11 19:02;tgautier;I can see how it would be reasonable to do the first approach.  It does limit one use case I was considering, which is to allow the consumer to decide in which order to fetch the topics after the poll is triggered, however, this can be done at request time when the topics are requested.

As you say, the response is 100% compatible, it's just the request that changes.  Therefore it would make sense I think to go ahead and make a new request type that doesn't yet exist and then the current fetch request remains the same on the wire and the behavior of it is just a degenerate case of this new use case with delay and bytes set to 0.

I think you might consider how useful is it to worry about user specified time/bytes?  It will add a lot of complexity to your implementation, and frankly if I have just the ability to do a multi-fetch that will wait until something has arrived and send me whatever it has at the current moment that will be good enough.  A minimum implementation should also probably provide a simple timeout that will respond with nothing if the timeout expires.

I think the simple implementation by itself a huge win and you might consider -- is that good enough?

For me it is - I would prefer to get the simple thing in the short term and wait for the harder thing in the long-term.;;;","31/Oct/11 19:18;jkreps;Hi Taylor,

Could you give a little more detail on your use case for ordering the fetches? I think you have a use case I haven't thought of, but I don't know if I understand it. Is your motivation some kind of quality of service over the topics?

As you say, this would definitely be a new request type for compatibility, and we would probably try to deprecate the old format over the next few releases as we can get clients updated.

Your point about complexity is valid. I think for our usage since we use kafka very heavily the pain of grandfathering in new APIs is the hardest part, and the socket server refactoring is next, so I was thinking the difficulty of implementing a few internal data structures is not too bad. I suppose it depends on if I work out a concrete plan there or not. If the best we can do is iterate over the full set of watchers it may not be worth it.;;;","31/Oct/11 20:36;tgautier;Actually, I don't have a valid use case for priority fetches, I was just thinking ahead.

I agree that it's painful to have message format upgrades.  On the flip side of course we probably also agree it's bad to have parameters in the message header that don't correspond to real features.  

Can you make a trade-off and reserve some bytes for these two int (or long) parameters and/or a few others but just call the space reserved?;;;","11/Nov/11 23:32;junrao;Just had a chance to look at the patch. Agree in principle this would work. It's probably better to create a separate jira for moving the requesthandler out of socket server. The long poll jira will depend on that jira.;;;","12/Nov/11 03:25;jkreps;Cool, moved it.;;;","21/Nov/11 08:15;tgautier;I've been staring at the code for a while - and I'm not sure I understand why you need KAFKA-202 to implement this feature.

What I am thinking to do is:
1) Every thread has to open a local socket for read/write
2) Each thread puts the socket into the poll set for reading
3) If a read request fails to read any messages, when it comes back to the handler, the handler adds a callback method to the appropriate log and puts the read request into a special queue.  When that log gets messages for write, it calls the callback.  The callback writes a byte into the special thread socket.
4) The byte wakes up the thread, which sees that the special socket had a byte written to it, and so it goes and re-handles the read requests in the special queue as if they had just come in from the network. Thus if there are any messages available in the log for a given request, they are read just like normal and transferred out onto the channel.  If not, they're re-queued as per step 3.

I think there is some pieces I haven't quite got right - in particular, I think there can only be one active response at a time.  Thus there will have to be some sort of response queue built up as each request generates a response, but I think that's simple - the handler just writes responses with non-zero messages into a response queue and the write logic of the socketserver is updated to drain this queue on write events (at the moment, it only deals with one response at a time, but now it may have many to send out queued up).

Some other work that is probably going to be more difficult is that the binary protocol has to change to include the topic name or else there is no way to disambiguate the responses coming back.;;;","02/Jan/12 17:19;junrao;Taylor,

Sorry for the late response. I am not sure that I understand your proposal. 

a. Why do we need a local socket? It seems that the same thing can be achieved by just turning on the write_interesting bit in the socket key corresponding to a client request.

b. It's not clear to me how you correlate a queued client request with the corresponding client socket.;;;","03/Feb/12 22:49;jkreps;This is a very rough draft of long poll support. It appears to work. Here are some remaining issues:
1. I need the updated request objects to properly get the new fields (min_bytes, max_wait). Currently I am just hard-coding some made-up values.
2. This patch is very specific to long poll support for fetch requests, it will require more generalization to support our other async case, namely delaying produce requests until a certain number of slaves are caught up.
3. There are still some unit test problems.
4. Code is a little rough still.

Take a look if interested, I will discuss with a few people and clean up a little more before asking for a real review.;;;","03/Feb/12 23:08;tgautier;Jay - that's great to hear!! Would you mind summarizing the way that the long-poll works?  I know that several different implementations were suggested here on the thread and I wanted to know which one you ultimately decided to go with.;;;","03/Feb/12 23:22;jkreps;Hey Taylor, here are the nitty gritty details:
- When a fetch request comes in we immediately check if we have sufficient data to satisfy it
   - if so we respond immediately
   - If not we add a ""watch"" on the topics that the fetch is for, and add it to a delay queue to expire it after the given timeout
   - There is a background thread that checks the delay queue for expired requests and responds to them with whatever data is available
- When a produce request comes in we update the watchers for all the topics it produces to, and increment their byte count. Any requests that have been satisfied by this produce, are then executed and responses are sent.

So one of the earlier questions was how to support polling on a very large number of topics AND wants very low latency, I think as you described it would be possible to implement this by simply multiplexing the requests on the single socket and letting the server respond to these as possible.;;;","03/Feb/12 23:29;jkreps;Two other issues with this patch, I forgot to mention:
- There is a race condition between checking the available bytes, and adding the watchers for the topics. I *think* this is okay since the min_bytes is a minimum not a maximum, so in the rare case that a produce comes in before the watchers are added we will just wait slightly longer than we should have. I think this is probably better than properly synchronizing and locking out all produces on that partition.
- The other issues is that the delay queue is only emptied right now when the delay expires. If the request is fulfilled before the delay expires, the request is marked completed, but it remains in the delay queue until it expires. This is a problem and needs to be fixed. The problem is that if the client sets a low min_bytes and a high max_wait these requests may accumulate. Currently we would have to do an O(N) walk of the waiting requests to fix this. I am going to try to come up with an improved set of data structures to fix this without requiring that.;;;","21/Feb/12 03:48;junrao;Overall, the patch looks good. Some comments:

1. DelayedItem.compareTo: yourEnd should be delayed.createdMs + delayed.delayMs
2. Suppose that a client issues MultiFetch requests on a hot topic and a cold topic. What can happen is that the watcher list for the cold topic won't be cleaned up for a long time. One solution is to have a cleaner thread that periodically wakes up to remove satisfied items. The cleaner thread can be used to clean up the DelayQueue too.
3. MessageSetSend.empty is not used.
;;;","04/Apr/12 18:29;jkreps;This version of the patch updates the code to work with the new request objects and correctly respect the min_bytes and max_fetch_wait settings.

Please review the new configs and make sure we are happy with the naming.;;;","04/Apr/12 18:39;jkreps;Oops, missing about a bazillion files on that last patch.;;;","05/Apr/12 17:39;junrao;Thanks for patch v3. Some comments:

31. DelayedFetch is keyed off topic. It should be keyed off (topic, partition) since a consumer may be interested in only a subset of partitions within a topic.

32. KafkaApis: The following 3 lines are duplicated in 2 places.
      val topicData = readMessageSets(delayed.fetch.offsetInfo)
      val response = new FetchResponse(FetchRequest.CurrentVersion, delayed.fetch.correlationId, topicData)
      requestChannel.sendResponse(new RequestChannel.Response(delayed.request, new FetchResponseSend(response, ErrorMapping.NoError), -1))
Should we put them in a private method and share the code?

33. ExpiredRequestReaper.purgeExpired(): We need to decrement unsatisfied count here.

34. FetchRequest: Can we have the default constants for correlationId, clientid, etc defined and shared btw the constructor and the request builder?

35. MessageSetSend.empty is unused. Should we remove it?;;;","09/Apr/12 20:35;jkreps;Jun, thanks for the feedback. This patch hopefully addresses your comments:
1. I removed the empty flag, as you suggested from MessageSetSend
2. I would like to leave the ugly duplicate code for now. Making a seperate method for this doesn't really make sense as it isn't really a stand alone piece of code. I think the root problem is that action you do when the request is satisfied can be done either synchronously (if possible), asynchronously when the criteria are satisfied, or asychronously when the request expires. I think the right way to do this is to refactor RequestPurgatory a bit and somehow always use the same callback for all three cases. I would like to address this as a seperate patch because this idea is not fully baked yet.
3. The default values are now shared between the builder and constructor.
4. I changed the key to be (topic, partition) for FetchRequestPurgatory. That was a major oversite.
5. The purgeExpired method is actually misnamed it is really purgeSatisfied, so it doesn't need to decrement the satisfied count. However there is a major bug in that count, it wasn't getting decremented by the processing thread. I added a new method to cover this.;;;","09/Apr/12 20:37;jkreps;I attached a diff that just shows the changes between v3 and v4 for folks who already looked at v3.;;;","10/Apr/12 17:35;junrao;Patch v4 looks good. Just one more comment.

41. RequestPurgatory.update(): if(w == null), could we return a singleton empty array, instead of creating a new one every time?;;;","10/Apr/12 17:54;jkreps;Good point Jun, now it is
    if(w == null)
      Seq.empty
    else
      w.collectSatisfiedRequests(request)

I will wait for more feedback before making a new patch since this is a pretty trivial change.;;;","10/Apr/12 18:20;nehanarkhede;This patch looks very good. Here are a few questions - 

1. I like the way the expired requests are handled by implementing the logic inside the FetchRequestPurgatory. However, can we not do the same for satisfied requests by providing a satisfy() abstract API in RequestPurgatory ? That gets rid of the handling of fetch requests inside handleProducerRequest() in KafkaApis, which is a little awkward to read. When we have the ProduceRequestPurgatory, the same satisfy() operation can send responses for produce requests once the fetch responses for the followers come in. 

2. I gave the RequestPurgatory data structure some thought. Not sure if this buys us anything over the current data structure. How about the following data structure for the RequestPurgatory - 

2.1. The watchers would be a priority heap (PriorityQueue), with the head being the DelayedItem with the least delay value (earliest expiration time). So for each (topic, partition), we have a PQ of watchers. 

2.2. The expiration data structure is another PQ of size n, where n is the number of keys in RequestPurgatory. This expiration PQ has the heads of each of the watcher lists above. 

2.3. The expiration thread will await on a condition variable with a timeout = delay of the head of the expiration PQ. The condition also gets signaled whenever the head of any of the n watcher list changes. 

2.4. When the expiration thread gets signaled, it removes its head element, expires it if its ready, ignores if its satisfied, and adds an element from the watch list it came from. It keeps doing this until its head has expiration time in the future. Then it goes back to awaiting on the condition variable. 

2.5. The item to be expired gets removed from its watch list as well as expiration PQ in O(1). 

2.6. The item that gets satisfied sets a flag and gets removed from its watcher list. If the satisfied item is the head of the watcher list, the expiration thread gets signaled to add new head to its PQ. 

2.7 Pros 
2.7.1. The watcher list doesn't maintain expired items, so doesn't need state-keeping for liveCount and maybePurge() 
2.7.2. During a watch operation, items only enter the expiration PQ if they are the head of the watcher list 
2.7.3. The expiration thread does a more informed get operation, instead of polling the queue in a loop. 

2.8. Cons 
2.8.1. watch operation is O(logn) where n is the number of DelayedItems for a key 
2.8.2 The forcePurge() operation on the expiration data structure still needs to happen in O(n) 

Did I miss something here ? Thoughts ? 

3. On the other hand, this is a huge non-trivial patch and you must be pretty tired of rebasing and working through unit tests. We could just discuss the above changes, and maybe file another JIRA to track it, instead of delaying this patch further. But that is your call.;;;","10/Apr/12 19:25;jkreps;Hey Neha, yes, my hope is to get the patch evaluated as is, and then take another pass at cleaning up the way we handle the satisfaction action as Jun and you requested and try out other approaches to the purgatory data structure asynchronously. That should take these cleanup/polishing items out of the critical path.

I like your idea of the dual priority queues, but I need to work through it more to fully understand it.;;;","17/Apr/12 23:50;jjkoshy;+1 on the patch. I have a few minor comments:

KafkaRequestHandlers :
- requestLogger unused.

ConsumerConfig:
- maxFetchWait -> rename the prop to max.fetch.wait.ms and the val to
  maxFetchWaitMs
- Can we get rid of fetcherBackoffMs? It says it is deprecated, but had a
  reference in FetcherRunnable which you removed.
- May want to have an explicit constraint that consumerTimeoutMs <=
  maxFetchWait

RequestPurgatory:

- Unused import.
- The parameterized types and overall tricky nature of this component make
  it somewhat difficult to follow. I (think) I understood it only after
  looking at its usage in KafkaApis, so the comments and javadocs (including
  class' summary on top) can only go so far.  Even so, I think the comments
  seem slightly out of sync with the code and can be improved a bit. E.g.,
  what is ""given size"" in the update method's comment? current keys in the
  comment for watch == the given request's keys. and so on.
- Also, it may be easier to follow if we do some renaming, but it's a matter
  of taste and I may have misunderstood the code to begin with:
  - I find it confusing that there's a map called watchers which is a map
    from keys to Watcher objects, and the Watcher class itself has a
    linked-list of delayed requests called watchers. May be unwieldy, but
    how about renaming:
    - RequestPurgatory.watchers to watchedRequestsForKey
    - Watchers to WatchedRequests
    - Watchers.watchers to requests
  - Rename DelayedRequest.satisfied to satisfiedOrExpired (I find it weird
    that the reaper marks expired requests as satisfied.)
  - update -> maybeNotify?
- In collectSatisfiedRequests, the comment on ""another thread has satisfied
  this request"". That can only be the ExpiredRequestReaper thread right?
- It is slightly odd that we have to call the reaper's satisfyRequest method
  from Watcher. Would it work to move the unsatisfied counter up to
  RequestPurgatory?
;;;","18/Apr/12 00:23;jkreps;Joel, this is great feedback. I will address these issues in the commit since most are naming/documentation related.;;;","30/Apr/12 21:42;jkreps;Included most of Joel's comments, and fixed a few lagging unit tests (in particular refactored AutoOffsetResetTest).

Comments on the general structure of request purgatory I am going to put off until we have our second use case ready to implement--the producer acks. When we have that I am going to look at refactoring so that the ""satisfaction action"" is a function included with the DelayedRequest which is executed regardless of whether the request is satsified or times out. But I want to put this off until we can check it against the specifics of the second use case.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create topic support and new ZK data structures for intra-cluster replication,KAFKA-47,12514684,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,junrao,,19/Jul/11 21:32,07/Sep/17 18:20,14/Jul/23 05:39,07/Sep/17 18:20,,,,,,,,,,,,,,0,,,,"We need the DDL syntax for creating new topics. May need to use things like javaCC. Also, we need to register new data structures in ZK accordingly.",,alexfo,lanzaa,omkreddy,rektide,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,64997,,,Thu Sep 07 18:20:54 UTC 2017,,,,,,,,,,"0|i029wv:",11207,,,,,,,,,,,,,,,,,,,,"14/Aug/11 21:40;jkreps;I recommend we just make a command line tool rather than a formal ddl. That will be more sysadmin friendly and easier to script up.

We should also think through how this will interact with topic auto-creation which is a feature we are currently reliant on.;;;","15/Aug/11 16:49;junrao;Yes, I agree that we can start with just a command line tool. 

For auto-creation of a topic, we can optionally enable the producer to automatically create a non-existing topic (with some preconfigured # partitions, replication factors, etc), using the same underlying logic of the command line tool.;;;","14/Nov/11 00:25;junrao;Some thoughts on the create/delete topic support.

1. What if the create process dies in the middle of the creation?
The create process will create 1 ZK node like the following for each partition in the topic. 
/brokers/topics/[topic]/[partition_id]/replicas --> {replica_id : broker_id …}

This means that if the process fails in the middle, some of the partitions may not be created. Ideally, we should probably use the multi-row transaction support feature in ZK (ZOOKEEPER-965), which will be released in ZK 3.4. Since this should be a relatively rare event, for now, we can probably just do this as a best effort. If the create command fails in the middle, we can always delete the topic and create it gain.

2. How to delete a topic?
We can simply delete all entries in /brokers/topics/[topic] in ZK. On receiving the delete event, each replica will delete its corresponding local log directory. 

A broker could be down when the delete command was issued. When the broker is restarted, it should check if a topic still exists in ZK. If not, it will delete the local log directory.

A more subtle issue can happen when a topic is deleted and recreated while a broker is down. When this broker is restarted, it should recognize that its local log directory is out of date. It should delete everything in the local log directory and create a new one. One way to do this is to store a version id (could be just a timestamp) in /brokers/topics/[topic]. The same version id is also stored in a .version file in the local log directory when it's first created. By comparing the version id in the local log directory and in ZK, a broker can detect when a local log directory is out of date during startup.

3. Where to store the log locally?
Currently, a log is stored in {log.dir}/topicname-x for partition x. Now, a partition can have multiple replicas. One possibility is {log.dir}/topicname/x-y for partition x and replica y.

4. What about auto-creation?
One possibility is to add an option in the producer so that it can automatically create a topic if the topic doesn't exist in ZK yet.

5. I'd like to break this jira into 2 parts. The first part just adds the create/delete command and the corresponding ZK path. The second part will change the local log directory, add the auto topic creation support, and simply route the produce/consume requests to the first replica of each partition. This can be a separate jira.
;;;","14/Nov/11 06:04;jkreps;1,2. The other approach would be to implement a zk structure representing some kind of queue of actions to be carried out for each node. I think the delete case may be a special case of needing to reliably issue a command to a broker. This could be a CREATE or DELETE command or some kind of other operational command (e.g. MIGRATE-PARTITION). I think this was why Kishore and co took the zk route for communciation for Helix, I think this is one of the problems they were trying to solve.

The other question is how this is initiated. Do we add some administrative APIs to the brokers or is the communication through zookeeper? Either is fine, but we should be consistent about how we do administrative actions. I recommend we brainstorm a list of admin type actions we might want and try to generalize something that will work for all them.

3. As a matter of taste, I like what you propose for the directory structure, topic/x-y, better then what we currently do. This does mean needing to rename lots of files when there is a mastership change, though.

4. One concern is that auto-create may be really slow if we are blocking on acknowledgement from all the brokers. Not sure if this is a major problem.;;;","14/Nov/11 17:40;junrao;1,2. It seems to me it's simpler to have all brokers read from a single ZK path that stores the source of true, instead of broadcasting messages to every broker. For the latter, one has to further worry about what if only part of but not all messages are posted.

To be consistent, I think all administrative commands simply create some data structures in ZK and should complete very quickly. Each broker watches those ZK paths and take actions accordingly.

3. The directory name is only tied to replica id and won't change with a mastership change. The mastership info is recorded in ZK, not in directory  names.

4.  Typically, auto-create should complete very quickly since it just writes a few data structures in ZK.;;;","08/Feb/12 14:22;prashanth.menon;Does how we store the logs locally still require changes in light of the modifications we've made to the protocol?  A replica for partition X is stored at most once on a broker so with the current naming conventon we'll never run into conflicts.  Perhaps to be explicit that a certain directory is a replica, we could put them into {log.dir}/topicname/replica/partitionId but I don't think it's entirely necessary? ;;;","08/Feb/12 16:39;junrao;Prashanth, good question.

Yes, we could continue using the current log structure {log.dir}/topicname-partitionid. The only thing is that we would like to store some per topic metadata on disk, e.g., the version id (creation time) of a topic (to deal with some of the edge cases during topic re-creation). With the current structure, we either have to duplicate the topic metadata in each partition directory or deterministically pick one partition (like the smallest one) to store the metadata. Neither is ideal. It's much cleaner if we use the new structure {log.dir}/topicname/partitionid. Then the topic metadata can be stored under {log.dir}/topicname.;;;","08/Feb/12 17:45;tgautier;Hmm, if you guys are considering changing something about the log structure might I request that you consider doing something to ease the pain when there are 1,000's or 10,000's topics? 

The current structure doesn't work well since most filesystems tend to have problems when you store 20k or more directories in one directory.

A hashing scheme is a good solution.  The tradeoff is that it is much more difficult to find the topic directory by hand.  A hash of even 10 top level directories would afford 10x more total topics (currently, the practical limit appears to hover around 20k, so 10x would give us 200k) -- this would probably be sufficient for my needs. ;;;","08/Feb/12 18:31;prashanth.menon;Just my two cents here. Hashing (even consistent) is a logical idea, the caveat being that it will require maintaining an upper bound on the number of topic ""buckets"" to avoid renaming and moving a bunch of large files around if we don't enforce such a limit.  To solve the other issue of not being able to quickly determine which bucket a topic falls into by-hand, we could maintain a file at the bucket-level that lists which topic belongs in which bucket, much like the metadata file Jun mentioned earlier.  It's extra overhead on the part of the system, I'm not familiar with a single broker requiring so many topics but it's certainly conceivable.;;;","09/Feb/12 00:07;junrao;By using {log.dir}/topicname/partitionid, we already reduce the # of directories from # total partitions to # total topics. We can think a bit more how to improve it further. This should probably be a separate jira.;;;","07/Sep/17 18:20;omkreddy;Closing this umbrella JIRA as all tasks are resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Commit thread, ReplicaFetcherThread for intra-cluster replication",KAFKA-46,12514683,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,junrao,,19/Jul/11 21:32,01/Jun/12 20:58,14/Jul/23 05:39,01/Jun/12 20:58,,,,0.8.0,,,,,,,,,,0,,,,We need to implement the commit thread at the leader and the fetcher thread at the follower for replication the data from the leader.,,jkreps,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-302,,KAFKA-50,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/May/12 02:55;nehanarkhede;kafka-46-draft.patch;https://issues.apache.org/jira/secure/attachment/12525557/kafka-46-draft.patch","18/May/12 20:28;nehanarkhede;kafka-46-v1.patch;https://issues.apache.org/jira/secure/attachment/12528144/kafka-46-v1.patch","26/May/12 01:59;nehanarkhede;kafka-46-v2.patch;https://issues.apache.org/jira/secure/attachment/12529841/kafka-46-v2.patch","30/May/12 19:00;nehanarkhede;kafka-46-v3.patch;https://issues.apache.org/jira/secure/attachment/12530254/kafka-46-v3.patch","01/Jun/12 01:42;nehanarkhede;kafka-46-v4.patch;https://issues.apache.org/jira/secure/attachment/12530496/kafka-46-v4.patch",,,,,,,,,,,,,,,,,,,,,,,5.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67069,,,Fri Jun 01 01:42:01 UTC 2012,,,,,,,,,,"0|i09m7r:",54032,,,,,,,,,,,,,,,,,,,,"06/Feb/12 21:48;prashanth.menon;Should this be broken down into two sub tasks: One for the commiting thread on the leader and one for the fetcher thread on the follower?;;;","22/Feb/12 18:19;junrao;Some notes on the implementation:

1. If required_acks for a produce request is not 0 or 1, the commit thread will put the request in a DelayedQueue and adds it to a watch list for that topic/partition (similar implementation as long poll in kafka-48).
2. There will be an ""offset watcher"" per topic/partition. The watcher fires everytime a follower in the in-sync set advances the offset. Every time the watcher fires, it checks from the head of watcher list and see if any produce request can be satisfied on this partition. If so, marks this partition as satisfied, if all partitions of the produce requests are satisfied, dequeue the request an send back the ack. 
3. How to add/delete follower from in-sync set? We can run a background insync-check thread that does the following:
for each topic/partition
  get and save the offset of each partition of the leader replica
  wait for KeepInSyncTime
  check if any insync replica hasn't caught up to the saved offset, if so, drop it out of insync set (need to fire the corresponding offset watcher) 
  check if any replica (not in insync set) has caught up to the saved offset, if so, add it to insync set.

If we follow this approach, we can have 2 subtasks that implement ""offset watcher"" and insync-check thread. We can also have 1 separate subtask for the FetcherThread and another for the commit thread.
;;;","02/Mar/12 01:35;nehanarkhede;>> Should this be broken down into two sub tasks: One for the commiting thread on the leader and one for the fetcher thread on the follower? 

Lets not worry about that for now. I'll take care of this JIRA, once we resolve the other JIRAs. This one has many other dependencies.;;;","24/Mar/12 23:30;nehanarkhede;Started work on this, will upload a patch, once KAFKA-45 is resolved;;;","04/May/12 02:55;nehanarkhede;Attaching a draft patch for message replication. This is just to give a high level overview of the changes involved and is by no means ready to be committed. So no need for a detailed review. There are probably a few bugs lurking around. Since the changes are pretty significant, I was hoping to get some early feedback. 

1. Added ReplicaFetcherThread that reads data from the leader and appends to local replica log

2. Added highwatermark maintenance at the leader and the follower. The highwatermark is checkpointed in the partition directory in a file named highwatermark.

3. Added ISR maintenance logic on the leader. This involves possibly expanding the ISR while handling a fetch request from a follower. 

4. Also added ISRExpirationThread that tracks the highwatermark update time for all partitions that the leader owns and shrinks it if (hw update time + keep.in.sync.time.ms) < current time. 

5. Note that to get this patch going, I had to put in code to cover KAFKA-302. I would encourage a more detailed review for the becomeLeader() and becomeFollower() APIs. We would either like to check it in from this patch, or if Prashanth has some patch, review that one too. 

I will probably add v1 patch with unit tests early next week. 

Also, I would like to check this in parts, if possible. Starting with probably KAFKA-302, then the actual message replication logic. But that is open for discussion as well. 

;;;","06/May/12 21:52;prashanth.menon;I can comment on point 5, related to KAFKA-302.  After a first pass this is what I've got, let me know what you think:

1. In KafkaZooKeeper.leaderElection, if the replica isn't elected as the leader, it should issue a becomeFollower after reading who became the leader from ZK.
2. In KafkaServer.becomeLeader, the leader should probably update the ISR and CUR list by performing the same type of logic as the ISRExpirationThread does.  If the intention was to rely on the ISRET to perform this asynchronously, I think it'll need to be modified to update the partition's CUR list along with the ISR.
3. In ISRExpirationThread, you'll need to add the slowest partition back into the queue in every case.
4. It seems like there are two ways to update a leader replica's HW, either through Replica.hw itself or Partition.leaderHW.  To avoid confusion, can we simplify and provide only one API through which all clients to perform this?  The latter seems to do the same thing but just update the hw update time.  
5. Can the leo and hw methods in Replica make use of the isLocal method?  The logic is a little more clear this way, IMO.
6. In KafkaApis.handleFetchRequest, it looks the maybeAddReplicaToISR call is unncessary and is rolled into readMessageSets?  Any reason we need it there?
7. KafkaApis.readMessageSets should probably verify that the leader for the partition exists on the broker.

In general, I'm not entirely sold on the ISRExpirationThread.  From my point of view, there is a function that, given a partition, determines whether its ISR/CUR list needs to be updated in-memory and in ZK.  Right now, there is a single thread that uses a heap to pick off the partitions with the oldest update times, waits for expiry if necessary, then updates accordingly.  I'm wondering if it's possible instead to leverage a scheduled executor that appropriately schedules the execution of the above function on a given partition based on the same criteria (the partition's HW updated time); when the task actually executes, it's possible the hw would have moved, making the task an no-op.  The benefit there is simplicity, added concurrency and a slightly more accurate/real-time reflection of the ISR list in ZK meaning a possible reduction in message loss during leadership changes?;;;","09/May/12 01:51;nehanarkhede;Prashanth, 
Thanks for reviewing the patch, in detail. Like I mentioned earlier, this is just a draft patch and will clearly miss some details like you've pointed out. I'm looking more for high level feedback on data structures used, any ideas for refactoring etc.
;;;","09/May/12 02:04;junrao;Some comments on the draft.

High level:
1. We should consider whether to have 1 HW checkpoint file per partition vs 1 HW checkpoint file for all partitions. The benefit of the latter is fewer file writes during checkpoint and fewer file reads during broker startup. Also, to avoid corrupting the checkpointed file, we should probably first write the file to a tmp file and rename to the actual checkpointed file. This probably can be done in a separate jira.

2. The benefit of using an ISRExpirationThread is that it's relatively simple since there is 1 thread doing all the ISR expiration. One drawback I can see is that idle partitions are still constantly checked by the thread. This may or may not be a big concern.

Low level:
3. KafkaApis:
3.1 Agreed with #6 in Prashanth's comment. Probably don't need to call maybeAddReplicaToISR directly from handlFetchRequest.
3.2 A subtle issue is that we should probably wait until a (replica) fetch request is successful before updating the follower replica's LEO. This is because during an unclean failover (no live brokers in ISR), the offset of the first fetch request from a follower may not be valid.
3.3 We need to update ISR in ZK and in memory atomically since the ISR can be expanded and shrunk from different threads.

4. Partition:
4.1 We probably don't need to add reassignedReplicas in the patch and can add it later when we get to kafka-42, if necessary.
4.2 We probably don't need both catchUpReplicas and assignedReplicas since we can always derive one from another together with ISR.
4.3 Do we need to maintain a HashMap of <replica_id., Replica>, instead of a set of replicas for faster lookup? This may not be a big deal since the replica set is small.
4.4 Should we keep highWatermarkUpdateTime in Log where the HW is stored?

5. Replica:
5.1 leo(), if log is present, we should return l.leo not l.getHighwaterMark.

6. KafkaConfig: All follower related properties should be probably be prefixed with ""follower"".

7. Log:
7.1 recoverUptoLastCheckpointedHW(): if there are k+1 log segment files need to be truncated, we should delete the last k and truncate the first one.
;;;","18/May/12 20:28;nehanarkhede;Prashanth,

Regarding your high level comment -

If you use a scheduled thread pool executor to schedule every partition greadily, you would be spinning up a bunch of threads that might not do any work, since the priority of the items in the queue keeps changing. Also, you will need to change the priority of an already scheduled task, which might not be possible to do. What's more, if the threadpool maxes out, a partition that requires immediate ISR expiration might not get scheduled on time. 

I'm guessing it is unlikely that all the partitions on a node expire at the same time. Even if they do, it might take maybe a few seconds for the last partition to shrink its ISR, which is not a big deal. In reality, there would be very few partitions, maybe 1 or 2, that need to shrink their ISR due to slower followers. That's why a single thread seems to suffice for handling the ISR expiration for all partitions. 

Regarding your detailed review comments -

1. Fixed that
2. That is a good suggestion. I've attempted a refactoring, let me know if you have more feedback on it. The ISR thread updates the ISR in ZK AND in memory cache, where as the become leader API is passed in the latest ISR read from ZK and it just has to update its cache with that state, on becoming a leader. So, I wrapped up the cache + Zk update in an updateLeaderAndISR API in KafkaServer. This will be used by the ISR expiration thread. The become leader should not be updating anything in Zk, it should just be reading from Zk and updating its cache. 
3. The patch already does it at the end of the while loop. Or do you mean something else ?
5. Yes
6. The replica should be added to the ISR as soon as the fetch request is received by the server. I intended to add the replica to the ISR even if it might get ended up in the purgatory waiting for additional data. Ideally, would like to get rid of it from the readMessageSets() API.
7. Good point. Added that.

Jun

Regarding your high level comments -

1. Yes, I kept it simple in this patch, since the main goal is to get the message replication to work. Persisting all the HW in one file would definitely be a better approach and can be another JIRA
2. Idle partitions will cause one delete and one insert into the priority queue. It doesn't look like an issue, but could be resolved by adding a TTL to items in the queue. Idle partitions will expire from the queue and will be added back to the queue when the leader receives a produce request for that partition. However, I'd like to push that to later, since it does not improve correctness and does not look like a performance issue. 

Regarding your detailed review comments -

3.2 Good point. I think that is better too.
3.3 Yes, this is one of those “details” that I didn't include in the draft patch.
4.1 Removed it
4.2 Removed CUR
4.3 Kept it simple since replication factors that make sense in production are typically 3-5.
4.4 Yes, but when will it be used ?

5.1 Good catch !

6. That exists in the patch. Did I miss any ?
7. Good point. Fixed it
;;;","21/May/12 15:40;junrao;Thanks for the patch. Overall, a very encouraging patch given the complexity of this jira. Some comments:

From previous reviews:
Were 4.1 and 4.2 addressed in the patch? I still see CUR and reassignedReplicas.
For 4.4, I think highWatermarkUpdateTime can be used as described in 15.2 below.
For 6, I meant that all local variable names should also be prefixed with follower.

New review comments:
11. KafkaApis:
11.1 handleFetchRequest(): if the leader of one partition is not on this broker, we reject the whole request. Ideally, we should just send the error code for that partition in the response and fulfill the rest of the request. 
11.2 handleFetchRequest() and readMessageSets(): If the leader is not on this broker, we should probably return a new type of error like NotLeaderException, instead of using InvalidPartionException or throwing IllegalStateException. 
11.3 readMessageSets(): add a comment of what -1 means for replicaId

12. ReplicaManager:
12.1 remove unused imports
12.2 maybeIncrementLeaderHW(): if(newHw < oldHw) should be if(newHw > oldHw)
12.3 We will need to either synchronize the methods in this class or use ConcurrentHashMap for allReplicas since allReplicas can be read and updated concurrently.

13. Replica:
13.1 hw(): to be consistent, we should probably throw IllegalStateException, instead of InvalidPartitionException.

14. KafkaServer:
14.1 There are a couple of TODOs. Will they be addressed in this jira or separate jiras?

15. ISRExpirationThread:
15.1 It seems that when the time expires, we always update the ISR. We should only update ISR if it actually shrinks.
15.2 Currently, we take a replica out of ISR if its LEO is less than leaderHW after keepInSync time. We probably should use the following condition:
  leaderHW - r.leo > keepInSyncBytes || currentTime - r.highWatermarkUpdateTime > keepInSyncTime
  The first condition handles a slow follower and the second condition handles a stuck follower.
15.3 I think we can potentially get rid of the inner while loop by putting all the logic when time expires in a if statement and the awaitUtil part in the else clause of the if statement.
15.4 Also, instead of using a priority queue and keep adding and deleting partitions into the queue, would it be simpler to have the thread just check the isInsyncCondition for each partition every keepInSyncTime?

16. LogDisk: recoverUptoLastCheckpointedHW(): 
16.1 The second condition in
          segments.view.find(segment =>  lastKnownHW >= segment.start && lastKnownHW < segment.size) 
       seems incorrect. It seems that you want to use ""lastKnownHW < segment.messageSet.getEndOffset""
16.2 The files of all deleted segments should be deleted like that in LogManager.cleanupExpiredSegments().

17. LogOffsetTest:
17.1 There is no need to keep testEmptyLogs(), since we have a test that covers fetching from a non-existing topic using SimpleConsumer.

18. PrimitiveApiTest:
18.1 testConsumerNotExistTopic(): we probably shouldn't create the topic in this case.

19. ProducerTest:
19.1 testZKSendToNewTopic(): Which should fix the comment that says ""Available partition ids should be 0, 1, 2 and 3"" since there is only 1 partition created.

20. ReplicaFetchTest:
20.1 Since the test is already using in-memory log, we can remove TODO in testReplicaFetcherThread().
20.2 testReplicaFetcherThreadI(): Instead of sleeping and then checking log.get.getLogEndOffset, could we create a utility method that keeps checking until LEO reaches certain value up to a certain max wait time? Maybe we should make a more general util that waits up to a certain amount of time until a condition is satisfied. 
;;;","23/May/12 05:29;jkreps;Comments
Some of these we discussed in person but I wanted to post them here for anyone else following along. A number of these are really just structural or naming comments. When we get closer to final form I will do a pass on trying to understand all the logic and see if I can find any corner cases, but I haven't done that yet. As a result I am not really sure if I understand everything I am commenting on, so take it all with a grain of salt.
 
1. This patch pays really good attention to code structure and testability which is awesome since we are adding gobs of hard logic. Nice to see things getting cleaner as we do this.
2. For some reason I preferred just having Log instead of DiskLog and MemoryLog. I feel like doing it twice tends to lead to similar logic in both. I do like the idea of lightening unit tests. I wonder if a helper method to set up a log wouldn't be good enough, though? Not sure if this is a rational preference or just inertia on my part, though, so feel free to ignore. If we do separate out a Log trait I feel we should clean up that interface a bit it is kind of a disaster right now (probably we should do that regardless).
3. Maybe HW mark should move out of Log since now it really indicates something about the state of other servers and Log is meant to be just a simple stand alone log implementation.
4. I think expanding out some of the acronyms in public methods would be nice: i.e. highWatermark and logEndOffset. Having a concise local variable name is helpful but for the poor person trying to learn the code i think the slightly more verbose name is helpful. If you prefer the more concise naming then just having good javadoc that explains the abbreviations would be good.
5. Consider making Replica just be a dumb pojos (posos?) and move the ReplicaFetcherThread elsewhere.
6. We currently have Partition and BrokerPartitionInfo. We should clarify why both of these and make the naming make sense. To me a partition is logically just (topic, partId). I think BrokerPartitionInfo is really a bit hard to understand, though that is unrelated to this patch. Partition is more like the broker's information about replicas of that partition. Also both Partition and Replica are in the cluster package which was originally shared by client and server. Now with all the additional stuff this is really part of the server only, right? Probably we should change the package name...?
7. I sent a separate email on setters/getters. I think overall there area a lot of setter/getter methods. We should pick a style for these and go with that uniformly (we haven't been consistent so far). 
8. I think we should figure out a general strategy for managing the zookeeper interactions. I think it is wise to wrap up the zookeeper dependency but having everything in one class is too much. Maybe the way to go is to have generic ZkUtils and reusable infra like ZkQueue and then split KafkaZookeeper into the logical functions it covers.
9. For the config I recommend the prefix ""replication"" or ""replica"" instead of ""follower"" (e.g. replication.socket.timeout.ms), I think this is more clear to someone who hasn't read about the internals of our replication design and doesn't know the terminology.
10. A bit of internals have spilled into KafkaServer, such as locking, ISR management, etc. I think KafkaServer should just interact with the main subsystems of kafka in a very abstract way. I think the core problem is that we need to think more about the functionality and API of ReplicaManager. To my mind the replica manager should be the one running the ISR maintence, doing its own locking, etc. To me the main subsystems are (1) logs managed by the LogManager, (2) the network layer wrapped up by SocketServer, (3) the request processor pool (4) and now the ReplicaManager or ReplicationManager or whatever we want to call it.
11. ISRMaintenceThread--I think you are right that we will need to be very prompt about handling ISR expiration since this is effectively part of our failure detection time. It might be good, though to just stick in the polling loop Jun suggested for now, and then come back to optimize it later (even though we almost certain will have to), just to reduce the scope in this iteration. 
12. Also make sure the ISR thread either uses the Utils.newThread helper or handles the common gotchas (thread name, set daemon properly, set uncaught exception handler). Also think through the details of the lifecycle.
13. We have a lot of threads that basically run in a loop and use an isRunning atomic boolean and count down latch. You added two but I think we had a few others. Consider factoring this out into a helper runnable that these can extend. Verifying the lifecycle details for each is kind of a pain and it pretty easy to either not cleanly shutdown all the threads or block indefinitely or whatever.
14. The changes in KafkaApis seem kind of brute forced. ensureLeaderOnThisBroker and the massive expansion of logic in readMessageSets seems like we are just brute forcing through this problem. We need to find a way to structure this into methods that make sense and don't reach into the internals of other parts of the system. readMessageSet is already doing crazy funky stuff that needs to be fixed. I think restructuring readMessageSet will help with some of the problems, and the rest can maybe be solved by pushing all the replica/leo/isr logic here into ReplicaManager. Basically the API level should just say ReplicaManager.leaderForPartition(id) as part of the request validation and ReplicaManager.recordFollowerPosition(...) and move all the other details out of the KafkaApis. Not sure if I understand this well enough for that to make sense...
15. We should always return the hw mark in the PartitionData, right? This way we can do monitoring on the consumers. Currently it looks like we only do this for replicas.
16. Name for ReplicaManager.makeSurePartitionExists and ReplicaManager.assurePartitionExists doesn't really call out the difference. I would recommend calling them getOrCreatePartition() and ensureExists()
17. Overall I would think through the public API for ReplicaManager. I think it may be possible to move much more replica/replication/partitioning logic under this classes wrapper and out of other parts of the system which would be good. ;;;","23/May/12 16:43;junrao;Jay's comments remind another thing:

21. The follower's HW should be min(follower LEO, leader HW). This is to handle the case that a follower is still catching up.;;;","26/May/12 01:59;nehanarkhede;Thanks for the great feedback ! This is probably the largest jira for KAFKA-50, I've done my best to include the review changes in this JIRA. I will file separate JIRAs to include other review suggestions. Let me attempt to describe the changes made in this patch -

1. Simplied the ISR maintenance logic to iterate through the partitions every keepInSyncTimeMs ms. I guess the overhead is O(n) for isr expiration and O(1) for replica fetch requests. This seems reasonable since keepInSyncTimeMs is expected to be in the order of several seconds and replica fetch requests are easily more frequent than that.
2. Fixed ISR expiration logic to remove a slow follower as well as a stuck follower from the ISR
2. Moved replication specific logic inside ReplicaManager and Partition. So KafkaApis and KafkaServer have minimum replication specific code
3. Removed InMemoryLog, I guess that was an over optimization
4. Kept the high watermarks in a separate file, will fix it in a separate JIRA to contain the changes in this JIRA. ;;;","26/May/12 02:08;nehanarkhede;Regarding Jun's comments -

4, 6: Done

New review comments:
11. KafkaApis:
11.1 Makes sense
11.2 Added a new exception class NotLeaderForPartitionException. We can improve the naming going forward.
11.3 Done

12. ReplicaManager:
12.1 Done
12.2 Good catch 
12.3 Ideally, I would like to get rid of allReplicas, maybe do it differently. I'm thinking of fixing this in another JIRA. Let me know if you prefer fixing it in this one.

13. Replica:
13.1 Done

14. KafkaServer:
14.1 Removed the TODOs. They are addressed.

15. ISRExpirationThread:
15.1 Done
15.2 I've included logic for handling slow and stuck followers, and unit tested it.
15.3 It has completely disappeared now.
15.4 Agreed

16. LogDisk: recoverUptoLastCheckpointedHW():
16.1 That's a good point.
16.2 Done

17. LogOffsetTest:
17.1 Deleted it

18. PrimitiveApiTest:
18.1 testConsumerNotExistTopic() Actually I'm not too sure this test makes sense in the replication branch. Is this testing that the server returns some meaningful error code if it receives a request for an unknown topic ? If yes, maybe we don't need a consumer to test that logic. I haven't fixed this test, maybe we can think more on what exactly we want to test here. 

19. ProducerTest:
19.1 testZKSendToNewTopic(): Done

20. ReplicaFetchTest:
20.1 Right
20.2 testReplicaFetcherThreadI(): That is a good suggestion. I'd like to clean up unit tests and add related helper APIs, maybe in another JIRA.;;;","26/May/12 02:15;jkreps;Indeed, this is replication! The rest of it is just a simple matter of handling failures and a little tooling. :-) Very nicely done.;;;","26/May/12 02:18;nehanarkhede;Jay,

Thanks for thinking through the code structure, I've included more refactoring changes in this patch. Some of the suggestions are orthogonal to this patch and I'd prefer to fix it in another JIRA, given the complexity of this patch. Maybe I can create a 'refactoring' JIRA after this one to cover some of these -

2. Makes sense. I guess that was an over optimization.
3. This is a good suggestion, al though would prefer keeping it to refactoring JIRA
4. Picked descriptive names
5. Somehow I like the idea of wrapping up enough logic inside Replica to figure out if it is a follower or leader. ReplicaFetcherThread inside Replica allows that. Al though, I'm not sure that is the best way to achieve it.
6. Yeah, probably something to think about. Will move it to the refactoring JIRA
7. I like Option 4 there, hoping that can be fixed in a separate JIRA
8. Yeah, I moved some zookeeper client access to ReplicaManager so that all replication specific logic can be moved there.
9. Changed configs to replication.*
11. Simplified the ISR expiration. Looks better now.
12. Hmm, Utils.newThread returns Thread, but I think it is useful to use some APIs specific to ReplicaFetcherThread like getIfFollowerAndLeader(). But I see your point here. Given a choice, it is always better to use a helper method. I set the daemon property and the thread handles all Throwables. 
13. Yeah, this is a good suggestion. This also fits in generic refactoring category that can be fixed separately.
14. This is another great suggestion. Please see the included patch if you like it. 
15. Fixed it
16. Fixed it
17. Yeah, this will keep changing with the v3 code. Will be good to keep this in mind though.

Overall, I liked your refactoring suggestions, and I might have been lazy to describe all of the changes I made here. Will really appreciate it if you can read through the new patch and suggest improvements. I'm fine with working through more in this patch itself, if you feel that works better. ;;;","26/May/12 02:28;nehanarkhede;Filed KAFKA-350 for improving the high watermark maintenance
Filed KAFKA-351 to cover the refactoring suggestions. 

Now, we need some serious system testing for all this code ! :-);;;","28/May/12 23:05;junrao;Thanks for patch v2. To help people review the code, I summarized the logic of handling the produce and fetch request on the server in this wiki: https://cwiki.apache.org/confluence/display/KAFKA/Handling+Produce+and+Fetch+Requests+in+KafkaApi

Some new comments:
21. ISRExpirationThread: It seems that this class is no longer used. Let's remove it.

22. Partition.getOutOfSyncReplicas(): The first condition doesn't seem to implement what's in the comment. It doesn't check the leader's leo update time. Also, the condition specified in the comment doesn't seem sufficient. Suppose that the leader gets 100 bytes of more data, after which no more data is coming. A follower gets the first 50 bytes and then stopped. The follower's leo has been updated after the leader's leo was last updated. However, we still need to take the follower out of ISR. How about changing the condition to: select replicas whose leo is less than the leo of leader and whose leo hasn't been updated for keepInsyncTime. 

23. ReplicaManager:
23.1 makeLeader(): remove comment ""also add this partition to the ISR expiration priority queue""
23.2 makeFollower(): If a follower switches leader, we should stop the old FetchThread before starting the new one.

24. Replica.leoUpdateTime(): use logEndOffsetUpdateTime to be consistent.

25. KafkaConfig: Let's keep the variable name and property name consistent. If we choose to use replication as the prefix for property name, use the same prefix for variable names.

26. KafkaServer: To be consistent, we should probably name becomeLeader and becomeFollower as makeLeader and makeFollower, respectively.

27. Log.recoverUptoLastCheckpointedHW(): not sure if comment 16.2 is addressed. Removed segments are not physically deleted.

28. ISRExpirationTest:
28.1 testISRExpirationForSlowFollowers(): the comment says set leo of remote replica to sth like 2, but the code set it to 4.
28.2 testISRExpirationForStuckFollowers() and testISRExpirationForSlowFollowers(): is Thread.sleep() really needed? testISRExpirationForMultiplePartitions() didn't seem to use Thread.sleep().

29. PrimitiveApiTest.testConsumerNotExistTopic(): I think this test is just to make sure that the client can get the error code on a non-existing topic.

30. TestUtils:follower.socket.timeout.ms is now renamed to replication.socket.timeout.ms
;;;","30/May/12 02:35;nehanarkhede;Updated patch to address Jun's suggestions -

1. Fixed the ISR expiration for stuck followers case
2. HW maintenance work is postponed to KAFKA-350
3. System test (KAFKA-341), that tests message replication without failures, works on this patch

More detailed comments -

21. Removed it

22. Partition.getOutOfSyncReplicas(): Good catch ! Fixed the logic and added another test case for this.

23. ReplicaManager: Done

24. Replica: Changed the name to logEndOffsetUpdateTime()

25. KafkaConfig: Changed the variable and config names to replica.*

26. KafkaServer: Well, become* makes sense on the entity that is changing its state (Replica), make*, I thought made sense on the actor (KafkaServer). But that is just a matter of personal taste :)

27. There are some nitty gritty details about HW maintenance that I would like to fix as part of KAFKA-350

28. ISRExpirationTest: Done

29. PrimitiveApiTest.testConsumerNotExistTopic(): I think the right fix is to throw a descriptive exception is UnknownTopicException when a client makes a produce/consume request for a topic that has never been created. Filed KAFKA-351 to fix it.

30. TestUtils:Fixed it;;;","30/May/12 20:32;junrao;+ 1 from me for patch v3. Let's see if there are more comments from others.

For 27, it's ok to resolve this in kafka-350. Could you update the jira so that we remember all the changes that need to be made? Ditto for kafka-351.;;;","01/Jun/12 01:30;nehanarkhede;Attaching an updated patch that includes the rebase changes from KAFKA-348

Also, updated the follow up JIRAs - KAFKA-350 and KAFKA-351;;;","01/Jun/12 01:42;junrao;+1 for patch v4.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Broker startup, leader election, becoming a leader/follower for intra-cluster replication",KAFKA-45,12514682,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,junrao,,19/Jul/11 21:32,10/Jun/12 20:40,14/Jul/23 05:39,10/Jun/12 20:40,,,,,,,,,,,,,,0,,,,"We need to implement the logic for starting a broker with replicated partitions, the leader election logic and how to become a leader and a follower.",,nehanarkhede,prashanth.menon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-307,KAFKA-306,KAFKA-50,,,,,,,,,,,,KAFKA-44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67070,,,Sun Jun 10 20:40:36 UTC 2012,,,,,,,,,,"0|i15yov:",242920,,,,,,,,,,,,,,,,,,,,"12/Feb/12 23:15;prashanth.menon;Hey everyone.  Since the ZK structures are effectively done as part of KAFKA-47, I thought I'd start on this ticket.  Something that came up was the log cleanup functionality within LogManager will need to be tweaked.  My thinking is that Replica's should manage the cleanup of their local logs (moving that functionality from LogManager) and need to be in-sync with the leader with respect to hw, logEndOffset, and also the logMinOffset; essentially, the amount of data available on all ISR for replica R must be the same reglardless of each individual broker's log cleanup configuration.  Not sure how that information should be propagated, whether through the follower's fetch request or somewhere in ZK, that should be left up to discussion.  Regardless, non-leader replica's can use this minOffset to perform local log cleanups, I suppose.  Please do let me know if I'm missing a peice of the puzzle here or if there's a simpler solution.
;;;","15/Feb/12 19:17;nehanarkhede;Prashanth,

Thanks for helping out on replication and getting in the format changes. Before starting on this, we are blocked on KAFKA-49 and KAFKA-44. It seems like it will be most effective to get some help on these JIRAs first. Just a suggestion, comments are welcome!

;;;","16/Feb/12 02:16;prashanth.menon;No worries.  The reason I chose to start here was that I felt it required the creation of some of the base entities required for KAFKA-49 and KAFKA-44.  Since 49 is still semi-blocked by 240 on the produce-side, I didn't want to make too many changes there.  44 on the other hand, makes use of some of algorithms used as part of start up.  Thinking about it, 44 is probably a better place to start in terms of complexity.  Thanks!  ;;;","21/Feb/12 22:27;junrao;Prashanth,

That's a good question. We'd like to make replicas identical with each other. This is easy for size-based log retention, but harder for time-based log retention. What you proposed is to have only the leader delete old log segments and propagate this information to the followers. This seems like a reasonable approach. The question is how should the leader communicate such information to the followers. One possibility is to piggyback on the FetchResponse returned to the followers. This will mean some extra optional fields in FetchResponse.;;;","09/Mar/12 21:56;jkreps;I think we are overthinking this. Currently cleanup is not a precise SLA, it is just a guarantee of the form ""we will never delete anything younger than X OR we will always maintain at least Y bytes of messages"". Trying to maintain this in synchronous form across nodes is overkill I think. It is fine if every node acts independently as long as each of them respects the SLA. I think this should be much simpler and more likely to work.;;;","12/Mar/12 22:40;nehanarkhede;Here is something to think about wrt to leader election and replica failures -

If there are 3 replicas for a partition, and the leader acks the produce request once the request is acked by the 2 followers. The produce request doesn't care about the replication factor. So if one of the followers is slow, the leader will receive less than 2 acks from the followers, and it will go ahead and send a success ACK to the producer. The replicas update their HW only on the next replica fetch response. Since the HW committer thread is running independently. it is possible that the checkpointed HW of one of the 3 replicas is lower than the others. 

If at this point, if leader fails, it will trigger the leader election procedure. According to the current design proposal, any replica in the ISR can become the leader. If the replica with the lower HW becomes the leader, then it will truncate its log upto this last checkpointed HW and start taking produce requests from there. The other 2 replicas, will send ReplicaFetchRequests with an offset that doesn't exist on the leader.

Effectively, it seems that we will end up losing some successfully acknowledged produce requests. Probably, the leader election procedure should check the HW of the participating replicas and give preference to replica with highest HW ?;;;","13/Mar/12 22:33;junrao;If there are 2 followers and leader receives ack from only follow 1, but not follower 2 (within timeout), the leader will kick follower 2 from ISR before it can commit the message and ack the producer. So, follower 2 will never get a chance to become the new leader should the current leader fail.;;;","10/Jun/12 20:40;nehanarkhede;Resolved as part of KAFKA-46;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Various ZK listeners to support intra-cluster replication,KAFKA-44,12514681,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,nehanarkhede,junrao,,19/Jul/11 21:32,10/Jun/12 20:42,14/Jul/23 05:39,10/Jun/12 20:42,,,,,,,,,,,,,,0,,,,We need to implement the new ZK listeners for the new paths registered in ZK.,,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-50,,,,,,,,,,KAFKA-45,KAFKA-301,,,,,,,,,,,,,,,,"28/Feb/12 04:32;prashanth.menon;KAFKA-44-DRAFT-v1.patch;https://issues.apache.org/jira/secure/attachment/12516272/KAFKA-44-DRAFT-v1.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67071,,,Sun Jun 10 20:42:28 UTC 2012,,,,,,,,,,"0|i15yon:",242919,,,,,,,,,,,,,,,,,,,,"21/Feb/12 03:38;prashanth.menon;Hi all,

I spent today working through this (and consequently KAFKA-45 because there's a lot of overlap), hoping to have a patch in mid-week, and had a suggestion regarding how new replicas are assigned to brokers.  Currently, every broker effectively listens to every possible path in /brokers/topic/[topic]/partitions/[partition] because it doesn't know which partition replica will be assigned to it.  This places heavy burden on ZK since it will have to fire an unncecessary high volume of callbacks to every broker.  

The problem is that the broker doesn't know for which partition it will be assigned a replica for after its up.  I propose an approach whereby a new path in ZK acts as a queue which brokers watch to receive new replica assignments.  This path will exist for every broker in the system, so something like /brokers/[brokerId]/new_partition/[topic:partitionId] or some such thing?  The admin utilities will create these paths along with the replica assignments.  I realize this is additional overhead and somewhat asymmetric with the other partition paths so I'm interested to hear peoples thoughts!;;;","21/Feb/12 21:57;junrao;Prshanth, 

This seems like a good idea to me. Ideally, this new ZK path /brokers/[brokerId]/new_partition/[topic:partitionId] should be created atomically with other ZK paths constructed when a new topic is created. We could leverage the ZK multiple-row transaction support for this when ZK 3.4 is more stable.;;;","22/Feb/12 02:10;prashanth.menon;Sounds good.  Another question is what to do with that path should the broker go down with pending assignments.  My vote would be for the broker to recreate it as part of the startup process, thereby deleting previous data.  My thinking is that the broker will discover all assigned replicas naturally as part of startup assuming the topic creation admin tool did its job correctly updating partition replica lists in ZK.;;;","22/Feb/12 04:36;junrao;Not sure if I follow exactly what you are proposing here. In particular, what does the broker recreate? My thinking is the following:

/brokers/[brokerId]/new_partition/[topic:partitionId] is the source of truth about what topic/partitions a broker has and that path is only created during topic creation. A broker listens to that path to pick up new topic/partition assigned to it. When a broker starts up, it simply reads all /brokers/[brokerId]/new_partition/[topic:partitionId] to determine the set of topic/partition that it should have. So the broker never needs to recreate that path.;;;","22/Feb/12 17:01;prashanth.menon;Ah, I think I see where you're going with this.  You'd like to use it as a persistent path listing all partitions assigned to a broker.  If so, perhaps the path can be changed slightly to /brokers/[brokerId]/assigned_partitions/[topic:partitionId] to indicate appropriately?  

My intention was to have that just for new partition assignment notifications and act more like a queue.  So the flow would be:

1. Tool assigns topic X, partition Y to broker Z
1a. New entry added to /broker/topics/X/partitions/Y/replicas to append broker Z to list.
1b. New znode created at /broker/Z/new_partition/[X:Y]
2. Broker listens on /broker/Z/new_partition
2a. New partition is assigned, bootstrap replicas
2b After successful bootstrap, remove /broker/Z/new_partition/[X:Y]

I like you're idea much better; it's clear which partitions are assigned to which broker and makes startup simpler as well.
;;;","24/Feb/12 04:18;prashanth.menon;Can we also use this new path for partition reassignment?  When the admin would like to reassign a partition, the broker's ID is appeneded to the list at /brokers/partitions_reassigned/[topic]/[partId] and a znode is created in /brokers/[brokerId]/assigned_partitions/[topic:partId] ?  Then only the leader listens on the reassignment path in order to update the leader replicas RAR and the new brokers become aware of new partitions and bootstrap as they normally would.  Thoughts?;;;","24/Feb/12 16:40;junrao;It may be possible, but this can be a bit tricky. When you move a partition, you want to wait until the partition in the new broker has fully caught up, before deleting the one on the old broker. So, one way to achieve this is to have another ZK path that indicates this transition state. After the transition is done, the new assignment will be added to the assigned_partition path.

In any case, let's start by just focusing on static partition assignment. We can worry about partition reassignment a bit later when we get to kafka-42.;;;","28/Feb/12 04:31;prashanth.menon;Okay, I've attached a very very rough draft of a patch.  I'm really looking for feedback and thoughts because there's just a lot of overlap with KAFKA-45.

1. New classes are Replica, Partition and ReplicaManager.  kafka.replica.Partition will need to be merged with kafka.cluster.Partition which is a little light at the moment.
2. There are a few placeholders for KAFKA-45 and KAFKA-46 in there.
3. Does not include the partition reassignment listener.  We'll need to revisit this because the current process suffers from the same problem as the previous partition assignment logic.
4. Includes the new assigned_partitions path but relies on old replica assignment path creation for testing purposes.  This will need to change once the tools change.
5. I've tried to make it as unintrusive as possible.

There is one issue I'm trying to wrap my head around.  Consider a broker A that comes up with no partitions and the admin reassigns a partition X to it.  It properly bootstraps it and catches up.  Upon catch up, the current leader executes a leader election and assume broker A wins.  Broker A then does some bootstrapping before ""switching on"" the partition and serving fetch/produce requests.  Part of the bootstrap is determining which replicas are in ISR (read from ZK) by waiting for the replica to catch up, but because the server isn't responding to fetch requests for the replica and it isn't aware of where every replica is in terms of its HW and LEO, none will ever catch up ""in time"".  Am I missing something?;;;","01/Mar/12 20:08;nehanarkhede;This approach of having a queue of state change requests that each replica acts upon, is something I'm leaning towards for all state changes. 

There are 2 ways of making state changes in a system which uses ZK listeners -

1. Each server listens on various ZK paths, registers the same listeners, and follows the same code path to apply state changes to itself. Here, the state machine, is replicated on each server.
2. A highly-available co-ordinator listens of various ZK paths, registers ZK listeners, verifies system state and state transitions. Then issues state transition requests to the various replicas. Here, only the co-ordinator executes the state machine.

We have been down approach 1 earlier with the zookeeper consumer, and through experience, found that though, it seems simpler to design and implement at first, it turns into a fairly buggy and high operational overhead system. This is because that approach suffers from 

1. herd effect
2. ""split brain"" problem. 
3. In addition to these, it will be pretty complicated to perform upgrades on the state machine and can leave the cluster in an undefined state during upgrades.  
4. Monitoring the state machine is a hassle, due to it being distributed in nature

Approach 2 ensures the state machine only on the co-ordinator, which itself is elected from amongst the brokers. This approach ensures that - 

1. at any point of time, we can reason about the state of the entire cluster.
2. Only after the state is verified, can further state changes be applied. If verification fails, alerts can be triggered preventing the system from getting into an indefinite state.
3. A big advantage of this approach is easier upgrades to the state machine. It is true that, theoretically, state machine logic doesn't change much over time, but in reality, state machine changes would need upgrades, due to improvements in the logic or fixing code bugs. 
4. Monitoring the state machine becomes much simpler

In general, both approaches are “doable”, but we need to weigh the cost of “patching” the code to make it work VS choosing a simple design that will be easy to maintain and monitor.

I would like to see a discussion on this fundamental design choice, before jumping to code and patches on KAFKA-44 and KAFKA-45.  ;;;","01/Mar/12 22:56;prashanth.menon;That sounds fair enough.  Can we create a wiki page attached to one created for the overall replication work as mentioned in KAFKA-50?;;;","05/Mar/12 03:52;prashanth.menon;Also, mind if I assign this to myself?;;;","13/Mar/12 00:24;nehanarkhede;This JIRA can implement the stateChangeListener() as described in the Kafka replication design document, and leave stubs for becomeFollower()/becomeLeader() which are part of KAFKA-302. Also, lets leave out anything about partition reassignment for now. That work is included as part of other JIRAs and can be done when the basic replication functionality is nailed and tested.

Prashanth, I've attempted to reduce dependencies and define scope of KAFKA-44 and KAFKA-45. Hopefully the above clarifies the scope of this JIRA. Assigning it to you, as per your request;;;","13/Mar/12 02:11;prashanth.menon;Sounds good to me.  I'm also fine with removing this ticket and rolling this work as another subtask of KAFKA-45 just for clarity.  Your decision :);;;","16/Mar/12 17:04;nehanarkhede;This is very closely related to the broker startup procedure;;;","10/Jun/12 20:42;nehanarkhede;Fixed as part of KAFKA-301;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rebalance to preferred broke with intra-cluster replication support,KAFKA-43,12514680,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,junrao,,19/Jul/11 21:32,13/Oct/12 00:45,14/Jul/23 05:39,13/Oct/12 00:40,,,,0.8.0,,,,,,,core,,,0,features,,,"We need to allow the leader to be moved to the preferred broker, for better load balancing.",,jjkoshy,junrao,nehanarkhede,,,,,,,,,,,,,,,,,,,,,,,,,,604800,604800,,0%,604800,604800,,,KAFKA-499,,KAFKA-50,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/12 23:05;nehanarkhede;kafka-43-v1.patch;https://issues.apache.org/jira/secure/attachment/12548818/kafka-43-v1.patch","12/Oct/12 23:27;nehanarkhede;kafka-43-v2.patch;https://issues.apache.org/jira/secure/attachment/12548985/kafka-43-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67072,,,Sat Oct 13 00:45:13 UTC 2012,,,,,,,,,,"0|i029d3:",11118,,,,,,,,,,,,,,,,,,,,"11/Oct/12 23:05;nehanarkhede;Added the ability to move the leader to the preferred replica for a partition. This patch contains the following changes -

1. A new admin command PreferredReplicaLeaderElectionCommand and corresponding bin script

2. The admin command takes list of partitions as an input json file and writes it to the zookeeper persistent path /admin/preferred_replica_election

3. The controller registers a data change listener on the admin path. For each such partition, it triggers the OnlinePartition -> OnlinePartition state change with the preferred replica election leader selector. It batches the state changes for all partitions to be able to batch the leader and isr requests sent to the brokers. This is done since there is a very high chance that the admin might use this feature to move some leaders to a particular broker. The controller deletes the admin path once it has finished processing all partitions

4. Added 2 new unit tests to AdminTest;;;","11/Oct/12 23:46;nehanarkhede;In addition to the above, I realized there was another bug while using the (String, Int) tuple for topic and partition. So I changed all such usages in controller with TopicAndPartition. Didn't change other places in code to keep this patch contained.;;;","12/Oct/12 20:25;jjkoshy;Patch doesn't apply to 0.8 head so you will need to rebase. Here are some additional comments:

PreferredReplicaLeaderElectionCommand:
- There's a Utils.checkRequiredArgs helper that you can use.
- Shutdown hook should close zkClient.
- updatePreferredReplicaElectionData is unused - or is this meant to support concurrent invocations of the tool in the
  future?
- Would be good to have a short --help string for all the admin tools.
- Spacing conventions: some places have no space between the last character and opening brace, else should be on new
  line, etc.

KafkaController:
- Instead of nulls in the controller context, can we switch to Set.empty? I can try that in KAFKA-340 as well.
- Would be clearer to rename removePartitionsFromPreferredReplicaElection to maybeRemove...
- Also, the name removePartitionsFromPreferredReplicationElection does not document the side-effect of deleting the
  PreferredReplicaLeaderElectionPath. The delete can be moved to a finally block of handleDataChange. In fact,
  it seems removePartitionsFromPreferredReplicaElection can also be moved (from onPreferredReplicaElection) to the
  finally block.
- Likewise, the initialize(ReassignedPartitions|PreferredElection)Context have the additional side-effect of actually
  triggering the operation so can we rename accordingly? e.g., triggerPreferredReplicaLeaderElection would be a
  sufficient name since these two ""contexts"" really live in ControllerContext.

Revert test log4j.properties

TestUtils.readFileIntoString can be removed.
;;;","12/Oct/12 23:22;junrao;Overall, patch looks good. It would useful to add an option so that the controller will try to move all partitions to the preferred replica.;;;","12/Oct/12 23:27;nehanarkhede;Thanks for the review, Jun and Joel ! Here is a follow up patch to address your review suggestions - 

Joel's review

>> - Would be good to have a short --help string for all the admin tools.
That is provided by the joptparser when you invoke it without any parameters

KafkaController:
>> - Instead of nulls in the controller context, can we switch to Set.empty? I can try that in KAFKA-340 as well.
Would like to do that in a follow up cleanup patch. If you get to it before, that's fine as well.

>> - Would be clearer to rename removePartitionsFromPreferredReplicaElection to maybeRemove...
Didn't rename this since it *always* removes the partitions

Rest of the comments are addressed

Jun's review

That's a great point. Changed the tool to default to all partitions. 

Also, found a bug in the PreferredReplicaPartitionLeaderSelector where it didn't check if the preferred replica was in the isr or not. Fixed that.
;;;","13/Oct/12 00:25;junrao;Thanks for patch v2.
Thanks for patch v2. Just some minor comments. Once addressed, the patch can be checked in without another review.

20. AdminTest:
20.1 testBasicPreferredReplicaElection(): remove the println statement
20.2 remove unused imports

21. KafkaController.removePartitionsFromPreferredReplicaElection(): If we can't move the leader, we should probably log it as warning instead of error.

22. PartitionLeaderSelector.PreferredReplicaPartitionLeaderSelector(): Instead of using match/case, it seems it's simpler if we write it as if/else. If the preferred replica is in isr and is live, then we move the leader to the preferred replica. Otherwise, we just throw an exception.

23. TestUtils.readFileIntoString(): not used.



;;;","13/Oct/12 00:40;nehanarkhede;Addressed v2 review comments and checked in.;;;","13/Oct/12 00:45;jjkoshy;re: removePartitionsFromPreferredREplicaElection - yes I misread that.

For --help I was referring to a description of what the tool does. On second thoughts, a well-named script + good docs on the arguments is better.

I'll address getting rid of the nulls in KAFKA-340.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support rebalancing the partitions with replication,KAFKA-42,12514679,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Blocker,Fixed,nehanarkhede,junrao,,19/Jul/11 21:32,30/Dec/14 00:36,14/Jul/23 05:39,10/Oct/12 21:27,,,,0.8.0,,,,,,,core,,,0,features,,,"As new brokers are added, we need to support moving partition replicas from one set of brokers to another, online.",,bongster,edgefox,guozhang,junrao,nehanarkhede,parth.brahmbhatt,,,,,,,,,,,,,,,,,,,,,,,864000,864000,,0%,864000,864000,,,KAFKA-498,KAFKA-499,KAFKA-50,,,,,KAFKA-525,,,,,,,,,,,,,,,,,,,,,,"17/Jul/14 01:35;bongster;KAFKA-42.patch;https://issues.apache.org/jira/secure/attachment/12656198/KAFKA-42.patch","24/Jul/14 15:36;edgefox;KAFKA-42_2014-07-24_15:36:04.patch;https://issues.apache.org/jira/secure/attachment/12657620/KAFKA-42_2014-07-24_15%3A36%3A04.patch","24/Jul/14 15:37;edgefox;KAFKA-42_2014-07-24_15:37:26.patch;https://issues.apache.org/jira/secure/attachment/12657621/KAFKA-42_2014-07-24_15%3A37%3A26.patch","30/Dec/14 00:36;parth.brahmbhatt;KAFKA-42_2014-12-29_16:36:41.patch;https://issues.apache.org/jira/secure/attachment/12689410/KAFKA-42_2014-12-29_16%3A36%3A41.patch","25/Sep/12 17:34;nehanarkhede;kafka-42-v1.patch;https://issues.apache.org/jira/secure/attachment/12546556/kafka-42-v1.patch","02/Oct/12 06:43;nehanarkhede;kafka-42-v2.patch;https://issues.apache.org/jira/secure/attachment/12547350/kafka-42-v2.patch","07/Oct/12 18:11;nehanarkhede;kafka-42-v3.patch;https://issues.apache.org/jira/secure/attachment/12548167/kafka-42-v3.patch","09/Oct/12 02:29;nehanarkhede;kafka-42-v4.patch;https://issues.apache.org/jira/secure/attachment/12548352/kafka-42-v4.patch","09/Oct/12 22:54;nehanarkhede;kafka-42-v5.patch;https://issues.apache.org/jira/secure/attachment/12548489/kafka-42-v5.patch",,,,,,,,,,,,,,,,,,,9.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67074,,,Tue Dec 30 00:36:48 UTC 2014,,,,,,,,,,"0|i014e7:",4481,,,,,,,,,,,,,,,,,,,,"25/Sep/12 17:34;nehanarkhede;This is a pretty tricky feature. Since it involves multiple state changes before reassignment can be marked complete, there are many failure conditions to think about and handle recovery correctly

1. Admin tool changes
1.1 Added a new admin command reassign-partition. Right now, it handles one partition, since I thought the failure/exit conditions and error messages are simpler to handle. But if people think we should add multiple partitions support in the same command invocation, that is fine too.
1.2 Added a new reassignPartition(topic, partition, RAR) API that registers a data change listener on /admin/reassign_partitions path and then creates the /admin/reassign_partitions={} path in zookeeper. It waits until that path is deleted from zookeeper. Once it is deleted, it checks if AR == RAR. If yes, it reports success otherwise failure.
1.3 Added a shutdown hook to handle command cancellation by the admin. In this case, it checks if reassignment was completed or not and logs the output accordingly.

2. Controller changes

Reassigning replicas for a partition goes through a few stages -
RAR = Reassigned replicas
AR = Original list of replicas for partition

1. Register listener for ISR changes to detect when the RAR is a subset of the ISR
2. Start new replicas RAR - AR.
3. Wait until new replicas are in sync with the leader
4. If the leader is not in RAR, elect a new leader from RAR
5. Stop old replicas AR - RAR
6. Write new AR
7. Remove partition from the /admin/reassign_partitions path

The above state changes steps are inside the onPartitionReassignment() callback in KafkaController.scala

3. Partition Reassignment failure cases

Broadly there are 2 types of failures we need to worry about -
1. Controller failover
2. Runtime error at the broker hosting the replica

Let's go through the failure cases and recovery -
1. If the controller fails over between steps 1 and 2, the new controller on startup will read the non-empty admin path and just restart the partition reassignment process from scratch
2a. If the controller fails over between steps 2 and 3 above, the new controller will check if the new replicas are in sync with the leader or not. In either case, it will resume partition reassignment for the partitions listed in the admin path
2b. If, for some reason, the broker is not able to start the replicas, the isr listener for reassigned partitions will not trigger. So, the controller will not resume partition reassignment process for that partition. After some time, the admin command can be killed and it will report failure and delete the admin path so it can be retried.
3. If the controller fails over between steps 4 and 5, the new controller will realize that the new replicas are already in sync. If the new leader is part of the new replicas and is alive, it will not trigger leader re-election. Else it will re-elect the leader from amongst the live reassigned replicas.
4a. If the controller fails over between steps 5 and 6, the new controller resumes partition reassignment and repeats steps 4 onwards
4b. If, for some reason, the broker does not complete the leader state change, the partition after reassignment will be offline. This is a problem we have today even for leader election of newly created partitions. The controller doesn't wait for an acknowledgement from the broker for the make-leader state change. Nevertheless, the broker can fail even after sending a successful ack, so there isn't much value in waiting for an ack. However, I think the leader broker should expose an mbean to signify the availability of a partition. If people think this is a good idea, I can file a bug to fix this.
5. If the controller fails over between steps 6 and 7, it deletes the partition from the admin path marking the completion of this partition's reassignment. The partition reassignment zookeeper listener should record partition to be reassigned only if RAR not equal AR.

4. PartitionReassignedListener

Starts the partition reassignment process unless -
1. Partition previously existed
2. New replicas are the same as existing replicas
3. Any replica in the new set of replicas are dead

If any of the above conditions are satisfies, it logs an error and removes the partition from list of reassigned partitions notifying the admin command about the failure/completion.

5. PartitionLeaderSelector

Added a self transition on the OnlinePartition state change. This is because, with cluster expansion and preferred replica leader election features, we need to move the leader for online partitions as well.

Added partition leader selector module since we have 3 different ways of selecting the leader for a partition -
1. Offline leader selector - Pick an alive in sync replica as the leader. Otherwise, pick an alive assigned replica
2. Reassigned partition leader selector - Pick one of the alive in-sync reassigned replicas as the new leader
3. Preferred replica leader selector - Pick the preferred replica as the new leader
4. Testing

6. Replica state machine changes
Added 2 new states to the replica state machine -
1. NewReplica        : The controller can create new replicas during partition reassignment. In this state, a
                       replica can only get become follower state change request.  Valid previous
                       state is NonExistentReplica
2. OnlineReplica     : Once a replica is started and part of the assigned replicas for its partition, it is in this
                       state. In this state, it can get either become leader or become follower state change requests.
                       Valid previous state are NewReplica, OnlineReplica or OfflineReplica
3. OfflineReplica    : If a replica dies, it moves to this state. This happens when the broker hosting the replica
                       is down. Valid previous state are NewReplica, OnlineReplica
4. NonExistentReplica: If a replica is deleted, it is moved to this state. Valid previous state is OfflineReplica

7. Added 6 unit test cases to test -
1. Partition reassignment with leader of the partition in the new list of replicas
2. Partition reassignment with leader of the partition NOT in the new list of replicas
3. Partition reassignment with existing assigned replicas NOT overlapping with new list of replicas
4. Partition reassignment for a non existing partition. This is a negative test case
5. Partition reassignment for a partition that was completed upto step 6 by previous controller. This tests if after controller failover, it handles marking that partition's reassignment as completed.
6. Partition reassignment for a partition that was completed upto step 3 by previous controller. This tests if after controller failover, it handles leader re-election correctly and completes rest of the partition reassignment process.
;;;","25/Sep/12 18:10;nehanarkhede;Patch v1 contains a fix for KAFKA-525;;;","26/Sep/12 22:00;junrao;Thanks for patch v1. Looks good overall. Some comments:

1. ReassignPartitionsCommand:
1.1 If a partition doesn't exist, we should fail the operation immediately without updating ReassignPartitionsPath.
1.2 I think it would be useful to support migrating multiple topics and partitions. We can just take a JSON file that describes the new replicas as input.
1.3 If ReassignPartitionsPath already exists, we should quit immediately and not overwrite the path. This means that we will only allow 1 outstanding cluster rebalance at a given point of time, which is ok as long as the admin command allows multiple topic/partition being specified. 

2. Currently, we fail the partition reassignment operation if any broker in RAR is down, during initialization. However, brokers in RAR can go down after initialization. So, it would be good if we can handle RAR failures. Probably the only change needed is that when a broker is online, we need to start those replicas in RAR too.

3. The logic is now get more complicated with the reassginment logic. Could we describe how it works in a comment? 

4. PartitionLeaderSelector.selectLeader(): describe what the return value is in the comment.

5. ReassignedPartitionsIsrChangeListener.handleDataChange(): The following statement is weird. Only controller can change leaders and the controller always updates the leader cache every time a leader is changed. So, there shouldn't be a need for updating the leader cache on ZK listeners.
                controllerContext.allLeaders.put((topic, partition), leaderAndIsr.leader)

6. KafkaController.onPartitionReassignment(): Could we put the logic that makes sure all replicas RAR are is ISR in  onPartitionReassignment()? Currently, that logic is duplicated in 2-3 places and that logic is always followed by a call to onPartitionReassignment(). If we do this, do we still need ReassignedPartitionsContext.areNewReplicasInIsr?

7. ReplicaStateChangeMachine:
7.1 NonExistentReplica: The controller holds on to all replicas in this state. Is this necessary? Can we just remove them from the replicaState map.
7.2 In the following code, we don't really need to read from ZK and can use the cached data.
            case _ =>
              // check if the leader for this partition is alive or even exists
              // NOTE: technically, we could get the leader from the allLeaders cache, but we need to read zookeeper
              // for the ISR anyways
              val leaderAndIsrOpt = ZkUtils.getLeaderAndIsrForPartition(zkClient, topic, partition)

8. AdminTest:
8.1 testPartitionReassignmentWithLeaderInNewReplicas: How do we make sure that replica 0 is always the leader?
8.2 testResumePartitionReassignmentThatWasCompleted: Towards the end, the comment says leader should be 2, but there is no broker 2 in the test.

9. ControllerBrokerRequestBatch: Should we rename the two addRequestForBrokers to addLeaderAndIsrRequestForBrokers and addStopReplicaRequestForBrokers respectively?

10. PartitionOfflineException,StateChangeFailedException: We can probably change the implementation to use RuntimeException(message, throwable) directly.

11. LeaderElectionTest.testLeaderElectionAndEpoch(): Not sure if the change is correct. If there is no leadership change, leader epoch shouldn't change, right?
;;;","02/Oct/12 06:43;nehanarkhede;1. ReassignPartitionsCommand:
1.1  Makes sense, changed that.

1.2 I think that makes sense. Thinking about this more, I guess it is not such a good idea to block the admin command until all the partitions are successfully reassigned. I changed the reassign partitions admin command to issue the partition reassignment request if that path doesn't already exist. This protects accidentally overwriting the zookeeper path. I also added a check reassignment status admin command that will report if the reassignment status of a partition is completed/failed/in progress. Also, another thing to be careful about a batch reassignment API is to avoid piling up important state change requests on the controller while it reassigns multiple partitions. Since reassignment of partitions is not an urgent state change, we should give up the controller lock after each partition is reassigned. That will ensure that other state changes can sneak in, if necessary

1.3 Yes, forgot to include that in v1 patch.

2. Initially, I thought the admin could just re-run the partition reassignment command, but I realize that it involes one manual step.

3, 4 Sure

5. Good point, removed it.

6. This check is not done on every single invocation of onPartitionReassignment, it is done on controller failover and isr change listener. It is not required to be done when the partition reassigned callback triggers. But I think it is a good idea to move it to the callback, just in case we have not covered scenarios when the check should be done.

7.1  While changing the state of a replica to NewReplica, we need to ensure that it was in the NonExistentReplica state. We can remove the replica from the replicaState map after it moves to the NonExistentReplica state explicitly, but there is a chance it will be added back to the map again. This can happen if we re-start the replica after stopping it. But, since this is infrequent, I made this change.

7.2 We do not cache the isr which is required for the controller to be able to send a leader and isr request to the broker
Besides, this operation is only invoked when a new broker is started or controller fails over. Both of these operations are rare enough that we don't need to worry about optimizing this.


8.1 There is a very good chance that it will be. This is because, we always pick the first alive assigned replica as the leader. Since replica 0 is the first assigned replica and is never shut down during the test, it will be the leader. Even if, due to some rare zookeeper session expiration issue, it is not the leader, the test will not fail.

8.2 The comment is redundant there, so I removed it

9, 10. Good point, fixed it

11. It is correct since the controller increments the epoch for isr changes made by itself.
;;;","03/Oct/12 22:53;junrao;Thanks for patch v2. Some more comments:

20. ReassignPartitionsCommand:
20.1 Could we add a description of the format of the jaon file in the command line option?
20.2 If partitionsToBeReassigned is an empty, should we just fail the command?
20.3 reassignPartitions(): Instead of check the existence of ReassignPartitionsPath and then write in ZK, it's better to use ZkUtils.createPersistentPath(), which throws an exception if node already exists. This will prevent the corner case that the path is created just after the existence check.
20.4 createReassignedPartitionsPathInZK: It seems that each call to this method just overwrites ReassignPartitionsPath with 1 partition's assignment. So we will lose the assignments of all partitions except the last one?

21. CheckReassignmentStatus: It's better to move checkIfReassignmentSucceeded and checkIfPartitionReassignmentSucceeded from ZkUtils to CheckReassignmentStatus since they are only used here and ZkUtils is getting big.

22. KafkaController.onBrokerStartup() : It seems that we can get partitionsBeingReassigned from the cache in controllerContext, instead of from ZK.

23. PartitionStateMachine.initializeLeaderAndIsrForPartiiton(): When writing the initial leaderAndIsr path for a new partition, there is no need to read the path first to make sure that it doesn't exists. createPersistentPath will throw an exception if the path exists.
;;;","07/Oct/12 18:11;nehanarkhede;20. ReassignPartitionsCommand:
20.1, 20.2 Sure, that is a good idea
20.3 For this corner case to happen, another instance of the admin command would have to run at the right time. If that happens, both the admin commands might see that the path doesn't exist and try to create it. At this point, one of the admin commands will get an error and it will exit.
20.4 Good catch, that is a bug. Initially, I wrote the entire map of all partitions using that API. But later, for per-partition sanity checks, changed it to get invoked for every partition and that probably introduced the bug.

21. They are used in AdminTest as well, but this makes sense.

22, 23. Included these optimizations.
;;;","08/Oct/12 18:29;junrao;Thanks for patch v3. Looks good to me overall. Just one comment:

20.3 The problem is that reassignPartitions() uses updatePartitionReassignmentData, which in turn uses updatePersistentPath. updatePersistentPath won't throw an exception if a node already exists. So, what could happen is that 2 admin commands are issued at the same time. Both pass the existence test of the ZK path. One command writes its data in the reassignment path first. The other one then overwrites it. Now, both commands appear to have completed successfully. Using ZkUtils.createPersistentPath() instead of updatePersistentPath() would prevent this since the former throws an exception if the path already exists.

;;;","08/Oct/12 22:30;nehanarkhede;20.3 Good point, I see what you are saying. Fixed it;;;","09/Oct/12 04:33;junrao;Thanks for patch v4. AdminTest.testResumePartitionReassignmentAfterLeaderWasMoved seems to fail.

[2012-10-08 21:30:06,005] ERROR [PartitionsReassignedListener on 0]: Error completing reassignment of partition [test, 0] (kafka.controller.PartitionsReassignedListener:102)
kafka.common.KafkaException: Only  replicas out of the new set of replicas 2,3 for partition [test, 0] to be reassigned are alive. Failing partition reassignment
	at kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.liftedTree1$1(KafkaController.scala:512)
	at kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.apply(KafkaController.scala:495)
	at kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.apply(KafkaController.scala:489)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
	at kafka.controller.PartitionsReassignedListener.handleDataChange(KafkaController.scala:489)
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:547)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[2012-10-08 21:30:06,280] ERROR [PartitionsReassignedListener on 0]: Error completing reassignment of partition [test, 0] (kafka.controller.PartitionsReassignedListener:102)
org.I0Itec.zkclient.exception.ZkInterruptedException: java.lang.InterruptedException
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:687)
	at org.I0Itec.zkclient.ZkClient.readData(ZkClient.java:766)
	at org.I0Itec.zkclient.ZkClient.readData(ZkClient.java:761)
	at kafka.utils.ZkUtils$.readDataMaybeNull(ZkUtils.scala:363)
	at kafka.utils.ZkUtils$.getLeaderAndIsrForPartition(ZkUtils.scala:78)
	at kafka.controller.KafkaController.areReplicasInIsr(KafkaController.scala:323)
	at kafka.controller.KafkaController.onPartitionReassignment(KafkaController.scala:183)
	at kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.liftedTree1$1(KafkaController.scala:509)
	at kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.apply(KafkaController.scala:495)
	at kafka.controller.PartitionsReassignedListener$$anonfun$handleDataChange$2.apply(KafkaController.scala:489)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:105)
	at kafka.controller.PartitionsReassignedListener.handleDataChange(KafkaController.scala:489)
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:547)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
Caused by: java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1344)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:925)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:956)
	at org.I0Itec.zkclient.ZkConnection.readData(ZkConnection.java:103)
	at org.I0Itec.zkclient.ZkClient$9.call(ZkClient.java:770)
	at org.I0Itec.zkclient.ZkClient$9.call(ZkClient.java:766)
	at org.I0Itec.zkclient.ZkClient.retryUntilConnected(ZkClient.java:675)
	... 13 more
[2012-10-08 21:30:06,377] ERROR [Replica state machine on Controller 3]: Error while changing state of replica 2 for partition [test, 0] to OnlineReplica (kafka.controller.ReplicaStateMachine:102)
java.lang.AssertionError: assertion failed: Replica 2 for partition [test, 0] should be in the NewReplica,OnlineReplica,OfflineReplica states before moving to OnlineReplica state. Instead it is in NonExistentReplica state
	at scala.Predef$.assert(Predef.scala:91)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:194)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:130)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
	at scala.collection.immutable.List.foreach(List.scala:45)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:86)
	at kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply$mcVI$sp(KafkaController.scala:187)
	at kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply(KafkaController.scala:186)
	at kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply(KafkaController.scala:186)
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
	at scala.collection.immutable.List.foreach(List.scala:45)
	at kafka.controller.KafkaController.onPartitionReassignment(KafkaController.scala:186)
	at kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:300)
	at kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:299)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
	at kafka.controller.KafkaController.initializeReassignedPartitionsContext(KafkaController.scala:299)
	at kafka.controller.KafkaController.initializeControllerContext(KafkaController.scala:284)
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:79)
	at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:52)
	at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:55)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:94)
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[2012-10-08 21:30:06,379] ERROR [Replica state machine on Controller 3]: Error while changing state of replica 3 for partition [test, 0] to OnlineReplica (kafka.controller.ReplicaStateMachine:102)
java.lang.AssertionError: assertion failed: Replica 3 for partition [test, 0] should be in the NewReplica,OnlineReplica,OfflineReplica states before moving to OnlineReplica state. Instead it is in NonExistentReplica state
	at scala.Predef$.assert(Predef.scala:91)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:194)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:130)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
	at scala.collection.immutable.List.foreach(List.scala:45)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:86)
	at kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply$mcVI$sp(KafkaController.scala:187)
	at kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply(KafkaController.scala:186)
	at kafka.controller.KafkaController$$anonfun$onPartitionReassignment$1.apply(KafkaController.scala:186)
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
	at scala.collection.immutable.List.foreach(List.scala:45)
	at kafka.controller.KafkaController.onPartitionReassignment(KafkaController.scala:186)
	at kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:300)
	at kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:299)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
	at kafka.controller.KafkaController.initializeReassignedPartitionsContext(KafkaController.scala:299)
	at kafka.controller.KafkaController.initializeControllerContext(KafkaController.scala:284)
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:79)
	at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:52)
	at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:55)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:94)
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[2012-10-08 21:30:06,381] ERROR [Replica state machine on Controller 3]: Error while changing state of replica 0 for partition [test, 0] to OfflineReplica (kafka.controller.ReplicaStateMachine:102)
java.lang.AssertionError: assertion failed: Replica 0 for partition [test, 0] should be in the NewReplica,OnlineReplica states before moving to OfflineReplica state. Instead it is in NonExistentReplica state
	at scala.Predef$.assert(Predef.scala:91)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:194)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:156)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:86)
	at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
	at scala.collection.immutable.List.foreach(List.scala:45)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:86)
	at kafka.controller.KafkaController$$anonfun$stopOldReplicasOfReassignedPartition$1.apply$mcVI$sp(KafkaController.scala:363)
	at kafka.controller.KafkaController$$anonfun$stopOldReplicasOfReassignedPartition$1.apply(KafkaController.scala:362)
	at kafka.controller.KafkaController$$anonfun$stopOldReplicasOfReassignedPartition$1.apply(KafkaController.scala:362)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:101)
	at kafka.controller.KafkaController.stopOldReplicasOfReassignedPartition(KafkaController.scala:362)
	at kafka.controller.KafkaController.onPartitionReassignment(KafkaController.scala:193)
	at kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:300)
	at kafka.controller.KafkaController$$anonfun$initializeReassignedPartitionsContext$5.apply(KafkaController.scala:299)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:80)
	at scala.collection.Iterator$class.foreach(Iterator.scala:631)
	at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:161)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:194)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:80)
	at kafka.controller.KafkaController.initializeReassignedPartitionsContext(KafkaController.scala:299)
	at kafka.controller.KafkaController.initializeControllerContext(KafkaController.scala:284)
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:79)
	at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:52)
	at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:55)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:94)
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)

Partition should have been reassigned to 0, 1 expected:<List(2, 3)> but was:<List(0, 1)>
junit.framework.AssertionFailedError: Partition should have been reassigned to 0, 1 expected:<List(2, 3)> but was:<List(0, 1)>
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.failNotEquals(Assert.java:277)
	at junit.framework.Assert.assertEquals(Assert.java:64)
	at kafka.admin.AdminTest.testResumePartitionReassignmentAfterLeaderWasMoved(AdminTest.scala:361)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
;;;","09/Oct/12 04:34;junrao;Also, could you make the two new scripts in bin/ executable?;;;","09/Oct/12 22:54;nehanarkhede;The intermittent test failure is due to the partition reassignment failing to complete due to ZkInterruptedException. This is probably due to the test trying to introduce a controller failover. But controller failover takes some time to restart the partition reassignment and the test failed due to a lower value of the timeout. I fixed the comment in the test assertion and increased the wait time. Now all test seem to pass after couple of iterations;;;","09/Oct/12 22:55;nehanarkhede;Also, I will make the scripts executable before checking in;;;","10/Oct/12 16:45;nehanarkhede;This is blocking work on KAFKA-43, if no one has objections on v5, I will commit this today;;;","10/Oct/12 16:52;junrao;+1 on the patch. Still see AdminTest.testResumePartitionReassignmentAfterLeaderWasMoved fails occasionally due to the same error. However, it seems to be less frequent now. 

Unfortunately, you will need to rebase. The patch can be committed after the rebase.;;;","10/Oct/12 17:23;nehanarkhede;I fear I might end up rebasing incorrectly and volunteer for applying KAFKA-510 top of this rather than other way around. This is going to require me to revert KAFKA-510, apply KAFKA-42 and then re-apply KAFKA-510. ;;;","10/Oct/12 21:27;nehanarkhede;Committed this;;;","17/Jul/14 01:35;bongster;Created reviewboard  against branch origin/trunk;;;","24/Jul/14 15:38;edgefox;Updated reviewboard  against branch apache/0.8.1;;;","24/Jul/14 17:07;junrao;This jira is already closed. Is the patch for this jira?;;;","24/Jul/14 17:17;guozhang;I think this is just due to the review-tool, which use the magic number ""42"" when no jira number is specified.;;;","30/Dec/14 00:36;parth.brahmbhatt;Updated reviewboard https://reviews.apache.org/r/29468/diff/
 against branch origin/trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
multi-produce and multi-fetch support with replication,KAFKA-41,12514678,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,junrao,,19/Jul/11 21:32,06/Apr/12 17:53,14/Jul/23 05:39,06/Apr/12 17:53,,,,,,,,,,,,,,0,replication,,,We need to figure out how to support multi-produce and multi-fetch smoothly with the replication support. The client has to figure out which partitions are collocated on the same broker and adjust accordingly when some partitions are moved.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67075,,,Fri Apr 06 17:53:22 UTC 2012,,,,,,,,,,"0|i15yof:",242918,,,,,,,,,,,,,,,,,,,,"23/Jan/12 19:42;prashanth.menon;Is this ticket still required considering the changes being made to the produce and fetch requests?  If we've agreed on implementing the backwards-incompatible wire-protocol changes, this ticket is uncessary, right?;;;","23/Jan/12 21:03;junrao;Yes, this may be covered by the new wire protocol change and KAFKA-239.;;;","06/Apr/12 17:53;nehanarkhede;This is fixed as part of KAFKA-239;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Quick start still references SimpleProducer,KAFKA-37,12514674,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,,,,0.6,,,,,,,,,,0,,,,"The quickstart kafka page still references ""SimpleProducer"". As of the latest github checkout, this no longer exists.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67482,,,2011-07-19 21:32:18.0,,,,,,,,,,"0|i15ynz:",242916,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NotCompliantMBeanException while creating kafka.javaapi.consumer.ZookeeperConsumerConnector,KAFKA-34,12514671,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,,,,0.6,,,,,,,,,,0,,,,"When you try to create a zk consumer through the java api - 

javax.management.NotCompliantMBeanException: MBean class kafka.javaapi.consumer.ZookeeperConsumerConnector does not implement DynamicMBean, neither follows the Standard MBean conventions (javax.management.NotCompliantMBeanException: Class kafka.javaapi.consumer.ZookeeperConsumerConnector is not a JMX compliant Standard MBean) nor the MXBean conventions (javax.management.NotCompliantMBeanException: kafka.javaapi.consumer.ZookeeperConsumerConnector: Class kafka.javaapi.consumer.ZookeeperConsumerConnector is not a JMX compliant MXBean) 
at com.sun.jmx.mbeanserver.Introspector.checkCompliance(Introspector.java:160) 
at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:305) 
at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67479,,,2011-07-19 21:32:18.0,,,,,,,,,,"0|i15ynb:",242913,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
consumer picks up wrong offset during rebalance,KAFKA-32,12514669,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,0.6,,,0.6,,,,,,,,,,0,,,,"Occasionally, we saw a consumer picks up wrong offset during rebalance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67484,,,2011-07-19 21:32:17.0,,,,,,,,,,"0|i15ymv:",242911,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle ZK exception properly in consumer,KAFKA-31,12514668,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,0.6,,,0.6,,,,,,,,,,0,,,,"Occasionally, during rebalance, we may hit a ZK exception because a ZK node that we thought is there suddenly disappear. When this happens, we need to reset the consumer state before the next rebalance happens.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67485,,,2011-07-19 21:32:17.0,,,,,,,,,,"0|i15ymn:",242910,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unrequired explicit dependency on specific versions of certain libraries,KAFKA-22,12514659,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,0.6,,,0.6,,,,,,,,,,0,,,,"Currently, kafka explicitly depends on a certain version of cglib - 2.1_3. On linux, this version causes NoSuchMethodErrors. 

Also, the dependency on asm is not required. The correct set of dependencies are - 

scalatest 
junit 
easymock 3.0 (this pulls in the correct versions of cglib and objenesis)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67496,,,2011-07-19 21:32:14.0,,,,,,,,,,"0|i15ykf:",242900,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Increase sleep timeout in AutoOffsetResetTest,KAFKA-21,12514658,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,0.6,,,0.6,,,,,,,,,,0,,,,"The auto offset reset integration tests in AutoOffsetResetTest.scala, the timeout of 1100 is not enough for some Linux machines. 

Increasing that timeout to 2seconds fixes the issue.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67493,,,2011-07-19 21:32:14.0,,,,,,,,,,"0|i15yk7:",242899,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZookeeperConsumerConnectorMBean needs to close SimpleConsumer when done,KAFKA-17,12514654,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,0.6,,,0.6,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67500,,,2011-07-19 21:32:13.0,,,,,,,,,,"0|i15yjb:",242895,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SBT release-zip target doesn't include bin and config directories anymore,KAFKA-15,12514652,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,junrao,junrao,,19/Jul/11 21:32,13/Jun/13 16:27,14/Jul/23 05:39,13/Jun/13 16:26,0.6,,,0.8.0,,,,,,,,,,0,,,,"SBT release-zip target is responsible for creating a fully deployable release zip containing all the package jars, scripts in the bin directory and config property files. 
Currently, it packages the kafka jar and the lib directories correctly.",,junrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,KAFKA-134,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,43998,,,Thu Jun 13 16:26:05 UTC 2013,,,,,,,,,,"0|i09mdj:",54058,,,,,,,,,,,,,,,,,,,,"04/Oct/11 02:31;cburroughs;Unless we want to make this a one off, we need to fix this for the release.;;;","05/Oct/11 17:27;junrao;We have someone in our team who can help address this issue as part of https://issues.apache.org/jira/browse/KAFKA-134. Maybe for the 0.7 release, we can just add the bin and config directories manually?;;;","06/Oct/11 22:33;nehanarkhede;Moving to 0.8;;;","13/Jun/13 16:26;junrao;This is fixed in 0.8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZK exception in consumer when no broker has registered,KAFKA-14,12514651,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,0.6,,,0.6,,,,,,,,,,0,,,,The consumer will get a ZKNoNodeException if no broker has ever registered in ZK.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67502,,,2011-07-19 21:32:12.0,,,,,,,,,,"0|i15yiv:",242893,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZKBrokerPartitionInfo doesn't allow load balancing on a new topic,KAFKA-13,12514650,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,0.6,,,0.6,,,,,,,,,,0,,,,"The problem is that initially no broker has registered for a topic in ZK. Once the producer sends a message to a broker, that broker is registered in ZK. After that, the producer sticks with that broker.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67503,,,2011-07-19 21:32:12.0,,,,,,,,,,"0|i15yin:",242892,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve EventHandler in AsyncProducer,KAFKA-12,12514649,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,0.6,,,0.6,,,,,,,,,,0,,,,"There are a few issues with the current EventHandler. 
1. If an EventHandler is specified in a config file, the instantiator requires that the EventHandler has an empty constructor. 
2. Today, a user has to pass in the Encoder twice, once through the Producer and another through the EventHandler. 
3. The default EventHandler is not set (say, for events of String type).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67504,,,2011-07-19 21:32:11.0,,,,,,,,,,"0|i15yif:",242891,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The zookeeper based Producer doesn't remove a dead broker from its list while serving a produce request,KAFKA-11,12514648,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,0.6,,,0.6,,,,,,,,,,0,,,,"The producer registers a watcher on /brokers/ids to detect the new set of brokers in the cluster. It uses that to keep the producer pool connections updated. But, this watcher callback should also remove the dead brokers, if any, from its in memory data structure. This is important so that we don't accidentally pick a dead broker to serve a produce request. T",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67505,,,2011-07-19 21:32:11.0,,,,,,,,,,"0|i15yi7:",242890,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consumer logs ERROR during close,KAFKA-9,12514646,Bug,Closed,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Major,Fixed,,,,19/Jul/11 21:32,19/Jul/11 21:32,14/Jul/23 05:39,19/Jul/11 21:32,0.6,,,0.6,,,,,,,,,,0,,,,"When closing the consumer, we sometimes get the following error: 

[2011-05-16 13:24:39,203] ERROR consumed offset: 862812384944 doesn't match fetch offset: 862813943889 for PageViewEvent:2-0; consumer may lose data (kafka.consumer.ConsumerIterator)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67507,,,2011-07-19 21:32:11.0,,,,,,,,,,"0|i15yhj:",242887,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Confusing Error mesage from producer when no kafka brokers are available,KAFKA-4,12514641,Bug,Resolved,KAFKA,Kafka,software,mimaison,Apache Kafka is a distributed streaming platform.,http://kafka.apache.org/,Minor,Fixed,,,,19/Jul/11 21:32,16/Nov/17 09:11,14/Jul/23 05:39,16/Nov/17 09:11,0.6,,,0.11.0.0,,,,,,,,,,0,,,,"If no kafka brokers are available the producer gives the following error: 

Exception in thread ""main"" kafka.common.InvalidPartitionException: Invalid number of partitions: 0 
Valid values are > 0 
at kafka.producer.Producer.kafka$producer$Producer$$getPartition(Producer.scala:144) 
at kafka.producer.Producer$$anonfun$3.apply(Producer.scala:112) 
at kafka.producer.Producer$$anonfun$3.apply(Producer.scala:102) 
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) 
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) 
at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) 
at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32) 
at scala.collection.TraversableLike$class.map(TraversableLike.scala:206) 
at scala.collection.mutable.WrappedArray.map(WrappedArray.scala:32) 
at kafka.producer.Producer.send(Producer.scala:102) 
at kafka.javaapi.producer.Producer.send(Producer.scala:101) 
at com.linkedin.nusviewer.PublishTestMessage.main(PublishTestMessage.java:45) 

This is confusing. The problem is that no brokers are available, we should make this more clear.",,sliebau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KAFKA-5179,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,67512,,,Thu Nov 16 09:05:14 UTC 2017,,,,,,,,,,"0|i15ygn:",242883,,,,,,,,,,,,,,,,,,,,"16/Nov/17 09:05;sliebau;Current error message when no broker is available is:

{code}
WARN Connection to node -1 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)
{code}

This message was introduced by KAFKA-5179, so I think it is save to say that we can close this ticket as well with the same fix version. Before that there were other error messages that also improved upon this message, but I don't think we need to provide the entire history here..
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
